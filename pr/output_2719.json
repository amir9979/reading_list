[{"title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning", "link": "https://arxiv.org/pdf/2405.10292", "details": "Y Zhai, H Bai, Z Lin, J Pan, S Tong, Y Zhou, A Suhr\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision-language models (VLMs) fine-tuned on specialized visual instruction- following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently \u2026"}, {"title": "ECR-Chain: Advancing Generative Language Models to Better Emotion-Cause Reasoners through Reasoning Chains", "link": "https://arxiv.org/pdf/2405.10860", "details": "Z Huang, J Zhao, Q Jin - arXiv preprint arXiv:2405.10860, 2024", "abstract": "Understanding the process of emotion generation is crucial for analyzing the causes behind emotions. Causal Emotion Entailment (CEE), an emotion-understanding task, aims to identify the causal utterances in a conversation that stimulate the emotions \u2026"}, {"title": "Oropharyngeal Cancer Staging Health Record Extraction Using Artificial Intelligence", "link": "https://jamanetwork.com/journals/jamaotolaryngology/article-abstract/2818998", "details": "E Baran, M Lee, S Aviv, J Weiss, C Pettengell, I Karam\u2026 - JAMA Otolaryngology\u2013Head \u2026, 2024", "abstract": "Importance Accurate, timely, and cost-effective methods for staging oropharyngeal cancers are crucial for patient prognosis and treatment decisions, but staging documentation is often inaccurate or incomplete. With the emergence of artificial \u2026"}, {"title": "LG AI Research & KAIST at EHRSQL 2024: Self-Training Large Language Models with Pseudo-Labeled Unanswerable Questions for a Reliable Text-to-SQL System \u2026", "link": "https://arxiv.org/pdf/2405.11162", "details": "Y Jo, S Lee, M Seo, SJ Hwang, M Lee - arXiv preprint arXiv:2405.11162, 2024", "abstract": "Text-to-SQL models are pivotal for making Electronic Health Records (EHRs) accessible to healthcare professionals without SQL knowledge. With the advancements in large language models, these systems have become more adept at \u2026"}, {"title": "Leveraging Large Language Models for Knowledge-free Weak Supervision in Clinical Natural Language Processing", "link": "https://ui.adsabs.harvard.edu/abs/2024arXiv240606723H/abstract", "details": "E Hsu, K Roberts - arXiv e-prints, 2024", "abstract": "The performance of deep learning-based natural language processing systems is based on large amounts of labeled training data which, in the clinical domain, are not easily available or affordable. Weak supervision and in-context learning offer partial \u2026"}, {"title": "UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language Models", "link": "https://arxiv.org/pdf/2405.10311", "details": "S Sharifymoghaddam, S Upadhyay, W Chen, J Lin - arXiv preprint arXiv:2405.10311, 2024", "abstract": "Recently, Multi-Modal (MM) Large Language Models (LLMs) have unlocked many complex use-cases that require MM understanding (eg, image captioning or visual question answering) and MM generation (eg, text-guided image generation or \u2026"}, {"title": "Impact of high-quality, mixed-domain data on the performance of medical language models", "link": "https://academic.oup.com/jamia/advance-article-abstract/doi/10.1093/jamia/ocae120/7680487", "details": "M Griot, C Hemptinne, J Vanderdonckt, D Yuksel - Journal of the American Medical \u2026, 2024", "abstract": "Objective To optimize the training strategy of large language models for medical applications, focusing on creating clinically relevant systems that efficiently integrate into healthcare settings, while ensuring high standards of accuracy and reliability \u2026"}, {"title": "Super Tiny Language Models", "link": "https://arxiv.org/pdf/2405.14159", "details": "D Hillier, L Guertler, C Tan, P Agrawal, C Ruirui\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancement of large language models (LLMs) has led to significant improvements in natural language processing but also poses challenges due to their high computational and energy demands. This paper introduces a series of research \u2026"}, {"title": "Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks", "link": "https://arxiv.org/pdf/2405.10548", "details": "A Chatterjee, E Tanwar, S Dutta, T Chakraborty - arXiv preprint arXiv:2405.10548, 2024", "abstract": "Large Language Models (LLMs) have transformed NLP with their remarkable In- context Learning (ICL) capabilities. Automated assistants based on LLMs are gaining popularity; however, adapting them to novel tasks is still challenging. While colossal \u2026"}]
