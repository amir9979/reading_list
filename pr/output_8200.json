[{"title": "Counterfactual Contrastive Learning: Robust Representations via Causal Image Synthesis", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3Dux8sEQAAQBAJ%26oi%3Dfnd%26pg%3DPA22%26ots%3D17LTKVq6Cf%26sig%3DoiGjm3tLe8wPh7Yrs50o4z8EpD4", "details": "B Glocker - Data Engineering in Medical Imaging: Second MICCAI \u2026", "abstract": "Contrastive pretraining is well-known to improve downstream task performance and model generalisation, especially in limited label settings. However, it is sensitive to the choice of augmentation pipeline. Positive pairs should preserve semantic \u2026"}, {"title": "Masked Momentum Contrastive Learning for Semantic Understanding by Observation", "link": "https://ieeexplore.ieee.org/abstract/document/10647831/", "details": "J Wu, S Mo, S Atito, Z Feng, J Kittler, SS Husain\u2026 - 2024 IEEE International \u2026, 2024", "abstract": "Large language models (LLMs) have shown excellent performance in zero-shot learning using natural language prompts. However, in the domain of computer vision (CV), the paradigm of pretraining followed by finetuning remains dominant. The aim \u2026"}, {"title": "BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities", "link": "https://arxiv.org/pdf/2410.14672%3F", "details": "S Hao, X Liu, X Qi, S Zhao, B Zi, R Xiao, K Han\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce BiGR, a novel conditional image generation model using compact binary latent codes for generative training, focusing on enhancing both generation and representation capabilities. BiGR is the first conditional generative model that \u2026"}, {"title": "On the Generalization and Causal Explanation in Self-Supervised Learning", "link": "https://link.springer.com/article/10.1007/s11263-024-02263-9", "details": "W Qiang, Z Song, Z Gu, J Li, C Zheng, F Sun, H Xiong - International Journal of \u2026, 2024", "abstract": "Self-supervised learning (SSL) methods learn from unlabeled data and achieve high generalization performance on downstream tasks. However, they may also suffer from overfitting to their training data and lose the ability to adapt to new tasks. To \u2026"}, {"title": "Improving Self-Supervised Vision Transformers for Visual Control", "link": "https://ieeexplore.ieee.org/abstract/document/10647522/", "details": "W Song, K Sohn, D Min - 2024 IEEE International Conference on Image \u2026, 2024", "abstract": "Despite the tremendous success of vision transformer (ViT) architectures in a broad range of computer vision tasks, the potential of ViT for vision-based deep reinforcement learning (RL) has not been fully explored yet. To improve the \u2026"}, {"title": "MinorityPrompt: Text to Minority Image Generation via Prompt Optimization", "link": "https://arxiv.org/pdf/2410.07838", "details": "S Um, JC Ye - arXiv preprint arXiv:2410.07838, 2024", "abstract": "We investigate the generation of minority samples using pretrained text-to-image (T2I) latent diffusion models. Minority instances, in the context of T2I generation, can be defined as ones living on low-density regions of text-conditional data distributions \u2026"}, {"title": "Data Extrapolation for Text-to-image Generation on Small Datasets", "link": "https://arxiv.org/pdf/2410.01638%3F", "details": "S Ye, F Liu - arXiv preprint arXiv:2410.01638, 2024", "abstract": "Text-to-image generation requires large amount of training data to synthesizing high- quality images. For augmenting training data, previous methods rely on data interpolations like cropping, flipping, and mixing up, which fail to introduce new \u2026"}, {"title": "V2M: Visual 2-Dimensional Mamba for Image Representation Learning", "link": "https://arxiv.org/pdf/2410.10382", "details": "C Wang, W Zheng, Y Huang, J Zhou, J Lu - arXiv preprint arXiv:2410.10382, 2024", "abstract": "Mamba has garnered widespread attention due to its flexible design and efficient hardware performance to process 1D sequences based on the state space model (SSM). Recent studies have attempted to apply Mamba to the visual domain by \u2026"}, {"title": "Enhancing JEPAs with Spatial Conditioning: Robust and Efficient Representation Learning", "link": "https://arxiv.org/pdf/2410.10773%3F", "details": "E Littwin, V Thilak, A Gopalakrishnan - arXiv preprint arXiv:2410.10773, 2024", "abstract": "Image-based Joint-Embedding Predictive Architecture (IJEPA) offers an attractive alternative to Masked Autoencoder (MAE) for representation learning using the Masked Image Modeling framework. IJEPA drives representations to capture useful \u2026"}]
