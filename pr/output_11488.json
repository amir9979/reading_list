[{"title": "Staged Multi\u2010Strategy Framework With Open\u2010Source Large Language Models for Natural Language to SQL Generation", "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/tee.24268", "details": "C Liu, W Liao, Z Xu - IEEJ Transactions on Electrical and Electronic \u2026, 2025", "abstract": "In the field of natural language to SQL (NL2SQL), significant progress has been made with large pre\u2010trained language models. However, these models still have deficiencies in terms of their ability to generalize, particularly in open\u2010source Large \u2026"}, {"title": "Language Models as Continuous Self-Evolving Data Engineers", "link": "https://arxiv.org/pdf/2412.15151%3F", "details": "P Wang, M Wang, Z Ma, X Yang, S Feng, D Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert \u2026"}, {"title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules", "link": "https://arxiv.org/pdf/2412.13536", "details": "K Chen, L Wang, Q Zhang, R Xu - arXiv preprint arXiv:2412.13536, 2024", "abstract": "Recent studies have highlighted the limitations of large language models in mathematical reasoning, particularly their inability to capture the underlying logic. Inspired by meta-learning, we propose that models should acquire not only task \u2026"}, {"title": "FastVLM: Efficient Vision Encoding for Vision Language Models", "link": "https://arxiv.org/pdf/2412.13303", "details": "PKA Vasu, F Faghri, CL Li, C Koc, N True, A Antony\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high \u2026"}, {"title": "Are Vision-Language Models Truly Understanding Multi-vision Sensor?", "link": "https://arxiv.org/pdf/2412.20750", "details": "S Chung, Y Yu, Y Chee, SY Kim, BK Lee, YM Ro - arXiv preprint arXiv:2412.20750, 2024", "abstract": "Large-scale Vision-Language Models (VLMs) have advanced by aligning vision inputs with text, significantly improving performance in computer vision tasks. Moreover, for VLMs to be effectively utilized in real-world applications, an \u2026"}, {"title": "Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach", "link": "https://arxiv.org/pdf/2412.18108", "details": "J Bi, J Guo, Y Tang, LB Wen, Z Liu, C Xu - arXiv preprint arXiv:2412.18108, 2024", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated remarkable progress in visual understanding. This impressive leap raises a compelling question: how can language models, initially trained solely on \u2026"}, {"title": "Learning Semantic-Aware Representation in Visual-Language Models for Multi-Label Recognition with Partial Labels", "link": "https://dl.acm.org/doi/pdf/10.1145/3708991", "details": "H Ruan, Z Xu, Z Yang, Y Lu, J Qin, T Chen - ACM Transactions on Multimedia \u2026, 2024", "abstract": "Multi-label recognition with partial labels (MLR-PL), in which only some labels are known while others are unknown for each image, is a practical task in computer vision, since collecting large-scale and complete multi-label datasets is difficult in \u2026"}, {"title": "Harnessing Large Language and Vision-Language Models for Robust Out-of-Distribution Detection", "link": "https://arxiv.org/pdf/2501.05228", "details": "PK Lee, JC Chen, JL Wu - arXiv preprint arXiv:2501.05228, 2025", "abstract": "Out-of-distribution (OOD) detection has seen significant advancements with zero- shot approaches by leveraging the powerful Vision-Language Models (VLMs) such as CLIP. However, prior research works have predominantly focused on enhancing \u2026"}, {"title": "From Models to Microtheories: Distilling a Model's Topical Knowledge for Grounded Question Answering", "link": "https://arxiv.org/pdf/2412.17701", "details": "N Weir, BD Mishra, O Weller, O Tafjord, S Hornstein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent reasoning methods (eg, chain-of-thought, entailment reasoning) help users understand how language models (LMs) answer a single question, but they do little to reveal the LM's overall understanding, or\" theory,\" about the question's $\\textit \u2026"}]
