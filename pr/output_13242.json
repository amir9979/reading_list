[{"title": "Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning", "link": "https://arxiv.org/pdf/2502.15401", "details": "X Ma, W Jiang, H Huang - arXiv preprint arXiv:2502.15401, 2025", "abstract": "In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples. Previous methods typically relied on simple \u2026"}, {"title": "The Relationship Between Reasoning and Performance in Large Language Models--o3 (mini) Thinks Harder, Not Longer", "link": "https://arxiv.org/pdf/2502.15631", "details": "M Ballon, A Algaba, V Ginis - arXiv preprint arXiv:2502.15631, 2025", "abstract": "Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token \u2026"}, {"title": "Tool Learning in the Wild: Empowering Language Models as Automatic Tool Agents", "link": "https://openreview.net/pdf%3Fid%3DT4wMdeFEjX", "details": "Z Shi, S Gao, L Yan, Y Feng, X Chen, Z Chen, D Yin\u2026 - THE WEB CONFERENCE 2025", "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a promising approach to extend their utility, enabling them to solve practical tasks. Previous methods manually parse tool documentation and create in-context \u2026"}, {"title": "Optimizing Singular Spectrum for Large Language Model Compression", "link": "https://arxiv.org/pdf/2502.15092", "details": "D Li, T Shen, Y Zhou, B Yang, Z Liu, M Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, yet prohibitive parameter complexity often hinders their deployment. Existing singular value decomposition (SVD) based compression methods simply deem singular \u2026"}, {"title": "Personalization Toolkit: Training Free Personalization of Large Vision Language Models", "link": "https://arxiv.org/pdf/2502.02452%3F", "details": "S Seifi, V Dorovatas, DO Reino, R Aljundi - arXiv preprint arXiv:2502.02452, 2025", "abstract": "Large Vision Language Models (LVLMs) have significant potential to deliver personalized assistance by adapting to individual users' unique needs and preferences. Personalization of LVLMs is an emerging area that involves \u2026"}, {"title": "Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs", "link": "https://arxiv.org/pdf/2502.15224", "details": "T Chen, S Anumasa, B Lin, V Shah, A Goyal, D Liu - arXiv preprint arXiv:2502.15224, 2025", "abstract": "Given the remarkable performance of Large Language Models (LLMs), an important question arises: Can LLMs conduct human-like scientific research and discover new knowledge, and act as an AI scientist? Scientific discovery is an iterative process that \u2026"}, {"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models", "link": "https://arxiv.org/pdf/2502.03199%3F", "details": "J Wu, Y Shen, S Liu, Y Tang, S Song, X Wang, L Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the \u2026"}, {"title": "Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models", "link": "https://arxiv.org/pdf/2502.15639", "details": "A Sundar, S Williamson, K Metcalf, BJ Theobald\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally \u2026"}, {"title": "CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language Model Question Answering", "link": "https://arxiv.org/pdf/2501.18457%3F", "details": "Y Wang, Z Fan, Q Wang, M Fung, H Ji - arXiv preprint arXiv:2501.18457, 2025", "abstract": "Large Language Models (LLMs) are pretrained on extensive multilingual corpora to acquire both language-specific cultural knowledge and general knowledge. Ideally, while LLMs should provide consistent responses to culture-independent questions \u2026"}]
