[{"title": "Content-Style Learning from Unaligned Domains: Identifiability under Unknown Latent Dimensions", "link": "https://arxiv.org/pdf/2411.03755", "details": "S Shrestha, X Fu - arXiv preprint arXiv:2411.03755, 2024", "abstract": "Understanding identifiability of latent content and style variables from unaligned multi- domain data is essential for tasks such as domain translation and data generation. Existing works on content-style identification were often developed under somewhat \u2026"}, {"title": "Interpretability as Approximation: Understanding Black-Box Models by Decision Boundary", "link": "https://www.mdpi.com/2079-9292/13/22/4339", "details": "H Dong, B Liu, D Ye, G Liu - Electronics, 2024", "abstract": "Currently, interpretability methods focus more on less objective human- understandable semantics. To objectify and standardize interpretability research, in this study, we provide notions of interpretability based on approximation theory. We \u2026"}, {"title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time", "link": "https://openreview.net/pdf%3Fid%3DKkYZmepjHn", "details": "Z Chen, H Yuan, Y Li, Y Kou, J Zhang, Q Gu - The Thirty-eighth Annual Conference \u2026, 2024", "abstract": "Discrete diffusion models have emerged as powerful tools for high-quality data generation. Despite their success in discrete spaces, such as text generation tasks, the acceleration of discrete diffusion models remains under-explored. In this paper \u2026"}, {"title": "On Domain Generalization Datasets as Proxy Benchmarks for Causal Representation Learning", "link": "https://openreview.net/pdf%3Fid%3DLbFK9pUlA5", "details": "OE Salaudeen, N Chiou, S Koyejo - NeurIPS 2024 Causal Representation Learning \u2026", "abstract": "Benchmarking causal representation learning for real-world high-dimensional settings where most relevant causal variables are not directly observed remains a challenge. Notably, one promise of causal representations is their robustness to \u2026"}, {"title": "Evolved Hierarchical Masking for Self-Supervised Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10742293/", "details": "Z Feng, S Zhang - IEEE Transactions on Pattern Analysis and Machine \u2026, 2024", "abstract": "Existing Masked Image Modeling methods apply fixed mask patterns to guide the self- supervised training. As those mask patterns resort to different criteria to depict image contents, sticking to a fixed pattern leads to a limited vision cues modeling capability \u2026"}, {"title": "Classification Done Right for Vision-Language Pre-Training", "link": "https://arxiv.org/pdf/2411.03313%3F", "details": "H Zilong, Y Qinghao, K Bingyi, F Jiashi, F Haoqi - arXiv preprint arXiv:2411.03313, 2024", "abstract": "We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data. Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised \u2026"}]
