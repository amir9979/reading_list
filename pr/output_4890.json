[{"title": "Was it Slander? Towards Exact Inversion of Generative Language Models", "link": "https://arxiv.org/pdf/2407.11059", "details": "A Skapars, E Manino, Y Sun, LC Cordeiro - arXiv preprint arXiv:2407.11059, 2024", "abstract": "Training large language models (LLMs) requires a substantial investment of time and money. To get a good return on investment, the developers spend considerable effort ensuring that the model never produces harmful and offensive outputs. However \u2026"}, {"title": "Mitigating Entity-Level Hallucination in Large Language Models", "link": "https://arxiv.org/pdf/2407.09417%3Ftrk%3Dpublic_post_comment-text", "details": "W Su, Y Tang, Q Ai, C Wang, Z Wu, Y Liu - arXiv preprint arXiv:2407.09417, 2024", "abstract": "The emergence of Large Language Models (LLMs) has revolutionized how users access information, shifting from traditional search engines to direct question-and- answer interactions with LLMs. However, the widespread adoption of LLMs has \u2026"}, {"title": "Towards Trustworthy AI in Cardiology: A Comparative Analysis of Explainable AI Methods for Electrocardiogram Interpretation", "link": "https://link.springer.com/chapter/10.1007/978-3-031-66535-6_36", "details": "N Gumpfer, B Dinov, S Sossalla, M Guckert, J Hannig - International Conference on \u2026, 2024", "abstract": "Deep learning models have successfully been applied for medical diagnoses. However, such models are perceived as opaque black boxes and their decisions are not comprehensible for human users. This intransparency and the potential resulting \u2026"}, {"title": "BEExAI: Benchmark to Evaluate Explainable AI", "link": "https://arxiv.org/pdf/2407.19897", "details": "S Sithakoul, S Meftah, C Feutry - World Conference on Explainable Artificial \u2026, 2024", "abstract": "Recent research in explainability has given rise to numerous post-hoc attribution methods aimed at enhancing our comprehension of the outputs of black-box machine learning models. However, evaluating the quality of explanations lacks a \u2026"}, {"title": "Refusing Safe Prompts for Multi-modal Large Language Models", "link": "https://arxiv.org/pdf/2407.09050", "details": "Z Shao, H Liu, Y Hu, NZ Gong - arXiv preprint arXiv:2407.09050, 2024", "abstract": "Multimodal large language models (MLLMs) have become the cornerstone of today's generative AI ecosystem, sparking intense competition among tech giants and startups. In particular, an MLLM generates a text response given a prompt consisting \u2026"}, {"title": "Patch-Level Training for Large Language Models", "link": "https://arxiv.org/pdf/2407.12665", "details": "C Shao, F Meng, J Zhou - arXiv preprint arXiv:2407.12665, 2024", "abstract": "As Large Language Models (LLMs) achieve remarkable progress in language understanding and generation, their training efficiency has become a critical concern. Traditionally, LLMs are trained to predict the next token in a sequence \u2026"}]
