[{"title": "Clustering Properties of Self-Supervised Learning", "link": "https://arxiv.org/pdf/2501.18452", "details": "X Weng, J An, X Ma, B Qi, J Luo, X Yang, JS Dong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Self-supervised learning (SSL) methods via joint embedding architectures have proven remarkably effective at capturing semantically rich representations with strong clustering properties, magically in the absence of label supervision. Despite \u2026"}, {"title": "InfoMAE: Pairing-Efficient Cross-Modal Alignment with Informational Masked Autoencoders for IoT Signals", "link": "https://openreview.net/pdf%3Fid%3DF8RBFdKXWZ", "details": "T Kimura, X Li, O Hanna, Y Chen, Y Chen, D Kara\u2026 - THE WEB CONFERENCE 2025", "abstract": "Standard multimodal self-supervised learning (SSL) algorithms regard cross-modal synchronization as implicit supervisory labels during pretraining, thus posing high requirements on the scale and quality of multimodal samples. These constraints \u2026"}, {"title": "Learning Disentangled Representation for Multi-Modal Time-Series Sensing Signals", "link": "https://openreview.net/pdf%3Fid%3DtrPIg0ECvv", "details": "R Cai, Z Jiang, K Zheng, Z Li, W Chen, X Chen, Y Shen\u2026 - THE WEB CONFERENCE 2025", "abstract": "Multi-modal time series data is common in web technologies like the Internet of Things (IoT). Existing methods for multi-modal time series representation learning aim to disentangle the modality-shared and modality-specific latent variables \u2026"}, {"title": "Certified Guidance for Planning with Deep Generative Models", "link": "https://arxiv.org/pdf/2501.12815%3F", "details": "F Giacomarra, M Hosseini, N Paoletti, F Cairoli - arXiv preprint arXiv:2501.12815, 2025", "abstract": "Deep generative models, such as generative adversarial networks and diffusion models, have recently emerged as powerful tools for planning tasks and behavior synthesis in autonomous systems. Various guidance strategies have been \u2026"}, {"title": "Pre-Trained Vision-Language Model Selection and Reuse for Downstream Tasks", "link": "https://arxiv.org/pdf/2501.18271", "details": "HZ Tan, Z Zhou, LZ Guo, YF Li - arXiv preprint arXiv:2501.18271, 2025", "abstract": "Pre-trained Vision-Language Models (VLMs) are becoming increasingly popular across various visual tasks, and several open-sourced VLM variants have been released. However, selecting the best-performing pre-trained VLM for a specific \u2026"}]
