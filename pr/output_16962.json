[{"title": "Scaling Up Biomedical Vision-Language Models: Fine-Tuning, Instruction Tuning, and Multi-Modal Learning", "link": "https://arxiv.org/pdf/2505.17436", "details": "C Peng, K Zhang, M Lyu, H Liu, L Sun, Y Wu - arXiv preprint arXiv:2505.17436, 2025", "abstract": "To advance biomedical vison-language model capabilities through scaling up, fine- tuning, and instruction tuning, develop vision-language models with improved performance in handling long text, explore strategies to efficiently adopt vision \u2026", "entry_id": "http://arxiv.org/abs/2505.17436v1", "updated": "2025-05-23 03:31:58", "published": "2025-05-23 03:31:58", "authors": "Cheng Peng;Kai Zhang;Mengxian Lyu;Hongfang Liu;Lichao Sun;Yonghui Wu", "summary": "To advance biomedical vison-language model capabilities through scaling up,\nfine-tuning, and instruction tuning, develop vision-language models with\nimproved performance in handling long text, explore strategies to efficiently\nadopt vision language models for diverse multi-modal biomedical tasks, and\nexamine the zero-shot learning performance.\n  We developed two biomedical vision language models, BiomedGPT-Large and\nBiomedGPT-XLarge, based on an encoder-decoder-based transformer architecture.\nWe fine-tuned the two models on 23 benchmark datasets from 6 multi-modal\nbiomedical tasks including one image-only task (image classification), three\nlanguage-only tasks (text understanding, text summarization and question\nanswering), and two vision-language tasks (visual question answering and image\ncaptioning). We compared the developed scaled models with our previous\nBiomedGPT-Base model and existing prestigious models reported in the\nliterature. We instruction-tuned the two models using a large-scale multi-modal\nbiomedical instruction-tuning dataset and assessed the zero-shot learning\nperformance and alignment accuracy.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.17436v1;http://arxiv.org/pdf/2505.17436v1", "pdf_url": "http://arxiv.org/pdf/2505.17436v1"}, {"title": "Multi-view contrastive learning and symptom extraction insights for medical report generation", "link": "https://www.nature.com/articles/s41598-025-00570-w", "details": "Q Bai, X Zou, A Alhaskawi, Y Dong, H Zhou, SHA Ezzi\u2026 - Scientific Reports, 2025", "abstract": "The task of generating medical reports automatically is of paramount importance in modern healthcare, offering a substantial reduction in the workload of radiologists and accelerating the processes of clinical diagnosis and treatment. Current \u2026"}, {"title": "Dual-domain explainability-driven data augmentation for enhanced COVID-19 detection in chest X-rays", "link": "https://link.springer.com/article/10.1007/s11042-025-20920-0", "details": "H El Mohamadi, M El Hassouni, R Jennane - Multimedia Tools and Applications, 2025", "abstract": "The classification and detection of COVID-19 from chest X-ray (CXR) images using deep learning, particularly transfer learning models, have shown promising results. However, the limited amount of available data often leads to performance \u2026"}, {"title": "Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays", "link": "https://arxiv.org/pdf/2505.17311", "details": "H Kim, Y Wang, M Ahn, H Choi, Y Zhou, C Hong - arXiv preprint arXiv:2505.17311, 2025", "abstract": "Unsupervised anomaly detection (UAD) in medical imaging is crucial for identifying pathological abnormalities without requiring extensive labeled data. However, existing diffusion-based UAD models rely solely on imaging features, limiting their \u2026", "entry_id": "http://arxiv.org/abs/2505.17311v1", "updated": "2025-05-22 22:02:47", "published": "2025-05-22 22:02:47", "authors": "Harim Kim;Yuhan Wang;Minkyu Ahn;Heeyoul Choi;Yuyin Zhou;Charmgil Hong", "summary": "Unsupervised anomaly detection (UAD) in medical imaging is crucial for\nidentifying pathological abnormalities without requiring extensive labeled\ndata. However, existing diffusion-based UAD models rely solely on imaging\nfeatures, limiting their ability to distinguish between normal anatomical\nvariations and pathological anomalies. To address this, we propose Diff3M, a\nmulti-modal diffusion-based framework that integrates chest X-rays and\nstructured Electronic Health Records (EHRs) for enhanced anomaly detection.\nSpecifically, we introduce a novel image-EHR cross-attention module to\nincorporate structured clinical context into the image generation process,\nimproving the model's ability to differentiate normal from abnormal features.\nAdditionally, we develop a static masking strategy to enhance the\nreconstruction of normal-like images from anomalies. Extensive evaluations on\nCheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art\nperformance, outperforming existing UAD methods in medical imaging. Our code is\navailable at this http URL https://github.com/nth221/Diff3M", "comment": "MICCAI 2025 early accept", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.LG", "links": "http://arxiv.org/abs/2505.17311v1;http://arxiv.org/pdf/2505.17311v1", "pdf_url": "http://arxiv.org/pdf/2505.17311v1"}, {"title": "Self-supervised learning for MRI reconstruction through mapping resampled k-space data to resampled k-space data", "link": "https://www.sciencedirect.com/science/article/pii/S0730725X25000888", "details": "J Huang, X Li, G Zhou, W Hu - Magnetic Resonance Imaging, 2025", "abstract": "In recent years, significant advancements have been achieved in applying deep learning (DL) to magnetic resonance imaging (MRI) reconstruction, which traditionally relies on fully sampled data. However, real-world clinical scenarios often \u2026"}]
