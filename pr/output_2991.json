[{"title": "Convergence Behavior of an Adversarial Weak Supervision Method", "link": "https://arxiv.org/pdf/2405.16013", "details": "S An, S Dasgupta - arXiv preprint arXiv:2405.16013, 2024", "abstract": "Labeling data via rules-of-thumb and minimal label supervision is central to Weak Supervision, a paradigm subsuming subareas of machine learning such as crowdsourced learning and semi-supervised ensemble learning. By using this \u2026"}, {"title": "EHR-SeqSQL: A Sequential Text-to-SQL Dataset For Interactively Exploring Electronic Health Records", "link": "https://arxiv.org/pdf/2406.00019", "details": "J Ryu, S Cho, G Lee, E Choi - arXiv preprint arXiv:2406.00019, 2024", "abstract": "In this paper, we introduce EHR-SeqSQL, a novel sequential text-to-SQL dataset for Electronic Health Record (EHR) databases. EHR-SeqSQL is designed to address critical yet underexplored aspects in text-to-SQL parsing: interactivity \u2026"}, {"title": "Watermarks in the Sand: Impossibility of Strong Watermarking for Language Models", "link": "https://openreview.net/pdf%3Fid%3DbM2s12t4hR", "details": "H Zhang, BL Edelman, D Francati, D Venturi\u2026 - Forty-first International Conference \u2026", "abstract": "Watermarking generative models consists of planting a statistical signal (watermark) in a model's output so that it can be later verified that the output was generated by the given model. A strong watermarking scheme satisfies the property that a \u2026"}, {"title": "DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling Language Models", "link": "https://openreview.net/pdf%3Fid%3Dpvg1OdUtDQ", "details": "S Lu, W Zhao, C Tao, A Gupta, S Wu, T Chung, N Peng - Forty-first International Conference \u2026", "abstract": "NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable generation with large language models. It is designed to avoid catastrophic forgetting while achieving guaranteed convergence to an entropy-maximized closed-form \u2026"}, {"title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions", "link": "https://arxiv.org/pdf/2405.19744", "details": "C Li, W Yang, J Zhang, J Lu, S Wang, C Zong - arXiv preprint arXiv:2405.19744, 2024", "abstract": "Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples \u2026"}, {"title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models", "link": "https://openreview.net/pdf%3Fid%3D1tRLxQzdep", "details": "P Dong, L Li, Z Tang, X Liu, X Pan, Q Wang, X Chu - Forty-first International \u2026, 2024", "abstract": "Despite the remarkable capabilities, Large Language Models (LLMs) face deployment challenges due to their extensive size. Pruning methods drop a subset of weights to accelerate, but many of them require retraining, which is prohibitively \u2026"}, {"title": "Amend to Alignment: Decoupled Prompt Tuning for Mitigating Spurious Correlation in Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3Df8G2KSCSdp", "details": "J Zhang, X Ma, S Guo, P Li, W Xu, X Tang, Z Hong - Forty-first International Conference on \u2026", "abstract": "Fine-tuning the learnable prompt for a pre-trained vision-language model (VLM), such as CLIP, has demonstrated exceptional efficiency in adapting to a broad range of downstream tasks. Existing prompt tuning methods for VLMs do not distinguish \u2026"}, {"title": "An Empirical Study of Mamba-based Language Models", "link": "https://arxiv.org/pdf/2406.07887", "details": "R Waleffe, WBDRB Norick, VKT Dao, A Gu, AHS Singh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Selective state-space models (SSMs) like Mamba (Gu and Dao 2023) overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements \u2026"}, {"title": "Calibrating Reasoning in Language Models with Internal Consistency", "link": "https://arxiv.org/pdf/2405.18711", "details": "Z Xie, J Guo, T Yu, S Li - arXiv preprint arXiv:2405.18711, 2024", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought (CoT) prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious \u2026"}]
