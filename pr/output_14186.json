[{"title": "BiasICL: In-Context Learning and Demographic Biases of Vision Language Models", "link": "https://arxiv.org/pdf/2503.02334", "details": "S Xu, J Janizek, Y Jiang, R Daneshjou - arXiv preprint arXiv:2503.02334, 2025", "abstract": "Vision language models (VLMs) show promise in medical diagnosis, but their performance across demographic subgroups when using in-context learning (ICL) remains poorly understood. We examine how the demographic composition of \u2026"}, {"title": "X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography from Chest Radiography via Tri-Modal Contrastive Learning", "link": "https://arxiv.org/pdf/2503.02162", "details": "J You, Y Gao, S Kim, C Mcintosh - arXiv preprint arXiv:2503.02162, 2025", "abstract": "Computed tomography (CT) is a key imaging modality for diagnosis, yet its clinical utility is marred by high radiation exposure and long turnaround times, restricting its use for larger-scale screening. Although chest radiography (CXR) is more accessible \u2026"}, {"title": "Cross-Modal Alignment Regularization: Enhancing Language Models with Vision Model Representations", "link": "https://openreview.net/pdf%3Fid%3D4Yag8mHVtc", "details": "Y Gan, KI Zhao, P Isola - Second Workshop on Representational Alignment at \u2026", "abstract": "Cross-modal distillation has emerged as a critical technique for leveraging the complementary strengths of different modalities. However, existing work has not enabled direct benefits between models trained on data from different modalities. In \u2026"}, {"title": "Large-scale benchmarking and boosting transfer learning for medical image analysis", "link": "https://www.sciencedirect.com/science/article/pii/S1361841525000350", "details": "MRH Taher, F Haghighi, MB Gotway, J Liang - Medical Image Analysis, 2025", "abstract": "Transfer learning, particularly fine-tuning models pretrained on photographic images to medical images, has proven indispensable for medical image analysis. There are numerous models with distinct architectures pretrained on various datasets using \u2026"}, {"title": "Large Language Models Are Highly Vulnerable to Adversarial Hallucination Attacks in Clinical Decision Support: A Multi-Model Assurance Analysis", "link": "https://www.medrxiv.org/content/10.1101/2025.03.18.25324184.full.pdf", "details": "M Omar, V Sorin Sr, J Collins, D Reich, R Freeman\u2026 - medRxiv, 2025", "abstract": "Background: Large language models (LLMs) show promise in clinical contexts but can generate false facts (often referred to as hallucinations). One subset of these errors arises from adversarial attacks, in which fabricated details embedded in \u2026"}, {"title": "Unsupervised Parameter Efficient Source-free Post-pretraining", "link": "https://arxiv.org/pdf/2502.21313%3F", "details": "A Jha, T Tuytelaars, YM Asano - arXiv preprint arXiv:2502.21313, 2025", "abstract": "Following the success in NLP, the best vision models are now in the billion parameter ranges. Adapting these large models to a target distribution has become computationally and economically prohibitive. Addressing this challenge, we \u2026"}, {"title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems", "link": "https://arxiv.org/pdf/2503.03686", "details": "R Ye, S Tang, R Ge, Y Du, Z Yin, S Chen, J Shao - arXiv preprint arXiv:2503.03686, 2025", "abstract": "LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability \u2026"}, {"title": "Enhancing Zero-shot Learning in Chest X-ray Diagnostics", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DRkxLEQAAQBAJ%26oi%3Dfnd%26pg%3DPA270%26ots%3D6yZXyqGGL3%26sig%3D0AHC2B49lR9ujpNhvzlxZ9Pdc40", "details": "P Bhardwaj, S Bhat, A Maier - Bildverarbeitung f\u00fcr die Medizin 2025: Proceedings \u2026, 2025", "abstract": "Accurate diagnosis of pulmonary diseases from chest X-rays remains a challenging task, especially due to the scarcity of labeled data. In this work, we propose an enhanced zero-shot learning framework that integrates BioBERT\u2014a pre-trained \u2026"}]
