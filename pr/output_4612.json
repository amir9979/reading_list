[{"title": "Evaluating LLMs for Text-to-SQL Generation With Complex SQL Workload", "link": "https://arxiv.org/pdf/2407.19517", "details": "L Ma, K Pu, Y Zhu - arXiv preprint arXiv:2407.19517, 2024", "abstract": "This study presents a comparative analysis of the a complex SQL benchmark, TPC- DS, with two existing text-to-SQL benchmarks, BIRD and Spider. Our findings reveal that TPC-DS queries exhibit a significantly higher level of structural complexity \u2026"}, {"title": "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models", "link": "https://arxiv.org/pdf/2407.05131", "details": "P Xia, K Zhu, H Li, H Zhu, Y Li, G Li, L Zhang, H Yao - arXiv preprint arXiv:2407.05131, 2024", "abstract": "The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical \u2026"}, {"title": "FlexAttention for Efficient High-Resolution Vision-Language Models", "link": "https://arxiv.org/pdf/2407.20228", "details": "J Li, D Chen, T Cai, P Chen, Y Hong, Z Chen, Y Shen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Current high-resolution vision-language models encode images as high-resolution image tokens and exhaustively take all these tokens to compute attention, which significantly increases the computational cost. To address this problem, we propose \u2026"}, {"title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models", "link": "https://arxiv.org/pdf/2407.03181", "details": "H Puerto, T Chubakov, X Zhu, HT Madabushi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model \u2026"}, {"title": "Quantized Prompt for Efficient Generalization of Vision-Language Models", "link": "https://arxiv.org/pdf/2407.10704", "details": "T Hao, X Ding, J Feng, Y Yang, H Chen, G Ding - arXiv preprint arXiv:2407.10704, 2024", "abstract": "In the past few years, large-scale pre-trained vision-language models like CLIP have achieved tremendous success in various fields. Naturally, how to transfer the rich knowledge in such huge pre-trained models to downstream tasks and datasets \u2026"}, {"title": "UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization", "link": "https://arxiv.org/pdf/2407.03525", "details": "MN Uddin, A Saeidi, D Handa, A Seth, TC Son\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces UnSeenTimeQA, a novel time-sensitive question-answering (TSQA) benchmark that diverges from traditional TSQA benchmarks by avoiding factual and web-searchable queries. We present a series of time-sensitive event \u2026"}, {"title": "MixPrompt: Enhancing Generalizability and Adversarial Robustness for Vision-Language Models via Prompt Fusion", "link": "https://link.springer.com/chapter/10.1007/978-981-97-5606-3_28", "details": "H Fan, Z Ma, Y Li, R Tian, Y Chen, C Gao - International Conference on Intelligent \u2026, 2024", "abstract": "Abstract Pretrained Vision-Language Models (VLMs) like CLIP have exhibited remarkable capacities across downstream tasks, while their image encoders are vulnerable to adversarial examples. A recently introduced lightweight approach \u2026"}, {"title": "On Joint Noise Scaling in Differentially Private Federated Learning with Multiple Local Steps", "link": "https://arxiv.org/pdf/2407.19286", "details": "MA Heikkil\u00e4 - arXiv preprint arXiv:2407.19286, 2024", "abstract": "Federated learning is a distributed learning setting where the main aim is to train machine learning models without having to share raw data but only what is required for learning. To guarantee training data privacy and high-utility models, differential \u2026"}, {"title": "SCFL: Spatio-temporal consistency federated learning for next POI recommendation", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324002115", "details": "L Zhong, J Zeng, Z Wang, W Zhou, J Wen - Information Processing & Management, 2024", "abstract": "Existing personalized federated learning frameworks fail to significantly improve the personalization of user preference learning in next Point-Of-Interest (POI) recommendations, causing notable performance deficits. These frameworks do not \u2026"}]
