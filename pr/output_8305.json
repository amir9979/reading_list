[{"title": "Applying sparse autoencoders to unlearn knowledge in language models", "link": "https://arxiv.org/pdf/2410.19278", "details": "E Farrell, YT Lau, A Conmy - arXiv preprint arXiv:2410.19278, 2024", "abstract": "We investigate whether sparse autoencoders (SAEs) can be used to remove knowledge from language models. We use the biology subset of the Weapons of Mass Destruction Proxy dataset and test on the gemma-2b-it and gemma-2-2b-it \u2026"}, {"title": "Model Attribution in LLM-Generated Disinformation: A Domain Generalization Approach with Supervised Contrastive Learning", "link": "https://www.researchgate.net/profile/Canyu-Chen-3/publication/382738891_Model_Attribution_in_Machine-Generated_Disinformation_A_Domain_Generalization_Approach_with_Supervised_Contrastive_Learning/links/66d89f3c2390e50b2c53df7c/Model-Attribution-in-Machine-Generated-Disinformation-A-Domain-Generalization-Approach-with-Supervised-Contrastive-Learning.pdf", "details": "A Beigi, Z Tan, N Mudiam, C Chen, K Shu, H Liu - 2024 IEEE 11th International \u2026, 2024", "abstract": "Model attribution for LLM-generated disinformation poses a significant challenge in understanding its origins and mitigating its spread. This task is especially challenging because modern large language models (LLMs) produce disinformation \u2026"}, {"title": "Disentangled and Self-Explainable Node Representation Learning", "link": "https://arxiv.org/pdf/2410.21043", "details": "S Piaggesi, A Panisson, M Khosla - arXiv preprint arXiv:2410.21043, 2024", "abstract": "Node representations, or embeddings, are low-dimensional vectors that capture node properties, typically learned through unsupervised structural similarity objectives or supervised tasks. While recent efforts have focused on explaining \u2026"}, {"title": "Considerations for Distribution Shift Robustness of Diagnostic Models in Healthcare", "link": "https://arxiv.org/pdf/2410.19575", "details": "A Blaas, A Goli\u0144ski, A Miller, L Zappella, JH Jacobsen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We consider robustness to distribution shifts in the context of diagnostic models in healthcare, where the prediction target $ Y $, eg, the presence of a disease, is causally upstream of the observations $ X $, eg, a biomarker. Distribution shifts may \u2026"}, {"title": "A Survey on Group Fairness in Federated Learning: Challenges, Taxonomy of Solutions and Directions for Future Research", "link": "https://arxiv.org/pdf/2410.03855", "details": "T Salazar, H Ara\u00fajo, A Cano, PH Abreu - arXiv preprint arXiv:2410.03855, 2024", "abstract": "Group fairness in machine learning is a critical area of research focused on achieving equitable outcomes across different groups defined by sensitive attributes such as race or gender. Federated learning, a decentralized approach to training \u2026"}, {"title": "$\\beta $-calibration of Language Model Confidence Scores for Generative QA", "link": "https://arxiv.org/pdf/2410.06615", "details": "P Manggala, A Mastakouri, E Kirschbaum\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim \u2026"}, {"title": "Poisonedrag: Knowledge corruption attacks to retrieval-augmented generation of large language models", "link": "https://openreview.net/pdf%3Fid%3DAJGfRZwINR", "details": "W Zou, R Geng, B Wang, J Jia - arXiv preprint arXiv:2402.07867, 2024", "abstract": "Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval \u2026"}, {"title": "Interpretable Image Classification with Adaptive Prototype-based Vision Transformers", "link": "https://arxiv.org/pdf/2410.20722", "details": "C Ma, J Donnelly, W Liu, S Vosoughi, C Rudin, C Chen - arXiv preprint arXiv \u2026, 2024", "abstract": "We present ProtoViT, a method for interpretable image classification combining deep learning and case-based reasoning. This method classifies an image by comparing it to a set of learned prototypes, providing explanations of the form``this looks like \u2026"}, {"title": "Chest X-ray synthetic data for better testing and evaluation of ML models", "link": "https://cs231n.stanford.edu/2024/papers/chest-x-ray-synthetic-data-for-better-testing-and-evaluation-of-.pdf", "details": "E Bismuth, A Geslin, M Paschali", "abstract": "Abstract The use of Machine Learning (ML) models in radiology has significantly advanced medical imaging diagnostics. However, model performance may not accurately reflect certain underrepresented conditions if such conditions or minorities \u2026"}]
