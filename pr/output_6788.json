[{"title": "MulCPred: Learning Multi-modal Concepts for Explainable Pedestrian Action Prediction", "link": "https://arxiv.org/pdf/2409.09446", "details": "Y Feng, A Carballo, K Fujii, R Karlsson, M Ding\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pedestrian action prediction is of great significance for many applications such as autonomous driving. However, state-of-the-art methods lack explainability to make trustworthy predictions. In this paper, a novel framework called MulCPred is \u2026"}, {"title": "RoMo: Robust Unsupervised Multimodal Learning with Noisy Pseudo Labels", "link": "https://ieeexplore.ieee.org/abstract/document/10653726/", "details": "Y Li, Y Qin, Y Sun, D Peng, X Peng, P Hu - IEEE Transactions on Image Processing, 2024", "abstract": "The rise of the metaverse and the increasing volume of heterogeneous 2D and 3D data have led to a growing demand for cross-modal retrieval, which allows users to query semantically relevant data across different modalities. Existing methods \u2026"}]
