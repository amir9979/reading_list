[{"title": "Optimizing generative AI by backpropagating language model feedback", "link": "https://www.nature.com/articles/s41586-025-08661-4", "details": "M Yuksekgonul, F Bianchi, J Boen, S Liu, P Lu\u2026 - Nature, 2025", "abstract": "Recent breakthroughs in artificial intelligence (AI) are increasingly driven by systems orchestrating multiple large language models (LLMs) and other specialized tools, such as search engines and simulators. So far, these systems are primarily \u2026"}, {"title": "Pathologyvlm: a large vision-language model for pathology image understanding", "link": "https://link.springer.com/article/10.1007/s10462-025-11190-1", "details": "D Dai, Y Zhang, Q Yang, L Xu, X Shen, S Xia, G Wang - Artificial Intelligence Review, 2025", "abstract": "The previous advancements in pathology image understanding primarily involved developing models tailored to specific tasks. Recent studies have demonstrated that the large vision-language model can enhance the performance of various \u2026"}, {"title": "Towards Training-free Anomaly Detection with Vision and Language Foundation Models", "link": "https://arxiv.org/pdf/2503.18325%3F", "details": "J Zhang, G Wang, Y Jin, D Huang - arXiv preprint arXiv:2503.18325, 2025", "abstract": "Anomaly detection is valuable for real-world applications, such as industrial quality inspection. However, most approaches focus on detecting local structural anomalies while neglecting compositional anomalies incorporating logical constraints. In this \u2026"}, {"title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models", "link": "https://arxiv.org/pdf/2503.18931", "details": "Y Chen, L Meng, W Peng, Z Wu, YG Jiang - arXiv preprint arXiv:2503.18931, 2025", "abstract": "Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of \u2026"}, {"title": "Leveraging Robust Optimization for LLM Alignment under Distribution Shifts", "link": "https://arxiv.org/pdf/2504.05831", "details": "M Zhu, Y Liu, J Guo, Q Wang, Y Zhang, Z Mao - arXiv preprint arXiv:2504.05831, 2025", "abstract": "Large language models (LLMs) increasingly rely on preference alignment methods to steer outputs toward human values, yet these methods are often constrained by the scarcity of high-quality human-annotated data. To tackle this, recent approaches \u2026"}, {"title": "Molecular property prediction based on graph contrastive learning with partial feature masking", "link": "https://www.sciencedirect.com/science/article/pii/S1093326325000749", "details": "K Dong, X Lin, Y Zhang - Journal of Molecular Graphics and Modelling, 2025", "abstract": "Molecular representation learning facilitates multiple downstream tasks such as molecular property prediction (MPP) and drug design. Recent studies have shown great promise in applying self-supervised learning (SSL) to cope with the data \u2026"}, {"title": "Fast-Slow-Thinking: Complex Task Solving with Large Language Models", "link": "https://arxiv.org/pdf/2504.08690", "details": "Y Sun, Y Zhang, Z Zhao, S Wan, D Tao, C Gong - arXiv preprint arXiv:2504.08690, 2025", "abstract": "Nowadays, Large Language Models (LLMs) have been gradually employed to solve complex tasks. To face the challenge, task decomposition has become an effective way, which proposes to divide a complex task into multiple simpler subtasks and \u2026"}, {"title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning", "link": "https://arxiv.org/pdf/2504.00406", "details": "J Han, W Buntine, E Shareghi - arXiv preprint arXiv:2504.00406, 2025", "abstract": "Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources \u2026"}, {"title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking", "link": "https://arxiv.org/pdf/2503.19855%3F", "details": "X Tian, S Zhao, H Wang, S Chen, Y Ji, Y Peng, H Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite \u2026"}]
