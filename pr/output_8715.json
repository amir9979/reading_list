[{"title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond", "link": "https://arxiv.org/pdf/2411.03590", "details": "H Nori, N Usuyama, N King, SM McKinney\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Run-time steering strategies like Medprompt are valuable for guiding large language models (LLMs) to top performance on challenging tasks. Medprompt demonstrates that a general LLM can be focused to deliver state-of-the-art performance on \u2026"}, {"title": "# ECR-Poster-07 aIRDetect: automatic detection and quantification of inherited retinal disease features in fundus autofluorescence using AI", "link": "https://bmjophth.bmj.com/content/9/Suppl_5/A3.3", "details": "W Woof, TAC de Guimar\u00e3es, S Al-Khuzaei, MD Varela\u2026 - 2024", "abstract": "We developed an algorithm, aIRDetect, to detect, segment and quantify relevant features in fundus autofluorescence (FAF) image features in inherited retinal diseases (IRDs), to enable gene-phenotype association studies and monitoring of \u2026"}, {"title": "White Matter Function and Network Abnormalities in Patients with Diabetic Retinopathy", "link": "https://www.tandfonline.com/doi/pdf/10.2147/DMSO.S492099", "details": "YL Zhong, RY Hu, YZ He, XT Li, ZC Li, X Huang - Diabetes, Metabolic Syndrome and \u2026, 2024", "abstract": "Background This study aims to explore changes in white matter function and network connectivity in individuals with DR. Methods This study included 46 patients with DR and 43 age-and gender-matched healthy control (HC) participants were enrolled in \u2026"}, {"title": "Language-Emphasized Cross-Lingual In-Context Learning for Multilingual LLM", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9437-9_26", "details": "J Li, X Wei, X Wang, N Zhuang, L Wang, J Dang - CCF International Conference on \u2026, 2024", "abstract": "With the recent rise of large language models (LLMs), in-context learning (ICL) has shown remarkable performance, eliminating the need for fine-tuning parameters and reducing the reliance on extensive labeled data. However, the intricacies of cross \u2026"}, {"title": "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "link": "https://arxiv.org/pdf/2411.04118", "details": "DP Jeong, S Garg, ZC Lipton, M Oberst - arXiv preprint arXiv:2411.04118, 2024", "abstract": "Several recent works seek to develop foundation models specifically for medical applications, adapting general-purpose large language models (LLMs) and vision- language models (VLMs) via continued pretraining on publicly available biomedical \u2026"}, {"title": "RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models", "link": "https://arxiv.org/pdf/2411.04097", "details": "M Varma, JB Delbrouck, Z Chen, A Chaudhari\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) \u2026"}]
