[{"title": "White-box Multimodal Jailbreaks Against Large Vision-Language Models", "link": "https://arxiv.org/pdf/2405.17894", "details": "R Wang, X Ma, H Zhou, C Ji, G Ye, YG Jiang - arXiv preprint arXiv:2405.17894, 2024", "abstract": "Recent advancements in Large Vision-Language Models (VLMs) have underscored their superiority in various multimodal tasks. However, the adversarial robustness of VLMs has not been fully explored. Existing methods mainly assess robustness \u2026"}, {"title": "Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration", "link": "https://openreview.net/pdf%3Fid%3DDLTjFFiuUJ", "details": "Z Yu, Z Wang, Y Fu, H Shi, K Shaikh, YC Lin - Forty-first International Conference on Machine \u2026", "abstract": "Attention is a fundamental component behind the remarkable achievements of large language models (LLMs). However, our current understanding of the attention mechanism, especially regarding how attention distributions are established \u2026"}, {"title": "Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots", "link": "https://arxiv.org/pdf/2405.07990", "details": "C Wu, Y Ge, Q Guo, J Wang, Z Liang, Z Lu, Y Shan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has attracted significant attention due to their superior performance in visual contexts. However, their capabilities in turning visual figure to executable code, have not been \u2026"}]
