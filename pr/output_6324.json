[{"title": "Designing Retrieval-Augmented Language Models for Clinical Decision Support", "link": "https://link.springer.com/chapter/10.1007/978-3-031-63592-2_13", "details": "K Quigley, T Koker, J Taylor, V Mancuso, L Brattain - AI for Health Equity and Fairness \u2026, 2024", "abstract": "Ever-increasing demands for physician expertise drive the need for trustworthy point- of-care tools that can help aid decision-making in all clinical settings. Retrieval- augmented language models carry potential to relieve the information burden on \u2026"}, {"title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism", "link": "https://arxiv.org/pdf/2408.10473", "details": "G Li, X Zhao, L Liu, Z Li, D Li, L Tian, J He, A Sirasao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational \u2026"}, {"title": "Cross-lingual Natural Language Processing on Limited Annotated Case/Radio-logy Reports in English and Japanese: Insights from the Real-MedNLP Work-shop", "link": "https://www.thieme-connect.com/products/ejournals/pdf/10.1055/a-2405-2489.pdf", "details": "S Yada, Y Nakamura, S Wakamiya, E Aramaki - Methods of information in medicine", "abstract": "Background: Textual datasets (corpora) are crucial for the application of natural language processing (NLP) models. However, corpus creation in the medical field is challenging, primarily because of privacy issues with raw clinical data such as health \u2026"}, {"title": "Self-Supervised Position Debiasing for Large Language Models", "link": "https://aclanthology.org/2024.findings-acl.170.pdf", "details": "Z Liu, Z Chen, M Zhang, Z Ren, P Ren, Z Chen - Findings of the Association for \u2026, 2024", "abstract": "Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Previous \u2026"}, {"title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios", "link": "https://aclanthology.org/2024.findings-acl.230.pdf", "details": "L Lin, J Fu, P Liu, Q Li, Y Gong, J Wan, F Zhang\u2026 - Findings of the Association \u2026, 2024", "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local \u2026"}, {"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "link": "https://arxiv.org/pdf/2408.12337", "details": "KS Phogat, SA Puranam, S Dasaratha, C Harsha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain \u2026"}, {"title": "Unraveling the Inner Workings of Massive Language Models: Architecture, Training, and Linguistic Capacities", "link": "https://www.igi-global.com/chapter/unraveling-the-inner-workings-of-massive-language-models/354398", "details": "CVS Babu, CSA Anniyappa - Challenges in Large Language Model Development \u2026, 2024", "abstract": "This study explores the evolution of language models, emphasizing the shift from traditional statistical methods to advanced neural networks, particularly the transformer architecture. It aims to understand the impact of these advancements on \u2026"}, {"title": "Enhancing the vision-language foundation model with key semantic knowledge-emphasized report refinement", "link": "https://www.sciencedirect.com/science/article/pii/S136184152400224X", "details": "W Huang, C Li, H Yang, J Liu, Y Liang, H Zheng\u2026 - Medical Image Analysis, 2024", "abstract": "Recently, vision-language representation learning has made remarkable advancements in building up medical foundation models, holding immense potential for transforming the landscape of clinical research and medical care. The underlying \u2026"}, {"title": "Multi-modal Concept Alignment Pre-training for Generative Medical Visual Question Answering", "link": "https://aclanthology.org/2024.findings-acl.319.pdf", "details": "Q Yan, J Duan, J Wang - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Medical Visual Question Answering (Med-VQA) seeks to accurately respond to queries regarding medical images, a task particularly challenging for open-ended questions. This study unveils the Multi-modal Concept Alignment Pre-training \u2026"}]
