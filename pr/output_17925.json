[{"title": "CXPMRG-Bench: Pre-training and Benchmarking for X-ray Medical Report Generation on CheXpert Plus Dataset\u2013Supplementary Material\u2013", "link": "https://openaccess.thecvf.com/content/CVPR2025/supplemental/Wang_CXPMRG-Bench_Pre-training_and_CVPR_2025_supplemental.pdf", "details": "X Wang, F Wang, Y Li, Q Ma, S Wang, B Jiang, J Tang", "abstract": "Since its introduction in 2017, Transformer [56] has quickly become the preferred model framework for researchers due to its strong performance. However, as the model scales and sequences become longer, its limitations have surfaced. One \u2026"}, {"title": "General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound", "link": "https://arxiv.org/pdf/2506.19552", "details": "J Ambsdorf, A Munk, S Llambias, AN Christensen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With access to large-scale, unlabeled medical datasets, researchers are confronted with two questions: Should they attempt to pretrain a custom foundation model on this medical data, or use transfer-learning from an existing generalist model? And, if \u2026", "entry_id": "http://arxiv.org/abs/2506.19552v1", "updated": "2025-06-24 12:00:13", "published": "2025-06-24 12:00:13", "authors": "Jakob Ambsdorf;Asbj\u00f8rn Munk;Sebastian Llambias;Anders Nymark Christensen;Kamil Mikolaj;Randall Balestriero;Martin Tolsgaard;Aasa Feragen;Mads Nielsen", "summary": "With access to large-scale, unlabeled medical datasets, researchers are\nconfronted with two questions: Should they attempt to pretrain a custom\nfoundation model on this medical data, or use transfer-learning from an\nexisting generalist model? And, if a custom model is pretrained, are novel\nmethods required? In this paper we explore these questions by conducting a\ncase-study, in which we train a foundation model on a large regional fetal\nultrasound dataset of 2M images. By selecting the well-established DINOv2\nmethod for pretraining, we achieve state-of-the-art results on three fetal\nultrasound datasets, covering data from different countries, classification,\nsegmentation, and few-shot tasks. We compare against a series of models\npretrained on natural images, ultrasound images, and supervised baselines. Our\nresults demonstrate two key insights: (i) Pretraining on custom data is worth\nit, even if smaller models are trained on less data, as scaling in natural\nimage pretraining does not translate to ultrasound performance. (ii) Well-tuned\nmethods from computer vision are making it feasible to train custom foundation\nmodels for a given medical domain, requiring no hyperparameter tuning and\nlittle methodological adaptation. Given these findings, we argue that a bias\ntowards methodological innovation should be avoided when developing domain\nspecific foundation models under common computational resource constraints.", "comment": "Submitted version of paper accepted at MICCAI 2025", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI;cs.LG;I.4", "links": "http://arxiv.org/abs/2506.19552v1;http://arxiv.org/pdf/2506.19552v1", "pdf_url": "http://arxiv.org/pdf/2506.19552v1"}, {"title": "Streamlining the annotation process by radiologists of volumetric medical images with few-shot learning", "link": "https://link.springer.com/article/10.1007/s11548-025-03457-3", "details": "A Ryabtsev, R Lederman, J Sosna, L Joskowicz - International Journal of Computer \u2026, 2025", "abstract": "Purpose Radiologist's manual annotations limit robust deep learning in volumetric medical imaging. While supervised methods excel with large annotated datasets, few- shot learning performs well for large structures but struggles with small ones, such as \u2026"}]
