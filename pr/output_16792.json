[{"title": "Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation", "link": "https://arxiv.org/pdf/2505.16222", "details": "J Moon, Y Hwang, D Lee, T Kang, Y Kim, K Jung - arXiv preprint arXiv:2505.16222, 2025", "abstract": "With the growing use of large language models(LLMs) as evaluators, their application has expanded to code evaluation tasks, where they assess the correctness of generated code without relying on reference implementations. While \u2026", "entry_id": "http://arxiv.org/abs/2505.16222v1", "updated": "2025-05-22 04:49:33", "published": "2025-05-22 04:49:33", "authors": "Jiwon Moon;Yerin Hwang;Dongryeol Lee;Taegwan Kang;Yongil Kim;Kyomin Jung", "summary": "With the growing use of large language models(LLMs) as evaluators, their\napplication has expanded to code evaluation tasks, where they assess the\ncorrectness of generated code without relying on reference implementations.\nWhile this offers scalability and flexibility, it also raises a critical,\nunresolved question: Can LLM judges fairly and robustly evaluate semantically\nequivalent code with superficial variations? Functionally correct code often\nexhibits variations-such as differences in variable names, comments, or\nformatting-that should not influence its correctness. Yet, whether LLM judges\ncan reliably handle these variations remains unclear. We present the first\ncomprehensive study of this issue, defining six types of potential bias in code\nevaluation and revealing their systematic impact on LLM judges. Across five\nprogramming languages and multiple LLMs, we empirically demonstrate that all\ntested LLM judges are susceptible to both positive and negative biases,\nresulting in inflated or unfairly low scores. Moreover, we observe that LLM\njudges remain vulnerable to these biases even when prompted to generate test\ncases before scoring, highlighting the need for more robust code evaluation\nmethods.", "comment": "26 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.SE", "links": "http://arxiv.org/abs/2505.16222v1;http://arxiv.org/pdf/2505.16222v1", "pdf_url": "http://arxiv.org/pdf/2505.16222v1"}, {"title": "Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation", "link": "https://arxiv.org/pdf/2505.16065", "details": "R Xi, H Ba, H Yuan, R Agrawal, A Prakash - arXiv preprint arXiv:2505.16065, 2025", "abstract": "\u2026 (b) **LLM** **Evaluation** \u2026 We note that the **LLM** **evaluation** may introduce a bias toward LLM-generated data, potentially inflating alignment scores due to shared generation patterns. Among the evaluated models, M2 \u2026", "entry_id": "http://arxiv.org/abs/2505.16065v1", "updated": "2025-05-21 22:33:40", "published": "2025-05-21 22:33:40", "authors": "Ruijie Xi;He Ba;Hao Yuan;Rishu Agrawal;Arul Prakash", "summary": "Embedding-Based Retrieval (EBR) is an important technique in modern search\nengines, enabling semantic match between search queries and relevant results.\nHowever, search logging data on platforms like Facebook Marketplace lacks the\ndiversity and details needed for effective EBR model training, limiting the\nmodels' ability to capture nuanced search patterns. To address this challenge,\nwe propose Aug2Search, an EBR-based framework leveraging synthetic data\ngenerated by Generative AI (GenAI) models, in a multimodal and multitask\napproach to optimize query-product relevance. This paper investigates the\ncapabilities of GenAI, particularly Large Language Models (LLMs), in generating\nhigh-quality synthetic data, and analyzing its impact on enhancing EBR models.\nWe conducted experiments using eight Llama models and 100 million data points\nfrom Facebook Marketplace logs. Our synthetic data generation follows three\nstrategies: (1) generate queries, (2) enhance product listings, and (3)\ngenerate queries from enhanced listings. We train EBR models on three different\ndatasets: sampled engagement data or original data ((e.g., \"Click\" and \"Listing\nInteractions\")), synthetic data, and a mixture of both engagement and synthetic\ndata to assess their performance across various training sets. Our findings\nunderscore the robustness of Llama models in producing synthetic queries and\nlistings with high coherence, relevance, and diversity, while maintaining low\nlevels of hallucination. Aug2Search achieves an improvement of up to 4% in\nROC_AUC with 100 million synthetic data samples, demonstrating the\neffectiveness of our approach. Moreover, our experiments reveal that with the\nsame volume of training data, models trained exclusively on synthetic data\noften outperform those trained on original data only or a mixture of original\nand synthetic data.", "comment": null, "journal_ref": null, "primary_category": "cs.IR", "categories": "cs.IR;cs.CL", "links": "http://arxiv.org/abs/2505.16065v1;http://arxiv.org/pdf/2505.16065v1", "pdf_url": "http://arxiv.org/pdf/2505.16065v1"}, {"title": "LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods", "link": "https://arxiv.org/pdf/2505.16129", "details": "H Cui - arXiv preprint arXiv:2505.16129, 2025", "abstract": "\u2026 This choice was made to reduce confounding variables and focus on the effects of the source language, especially given that prior studies have reached opposing conclusions about its role in **LLM** **evaluation**. While this design helped ensure \u2026", "entry_id": "http://arxiv.org/abs/2505.16129v1", "updated": "2025-05-22 02:14:38", "published": "2025-05-22 02:14:38", "authors": "Hyang Cui", "summary": "Recent studies have applied large language models (LLMs) to machine\ntranslation quality estimation (MTQE) by prompting models to assign numeric\nscores. Nonetheless, these direct scoring methods tend to show low\nsegment-level correlation with human judgments. In this paper, we propose a\ngeneration-based evaluation paradigm that leverages decoder-only LLMs to\nproduce high-quality references, followed by semantic similarity scoring using\nsentence embeddings. We conduct the most extensive evaluation to date in MTQE,\ncovering 8 LLMs and 8 language pairs. Empirical results show that our method\noutperforms both intra-LLM direct scoring baselines and external non-LLM\nreference-free metrics from MTME. These findings demonstrate the strength of\ngeneration-based evaluation and support a shift toward hybrid approaches that\ncombine fluent generation with accurate semantic assessment.", "comment": "5 pages, 2 figures, 2 tables. Conforms to the ACL Rolling Review\n  (ARR) short paper track. Code and data available at:\n  https://github.com/CuiNiki/LLMs-Are-Not-Scorers", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;I.2.7", "links": "http://arxiv.org/abs/2505.16129v1;http://arxiv.org/pdf/2505.16129v1", "pdf_url": "http://arxiv.org/pdf/2505.16129v1"}, {"title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios", "link": "https://arxiv.org/pdf/2505.16944", "details": "Y Qi, H Peng, X Wang, A Xin, Y Liu, B Xu, L Hou, J Li - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 For each instruction, we annotate the associated constraints and corresponding evaluation metrics, including code-based evaluation, LLM-based evaluation, and hybrid code- **LLM** **evaluation**. We use AGENTIF to systematically evaluate existing \u2026", "entry_id": "http://arxiv.org/abs/2505.16944v1", "updated": "2025-05-22 17:31:10", "published": "2025-05-22 17:31:10", "authors": "Yunjia Qi;Hao Peng;Xiaozhi Wang;Amy Xin;Youfeng Liu;Bin Xu;Lei Hou;Juanzi Li", "summary": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL", "links": "http://arxiv.org/abs/2505.16944v1;http://arxiv.org/pdf/2505.16944v1", "pdf_url": "http://arxiv.org/pdf/2505.16944v1"}, {"title": "A Comparative Study of Frameworks Used to Evaluate Large Language Models in Automated Software Testing", "link": "https://www.researchgate.net/profile/Habeeb_Agoro/publication/391985822_A_Comparative_Study_of_Frameworks_Used_to_Evaluate_Large_Language_Models_in_Automated_Software_Testing/links/682f68b2026fee1034fa276c/A-Comparative-Study-of-Frameworks-Used-to-Evaluate-Large-Language-Models-in-Automated-Software-Testing.pdf", "details": "H Agoro, S Andre - 2025", "abstract": "\u2026 The findings contribute to the ongoing discourse on **LLM** **evaluation** , emphasizing the importance of comprehensive assessment \u2026 This chapter highlights the importance of forward-thinking approaches in **LLM** **evaluation** , paving the way for \u2026"}, {"title": "OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models", "link": "https://arxiv.org/pdf/2505.16036", "details": "BE \u00c7etin, Y \u00d6zen, EN Demiry\u0131lmaz, K Eng\u00fcr\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Language Gap A significant limitation of the existing ethical **LLM** **evaluation** landscape is its strong Anglocentric bias. The majority of studies are developed primarily for English. Since model performance, safety alignment, and reliability can \u2026", "entry_id": "http://arxiv.org/abs/2505.16036v1", "updated": "2025-05-21 21:31:35", "published": "2025-05-21 21:31:35", "authors": "Burak Erin\u00e7 \u00c7etin;Y\u0131ld\u0131r\u0131m \u00d6zen;Elif Naz Demiry\u0131lmaz;Kaan Eng\u00fcr;Cagri Toraman", "summary": "Generative large language models present significant potential but also raise\ncritical ethical concerns. Most studies focus on narrow ethical dimensions, and\nalso limited diversity of languages and models. To address these gaps, we\nconduct a broad ethical evaluation of 29 recent open-source large language\nmodels using a novel data collection including four ethical aspects:\nRobustness, reliability, safety, and fairness. We analyze model behavior in\nboth a commonly used language, English, and a low-resource language, Turkish.\nOur aim is to provide a comprehensive ethical assessment and guide safer model\ndevelopment by filling existing gaps in evaluation breadth, language coverage,\nand model diversity. Our experimental results, based on LLM-as-a-Judge, reveal\nthat optimization efforts for many open-source models appear to have\nprioritized safety and fairness, and demonstrated good robustness while\nreliability remains a concern. We demonstrate that ethical evaluation can be\neffectively conducted independently of the language used. In addition, models\nwith larger parameter counts tend to exhibit better ethical performance, with\nGemma and Qwen models demonstrating the most ethical behavior among those\nevaluated.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.16036v1;http://arxiv.org/pdf/2505.16036v1", "pdf_url": "http://arxiv.org/pdf/2505.16036v1"}, {"title": "Leveraging Domain Expertise: The Impact of Media Experts in Developing", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DdD5fEQAAQBAJ%26oi%3Dfnd%26pg%3DPA308%26dq%3D%2522llm%2Bevaluation%2522%26ots%3DiyZfLv9rtU%26sig%3D7c2Xy1soOjrfVjhGX0p7pNwstmk", "details": "ZWH Deng, C Peng, J Cai, Z Lu - \u2026 : First International Conference, IMLIP 2024, Beijing \u2026, 2025", "abstract": "Large language models (LLMs) have shown remarkable ca\u0440\u0430bilities in generating high-quality text based on large amounts of data. Nevertheless, in practical applications, the general LLMs do not sufficiently address the unique demands of \u2026"}, {"title": "ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts", "link": "https://arxiv.org/pdf/2505.16566", "details": "D Noh, D Koh, J Yuk, G Kim, J Lee, K Lim, C Park - arXiv preprint arXiv:2505.16566, 2025", "abstract": "\u2026 Thus, both domain generalization ability and domainspecific reasoning ability should be considered in **LLM** **evaluation**. Fine-grained benchmarks like ScholarBench are effective in quantitatively revealing these imbalances in performance distribution. \u2026", "entry_id": "http://arxiv.org/abs/2505.16566v1", "updated": "2025-05-22 11:59:06", "published": "2025-05-22 11:59:06", "authors": "Dongwon Noh;Donghyeok Koh;Junghun Yuk;Gyuwan Kim;Jaeyong Lee;Kyungtae Lim;Cheoneum Park", "summary": "Prior benchmarks for evaluating the domain-specific knowledge of large\nlanguage models (LLMs) lack the scalability to handle complex academic tasks.\nTo address this, we introduce \\texttt{ScholarBench}, a benchmark centered on\ndeep expert knowledge and complex academic problem-solving, which evaluates the\nacademic reasoning ability of LLMs and is constructed through a three-step\nprocess. \\texttt{ScholarBench} targets more specialized and logically complex\ncontexts derived from academic literature, encompassing five distinct problem\ntypes. Unlike prior benchmarks, \\texttt{ScholarBench} evaluates the\nabstraction, comprehension, and reasoning capabilities of LLMs across eight\ndistinct research domains. To ensure high-quality evaluation data, we define\ncategory-specific example attributes and design questions that are aligned with\nthe characteristic research methodologies and discourse structures of each\ndomain. Additionally, this benchmark operates as an English-Korean bilingual\ndataset, facilitating simultaneous evaluation for linguistic capabilities of\nLLMs in both languages. The benchmark comprises 5,031 examples in Korean and\n5,309 in English, with even state-of-the-art models like o3-mini achieving an\naverage evaluation score of only 0.543, demonstrating the challenging nature of\nthis benchmark.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.16566v1;http://arxiv.org/pdf/2505.16566v1", "pdf_url": "http://arxiv.org/pdf/2505.16566v1"}, {"title": "BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators", "link": "https://arxiv.org/pdf/2505.16081", "details": "KMA Solaiman - arXiv preprint arXiv:2505.16081, 2025", "abstract": "We present BiasLab, a dataset of 300 political news articles annotated for perceived ideological bias. These articles were selected from a curated 900-document pool covering diverse political events and source biases. Each article is labeled by \u2026", "entry_id": "http://arxiv.org/abs/2505.16081v1", "updated": "2025-05-21 23:50:42", "published": "2025-05-21 23:50:42", "authors": "KMA Solaiman", "summary": "We present BiasLab, a dataset of 300 political news articles annotated for\nperceived ideological bias. These articles were selected from a curated\n900-document pool covering diverse political events and source biases. Each\narticle is labeled by crowdworkers along two independent scales, assessing\nsentiment toward the Democratic and Republican parties, and enriched with\nrationale indicators. The annotation pipeline incorporates targeted worker\nqualification and was refined through pilot-phase analysis. We quantify\ninter-annotator agreement, analyze misalignment with source-level outlet bias,\nand organize the resulting labels into interpretable subsets. Additionally, we\nsimulate annotation using schema-constrained GPT-4o, enabling direct comparison\nto human labels and revealing mirrored asymmetries, especially in\nmisclassifying subtly right-leaning content. We define two modeling tasks:\nperception drift prediction and rationale type classification, and report\nbaseline performance to illustrate the challenge of explainable bias detection.\nBiasLab's rich rationale annotations provide actionable interpretations that\nfacilitate explainable modeling of political bias, supporting the development\nof transparent, socially aware NLP systems. We release the dataset, annotation\nschema, and modeling code to encourage research on human-in-the-loop\ninterpretability and the evaluation of explanation effectiveness in real-world\nsettings.", "comment": "Under review", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.16081v1;http://arxiv.org/pdf/2505.16081v1", "pdf_url": "http://arxiv.org/pdf/2505.16081v1"}]
