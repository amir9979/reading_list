[{"title": "Prompt Learning with Extended Kalman Filter for Pre-trained Language Models", "link": "https://www.ijcai.org/proceedings/2024/0492.pdf", "details": "Q Li, X Xie, C Wang, SK Zhou", "abstract": "Prompt learning has gained popularity as a means to leverage the knowledge embedded in pre-trained language models (PLMs) for NLP tasks while using a limited number of trainable parameters. While it has shown promise in tasks like \u2026"}, {"title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "link": "https://arxiv.org/pdf/2407.21417", "details": "Z Wu, Y Zhang, P Qi, Y Xu, R Han, Y Zhang, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (ie, follow open-ended instructions) and \u2026"}, {"title": "Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models", "link": "https://aclanthology.org/2024.acl-long.765.pdf", "details": "S Li, R Lin, S Pei - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities in production. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer \u2026"}, {"title": "KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models", "link": "https://arxiv.org/pdf/2408.03297", "details": "R Zhang, Y Xu, Y Xiao, R Zhu, X Jiang, X Chu, J Zhao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks \u2026"}, {"title": "Making Long-Context Language Models Better Multi-Hop Reasoners", "link": "https://arxiv.org/pdf/2408.03246", "details": "Y Li, S Liang, MR Lyu, L Wang - arXiv preprint arXiv:2408.03246, 2024", "abstract": "Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased \u2026"}, {"title": "LLMEmbed: Rethinking Lightweight LLM's Genuine Function in Text Classification", "link": "https://aclanthology.org/2024.acl-long.433.pdf", "details": "CL ChunLiu, H Zhang, K Zhao, X Ju, L Yang - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "With the booming of Large Language Models (LLMs), prompt-learning has become a promising method mainly researched in various research areas. Recently, many attempts based on prompt-learning have been made to improve the performance of \u2026"}, {"title": "Tad: A plug-and-play task-aware decoding method to better adapt llms on downstream tasks", "link": "https://www.ijcai.org/proceedings/2024/0728.pdf", "details": "X Xu, H Chen, Z Lin, J Han, L Gong, G Wang, Y Bao\u2026 - Proceedings of the Thirty \u2026, 2024", "abstract": "Fine-tuning pre-trained models on downstream tasks is a common practice in leveraging large language models (LLMs) today. A critical issue is how to adapt pre- trained models to downstream tasks better, thereby enhancing their performance \u2026"}, {"title": "Monotonic Representation of Numeric Attributes in Language Models", "link": "https://aclanthology.org/2024.acl-short.18.pdf", "details": "B Heinzerling, K Inui - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Abstract Language models (LMs) can express factual knowledge involving numeric properties such as Karl Popper was born in 1902. However, how this information is encoded in the model's internal representations is not understood well. Here, we \u2026"}, {"title": "KnowledgeFMath: A Knowledge-Intensive Math Reasoning Dataset in Finance Domains", "link": "https://aclanthology.org/2024.acl-long.693.pdf", "details": "Y Zhao, H Liu, Y Long, R Zhang, C Zhao, A Cohan - \u2026 of the 62nd Annual Meeting of \u2026, 2024", "abstract": "We introduce KnowledgeFMath, a novel benchmark designed to evaluate LLMs' capabilities in solving knowledge-intensive math reasoning problems. Compared to prior works, this study features three core advancements. First, KnowledgeFMath \u2026"}]
