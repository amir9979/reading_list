[{"title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement", "link": "https://arxiv.org/pdf/2503.17352", "details": "Y Deng, H Bansal, F Yin, N Peng, W Wang, KW Chang - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex reasoning abilities in large language models (LLMs), including sophisticated behaviors such as self-verification and self-correction, can be achieved by RL with \u2026"}, {"title": "SWI: Speaking with Intent in Large Language Models", "link": "https://arxiv.org/pdf/2503.21544%3F", "details": "Y Yin, EJ Hwang, G Carenini - arXiv preprint arXiv:2503.21544, 2025", "abstract": "Intent, typically clearly formulated and planned, functions as a cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated \u2026"}, {"title": "No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language models", "link": "https://arxiv.org/pdf/2503.11985", "details": "CV Kumar, A Urlana, G Kanumolu, BM Garlapati\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Advancements in Large Language Models (LLMs) have increased the performance of different natural language understanding as well as generation tasks. Although LLMs have breached the state-of-the-art performance in various tasks, they often \u2026"}, {"title": "Corrective In-Context Learning: Evaluating Self-Correction in Large Language Models", "link": "https://arxiv.org/pdf/2503.16022", "details": "M Sanz-Guerrero, K von der Wense - arXiv preprint arXiv:2503.16022, 2025", "abstract": "In-context learning (ICL) has transformed the use of large language models (LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled examples without finetuning. Despite its effectiveness, ICL is prone to errors, especially for \u2026"}]
