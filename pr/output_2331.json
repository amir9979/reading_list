[{"title": "Enhancing Antibiotic Stewardship using a Natural Language Approach for Better Feature Representation", "link": "https://arxiv.org/pdf/2405.20419", "details": "SA Lee, T Brokowski, JN Chiang - arXiv preprint arXiv:2405.20419, 2024", "abstract": "The rapid emergence of antibiotic-resistant bacteria is recognized as a global healthcare crisis, undermining the efficacy of life-saving antibiotics. This crisis is driven by the improper and overuse of antibiotics, which escalates bacterial \u2026"}, {"title": "D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models", "link": "https://arxiv.org/pdf/2406.01375", "details": "H Que, J Liu, G Zhang, C Zhang, X Qu, Y Ma, F Duan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Continual Pre-Training (CPT) on Large Language Models (LLMs) has been widely used to expand the model's fundamental understanding of specific downstream domains (eg, math and code). For the CPT on domain-specific LLMs, one important \u2026"}, {"title": "EHR-SeqSQL: A Sequential Text-to-SQL Dataset For Interactively Exploring Electronic Health Records", "link": "https://arxiv.org/pdf/2406.00019", "details": "J Ryu, S Cho, G Lee, E Choi - arXiv preprint arXiv:2406.00019, 2024", "abstract": "In this paper, we introduce EHR-SeqSQL, a novel sequential text-to-SQL dataset for Electronic Health Record (EHR) databases. EHR-SeqSQL is designed to address critical yet underexplored aspects in text-to-SQL parsing: interactivity \u2026"}, {"title": "Pathophysiological Features in Electronic Medical Records Sustain Model Performance under Temporal Dataset Shift", "link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11141811/", "details": "R Brosula, CK Corbin, JH Chen - AMIA Summits on Translational Science \u2026, 2024", "abstract": "Access to real-world data streams like electronic medical records (EMRs) has accelerated the development of supervised machine learning (ML) models for clinical applications. However, few studies investigate the differential impact of \u2026"}, {"title": "Textual Inversion and Self-supervised Refinement for Radiology Report Generation", "link": "https://arxiv.org/pdf/2405.20607", "details": "Y Luo, H Li, X Wu, M Cao, X Huang, Z Zhu, P Liao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing mainstream approaches follow the encoder-decoder paradigm for generating radiology reports. They focus on improving the network structure of encoders and decoders, which leads to two shortcomings: overlooking the modality \u2026"}, {"title": "A Temporal Dataset for Respiratory Support in Critically Ill Patients", "link": "https://physionet.org/content/temporal-respiratory-support/1.0.0/", "details": "M Moukheiber, L Moukheiber, D Moukheiber, S Hao\u2026", "abstract": "We present a temporal benchmark dataset for clinical respiratory intervention tasks in intensive care unit (ICU) patients, derived from the MIMIC v2. 2 dataset. The data consists of 50,920 adult ICU patients and includes 90-day hourly ventilation data \u2026"}, {"title": "Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2405.20775", "details": "X Huang, X Wang, H Zhang, J Xi, J An, H Wang, C Pan - arXiv preprint arXiv \u2026, 2024", "abstract": "Security concerns related to Large Language Models (LLMs) have been extensively explored, yet the safety implications for Multimodal Large Language Models (MLLMs), particularly in medical contexts (MedMLLMs), remain insufficiently studied \u2026"}, {"title": "Evaluation of large language model performance on the Biomedical Language Understanding and Reasoning Benchmark", "link": "https://www.medrxiv.org/content/10.1101/2024.05.17.24307411.full.pdf", "details": "H Feng, F Ronzano, J LaFleur, M Garber, R de Oliveira\u2026 - medRxiv, 2024", "abstract": "Background: The ability of large language models (LLMs) to interpret and generate human-like text has been accompanied with speculation about their application in medicine and clinical research. There is limited data available to inform evidence \u2026"}, {"title": "Code Pretraining Improves Entity Tracking Abilities of Language Models", "link": "https://arxiv.org/pdf/2405.21068", "details": "N Kim, S Schuster, S Toshniwal - arXiv preprint arXiv:2405.21068, 2024", "abstract": "Recent work has provided indirect evidence that pretraining language models on code improves the ability of models to track state changes of discourse entities expressed in natural language. In this work, we systematically test this claim by \u2026"}]
