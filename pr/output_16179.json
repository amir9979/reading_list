[{"title": "PixCon: Pixel-Level Contrastive Learning Revisited", "link": "https://www.mdpi.com/2079-9292/14/8/1623", "details": "Z Pang, Y Nakashima, M Otani, H Nagahara - Electronics, 2025", "abstract": "Contrastive image representation learning has been essential for pre-training vision foundation models to deliver excellent transfer learning performance. It was originally developed based on instance discrimination, which focuses on instance-level \u2026"}, {"title": "Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks", "link": "https://arxiv.org/pdf/2505.04046", "details": "X Wang, S Duan, Q Li, G Duan, Y Sun, D Peng - arXiv preprint arXiv:2505.04046, 2025", "abstract": "Recently, trustworthy multi-view learning has attracted extensive attention because evidence learning can provide reliable uncertainty estimation to enhance the credibility of multi-view predictions. Existing trusted multi-view learning methods \u2026"}, {"title": "Joint image clustering and self-supervised representation learning through debiased contrastive loss", "link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13406/1340614/Joint-image-clustering-and-self-supervised-representation-learning-through-debiased/10.1117/12.3047438.short", "details": "SF Zheng, J Nam, S Baur, M Wang, N Zebardast\u2026 - Medical Imaging 2025 \u2026, 2025", "abstract": "Joint self-supervised representation learning and image clustering have emerged as some of the most effective techniques for visual representation learning. However, existing methods often rely on artificially balanced datasets, raising concerns about \u2026"}, {"title": "Representation Learning via Non-Contrastive Mutual Information", "link": "https://arxiv.org/pdf/2504.16667%3F", "details": "ZD Guo, BA Pires, K Khetarpal, D Schuurmans, B Dai - arXiv preprint arXiv \u2026, 2025", "abstract": "Labeling data is often very time consuming and expensive, leaving us with a majority of unlabeled data. Self-supervised representation learning methods such as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very successful at learning \u2026"}]
