[{"title": "Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination", "link": "https://arxiv.org/pdf/2409.12746", "details": "ES Salido, R Morante, J Gonzalo, G Marco\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this article we present UNED-ACCESS 2024, a bilingual dataset that consists of 1003 multiple-choice questions of university entrance level exams in Spanish and English. Questions are originally formulated in Spanish and translated manually into \u2026"}, {"title": "Are Expert-Level Language Models Expert-Level Annotators?", "link": "https://arxiv.org/pdf/2410.03254", "details": "YM Tseng, WL Chen, CC Chen, HH Chen - arXiv preprint arXiv:2410.03254, 2024", "abstract": "Data annotation refers to the labeling or tagging of textual data with relevant information. A large body of works have reported positive results on leveraging LLMs as an alternative to human annotators. However, existing studies focus on classic \u2026"}, {"title": "Reasoning Elicitation in Language Models via Counterfactual Feedback", "link": "https://arxiv.org/pdf/2410.03767", "details": "A H\u00fcy\u00fck, X Xu, J Maasch, AV Nori, J Gonz\u00e1lez - arXiv preprint arXiv:2410.03767, 2024", "abstract": "Despite the increasing effectiveness of language models, their reasoning capabilities remain underdeveloped. In particular, causal reasoning through counterfactual question answering is lacking. This work aims to bridge this gap. We first derive \u2026"}, {"title": "AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text", "link": "https://arxiv.org/pdf/2410.04265", "details": "X Lu, M Sclar, S Hallinan, N Mireshghallah, J Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Creativity has long been considered one of the most difficult aspect of human intelligence for AI to mimic. However, the rise of Large Language Models (LLMs), like ChatGPT, has raised questions about whether AI can match or even surpass human \u2026"}, {"title": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models", "link": "https://arxiv.org/pdf/2410.02355", "details": "J Fang, H Jiang, K Wang, Y Ma, X Wang, X He, T Chua - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) often exhibit hallucinations due to incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating \u2026"}, {"title": "A Capture-Recapture Approach to Facilitate Causal Inference for a Trial-eligible Observational Cohort", "link": "https://arxiv.org/pdf/2409.18358", "details": "L Ge, Y Zhang, LA Waller, RH Lyles - arXiv preprint arXiv:2409.18358, 2024", "abstract": "Background: We extend recently proposed design-based capture-recapture methods for prevalence estimation among registry participants, in order to support causal inference among a trial-eligible target population. The proposed design for CRC \u2026"}, {"title": "Emerging Reliance Behaviors in Human-AI Text Generation: Hallucinations, Data Quality Assessment, and Cognitive Forcing Functions", "link": "https://arxiv.org/pdf/2409.08937", "details": "Z Ashktorab, Q Pan, W Geyer, M Desmond\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we investigate the impact of hallucinations and cognitive forcing functions in human-AI collaborative text generation tasks, focusing on the use of Large Language Models (LLMs) to assist in generating high-quality conversational \u2026"}, {"title": "SpecEval: Evaluating Code Comprehension in Large Language Models via Program Specifications", "link": "https://arxiv.org/pdf/2409.12866", "details": "L Ma, S Liu, L Bu, S Li, Y Wang, Y Liu - arXiv preprint arXiv:2409.12866, 2024", "abstract": "Large Language models have achieved impressive performance in automated software engineering. Extensive efforts have been made to evaluate the abilities of code LLMs in various aspects, with an increasing number of benchmarks and \u2026"}, {"title": "CEval: A Benchmark for Evaluating Counterfactual Text Generation", "link": "https://aclanthology.org/2024.inlg-main.6.pdf", "details": "C Seifert, J Schl\u00f6tterer - Proceedings of the 17th International Natural \u2026, 2024", "abstract": "Counterfactual text generation aims to minimally change a text, such that it is classified differently. Assessing progress in method development for counterfactual text generation is hindered by a non-uniform usage of data sets and metrics in \u2026"}]
