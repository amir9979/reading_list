[{"title": "Show, Don't Tell: Aligning Language Models with Demonstrated Feedback", "link": "https://arxiv.org/pdf/2406.00888", "details": "O Shaikh, M Lam, J Hejna, Y Shao, M Bernstein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large \u2026"}, {"title": "Probing Language Models for Pre-training Data Detection", "link": "https://arxiv.org/pdf/2406.01333", "details": "Z Liu, T Zhu, C Tan, H Lu, B Liu, W Chen - arXiv preprint arXiv:2406.01333, 2024", "abstract": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to \u2026"}, {"title": "Exploring Clean Label Backdoor Attacks and Defense in Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10549768/", "details": "S Zhao, LA Tuan, J Fu, J Wen, W Luo - IEEE/ACM Transactions on Audio, Speech \u2026, 2024", "abstract": "Despite being widely applied, pre-trained language models have been proven vulnerable to backdoor attacks. Backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger \u2026"}, {"title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning", "link": "https://arxiv.org/pdf/2406.00645", "details": "Y Fu, H Zhang, D Wu, W Xu, B Boulet - arXiv preprint arXiv:2406.00645, 2024", "abstract": "In this work, we investigate how to leverage pre-trained visual-language models (VLM) for online Reinforcement Learning (RL). In particular, we focus on sparse reward tasks with pre-defined textual task descriptions. We first identify the problem \u2026"}, {"title": "Code Pretraining Improves Entity Tracking Abilities of Language Models", "link": "https://arxiv.org/pdf/2405.21068", "details": "N Kim, S Schuster, S Toshniwal - arXiv preprint arXiv:2405.21068, 2024", "abstract": "Recent work has provided indirect evidence that pretraining language models on code improves the ability of models to track state changes of discourse entities expressed in natural language. In this work, we systematically test this claim by \u2026"}, {"title": "FOCUS: Forging Originality through Contrastive Use in Self-Plagiarism for Language Models", "link": "https://arxiv.org/pdf/2406.00839", "details": "K Lan, T Fang, DF Wong, Y Xu, LS Chao, CG Zhao - arXiv preprint arXiv:2406.00839, 2024", "abstract": "Pre-trained Language Models (PLMs) have shown impressive results in various Natural Language Generation (NLG) tasks, such as powering chatbots and generating stories. However, an ethical concern arises due to their potential to \u2026"}, {"title": "mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models", "link": "https://arxiv.org/pdf/2406.02301", "details": "H Lai, M Nissim - arXiv preprint arXiv:2406.02301, 2024", "abstract": "Large language models (LLMs) with Chain-of-thought (CoT) have recently emerged as a powerful technique for eliciting reasoning to improve various downstream tasks. As most research mainly focuses on English, with few explorations in a multilingual \u2026"}, {"title": "Are language models rational? The case of coherence norms and belief revision", "link": "https://arxiv.org/abs/2406.03442", "details": "T Hofweber, P Hase, E Stengel-Eskin, M Bansal - arXiv preprint arXiv:2406.03442, 2024", "abstract": "Do norms of rationality apply to machine learning models, in particular language models? In this paper we investigate this question by focusing on a special subset of rational norms: coherence norms. We consider both logical coherence norms as well \u2026"}, {"title": "Automatic Instruction Evolving for Large Language Models", "link": "https://arxiv.org/pdf/2406.00770", "details": "W Zeng, C Xu, Y Zhao, JG Lou, W Chen - arXiv preprint arXiv:2406.00770, 2024", "abstract": "Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This \u2026"}]
