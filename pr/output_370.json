'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Can 3D Vision-Language Models Truly Understand Natural'
[{"title": "Automated Extraction of Stroke Severity from Unstructured Electronic Health Records using Natural Language Processing", "link": "https://www.medrxiv.org/content/10.1101/2024.03.08.24304011.full.pdf", "details": "M Bento Fernandes, B Westover, AB Singhal, SF Zafar - medRxiv, 2024", "abstract": "BACKGROUND: Multi-center electronic health records (EHR) can support quality improvement initiatives and comparative effectiveness research in stroke care. However, limitations of EHR-based research include challenges in abstracting key \u2026"}, {"title": "Decomposed Meta-Learning for Few-Shot Sequence Labeling", "link": "https://ieeexplore.ieee.org/abstract/document/10458261/", "details": "T Ma, Q Wu, H Jiang, J Lin, BF Karlsson, T Zhao\u2026 - IEEE/ACM Transactions on \u2026, 2024", "abstract": "Few-shot sequence labeling is a general problem formulation for many natural language understanding tasks in data-scarcity scenarios, which require models to generalize to new types via only a few labeled examples. Recent advances mostly \u2026"}, {"title": "Overview of the clpsych 2024 shared task: Leveraging large language models to identify evidence of suicidality risk in online posts", "link": "https://aclanthology.org/2024.clpsych-1.15.pdf", "details": "J Chim, A Tsakalidis, D Gkoumas, D Atzil-Slonim\u2026 - Proceedings of the 9th \u2026, 2024", "abstract": "We present the overview of the CLPsych 2024 Shared Task, focusing on leveraging open source Large Language Models (LLMs) for identifying textual evidence that supports the suicidal risk level of individuals on Reddit. In particular, given a Reddit \u2026"}, {"title": "Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models", "link": "https://arxiv.org/html/2403.13590v1", "details": "A Liusie, Y Fathullah, MJF Gales - arXiv preprint arXiv:2403.13590, 2024", "abstract": "Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities and versatility in NLP tasks, however they sometimes fail to maintain crucial invariances for specific tasks. One example is permutation sensitivity, where \u2026"}, {"title": "A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries", "link": "https://arxiv.org/pdf/2403.05720", "details": "A Aali, D Van Veen, YI Arefeen, J Hom, C Bluethgen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for \u2026"}, {"title": "Where does In-context Translation Happen in Large Language Models", "link": "https://arxiv.org/html/2403.04510v1", "details": "S Sia, D Mueller, K Duh - arXiv preprint arXiv:2403.04510, 2024", "abstract": "Self-supervised large language models have demonstrated the ability to perform Machine Translation (MT) via in-context learning, but little is known about where the model performs the task with respect to prompt instructions and demonstration \u2026"}, {"title": "Online Training of Large Language Models: Learn while chatting", "link": "https://arxiv.org/pdf/2403.04790", "details": "J Liang, Z Wang, Z Ma, J Li, Z Zhang, X Wu, B Wang - arXiv preprint arXiv:2403.04790, 2024", "abstract": "Large Language Models (LLMs) have dramatically revolutionized the field of Natural Language Processing (NLP), offering remarkable capabilities that have garnered widespread usage. However, existing interaction paradigms between LLMs and \u2026"}, {"title": "When Is a Name Sensitive? Eponyms in Clinical Text and Implications for De-Identification", "link": "https://aclanthology.org/2024.caldpseudo-1.9.pdf", "details": "T Vakili, T Hullmann, A Henriksson, H Dalianis - Proceedings of the Workshop on \u2026, 2024", "abstract": "Clinical data, in the form of electronic health records, are rich resources that can be tapped using natural language processing. At the same time, they contain very sensitive information that must be protected. One strategy is to remove or obscure \u2026"}]
