[{"title": "Human interpretable structure-property relationships in chemistry using explainable machine learning and large language models", "link": "https://www.nature.com/articles/s42004-024-01393-y", "details": "GP Wellawatte, P Schwaller - Communications Chemistry, 2025", "abstract": "Abstract Explainable Artificial Intelligence (XAI) is an emerging field in AI that aims to address the opaque nature of machine learning models. Furthermore, it has been shown that XAI can be used to extract input-output relationships, making them a \u2026"}, {"title": "spEMO: Exploring the Capacity of Foundation Models for Analyzing Spatial Multi-Omic Data", "link": "https://www.biorxiv.org/content/10.1101/2025.01.13.632818.full.pdf", "details": "T Liu, T Huang, R Ying, H Zhao - bioRxiv, 2025", "abstract": "Several pathology foundation models have been designed by pre-training a model with pathology information for disease-centered downstream applications. These models have been treated as a breakthrough for pathology research. Along with \u2026"}, {"title": "Foundations of Large Language Models", "link": "https://arxiv.org/pdf/2501.09223", "details": "T Xiao, J Zhu - arXiv preprint arXiv:2501.09223, 2025", "abstract": "This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting- edge technologies. The book is structured into four main chapters, each exploring a \u2026"}, {"title": "ArithmeticGPT: empowering small-size large language models with advanced arithmetic skills", "link": "https://link.springer.com/article/10.1007/s10994-024-06681-1", "details": "Z Liu, Y Zheng, Z Yin, J Chen, T Liu, M Tian, W Luo - Machine Learning, 2025", "abstract": "Large language models (LLMs) have shown remarkable capabilities in understanding and generating language across a wide range of domains. However, their performance in advanced arithmetic calculation remains a significant challenge \u2026"}, {"title": "Path-of-Thoughts: Extracting and Following Paths for Robust Relational Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2412.17963", "details": "G Zhang, MA Alomrani, H Gu, J Zhou, Y Hu, B Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) possess vast semantic knowledge but often struggle with complex reasoning tasks, particularly in relational reasoning problems such as kinship or spatial reasoning. In this paper, we present Path-of-Thoughts (PoT), a \u2026"}]
