[{"title": "Adaptive Human-in-the-Loop Testing for LLM-Integrated Applications", "link": "https://www.researchgate.net/profile/Farinu-Hamzah/publication/391908960_Adaptive_Human-in-the-Loop_Testing_for_LLM-Integrated_Applications/links/682d140a026fee1034f9665f/Adaptive-Human-in-the-Loop-Testing-for-LLM-Integrated-Applications.pdf", "details": "B John, BJ Mary, F Hamzah - 2025", "abstract": "As large language models (LLMs) become increasingly integrated into software applications, ensuring their reliable and safe operation across dynamic contexts presents new challenges. Traditional automated testing approaches fall short in \u2026"}, {"title": "Reverse Engineering Human Preferences with Reinforcement Learning", "link": "https://arxiv.org/pdf/2505.15795", "details": "L Alazraki, T Yi-Chern, JA Campos, M Mozes, M Rei\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 We find that frozen LLMs pipelined with these models attain higher **LLM** - **evaluation** scores than existing frameworks. Crucially, unlike other frameworks which intervene directly on the model\u2019s response, our method is virtually undetectable. We also \u2026", "entry_id": "http://arxiv.org/abs/2505.15795v1", "updated": "2025-05-21 17:48:16", "published": "2025-05-21 17:48:16", "authors": "Lisa Alazraki;Tan Yi-Chern;Jon Ander Campos;Maximilian Mozes;Marek Rei;Max Bartolo", "summary": "The capabilities of Large Language Models (LLMs) are routinely evaluated by\nother LLMs trained to predict human preferences. This framework--known as\nLLM-as-a-judge--is highly scalable and relatively low cost. However, it is also\nvulnerable to malicious exploitation, as LLM responses can be tuned to overfit\nthe preferences of the judge. Previous work shows that the answers generated by\na candidate-LLM can be edited post hoc to maximise the score assigned to them\nby a judge-LLM. In this study, we adopt a different approach and use the signal\nprovided by judge-LLMs as a reward to adversarially tune models that generate\ntext preambles designed to boost downstream performance. We find that frozen\nLLMs pipelined with these models attain higher LLM-evaluation scores than\nexisting frameworks. Crucially, unlike other frameworks which intervene\ndirectly on the model's response, our method is virtually undetectable. We also\ndemonstrate that the effectiveness of the tuned preamble generator transfers\nwhen the candidate-LLM and the judge-LLM are replaced with models that are not\nused during training. These findings raise important questions about the design\nof more reliable LLM-as-a-judge evaluation settings. They also demonstrate that\nhuman preferences can be reverse engineered effectively, by pipelining LLMs to\noptimise upstream preambles via reinforcement learning--an approach that could\nfind future applications in diverse tasks and domains beyond adversarial\nattacks.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15795v1;http://arxiv.org/pdf/2505.15795v1", "pdf_url": "http://arxiv.org/pdf/2505.15795v1"}, {"title": "Is LLM an Overconfident Judge? Unveiling the Capabilities of LLMs in Detecting Offensive Language with Annotation Disagreement", "link": "https://www.researchgate.net/profile/Junyu-Lu-11/publication/388883011_Is_LLM_an_Overconfident_Judge_Unveiling_the_Capabilities_of_LLMs_in_Detecting_Offensive_Language_with_Annotation_Disagreement/links/682bf19bd1054b0207eff499/Is-LLM-an-Overconfident-Judge-Unveiling-the-Capabilities-of-LLMs-in-Detecting-Offensive-Language-with-Annotation-Disagreement.pdf", "details": "J Lu, K Ma, K Wang, K Xiao, RKW Lee, B Xu, L Yang\u2026", "abstract": "Large Language Models (LLMs) have become essential for offensive language detection, yet their ability to handle annotation disagreement remains underexplored. Disagreement samples, which arise from subjective interpretations, pose a unique \u2026"}, {"title": "FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain", "link": "https://arxiv.org/pdf/2505.14826", "details": "R Deb, K Thekumparampil, K Kalantari, G Hiranandani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 We demonstrate this on several problems, and back our claims with both quantitative results and an **LLM** **evaluation**. \u2026", "entry_id": "http://arxiv.org/abs/2505.14826v1", "updated": "2025-05-20 18:41:34", "published": "2025-05-20 18:41:34", "authors": "Rohan Deb;Kiran Thekumparampil;Kousha Kalantari;Gaurush Hiranandani;Shoham Sabach;Branislav Kveton", "summary": "Supervised fine-tuning (SFT) is a standard approach to adapting large\nlanguage models (LLMs) to new domains. In this work, we improve the statistical\nefficiency of SFT by selecting an informative subset of training examples.\nSpecifically, for a fixed budget of training examples, which determines the\ncomputational cost of fine-tuning, we determine the most informative ones. The\nkey idea in our method is to select examples that maximize information gain,\nmeasured by the Hessian of the log-likelihood of the LLM. We approximate it\nefficiently by linearizing the LLM at the last layer using multinomial logistic\nregression models. Our approach is computationally efficient, analyzable, and\nperforms well empirically. We demonstrate this on several problems, and back\nour claims with both quantitative results and an LLM evaluation.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CL;stat.ML", "links": "http://arxiv.org/abs/2505.14826v1;http://arxiv.org/pdf/2505.14826v1", "pdf_url": "http://arxiv.org/pdf/2505.14826v1"}, {"title": "Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory", "link": "https://arxiv.org/pdf/2505.15055", "details": "H Zhou, H Huang, Z Zhao, L Han, H Wang, K Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 The pursuit of comprehensive **LLM** **evaluation** has often led to the creation of increasingly large and diverse benchmark datasets. However, the sheer volume of benchmark items does not inherently guarantee higher quality or more insightful \u2026", "entry_id": "http://arxiv.org/abs/2505.15055v1", "updated": "2025-05-21 03:24:11", "published": "2025-05-21 03:24:11", "authors": "Hongli Zhou;Hui Huang;Ziqing Zhao;Lvyuan Han;Huicheng Wang;Kehai Chen;Muyun Yang;Wei Bao;Jian Dong;Bing Xu;Conghui Zhu;Hailong Cao;Tiejun Zhao", "summary": "The evaluation of large language models (LLMs) via benchmarks is widespread,\nyet inconsistencies between different leaderboards and poor separability among\ntop models raise concerns about their ability to accurately reflect authentic\nmodel capabilities. This paper provides a critical analysis of benchmark\neffectiveness, examining main-stream prominent LLM benchmarks using results\nfrom diverse models. We first propose a new framework for accurate and reliable\nestimations of item characteristics and model abilities. Specifically, we\npropose Pseudo-Siamese Network for Item Response Theory (PSN-IRT), an enhanced\nItem Response Theory framework that incorporates a rich set of item parameters\nwithin an IRT-grounded architecture. Based on PSN-IRT, we conduct extensive\nanalysis which reveals significant and varied shortcomings in the measurement\nquality of current benchmarks. Furthermore, we demonstrate that leveraging\nPSN-IRT is able to construct smaller benchmarks while maintaining stronger\nalignment with human preference.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15055v1;http://arxiv.org/pdf/2505.15055v1", "pdf_url": "http://arxiv.org/pdf/2505.15055v1"}, {"title": "A Self-Supervised Multi-Agent Large Language Model Framework for Customized Traffic Mobility Analysis Using Machine Learning Models", "link": "https://journals.sagepub.com/doi/abs/10.1177/03611981251322468", "details": "F Yang, XC Liu, L Lu, B Wang, C Liu - Transportation Research Record, 2025", "abstract": "\u2026 criteria include coherence, relevance, factual accuracy, and logical flow, which are commonly applied in **LLM** **evaluation** frameworks such as BLEU, ROUGE, METEOR, and GPTScore. The detailed evaluation framework ensures a \u2026"}, {"title": "Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering", "link": "https://arxiv.org/pdf/2505.15805", "details": "H Chang, Y Kim, Y Jun, H Lee - arXiv preprint arXiv:2505.15805, 2025", "abstract": "As Large Language Models (LLMs) are increasingly deployed in sensitive domains such as enterprise and government, ensuring that they adhere to user-defined security policies within context is critical-especially with respect to information non-disclosure \u2026", "entry_id": "http://arxiv.org/abs/2505.15805v1", "updated": "2025-05-21 17:58:11", "published": "2025-05-21 17:58:11", "authors": "Hwan Chang;Yumin Kim;Yonghyun Jun;Hwanhee Lee", "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15805v1;http://arxiv.org/pdf/2505.15805v1", "pdf_url": "http://arxiv.org/pdf/2505.15805v1"}, {"title": "ConspEmoLLM-v2: A robust and stable model to detect sentiment-transformed conspiracy theories", "link": "https://arxiv.org/pdf/2505.14917", "details": "Z Liu, P Thompson, J Rong, S Ananiadou - arXiv preprint arXiv:2505.14917, 2025", "abstract": "Despite the many benefits of large language models (LLMs), they can also cause harm, eg, through automatic generation of misinformation, including conspiracy theories. Moreover, LLMs can also ''disguise'' conspiracy theories by altering \u2026", "entry_id": "http://arxiv.org/abs/2505.14917v1", "updated": "2025-05-20 21:12:30", "published": "2025-05-20 21:12:30", "authors": "Zhiwei Liu;Paul Thompson;Jiaqi Rong;Sophia Ananiadou", "summary": "Despite the many benefits of large language models (LLMs), they can also\ncause harm, e.g., through automatic generation of misinformation, including\nconspiracy theories. Moreover, LLMs can also ''disguise'' conspiracy theories\nby altering characteristic textual features, e.g., by transforming their\ntypically strong negative emotions into a more positive tone. Although several\nstudies have proposed automated conspiracy theory detection methods, they are\nusually trained using human-authored text, whose features can vary from\nLLM-generated text. Furthermore, several conspiracy detection models, including\nthe previously proposed ConspEmoLLM, rely heavily on the typical emotional\nfeatures of human-authored conspiracy content. As such, intentionally disguised\ncontent may evade detection. To combat such issues, we firstly developed an\naugmented version of the ConDID conspiracy detection dataset, ConDID-v2, which\nsupplements human-authored conspiracy tweets with versions rewritten by an LLM\nto reduce the negativity of their original sentiment. The quality of the\nrewritten tweets was verified by combining human and LLM-based assessment. We\nsubsequently used ConDID-v2 to train ConspEmoLLM-v2, an enhanced version of\nConspEmoLLM. Experimental results demonstrate that ConspEmoLLM-v2 retains or\nexceeds the performance of ConspEmoLLM on the original human-authored content\nin ConDID, and considerably outperforms both ConspEmoLLM and several other\nbaselines when applied to sentiment-transformed tweets in ConDID-v2. The\nproject will be available at https://github.com/lzw108/ConspEmoLLM.", "comment": "work in progress", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.14917v1;http://arxiv.org/pdf/2505.14917v1", "pdf_url": "http://arxiv.org/pdf/2505.14917v1"}, {"title": "Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications", "link": "https://arxiv.org/pdf/2505.14918", "details": "FM Megahed, YJ Chen, LA Jones-Farmer, Y Lee\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This study introduces a framework for evaluating consistency in large language model (LLM) binary text classification, addressing the lack of established reliability assessment methods. Adapting psychometric principles, we determine sample size \u2026", "entry_id": "http://arxiv.org/abs/2505.14918v1", "updated": "2025-05-20 21:12:58", "published": "2025-05-20 21:12:58", "authors": "Fadel M. Megahed;Ying-Ju Chen;L. Allision Jones-Farmer;Younghwa Lee;Jiawei Brooke Wang;Inez M. Zwetsloot", "summary": "This study introduces a framework for evaluating consistency in large\nlanguage model (LLM) binary text classification, addressing the lack of\nestablished reliability assessment methods. Adapting psychometric principles,\nwe determine sample size requirements, develop metrics for invalid responses,\nand evaluate intra- and inter-rater reliability. Our case study examines\nfinancial news sentiment classification across 14 LLMs (including\nclaude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and\ncommand-r-plus), with five replicates per model on 1,350 articles. Models\ndemonstrated high intra-rater consistency, achieving perfect agreement on\n90-98% of examples, with minimal differences between expensive and economical\nmodels from the same families. When validated against StockNewsAPI labels,\nmodels achieved strong performance (accuracy 0.76-0.88), with smaller models\nlike gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger\ncounterparts. All models performed at chance when predicting actual market\nmovements, indicating task constraints rather than model limitations. Our\nframework provides systematic guidance for LLM selection, sample size planning,\nand reliability assessment, enabling organizations to optimize resources for\nclassification tasks.", "comment": "25 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG;stat.ML", "links": "http://arxiv.org/abs/2505.14918v1;http://arxiv.org/pdf/2505.14918v1", "pdf_url": "http://arxiv.org/pdf/2505.14918v1"}]
