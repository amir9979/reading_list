[{"title": "uMedSum: A Unified Framework for Advancing Medical Abstractive Summarization", "link": "https://arxiv.org/pdf/2408.12095", "details": "A Nagar, Y Liu, AT Liu, V Schlegel, VP Dwivedi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical abstractive summarization faces the challenge of balancing faithfulness and informativeness. Current methods often sacrifice key information for faithfulness or introduce confabulations when prioritizing informativeness. While recent \u2026"}, {"title": "XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models", "link": "https://aclanthology.org/2024.bionlp-1.35.pdf", "details": "OC Thawakar, AM Shaker, SS Mullappilly, H Cholakkal\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "The latest breakthroughs in large language models (LLMs) and vision-language models (VLMs) have showcased promising capabilities toward performing a wide range of tasks. Such models are typically trained on massive datasets comprising \u2026"}, {"title": "Advancing Diabetic Macular Edema Detection from 3D Optical Coherence Tomography Scans: Integrating Privacy-Preserving AI and Generalizability Techniques\u2014A \u2026", "link": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400091", "details": "TX Nguyen, M Jiang, D Yang, AR Ran, Z Tang\u2026 - NEJM AI, 2024", "abstract": "Background Diabetic macular edema (DME) is the primary cause of irreversible vision loss among people with diabetes and can be accurately detected by using optical coherence tomography (OCT). We developed and validated a deep learning \u2026"}, {"title": "TRRG: Towards Truthful Radiology Report Generation With Cross-modal Disease Clue Enhanced Large Language Model", "link": "https://arxiv.org/pdf/2408.12141", "details": "Y Wang, C Hao, Y Cui, X Su, W Xie, T Tan, Z Yu - arXiv preprint arXiv:2408.12141, 2024", "abstract": "The vision-language modeling capability of multi-modal large language models has attracted wide attention from the community. However, in medical domain, radiology report generation using vision-language models still faces significant challenges due \u2026"}, {"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "link": "https://arxiv.org/pdf/2408.12337", "details": "KS Phogat, SA Puranam, S Dasaratha, C Harsha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain \u2026"}, {"title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios", "link": "https://aclanthology.org/2024.findings-acl.230.pdf", "details": "L Lin, J Fu, P Liu, Q Li, Y Gong, J Wan, F Zhang\u2026 - Findings of the Association \u2026, 2024", "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local \u2026"}]
