[{"title": "Reasoning3D--Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models", "link": "https://arxiv.org/pdf/2405.19326", "details": "T Chen, C Yu, J Li, J Zhang, L Zhu, D Ji, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we introduce a new task: Zero-Shot 3D Reasoning Segmentation for parts searching and localization for objects, which is a new paradigm to 3D segmentation that transcends limitations for previous category-specific 3D semantic \u2026"}, {"title": "TopViewRS: Vision-Language Models as Top-View Spatial Reasoners", "link": "https://arxiv.org/pdf/2406.02537", "details": "C Li, C Zhang, H Zhou, N Collier, A Korhonen, I Vuli\u0107 - arXiv preprint arXiv \u2026, 2024", "abstract": "Top-view perspective denotes a typical way in which humans read and reason over different types of maps, and it is vital for localization and navigation of humans as well as ofnon-human'agents, such as the ones backed by large Vision-Language \u2026"}, {"title": "Investigating Compositional Challenges in Vision-Language Models for Visual Grounding", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_Investigating_Compositional_Challenges_in_Vision-Language_Models_for_Visual_Grounding_CVPR_2024_paper.pdf", "details": "Y Zeng, Y Huang, J Zhang, Z Jie, Z Chai, L Wang - \u2026 of the IEEE/CVF Conference on \u2026, 2024", "abstract": "Pre-trained vision-language models (VLMs) have achieved high performance on various downstream tasks which have been widely used for visual grounding tasks in a weakly supervised manner. However despite the performance gains contributed by \u2026"}, {"title": "GraphCoder: Enhancing Repository-Level Code Completion via Code Context Graph-based Retrieval and Language Model", "link": "https://arxiv.org/pdf/2406.07003", "details": "W Liu, A Yu, D Zan, B Shen, W Zhang, H Zhao, Z Jin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of repository-level code completion depends upon the effective leverage of both general and repository-specific knowledge. Despite the impressive capability of code LLMs in general code completion tasks, they often exhibit less \u2026"}, {"title": "3D Snapshot: Invertible Embedding of 3D Neural Representations in a Single Image", "link": "https://ieeexplore.ieee.org/abstract/document/10552101/", "details": "Y Lu, B Deng, Z Zhong, T Zhang, Y Quan, H Cai, S He - IEEE Transactions on Pattern \u2026, 2024", "abstract": "3D neural rendering enables photo-realistic reconstruction of a specific scene by encoding discontinuous inputs into a neural representation. Despite the remarkable rendering results, the storage of network parameters is not transmission-friendly and \u2026"}, {"title": "Fourier Priors-Guided Diffusion for Zero-Shot Joint Low-Light Enhancement and Deblurring", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Lv_Fourier_Priors-Guided_Diffusion_for_Zero-Shot_Joint_Low-Light_Enhancement_and_Deblurring_CVPR_2024_paper.pdf", "details": "X Lv, S Zhang, C Wang, Y Zheng, B Zhong, C Li, L Nie - Proceedings of the IEEE/CVF \u2026, 2024", "abstract": "Existing joint low-light enhancement and deblurring methods learn pixel-wise mappings from paired synthetic data which results in limited generalization in real- world scenes. While some studies explore the rich generative prior of pre-trained \u2026"}, {"title": "MMA: Multi-Modal Adapter for Vision-Language Models", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_MMA_Multi-Modal_Adapter_for_Vision-Language_Models_CVPR_2024_paper.pdf", "details": "L Yang, RY Zhang, Y Wang, X Xie - Proceedings of the IEEE/CVF Conference on \u2026, 2024", "abstract": "Abstract Pre-trained Vision-Language Models (VLMs) have served as excellent foundation models for transfer learning in diverse downstream tasks. However tuning VLMs for few-shot generalization tasks faces a discrimination--generalization \u2026"}, {"title": "JoAPR: Cleaning the Lens of Prompt Learning for Vision-Language Models", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_JoAPR_Cleaning_the_Lens_of_Prompt_Learning_for_Vision-Language_Models_CVPR_2024_paper.pdf", "details": "Y Guo, X Gu - Proceedings of the IEEE/CVF Conference on Computer \u2026, 2024", "abstract": "Leveraging few-shot datasets in prompt learning for Vision-Language Models eliminates the need for manual prompt engineering while highlighting the necessity of accurate annotations for the labels. However high-level or complex label noise \u2026"}, {"title": "SocialCounterfactuals: Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Howard_SocialCounterfactuals_Probing_and_Mitigating_Intersectional_Social_Biases_in_Vision-Language_Models_CVPR_2024_paper.pdf", "details": "P Howard, A Madasu, T Le, GL Moreno\u2026 - Proceedings of the IEEE \u2026, 2024", "abstract": "While vision-language models (VLMs) have achieved remarkable performance improvements recently there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior \u2026"}]
