[{"title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models", "link": "https://arxiv.org/pdf/2410.18252", "details": "M Noukhovitch, S Huang, S Xhonneux, A Hosseini\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this \u2026"}, {"title": "Task-Aware Harmony Multi-Task Decision Transformer for Offline Reinforcement Learning", "link": "https://arxiv.org/pdf/2411.01146", "details": "Z Fan, S Hu, Y Zhou, L Shen, Y Zhang, Y Wang, D Tao - arXiv preprint arXiv \u2026, 2024", "abstract": "The purpose of offline multi-task reinforcement learning (MTRL) is to develop a unified policy applicable to diverse tasks without the need for online environmental interaction. Recent advancements approach this through sequence modeling \u2026"}, {"title": "Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in Federated Fine-Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2410.23111", "details": "N Mahla, G Ramakrishnan - arXiv preprint arXiv:2410.23111, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, particularly in task generalization for both text and vision data. While fine-tuning these models can significantly enhance their performance on \u2026"}, {"title": "ImageNet-RIB Benchmark: Large Pre-Training Datasets Don't Guarantee Robustness after Fine-Tuning", "link": "https://arxiv.org/pdf/2410.21582", "details": "J Hwang, B Cheung, ZW Hong, A Boopathy, P Agrawal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Highly performant large-scale pre-trained models promise to also provide a valuable foundation for learning specialized tasks, by fine-tuning the model to the desired task. By starting from a good general-purpose model, the goal is to achieve both \u2026"}, {"title": "SeRA: Self-Reviewing and Alignment of Large Language Models using Implicit Reward Margins", "link": "https://arxiv.org/pdf/2410.09362", "details": "J Ko, S Dingliwal, B Ganesh, S Sengupta, S Bodapati\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Direct alignment algorithms (DAAs), such as direct preference optimization (DPO), have become popular alternatives for Reinforcement Learning from Human Feedback (RLHF) due to their simplicity, efficiency, and stability. However, the \u2026"}, {"title": "Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy", "link": "https://arxiv.org/pdf/2410.17234", "details": "BA Tjandra, M Razzak, J Kossen, K Handa, Y Gal - arXiv preprint arXiv:2410.17234, 2024", "abstract": "Large Language Models (LLMs) are known to hallucinate, whereby they generate plausible but inaccurate text. This phenomenon poses significant risks in critical applications, such as medicine or law, necessitating robust hallucination mitigation \u2026"}, {"title": "Enhancing Language Model Reasoning via Weighted Reasoning in Self-Consistency", "link": "https://arxiv.org/pdf/2410.07839", "details": "T Knappe, R Li, A Chauhan, K Chhua, K Zhu, S O'Brien - arXiv preprint arXiv \u2026, 2024", "abstract": "While large language models (LLMs) have rapidly improved their performance on a broad number of tasks, they still often fall short on reasoning tasks. As LLMs become more integrated in diverse real-world tasks, advancing their reasoning capabilities is \u2026"}, {"title": "Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM Safety Alignment", "link": "https://arxiv.org/pdf/2411.02785", "details": "J Vega, J Huang, G Zhang, H Kang, M Zhang, G Singh - arXiv preprint arXiv \u2026, 2024", "abstract": "Safety alignment of Large Language Models (LLMs) has recently become a critical objective of model developers. In response, a growing body of work has been investigating how safety alignment can be bypassed through various jailbreaking \u2026"}, {"title": "Self-supervised Hierarchical Representation for Medication Recommendation", "link": "https://arxiv.org/pdf/2411.03143", "details": "Y Liang, Y Liu, Y Dang, E Yang, G Guo, W Cai, J Zhao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medication recommender is to suggest appropriate medication combinations based on a patient's health history, eg, diagnoses and procedures. Existing works represent different diagnoses/procedures well separated by one-hot encodings. However, they \u2026"}]
