[{"title": "Does Knowledge Localization Hold True? Surprising Differences Between Entity and Relation Perspectives in Language Models", "link": "https://arxiv.org/pdf/2409.00617", "details": "Y Wei, X Yu, Y Weng, H Ma, Y Zhang, J Zhao, K Liu - arXiv preprint arXiv:2409.00617, 2024", "abstract": "Large language models encapsulate knowledge and have demonstrated superior performance on various natural language processing tasks. Recent studies have localized this knowledge to specific model parameters, such as the MLP weights in \u2026"}, {"title": "Reliable machine learning models in genomic medicine using conformal prediction", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/09/10/2024.09.09.24312995.full.pdf", "details": "C Papangelou, K Kyriakidis, P Natsiavas, I Chouvarda\u2026 - medRxiv, 2024", "abstract": "Machine learning and genomic medicine are the mainstays of research in delivering personalized healthcare services for disease diagnosis, risk stratification, tailored treatment, and prediction of adverse effects. However, potential prediction errors in \u2026"}, {"title": "LLM Comparator: Interactive Analysis of Side-by-Side Evaluation of Large Language Models", "link": "https://ieeexplore.ieee.org/iel8/2945/4359476/10670495.pdf", "details": "M Kahng, I Tenney, M Pushkarna, MX Liu, J Wexler\u2026 - IEEE Transactions on \u2026, 2024", "abstract": "Evaluating large language models (LLMs) presents unique challenges. While automatic side-by-side evaluation, also known as LLM-as-a-judge, has become a promising solution, model developers and researchers face difficulties with \u2026"}, {"title": "Masked Channel Modeling for Bootstrapping Visual Pre-training", "link": "https://link.springer.com/article/10.1007/s11263-024-02204-6", "details": "Y Liu, X Wang, M Zhu, Y Cao, T Huang, C Shen - International Journal of Computer \u2026, 2024", "abstract": "Large vision models have achieved great success in computer vision recently, eg, CLIP for large-scale image-text contrastive learning. They have prominent potential in representation learning and show strong transfer ability in various downstream \u2026"}]
