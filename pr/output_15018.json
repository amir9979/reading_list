[{"title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability", "link": "https://arxiv.org/pdf/2504.06260", "details": "N Mudur, H Cui, S Venugopalan, P Raccuglia\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science. We present FEABench, a benchmark to evaluate the ability of large \u2026"}, {"title": "Multi-Sense Embeddings for Language Models and Knowledge Distillation", "link": "https://arxiv.org/pdf/2504.06036", "details": "Q Wang, MJ Zaki, G Kollias, V Kalantzis - arXiv preprint arXiv:2504.06036, 2025", "abstract": "Transformer-based large language models (LLMs) rely on contextual embeddings which generate different (continuous) representations for the same token depending on its surrounding context. Nonetheless, words and tokens typically have a limited \u2026"}, {"title": "Predicting Explainable Dementia Types with LLM-aided Feature Engineering", "link": "https://academic.oup.com/bioinformatics/advance-article-pdf/doi/10.1093/bioinformatics/btaf156/62892640/btaf156.pdf", "details": "AM Kashyap, D Rao, MR Boland, L Shen\u2026 - Bioinformatics, 2025", "abstract": "Abstract Motivation The integration of Machine Learning (ML) and Artificial Intelligence (AI) into healthcare has immense potential due to the rapidly growing volume of clinical data. However, existing AI models, particularly Large Language \u2026"}, {"title": "MDAL: Modality-difference-based active learning for multimodal medical image analysis via contrastive learning and pointwise mutual information", "link": "https://www.sciencedirect.com/science/article/pii/S0895611125000539", "details": "H Wang, Q Jin, X Du, L Wang, Q Guo, H Li, M Wang\u2026 - \u2026 Medical Imaging and \u2026, 2025", "abstract": "Multimodal medical images reveal different characteristics of the same anatomy or lesion, offering significant clinical value. Deep learning has achieved widespread success in medical image analysis with large-scale labeled datasets. However \u2026"}, {"title": "MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2504.05782", "details": "P Zhou, F Zhang, X Peng, Z Xu, J Ai, Y Qiu, C Li, Z Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal reasoning, which integrates language and visual cues into problem solving and decision making, is a fundamental aspect of human intelligence and a crucial step toward artificial general intelligence. However, the evaluation of \u2026"}, {"title": "V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2504.06148", "details": "X Zheng, L Li, Z Yang, P Yu, AJ Wang, R Yan, Y Yao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current \u2026"}, {"title": "Metascale: Test-time scaling with evolving meta-thoughts", "link": "https://arxiv.org/pdf/2503.13447", "details": "Q Liu, W Zhou, N Xu, JY Huang, F Wang, S Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "One critical challenge for large language models (LLMs) for making complex reasoning is their reliance on matching reasoning patterns from training data, instead of proactively selecting the most appropriate cognitive strategy to solve a given task \u2026"}, {"title": "LLM $\\times $ MapReduce-V2: Entropy-Driven Convolutional Test-Time Scaling for Generating Long-Form Articles from Extremely Long Resources", "link": "https://arxiv.org/pdf/2504.05732", "details": "H Wang, Y Fu, Z Zhang, S Wang, Z Ren, X Wang, Z Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Long-form generation is crucial for a wide range of practical applications, typically categorized into short-to-long and long-to-long generation. While short-to-long generations have received considerable attention, generating long texts from \u2026"}]
