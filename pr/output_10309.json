[{"title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators", "link": "https://arxiv.org/pdf/2412.02467%3F", "details": "T Afonja, HP Wang, R Kerkouche, M Fritz - arXiv preprint arXiv:2412.02467, 2024", "abstract": "Generating tabular data under differential privacy (DP) protection ensures theoretical privacy guarantees but poses challenges for training machine learning models, primarily due to the need to capture complex structures under noisy supervision \u2026"}, {"title": "Detecting critical diseases associated with higher mortality in electronic health records using a hybrid attention-based transformer", "link": "https://www.sciencedirect.com/science/article/pii/S0952197624018074", "details": "D Kodati, CM Dasari - Engineering Applications of Artificial Intelligence, 2025", "abstract": "Electronic health records (EHRs) are crucial for modern medical practices, providing digital storage of patient health information. However, accurately identifying diseases that lead to mortality within EHRs remains challenging. This study introduces a novel \u2026"}, {"title": "CNNSum: Exploring Long-Conext Summarization with Large Language Models in Chinese Novels", "link": "https://arxiv.org/pdf/2412.02819", "details": "L Wei, H Yan, X Lu, J Zhu, J Wang, W Zhang - arXiv preprint arXiv:2412.02819, 2024", "abstract": "Large Language Models (LLMs) have been well-researched in many long-context tasks. However, due to high annotation costs, high-quality long-context summary datasets for training or evaluation are scarce, limiting further research. In this work \u2026"}, {"title": "The Vulnerability of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?", "link": "https://arxiv.org/pdf/2412.03597%3F", "details": "S Banerjee, A Agarwal, E Singh - arXiv preprint arXiv:2412.03597, 2024", "abstract": "The pursuit of leaderboard rankings in Large Language Models (LLMs) has created a fundamental paradox: models excel at standardized tests while failing to demonstrate genuine language understanding and adaptability. Our systematic \u2026"}]
