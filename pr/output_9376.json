[{"title": "metaTextGrad: Learning to learn with language models as optimizers", "link": "https://openreview.net/forum%3Fid%3DyzieYIT9hu", "details": "G Xu, M Yuksekgonul, C Guestrin, J Zou - Adaptive Foundation Models: Evolving AI for \u2026", "abstract": "Large language models (LLMs) are increasingly used in learning algorithms, evaluations, and optimization tasks. Recent studies have shown that incorporating self-criticism into LLMs can significantly enhance model performance, with \u2026"}, {"title": "Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models", "link": "https://arxiv.org/pdf/2411.00492%3F", "details": "DX Long, DN Yen, AT Luu, K Kawaguchi, MY Kan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple \u2026"}, {"title": "Skills-in-Context: Unlocking Compositionality in Large Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.812.pdf", "details": "J Chen, X Pan, D Yu, K Song, X Wang, D Yu, J Chen - Findings of the Association for \u2026, 2024", "abstract": "We investigate how to elicit compositional generalization capabilities in large language models (LLMs). Compositional generalization empowers LLMs to solve complex problems by combining foundational skills, a critical reasoning ability akin to \u2026"}]
