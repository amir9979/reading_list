[{"title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?", "link": "https://arxiv.org/pdf/2410.23856", "details": "Z Zhou, R Tao, J Zhu, Y Luo, Z Wang, B Han - arXiv preprint arXiv:2410.23856, 2024", "abstract": "This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We \u2026"}, {"title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines", "link": "https://arxiv.org/pdf/2410.21220", "details": "Z Zhang, Y Zhang, X Ding, X Yue - arXiv preprint arXiv:2410.21220, 2024", "abstract": "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This \u2026"}, {"title": "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging", "link": "https://arxiv.org/pdf/2410.15661", "details": "C Na, I Magnusson, AH Jha, T Sherborne, E Strubell\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Training data compositions for Large Language Models (LLMs) can significantly affect their downstream performance. However, a thorough data ablation study exploring large sets of candidate data mixtures is typically prohibitively expensive \u2026"}, {"title": "Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass", "link": "https://arxiv.org/pdf/2411.05877", "details": "T Chen, H Fang, P Xia, X Liu, B Van Durme\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LMs) are typically adapted to improve performance on new contexts (\\eg text prompts that define new tasks or domains) through fine-tuning or prompting. However, there is an accuracy compute tradeoff--fine-tuning incurs \u2026"}, {"title": "Leveraging explainable artificial intelligence for early prediction of bloodstream infections using historical electronic health records", "link": "https://journals.plos.org/digitalhealth/article%3Fid%3D10.1371/journal.pdig.0000506", "details": "R Bopche, LT Gustad, JE Afset, B Ehrnstr\u00f6m\u2026 - PLOS Digital Health, 2024", "abstract": "Bloodstream infections (BSIs) are a severe public health threat due to their rapid progression into critical conditions like sepsis. This study presents a novel eXplainable Artificial Intelligence (XAI) framework to predict BSIs using historical \u2026"}, {"title": "Learning by Aligning 2D Skeleton Sequences and Multi-modality Fusion", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-72973-7_9.pdf", "details": "MH Ahmed, A Konin, MZ Zia", "abstract": "This paper presents a self-supervised temporal video alignment framework which is useful for several fine-grained human activity understanding applications. In contrast with the state-of-the-art method of CASA, where sequences of 3D skeleton \u2026"}, {"title": "Computational Ethnography: Automated and Unobtrusive Means for Collecting", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DjewwEQAAQBAJ%26oi%3Dfnd%26pg%3DPA121%26ots%3DYyh7_Y8WsW%26sig%3DeFDQMQ0X1tVxMQJm61WmM5DGGjg", "details": "K Zheng, DA Hanauer, N Weibel, Z Agha - Human Computer Interaction in Healthcare: The \u2026", "abstract": "Health information technology (IT) holds great promise to cross the quality chasm of the US healthcare system and to bend the curve of ever-rising costs. However, many successfully deployed health IT systems have failed to generate anticipated benefits \u2026"}, {"title": "A Survey of Small Language Models", "link": "https://arxiv.org/pdf/2410.20011", "details": "C Van Nguyen, X Shen, R Aponte, Y Xia, S Basu, Z Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Small Language Models (SLMs) have become increasingly important due to their efficiency and performance to perform various language tasks with minimal computational resources, making them ideal for various settings including on-device \u2026"}, {"title": "Optimizing Fine-Tuning in Quantized Language Models: An In-Depth Analysis of Key Variables", "link": "https://cdn.techscience.cn/files/cmc/2024/online/CMC1030/TSP_CMC_57491/TSP_CMC_57491.pdf", "details": "A Shen, Z Lai, D Li, X Hu - 2024", "abstract": "ABSTRACT Large-scale Language Models (LLMs) have achieved significant breakthroughs in Natural Language Processing (NLP), driven by the pre-training and fine-tuning paradigm. While this approach allows models to specialize in specific \u2026"}]
