'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Pre-training enhanced unsupervised contrastive domain adapta'
[{"title": "Thorax computed tomography (CTX) guided ground truth annotation of CHEST radiographs (CXR) for improved classification and detection of COVID\u201019", "link": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cnm.3823", "details": "\u015eM Ert\u00fcrk, T Toprak, RG C\u00f6mert, C Candemir\u2026 - International Journal for \u2026, 2024", "abstract": "Several data sets have been collected and various artificial intelligence models have been developed for COVID\u201019 classification and detection from both chest radiography (CXR) and thorax computed tomography (CTX) images. However, the \u2026"}, {"title": "Source-Aware Training Enables Knowledge Attribution in Language Models", "link": "https://arxiv.org/pdf/2404.01019", "details": "M Khalifa, D Wadden, E Strubell, H Lee, L Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source (s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining \u2026"}, {"title": "Emergent Abilities in Reduced-Scale Generative Language Models", "link": "https://arxiv.org/pdf/2404.02204", "details": "S Muckatira, V Deshpande, V Lialin, A Rumshisky - arXiv preprint arXiv:2404.02204, 2024", "abstract": "Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study \u2026"}, {"title": "Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4", "link": "https://arxiv.org/pdf/2404.00484", "details": "AP Gema, G Hong, P Minervini, L Daines, B Alex - arXiv preprint arXiv:2404.00484, 2024", "abstract": "The NLI4CT task assesses Natural Language Inference systems in predicting whether hypotheses entail or contradict evidence from Clinical Trial Reports. In this study, we evaluate various Large Language Models (LLMs) with multiple strategies \u2026"}, {"title": "Data Augmentation and Large Language Model for Legal Case Retrieval and Entailment", "link": "https://link.springer.com/article/10.1007/s12626-024-00158-2", "details": "MQ Bui, DT Do, NK Le, DH Nguyen, KVH Nguyen\u2026 - The Review of Socionetwork \u2026, 2024", "abstract": "Abstract The Competition on Legal Information Extraction and Entailment (COLIEE) is a well-known international competition organized each year with the goal of applying machine learning algorithms and techniques in the analysis and \u2026"}]
