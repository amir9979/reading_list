[{"title": "Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts", "link": "https://arxiv.org/pdf/2504.17921", "details": "ME Zarlenga, G Dominici, P Barbiero, Z Shams\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this paper, we investigate how concept-based models (CMs) respond to out-of- distribution (OOD) inputs. CMs are interpretable neural architectures that first predict a set of high-level concepts (eg, stripes, black) and then predict a task label from \u2026"}, {"title": "Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization", "link": "https://arxiv.org/pdf/2504.18026", "details": "E Penaloza, TH Zhan, L Charlin, ME Zarlenga - arXiv preprint arXiv:2504.18026, 2025", "abstract": "Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI systems by constraining their decisions on a set of human understandable concepts. However, CBMs typically assume that datasets contains accurate concept labels an \u2026"}, {"title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning", "link": "https://arxiv.org/pdf/2504.19627", "details": "R Luo, R Shan, L Chen, Z Liu, L Wang, M Yang, X Xia - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like embodied intelligence due to their strong vision-language reasoning abilities. However, current LVLMs process entire images at the token level, which is inefficient \u2026"}, {"title": "Contextures: The Mechanism of Representation Learning", "link": "https://arxiv.org/pdf/2504.19792", "details": "R Zhai - arXiv preprint arXiv:2504.19792, 2025", "abstract": "This dissertation establishes the contexture theory to mathematically characterize the mechanism of representation learning, or pretraining. Despite the remarkable empirical success of foundation models, it is not very clear what representations they \u2026"}, {"title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video", "link": "https://arxiv.org/pdf/2504.19475", "details": "S Joseph, P Suresh, L Hufe, E Stevinson, R Graham\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Robust tooling and publicly available pre-trained models have helped drive recent advances in mechanistic interpretability for language models. However, similar progress in vision mechanistic interpretability has been hindered by the lack of \u2026"}, {"title": "Debiasing Guidance for Discrete Diffusion with Sequential Monte Carlo", "link": "https://openreview.net/pdf%3Fid%3Dvg0dOu3w9g", "details": "LC Kit, P Jeha, J Frellsen, P Lio, MS Albergo, F Vargas - Frontiers in Probabilistic Inference \u2026", "abstract": "Discrete diffusion models are a class of generative models that produce samples from an approximated data distribution within a discrete state space. Often, there is a need to target specific regions of the data distribution. Current guidance methods aim \u2026"}]
