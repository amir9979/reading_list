[{"title": "RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment", "link": "https://arxiv.org/pdf/2501.07525", "details": "D Gu, Y Gao, Y Zhou, M Zhou, D Metaxas - arXiv preprint arXiv:2501.07525, 2025", "abstract": "Automated chest radiographs interpretation requires both accurate disease classification and detailed radiology report generation, presenting a significant challenge in the clinical workflow. Current approaches either focus on classification \u2026"}, {"title": "DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision Language Models in Real-World Scenarios with Driving Theory Tests", "link": "https://arxiv.org/pdf/2501.04671", "details": "C Corbi\u00e8re, S Roburin, S Montariol, A Bosselut, A Alahi - arXiv preprint arXiv \u2026, 2025", "abstract": "Large vision-language models (LVLMs) augment language models with visual understanding, enabling multimodal reasoning. However, due to the modality gap between textual and visual data, they often face significant challenges, such as over \u2026"}, {"title": "A generalist medical language model for disease diagnosis assistance", "link": "https://www.nature.com/articles/s41591-024-03416-6", "details": "X Liu, H Liu, G Yang, Z Jiang, S Cui, Z Zhang, H Wang\u2026 - Nature Medicine, 2025", "abstract": "The delivery of accurate diagnoses is crucial in healthcare and represents the gateway to appropriate and timely treatment. Although recent large language models (LLMs) have demonstrated impressive capabilities in few-shot or zero-shot learning \u2026"}, {"title": "Self-supervised analogical learning using language models", "link": "https://arxiv.org/pdf/2502.00996", "details": "B Zhou, S Jain, Y Zhang, Q Ning, S Wang, Y Benajiba\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can \u2026"}, {"title": "Prompt-Agent-Driven Integration of Foundation Model Priors for Low-Count PET Reconstruction", "link": "https://ieeexplore.ieee.org/abstract/document/10833823/", "details": "X Xie, W Zhao, M Nan, Z Zhang, Y Wu, H Zheng\u2026 - IEEE Transactions on \u2026, 2025", "abstract": "Low-count Positron Emission Tomography reconstruction is critical for maintaining high imaging quality while minimizing tracer doses and radiation exposure. Although integrating structural information from CT and MR data has been shown to enhance \u2026"}, {"title": "Scan-Specific Isotropic Reconstruction and Denoising of Multi-Plane MRI via Implicit Neural Representations and Diffusion Models", "link": "https://openreview.net/forum%3Fid%3DEV0jgjnRti", "details": "D Kocanaogullari, C Ariyurek, S Kurugol, O Afacan - Medical Imaging with Deep Learning", "abstract": "Magnetic resonance imaging (MRI) protocols often acquire anisotropic scans along multiple orthogonal planes to balance image quality, coverage, and acquisition time. However, these anisotropic acquisitions complicate downstream analyses and may \u2026"}, {"title": "Self-supervised motion-corrected reconstruction for single heartbeat cardiac cine MRI using neural fields and deep image prior", "link": "https://www.journalofcmr.com/article/S1097-6647\\(24\\)01546-1/fulltext", "details": "T Catal\u00e1n, R Botnar, F Sahli, C Prieto - Journal of Cardiovascular Magnetic \u2026, 2025", "abstract": "Background: 2D cardiac cine is the gold standard for cardiac functional assessment. Recent works have successfully incorporated cardiac motion correction (MC) in the reconstruction, demonstrating the feasibility of single heartbeat imaging for both \u2026"}, {"title": "Mordal: Automated Pretrained Model Selection for Vision Language Models", "link": "https://arxiv.org/pdf/2502.00241", "details": "S He, I Jang, M Chowdhury - arXiv preprint arXiv:2502.00241, 2025", "abstract": "Incorporating multiple modalities into large language models (LLMs) is a powerful way to enhance their understanding of non-textual data, enabling them to perform multimodal tasks. Vision language models (VLMs) form the fastest growing category \u2026"}, {"title": "Vision Language Models as Values Detectors", "link": "https://arxiv.org/pdf/2501.03957", "details": "GA Abbo, T Belpaeme - arXiv preprint arXiv:2501.03957, 2025", "abstract": "Large Language Models integrating textual and visual inputs have introduced new possibilities for interpreting complex data. Despite their remarkable ability to generate coherent and contextually relevant text based on visual stimuli, the \u2026"}]
