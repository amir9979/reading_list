[{"title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures", "link": "https://arxiv.org/pdf/2410.07698", "details": "Y Chen, Y Zhang, L Cao, K Yuan, Z Wen - arXiv preprint arXiv:2410.07698, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs when adapting large language models (LLMs) for downstream applications. However, traditional first-order (FO) fine-tuning algorithms incur substantial memory overhead \u2026"}, {"title": "How to Train Long-Context Language Models (Effectively)", "link": "https://arxiv.org/pdf/2410.02660%3F", "details": "T Gao, A Wettig, H Yen, D Chen - arXiv preprint arXiv:2410.02660, 2024", "abstract": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development--Instead of perplexity or simple \u2026"}, {"title": "Belief in the Machine: Investigating Epistemological Blind Spots of Language Models", "link": "https://arxiv.org/pdf/2410.21195", "details": "M Suzgun, T Gur, F Bianchi, DE Ho, T Icard, D Jurafsky\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As language models (LMs) become integral to fields like healthcare, law, and journalism, their ability to differentiate between fact, belief, and knowledge is essential for reliable decision-making. Failure to grasp these distinctions can lead to \u2026"}, {"title": "POSIX: A Prompt Sensitivity Index For Large Language Models", "link": "https://arxiv.org/pdf/2410.02185", "details": "A Chatterjee, HK Renduchintala, S Bhatia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are found to be surprisingly sensitive to minor variations in prompts, often generating significantly divergent outputs in response to minor variations in the prompts, such as spelling \u2026"}, {"title": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement", "link": "https://arxiv.org/pdf/2410.02108", "details": "X Peng, C Xia, X Yang, C Xiong, CS Wu, C Xing - arXiv preprint arXiv:2410.02108, 2024", "abstract": "Post-training Large Language Models (LLMs) with explicit reasoning trajectories can enhance their reasoning abilities. However, acquiring such high-quality trajectory data typically demands meticulous supervision from humans or superior models \u2026"}, {"title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style", "link": "https://arxiv.org/pdf/2410.16184%3F", "details": "Y Liu, Z Yao, R Min, Y Cao, L Hou, J Li - arXiv preprint arXiv:2410.16184, 2024", "abstract": "Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward \u2026"}, {"title": "Beyond Graphs: Can Large Language Models Comprehend Hypergraphs?", "link": "https://arxiv.org/pdf/2410.10083", "details": "Y Feng, C Yang, X Hou, S Du, S Ying, Z Wu, Y Gao - arXiv preprint arXiv:2410.10083, 2024", "abstract": "Existing benchmarks like NLGraph and GraphQA evaluate LLMs on graphs by focusing mainly on pairwise relationships, overlooking the high-order correlations found in real-world data. Hypergraphs, which can model complex beyond-pairwise \u2026"}, {"title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs", "link": "https://arxiv.org/pdf/2410.12881%3F", "details": "SN Akter, S Prabhumoye, J Kamalu, S Satheesh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models (LLMs). Yet, these approaches fall inadequate in complex, multi-hop and \u2026"}, {"title": "Understanding Reasoning in Chain-of-Thought from the Hopfieldian View", "link": "https://arxiv.org/pdf/2410.03595", "details": "L Hu, L Liu, S Yang, X Chen, Z Tan, MA Ali, M Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models have demonstrated remarkable abilities across various tasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to enhance reasoning capabilities. However, existing research primarily focuses on \u2026"}]
