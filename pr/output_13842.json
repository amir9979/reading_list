[{"title": "GPIoT: Tailoring Small Language Models for IoT Program Synthesis and Development", "link": "https://arxiv.org/pdf/2503.00686", "details": "L Shen, Q Yang, X Huang, Z Ma, Y Zheng - arXiv preprint arXiv:2503.00686, 2025", "abstract": "Code Large Language Models (LLMs) enhance software development efficiency by automatically generating code and documentation in response to user requirements. However, code LLMs cannot synthesize specialized programs when tasked with IoT \u2026"}, {"title": "TINY LongProLIP: A Probabilistic Vision-Language Model with Long Context Text", "link": "https://openreview.net/pdf%3Fid%3DjfwxBVBCiD", "details": "S Chun, S Yun - \u2026 Workshop: Quantify Uncertainty and Hallucination in \u2026", "abstract": "Recently, Probabilistic Language-Image Pre-Training (ProLIP) has been proposed to tackle the multiplicity issue of vision-language (VL) tasks. Despite their success in probabilistic representation learning at a scale, the ProLIP models cannot handle \u2026"}, {"title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning", "link": "https://arxiv.org/pdf/2502.18080", "details": "W Yang, S Ma, Y Lin, F Wei - arXiv preprint arXiv:2502.18080, 2025", "abstract": "Recent studies have shown that making a model spend more time thinking through longer Chain of Thoughts (CoTs) enables it to gain significant improvements in complex reasoning tasks. While current researches continue to explore the benefits \u2026"}, {"title": "A Shared Encoder Approach to Multimodal Representation Learning", "link": "https://arxiv.org/pdf/2503.01654", "details": "S Roy, F Ogidi, A Etemad, E Dolatabadi, A Afkanpour - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal representation learning has demonstrated remarkable potential in enabling models to process and integrate diverse data modalities, such as text and images, for improved understanding and performance. While the medical domain \u2026"}, {"title": "Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety", "link": "https://arxiv.org/pdf/2503.05021", "details": "Y Zhang, M Li, W Han, Y Yao, Z Cen, D Zhao - arXiv preprint arXiv:2503.05021, 2025", "abstract": "Large Language Models (LLMs) are vulnerable to jailbreak attacks that exploit weaknesses in traditional safety alignment, which often relies on rigid refusal heuristics or representation engineering to block harmful outputs. While they are \u2026"}, {"title": "PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection", "link": "https://arxiv.org/pdf/2502.12119", "details": "J Bi, Y Wang, D Yan, X Xiao, A Hecker, V Tresp, Y Ma - arXiv preprint arXiv \u2026, 2025", "abstract": "Visual instruction tuning refines pre-trained Multimodal Large Language Models (MLLMs) to enhance their real-world task performance. However, the rapid expansion of visual instruction datasets introduces significant data redundancy \u2026"}, {"title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large Vision-Language Model Compression", "link": "https://arxiv.org/pdf/2503.04982", "details": "S Kundu, A Bhiwandiwalla, S Yu, P Howard, T Le\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite recent efforts in understanding the compression impact on large language models (LLMs) in terms of their downstream task performance and trustworthiness on relatively simpler uni-modal benchmarks (for example, question answering, common \u2026"}, {"title": "Forecasting Rare Language Model Behaviors", "link": "https://arxiv.org/pdf/2502.16797", "details": "E Jones, M Tong, J Mu, M Mahfoud, J Leike, R Grosse\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Standard language model evaluations can fail to capture risks that emerge only at deployment scale. For example, a model may produce safe responses during a small- scale beta test, yet reveal dangerous information when processing billions of \u2026"}, {"title": "Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation", "link": "https://arxiv.org/pdf/2502.15040", "details": "YW Chu, K Zhang, C Malon, MR Min - arXiv preprint arXiv:2502.15040, 2025", "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive performance in vision and text tasks. However, hallucination remains a major challenge, especially in fields like healthcare where details are critical. In this work, we show \u2026"}]
