[{"title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment", "link": "https://arxiv.org/pdf/2405.19332", "details": "S Zhang, D Yu, H Sharma, Z Yang, S Wang, H Hassan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed \u2026"}, {"title": "Why are Visually-Grounded Language Models Bad at Image Classification?", "link": "https://arxiv.org/pdf/2405.18415", "details": "Y Zhang, A Unell, X Wang, D Ghosh, Y Su, L Schmidt\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Image classification is one of the most fundamental capabilities of machine vision intelligence. In this work, we revisit the image classification task using visually- grounded language models (VLMs) such as GPT-4V and LLaVA. We find that \u2026"}, {"title": "A Systematic Analysis on the Temporal Generalization of Language Models in Social Media", "link": "https://arxiv.org/pdf/2405.13017", "details": "A Ushio, J Camacho-Collados - arXiv preprint arXiv:2405.13017, 2024", "abstract": "In machine learning, temporal shifts occur when there are differences between training and test splits in terms of time. For streaming data such as news or social media, models are commonly trained on a fixed corpus from a certain period of time \u2026"}, {"title": "Small Language Models for Application Interactions: A Case Study", "link": "https://arxiv.org/pdf/2405.20347", "details": "B Li, Y Zhang, S Bubeck, J Pathuri, I Menache - arXiv preprint arXiv:2405.20347, 2024", "abstract": "We study the efficacy of Small Language Models (SLMs) in facilitating application usage through natural language interactions. Our focus here is on a particular internal application used in Microsoft for cloud supply chain fulfilment. Our \u2026"}, {"title": "A Causal Framework for Evaluating Deferring Systems", "link": "https://arxiv.org/pdf/2405.18902", "details": "F Palomba, A Pugnana, JM Alvarez, S Ruggieri - arXiv preprint arXiv:2405.18902, 2024", "abstract": "Deferring systems extend supervised Machine Learning (ML) models with the possibility to defer predictions to human experts. However, evaluating the impact of a deferring strategy on system accuracy is still an overlooked area. This paper fills this \u2026"}, {"title": "FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2405.18218", "details": "Y Zhang, Y Li, X Wang, Q Shen, B Plank, B Bischl\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Overparametrized transformer networks are the state-of-the-art architecture for Large Language Models (LLMs). However, such models contain billions of parameters making large compute a necessity, while raising environmental concerns. To \u2026"}, {"title": "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "link": "https://arxiv.org/pdf/2405.07551", "details": "S Yin, W You, Z Ji, G Zhong, J Bai - arXiv preprint arXiv:2405.07551, 2024", "abstract": "The tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs, while tool-free methods chose another track: augmenting math \u2026"}, {"title": "Backdoor Removal for Generative Large Language Models", "link": "https://arxiv.org/pdf/2405.07667", "details": "H Li, Y Chen, Z Zheng, Q Hu, C Chan, H Liu, Y Song - arXiv preprint arXiv \u2026, 2024", "abstract": "With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased \u2026"}, {"title": "Cost-Effective LLM Utilization for Machine Learning Tasks over Tabular Data", "link": "https://dl.acm.org/doi/abs/10.1145/3665601.3669848", "details": "Y Einy, T Milo, S Novgorodov - Proceedings of the Conference on Governance \u2026, 2024", "abstract": "Classic machine learning (ML) models excel in modeling tabular datasets but lack broader world knowledge due to the absence of pre-training, an area where Large Language Models (LLMs) stand out. This paper presents an effective method that \u2026"}]
