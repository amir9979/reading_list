[{"title": "MACAROON: Training Vision-Language Models To Be Your Engaged Partners", "link": "https://arxiv.org/pdf/2406.14137", "details": "S Wu, YR Fung, S Li, Y Wan, KW Chang, H Ji - arXiv preprint arXiv:2406.14137, 2024", "abstract": "Large vision-language models (LVLMs), while proficient in following instructions and responding to diverse questions, invariably generate detailed responses even when questions are ambiguous or unanswerable, leading to hallucinations and bias \u2026"}, {"title": "Automatic uncovering of patient primary concerns in portal messages using a fusion framework of pretrained language models", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae144/7700019", "details": "Y Ren, Y Wu, JW Fan, A Khurana, S Fu, D Wu, H Liu\u2026 - Journal of the American \u2026, 2024", "abstract": "Objectives The surge in patient portal messages (PPMs) with increasing needs and workloads for efficient PPM triage in healthcare settings has spurred the exploration of AI-driven solutions to streamline the healthcare workflow processes, ensuring \u2026"}, {"title": "Using large language model to guide patients to create efficient and comprehensive clinical care message", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae142/7699038", "details": "S Liu, AP Wright, AB Mccoy, SS Huang, JZ Genkins\u2026 - Journal of the American \u2026, 2024", "abstract": "Objective This study aims to investigate the feasibility of using Large Language Models (LLMs) to engage with patients at the time they are drafting a question to their healthcare providers, and generate pertinent follow-up questions that the patient can \u2026"}, {"title": "Outlier Reduction with Gated Attention for Improved Post-training Quantization in Large Sequence-to-sequence Speech Foundation Models", "link": "https://arxiv.org/pdf/2406.11022", "details": "D Wagner, I Baumann, K Riedhammer, T Bocklet - arXiv preprint arXiv:2406.11022, 2024", "abstract": "This paper explores the improvement of post-training quantization (PTQ) after knowledge distillation in the Whisper speech foundation model family. We address the challenge of outliers in weights and activation tensors, known to impede \u2026"}]
