[{"title": "Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific Expert Pruning", "link": "https://arxiv.org/pdf/2409.01483", "details": "S Sarkar, L Lausen, V Cevher, S Zha, T Brox, G Karypis - arXiv preprint arXiv \u2026, 2024", "abstract": "Sparse Mixture of Expert (SMoE) models have emerged as a scalable alternative to dense models in language modeling. These models use conditionally activated feedforward subnetworks in transformer blocks, allowing for a separation between \u2026"}, {"title": "CARL: Unsupervised Code-Based Adversarial Attacks for Programming Language Models via Reinforcement Learning", "link": "https://dl.acm.org/doi/abs/10.1145/3688839", "details": "K Yao, H Wang, C Qin, H Zhu, Y Wu, L Zhang - ACM Transactions on Software Engineering \u2026", "abstract": "Code based adversarial attacks play a crucial role in revealing vulnerabilities of software system. Recently, pre-trained programming language models (PLMs) have demonstrated remarkable success in various significant software engineering tasks \u2026"}, {"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "link": "https://arxiv.org/pdf/2408.12337", "details": "KS Phogat, SA Puranam, S Dasaratha, C Harsha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain \u2026"}, {"title": "DP-MemArc: Differential Privacy Transfer Learning for Memory Efficient Language Models", "link": "https://www.researchgate.net/profile/Yanming-Liu-16/publication/383395255_DP-MemArc_Differential_Privacy_Transfer_Learning_for_Memory_Efficient_Language_Models/links/66ca3a35c2eaa5002314bfbf/DP-MemArc-Differential-Privacy-Transfer-Learning-for-Memory-Efficient-Language-Models.pdf", "details": "Y Liu, X Peng, Y Zhang, X Ke, S Deng, J Cao, C Ma\u2026", "abstract": "Large language models have repeatedly shown outstanding performance across diverse applications. However, deploying these models can inadvertently risk user privacy. The significant memory demands during training pose a major challenge in \u2026"}, {"title": "Enhancing Discriminative Tasks by Guiding the Pre-trained Language Model with Large Language Model's Experience", "link": "https://arxiv.org/pdf/2408.08553", "details": "X Yin, C Ni, X Xu, X Li, X Yang - arXiv preprint arXiv:2408.08553, 2024", "abstract": "Large Language Models (LLMs) and pre-trained Language Models (LMs) have achieved impressive success on many software engineering tasks (eg, code completion and code generation). By leveraging huge existing code corpora (eg \u2026"}, {"title": "LLM-GAN: Construct Generative Adversarial Network Through Large Language Models For Explainable Fake News Detection", "link": "https://arxiv.org/pdf/2409.01787", "details": "Y Wang, Z Gu, S Zhang, S Zheng, T Wang, T Li, H Feng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Explainable fake news detection predicts the authenticity of news items with annotated explanations. Today, Large Language Models (LLMs) are known for their powerful natural language understanding and explanation generation abilities \u2026"}, {"title": "DOSSIER: Fact checking in electronic health records while preserving patient privacy", "link": "https://www.amazon.science/publications/dossier-fact-checking-in-electronic-health-records-while-preserving-patient-privacy", "details": "H Zhang, S Nagesh, M Shyani, N Mishra - 2024", "abstract": "Given a particular claim about a specific document, the fact checking problem is to determine if the claim is true and, if so, provide corroborating evidence. The problem is motivated by contexts where a document is too lengthy to quickly read and find an \u2026"}, {"title": "Reinforcement Learning-Driven LLM Agent for Automated Attacks on LLMs", "link": "https://aclanthology.org/2024.privatenlp-1.17.pdf", "details": "X Wang, J Peng, K Xu, H Yao, T Chen - Proceedings of the Fifth Workshop on Privacy \u2026, 2024", "abstract": "Recently, there has been a growing focus on conducting attacks on large language models (LLMs) to assess LLMs' safety. Yet, existing attack methods face challenges, including the need to access model weights or merely ensuring LLMs output harmful \u2026"}, {"title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models", "link": "https://arxiv.org/pdf/2408.12798", "details": "Y Li, H Huang, Y Zhao, X Ma, J Sun - arXiv preprint arXiv:2408.12798, 2024", "abstract": "Generative Large Language Models (LLMs) have made significant strides across various tasks, but they remain vulnerable to backdoor attacks, where specific triggers in the prompt cause the LLM to generate adversary-desired responses. While most \u2026"}]
