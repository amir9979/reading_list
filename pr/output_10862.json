[{"title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding", "link": "https://arxiv.org/pdf/2412.10302%3F", "details": "Z Wu, X Chen, Z Pan, X Liu, W Liu, D Dai, H Gao, Y Ma\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL, through two key major upgrades. For the vision component, we \u2026"}, {"title": "Early detection of heart failure using in-patient longitudinal electronic health records", "link": "https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0314145", "details": "I Drozdov, B Szubert, C Murphy, K Brooksbank\u2026 - PloS one, 2024", "abstract": "Heart Failure (HF) is common, with worldwide prevalence of 1%-3% and a lifetime risk of 20% for individuals 40 years or older. Despite its considerable health economic burden, techniques for early detection of HF in the general population are \u2026"}, {"title": "VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models", "link": "https://arxiv.org/pdf/2412.01822", "details": "BK Lee, R Hachiuma, YCF Wang, YM Ro, YH Wu - arXiv preprint arXiv:2412.01822, 2024", "abstract": "The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve \u2026"}]
