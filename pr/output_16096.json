[{"title": "GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models", "link": "https://aclanthology.org/2025.naacl-long.420.pdf", "details": "H Kohli, S Kumar, H Sun - Proceedings of the 2025 Conference of the Nations of \u2026, 2025", "abstract": "The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their reasoning to \u2026"}, {"title": "R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation", "link": "https://arxiv.org/pdf/2505.02018", "details": "MH Guo, J Xu, Y Zhang, J Song, H Peng, YX Deng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. Despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning \u2026"}, {"title": "LLM Ethics Benchmark: A Three-Dimensional Assessment System for Evaluating Moral Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2505.00853", "details": "J Jiao, S Afroogh, A Murali, K Chen, D Atkinson\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This study establishes a novel framework for systematically evaluating the moral reasoning capabilities of large language models (LLMs) as they increasingly integrate into critical societal domains. Current assessment methodologies lack the \u2026"}, {"title": "Multi-Modal Language Models as Text-to-Image Model Evaluators", "link": "https://arxiv.org/pdf/2505.00759", "details": "J Chen, C Ross, R Askari-Hemmat, K Sinha, M Hall\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The steady improvements of text-to-image (T2I) generative models lead to slow deprecation of automatic evaluation benchmarks that rely on static datasets, motivating researchers to seek alternative ways to evaluate the T2I progress. In this \u2026"}, {"title": "Atoxia: Red-teaming Large Language Models with Target Toxic Answers", "link": "https://aclanthology.org/2025.findings-naacl.179.pdf", "details": "Y Du, Z Li, P Cheng, X Wan, A Gao - Findings of the Association for Computational \u2026, 2025", "abstract": "Despite the substantial advancements in artificial intelligence, large language models (LLMs) remain being challenged by generation safety. With adversarial jailbreaking prompts, one can effortlessly induce LLMs to output harmful content \u2026"}, {"title": "Semantic Probabilistic Control of Language Models", "link": "https://arxiv.org/pdf/2505.01954", "details": "K Ahmed, CG Belem, P Smyth, S Singh - arXiv preprint arXiv:2505.01954, 2025", "abstract": "Semantic control entails steering LM generations towards satisfying subtle non- lexical constraints, eg, toxicity, sentiment, or politeness, attributes that can be captured by a sequence-level verifier. It can thus be viewed as sampling from the LM \u2026"}, {"title": "Biases in Opinion Dynamics in Multi-Agent Systems of Large Language Models: A Case Study on Funding Allocation", "link": "https://aclanthology.org/2025.findings-naacl.101.pdf", "details": "P Cisneros-Velarde - Findings of the Association for Computational \u2026, 2025", "abstract": "We study the evolution of opinions inside a population of interacting large language models (LLMs). Every LLM needs to decide how much funding to allocate to an item with three initial possibilities: full, partial, or no funding. We identify biases that drive \u2026"}, {"title": "Analyzing and Improving Coherence of Large Language Models in Question Answering", "link": "https://aclanthology.org/2025.naacl-long.588.pdf", "details": "I Lauriola, S Campese, A Moschitti - Proceedings of the 2025 Conference of the \u2026, 2025", "abstract": "Large language models (LLMs) have recently revolutionized natural language processing. These models, however, often suffer from instability or lack of coherence, that is the ability of the models to generate semantically equivalent outputs when \u2026"}, {"title": "Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2504.21277", "details": "G Zhou, P Qiu, C Chen, J Wang, Z Yang, J Xu, M Qiu - arXiv preprint arXiv \u2026, 2025", "abstract": "The integration of reinforcement learning (RL) into the reasoning capabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as a transformative research direction. While MLLMs significantly extend Large Language \u2026"}]
