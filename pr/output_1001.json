'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Refining Pre-trained Language Models for Domain Adapta'
[{"title": "VLRM: Vision-Language Models act as Reward Models for Image Captioning", "link": "https://arxiv.org/pdf/2404.01911", "details": "M Dzabraev, A Kunitsyn, A Ivaniuta - arXiv preprint arXiv:2404.01911, 2024", "abstract": "In this work, we present an unsupervised method for enhancing an image captioning model (in our case, BLIP2) using reinforcement learning and vision-language models like CLIP and BLIP2-ITM as reward models. The RL-tuned model is able to \u2026"}, {"title": "Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation", "link": "https://arxiv.org/pdf/2404.17489", "details": "W Cui, R Hosseinzadeh, J Ma, T Wu, Y Sui, K Golestan - arXiv preprint arXiv \u2026, 2024", "abstract": "Contrastive learning is a model pre-training technique by first creating similar views of the original data, and then encouraging the data and its corresponding views to be close in the embedding space. Contrastive learning has witnessed success in image \u2026"}, {"title": "HyperFLoRA: Federated Learning with Instantaneous Personalization", "link": "https://epubs.siam.org/doi/pdf/10.1137/1.9781611978032.94", "details": "Q Lu, D Niu, MS Khoshkho, B Li - Proceedings of the 2024 SIAM International \u2026, 2024", "abstract": "Federated learning is a decentralized approach to training machine learning models while preserving data privacy. To accommodate data heterogeneity among clients, a longstanding issue in Federated Learning, many Personalized Federated Learning \u2026"}, {"title": "Improving Representation With Hierarchical Contrastive Learning for Emotion-Cause Pair Extraction", "link": "https://ieeexplore.ieee.org/abstract/document/10509759/", "details": "G Hu, Y Zhao, G Lu - IEEE Transactions on Affective Computing, 2024", "abstract": "Emotion-cause pair extraction (ECPE) aims to extract emotions and their corresponding cause from a document. The previous works have made great progress. However, there exist two major issues in existing works. First, most existing \u2026"}, {"title": "ATG: Benchmarking Automated Theorem Generation for Generative Language Models", "link": "https://eleanor-h.github.io/publication/confnaacl-2024-atg/confnaacl-2024-atg.pdf", "details": "X Lin, Q Cao, Y Huang, Z Yang, Z Liu, Z Li, X Liang15", "abstract": "Humans can develop new theorems to explore broader and more complex mathematical results. While current generative language models (LMs) have achieved significant improvement in automatically proving theorems, their ability to \u2026"}, {"title": "Generative AI-Based Text Generation Methods Using Pre-Trained GPT-2 Model", "link": "https://arxiv.org/pdf/2404.01786", "details": "R Pandey, H Waghela, S Rakshit, A Rangari, A Singh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This work delved into the realm of automatic text generation, exploring a variety of techniques ranging from traditional deterministic approaches to more modern stochastic methods. Through analysis of greedy search, beam search, top-k \u2026"}, {"title": "More Room for Language: Investigating the Effect of Retrieval on Language Models", "link": "https://arxiv.org/pdf/2404.10939", "details": "D Samuel, LGG Charpentier, S Wold - arXiv preprint arXiv:2404.10939, 2024", "abstract": "Retrieval-augmented language models pose a promising alternative to standard language modeling. During pretraining, these models search in a corpus of documents for contextually relevant information that could aid the language \u2026"}, {"title": "Source-Aware Training Enables Knowledge Attribution in Language Models", "link": "https://arxiv.org/pdf/2404.01019", "details": "M Khalifa, D Wadden, E Strubell, H Lee, L Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source (s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining \u2026"}, {"title": "Iterated Learning Improves Compositionality in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2404.02145", "details": "C Zheng, J Zhang, A Kembhavi, R Krishna - arXiv preprint arXiv:2404.02145, 2024", "abstract": "A fundamental characteristic common to both human vision and natural language is their compositional nature. Yet, despite the performance gains contributed by large vision and language pretraining, recent investigations find that most-if not all-our \u2026"}]
