[{"title": "Transferable Adversarial Attacks on Black-Box Vision-Language Models", "link": "https://arxiv.org/pdf/2505.01050", "details": "K Hu, W Yu, L Zhang, A Robey, A Zou, C Xu, H Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision Large Language Models (VLLMs) are increasingly deployed to offer advanced capabilities on inputs comprising both text and images. While prior research has shown that adversarial attacks can transfer from open-source to \u2026", "entry_id": "http://arxiv.org/abs/2505.01050v1", "updated": "2025-05-02 06:51:11", "published": "2025-05-02 06:51:11", "authors": "Kai Hu;Weichen Yu;Li Zhang;Alexander Robey;Andy Zou;Chengming Xu;Haoqi Hu;Matt Fredrikson", "summary": "Vision Large Language Models (VLLMs) are increasingly deployed to offer\nadvanced capabilities on inputs comprising both text and images. While prior\nresearch has shown that adversarial attacks can transfer from open-source to\nproprietary black-box models in text-only and vision-only contexts, the extent\nand effectiveness of such vulnerabilities remain underexplored for VLLMs. We\npresent a comprehensive analysis demonstrating that targeted adversarial\nexamples are highly transferable to widely-used proprietary VLLMs such as\nGPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to\ninduce specific attacker-chosen interpretations of visual information, such as\nmisinterpreting hazardous content as safe, overlooking sensitive or restricted\nmaterial, or generating detailed incorrect responses aligned with the\nattacker's intent. Furthermore, we discover that universal perturbations --\nmodifications applicable to a wide set of images -- can consistently induce\nthese misinterpretations across multiple proprietary VLLMs. Our experimental\nresults on object recognition, visual question answering, and image captioning\nshow that this vulnerability is common across current state-of-the-art models,\nand underscore an urgent need for robust mitigations to ensure the safe and\nsecure deployment of VLLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.LG", "links": "http://arxiv.org/abs/2505.01050v1;http://arxiv.org/pdf/2505.01050v1", "pdf_url": "http://arxiv.org/pdf/2505.01050v1"}, {"title": "Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts", "link": "https://arxiv.org/pdf/2505.17222", "details": "G Chochlakis, P Wu, A Bedi, M Ma, K Lerman\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Modeling complex subjective tasks in Natural Language Processing, such as recognizing emotion and morality, is considerably challenging due to significant variation in human annotations. This variation often reflects reasonable differences in \u2026", "entry_id": "http://arxiv.org/abs/2505.17222v1", "updated": "2025-05-22 18:55:22", "published": "2025-05-22 18:55:22", "authors": "Georgios Chochlakis;Peter Wu;Arjun Bedi;Marcus Ma;Kristina Lerman;Shrikanth Narayanan", "summary": "Modeling complex subjective tasks in Natural Language Processing, such as\nrecognizing emotion and morality, is considerably challenging due to\nsignificant variation in human annotations. This variation often reflects\nreasonable differences in semantic interpretations rather than mere noise,\nnecessitating methods to distinguish between legitimate subjectivity and error.\nWe address this challenge by exploring label verification in these contexts\nusing Large Language Models (LLMs). First, we propose a simple In-Context\nLearning binary filtering baseline that estimates the reasonableness of a\ndocument-label pair. We then introduce the Label-in-a-Haystack setting: the\nquery and its label(s) are included in the demonstrations shown to LLMs, which\nare prompted to predict the label(s) again, while receiving task-specific\ninstructions (e.g., emotion recognition) rather than label copying. We show how\nthe failure to copy the label(s) to the output of the LLM are task-relevant and\ninformative. Building on this, we propose the Label-in-a-Haystack Rectification\n(LiaHR) framework for subjective label correction: when the model outputs\ndiverge from the reference gold labels, we assign the generated labels to the\nexample instead of discarding it. This approach can be integrated into\nannotation pipelines to enhance signal-to-noise ratios. Comprehensive analyses,\nhuman evaluations, and ecological validity studies verify the utility of LiaHR\nfor label correction. Code is available at https://github.com/gchochla/LiaHR.", "comment": "17 pages, 16 figures, 9 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.17222v1;http://arxiv.org/pdf/2505.17222v1", "pdf_url": "http://arxiv.org/pdf/2505.17222v1"}, {"title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks", "link": "https://arxiv.org/pdf/2505.12371", "details": "Y Zhu, Z He, H Hu, X Zheng, X Zhang, Z Wang, J Gao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The rapid advancement of Large Language Models (LLMs) has stimulated interest in multi-agent collaboration for addressing complex medical tasks. However, the practical advantages of multi-agent collaboration approaches remain insufficiently \u2026", "entry_id": "http://arxiv.org/abs/2505.12371v1", "updated": "2025-05-18 11:28:17", "published": "2025-05-18 11:28:17", "authors": "Yinghao Zhu;Ziyi He;Haoran Hu;Xiaochen Zheng;Xichen Zhang;Zixiang Wang;Junyi Gao;Liantao Ma;Lequan Yu", "summary": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL;cs.LG;cs.MA", "links": "http://arxiv.org/abs/2505.12371v1;http://arxiv.org/pdf/2505.12371v1", "pdf_url": "http://arxiv.org/pdf/2505.12371v1"}, {"title": "Dual-Domain Masked Image Modeling: A Self-Supervised Pretraining Strategy Using Spatial and Frequency Domain Masking for Hyperspectral Data", "link": "https://arxiv.org/pdf/2505.03220", "details": "S Mohamed, T Fernando, S Sridharan, P Moghadam\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Hyperspectral images (HSIs) capture rich spectral signatures that reveal vital material properties, offering broad applicability across various domains. However, the scarcity of labeled HSI data limits the full potential of deep learning, especially for \u2026", "entry_id": "http://arxiv.org/abs/2505.03220v1", "updated": "2025-05-06 06:24:21", "published": "2025-05-06 06:24:21", "authors": "Shaheer Mohamed;Tharindu Fernando;Sridha Sridharan;Peyman Moghadam;Clinton Fookes", "summary": "Hyperspectral images (HSIs) capture rich spectral signatures that reveal\nvital material properties, offering broad applicability across various domains.\nHowever, the scarcity of labeled HSI data limits the full potential of deep\nlearning, especially for transformer-based architectures that require\nlarge-scale training. To address this constraint, we propose Spatial-Frequency\nMasked Image Modeling (SFMIM), a self-supervised pretraining strategy for\nhyperspectral data that utilizes the large portion of unlabeled data. Our\nmethod introduces a novel dual-domain masking mechanism that operates in both\nspatial and frequency domains. The input HSI cube is initially divided into\nnon-overlapping patches along the spatial dimension, with each patch comprising\nthe entire spectrum of its corresponding spatial location. In spatial masking,\nwe randomly mask selected patches and train the model to reconstruct the masked\ninputs using the visible patches. Concurrently, in frequency masking, we remove\nportions of the frequency components of the input spectra and predict the\nmissing frequencies. By learning to reconstruct these masked components, the\ntransformer-based encoder captures higher-order spectral-spatial correlations.\nWe evaluate our approach on three publicly available HSI classification\nbenchmarks and demonstrate that it achieves state-of-the-art performance.\nNotably, our model shows rapid convergence during fine-tuning, highlighting the\nefficiency of our pretraining strategy.", "comment": "Preprint to appear in IEEE IGARSS 2025", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.03220v1;http://arxiv.org/pdf/2505.03220v1", "pdf_url": "http://arxiv.org/pdf/2505.03220v1"}, {"title": "Language models should be subject to repeatable, open, domain-contextualized hallucination benchmarking", "link": "https://arxiv.org/pdf/2505.17345", "details": "JD Norman, MU Rivera, DA Hughes - arXiv preprint arXiv:2505.17345, 2025", "abstract": "Plausible, but inaccurate, tokens in model-generated text are widely believed to be pervasive and problematic for the responsible adoption of language models. Despite this concern, there is little scientific work that attempts to measure the prevalence of \u2026", "entry_id": "http://arxiv.org/abs/2505.17345v1", "updated": "2025-05-22 23:36:28", "published": "2025-05-22 23:36:28", "authors": "Justin D. Norman;Michael U. Rivera;D. Alex Hughes", "summary": "Plausible, but inaccurate, tokens in model-generated text are widely believed\nto be pervasive and problematic for the responsible adoption of language\nmodels. Despite this concern, there is little scientific work that attempts to\nmeasure the prevalence of language model hallucination in a comprehensive way.\nIn this paper, we argue that language models should be evaluated using\nrepeatable, open, and domain-contextualized hallucination benchmarking. We\npresent a taxonomy of hallucinations alongside a case study that demonstrates\nthat when experts are absent from the early stages of data creation, the\nresulting hallucination metrics lack validity and practical utility.", "comment": "9 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.17345v1;http://arxiv.org/pdf/2505.17345v1", "pdf_url": "http://arxiv.org/pdf/2505.17345v1"}]
