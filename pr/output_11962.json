[{"title": "Classifying Unstructured Text in Electronic Health Records for Mental Health Prediction Models: Large Language Model Evaluation Study", "link": "https://medinform.jmir.org/2025/1/e65454/", "details": "NC Cardamone, M Olfson, T Schmutte, L Ungar, T Liu\u2026 - JMIR Medical Informatics, 2025", "abstract": "Background: Prediction models have demonstrated a range of applications across medicine, including using electronic health record (EHR) data to identify hospital readmission and mortality risk. Large language models (LLMs) can transform \u2026"}, {"title": "Reusing routine electronic health record data for nationwide COVID-19 surveillance in nursing homes: barriers, facilitators, and lessons learned", "link": "https://link.springer.com/article/10.1186/s12911-024-02818-3", "details": "Y Wieland-Jorna, RA Verheij, AL Francke, R Coppen\u2026 - BMC Medical Informatics \u2026, 2024", "abstract": "Background At the beginning of the COVID-19 pandemic in 2020, little was known about the spread of COVID-19 in Dutch nursing homes while older people were particularly at risk of severe symptoms. Therefore, attempts were made to develop a \u2026"}, {"title": "What large language models know and what people think they know", "link": "https://www.nature.com/articles/s42256-024-00976-7", "details": "M Steyvers, H Tejeda, A Kumar, C Belem, S Karny\u2026 - Nature Machine Intelligence, 2025", "abstract": "As artificial intelligence systems, particularly large language models (LLMs), become increasingly integrated into decision-making processes, the ability to trust their outputs is crucial. To earn human trust, LLMs must be well calibrated such that they \u2026"}, {"title": "Cognitive Biases, Task Complexity, and Result Intepretability in Large Language Models", "link": "https://aclanthology.org/2025.coling-main.120.pdf", "details": "M Mina, V Ru\u00edz-Fern\u00e1ndez, J Falc\u00e3o, L Vasquez-Reina\u2026 - Proceedings of the 31st \u2026, 2025", "abstract": "In humans, cognitive biases are systematic deviations from rationality in judgment that simplify complex decisions. They typically manifest as a consequence of learned behaviors or limitations on information processing capabilities. Recent work has \u2026"}, {"title": "Topology-of-Question-Decomposition: Enhancing Large Language Models with Information Retrieval for Knowledge-Intensive Tasks", "link": "https://aclanthology.org/2025.coling-main.191.pdf", "details": "W Li, J Wang, LC Yu, X Zhang - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Large language models (LLMs) are increasingly deployed for general problem- solving across various domains yet remain constrained to chaining immediate reasoning steps and depending solely on parametric knowledge. Integrating an \u2026"}, {"title": "Exploring the Reliability of Large Language Models as Customized Evaluators for Diverse NLP Tasks", "link": "https://aclanthology.org/2025.coling-main.688.pdf", "details": "Q Li, L Cui, L Kong, W Bi - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Previous work adopts large language models (LLMs) as evaluators to evaluate natural language process (NLP) tasks. However, certain shortcomings, eg, fairness, scope, and accuracy, persist for current LLM evaluators. To analyze whether LLMs \u2026"}, {"title": "Anonymization by Design of Language Modeling", "link": "https://arxiv.org/pdf/2501.02407", "details": "A Boutet, ZE Kazdam, L Magnana, H Zimmermann - arXiv preprint arXiv:2501.02407, 2025", "abstract": "Rapid advances in Natural Language Processing (NLP) have revolutionized many fields, including healthcare. However, these advances raise significant privacy concerns, especially when models specialized on sensitive data can memorize and \u2026"}]
