'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Concept-aware Data Construction Improves In-context Le'
[{"title": "Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models", "link": "https://arxiv.org/pdf/2402.13492", "details": "S Maekawa, H Iso, S Gurajada, N Bhutani - arXiv preprint arXiv:2402.13492, 2024", "abstract": "While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant \u2026"}, {"title": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "link": "https://arxiv.org/html/2402.12842v1", "details": "G Kim, D Jang, E Yang - arXiv preprint arXiv:2402.12842, 2024", "abstract": "Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for \u2026"}, {"title": "Q-Probe: A Lightweight Approach to Reward Maximization for Language Models", "link": "https://arxiv.org/html/2402.14688v1", "details": "K Li, S Jelassi, H Zhang, S Kakade, M Wattenberg\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present an approach called Q-probing to adapt a pre-trained language model to maximize a task-specific reward function. At a high level, Q-probing sits between heavier approaches such as finetuning and lighter approaches such as few shot \u2026"}, {"title": "Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models", "link": "https://arxiv.org/pdf/2403.11838", "details": "Y Luo, Z Lin, Y Zhang, J Sun, C Lin, C Xu, X Su, Y Shen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from \u2026"}, {"title": "Unfamiliar Finetuning Examples Control How Language Models Hallucinate", "link": "https://arxiv.org/pdf/2403.05612", "details": "K Kang, E Wallace, C Tomlin, A Kumar, S Levine - arXiv preprint arXiv:2403.05612, 2024", "abstract": "Large language models (LLMs) have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts. In this work, we explore the underlying mechanisms that govern how finetuned LLMs \u2026"}, {"title": "RAFT: Adapting Language Model to Domain Specific RAG", "link": "https://arxiv.org/pdf/2403.10131", "details": "T Zhang, SG Patil, N Jain, S Shen, M Zaharia, I Stoica\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (eg, time-critical news, or private \u2026"}, {"title": "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models", "link": "https://arxiv.org/html/2403.02178v1", "details": "C Chen, X Wang, TE Lin, A Lv, Y Wu, X Gao, JR Wen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains. Earlier fine- tuning approaches sought to mitigate this by leveraging more precise supervisory \u2026"}, {"title": "Algorithmic progress in language models", "link": "https://arxiv.org/html/2403.05812v1", "details": "A Ho, T Besiroglu, E Erdil, D Owen, R Rahman, ZC Guo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We investigate the rate at which algorithms for pre-training language models have improved since the advent of deep learning. Using a dataset of over 200 language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we find that \u2026"}, {"title": "An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis", "link": "https://arxiv.org/pdf/2403.12402", "details": "Y Peng, I Kulikov, Y Yang, S Popuri, H Lu, C Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Speech language models (LMs) are promising for high-quality speech synthesis through in-context learning. A typical speech LM takes discrete semantic units as content and a short utterance as prompt, and synthesizes speech which preserves \u2026"}]
