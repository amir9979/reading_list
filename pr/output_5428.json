[{"title": "Fairness Definitions in Language Models Explained", "link": "https://arxiv.org/pdf/2407.18454", "details": "TV Doan, Z Chu, Z Wang, W Zhang - arXiv preprint arXiv:2407.18454, 2024", "abstract": "Language Models (LMs) have demonstrated exceptional performance across various Natural Language Processing (NLP) tasks. Despite these advancements, LMs can inherit and amplify societal biases related to sensitive attributes such as \u2026"}, {"title": "Do Language Models Have a Critical Period for Language Acquisition?", "link": "https://arxiv.org/pdf/2407.19325", "details": "I Constantinescu, T Pimentel, R Cotterell, A Warstadt - arXiv preprint arXiv \u2026, 2024", "abstract": "Humans appear to have a critical period (CP) for language acquisition: Second language (L2) acquisition becomes harder after early childhood, and ceasing exposure to a first language (L1) after this period (but not before) typically does not \u2026"}, {"title": "Assessing the Ability of a Large Language Model to Score Free-Text Medical Student Clinical Notes: Quantitative Study", "link": "https://mededu.jmir.org/2024/1/e56342", "details": "HB Burke, A Hoang, JO Lopreiato, H King, P Hemmer\u2026 - JMIR Medical Education, 2024", "abstract": "Background Teaching medical students the skills required to acquire, interpret, apply, and communicate clinical information is an integral part of medical education. A crucial aspect of this process involves providing students with feedback regarding \u2026"}, {"title": "Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering", "link": "https://arxiv.org/pdf/2407.21368", "details": "D Guo, D Terzopoulos - arXiv preprint arXiv:2407.21368, 2024", "abstract": "Large Vision-Language Models (LVLMs) have achieved significant success in recent years, and they have been extended to the medical domain. Although demonstrating satisfactory performance on medical Visual Question Answering (VQA) tasks \u2026"}, {"title": "Fine-tuning Language Models for Joint Rewriting and Completion of Code with Potential Bugs", "link": "https://aclanthology.org/2024.findings-acl.938.pdf", "details": "D Wang, J Zhao, H Pei, S Tan, S Zha - Findings of the Association for Computational \u2026, 2024", "abstract": "Handling drafty partial code remains a notable challenge in real-time code suggestion applications. Previous work has demonstrated shortcomings of large language models of code (CodeLLMs) in completing partial code with potential bugs \u2026"}, {"title": "Prompting Encoder Models for Zero-Shot Classification: A Cross-Domain Study in Italian", "link": "https://arxiv.org/pdf/2407.20654", "details": "S Auriemma, M Miliani, M Madeddu, A Bondielli\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Addressing the challenge of limited annotated data in specialized fields and low- resource languages is crucial for the effective use of Language Models (LMs). While most Large Language Models (LLMs) are trained on general-purpose English \u2026"}, {"title": "Heterogeneous-Graph Reasoning with Context Paraphrase for Commonsense Question Answering", "link": "https://ieeexplore.ieee.org/abstract/document/10612243/", "details": "Y Wang, H Zhang, J Liang, R Li - IEEE/ACM Transactions on Audio, Speech, and \u2026, 2024", "abstract": "Commonsense question answering (CQA) generally means that the machine uses its mastered commonsense to answer questions without relevant background material, which is a challenging task in natural language processing. Existing \u2026"}, {"title": "Improving Self-training with Prototypical Learning for Source-Free Domain Adaptation on Clinical Text", "link": "https://aclanthology.org/2024.bionlp-1.1.pdf", "details": "S Shimizu, S Yada, L Raithel, E Aramaki - Proceedings of the 23rd Workshop on \u2026, 2024", "abstract": "Abstract Domain adaptation is crucial in the clinical domain since the performance of a model trained on one domain (source) degrades seriously when applied to another domain (target). However, conventional domain adaptation methods often cannot be \u2026"}, {"title": "Position Paper: Dual-System Language Models via Next-Action Prediction", "link": "https://openreview.net/pdf%3Fid%3D9ZVfz8DGC8", "details": "Z Du, WJ Su - ICML 2024 Workshop on LLMs and Cognition", "abstract": "In current Large Language Model (LLM) practices, each token is appended sequentially to the output. In contrast, humans are capable of revising and correcting what we write. Inspired by this gap, in this position paper, we propose a dual-system \u2026"}]
