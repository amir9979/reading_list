[{"title": "Decoding Report Generators: A Cyclic Vision-Language Adapter for Counterfactual Explanations", "link": "https://arxiv.org/pdf/2411.05261", "details": "Y Fang, Z Jin, S Guo, J Liu, Y Gao, J Ning, Z Yue, Z Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite significant advancements in report generation methods, a critical limitation remains: the lack of interpretability in the generated text. This paper introduces an innovative approach to enhance the explainability of text generated by report \u2026"}, {"title": "VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models", "link": "https://arxiv.org/pdf/2411.17451", "details": "L Li, Y Wei, Z Xie, X Yang, Y Song, P Wang, C An, T Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language generative reward models (VL-GenRMs) play a crucial role in aligning and evaluating multimodal AI systems, yet their own evaluation remains under-explored. Current assessment methods primarily rely on AI-annotated \u2026"}, {"title": "The Extractive-Abstractive Spectrum: Uncovering Verifiability Trade-offs in LLM Generations", "link": "https://arxiv.org/pdf/2411.17375", "details": "T Worledge, T Hashimoto, C Guestrin - arXiv preprint arXiv:2411.17375, 2024", "abstract": "Across all fields of academic study, experts cite their sources when sharing information. While large language models (LLMs) excel at synthesizing information, they do not provide reliable citation to sources, making it difficult to trace and verify \u2026"}, {"title": "Curriculum-enhanced GroupDRO: Challenging the Norm of Avoiding Curriculum Learning in Subpopulation Shift Setups", "link": "https://arxiv.org/pdf/2411.15272", "details": "A Barbalau - arXiv preprint arXiv:2411.15272, 2024", "abstract": "In subpopulation shift scenarios, a Curriculum Learning (CL) approach would only serve to imprint the model weights, early on, with the easily learnable spurious correlations featured. To the best of our knowledge, none of the current state-of-the \u2026"}, {"title": "Skills-in-Context: Unlocking Compositionality in Large Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.812.pdf", "details": "J Chen, X Pan, D Yu, K Song, X Wang, D Yu, J Chen - Findings of the Association for \u2026, 2024", "abstract": "We investigate how to elicit compositional generalization capabilities in large language models (LLMs). Compositional generalization empowers LLMs to solve complex problems by combining foundational skills, a critical reasoning ability akin to \u2026"}, {"title": "FactTest: Factuality Testing in Large Language Models with Statistical Guarantees", "link": "https://arxiv.org/pdf/2411.02603", "details": "F Nie, X Hou, S Lin, J Zou, H Yao, L Zhang - arXiv preprint arXiv:2411.02603, 2024", "abstract": "The propensity of Large Language Models (LLMs) to generate hallucinations and non-factual content undermines their reliability in high-stakes domains, where rigorous control over Type I errors (the conditional probability of incorrectly \u2026"}, {"title": "SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models", "link": "https://arxiv.org/pdf/2411.02433", "details": "J Zhang, DC Juan, C Rashtchian, CS Ferng, H Jiang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that \u2026"}, {"title": "The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on Large Language Models", "link": "https://arxiv.org/pdf/2411.11407", "details": "X Yang, X Tang, J Han, S Hu - arXiv preprint arXiv:2411.11407, 2024", "abstract": "The widespread deployment of large language models (LLMs) across various domains has showcased their immense potential while exposing significant safety vulnerabilities. A major concern is ensuring that LLM-generated content aligns with \u2026"}]
