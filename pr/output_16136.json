[{"title": "STP: Special token prompt for parameter-efficient tuning of pre-trained language models", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425012874", "details": "Y Yan, H Yu, D Wang, J Ye, F Liu, W Xu - Expert Systems with Applications, 2025", "abstract": "Fine-tuning has become the standard method for using large pre-trained language models to accomplish specific downstream tasks. However, full fine-tuning requires updating all model parameters, which is not only computationally expensive but also \u2026"}, {"title": "Eliciting Critical Reasoning in Retrieval-Augmented Generation via Contrastive Explanations", "link": "https://aclanthology.org/2025.naacl-long.557.pdf", "details": "L Ranaldi, M Valentino, A Freitas - Proceedings of the 2025 Conference of the \u2026, 2025", "abstract": "Retrieval-augmented generation (RAG) have emerged as a critical mechanism in contemporary NLP to support Large Language Models (LLMs) in systematically accessing richer factual context. However, the integration of RAG mechanisms bring \u2026"}]
