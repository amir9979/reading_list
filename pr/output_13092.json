[{"title": "Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization", "link": "https://arxiv.org/pdf/2502.13632", "details": "OR Bidusa, S Markovitch - arXiv preprint arXiv:2502.13632, 2025", "abstract": "The opaque nature of Large Language Models (LLMs) has led to significant research efforts aimed at enhancing their interpretability, primarily through post-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck Models (CBMs), offer \u2026"}, {"title": "A theoretical design of concept sets: improving the predictability of concept bottleneck models", "link": "https://proceedings.neurips.cc/paper_files/paper/2024/file/b5a412531110b92961fa13c90938806a-Paper-Conference.pdf", "details": "M Ruiz Luyten, M van der Schaar - Advances in Neural Information Processing \u2026, 2025", "abstract": "Abstract Concept-based learning, a promising approach in machine learning, emphasizes the value of high-level representations called concepts. However, despite growing interest in concept-bottleneck models (CBMs), there is a lack of clear \u2026"}, {"title": "DCBM: Data-Efficient Visual Concept Bottleneck Models", "link": "https://pure.mpg.de/rest/items/item_3636912/component/file_3636913/content", "details": "K Prasse, P Knab, S Marton, C Bartelt, M Keuper - arXiv preprint arXiv:2412.11576, 2025", "abstract": "Abstract Concept Bottleneck Models (CBMs) enhance the interpretability of neural networks by basing predictions on human-understandable concepts. However, current CBMs typically rely on concept sets extracted from large language models or \u2026"}, {"title": "Aerial Image Classification in Post Flood Scenarios Using Robust Deep Learning and Explainable Artificial Intelligence", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10891453.pdf", "details": "A Manaf, N Mughal, KR Talpur, BA Talpur, G Mujtaba\u2026 - IEEE Access, 2025", "abstract": "Efficiently delivering timely assistance to flooded regions is a critical imperative, and leveraging deep-learning methodologies has demonstrated significant efficacy in addressing environmental challenges. Several authors have collected data from \u2026"}, {"title": "EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models", "link": "https://arxiv.org/pdf/2502.06663", "details": "X Xing, Z Liu, S Xiao, B Gao, Y Liang, W Zhang, H Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Modern large language models (LLMs) driven by scaling laws, achieve intelligence emergency in large model sizes. Recently, the increasing concerns about cloud costs, latency, and privacy make it an urgent requirement to develop compact edge \u2026"}, {"title": "Time-series attribution maps with regularized contrastive learning", "link": "https://arxiv.org/pdf/2502.12977", "details": "S Schneider, RG Laiz, A Filippova, M Frey, MW Mathis - arXiv preprint arXiv \u2026, 2025", "abstract": "Gradient-based attribution methods aim to explain decisions of deep learning models but so far lack identifiability guarantees. Here, we propose a method to generate attribution maps with identifiability guarantees by developing a regularized \u2026"}, {"title": "LLM4GNAS: A Large Language Model Based Toolkit for Graph Neural Architecture Search", "link": "https://arxiv.org/pdf/2502.10459", "details": "Y Gao, H Yang, Y Chen, J Wu, P Zhang, H Wang - arXiv preprint arXiv:2502.10459, 2025", "abstract": "Graph Neural Architecture Search (GNAS) facilitates the automatic design of Graph Neural Networks (GNNs) tailored to specific downstream graph learning tasks. However, existing GNAS approaches often require manual adaptation to new graph \u2026"}, {"title": "Slot-Guided Adaptation of Pre-trained Diffusion Models for Object-Centric Learning and Compositional Generation", "link": "https://arxiv.org/pdf/2501.15878", "details": "AK Akan, Y Yemez - arXiv preprint arXiv:2501.15878, 2025", "abstract": "We present SlotAdapt, an object-centric learning method that combines slot attention with pretrained diffusion models by introducing adapters for slot-based conditioning. Our method preserves the generative power of pretrained diffusion models, while \u2026"}, {"title": "Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement", "link": "https://arxiv.org/pdf/2502.02573%3F", "details": "S Abbasloo - arXiv preprint arXiv:2502.02573, 2025", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem- solving, a crucial, ubiquitous, and complex domain. This paper explores the \u2026"}]
