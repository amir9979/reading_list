[{"title": "Guiding Medical Vision-Language Models with Explicit Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations", "link": "https://arxiv.org/pdf/2501.02385", "details": "K Zhu, Z Qin, H Yi, Z Jiang, Q Lao, S Zhang, K Li - arXiv preprint arXiv:2501.02385, 2025", "abstract": "With the recent advancements in vision-language models (VLMs) driven by large language models (LLMs), many researchers have focused on models that comprised of an image encoder, an image-to-language projection layer, and a text decoder \u2026"}, {"title": "DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision Language Models in Real-World Scenarios with Driving Theory Tests", "link": "https://arxiv.org/pdf/2501.04671", "details": "C Corbi\u00e8re, S Roburin, S Montariol, A Bosselut, A Alahi - arXiv preprint arXiv \u2026, 2025", "abstract": "Large vision-language models (LVLMs) augment language models with visual understanding, enabling multimodal reasoning. However, due to the modality gap between textual and visual data, they often face significant challenges, such as over \u2026"}, {"title": "Language Models Encode the Value of Numbers Linearly", "link": "https://aclanthology.org/2025.coling-main.47.pdf", "details": "F Zhu, D Dai, Z Sui - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Large language models (LLMs) have exhibited impressive competence in various tasks, but their internal mechanisms on mathematical problems are still under- explored. In this paper, we study a fundamental question: how language models \u2026"}, {"title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers", "link": "https://arxiv.org/pdf/2501.16961", "details": "M Raza, N Milic-Frayling - arXiv preprint arXiv:2501.16961, 2025", "abstract": "Robustness of reasoning remains a significant challenge for large language models, and addressing it is essential for the practical applicability of AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a novel approach that \u2026"}, {"title": "Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models", "link": "https://arxiv.org/pdf/2501.14818", "details": "Z Li, G Chen, S Liu, S Wang, V VS, Y Ji, S Lan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, promising progress has been made by open-source vision-language models (VLMs) in bringing their capabilities closer to those of proprietary frontier models. However, most open-source models only publish their final model weights \u2026"}, {"title": "When Large Vision Language Models Meet Multimodal Sequential Recommendation: An Empirical Study", "link": "https://openreview.net/pdf%3Fid%3DE8bjWloEvU", "details": "P Zhou, C Liu, J Ren, X Zhou, XIE Yueqi, M Cao, Z Rao\u2026 - THE WEB CONFERENCE 2025", "abstract": "As multimedia content continues to grow on the Web, the integration of visual and textual data has become a crucial challenge for Web applications, particularly in recommendation systems. Large Vision Language Models (LVLMs) have \u2026"}, {"title": "Efficient Architectures for High Resolution Vision-Language Models", "link": "https://arxiv.org/pdf/2501.02584", "details": "M Carvalho, B Martins - arXiv preprint arXiv:2501.02584, 2025", "abstract": "Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This \u2026"}, {"title": "SKIntern: Internalizing Symbolic Knowledge for Distilling Better CoT Capabilities into Small Language Models", "link": "https://aclanthology.org/2025.coling-main.215.pdf", "details": "H Liao, S He, Y Hao, X Li, Y Zhang, J Zhao, K Liu - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract Small Language Models (SLMs) are attracting attention due to the high computational demands and privacy concerns of Large Language Models (LLMs). Some studies fine-tune SLMs using Chains of Thought (CoT) data distilled from \u2026"}, {"title": "Exploring Primitive Visual Measurement Understanding and the Role of Output Format in Learning in Vision-Language Models", "link": "https://arxiv.org/pdf/2501.15144", "details": "A Yadav, L Liu, Y Qi - arXiv preprint arXiv:2501.15144, 2025", "abstract": "This work investigates the capabilities of current vision-language models (VLMs) in visual understanding and attribute measurement of primitive shapes using a benchmark focused on controlled 2D shape configurations with variations in spatial \u2026"}]
