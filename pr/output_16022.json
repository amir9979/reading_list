[{"title": "Parameter-efficient fine-tuning in large language models: a survey of methodologies", "link": "https://link.springer.com/article/10.1007/s10462-025-11236-4", "details": "L Wang, S Chen, L Jiang, S Pan, R Cai, S Yang\u2026 - Artificial Intelligence Review, 2025", "abstract": "The large language models, as predicted by scaling law forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the \u2026"}, {"title": "DICE: A Framework for Dimensional and Contextual Evaluation of Language Models", "link": "https://arxiv.org/pdf/2504.10359%3F", "details": "A Shrivastava, PA Aoyagui - arXiv preprint arXiv:2504.10359, 2025", "abstract": "Language models (LMs) are increasingly being integrated into a wide range of applications, yet the modern evaluation paradigm does not sufficiently reflect how they are actually being used. Current evaluations rely on benchmarks that often lack \u2026"}, {"title": "Benchmarking Vision Language Models on German Factual Data", "link": "https://arxiv.org/pdf/2504.11108%3F", "details": "R Peinl, V Tischler - arXiv preprint arXiv:2504.11108, 2025", "abstract": "Similar to LLMs, the development of vision language models is mainly driven by English datasets and models trained in English and Chinese language, whereas support for other languages, even those considered high-resource languages such \u2026"}, {"title": "Summarization of Multimodal Presentations with Vision-Language Models: Study of the Effect of Modalities and Structure", "link": "https://arxiv.org/pdf/2504.10049%3F", "details": "T Gigant, C Guinaudeau, F Dufaux - arXiv preprint arXiv:2504.10049, 2025", "abstract": "Vision-Language Models (VLMs) can process visual and textual information in multiple formats: texts, images, interleaved texts and images, or even hour-long videos. In this work, we conduct fine-grained quantitative and qualitative analyses of \u2026"}, {"title": "Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models", "link": "https://arxiv.org/pdf/2505.00150", "details": "MH Van, X Wu - arXiv preprint arXiv:2505.00150, 2025", "abstract": "The rapid evolution of social media has provided enhanced communication channels for individuals to create online content, enabling them to express their thoughts and opinions. Multimodal memes, often utilized for playful or humorous \u2026"}, {"title": "Pre-Trained Language Models for Mental Health: An Empirical Study on Arabic Q&A Classification", "link": "https://www.mdpi.com/2227-9032/13/9/985", "details": "H Alhuzali, A Alasmari - Healthcare, 2025", "abstract": "Background: Pre-Trained Language Models hold significant promise for revolutionizing mental health care by delivering accessible and culturally sensitive resources. Despite this potential, their efficacy in mental health applications \u2026"}, {"title": "RAISE: Reinforenced Adaptive Instruction Selection For Large Language Models", "link": "https://arxiv.org/pdf/2504.07282", "details": "L Qingsong, Y Li, Z Lan, Z Xu, J Tang, Y Li, W Jiang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In the instruction fine-tuning of large language models (LLMs), it has become a consensus that a few high-quality instructions are superior to a large number of low- quality instructions. At present, many instruction selection methods have been \u2026"}, {"title": "Learning to Erase Private Knowledge from Multi-Documents for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2504.09910", "details": "Y Wang, H Zhang, L Pang, Y Tong, B Guo, H Zheng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-Augmented Generation (RAG) is a promising technique for applying LLMs to proprietary domains. However, retrieved documents may contain sensitive knowledge, posing risks of privacy leakage in generative results. Thus, effectively \u2026"}, {"title": "Racing Thoughts: Explaining Contextualization Errors in Large Language Models", "link": "https://aclanthology.org/2025.naacl-long.155.pdf", "details": "MA Lepori, MC Mozer, A Ghandeharioun - Proceedings of the 2025 Conference of the \u2026, 2025", "abstract": "The profound success of transformer-based language models can largely be attributed to their ability to integrate relevant contextual information from an input sequence in order to generate a response or complete a task. However, we know \u2026"}]
