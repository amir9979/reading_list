[{"title": "FlexCare: Leveraging Cross-Task Synergy for Flexible Multimodal Healthcare Prediction", "link": "https://arxiv.org/pdf/2406.11928", "details": "M Xu, Z Zhu, Y Li, S Zheng, Y Zhao, K He, Y Zhao - arXiv preprint arXiv:2406.11928, 2024", "abstract": "Multimodal electronic health record (EHR) data can offer a holistic assessment of a patient's health status, supporting various predictive healthcare tasks. Recently, several studies have embraced the multitask learning approach in the healthcare \u2026"}, {"title": "Self-Hint Prompting Improves Zero-shot Reasoning in Large Language Models via Reflective Cycle", "link": "https://escholarship.org/content/qt5ht3f0dt/qt5ht3f0dt_noSplash_508be8c9920e4bd796bec268a73a6b1a.pdf", "details": "J Chen, J Tian, Y Jin - Proceedings of the Annual Meeting of the Cognitive \u2026, 2024", "abstract": "Chain-of-Thought (CoT) has brought a fresh perspective to improve the reasoning ability of large language models (LLMs). To relieve the burden of manual design in CoT, Zero-shot CoT has pioneered a direct interaction with LLMs. Based on it \u2026"}, {"title": "A Study of Biomedical Relation Extraction Using GPT Models", "link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11141827/", "details": "J Zhang, M Wibert, H Zhou, X Peng, Q Chen, VK Keloth\u2026 - AMIA Summits on \u2026, 2024", "abstract": "Relation Extraction (RE) is a natural language processing (NLP) task for extracting semantic relations between biomedical entities. Recent developments in pre-trained large language models (LLM) motivated NLP researchers to use them for various \u2026"}, {"title": "Fusion Makes Perfection: An Efficient Multi-Grained Matching Approach for Zero-Shot Relation Extraction", "link": "https://aclanthology.org/2024.naacl-short.7.pdf", "details": "S Li, G Bai, Z Zhang, Y Liu, C Lu, D Guo, R Liu, S Yong - Proceedings of the 2024 \u2026, 2024", "abstract": "Predicting unseen relations that cannot be observed during the training phase is a challenging task in relation extraction. Previous works have made progress by matching the semantics between input instances and label descriptions. However \u2026"}, {"title": "Towards a Personal Health Large Language Model", "link": "https://arxiv.org/pdf/2406.06474", "details": "J Cosentino, A Belyaeva, X Liu, NA Furlotte, Z Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In health, most large language model (LLM) research has focused on clinical tasks. However, mobile and wearable devices, which are rarely integrated into such tasks, provide rich, longitudinal data for personal health monitoring. Here we present \u2026"}]
