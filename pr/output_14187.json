[{"title": "Towards statistical factuality guarantee for large vision-language models", "link": "https://arxiv.org/pdf/2502.20560", "details": "Z Li, C Yan, NJ Jackson, W Cui, B Li, J Zhang, BA Malin - arXiv preprint arXiv \u2026, 2025", "abstract": "Advancements in Large Vision-Language Models (LVLMs) have demonstrated promising performance in a variety of vision-language tasks involving image- conditioned free-form text generation. However, growing concerns about \u2026"}, {"title": "Capturing nuanced preferences: Preference-aligned distillation for small language models", "link": "https://arxiv.org/pdf/2502.14272", "details": "Y Gu, J Li, S Huang, X Zou, Z Li, X Hu - arXiv preprint arXiv:2502.14272, 2025", "abstract": "Aligning small language models (SLMs) with human values typically involves distilling preference knowledge from large language models (LLMs). However, existing distillation methods model preference knowledge in teacher LLMs by \u2026"}, {"title": "Stackelberg Game Preference Optimization for Data-Efficient Alignment of Language Models", "link": "https://arxiv.org/pdf/2502.18099", "details": "X Chu, Z Zhang, T Jia, Y Jin - arXiv preprint arXiv:2502.18099, 2025", "abstract": "Aligning language models with human preferences is critical for real-world deployment, but existing methods often require large amounts of high-quality human annotations. Aiming at a data-efficient alignment method, we propose Stackelberg \u2026"}, {"title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?", "link": "https://arxiv.org/pdf/2503.02199", "details": "A Deng, T Cao, Z Chen, B Hooi - arXiv preprint arXiv:2503.02199, 2025", "abstract": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual \u2026"}, {"title": "MMSciBench: Benchmarking Language Models on Multimodal Scientific Problems", "link": "https://arxiv.org/pdf/2503.01891", "details": "X Ye, C Li, S Chen, X Tang, W Wei - arXiv preprint arXiv:2503.01891, 2025", "abstract": "Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present \u2026"}, {"title": "Self-training elicits concise reasoning in large language models", "link": "https://arxiv.org/pdf/2502.20122", "details": "T Munkhbat, N Ho, S Kim, Y Yang, Y Kim, SY Yun - arXiv preprint arXiv:2502.20122, 2025", "abstract": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens \u2026"}, {"title": "Improving Foundation Models on Electronic Health Records", "link": "https://udspace.udel.edu/bitstreams/581bb829-eb4b-43de-b937-9e9637390a98/download", "details": "R Poulain - 2025", "abstract": "Recent advances in foundation models have opened up new possibilities for healthcare applications, particularly by utilizing transformer-based models to take advantage of the longitudinal nature of both natural language and electronic health \u2026"}, {"title": "XAttack: Counterfactual Explainable Prompt Attack Analysis on Large Language Models", "link": "https://scholar.xjtlu.edu.cn/files/52603047/IEEE_Big_Data_XAttack_Counterfactual_Explainable_Prompt_Attack_Analysis.pdf", "details": "D Shu, M Jin, C Zhang, T Chen, L Li, Y Zhang", "abstract": "This study sheds light on the imperative need to bolster safety and privacy measures in large language models (LLMs), such as GPT-4 and llama-2, by identifying and mitigating their vulnerabilities through explainable analysis of prompt attacks. We \u2026"}, {"title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language Models", "link": "https://www.researchgate.net/profile/Meng_Cao31/publication/389979929_Video_SimpleQA_Towards_Factuality_Evaluation_in_Large_Video_Language_Models/links/67dae6e03ad6d174c48cbb42/Video-SimpleQA-Towards-Factuality-Evaluation-in-Large-Video-Language-Models.pdf", "details": "M Cao, P Hu, Y Wang, J Gu, H Tang, H Zhao, J Dong\u2026", "abstract": "Abstract Recent advancements in Large Video Language Models (LVLMs) have highlighted their potential for multi-modal understanding, yet evaluating their factual grounding in video contexts remains a critical unsolved challenge. To address this \u2026"}]
