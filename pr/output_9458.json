[{"title": "LM2: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.920.pdf", "details": "G Juneja, S Dutta, T Chakraborty - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}, {"title": "Advancing Cross-Lingual Entity Alignment with Large Language Models: Tailored Sample Segmentation and Zero-Shot Prompts", "link": "https://aclanthology.org/2024.findings-emnlp.475.pdf", "details": "L Yang, J Cheng, F Zhang - Findings of the Association for Computational \u2026, 2024", "abstract": "In recent years, the advent of large language models (LLMs) like GPT and Llama has significantly influenced numerous domains, particularly in advancing natural language processing (NLP) capabilities. LLMs have shown remarkable performance \u2026"}, {"title": "Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2410.21728", "details": "K Luo, Z Ding, Z Weng, L Qiao, M Zhao, X Li, D Yin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Chain of Thought (CoT) prompting approaches have significantly consolidated the reasoning capabilities of large language models (LLMs), they still face limitations that require extensive human effort or have performance needs to be improved \u2026"}, {"title": "Generating Realistic Tabular Data with Large Language Models", "link": "https://arxiv.org/pdf/2410.21717", "details": "D Nguyen, S Gupta, K Do, T Nguyen, S Venkatesh - arXiv preprint arXiv:2410.21717, 2024", "abstract": "While most generative models show achievements in image data generation, few are developed for tabular data generation. Recently, due to success of large language models (LLM) in diverse tasks, they have also been used for tabular data generation \u2026"}, {"title": "Rethinking the Role of Proxy Rewards in Language Model Alignment", "link": "https://aclanthology.org/2024.emnlp-main.1150.pdf", "details": "S Kim, M Seo - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Learning from human feedback via proxy reward modeling has been studied to align Large Language Models (LLMs) with human values. However, achieving reliable training through that proxy reward model (RM) is not a trivial problem, and its \u2026"}, {"title": "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.313.pdf", "details": "M Xu, Y Li, K Sun, T Qian - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Large language models (LLMs) have shown excellent capability for solving reasoning problems. Existing approaches do not differentiate the question difficulty when designing prompting methods for them. Clearly, a simple method cannot elicit \u2026"}, {"title": "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention", "link": "https://arxiv.org/pdf/2411.02063", "details": "X Lv, N Ding, K Zhang, E Hua, G Cui, B Zhou - arXiv preprint arXiv:2411.02063, 2024", "abstract": "Improving the effectiveness and efficiency of large language models (LLMs) simultaneously is a critical yet challenging research goal. In this paper, we find that low-rank pre-training, normally considered as efficient methods that will compromise \u2026"}, {"title": "Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination", "link": "https://arxiv.org/pdf/2411.12591", "details": "H Zheng, T Xu, H Sun, S Pu, R Chen, L Sun - arXiv preprint arXiv:2411.12591, 2024", "abstract": "Multimodal large language models (MLLMs) have advanced the integration of visual and linguistic modalities, establishing themselves as the dominant paradigm for visual-language tasks. Current approaches like chain of thought (CoT) reasoning \u2026"}, {"title": "A Demonstration of Adaptive Collaboration of Large Language Models for Medical Decision-Making", "link": "https://arxiv.org/pdf/2411.00248", "details": "Y Kim, C Park, H Jeong, C Grau-Vilchez, YS Chan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical Decision-Making (MDM) is a multi-faceted process that requires clinicians to assess complex multi-modal patient data patient, often collaboratively. Large Language Models (LLMs) promise to streamline this process by synthesizing vast \u2026"}]
