[{"title": "Beyond the Hype: A dispassionate look at vision-language models in medical scenario", "link": "https://arxiv.org/pdf/2408.08704", "details": "Y Nan, H Zhou, X Xing, G Yang - arXiv preprint arXiv:2408.08704, 2024", "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across diverse tasks, garnering significant attention in AI communities. However, their performance and reliability in specialized \u2026"}, {"title": "VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models", "link": "https://aclanthology.org/2024.acl-long.658.pdf", "details": "Q Cao, J Cheng, X Liang, L Lin - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Despite the significant success of large vision-language models (LVLMs), some studies have revealed that LVLMs suffer from the hallucination problem, where the LVLMs' response contains descriptions of non-existent objects. Although various \u2026"}, {"title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "link": "https://arxiv.org/pdf/2407.21417", "details": "Z Wu, Y Zhang, P Qi, Y Xu, R Han, Y Zhang, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (ie, follow open-ended instructions) and \u2026"}, {"title": "Effective prompt extraction from language models", "link": "https://openreview.net/pdf%3Fid%3D0o95CVdNuz", "details": "Y Zhang, N Carlini, D Ippolito - First Conference on Language Modeling, 2024", "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden \u2026"}, {"title": "Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models", "link": "https://aclanthology.org/2024.acl-long.404.pdf", "details": "Q Ai, J Li, J Dai, J Zhou, L Liu, H Jiang, S Shi - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields. In this paper, we propose a new paradigm for interactive and instructional graph data \u2026"}, {"title": "Towards Holistic Disease Risk Prediction using Small Language Models", "link": "https://arxiv.org/pdf/2408.06943", "details": "L Bj\u00f6rkdahl, O Pauli, J \u00d6stman, C Ceccobello\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Data in the healthcare domain arise from a variety of sources and modalities, such as x-ray images, continuous measurements, and clinical notes. Medical practitioners integrate these diverse data types daily to make informed and accurate decisions \u2026"}, {"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "link": "https://arxiv.org/pdf/2408.00724", "details": "Y Wu, Z Sun, S Li, S Welleck, Y Yang - arXiv preprint arXiv:2408.00724, 2024", "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth \u2026"}, {"title": "Making Large Vision Language Models to be Good Few-shot Learners", "link": "https://arxiv.org/pdf/2408.11297", "details": "F Liu, W Cai, J Huo, C Zhang, D Chen, J Zhou - arXiv preprint arXiv:2408.11297, 2024", "abstract": "Few-shot classification (FSC) is a fundamental yet challenging task in computer vision that involves recognizing novel classes from limited data. While previous methods have focused on enhancing visual features or incorporating additional \u2026"}, {"title": "Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation", "link": "https://arxiv.org/pdf/2408.00555", "details": "X Qu, Q Chen, W Wei, J Sun, J Dong - arXiv preprint arXiv:2408.00555, 2024", "abstract": "Despite the remarkable ability of large vision-language models (LVLMs) in image comprehension, these models frequently generate plausible yet factually incorrect responses, a phenomenon known as hallucination. Recently, in large language \u2026"}]
