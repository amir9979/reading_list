[{"title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT", "link": "https://arxiv.org/pdf/2407.21058", "details": "M Ali, S Panda, Q Shen, M Wick, A Kobren - arXiv preprint arXiv:2407.21058, 2024", "abstract": "In the current landscape of language model research, larger models, larger datasets and more compute seems to be the only way to advance towards intelligence. While there have been extensive studies of scaling laws and models' scaling behaviors, the \u2026"}, {"title": "Domain Shift Analysis in Chest Radiographs Classification in a Veterans Healthcare Administration Population", "link": "https://arxiv.org/pdf/2407.21149", "details": "M Chandrashekar, I Goethert, MIU Haque, B McMahon\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Objectives: This study aims to assess the impact of domain shift on chest X-ray classification accuracy and to analyze the influence of ground truth label quality and demographic factors such as age group, sex, and study year. Materials and Methods \u2026"}, {"title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "link": "https://arxiv.org/pdf/2407.21417", "details": "Z Wu, Y Zhang, P Qi, Y Xu, R Han, Y Zhang, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (ie, follow open-ended instructions) and \u2026"}, {"title": "How do you know that? Teaching Generative Language Models to Reference Answers to Biomedical Questions", "link": "https://arxiv.org/pdf/2407.05015", "details": "B Ba\u0161aragin, A Ljaji\u0107, D Medvecki, L Cassano\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have recently become the leading source of answers for users' questions online. Despite their ability to offer eloquent answers, their accuracy and reliability can pose a significant challenge. This is especially true for \u2026"}, {"title": "Model Attribution in Machine-Generated Disinformation: A Domain Generalization Approach with Supervised Contrastive Learning", "link": "https://arxiv.org/pdf/2407.21264", "details": "A Beigi, Z Tan, N Mudiam, C Chen, K Shu, H Liu - arXiv preprint arXiv:2407.21264, 2024", "abstract": "Model attribution for machine-generated disinformation poses a significant challenge in understanding its origins and mitigating its spread. This task is especially challenging because modern large language models (LLMs) produce disinformation \u2026"}, {"title": "Multi-group Uncertainty Quantification for Long-form Text Generation", "link": "https://arxiv.org/pdf/2407.21057", "details": "T Liu, ZS Wu - arXiv preprint arXiv:2407.21057, 2024", "abstract": "While large language models are rapidly moving towards consumer-facing applications, they are often still prone to factual errors and hallucinations. In order to reduce the potential harms that may come from these errors, it is important for users \u2026"}, {"title": "Towards Trustworthy AI in Cardiology: A Comparative Analysis of Explainable AI Methods for Electrocardiogram Interpretation", "link": "https://link.springer.com/chapter/10.1007/978-3-031-66535-6_36", "details": "N Gumpfer, B Dinov, S Sossalla, M Guckert, J Hannig - International Conference on \u2026, 2024", "abstract": "Deep learning models have successfully been applied for medical diagnoses. However, such models are perceived as opaque black boxes and their decisions are not comprehensible for human users. This intransparency and the potential resulting \u2026"}, {"title": "A Single Transformer for Scalable Vision-Language Modeling", "link": "https://arxiv.org/pdf/2407.06438", "details": "Y Chen, X Wang, H Peng, H Ji - arXiv preprint arXiv:2407.06438, 2024", "abstract": "We present SOLO, a single transformer for Scalable visiOn-Language mOdeling. Current large vision-language models (LVLMs) such as LLaVA mostly employ heterogeneous architectures that connect pre-trained visual encoders with large \u2026"}, {"title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models", "link": "https://arxiv.org/pdf/2407.04693", "details": "Y Gu, Z Ji, W Zhang, C Lyu, D Lin, K Chen - arXiv preprint arXiv:2407.04693, 2024", "abstract": "Large language models (LLMs) exhibit hallucinations in long-form question- answering tasks across various domains and wide applications. Current hallucination detection and mitigation datasets are limited in domains and sizes \u2026"}]
