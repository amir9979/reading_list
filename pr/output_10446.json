[{"title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models", "link": "https://arxiv.org/pdf/2412.04467", "details": "S Yang, Y Chen, Z Tian, C Wang, J Li, B Yu, J Jia - arXiv preprint arXiv:2412.04467, 2024", "abstract": "Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens \u2026"}, {"title": "ChatGPT based contrastive learning for radiology report summarization", "link": "https://www.sciencedirect.com/science/article/pii/S0957417424026940", "details": "Z Luo, Z Jiang, M Wang, X Cai, D Gao, L Yang - Expert Systems with Applications, 2024", "abstract": "Abstract Automatically Impression Generation (AIG) can conclude essential information of the \u201cFindings\u201d section, thus facilitating more effective communication between radiographers and physicians. Different from general abstractive \u2026"}, {"title": "AdvDreamer Unveils: Are Vision-Language Models Truly Ready for Real-World 3D Variations?", "link": "https://arxiv.org/pdf/2412.03002", "details": "S Ruan, H Liu, Y Huang, X Wang, C Kang, H Su\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision Language Models (VLMs) have exhibited remarkable generalization capabilities, yet their robustness in dynamic real-world scenarios remains largely unexplored. To systematically evaluate VLMs' robustness to real-world 3D variations \u2026"}, {"title": "Exploring Visual Multiple-Choice Question Answering with Pre-trained Vision-Language Models", "link": "https://openaccess.thecvf.com/content/ACCV2024W/LAVA/papers/Tran_Exploring_Visual_Multiple-Choice_Question_Answering_with_Pre-trained_Vision-Language_Models_ACCVW_2024_paper.pdf", "details": "GN Tran, DT Luu - Proceedings of the Asian Conference on Computer \u2026, 2024", "abstract": "Visual question answering is a challenging task in computer vision and natural language processing that involves answering questions about an image using both visual and textual information. This task is more challenging when it comes to the \u2026"}, {"title": "Domain Aware Multi-Task Pre-Training of 3D Swin Transformer for Brain MRI", "link": "https://openaccess.thecvf.com/content/ACCV2024/papers/Kim_Domain_Aware_Multi-Task_Pre-Training_of_3D_Swin_Transformer_for_Brain_ACCV_2024_paper.pdf", "details": "J Kim, M Kim, H Park - Proceedings of the Asian Conference on Computer \u2026, 2024", "abstract": "The scarcity of annotated medical images is a major bottleneck in developing learning models for medical image analysis. Hence, recent studies have focused on pretrained models with fewer annotation requirements that can be fine-tuned for \u2026"}, {"title": "The Radiance of Neural Fields: Democratizing Photorealistic and Dynamic Robotic Simulation", "link": "https://arxiv.org/pdf/2411.16940", "details": "G Nuthall, R Bowden, O Mendez - arXiv preprint arXiv:2411.16940, 2024", "abstract": "As robots increasingly coexist with humans, they must navigate complex, dynamic environments rich in visual information and implicit social dynamics, like when to yield or move through crowds. Addressing these challenges requires significant \u2026"}, {"title": "Informed Augmentation Selection Improves Tabular Contrastive Learning", "link": "https://openreview.net/pdf%3Fid%3DGFu8qDtVQa", "details": "A Khoeini, S Peng, M Ester - NeurIPS 2024 Workshop: Self-Supervised Learning \u2026", "abstract": "While contrastive learning (CL) has demonstrated success in image data, its application to tabular data remains relatively unexplored. The effectiveness of CL heavily depends on data augmentations, yet the suitability of tabular augmentation \u2026"}, {"title": "HandsOnVLM: Vision-Language Models for Hand-Object Interaction Prediction", "link": "https://arxiv.org/pdf/2412.13187", "details": "C Bao, J Xu, X Wang, A Gupta, H Bharadhwaj - arXiv preprint arXiv:2412.13187, 2024", "abstract": "How can we predict future interaction trajectories of human hands in a scene given high-level colloquial task specifications in the form of natural language? In this paper, we extend the classic hand trajectory prediction task to two tasks involving \u2026"}, {"title": "CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.12932", "details": "Z Cheng, Q Chen, J Zhang, H Fei, X Feng, W Che, M Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) have recently demonstrated amazing success in multi-modal tasks, including advancements in Multi-modal Chain-of- Thought (MCoT) reasoning. Despite these successes, current benchmarks still follow \u2026"}]
