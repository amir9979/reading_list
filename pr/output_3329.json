[{"title": "An Empirical Study of Mamba-based Language Models", "link": "https://arxiv.org/pdf/2406.07887", "details": "R Waleffe, W Byeon, D Riach, B Norick, V Korthikanti\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value \u2026"}, {"title": "Detecting and Evaluating Medical Hallucinations in Large Vision Language Models", "link": "https://arxiv.org/pdf/2406.10185", "details": "J Chen, D Yang, T Wu, Y Jiang, X Hou, M Li, S Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision Language Models (LVLMs) are increasingly integral to healthcare applications, including medical visual question answering and imaging report generation. While these models inherit the robust capabilities of foundational Large \u2026"}, {"title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2406.13233", "details": "Z Zeng, Y Miao, H Gao, H Zhang, Z Deng - arXiv preprint arXiv:2406.13233, 2024", "abstract": "Mixture of experts (MoE) has become the standard for constructing production-level large language models (LLMs) due to its promise to boost model capacity without causing significant overheads. Nevertheless, existing MoE methods usually enforce \u2026"}, {"title": "Timo: Towards Better Temporal Reasoning for Language Models", "link": "https://arxiv.org/pdf/2406.14192", "details": "Z Su, J Zhang, T Zhu, X Qu, J Li, M Zhang, Y Cheng - arXiv preprint arXiv:2406.14192, 2024", "abstract": "Reasoning about time is essential for Large Language Models (LLMs) to understand the world. Previous works focus on solving specific tasks, primarily on time-sensitive question answering. While these methods have proven effective, they cannot \u2026"}, {"title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models", "link": "https://aclanthology.org/2024.findings-naacl.18.pdf", "details": "Z Su, Z Lin, B Baixue, H Chen, S Hu, W Zhou, G Ding\u2026 - Findings of the Association \u2026, 2024", "abstract": "Generative language models are usually pre-trained on large text corpus via predicting the next token (ie, sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language \u2026"}, {"title": "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "link": "https://arxiv.org/pdf/2406.12050", "details": "Z Zhang, Z Liang, W Yu, D Yu, M Jia, D Yu, M Jiang - arXiv preprint arXiv:2406.12050, 2024", "abstract": "Supervised fine-tuning enhances the problem-solving abilities of language models across various mathematical reasoning tasks. To maximize such benefits, existing research focuses on broadening the training set with various data augmentation \u2026"}, {"title": "Candidate Pseudolabel Learning: Enhancing Vision-Language Models by Prompt Tuning with Unlabeled Data", "link": "https://arxiv.org/pdf/2406.10502", "details": "J Zhang, Q Wei, F Liu, L Feng - arXiv preprint arXiv:2406.10502, 2024", "abstract": "Fine-tuning vision-language models (VLMs) with abundant unlabeled data recently has attracted increasing attention. Existing methods that resort to the pseudolabeling strategy would suffer from heavily incorrect hard pseudolabels when VLMs exhibit \u2026"}, {"title": "Direct Preference Knowledge Distillation for Large Language Models", "link": "https://arxiv.org/pdf/2406.19774", "details": "Y Li, Y Gu, L Dong, D Wang, Y Cheng, F Wei - arXiv preprint arXiv:2406.19774, 2024", "abstract": "In the field of large language models (LLMs), Knowledge Distillation (KD) is a critical technique for transferring capabilities from teacher models to student models. However, existing KD methods face limitations and challenges in distillation of LLMs \u2026"}, {"title": "Monitoring Latent World States in Language Models with Propositional Probes", "link": "https://arxiv.org/pdf/2406.19501", "details": "J Feng, S Russell, J Steinhardt - arXiv preprint arXiv:2406.19501, 2024", "abstract": "Language models are susceptible to bias, sycophancy, backdoors, and other tendencies that lead to unfaithful responses to the input context. Interpreting internal states of language models could help monitor and correct unfaithful behavior. We \u2026"}]
