[{"title": "Emerging Reliance Behaviors in Human-AI Text Generation: Hallucinations, Data Quality Assessment, and Cognitive Forcing Functions", "link": "https://arxiv.org/pdf/2409.08937", "details": "Z Ashktorab, Q Pan, W Geyer, M Desmond\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we investigate the impact of hallucinations and cognitive forcing functions in human-AI collaborative text generation tasks, focusing on the use of Large Language Models (LLMs) to assist in generating high-quality conversational \u2026"}, {"title": "Reasoning and Planning with Large Language Models in Code Development", "link": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671452", "details": "H Ding, Z Fan, I Guehring, G Gupta, W Ha, J Huan\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code \u2026"}, {"title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models", "link": "https://arxiv.org/pdf/2408.12798", "details": "Y Li, H Huang, Y Zhao, X Ma, J Sun - arXiv preprint arXiv:2408.12798, 2024", "abstract": "Generative Large Language Models (LLMs) have made significant strides across various tasks, but they remain vulnerable to backdoor attacks, where specific triggers in the prompt cause the LLM to generate adversary-desired responses. While most \u2026"}, {"title": "Modeling perceived information needs in human-AI teams: improving AI teammate utility and driving team cognition", "link": "https://www.tandfonline.com/doi/abs/10.1080/0144929X.2024.2396476", "details": "BG Schelble, C Flathmann, JP Macdonald\u2026 - Behaviour & Information \u2026, 2024", "abstract": "As AI technologies advance, teams are beginning to see AI transition from a tool to a full-fledged teammate. Introducing an AI teammate brings several challenges, ranging from how human teammates perceive their new AI teammates from an \u2026"}, {"title": "CRADLE-VAE: Enhancing Single-Cell Gene Perturbation Modeling with Counterfactual Reasoning-based Artifact Disentanglement", "link": "https://arxiv.org/pdf/2409.05484", "details": "S Baek, S Park, YT Chok, J Lee, J Park, M Gim, J Kang - arXiv preprint arXiv \u2026, 2024", "abstract": "Predicting cellular responses to various perturbations is a critical focus in drug discovery and personalized therapeutics, with deep learning models playing a significant role in this endeavor. Single-cell datasets contain technical artifacts that \u2026"}, {"title": "Pairwise Proximal Policy Optimization: Language Model Alignment with Comparative RL", "link": "https://openreview.net/pdf%3Fid%3D7iaAlIlV2H", "details": "T Wu, B Zhu, R Zhang, Z Wen, K Ramchandran, J Jiao - First Conference on Language \u2026", "abstract": "LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior is Reinforcement Learning with Human Feedback (RLHF). This involves training a reward model with \u2026"}, {"title": "Path-Consistency: Prefix Enhancement for Efficient Inference in LLM", "link": "https://arxiv.org/pdf/2409.01281", "details": "J Zhu, Y Shen, J Zhao, A Zou - arXiv preprint arXiv:2409.01281, 2024", "abstract": "To enhance the reasoning capabilities of large language models (LLMs), self- consistency has gained significant popularity by combining multiple sampling with majority voting. However, the state-of-the-art self-consistency approaches consume \u2026"}, {"title": "Towards Harnessing Large Language Models as Autonomous Agents for Semantic Triple Extraction from Unstructured Text", "link": "https://ceur-ws.org/Vol-3747/text2kg_paper1.pdf", "details": "A Ananya, S Tiwari, N Mihindukulasooriya, T Soru\u2026 - 2024", "abstract": "Abstract The use of Large Language Models as autonomous agents interacting with tools has shown to improve the performance of several tasks from code generation to API calling and sequencing. This paper proposes a framework for using Large \u2026"}, {"title": "Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators", "link": "https://arxiv.org/pdf/2408.12325", "details": "D Yang, D Xiao, J Wei, M Li, Z Chen, K Li, L Zhang - arXiv preprint arXiv:2408.12325, 2024", "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone to generate responses that contradict verifiable facts, ie, unfaithful hallucination content. Existing efforts generally focus on optimizing model parameters or editing semantic \u2026"}]
