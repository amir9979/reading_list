[{"title": "The Transformative Potential of Large Language Models in Mining Electronic Health Records Data: Content Analysis", "link": "https://medinform.jmir.org/2025/1/e58457", "details": "AJ Wals Zurita, H Miras del Rio\u2026 - JMIR Medical Informatics, 2025", "abstract": "Background In this study, we evaluate the accuracy, efficiency, and cost- effectiveness of large language models in extracting and structuring information from free-text clinical reports, particularly in identifying and classifying patient \u2026"}, {"title": "Wearable Accelerometer Foundation Models for Health via Knowledge Distillation", "link": "https://arxiv.org/pdf/2412.11276", "details": "S Abbaspourazad, A Mishra, J Futoma, AC Miller\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern wearable devices can conveniently and continuously record various biosignals in the many different environments of daily living, ultimately enabling a rich view of individual health. However, not all biosignals are the same: high-fidelity \u2026"}, {"title": "BenCzechMark: A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism", "link": "https://arxiv.org/pdf/2412.17933", "details": "M Fajcik, M Docekal, J Dolezal, K Ondrej, K Bene\u0161\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present BenCzechMark (BCM), the first comprehensive Czech language benchmark designed for large language models, offering diverse tasks, multiple task formats, and multiple evaluation metrics. Its scoring system is grounded in statistical \u2026"}, {"title": "Vinci: A Real-time Embodied Smart Assistant based on Egocentric Vision-Language Model", "link": "https://arxiv.org/pdf/2412.21080%3F", "details": "Y Huang, J Xu, B Pei, Y He, G Chen, L Yang, X Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce Vinci, a real-time embodied smart assistant built upon an egocentric vision-language model. Designed for deployment on portable devices such as smartphones and wearable cameras, Vinci operates in an\" always on\" mode \u2026"}, {"title": "ChartAdapter: Large Vision-Language Model for Chart Summarization", "link": "https://arxiv.org/pdf/2412.20715", "details": "P Xu, Y Ding, W Fan - arXiv preprint arXiv:2412.20715, 2024", "abstract": "Chart summarization, which focuses on extracting key information from charts and interpreting it in natural language, is crucial for generating and delivering insights through effective and accessible data analysis. Traditional methods for chart \u2026"}, {"title": "Grounding Descriptions in Images informs Zero-Shot Visual Recognition", "link": "https://arxiv.org/pdf/2412.04429", "details": "S Halbe, J Tian, KJ Joseph, JS Smith, K Stevo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language models (VLMs) like CLIP have been cherished for their ability to perform zero-shot visual recognition on open-vocabulary concepts. This is achieved by selecting the object category whose textual representation bears the highest \u2026"}]
