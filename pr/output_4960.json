[{"title": "Can Language Models Safeguard Themselves, Instantly and For Free?", "link": "https://openreview.net/pdf%3Fid%3DALRWSxT1rl", "details": "D Adila, C Shin, Y Zhang, F Sala - ICML 2024 Next Generation of AI Safety Workshop", "abstract": "Aligning pretrained language models (LMs) to handle a new safety scenario is normally difficult and expensive, often requiring access to large amounts of ground- truth preference data and substantial compute. Are these costs necessary? That is, is \u2026"}, {"title": "Reporting and Analysing the Environmental Impact of Language Models on the Example of Commonsense Question Answering with External Knowledge", "link": "https://arxiv.org/pdf/2408.01453", "details": "A Usmanova, J Huang, D Banerjee, R Usbeck - arXiv preprint arXiv:2408.01453, 2024", "abstract": "Human-produced emissions are growing at an alarming rate, causing already observable changes in the climate and environment in general. Each year global carbon dioxide emissions hit a new record, and it is reported that 0.5% of total US \u2026"}, {"title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Trustworthy Response Generation in Chinese", "link": "https://dl.acm.org/doi/pdf/10.1145/3686807", "details": "H Wang, S Zhao, Z Qiang, Z Li, C Liu, N Xi, Y Du, B Qin\u2026 - ACM Transactions on \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to \u2026"}, {"title": "Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference", "link": "https://arxiv.org/pdf/2408.01935", "details": "K Shen, M Kejriwal - arXiv preprint arXiv:2408.01935, 2024", "abstract": "Despite their impressive performance, large language models (LLMs) such as ChatGPT are known to pose important risks. One such set of risks arises from misplaced confidence, whether over-confidence or under-confidence, that the \u2026"}, {"title": "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "link": "https://arxiv.org/pdf/2408.01963", "details": "S Ackerman, E Rabinovich, E Farchi, A Anaby-Tavor - arXiv preprint arXiv \u2026, 2024", "abstract": "We evaluate the robustness of several large language models on multiple datasets. Robustness here refers to the relative insensitivity of the model's answers to meaning- preserving variants of their input. Benchmark datasets are constructed by introducing \u2026"}, {"title": "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "link": "https://arxiv.org/pdf/2408.02103", "details": "P Wang, X Wang, C Lou, S Mao, P Xie, Y Jiang - arXiv preprint arXiv:2408.02103, 2024", "abstract": "In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language \u2026"}, {"title": "SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models", "link": "https://arxiv.org/pdf/2408.02632", "details": "M Diao, R Li, S Liu, G Liao, J Wang, X Cai, W Xu - arXiv preprint arXiv:2408.02632, 2024", "abstract": "As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to \u2026"}, {"title": "Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?", "link": "https://arxiv.org/pdf/2408.02651", "details": "MB Karkevandi, N Vishwamitra, P Najafirad - arXiv preprint arXiv:2408.02651, 2024", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in natural language tasks, but their safety and morality remain contentious due to their training on internet text corpora. To address these concerns, alignment techniques \u2026"}, {"title": "Reinforced Prompt Personalization for Recommendation with Large Language Models", "link": "https://arxiv.org/pdf/2407.17115", "details": "W Mao, J Wu, W Chen, C Gao, X Wang, X He - arXiv preprint arXiv:2407.17115, 2024", "abstract": "Designing effective prompts can empower LLMs to understand user preferences and provide recommendations by leveraging LLMs' intent comprehension and knowledge utilization capabilities. However, existing research predominantly \u2026"}]
