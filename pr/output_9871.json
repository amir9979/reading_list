[{"title": "Liquid: Language Models are Scalable Multi-modal Generators", "link": "https://arxiv.org/pdf/2412.04332", "details": "J Wu, Y Jiang, C Ma, Y Liu, H Zhao, Z Yuan, S Bai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Liquid, an auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared \u2026"}, {"title": "NVILA: Efficient Frontier Visual Language Models", "link": "https://arxiv.org/pdf/2412.04468", "details": "Z Liu, L Zhu, B Shi, Z Zhang, Y Lou, S Yang, H Xi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and \u2026"}, {"title": "On Adversarial Robustness of Language Models in Transfer Learning", "link": "https://openreview.net/forum%3Fid%3DDUMVB9a9sX", "details": "B Turbal, A Mazur, J Zhao, M Pechenizkiy - Workshop on Socially Responsible Language \u2026", "abstract": "We investigate the adversarial robustness of LLMs in transfer learning scenarios. Through comprehensive experiments on multiple datasets (MBIB Hate Speech, MBIB Political Bias, MBIB Gender Bias) and various model architectures (BERT, RoBERTa \u2026"}, {"title": "Steering Language Model Refusal with Sparse Autoencoders", "link": "https://arxiv.org/pdf/2411.11296", "details": "K O'Brien, D Majercak, X Fernandes, R Edgar, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Responsible practices for deploying language models include guiding models to recognize and refuse answering prompts that are considered unsafe, while complying with safe prompts. Achieving such behavior typically requires updating \u2026"}, {"title": "Decoding Report Generators: A Cyclic Vision-Language Adapter for Counterfactual Explanations", "link": "https://arxiv.org/pdf/2411.05261", "details": "Y Fang, Z Jin, S Guo, J Liu, Y Gao, J Ning, Z Yue, Z Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite significant advancements in report generation methods, a critical limitation remains: the lack of interpretability in the generated text. This paper introduces an innovative approach to enhance the explainability of text generated by report \u2026"}]
