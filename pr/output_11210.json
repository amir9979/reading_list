[{"title": "Low-Rank Adaptation with Task-Relevant Feature Enhancement for Fine-tuning Language Models", "link": "https://arxiv.org/pdf/2412.09827", "details": "C Li, C Ding, K Luan, X Di - arXiv preprint arXiv:2412.09827, 2024", "abstract": "Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. LoRA is one of the most widely used methods, which assumes that the optimization process is essentially low \u2026"}, {"title": "Enhancing radiology report generation through pre-trained language models", "link": "https://link.springer.com/article/10.1007/s13748-024-00358-5", "details": "G Leonardi, L Portinale, A Santomauro - Progress in Artificial Intelligence, 2024", "abstract": "In the healthcare field, the ability to integrate and process data from various modalities, such as medical images, clinical notes, and patient records, plays a central role in enabling Artificial Intelligence models to provide more informed \u2026"}, {"title": "Synthetic Vision: Training Vision-Language Models to Understand Physics", "link": "https://arxiv.org/pdf/2412.08619", "details": "V Balazadeh, M Ataei, H Cheong, AH Khasahmadi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Physical reasoning, which involves the interpretation, understanding, and prediction of object behavior in dynamic environments, remains a significant challenge for current Vision-Language Models (VLMs). In this work, we propose two methods to \u2026"}, {"title": "HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.08378", "details": "S Zhu, W Dong, J Song, Y Guo, B Zheng - arXiv preprint arXiv:2412.08378, 2024", "abstract": "Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into \u2026"}, {"title": "Dynamically Scaled Temperature in Self-Supervised Contrastive Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10820841/", "details": "S Manna, S Chattopadhyay, R Dey, U Pal\u2026 - IEEE Transactions on \u2026, 2025", "abstract": "In contemporary self-supervised contrastive algorithms like SimCLR, MoCo, etc., the task of balancing attraction between two semantically similar samples and repulsion between two samples of different classes is primarily affected by the presence of \u2026"}, {"title": "Evaluating the performance of artificial intelligence in supporting evidence synthesis: a blinded comparison between chatbots and humans", "link": "https://www.researchsquare.com/article/rs-5710671/latest.pdf", "details": "K Nordmann, S Sauter, M Stein, J Aigner, MC Redlich\u2026 - 2025", "abstract": "Background With the rise of large language models, the application of artificial intelligence in research is expanding, possibly accelerating specific stages of the research processes. This study aims to compare the accuracy, completeness and \u2026"}, {"title": "Benchmarking and Improving Large Vision-Language Models for Fundamental Visual Graph Understanding and Reasoning", "link": "https://arxiv.org/pdf/2412.13540", "details": "Y Zhu, X Bai, K Chen, Y Xiang, M Zhang - arXiv preprint arXiv:2412.13540, 2024", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance across diverse tasks. Despite great success, recent studies show that LVLMs encounter substantial limitations when engaging with visual graphs. To study \u2026"}, {"title": "From Uncertainty to Trust: Enhancing Reliability in Vision-Language Models with Uncertainty-Guided Dropout Decoding", "link": "https://arxiv.org/pdf/2412.06474", "details": "Y Fang, Z Yang, Z Chen, Z Zhao, J Zhou - arXiv preprint arXiv:2412.06474, 2024", "abstract": "Large vision-language models (LVLMs) demonstrate remarkable capabilities in multimodal tasks but are prone to misinterpreting visual inputs, often resulting in hallucinations and unreliable outputs. To address these challenges, we propose \u2026"}, {"title": "Overview of the First Workshop on Language Models for Low-Resource Languages (LoResLM 2025)", "link": "https://arxiv.org/pdf/2412.16365", "details": "H Hettiarachchi, T Ranasinghe, P Rayson, R Mitkov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The first Workshop on Language Models for Low-Resource Languages (LoResLM 2025) was held in conjunction with the 31st International Conference on Computational Linguistics (COLING 2025) in Abu Dhabi, United Arab Emirates. This \u2026"}]
