[{"title": "VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents", "link": "https://arxiv.org/pdf/2408.06327", "details": "X Liu, T Zhang, Y Gu, IL Iong, Y Xu, X Song, S Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Multimodal Models (LMMs) have ushered in a new era in artificial intelligence, merging capabilities in both language and vision to form highly capable Visual Foundation Agents. These agents are postulated to excel across a myriad of tasks \u2026"}, {"title": "PenHeal: A Two-Stage LLM Framework for Automated Pentesting and Optimal Remediation", "link": "https://arxiv.org/pdf/2407.17788", "details": "J Huang, Q Zhu - arXiv preprint arXiv:2407.17788, 2024", "abstract": "Recent advances in Large Language Models (LLMs) have shown significant potential in enhancing cybersecurity defenses against sophisticated threats. LLM- based penetration testing is an essential step in automating system security \u2026"}, {"title": "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents", "link": "https://arxiv.org/pdf/2407.18901", "details": "H Trivedi, T Khot, M Hartmann, R Manku, V Dong, E Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Autonomous agents that address day-to-day digital tasks (eg, ordering groceries for a household), must not only operate multiple apps (eg, notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative \u2026"}, {"title": "Large Language Models Prompting With Episodic Memory", "link": "https://arxiv.org/pdf/2408.07465", "details": "D Do, Q Tran, S Venkatesh, H Le - arXiv preprint arXiv:2408.07465, 2024", "abstract": "Prompt optimization is essential for enhancing the performance of Large Language Models (LLMs) in a range of Natural Language Processing (NLP) tasks, particularly in scenarios of few-shot learning where training examples are incorporated directly \u2026"}, {"title": "Jellybell at textgraphs-17 shared task: Fusing large language models with external knowledge for enhanced question answering", "link": "https://aclanthology.org/2024.textgraphs-1.15.pdf", "details": "J Belikova, E Beliakin, V Konovalov - Proceedings of TextGraphs-17: Graph-based \u2026, 2024", "abstract": "This work describes an approach to develop Knowledge Graph Question Answering (KGQA) system for TextGraphs-17 shared task. The task focuses on the fusion of Large Language Models (LLMs) with Knowledge Graphs (KGs). The goal is to select \u2026"}, {"title": "Jailbreak Open-Sourced Large Language Models via Enforced Decoding", "link": "https://aclanthology.org/2024.acl-long.299.pdf", "details": "H Zhang, Z Guo, H Zhu, B Cao, L Lin, J Jia, J Chen\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In \u2026"}, {"title": "Relation labeling in product knowledge graphs with large language models for e-commerce", "link": "https://link.springer.com/article/10.1007/s13042-024-02274-5", "details": "J Chen, L Ma, X Li, J Xu, JHD Cho, K Nag, E Korpeoglu\u2026 - International Journal of \u2026, 2024", "abstract": "Abstract Product Knowledge Graphs (PKGs) play a crucial role in enhancing e- commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between \u2026"}, {"title": "Mitigating Privacy Seesaw in Large Language Models: Augmented Privacy Neuron Editing via Activation Patching", "link": "https://aclanthology.org/2024.findings-acl.315.pdf", "details": "X Wu, W Dong, S Xu, D Xiong - Findings of the Association for Computational \u2026, 2024", "abstract": "Protecting privacy leakage in large language models remains a paramount challenge. In this paper, we reveal Privacy Seesaw in LLM privacy safeguarding, a phenomenon where measures to secure specific private information inadvertently \u2026"}, {"title": "Enhancing Discriminative Tasks by Guiding the Pre-trained Language Model with Large Language Model's Experience", "link": "https://arxiv.org/pdf/2408.08553", "details": "X Yin, C Ni, X Xu, X Li, X Yang - arXiv preprint arXiv:2408.08553, 2024", "abstract": "Large Language Models (LLMs) and pre-trained Language Models (LMs) have achieved impressive success on many software engineering tasks (eg, code completion and code generation). By leveraging huge existing code corpora (eg \u2026"}]
