[{"title": "Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image Analysis", "link": "https://arxiv.org/pdf/2503.20047%3F", "details": "Y Xin, GC Ates, K Gong, W Shao - arXiv preprint arXiv:2503.20047, 2025", "abstract": "Vision-language models (VLMs) have shown promise in 2D medical image analysis, but extending them to 3D remains challenging due to the high computational demands of volumetric data and the difficulty of aligning 3D spatial features with \u2026"}, {"title": "Improving Sequential Recommenders through Counterfactual Augmentation of System Exposure", "link": "https://arxiv.org/pdf/2504.13482", "details": "Z Zhao, Z Ren, J Yang, Z Yan, Z Wang, L Yang, P Ren\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In sequential recommendation (SR), system exposure refers to items that are exposed to the user. Typically, only a few of the exposed items would be interacted with by the user. Although SR has achieved great success in predicting future user \u2026"}, {"title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering", "link": "https://arxiv.org/pdf/2504.13828", "details": "S Xia, Y Qin, X Li, Y Ma, RZ Fan, S Chern, H Zou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The first generation of Large Language Models-what might be called\" Act I\" of generative AI (2020-2023)-achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations in knowledge \u2026"}, {"title": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning", "link": "https://arxiv.org/pdf/2504.13500", "details": "J Wang, J Jiang, Y Liu, M Zhang, X Cai - arXiv preprint arXiv:2504.13500, 2025", "abstract": "In this paper, we introduce a new\\emph {process prejudge} strategy in LLM reasoning to demonstrate that bootstrapping with process prejudge allows the LLM to adaptively anticipate the errors encountered when advancing the subsequent \u2026"}, {"title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning", "link": "https://arxiv.org/pdf/2504.00406", "details": "J Han, W Buntine, E Shareghi - arXiv preprint arXiv:2504.00406, 2025", "abstract": "Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources \u2026"}, {"title": "CSPO: chain-structured prompt optimisation for large language models", "link": "https://www.inderscienceonline.com/doi/abs/10.1504/IJAHUC.2025.145202", "details": "J Wang, S Lin, X Xue, S Chen, Z Tang - International Journal of Ad Hoc and \u2026, 2025", "abstract": "Large language models (LLMs) show promise in improving content distribution in mobile communication networks, but their performance heavily depends on input prompts. Manually crafting effective prompts for complex tasks is time-consuming \u2026"}]
