[{"title": "Retrieval Augmented Deep Anomaly Detection for Tabular Data", "link": "https://dl.acm.org/doi/pdf/10.1145/3627673.3679559", "details": "H Thimonier, F Popineau, A Rimmel, BL Doan - Proceedings of the 33rd ACM \u2026, 2024", "abstract": "Deep learning for tabular data has garnered increasing attention in recent years, yet employing deep models for structured data remains challenging. While these models excel with unstructured data, their efficacy with structured data has been limited \u2026"}, {"title": "IdenBAT: Disentangled Representation Learning for Identity-Preserved Brain Age Transformation", "link": "https://arxiv.org/pdf/2410.16945", "details": "J Maeng, K Oh, W Jung, HI Suk - arXiv preprint arXiv:2410.16945, 2024", "abstract": "Brain age transformation aims to convert reference brain images into synthesized images that accurately reflect the age-specific features of a target age group. The primary objective of this task is to modify only the age-related attributes of the \u2026"}, {"title": "GMNI: Achieve good data augmentation in unsupervised graph contrastive learning", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007287", "details": "X Xiong, X Wang, S Yang, F Shen, J Zhao - Neural Networks, 2024", "abstract": "Graph contrastive learning (GCL) shows excellent potential in unsupervised graph representation learning. Data augmentation (DA), responsible for generating diverse views, plays a vital role in GCL, and its optimal choice heavily depends on the \u2026"}, {"title": "Learning Global Object-Centric Representations via Disentangled Slot Attention", "link": "https://arxiv.org/pdf/2410.18809", "details": "T Chen, Y Huang, Z Shen, J Huang, B Li, X Xue - arXiv preprint arXiv:2410.18809, 2024", "abstract": "Humans can discern scene-independent features of objects across various environments, allowing them to swiftly identify objects amidst changing factors such as lighting, perspective, size, and position and imagine the complete images of the \u2026"}, {"title": "Failure-Proof Non-Contrastive Self-Supervised Learning", "link": "https://arxiv.org/pdf/2410.04959", "details": "E Sansone, T Lebailly, T Tuytelaars - arXiv preprint arXiv:2410.04959, 2024", "abstract": "We identify sufficient conditions to avoid known failure modes, including representation, dimensional, cluster and intracluster collapses, occurring in non- contrastive self-supervised learning. Based on these findings, we propose a \u2026"}]
