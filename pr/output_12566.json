[{"title": "Beyond Topological Self-Explainable GNNs: A Formal Explainability Perspective", "link": "https://arxiv.org/pdf/2502.02719", "details": "S Azzolin, S Malhotra, A Passerini, S Teso - arXiv preprint arXiv:2502.02719, 2025", "abstract": "Self-Explainable Graph Neural Networks (SE-GNNs) are popular explainable-by- design GNNs, but the properties and the limitations of their explanations are not well understood. Our first contribution fills this gap by formalizing the explanations \u2026"}, {"title": "Graph contrastive learning of modeling global-local interactions under hierarchical strategy: Application in anomaly detection", "link": "https://www.sciencedirect.com/science/article/pii/S0957582025001387", "details": "W Guo, Y Wang, L Zhou, M Jia, Y Liu - Process Safety and Environmental Protection, 2025", "abstract": "Lack of labeled samples and complexity of unit interactions pose significant challenges for effective anomaly detection in complex industrial processes. This work proposes an unsupervised anomaly detection framework, hierarchical strategy \u2026"}, {"title": "Contrastive Learning for Efficient Anomaly Detection in Electricity Load Data", "link": "https://www.sciencedirect.com/science/article/pii/S2352467725000219", "details": "M Choubey, RK Chaurasiya, JS Yadav - Sustainable Energy, Grids and Networks, 2025", "abstract": "Identifying irregularities in electricity load data is essential for maintaining dependable and effective power systems. Traditional approaches necessitate a significant amount of labeled data in order to achieve high accuracy, resulting in \u2026"}, {"title": "A Revisit of Total Correlation in Disentangled Variational Auto-Encoder with Partial Disentanglement", "link": "https://arxiv.org/pdf/2502.02279", "details": "C Li, Y Wang, Y Wang, W Li, D Jaeger, A Wu - arXiv preprint arXiv:2502.02279, 2025", "abstract": "A fully disentangled variational auto-encoder (VAE) aims to identify disentangled latent components from observations. However, enforcing full independence between all latent components may be too strict for certain datasets. In some cases \u2026"}, {"title": "Masked Autoencoders Are Effective Tokenizers for Diffusion Models", "link": "https://arxiv.org/abs/2502.03444", "details": "H Chen, Y Han, F Chen, X Li, Y Wang, J Wang, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in latent diffusion models have demonstrated their effectiveness for high-resolution image synthesis. However, the properties of the latent space from tokenizer for better learning and generation of diffusion models remain under \u2026"}, {"title": "Supervision-free Vision-Language Alignment", "link": "https://arxiv.org/pdf/2501.04568%3F", "details": "G Giannone, R Li, Q Feng, E Perevodchikov, R Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) have demonstrated remarkable potential in integrating visual and linguistic information, but their performance is often constrained by the need for extensive, high-quality image-text training data. Curation \u2026"}, {"title": "Sparse Data Generation Using Diffusion Models", "link": "https://arxiv.org/pdf/2502.02448", "details": "P Ostheimer, M Nagda, M Kloft, S Fellenz - arXiv preprint arXiv:2502.02448, 2025", "abstract": "Sparse data is ubiquitous, appearing in numerous domains, from economics and recommender systems to astronomy and biomedical sciences. However, efficiently and realistically generating sparse data remains a significant challenge. We \u2026"}, {"title": "Calibrated Multi-Preference Optimization for Aligning Diffusion Models", "link": "https://arxiv.org/pdf/2502.02588", "details": "K Lee, X Li, Q Wang, J He, J Ke, MH Yang, I Essa\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Aligning text-to-image (T2I) diffusion models with preference optimization is valuable for human-annotated datasets, but the heavy cost of manual data collection limits scalability. Using reward models offers an alternative, however, current preference \u2026"}]
