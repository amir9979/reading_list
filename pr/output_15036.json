[{"title": "Can Language Models Follow Multiple Turns of Entangled Instructions?", "link": "https://arxiv.org/pdf/2503.13222%3F", "details": "C Han - arXiv preprint arXiv:2503.13222, 2025", "abstract": "Despite significant achievements in improving the instruction-following capabilities of large language models (LLMs), the ability to process multiple potentially entangled or conflicting instructions remains a considerable challenge. Real-world scenarios \u2026"}, {"title": "Understanding hospital activity and outcomes for people with multimorbidity using electronic health records", "link": "https://www.nature.com/articles/s41598-025-92940-7", "details": "K Georgiev, J McPeake, SD Shenkin, J Fleuriot, N Lone\u2026 - Scientific Reports, 2025", "abstract": "As the prevalence of multimorbidity grows, provision of effective healthcare is more challenging. Both multimorbidity and complexity in healthcare delivery may be associated with worse outcomes. We studied consecutive, unique emergency non \u2026"}, {"title": "A Lightweight Large Vision-language Model for Multimodal Medical Images", "link": "https://arxiv.org/pdf/2504.05575", "details": "B Alsinglawi, C McCarthy, S Webb, C Fluke, NT Saidy - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical Visual Question Answering (VQA) enhances clinical decision-making by enabling systems to interpret medical images and answer clinical queries. However, developing efficient, high-performance VQA models is challenging due to the \u2026"}, {"title": "Knowledge-Instruct: Effective Continual Pre-training from Limited Data using Instructions", "link": "https://arxiv.org/pdf/2504.05571", "details": "O Ovadia, M Brief, R Lemberg, E Sheetrit - arXiv preprint arXiv:2504.05571, 2025", "abstract": "While Large Language Models (LLMs) acquire vast knowledge during pre-training, they often lack domain-specific, new, or niche information. Continual pre-training (CPT) attempts to address this gap but suffers from catastrophic forgetting and \u2026"}, {"title": "SWI: Speaking with Intent in Large Language Models", "link": "https://arxiv.org/pdf/2503.21544%3F", "details": "Y Yin, EJ Hwang, G Carenini - arXiv preprint arXiv:2503.21544, 2025", "abstract": "Intent, typically clearly formulated and planned, functions as a cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated \u2026"}, {"title": "Corrective In-Context Learning: Evaluating Self-Correction in Large Language Models", "link": "https://arxiv.org/pdf/2503.16022%3F", "details": "M Sanz-Guerrero, K von der Wense - arXiv preprint arXiv:2503.16022, 2025", "abstract": "In-context learning (ICL) has transformed the use of large language models (LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled examples without finetuning. Despite its effectiveness, ICL is prone to errors, especially for \u2026"}]
