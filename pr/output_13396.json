[{"title": "FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models", "link": "https://arxiv.org/pdf/2502.18573", "details": "R Marinescu, D Bhattacharjya, J Lee, T Tchrakian\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have demonstrated vast capabilities on generative tasks in recent years, yet they struggle with guaranteeing the factual correctness of the generated content. This makes these models unreliable in realistic situations \u2026"}, {"title": "Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models", "link": "https://arxiv.org/pdf/2502.19160", "details": "R G\u00f6rge, M Mock, H Allende-Cid - arXiv preprint arXiv:2502.19160, 2025", "abstract": "Social categories and stereotypes are embedded in language and can introduce data bias into Large Language Models (LLMs). Despite safeguards, these biases often persist in model behavior, potentially leading to representational harm in \u2026"}]
