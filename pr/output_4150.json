[{"title": "EHRmonize: A Framework for Medical Concept Abstraction from Electronic Health Records using Large Language Models", "link": "https://arxiv.org/pdf/2407.00242", "details": "J Matos, J Gallifant, J Pei, AI Wong - arXiv preprint arXiv:2407.00242, 2024", "abstract": "Electronic health records (EHRs) contain vast amounts of complex data, but harmonizing and processing this information remains a challenging and costly task requiring significant clinical expertise. While large language models (LLMs) have \u2026"}, {"title": "PharmGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry", "link": "https://arxiv.org/pdf/2406.18045", "details": "L Chen, W Wang, Z Bai, P Xu, Y Fang, J Fang, W Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have revolutionized Natural Language Processing (NLP) by by minimizing the need for complex feature engineering. However, the application of LLMs in specialized domains like biopharmaceuticals and chemistry \u2026"}, {"title": "Exploring Universal Intrinsic Task Subspace for Few-shot Learning via Prompt Tuning", "link": "https://ieeexplore.ieee.org/iel8/6570655/6633080/10603438.pdf", "details": "Y Qin, X Wang, Y Su, Y Lin, N Ding, J Yi, W Chen, Z Liu\u2026 - IEEE/ACM Transactions on \u2026, 2024", "abstract": "Why can pre-trained language models (PLMs) learn universal representations and effectively adapt to broad NLP tasks differing a lot superficially? In this work, we empirically find evidence indicating that the adaptations of PLMs to various fewshot \u2026"}]
