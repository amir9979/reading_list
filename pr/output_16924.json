[{"title": "Misaligning Reasoning with Answers -- A Framework for Assessing LLM CoT Robustness", "link": "https://arxiv.org/pdf/2505.17406", "details": "E Jiang, C Xu, N Singh, G Singh - arXiv preprint arXiv:2505.17406, 2025", "abstract": "LLMs' decision-making process is opaque, prompting the need for explanation techniques like Chain-of-Thought. To investigate the relationship between answer and reasoning, we design a novel evaluation framework, MATCHA. In domains like \u2026", "entry_id": "http://arxiv.org/abs/2505.17406v1", "updated": "2025-05-23 02:42:16", "published": "2025-05-23 02:42:16", "authors": "Enyi Jiang;Changming Xu;Nischay Singh;Gagandeep Singh", "summary": "LLMs' decision-making process is opaque, prompting the need for explanation\ntechniques like Chain-of-Thought. To investigate the relationship between\nanswer and reasoning, we design a novel evaluation framework, MATCHA. In\ndomains like education and healthcare, reasoning is key for model\ntrustworthiness. MATCHA reveals that LLMs under input perturbations can give\ninconsistent or nonsensical reasoning. Additionally, we use LLM judges to\nassess reasoning robustness across models. Our results show that LLMs exhibit\ngreater vulnerability to input perturbations for multi-step and commonsense\ntasks than compared to logical tasks. Also, we show non-trivial transfer rates\nof our successful examples to black-box models. Our evaluation framework helps\nto better understand LLM reasoning mechanisms and guides future models toward\nmore robust and reasoning-driven architectures, enforcing answer-reasoning\nconsistency.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.17406v1;http://arxiv.org/pdf/2505.17406v1", "pdf_url": "http://arxiv.org/pdf/2505.17406v1"}, {"title": "ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction", "link": "https://arxiv.org/pdf/2505.17691", "details": "Y Yu, Y Liu, M He, S Tao, W Meng, X Yang, L Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 The typical **LLM** **evaluation** systems rely on the transitivity assumption, that is, if the evaluation results are A \u227b B and B \u227b C, then it is assumed that A \u227b C . However, evaluations in the real world often reveal non-transitivity preference cycles (A \u227b B \u227b \u2026", "entry_id": "http://arxiv.org/abs/2505.17691v1", "updated": "2025-05-23 10:00:03", "published": "2025-05-23 10:00:03", "authors": "Yan Yu;Yilun Liu;Minggui He;Shimin Tao;Weibin Meng;Xinhua Yang;Li Zhang;Hongxia Ma;Chang Su;Hao Yang;Fuliang Li", "summary": "Large language models (LLMs) are widely used as evaluators for open-ended\ntasks, while previous research has emphasized biases in LLM evaluations, the\nissue of non-transitivity in pairwise comparisons remains unresolved:\nnon-transitive preferences for pairwise comparisons, where evaluators prefer A\nover B, B over C, but C over A. Our results suggest that low-quality training\ndata may reduce the transitivity of preferences generated by the Evaluator LLM.\nTo address this, We propose a graph-theoretic framework to analyze and mitigate\nthis problem by modeling pairwise preferences as tournament graphs. We quantify\nnon-transitivity and introduce directed graph structural entropy to measure the\noverall clarity of preferences. Our analysis reveals significant\nnon-transitivity in advanced Evaluator LLMs (with Qwen2.5-Max exhibiting\n67.96%), as well as high entropy values (0.8095 for Qwen2.5-Max), reflecting\nlow overall clarity of preferences. To address this issue, we designed a\nfiltering strategy, ELSPR, to eliminate preference data that induces\nnon-transitivity, retaining only consistent and transitive preference data for\nmodel fine-tuning. Experiments demonstrate that models fine-tuned with filtered\ndata reduce non-transitivity by 13.78% (from 64.28% to 50.50%), decrease\nstructural entropy by 0.0879 (from 0.8113 to 0.7234), and align more closely\nwith human evaluators (human agreement rate improves by 0.6% and Spearman\ncorrelation increases by 0.01).", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.17691v1;http://arxiv.org/pdf/2505.17691v1", "pdf_url": "http://arxiv.org/pdf/2505.17691v1"}, {"title": "Large Language Models and Surgical Decision-Making: Evaluation of Generative Unimodal AI in Facial Traumatology Practice", "link": "https://link.springer.com/article/10.1007/s12663-025-02556-7", "details": "S Benedetti, A Frosolini, L Catarzi, LA Vaira, G Consorti\u2026 - Journal of Maxillofacial and \u2026, 2025", "abstract": "\u2026 A recent systematic review of **LLM** **evaluation** methods by Bedi et al. (2024) highlighted critical gaps in the evaluation of LLMs in healthcare, with accuracy (95.4%) and comprehensiveness (47.0%) being the most commonly assessed dimensions \u2026"}, {"title": "Content Moderation in TV Search: Balancing Policy Compliance, Relevance, and User Experience", "link": "https://arxiv.org/pdf/2505.17207", "details": "A Hande, K Sundararajan, S Hamidian, F Ture - arXiv preprint arXiv:2505.17207, 2025", "abstract": "\u2026 The weighting factor \ud835\udefc balances historical lexicon scores\u2019 relevancy with real-time **LLM** **evaluation** feedback, preventing sudden shifts in \u2026 Over an eight-week evaluation, the system collected 1,814 queryresult pairs validated as true positives (TPs) \u2026", "entry_id": "http://arxiv.org/abs/2505.17207v1", "updated": "2025-05-22 18:32:39", "published": "2025-05-22 18:32:39", "authors": "Adeep Hande;Kishorekumar Sundararajan;Sardar Hamidian;Ferhan Ture", "summary": "Millions of people rely on search functionality to find and explore content\non entertainment platforms. Modern search systems use a combination of\ncandidate generation and ranking approaches, with advanced methods leveraging\ndeep learning and LLM-based techniques to retrieve, generate, and categorize\nsearch results. Despite these advancements, search algorithms can still surface\ninappropriate or irrelevant content due to factors like model unpredictability,\nmetadata errors, or overlooked design flaws. Such issues can misalign with\nproduct goals and user expectations, potentially harming user trust and\nbusiness outcomes. In this work, we introduce an additional monitoring layer\nusing Large Language Models (LLMs) to enhance content moderation. This\nadditional layer flags content if the user did not intend to search for it.\nThis approach serves as a baseline for product quality assurance, with\ncollected feedback used to refine the initial retrieval mechanisms of the\nsearch model, ensuring a safer and more reliable user experience.", "comment": "Accepted at SIGIR 2025 Industry Track. 5 pages, 1 figure, 2 tables.\n  DOI: 10.1145/3726302.3731962", "journal_ref": null, "primary_category": "cs.IR", "categories": "cs.IR;cs.LG", "links": "http://dx.doi.org/10.1145/3726302.3731962;http://arxiv.org/abs/2505.17207v1;http://arxiv.org/pdf/2505.17207v1", "pdf_url": "http://arxiv.org/pdf/2505.17207v1"}, {"title": "Automated Capability Evaluation of Foundation Models", "link": "https://arxiv.org/pdf/2505.17228", "details": "A Afkanpour, O Dige, F Tavakoli - arXiv preprint arXiv:2505.17228, 2025", "abstract": "\u2026 There is a clear trend toward automating and scaling up **LLM** **evaluation**. Traditional benchmarks like BIG-bench (Srivastava et al.\u2026 Examining the robustness of **llm** **evaluation** to the distributional assumptions of benchmarks. In \u2026", "entry_id": "http://arxiv.org/abs/2505.17228v1", "updated": "2025-05-22 19:09:57", "published": "2025-05-22 19:09:57", "authors": "Arash Afkanpour;Omkar Dige;Fatemeh Tavakoli", "summary": "Current evaluation frameworks for foundation models rely heavily on fixed,\nmanually curated benchmarks, limiting their ability to capture the full breadth\nof model capabilities. This paper introduces Active learning for Capability\nEvaluation (ACE), a novel framework for scalable, automated, and fine-grained\nevaluation of foundation models. ACE leverages the knowledge embedded in\npowerful language models to decompose a domain into semantically meaningful\ncapabilities and generate diverse evaluation tasks, significantly reducing\nhuman effort. To maximize coverage and efficiency, ACE models a subject model's\nperformance as a capability function over a latent semantic space and uses\nactive learning to prioritize the evaluation of the most informative\ncapabilities. This adaptive evaluation strategy enables cost-effective\ndiscovery of strengths, weaknesses, and failure modes that static benchmarks\nmay miss. Our results suggest that ACE provides a more complete and informative\npicture of model capabilities, which is essential for safe and well-informed\ndeployment of foundation models.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.17228v1;http://arxiv.org/pdf/2505.17228v1", "pdf_url": "http://arxiv.org/pdf/2505.17228v1"}, {"title": "Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector", "link": "https://arxiv.org/pdf/2505.17100", "details": "H Yang, R Bao, C Xiao, J Ma, P Bhatia, S Gao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 To address these gaps, we propose a new approach that introduces a Reasoning-Based Bias Detector (RBD) to identify potential biases in **LLM** **evaluation** and generate reasoning-based analyses to assist the evaluator in self-reflection. Instead of directly \u2026", "entry_id": "http://arxiv.org/abs/2505.17100v1", "updated": "2025-05-21 07:23:05", "published": "2025-05-21 07:23:05", "authors": "Haoyan Yang;Runxue Bao;Cao Xiao;Jun Ma;Parminder Bhatia;Shangqian Gao;Taha Kass-Hout", "summary": "LLM-as-a-Judge has emerged as a promising tool for automatically evaluating\ngenerated outputs, but its reliability is often undermined by potential biases\nin judgment. Existing efforts to mitigate these biases face key limitations:\nin-context learning-based methods fail to address rooted biases due to the\nevaluator's limited capacity for self-reflection, whereas fine-tuning is not\napplicable to all evaluator types, especially closed-source models. To address\nthis challenge, we introduce the Reasoning-based Bias Detector (RBD), which is\na plug-in module that identifies biased evaluations and generates structured\nreasoning to guide evaluator self-correction. Rather than modifying the\nevaluator itself, RBD operates externally and engages in an iterative process\nof bias detection and feedback-driven revision. To support its development, we\ndesign a complete pipeline consisting of biased dataset construction,\nsupervision collection, distilled reasoning-based fine-tuning of RBD, and\nintegration with LLM evaluators. We fine-tune four sizes of RBD models, ranging\nfrom 1.5B to 14B, and observe consistent performance improvements across all\nscales. Experimental results on 4 bias types--verbosity, position, bandwagon,\nand sentiment--evaluated using 8 LLM evaluators demonstrate RBD's strong\neffectiveness. For example, the RBD-8B model improves evaluation accuracy by an\naverage of 18.5% and consistency by 10.9%, and surpasses prompting-based\nbaselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results\nhighlight RBD's effectiveness and scalability. Additional experiments further\ndemonstrate its strong generalization across biases and domains, as well as its\nefficiency.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.17100v1;http://arxiv.org/pdf/2505.17100v1", "pdf_url": "http://arxiv.org/pdf/2505.17100v1"}, {"title": "ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling", "link": "https://arxiv.org/pdf/2505.17987", "details": "W You, H Jiang, Z Liu, Z Xie, T Liu, J Lu, F Dou - arXiv preprint arXiv:2505.17987, 2025", "abstract": "Real world collection of Activities of Daily Living data is challenging due to privacy concerns, costly deployment and labeling, and the inherent sparsity and imbalance of human behavior. We present ADLGen, a generative framework specifically \u2026", "entry_id": "http://arxiv.org/abs/2505.17987v1", "updated": "2025-05-23 14:52:48", "published": "2025-05-23 14:52:48", "authors": "Weihang You;Hanqi Jiang;Zishuai Liu;Zihang Xie;Tianming Liu;Jin Lu;Fei Dou", "summary": "Real world collection of Activities of Daily Living data is challenging due\nto privacy concerns, costly deployment and labeling, and the inherent sparsity\nand imbalance of human behavior. We present ADLGen, a generative framework\nspecifically designed to synthesize realistic, event triggered, and symbolic\nsensor sequences for ambient assistive environments. ADLGen integrates a\ndecoder only Transformer with sign based symbolic temporal encoding, and a\ncontext and layout aware sampling mechanism to guide generation toward\nsemantically rich and physically plausible sensor event sequences. To enhance\nsemantic fidelity and correct structural inconsistencies, we further\nincorporate a large language model into an automatic generate evaluate refine\nloop, which verifies logical, behavioral, and temporal coherence and generates\ncorrection rules without manual intervention or environment specific tuning.\nThrough comprehensive experiments with novel evaluation metrics, ADLGen is\nshown to outperform baseline generators in statistical fidelity, semantic\nrichness, and downstream activity recognition, offering a scalable and\nprivacy-preserving solution for ADL data synthesis.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.17987v1;http://arxiv.org/pdf/2505.17987v1", "pdf_url": "http://arxiv.org/pdf/2505.17987v1"}, {"title": "DailyQA: A Benchmark to Evaluate Web Retrieval Augmented LLMs Based on Capturing Real-World Changes", "link": "https://arxiv.org/pdf/2505.17162", "details": "J Cheng, Z Dou - arXiv preprint arXiv:2505.17162, 2025", "abstract": "We propose DailyQA, an automatically updated dynamic dataset that updates questions weekly and contains answers to questions on any given date. DailyQA utilizes daily updates from Wikipedia revision logs to implement a fully automated \u2026", "entry_id": "http://arxiv.org/abs/2505.17162v1", "updated": "2025-05-22 15:13:33", "published": "2025-05-22 15:13:33", "authors": "Jiehan Cheng;Zhicheng Dou", "summary": "We propose DailyQA, an automatically updated dynamic dataset that updates\nquestions weekly and contains answers to questions on any given date. DailyQA\nutilizes daily updates from Wikipedia revision logs to implement a fully\nautomated pipeline of data filtering, query generation synthesis, quality\nchecking, answer extraction, and query classification. The benchmark requires\nlarge language models (LLMs) to process and answer questions involving\nfast-changing factual data and covering multiple domains. We evaluate several\nopen-source and closed-source LLMs using different RAG pipelines with web\nsearch augmentation. We compare the ability of different models to process\ntime-sensitive web information and find that rerank of web retrieval results is\ncritical. Our results indicate that LLMs still face significant challenges in\nhandling frequently updated information, suggesting that DailyQA benchmarking\nprovides valuable insights into the direction of progress for LLMs and RAG\nsystems.", "comment": null, "journal_ref": null, "primary_category": "cs.IR", "categories": "cs.IR;cs.AI", "links": "http://arxiv.org/abs/2505.17162v1;http://arxiv.org/pdf/2505.17162v1", "pdf_url": "http://arxiv.org/pdf/2505.17162v1"}, {"title": "AI-Augmented LLMs Achieve Therapist-Level Responses in Motivational Interviewing", "link": "https://arxiv.org/pdf/2505.17380", "details": "Y Huang, Y Jiang, H Liu, Y Cai, W Li, X Hu - arXiv preprint arXiv:2505.17380, 2025", "abstract": "\u2026 **LLM** **evaluation** methods can be broadly classified into automated and manual assessments. Though manual evaluation offers real-world relevance and detailed insights[70], it demands substantial financial resources and human labor. Moreover \u2026", "entry_id": "http://arxiv.org/abs/2505.17380v1", "updated": "2025-05-23 01:33:04", "published": "2025-05-23 01:33:04", "authors": "Yinghui Huang;Yuxuan Jiang;Hui Liu;Yixin Cai;Weiqing Li;Xiangen Hu", "summary": "Large language models (LLMs) like GPT-4 show potential for scaling\nmotivational interviewing (MI) in addiction care, but require systematic\nevaluation of therapeutic capabilities. We present a computational framework\nassessing user-perceived quality (UPQ) through expected and unexpected MI\nbehaviors. Analyzing human therapist and GPT-4 MI sessions via human-AI\ncollaboration, we developed predictive models integrating deep learning and\nexplainable AI to identify 17 MI-consistent (MICO) and MI-inconsistent (MIIN)\nbehavioral metrics. A customized chain-of-thought prompt improved GPT-4's MI\nperformance, reducing inappropriate advice while enhancing reflections and\nempathy. Although GPT-4 remained marginally inferior to therapists overall, it\ndemonstrated superior advice management capabilities. The model achieved\nmeasurable quality improvements through prompt engineering, yet showed\nlimitations in addressing complex emotional nuances. This framework establishes\na pathway for optimizing LLM-based therapeutic tools through targeted\nbehavioral metric analysis and human-AI co-evaluation. Findings highlight both\nthe scalability potential and current constraints of LLMs in clinical\ncommunication applications.", "comment": "21 pages, 5 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;H.1.2; I.2.7", "links": "http://arxiv.org/abs/2505.17380v1;http://arxiv.org/pdf/2505.17380v1", "pdf_url": "http://arxiv.org/pdf/2505.17380v1"}]
