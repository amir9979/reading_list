[{"title": "Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models", "link": "https://arxiv.org/pdf/2505.10446", "details": "Z Huang, Z Chen, Z Wang, T Li, GJ Qi - arXiv preprint arXiv:2505.10446, 2025", "abstract": "We introduce the\\emph {Diffusion Chain of Lateral Thought (DCoLT)}, a reasoning framework for diffusion language models. DCoLT treats each intermediate step in the reverse diffusion process as a latent\" thinking\" action and optimizes the entire \u2026"}, {"title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation", "link": "https://arxiv.org/pdf/2504.20859", "details": "G Hadad, H Roitman, Y Eshel, B Shapira, L Rokach - arXiv preprint arXiv:2504.20859, 2025", "abstract": "As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents``X-Cross''--a novel cross-domain sequential-recommendation model \u2026"}, {"title": "MilChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Remote Sensing", "link": "https://arxiv.org/pdf/2505.07984", "details": "A Koksal, AA Alatan - arXiv preprint arXiv:2505.07984, 2025", "abstract": "Remarkable capabilities in understanding and generating text-image content have been demonstrated by recent advancements in multimodal large language models (MLLMs). However, their effectiveness in specialized domains-particularly those \u2026"}, {"title": "DeepCritic: Deliberate Critique with Large Language Models", "link": "https://arxiv.org/pdf/2505.00662%3F", "details": "W Yang, J Chen, Y Lin, JR Wen - arXiv preprint arXiv:2505.00662, 2025", "abstract": "As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a \u2026"}, {"title": "Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers", "link": "https://arxiv.org/pdf/2505.04842", "details": "K Sareen, MM Moss, A Sordoni, R Agarwal, A Hosseini - arXiv preprint arXiv \u2026, 2025", "abstract": "Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners, such as GRPO or Leave-one-out PPO, abandon the learned value function in favor of empirically estimated returns. This hinders test-time compute scaling that relies on \u2026"}, {"title": "Unleashing the potential of prompt engineering for large language models", "link": "https://www.cell.com/patterns/fulltext/S2666-3899\\(25\\)00108-4", "details": "B Chen, Z Zhang, N Langren\u00e9, S Zhu - Patterns, 2025", "abstract": "This review explores the role of prompt engineering in unleashing the capabilities of large language models (LLMs). Prompt engineering is the process of structuring inputs, and it has emerged as a crucial technique for maximizing the utility and \u2026"}, {"title": "Which agent causes task failures and when? on automated failure attribution of llm multi-agent systems", "link": "https://arxiv.org/pdf/2505.00212", "details": "S Zhang, M Yin, J Zhang, J Liu, Z Han, J Zhang, B Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate \u2026"}, {"title": "Scalable LLM Math Reasoning Acceleration with Low-rank Distillation", "link": "https://arxiv.org/pdf/2505.07861", "details": "H Dong, B Acun, B Chen, Y Chi - arXiv preprint arXiv:2505.07861, 2025", "abstract": "Due to long generations, large language model (LLM) math reasoning demands significant computational resources and time. While many existing efficient inference methods have been developed with excellent performance preservation on \u2026"}, {"title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning", "link": "https://arxiv.org/pdf/2505.10320", "details": "C Whitehouse, T Wang, P Yu, X Li, J Weston, I Kulikov\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as- a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best \u2026"}]
