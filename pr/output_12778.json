[{"title": "scGPT-spatial: Continual Pretraining of Single-Cell Foundation Model for Spatial Transcriptomics", "link": "https://www.biorxiv.org/content/10.1101/2025.02.05.636714.full.pdf", "details": "CX Wang, H Cui, AH Zhang, R Xie, H Goodarzi\u2026 - bioRxiv, 2025", "abstract": "Spatial transcriptomics has emerged as a pivotal technology for profiling gene expression of cells within their spatial context. The rapid growth of publicly available spatial data presents an opportunity to further our understanding of \u2026"}, {"title": "Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models", "link": "https://arxiv.org/pdf/2501.08248%3F", "details": "Y Qiu, V Embar, Y Zhang, N Jaitly, SB Cohen, B Han - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in long-context language models (LCLMs) promise to transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With their expanded context windows, LCLMs can process entire knowledge bases and \u2026"}, {"title": "HiMix: Reducing Computational Complexity in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2501.10318%3F", "details": "X Zhang, D Li, B Liu, Z Bao, Y Zhou, B Yang, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Benefiting from recent advancements in large language models and modality alignment techniques, existing Large Vision-Language Models (LVLMs) have achieved prominent performance across a wide range of scenarios. However, the \u2026"}, {"title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models", "link": "https://arxiv.org/pdf/2501.13629", "details": "Z Lin, Z Tang, X Liu, Y Gong, Y Cheng, Q Chen, H Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce Sigma, an efficient large language model specialized for the system domain, empowered by a novel architecture including DiffQKV attention, and pre- trained on our meticulously collected system domain data. DiffQKV attention \u2026"}, {"title": "Exploring the Efficacy of Meta-Learning: Unveiling Superior Data Diversity Utilization of MAML Over Pre-training", "link": "https://arxiv.org/pdf/2501.08506", "details": "K Selva, S Vittayaareekul, B Miranda - arXiv preprint arXiv:2501.08506, 2025", "abstract": "Currently, data and model size dominate the narrative in the training of super-large, powerful models. However, there has been a lack of exploration on the effect of other attributes of the training dataset on model performance. We hypothesize that dataset \u2026"}, {"title": "Using Large Language Models to Promote Health Equity", "link": "https://ai.nejm.org/doi/abs/10.1056/AIp2400889", "details": "E Pierson, D Shanmugam, R Movva, J Kleinberg\u2026 - NEJM AI, 2025", "abstract": "While the discussion about the effects of large language models (LLMs) on health equity has been largely cautionary, LLMs also present significant opportunities for improving health equity. We highlight three such opportunities: improving the \u2026"}, {"title": "Calling a Spade a Heart: Gaslighting Multimodal Large Language Models via Negation", "link": "https://arxiv.org/pdf/2501.19017%3F", "details": "B Zhu, Y Gui, J Chen, CW Ngo, EP Lim - arXiv preprint arXiv:2501.19017, 2025", "abstract": "Multimodal Large Language Models (MLLMs) have exhibited remarkable advancements in integrating different modalities, excelling in complex understanding and generation tasks. Despite their success, MLLMs remain vulnerable to \u2026"}]
