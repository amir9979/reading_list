[{"title": "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties", "link": "https://aclanthology.org/2024.emnlp-main.1137.pdf", "details": "K Yu, Z Zhang, F Hu, S Storks, J Chai - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "A major reason behind the recent success of large language models (LLMs) is their incontext learning capability, which makes it possible to rapidly adapt them to downstream textbased tasks by prompting them with a small number of relevant \u2026"}, {"title": "FoPru: Focal Pruning for Efficient Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.14164", "details": "L Jiang, W Huang, T Liu, Y Zeng, J Li, L Cheng, X Xu - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) represent a significant advancement toward achieving superior multimodal capabilities by enabling powerful Large Language Models (LLMs) to understand visual input. Typically, LVLMs utilize visual \u2026"}, {"title": "Dynamic Strategy Planning for Efficient Question Answering with Large Language Models", "link": "https://arxiv.org/pdf/2410.23511", "details": "T Parekh, P Prakash, A Radovic, A Shekher\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Research has shown the effectiveness of reasoning (eg, Chain-of-Thought), planning (eg, SelfAsk), and retrieval augmented generation strategies to improve the performance of Large Language Models (LLMs) on various tasks, such as question \u2026"}, {"title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?", "link": "https://arxiv.org/pdf/2410.23856", "details": "Z Zhou, R Tao, J Zhu, Y Luo, Z Wang, B Han - arXiv preprint arXiv:2410.23856, 2024", "abstract": "This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We \u2026"}, {"title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step", "link": "https://arxiv.org/pdf/2411.10440%3F", "details": "G Xu, P Jin, L Hao, Y Song, L Sun, L Yuan - arXiv preprint arXiv:2411.10440, 2024", "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to \u2026"}, {"title": "Multifaceted Natural Language Processing Task\u2013Based Evaluation of Bidirectional Encoder Representations From Transformers Models for Bilingual (Korean and \u2026", "link": "https://medinform.jmir.org/2024/1/e52897/", "details": "K Kim, S Park, J Min, S Park, JY Kim, J Eun, K Jung\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background: The bidirectional encoder representations from transformers (BERT) model has attracted considerable attention in clinical applications, such as patient classification and disease prediction. However, current studies have typically \u2026"}, {"title": "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.884.pdf", "details": "P Gao, T Yamasaki, K Imoto - Findings of the Association for Computational \u2026, 2024", "abstract": "We propose VE-KD, a novel method that balances knowledge distillation and vocabulary expansion with the aim of training efficient domain-specific language models. Compared with traditional pre-training approaches, VE-KD exhibits \u2026"}, {"title": "Limits of transformer language models on learning to compose algorithms", "link": "https://openreview.net/pdf%3Fid%3Dx7AD0343Jz", "details": "J Thomm, G Camposampiero, A Terzic, M Hersche\u2026 - The Thirty-eighth Annual \u2026, 2024", "abstract": "We analyze the capabilities of Transformer language models in learning compositional discrete tasks. To this end, we evaluate training LLaMA models and prompting GPT-4 and Gemini on four tasks demanding to learn a composition of \u2026"}, {"title": "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.1052.pdf", "details": "W Zhai, A Zubiaga, B Liu, CJ Sun, Y Zhao - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "The fusion of language models (LMs) and knowledge graphs (KGs) is widely used in commonsense question answering, but generating faithful explanations remains challenging. Current methods often overlook path decoding faithfulness, leading to \u2026"}]
