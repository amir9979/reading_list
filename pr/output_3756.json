[{"title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models", "link": "https://arxiv.org/pdf/2407.06460", "details": "W Shi, J Lee, Y Huang, S Malladi, J Zhao, A Holtzman\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly \u2026"}, {"title": "Memory Augmented Language Models through Mixture of Word Experts", "link": "https://aclanthology.org/2024.naacl-long.249.pdf", "details": "C dos Santos, J Lee-Thorp, I Noble, CC Chang\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Scaling up the number of parameters of language models has proven to be an effective approach to improve performance. For dense models, increasing their size proportionally increases their computational footprint. In this work, we seek to \u2026"}, {"title": "$\\texttt {MoE-RBench} $: Towards Building Reliable Language Models with Sparse Mixture-of-Experts", "link": "https://arxiv.org/pdf/2406.11353", "details": "G Chen, X Zhao, T Chen, Y Cheng - arXiv preprint arXiv:2406.11353, 2024", "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new \u2026"}, {"title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space", "link": "https://arxiv.org/pdf/2406.09325", "details": "T Ashuach, M Tutek, Y Belinkov - arXiv preprint arXiv:2406.09325, 2024", "abstract": "Large language models (LLMs) risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in training data, causing privacy concerns. Current approaches to address this issue involve costly dataset \u2026"}, {"title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models", "link": "https://aclanthology.org/2024.findings-naacl.18.pdf", "details": "Z Su, Z Lin, B Baixue, H Chen, S Hu, W Zhou, G Ding\u2026 - Findings of the Association \u2026, 2024", "abstract": "Generative language models are usually pre-trained on large text corpus via predicting the next token (ie, sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language \u2026"}, {"title": "Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs", "link": "https://arxiv.org/pdf/2406.11514", "details": "Y Fang, M Li, W Wang, H Lin, F Feng - arXiv preprint arXiv:2406.11514, 2024", "abstract": "Large Language Models (LLMs) excel in various natural language processing tasks but struggle with hallucination issues. Existing solutions have considered utilizing LLMs' inherent reasoning abilities to alleviate hallucination, such as self-correction \u2026"}, {"title": "Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications", "link": "https://aclanthology.org/2024.naacl-long.198.pdf", "details": "Y Liu, S Gautam, J Ma, H Lakkaraju - Proceedings of the 2024 Conference of the \u2026, 2024", "abstract": "Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society \u2026"}, {"title": "UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages", "link": "https://arxiv.org/pdf/2406.09717", "details": "T Pham, KM Le, LA Tuan - arXiv preprint arXiv:2406.09717, 2024", "abstract": "In this paper, we introduce UniBridge (Cross-Lingual Transfer Learning with Optimized Embeddings and Vocabulary), a comprehensive approach developed to improve the effectiveness of Cross-Lingual Transfer Learning, particularly in \u2026"}, {"title": "Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for Information Retrieval", "link": "https://arxiv.org/pdf/2406.12169", "details": "Z Li, H Zhang, J Zhang - arXiv preprint arXiv:2406.12169, 2024", "abstract": "Recent research has explored distilling knowledge from large language models (LLMs) to optimize retriever models, especially within the retrieval-augmented generation (RAG) framework. However, most existing training methods rely on \u2026"}]
