[{"title": "Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena", "link": "https://arxiv.org/pdf/2406.07545", "details": "A Myrzakhan, SM Bsharat, Z Shen - arXiv preprint arXiv:2406.07545, 2024", "abstract": "Multiple-choice questions (MCQ) are frequently used to assess large language models (LLMs). Typically, an LLM is given a question and selects the answer deemed most probable after adjustments for factors like length. Unfortunately, LLMs \u2026"}, {"title": "The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models", "link": "https://arxiv.org/pdf/2406.05761", "details": "S Kim, J Suk, JY Cho, S Longpre, C Kim, D Yoon\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As language models (LMs) become capable of handling a wide range of tasks, their evaluation is becoming as challenging as their development. Most generation benchmarks currently assess LMs using abstract evaluation criteria like helpfulness \u2026"}, {"title": "FedAS: Bridging Inconsistency in Personalized Federated Learning", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_FedAS_Bridging_Inconsistency_in_Personalized_Federated_Learning_CVPR_2024_paper.pdf", "details": "X Yang, W Huang, M Ye - Proceedings of the IEEE/CVF Conference on Computer \u2026, 2024", "abstract": "Abstract Personalized Federated Learning (PFL) is primarily designed to provide customized models for each client to better fit the non-iid distributed client data which is a inherent challenge in Federated Learning. However current PFL methods suffer \u2026"}, {"title": "FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2405.18218", "details": "Y Zhang, Y Li, X Wang, Q Shen, B Plank, B Bischl\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Overparametrized transformer networks are the state-of-the-art architecture for Large Language Models (LLMs). However, such models contain billions of parameters making large compute a necessity, while raising environmental concerns. To \u2026"}, {"title": "SeBot: Structural Entropy Guided Multi-View Contrastive Learning for Social Bot Detection", "link": "https://arxiv.org/pdf/2405.11225", "details": "Y Yang, Q Wu, B He, H Peng, R Yang, Z Hao, Y Liao - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in social bot detection have been driven by the adoption of Graph Neural Networks. The social graph, constructed from social network interactions, contains benign and bot accounts that influence each other. However \u2026"}, {"title": "Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?", "link": "https://arxiv.org/pdf/2406.07546", "details": "X Fu, M He, Y Lu, WY Wang, D Roth - arXiv preprint arXiv:2406.07546, 2024", "abstract": "We present a novel task and benchmark for evaluating the ability of text-to-image (T2I) generation models to produce images that fit commonsense in real life, which we call Commonsense-T2I. Given two adversarial text prompts containing an \u2026"}, {"title": "Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning", "link": "https://openreview.net/pdf%3Fid%3DXwnABAdH5y", "details": "Z Zhang, Y Shi, J Zhu, W Zhou, X Qi, H Li - Forty-first International Conference on Machine \u2026", "abstract": "Trustworthiness is an essential prerequisite for the real-world application of large language models. In this paper, we focus on the trustworthiness of language models with respect to retrieval augmentation. Despite being supported with external \u2026"}, {"title": "Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging General-Purpose Knowledge Graphs for Enriched Embeddings", "link": "https://arxiv.org/pdf/2405.10745", "details": "A Sawczyn, J Binkowski, P Bielak, T Kajdanowicz - arXiv preprint arXiv:2405.10745, 2024", "abstract": "Knowledge-intensive tasks pose a significant challenge for Machine Learning (ML) techniques. Commonly adopted methods, such as Large Language Models (LLMs), often exhibit limitations when applied to such tasks. Nevertheless, there have been \u2026"}, {"title": "TAeKD: Teacher Assistant Enhanced Knowledge Distillation for Closed-Source Multilingual Neural Machine Translation", "link": "https://aclanthology.org/2024.lrec-main.1350.pdf", "details": "B Lv, X Liu, K Wei, P Luo, Y Yu - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Knowledge Distillation (KD) serves as an efficient method for transferring language knowledge from open-source large language models (LLMs) to more computationally efficient models. However, challenges arise when attempting to \u2026"}]
