[{"title": "Empowering Agentic Video Analytics Systems with Video Language Models", "link": "https://arxiv.org/pdf/2505.00254", "details": "Y Yan, S Jiang, T Cao, Y Yang, Q Yang, Y Shu, Y Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "AI-driven video analytics has become increasingly pivotal across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of \u2026", "entry_id": "http://arxiv.org/abs/2505.00254v3", "updated": "2025-05-16 10:00:32", "published": "2025-05-01 02:40:23", "authors": "Yuxuan Yan;Shiqi Jiang;Ting Cao;Yifan Yang;Qianqian Yang;Yuanchao Shu;Yuqing Yang;Lili Qiu", "summary": "AI-driven video analytics has become increasingly pivotal across diverse\ndomains. However, existing systems are often constrained to specific,\npredefined tasks, limiting their adaptability in open-ended analytical\nscenarios. The recent emergence of Video-Language Models (VLMs) as\ntransformative technologies offers significant potential for enabling\nopen-ended video understanding, reasoning, and analytics. Nevertheless, their\nlimited context windows present challenges when processing ultra-long video\ncontent, which is prevalent in real-world applications. To address this, we\nintroduce AVAS, a VLM-powered system designed for open-ended, advanced video\nanalytics. AVAS incorporates two key innovations: (1) the near real-time\nconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long or\ncontinuous video streams, and (2) an agentic retrieval-generation mechanism\nthat leverages EKGs to handle complex and diverse queries. Comprehensive\nevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that\nAVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,\nrespectively, significantly surpassing existing VLM and video\nRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video\nanalytics in ultra-long and open-world video scenarios, we introduce a new\nbenchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hours\nin duration, along with 120 manually annotated, diverse, and complex\nquestion-answer pairs. On AVAS-100, AVAS achieves top-tier performance with an\naccuracy of 75.8%.", "comment": "15 pages, AVAS, add latency breakdown", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.00254v3;http://arxiv.org/pdf/2505.00254v3", "pdf_url": "http://arxiv.org/pdf/2505.00254v3"}, {"title": "Patient medical report analyser: A multi-stage workflow integrating image processing, OCR, and language models for summarization", "link": "https://www.taylorfrancis.com/chapters/edit/10.1201/9781003650010-61/patient-medical-report-analyser-multi-stage-workflow-integrating-image-processing-ocr-language-models-summarization-nitish-ramaraj-girish-murugan-tejas-anil-vetriselvi-jagadeesan", "details": "N Ramaraj, G Murugan, T Anil, T Vetriselvi\u2026 - \u2026 , Information Technology and \u2026", "abstract": "This work presents an advanced automated system designed to streamline the processing of medical prescriptions. The system follows a multi-stage workflow aimed at enhancing efficiency in handling and interpreting prescription data. The \u2026"}, {"title": "61 Patient medical report analyser: A multi-stage workflow integrating image processing, OCR, and language models for summarization", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DmhReEQAAQBAJ%26oi%3Dfnd%26pg%3DPA377%26ots%3D4uo73ujPuf%26sig%3DLaAQtsezsxtlnw-2FOXxoHGCIQc", "details": "S Jagadeesan - Progressive Computational Intelligence, Information \u2026, 2025", "abstract": "This work presents an advanced automated system designed to streamline the processing of medical prescriptions. The system follows a multi-stage workflow aimed at enhancing efficiency in handling and interpreting prescription data. The \u2026"}, {"title": "Exploring Multimodal Language Models for Sustainability Disclosure Extraction: A Comparative Study", "link": "https://aclanthology.org/2025.insights-1.13.pdf", "details": "T Gupta, T Goel, I Verma - The Sixth Workshop on Insights from Negative Results \u2026, 2025", "abstract": "Sustainability metrics have increasingly become a crucial non-financial criterion in investment decision-making. Organizations worldwide are recognizing the importance of sustainability and are proactively highlighting their efforts through \u2026"}, {"title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records", "link": "https://arxiv.org/pdf/2505.16941", "details": "C Pang, V Jeanselme, YS Choi, X Jiang, Z Jing\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Foundation models hold significant promise in healthcare, given their capacity to extract meaningful representations independent of downstream tasks. This property has enabled state-of-the-art performance across several clinical applications trained \u2026", "entry_id": "http://arxiv.org/abs/2505.16941v2", "updated": "2025-05-23 02:06:25", "published": "2025-05-22 17:29:52", "authors": "Chao Pang;Vincent Jeanselme;Young Sang Choi;Xinzhuo Jiang;Zilin Jing;Aparajita Kashyap;Yuta Kobayashi;Yanwei Li;Florent Pollet;Karthik Natarajan;Shalmali Joshi", "summary": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.16941v2;http://arxiv.org/pdf/2505.16941v2", "pdf_url": "http://arxiv.org/pdf/2505.16941v2"}, {"title": "Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports", "link": "https://arxiv.org/pdf/2505.16624", "details": "FD Serra, P Schrempf, C Wang, Z Meng, F Deligianni\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present a novel approach to Chest X-ray (CXR) Visual Question Answering (VQA), addressing both single-image image-difference questions. Single-image questions focus on abnormalities within a specific CXR (\" What abnormalities are \u2026", "entry_id": "http://arxiv.org/abs/2505.16624v1", "updated": "2025-05-22 12:57:35", "published": "2025-05-22 12:57:35", "authors": "Francesco Dalla Serra;Patrick Schrempf;Chaoyang Wang;Zaiqiao Meng;Fani Deligianni;Alison Q. O'Neil", "summary": "We present a novel approach to Chest X-ray (CXR) Visual Question Answering\n(VQA), addressing both single-image image-difference questions. Single-image\nquestions focus on abnormalities within a specific CXR (\"What abnormalities are\nseen in image X?\"), while image-difference questions compare two longitudinal\nCXRs acquired at different time points (\"What are the differences between image\nX and Y?\"). We further explore how the integration of radiology reports can\nenhance the performance of VQA models. While previous approaches have\ndemonstrated the utility of radiology reports during the pre-training phase, we\nextend this idea by showing that the reports can also be leveraged as\nadditional input to improve the VQA model's predicted answers. First, we\npropose a unified method that handles both types of questions and\nauto-regressively generates the answers. For single-image questions, the model\nis provided with a single CXR. For image-difference questions, the model is\nprovided with two CXRs from the same patient, captured at different time\npoints, enabling the model to detect and describe temporal changes. Taking\ninspiration from 'Chain-of-Thought reasoning', we demonstrate that performance\non the CXR VQA task can be improved by grounding the answer generator module\nwith a radiology report predicted for the same CXR. In our approach, the VQA\nmodel is divided into two steps: i) Report Generation (RG) and ii) Answer\nGeneration (AG). Our results demonstrate that incorporating predicted radiology\nreports as evidence to the AG model enhances performance on both single-image\nand image-difference questions, achieving state-of-the-art results on the\nMedical-Diff-VQA dataset.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.CL", "links": "http://arxiv.org/abs/2505.16624v1;http://arxiv.org/pdf/2505.16624v1", "pdf_url": "http://arxiv.org/pdf/2505.16624v1"}, {"title": "UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation", "link": "https://arxiv.org/pdf/2504.21336", "details": "L Wu, Y Nie, S He, J Zhuang, H Chen - arXiv preprint arXiv:2504.21336, 2025", "abstract": "Multi-modal interpretation of biomedical images opens up novel opportunities in biomedical image analysis. Conventional AI approaches typically rely on disjointed training, ie, Large Language Models (LLMs) for clinical text generation and \u2026", "entry_id": "http://arxiv.org/abs/2504.21336v1", "updated": "2025-04-30 05:51:48", "published": "2025-04-30 05:51:48", "authors": "Linshan Wu;Yuxiang Nie;Sunan He;Jiaxin Zhuang;Hao Chen", "summary": "Multi-modal interpretation of biomedical images opens up novel opportunities\nin biomedical image analysis. Conventional AI approaches typically rely on\ndisjointed training, i.e., Large Language Models (LLMs) for clinical text\ngeneration and segmentation models for target extraction, which results in\ninflexible real-world deployment and a failure to leverage holistic biomedical\ninformation. To this end, we introduce UniBiomed, the first universal\nfoundation model for grounded biomedical image interpretation. UniBiomed is\nbased on a novel integration of Multi-modal Large Language Model (MLLM) and\nSegment Anything Model (SAM), which effectively unifies the generation of\nclinical texts and the segmentation of corresponding biomedical objects for\ngrounded interpretation. In this way, UniBiomed is capable of tackling a wide\nrange of biomedical tasks across ten diverse biomedical imaging modalities. To\ndevelop UniBiomed, we curate a large-scale dataset comprising over 27 million\ntriplets of images, annotations, and text descriptions across ten imaging\nmodalities. Extensive validation on 84 internal and external datasets\ndemonstrated that UniBiomed achieves state-of-the-art performance in\nsegmentation, disease recognition, region-aware diagnosis, visual question\nanswering, and report generation. Moreover, unlike previous models that rely on\nclinical experts to pre-diagnose images and manually craft precise textual or\nvisual prompts, UniBiomed can provide automated and end-to-end grounded\ninterpretation for biomedical image analysis. This represents a novel paradigm\nshift in clinical workflows, which will significantly improve diagnostic\nefficiency. In summary, UniBiomed represents a novel breakthrough in biomedical\nAI, unlocking powerful grounded interpretation capabilities for more accurate\nand efficient biomedical image analysis.", "comment": "The first universal foundation model for grounded biomedical image\n  interpretation", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2504.21336v1;http://arxiv.org/pdf/2504.21336v1", "pdf_url": "http://arxiv.org/pdf/2504.21336v1"}, {"title": "AICOE at PerAnsSumm 2025: An Ensemble of Large Language Models for Perspective-Aware Healthcare Answer Summarization", "link": "https://aclanthology.org/2025.cl4health-1.36.pdf", "details": "R Rakshith, MS Khan, A Chopra - Proceedings of the Second Workshop on Patient \u2026, 2025", "abstract": "The PerAnsSumm 2024 shared task at the CL4Health workshop focuses on generating structured, perspective-specific summaries to enhance the accessibility of health-related information. Given a Healthcare community QA dataset containing a \u2026"}, {"title": "Towards trustworthy and reliable language models", "link": "https://dr.ntu.edu.sg/bitstream/10356/184392/2/Amended%2520Thesis.pdf", "details": "R Zhao - 2025", "abstract": "This thesis addresses the critical challenge of developing trustworthy and reliable Natural Language Processing (NLP) systems, specifically the newly emerged Large Language Models (LLMs). As LLMs become increasingly prevalent in various \u2026"}]
