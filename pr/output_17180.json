[{"title": "TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine", "link": "https://arxiv.org/pdf/2505.24063", "details": "J Xie, Y Yu, Z Zhang, S Zeng, J He, A Vasireddy\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 As illustrated in Figure 1, based on the TCM-Ladder dataset, we design a series of evaluation tasks to comprehensively assess the capabilities of TCM-specific **large** **language** **models** across multiple dimensions. We constructed a total of 21,326 high-quality \u2026", "entry_id": "http://arxiv.org/abs/2505.24063v1", "updated": "2025-05-29 23:13:57", "published": "2025-05-29 23:13:57", "authors": "Jiacheng Xie;Yang Yu;Ziyang Zhang;Shuai Zeng;Jiaxuan He;Ayush Vasireddy;Xiaoting Tang;Congyu Guo;Lening Zhao;Congcong Jing;Guanghui An;Dong Xu", "summary": "Traditional Chinese Medicine (TCM), as an effective alternative medicine, has\nbeen receiving increasing attention. In recent years, the rapid development of\nlarge language models (LLMs) tailored for TCM has underscored the need for an\nobjective and comprehensive evaluation framework to assess their performance on\nreal-world tasks. However, existing evaluation datasets are limited in scope\nand primarily text-based, lacking a unified and standardized multimodal\nquestion-answering (QA) benchmark. To address this issue, we introduce\nTCM-Ladder, the first multimodal QA dataset specifically designed for\nevaluating large TCM language models. The dataset spans multiple core\ndisciplines of TCM, including fundamental theory, diagnostics, herbal formulas,\ninternal medicine, surgery, pharmacognosy, and pediatrics. In addition to\ntextual content, TCM-Ladder incorporates various modalities such as images and\nvideos. The datasets were constructed using a combination of automated and\nmanual filtering processes and comprise 52,000+ questions in total. These\nquestions include single-choice, multiple-choice, fill-in-the-blank, diagnostic\ndialogue, and visual comprehension tasks. We trained a reasoning model on\nTCM-Ladder and conducted comparative experiments against 9 state-of-the-art\ngeneral domain and 5 leading TCM-specific LLMs to evaluate their performance on\nthe datasets. Moreover, we propose Ladder-Score, an evaluation method\nspecifically designed for TCM question answering that effectively assesses\nanswer quality regarding terminology usage and semantic expression. To our\nknowledge, this is the first work to evaluate mainstream general domain and\nTCM-specific LLMs on a unified multimodal benchmark. The datasets and\nleaderboard are publicly available at https://tcmladder.com or\nhttps://54.211.107.106 and will be continuously updated.", "comment": "22 pages, 4 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.DB", "links": "http://arxiv.org/abs/2505.24063v1;http://arxiv.org/pdf/2505.24063v1", "pdf_url": "http://arxiv.org/pdf/2505.24063v1"}, {"title": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks", "link": "https://arxiv.org/pdf/2505.23802", "details": "S Bedi, H Cui, M Fuentes, A Unell, M Wornow\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **large** **language** **models** (LLMs) achieve near-perfect scores on **medical** licensing exams, these evaluations inadequately reflect the complexity and diversity of real-world **clinical** \u2026 Medmcqa: A large-scale multisubject multi-choice dataset for **medical** \u2026", "entry_id": "http://arxiv.org/abs/2505.23802v2", "updated": "2025-06-02 04:19:10", "published": "2025-05-26 22:55:49", "authors": "Suhana Bedi;Hejie Cui;Miguel Fuentes;Alyssa Unell;Michael Wornow;Juan M. Banda;Nikesh Kotecha;Timothy Keyes;Yifan Mai;Mert Oez;Hao Qiu;Shrey Jain;Leonardo Schettini;Mehr Kashyap;Jason Alan Fries;Akshay Swaminathan;Philip Chung;Fateme Nateghi;Asad Aali;Ashwin Nayak;Shivam Vedak;Sneha S. Jain;Birju Patel;Oluseyi Fayanju;Shreya Shah;Ethan Goh;Dong-han Yao;Brian Soetikno;Eduardo Reis;Sergios Gatidis;Vasu Divi;Robson Capasso;Rachna Saralkar;Chia-Chun Chiang;Jenelle Jindal;Tho Pham;Faraz Ghoddusi;Steven Lin;Albert S. Chiou;Christy Hong;Mohana Roy;Michael F. Gensheimer;Hinesh Patel;Kevin Schulman;Dev Dash;Danton Char;Lance Downing;Francois Grolleau;Kameron Black;Bethel Mieso;Aydin Zahedivash;Wen-wai Yim;Harshita Sharma;Tony Lee;Hannah Kirsch;Jennifer Lee;Nerissa Ambers;Carlene Lugtu;Aditya Sharma;Bilal Mawji;Alex Alekseyev;Vicky Zhou;Vikas Kakkar;Jarrod Helzer;Anurang Revri;Yair Bannett;Roxana Daneshjou;Jonathan Chen;Emily Alsentzer;Keith Morse;Nirmal Ravi;Nima Aghaeepour;Vanessa Kennedy;Akshay Chaudhari;Thomas Wang;Sanmi Koyejo;Matthew P. Lungren;Eric Horvitz;Percy Liang;Mike Pfeffer;Nigam H. Shah", "summary": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.23802v2;http://arxiv.org/pdf/2505.23802v2", "pdf_url": "http://arxiv.org/pdf/2505.23802v2"}, {"title": "Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs", "link": "https://arxiv.org/pdf/2505.24830", "details": "J Vladika, A Domres, M Nguyen, R Moser, J Nano\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) exhibit extensive **medical** knowledge.However, LLMs are prone to hallucinations that may lead to harmful **medical** advice, and their often inaccurate citations reduce overall explainability.Such limitations also \u2026", "entry_id": "http://arxiv.org/abs/2505.24830v1", "updated": "2025-05-30 17:33:07", "published": "2025-05-30 17:33:07", "authors": "Juraj Vladika;Annika Domres;Mai Nguyen;Rebecca Moser;Jana Nano;Felix Busch;Lisa C. Adams;Keno K. Bressem;Denise Bernhardt;Stephanie E. Combs;Kai J. Borm;Florian Matthes;Jan C. Peeken", "summary": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare.", "comment": "11 pages, 4 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.24830v1;http://arxiv.org/pdf/2505.24830v1", "pdf_url": "http://arxiv.org/pdf/2505.24830v1"}, {"title": "MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering", "link": "https://arxiv.org/pdf/2505.24040", "details": "Y Hao, K Alhamoud, H Jeong, H Zhang, I Puri, P Torr\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) have shown strong performance across a range of **medical** tasks, with systems like GPT-4 and MedPaLM outperforming human averages on standardized **medical** examinations [9, 42]. However, many tasks do \u2026", "entry_id": "http://arxiv.org/abs/2505.24040v1", "updated": "2025-05-29 22:23:48", "published": "2025-05-29 22:23:48", "authors": "Yuexing Hao;Kumail Alhamoud;Hyewon Jeong;Haoran Zhang;Isha Puri;Philip Torr;Mike Schaekermann;Ariel D. Stern;Marzyeh Ghassemi", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious medical question-answering (QA) benchmarks, including standardized\nmedical exams. However, correct answers alone do not ensure correct logic, and\nmodels may reach accurate conclusions through flawed processes. In this study,\nwe introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance\nEstimation and Question Answering) dataset to evaluate how physician trainees\nand LLMs prioritize relevant information when answering QA questions. We obtain\nannotations on 1,300 QA pairs from 36 physician trainees, labeling each\nsentence within the question components for relevance. We compare these\nrelevance estimates to those for LLMs, and further evaluate the impact of these\n\"relevant\" subsets on downstream task performance for both physician trainees\nand LLMs. We find that LLMs are frequently not aligned with the content\nrelevance estimates of physician trainees. After filtering out physician\ntrainee-labeled irrelevant sentences, accuracy improves for both the trainees\nand the LLMs. All LLM and physician trainee-labeled data are available at:\nhttp://medpair.csail.mit.edu/.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.24040v1;http://arxiv.org/pdf/2505.24040v1", "pdf_url": "http://arxiv.org/pdf/2505.24040v1"}, {"title": "Automated Response Generation Using Language Models: An Approach to Enhancing User Interaction", "link": "https://link.springer.com/chapter/10.1007/978-3-031-92967-0_11", "details": "C Asaju, H Vadapalli - International Conference on Human-Computer \u2026, 2025", "abstract": "\u2026 With self-optimizing prompts and an evaluation mechanism based on LLMs, the system generates automatic responses to customer reviews using retrieval-augmented generation (RAG) and **large** **language** **models** (LLMs). With an improvement of over \u2026"}, {"title": "Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation", "link": "https://arxiv.org/pdf/2505.23867", "details": "Z Liu, Z Hou, Y Di, K Yang, Z Sang, C Xie, J Yang, S Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Med-PaLM 2 fine-tunes on advanced **medical** **question** - **answering** tasks using domain-\u2026 to achieve competitive results in multilingual **medical** **question** **answering** [11], while Radiology\u2026 of state-of-the-art multimodal **large** **language** **models** (MLLMs) \u2026", "entry_id": "http://arxiv.org/abs/2505.23867v1", "updated": "2025-05-29 10:31:57", "published": "2025-05-29 10:31:57", "authors": "Zeyu Liu;Zhitian Hou;Yining Di;Kejing Yang;Zhijie Sang;Congkai Xie;Jingwen Yang;Siyuan Liu;Jialu Wang;Chunming Li;Ming Li;Hongxia Yang", "summary": "Multimodal large language models (MLLMs) have demonstrated promising\nprospects in healthcare, particularly for addressing complex medical tasks,\nsupporting multidisciplinary treatment (MDT), and enabling personalized\nprecision medicine. However, their practical deployment faces critical\nchallenges in resource efficiency, diagnostic accuracy, clinical\nconsiderations, and ethical privacy. To address these limitations, we propose\nInfi-Med, a comprehensive framework for medical MLLMs that introduces three key\ninnovations: (1) a resource-efficient approach through curating and\nconstructing high-quality supervised fine-tuning (SFT) datasets with minimal\nsample requirements, with a forward-looking design that extends to both\npretraining and posttraining phases; (2) enhanced multimodal reasoning\ncapabilities for cross-modal integration and clinical task understanding; and\n(3) a systematic evaluation system that assesses model performance across\nmedical modalities and task types. Our experiments demonstrate that Infi-Med\nachieves state-of-the-art (SOTA) performance in general medical reasoning while\nmaintaining rapid adaptability to clinical scenarios. The framework establishes\na solid foundation for deploying MLLMs in real-world healthcare settings by\nbalancing model effectiveness with operational constraints.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.23867v1;http://arxiv.org/pdf/2505.23867v1", "pdf_url": "http://arxiv.org/pdf/2505.23867v1"}, {"title": "LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews", "link": "https://arxiv.org/pdf/2505.24757", "details": "C Jaumann, A Wiedholz, A Friedrich - arXiv preprint arXiv:2505.24757, 2025", "abstract": "\u2026 To date, abstract screening methods using **large** **language** **models** (LLMs) focus on binary classification settings; existing **question** **answering** (\u2026 We manually extract these criteria as well as research **questions** for 57 SLRs, mostly in the **medical** \u2026", "entry_id": "http://arxiv.org/abs/2505.24757v1", "updated": "2025-05-30 16:18:50", "published": "2025-05-30 16:18:50", "authors": "Christian Jaumann;Andreas Wiedholz;Annemarie Friedrich", "summary": "The scientific literature is growing rapidly, making it hard to keep track of\nthe state-of-the-art. Systematic literature reviews (SLRs) aim to identify and\nevaluate all relevant papers on a topic. After retrieving a set of candidate\npapers, the abstract screening phase determines initial relevance. To date,\nabstract screening methods using large language models (LLMs) focus on binary\nclassification settings; existing question answering (QA) based ranking\napproaches suffer from error propagation. LLMs offer a unique opportunity to\nevaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks\ndo not provide them exhaustively. We manually extract these criteria as well as\nresearch questions for 57 SLRs, mostly in the medical domain, enabling\nprincipled comparisons between approaches. Moreover, we propose LGAR, a\nzero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance\nscorer and a dense re-ranker. Our extensive experiments show that LGAR\noutperforms existing QA-based methods by 5-10 pp. in mean average precision.\nOur code and data is publicly available.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.24757v1;http://arxiv.org/pdf/2505.24757v1", "pdf_url": "http://arxiv.org/pdf/2505.24757v1"}, {"title": "Semi-structured LLM Reasoners Can Be Rigorously Audited", "link": "https://arxiv.org/pdf/2505.24217", "details": "J Leng, CA Cohen, Z Zhang, C Xiong, WW Cohen - arXiv preprint arXiv:2505.24217, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) are often more effective when they employ \u201creasoning\u201d techniques such as short Chain-of-Thought (CoT) \u2026 Enhancing chain of thought prompting in **large** **language** **models** via reasoning patterns. In Proceedings of the \u2026", "entry_id": "http://arxiv.org/abs/2505.24217v1", "updated": "2025-05-30 05:06:10", "published": "2025-05-30 05:06:10", "authors": "Jixuan Leng;Cassandra A. Cohen;Zhixian Zhang;Chenyan Xiong;William W. Cohen", "summary": "As Large Language Models (LLMs) become increasingly capable at reasoning, the\nproblem of \"faithfulness\" persists: LLM \"reasoning traces\" can contain errors\nand omissions that are difficult to detect, and may obscure biases in model\noutputs. To address these limitations, we introduce Semi-Structured Reasoning\nModels (SSRMs), which internalize a semi-structured Chain-of-Thought (CoT)\nreasoning format within the model. Our SSRMs generate reasoning traces in a\nPythonic syntax. While SSRM traces are not executable, they adopt a restricted,\ntask-specific vocabulary to name distinct reasoning steps, and to mark each\nstep's inputs and outputs. Through extensive evaluation on ten benchmarks,\nSSRMs demonstrate strong performance and generality: they outperform comparably\nsized baselines by nearly ten percentage points on in-domain tasks while\nremaining competitive with specialized models on out-of-domain medical\nbenchmarks. Furthermore, we show that semi-structured reasoning is more\namenable to analysis: in particular, they can be automatically audited to\nidentify reasoning flaws. We explore both hand-crafted structured audits, which\ndetect task-specific problematic reasoning patterns, and learned typicality\naudits, which apply probabilistic models over reasoning patterns, and show that\nboth audits can be used to effectively flag probable reasoning errors.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.24217v1;http://arxiv.org/pdf/2505.24217v1", "pdf_url": "http://arxiv.org/pdf/2505.24217v1"}, {"title": "Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning", "link": "https://arxiv.org/pdf/2505.24105", "details": "J Lin, Z Wu, J Sun - arXiv preprint arXiv:2505.24105, 2025", "abstract": "\u2026 We present EHRMIND, a practical recipe for adapting **large** **language** **models** (LLMs) to complex **clinical** reasoning tasks using \u2026 LLMs for **Clinical** Reasoning. LLMs are increasingly being explored for a range of **clinical** tasks, including summarization \u2026", "entry_id": "http://arxiv.org/abs/2505.24105v1", "updated": "2025-05-30 01:13:22", "published": "2025-05-30 01:13:22", "authors": "Jiacheng Lin;Zhenbang Wu;Jimeng Sun", "summary": "We present EHRMIND, a practical recipe for adapting large language models\n(LLMs) to complex clinical reasoning tasks using reinforcement learning with\nverifiable rewards (RLVR). While RLVR has succeeded in mathematics and coding,\nits application to healthcare contexts presents unique challenges due to the\nspecialized knowledge and reasoning required for electronic health record (EHR)\ninterpretation. Our pilot study on the MEDCALC benchmark reveals two key\nfailure modes: (1) misapplied knowledge, where models possess relevant medical\nknowledge but apply it incorrectly, and (2) missing knowledge, where models\nlack essential domain knowledge. To address these cases, EHRMIND applies a\ntwo-stage solution: a lightweight supervised fine-tuning (SFT) warm-up that\ninjects missing domain knowledge, stabilizes subsequent training, and\nencourages structured, interpretable outputs; followed by RLVR, which\nreinforces outcome correctness and refines the model's decision-making. We\ndemonstrate the effectiveness of our method across diverse clinical\napplications, including medical calculations (MEDCALC), patient-trial matching\n(TREC CLINICAL TRIALS), and disease diagnosis (EHRSHOT). EHRMIND delivers\nconsistent gains in accuracy, interpretability, and cross-task generalization.\nThese findings offer practical guidance for applying RLVR to enhance LLM\ncapabilities in healthcare settings.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.24105v1;http://arxiv.org/pdf/2505.24105v1", "pdf_url": "http://arxiv.org/pdf/2505.24105v1"}]
