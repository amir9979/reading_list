[{"title": "Scaling Laws for Linear Complexity Language Models", "link": "https://arxiv.org/pdf/2406.16690", "details": "X Shen, D Li, R Leng, Z Qin, W Sun, Y Zhong - arXiv preprint arXiv:2406.16690, 2024", "abstract": "The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their \u2026"}, {"title": "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models", "link": "https://arxiv.org/pdf/2407.05131", "details": "P Xia, K Zhu, H Li, H Zhu, Y Li, G Li, L Zhang, H Yao - arXiv preprint arXiv:2407.05131, 2024", "abstract": "The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical \u2026"}, {"title": "DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning", "link": "https://arxiv.org/pdf/2406.17659", "details": "X Zhang, Z Altaweel, Y Hayamizu, Y Ding, S Amiri\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language models (VLMs) have been applied to robot task planning problems, where the robot receives a task in natural language and generates plans based on visual inputs. While current VLMs have demonstrated strong vision-language \u2026"}, {"title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models", "link": "https://arxiv.org/pdf/2407.03181", "details": "H Puerto, T Chubakov, X Zhu, HT Madabushi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model \u2026"}, {"title": "Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models", "link": "https://arxiv.org/pdf/2407.02716", "details": "X Han, L Jin, X Ma, X Liu - arXiv preprint arXiv:2407.02716, 2024", "abstract": "Fine-tuning pre-trained Vision-Language Models (VLMs) has shown remarkable capabilities in medical image and textual depiction synergy. Nevertheless, many pre- training datasets are restricted by patient privacy concerns, potentially containing \u2026"}, {"title": "MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models", "link": "https://arxiv.org/pdf/2407.02775", "details": "Y Zhang, Z Yang, S Ji - arXiv preprint arXiv:2407.02775, 2024", "abstract": "Knowledge distillation is an effective technique for pre-trained language model compression. Although existing knowledge distillation methods perform well for the most typical model BERT, they could be further improved in two aspects: the relation \u2026"}, {"title": "Semantic Compositions Enhance Vision-Language Contrastive Learning", "link": "https://arxiv.org/pdf/2407.01408", "details": "M Aladago, L Torresani, S Vosoughi - arXiv preprint arXiv:2407.01408, 2024", "abstract": "In the field of vision-language contrastive learning, models such as CLIP capitalize on matched image-caption pairs as positive examples and leverage within-batch non- matching pairs as negatives. This approach has led to remarkable outcomes in zero \u2026"}, {"title": "UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization", "link": "https://arxiv.org/pdf/2407.03525", "details": "MN Uddin, A Saeidi, D Handa, A Seth, TC Son\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces UnSeenTimeQA, a novel time-sensitive question-answering (TSQA) benchmark that diverges from traditional TSQA benchmarks by avoiding factual and web-searchable queries. We present a series of time-sensitive event \u2026"}, {"title": "MEDFuse: Multimodal EHR Data Fusion with Masked Lab-Test Modeling and Large Language Models", "link": "https://arxiv.org/pdf/2407.12309", "details": "TMN Phan, CT Dao, C Wu, JZ Wang, S Liu, JE Ding\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Electronic health records (EHRs) are multimodal by nature, consisting of structured tabular features like lab tests and unstructured clinical notes. In real-life clinical practice, doctors use complementary multimodal EHR data sources to get a clearer \u2026"}]
