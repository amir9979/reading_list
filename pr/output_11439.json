[{"title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining", "link": "https://arxiv.org/pdf/2501.00958", "details": "W Zhang, H Zhang, X Li, J Sun, Y Shen, W Lu, D Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge \u2026"}, {"title": "Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models", "link": "https://arxiv.org/pdf/2412.18609%3F", "details": "J Yi, ST Wasim, Y Luo, M Naseer, J Gall - arXiv preprint arXiv:2412.18609, 2024", "abstract": "We present an efficient encoder-free approach for video-language understanding that achieves competitive performance while significantly reducing computational overhead. Current video-language models typically rely on heavyweight image \u2026"}, {"title": "Unifying Specialized Visual Encoders for Video Language Models", "link": "https://arxiv.org/pdf/2501.01426", "details": "J Chung, T Zhu, MG Saez-Diez, JC Niebles, H Zhou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of \u2026"}, {"title": "Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach", "link": "https://arxiv.org/pdf/2412.18108", "details": "J Bi, J Guo, Y Tang, LB Wen, Z Liu, C Xu - arXiv preprint arXiv:2412.18108, 2024", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated remarkable progress in visual understanding. This impressive leap raises a compelling question: how can language models, initially trained solely on \u2026"}, {"title": "MMFactory: A Universal Solution Search Engine for Vision-Language Tasks", "link": "https://arxiv.org/pdf/2412.18072", "details": "WC Fan, T Rahman, L Sigal - arXiv preprint arXiv:2412.18072, 2024", "abstract": "With advances in foundational and vision-language models, and effective fine-tuning techniques, a large number of both general and special-purpose models have been developed for a variety of visual tasks. Despite the flexibility and accessibility of these \u2026"}, {"title": "Supervision-free Vision-Language Alignment", "link": "https://arxiv.org/pdf/2501.04568", "details": "G Giannone, R Li, Q Feng, E Perevodchikov, R Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) have demonstrated remarkable potential in integrating visual and linguistic information, but their performance is often constrained by the need for extensive, high-quality image-text training data. Curation \u2026"}, {"title": "ICONS: Influence Consensus for Vision-Language Data Selection", "link": "https://arxiv.org/pdf/2501.00654", "details": "X Wu, M Xia, R Shao, Z Deng, PW Koh, O Russakovsky - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual Instruction Tuning typically requires a large amount of vision-language training data. This data often containing redundant information that increases computational costs without proportional performance gains. In this work, we \u2026"}, {"title": "Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment", "link": "https://arxiv.org/pdf/2412.19326", "details": "Z Yan, Z Li, Y He, C Wang, K Li, X Li, X Zeng, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Current multimodal large language models (MLLMs) struggle with fine-grained or precise understanding of visuals though they give comprehensive perception and reasoning in a spectrum of vision applications. Recent studies either develop tool \u2026"}, {"title": "URFM: a general Ultrasound Representation Foundation Model for advancing ultrasound image diagnosis", "link": "https://www.researchsquare.com/article/rs-5662163/latest.pdf", "details": "Q Kang, Q Lao, J Gao, W Bao, Q Lu, K Li - 2024", "abstract": "Ultrasound imaging is pivotal in clinical diagnostics, providing critical insights into a wide range of diseases and organs. However, advancing artificial intelligence (AI) in this field is hindered by challenges such as the reliance on large labeled datasets \u2026"}]
