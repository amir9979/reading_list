[{"title": "An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations", "link": "https://arxiv.org/pdf/2505.16705", "details": "S Park, J Mun, D Oh, N Lee - arXiv preprint arXiv:2505.16705, 2025", "abstract": "Concept bottleneck models (CBMs) ensure interpretability by decomposing predictions into human interpretable concepts. Yet the annotations used for training CBMs that enable this transparency are often noisy, and the impact of such \u2026", "entry_id": "http://arxiv.org/abs/2505.16705v1", "updated": "2025-05-22 14:06:55", "published": "2025-05-22 14:06:55", "authors": "Seonghwan Park;Jueun Mun;Donghyun Oh;Namhoon Lee", "summary": "Concept bottleneck models (CBMs) ensure interpretability by decomposing\npredictions into human interpretable concepts. Yet the annotations used for\ntraining CBMs that enable this transparency are often noisy, and the impact of\nsuch corruption is not well understood. In this study, we present the first\nsystematic study of noise in CBMs and show that even moderate corruption\nsimultaneously impairs prediction performance, interpretability, and the\nintervention effectiveness. Our analysis identifies a susceptible subset of\nconcepts whose accuracy declines far more than the average gap between noisy\nand clean supervision and whose corruption accounts for most performance loss.\nTo mitigate this vulnerability we propose a two-stage framework. During\ntraining, sharpness-aware minimization stabilizes the learning of\nnoise-sensitive concepts. During inference, where clean labels are unavailable,\nwe rank concepts by predictive entropy and correct only the most uncertain\nones, using uncertainty as a proxy for susceptibility. Theoretical analysis and\nextensive ablations elucidate why sharpness-aware training confers robustness\nand why uncertainty reliably identifies susceptible concepts, providing a\nprincipled basis that preserves both interpretability and resilience in the\npresence of noise.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.16705v1;http://arxiv.org/pdf/2505.16705v1", "pdf_url": "http://arxiv.org/pdf/2505.16705v1"}, {"title": "Deep anomaly detection with partition contrastive learning for tabular data", "link": "https://link.springer.com/article/10.1007/s10618-025-01102-w", "details": "Y Li, Y Wang, H Xu, B Li, X Zhou - Data Mining and Knowledge Discovery, 2025", "abstract": "Self-supervised anomaly detection (AD) methods define transformations and surrogate tasks to deeply learn data \u201cnormality\u201d, presenting superior performance. Different from most existing work designed for images, this paper considers self \u2026"}, {"title": "GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data", "link": "https://arxiv.org/pdf/2505.03233", "details": "S Deng, M Yan, S Wei, H Ma, Y Yang, J Chen, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post- training. However, existing models rely heavily on real-world data, which is costly \u2026", "entry_id": "http://arxiv.org/abs/2505.03233v1", "updated": "2025-05-06 06:59:28", "published": "2025-05-06 06:59:28", "authors": "Shengliang Deng;Mi Yan;Songlin Wei;Haixin Ma;Yuxin Yang;Jiayi Chen;Zhiqi Zhang;Taoyu Yang;Xuheng Zhang;Heming Cui;Zhizheng Zhang;He Wang", "summary": "Embodied foundation models are gaining increasing attention for their\nzero-shot generalization, scalability, and adaptability to new tasks through\nfew-shot post-training. However, existing models rely heavily on real-world\ndata, which is costly and labor-intensive to collect. Synthetic data offers a\ncost-effective alternative, yet its potential remains largely underexplored. To\nbridge this gap, we explore the feasibility of training Vision-Language-Action\nmodels entirely with large-scale synthetic action data. We curate SynGrasp-1B,\na billion-frame robotic grasping dataset generated in simulation with\nphotorealistic rendering and extensive domain randomization. Building on this,\nwe present GraspVLA, a VLA model pretrained on large-scale synthetic action\ndata as a foundational model for grasping tasks. GraspVLA integrates\nautoregressive perception tasks and flow-matching-based action generation into\na unified Chain-of-Thought process, enabling joint training on synthetic action\ndata and Internet semantics data. This design helps mitigate sim-to-real gaps\nand facilitates the transfer of learned actions to a broader range of\nInternet-covered objects, achieving open-vocabulary generalization in grasping.\nExtensive evaluations across real-world and simulation benchmarks demonstrate\nGraspVLA's advanced zero-shot generalizability and few-shot adaptability to\nspecific human preferences. We will release SynGrasp-1B dataset and pre-trained\nweights to benefit the community.", "comment": null, "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO", "links": "http://arxiv.org/abs/2505.03233v1;http://arxiv.org/pdf/2505.03233v1", "pdf_url": "http://arxiv.org/pdf/2505.03233v1"}, {"title": "Unsupervised Network Anomaly Detection with Autoencoders and Traffic Images", "link": "https://arxiv.org/pdf/2505.16650", "details": "M Neri, S Baldoni - arXiv preprint arXiv:2505.16650, 2025", "abstract": "Due to the recent increase in the number of connected devices, the need to promptly detect security issues is emerging. Moreover, the high number of communication flows creates the necessity of processing huge amounts of data. Furthermore, the \u2026", "entry_id": "http://arxiv.org/abs/2505.16650v1", "updated": "2025-05-22 13:19:30", "published": "2025-05-22 13:19:30", "authors": "Michael Neri;Sara Baldoni", "summary": "Due to the recent increase in the number of connected devices, the need to\npromptly detect security issues is emerging. Moreover, the high number of\ncommunication flows creates the necessity of processing huge amounts of data.\nFurthermore, the connected devices are heterogeneous in nature, having\ndifferent computational capacities. For this reason, in this work we propose an\nimage-based representation of network traffic which allows to realize a compact\nsummary of the current network conditions with 1-second time windows. The\nproposed representation highlights the presence of anomalies thus reducing the\nneed for complex processing architectures. Finally, we present an unsupervised\nlearning approach which effectively detects the presence of anomalies. The code\nand the dataset are available at\nhttps://github.com/michaelneri/image-based-network-traffic-anomaly-detection.", "comment": "Accepted for publication in EUSIPCO 2025", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.CR;eess.IV;eess.SP", "links": "http://arxiv.org/abs/2505.16650v1;http://arxiv.org/pdf/2505.16650v1", "pdf_url": "http://arxiv.org/pdf/2505.16650v1"}, {"title": "Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable Structured Latent Modelling", "link": "https://arxiv.org/pdf/2505.16481", "details": "X Shi, X Jiang, MA \u00c1lvarez - arXiv preprint arXiv:2505.16481, 2025", "abstract": "Gaussian Process (GP) Variational Autoencoders (VAEs) extend standard VAEs by replacing the fully factorised Gaussian prior with a GP prior, thereby capturing richer correlations among latent variables. However, performing exact GP inference in \u2026", "entry_id": "http://arxiv.org/abs/2505.16481v1", "updated": "2025-05-22 10:07:33", "published": "2025-05-22 10:07:33", "authors": "Xinxing Shi;Xiaoyu Jiang;Mauricio A. \u00c1lvarez", "summary": "Gaussian Process (GP) Variational Autoencoders (VAEs) extend standard VAEs by\nreplacing the fully factorised Gaussian prior with a GP prior, thereby\ncapturing richer correlations among latent variables. However, performing exact\nGP inference in large-scale GPVAEs is computationally prohibitive, often\nforcing existing approaches to rely on restrictive kernel assumptions or large\nsets of inducing points. In this work, we propose a neighbour-driven\napproximation strategy that exploits local adjacencies in the latent space to\nachieve scalable GPVAE inference. By confining computations to the nearest\nneighbours of each data point, our method preserves essential latent\ndependencies, allowing more flexible kernel choices and mitigating the need for\nnumerous inducing points. Through extensive experiments on tasks including\nrepresentation learning, data imputation, and conditional generation, we\ndemonstrate that our approach outperforms other GPVAE variants in both\npredictive performance and computational efficiency.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;stat.ML", "links": "http://arxiv.org/abs/2505.16481v1;http://arxiv.org/pdf/2505.16481v1", "pdf_url": "http://arxiv.org/pdf/2505.16481v1"}, {"title": "SPAR: Self-supervised Placement-Aware Representation Learning for Multi-Node IoT Systems", "link": "https://arxiv.org/pdf/2505.16936", "details": "Y Chen, T Wang, Y Lyu, Y Hu, J Li, T Kimura, H Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This work develops the underpinnings of self-supervised placement-aware representation learning given spatially-distributed (multi-view and multimodal) sensor observations, motivated by the need to represent external environmental \u2026", "entry_id": "http://arxiv.org/abs/2505.16936v2", "updated": "2025-05-23 05:14:24", "published": "2025-05-22 17:26:23", "authors": "Yizhuo Chen;Tianchen Wang;You Lyu;Yanlan Hu;Jinyang Li;Tomoyoshi Kimura;Hongjue Zhao;Yigong Hu;Denizhan Kara;Tarek Abdelzaher", "summary": "This work develops the underpinnings of self-supervised placement-aware\nrepresentation learning given spatially-distributed (multi-view and multimodal)\nsensor observations, motivated by the need to represent external environmental\nstate in multi-sensor IoT systems in a manner that correctly distills spatial\nphenomena from the distributed multi-vantage observations. The objective of\nsensing in IoT systems is, in general, to collectively represent an externally\nobserved environment given multiple vantage points from which sensory\nobservations occur. Pretraining of models that help interpret sensor data must\ntherefore encode the relation between signals observed by sensors and the\nobservers' vantage points in order to attain a representation that encodes the\nobserved spatial phenomena in a manner informed by the specific placement of\nthe measuring instruments, while allowing arbitrary placement. The work\nsignificantly advances self-supervised model pretraining from IoT signals\nbeyond current solutions that often overlook the distinctive spatial nature of\nIoT data. Our framework explicitly learns the dependencies between measurements\nand geometric observer layouts and structural characteristics, guided by a core\ndesign principle: the duality between signals and observer positions. We\nfurther provide theoretical analyses from the perspectives of information\ntheory and occlusion-invariant representation learning to offer insight into\nthe rationale behind our design. Experiments on three real-world\ndatasets--covering vehicle monitoring, human activity recognition, and\nearthquake localization--demonstrate the superior generalizability and\nrobustness of our method across diverse modalities, sensor placements,\napplication-level inference tasks, and spatial scales.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.16936v2;http://arxiv.org/pdf/2505.16936v2", "pdf_url": "http://arxiv.org/pdf/2505.16936v2"}, {"title": "Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization", "link": "https://arxiv.org/pdf/2505.15660", "details": "J Zhou, K Ye, J Liu, T Ma, Z Wang, R Qiu, KY Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The generalization capabilities of vision-language-action (VLA) models to unseen tasks are crucial to achieving general-purpose robotic manipulation in open-world settings. However, the cross-task generalization capabilities of existing VLA models \u2026", "entry_id": "http://arxiv.org/abs/2505.15660v1", "updated": "2025-05-21 15:35:57", "published": "2025-05-21 15:35:57", "authors": "Jiaming Zhou;Ke Ye;Jiayi Liu;Teli Ma;Zifang Wang;Ronghe Qiu;Kun-Yu Lin;Zhilin Zhao;Junwei Liang", "summary": "The generalization capabilities of vision-language-action (VLA) models to\nunseen tasks are crucial to achieving general-purpose robotic manipulation in\nopen-world settings. However, the cross-task generalization capabilities of\nexisting VLA models remain significantly underexplored. To address this gap, we\nintroduce AGNOSTOS, a novel simulation benchmark designed to rigorously\nevaluate cross-task zero-shot generalization in manipulation. AGNOSTOS\ncomprises 23 unseen manipulation tasks for testing, distinct from common\ntraining task distributions, and incorporates two levels of generalization\ndifficulty to assess robustness. Our systematic evaluation reveals that current\nVLA models, despite being trained on diverse datasets, struggle to generalize\neffectively to these unseen tasks. To overcome this limitation, we propose\nCross-Task In-Context Manipulation (X-ICM), a method that conditions large\nlanguage models (LLMs) on in-context demonstrations from seen tasks to predict\naction sequences for unseen tasks. Additionally, we introduce a dynamics-guided\nsample selection strategy that identifies relevant demonstrations by capturing\ncross-task dynamics. On AGNOSTOS, X-ICM significantly improves cross-task\nzero-shot generalization performance over leading VLAs. We believe AGNOSTOS and\nX-ICM will serve as valuable tools for advancing general-purpose robotic\nmanipulation.", "comment": "Project Page: https://jiaming-zhou.github.io/AGNOSTOS", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.CV", "links": "http://arxiv.org/abs/2505.15660v1;http://arxiv.org/pdf/2505.15660v1", "pdf_url": "http://arxiv.org/pdf/2505.15660v1"}, {"title": "Interactive Post-Training for Vision-Language-Action Models", "link": "https://arxiv.org/pdf/2505.17016", "details": "S Tan, K Dou, Y Zhao, P Kr\u00e4henb\u00fchl - arXiv preprint arXiv:2505.17016, 2025", "abstract": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training \u2026", "entry_id": "http://arxiv.org/abs/2505.17016v1", "updated": "2025-05-22 17:59:45", "published": "2025-05-22 17:59:45", "authors": "Shuhan Tan;Kairan Dou;Yue Zhao;Philipp Kr\u00e4henb\u00fchl", "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.", "comment": "Project page: https://ariostgx.github.io/ript_vla/", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CV;cs.RO", "links": "http://arxiv.org/abs/2505.17016v1;http://arxiv.org/pdf/2505.17016v1", "pdf_url": "http://arxiv.org/pdf/2505.17016v1"}]
