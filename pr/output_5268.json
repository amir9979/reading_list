[{"title": "Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models", "link": "https://aclanthology.org/2024.acl-long.525.pdf", "details": "Y Sun, C Liu, K Zhou, J Huang, R Song, WX Zhao\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Humans often interact with large language models (LLMs) in multi-turn interaction to obtain desired answers or more information. However, most existing studies overlook the multi-turn instruction following ability of LLMs, in terms of training dataset, training \u2026"}, {"title": "When Phrases Meet Probabilities: Enabling Open Relation Extraction with Cooperating Large Language Models", "link": "https://aclanthology.org/2024.acl-long.709.pdf", "details": "J Wang, L Zhang, WS Lee, Y Zhong, L Kang, J Liu - \u2026 of the 62nd Annual Meeting of \u2026, 2024", "abstract": "Current clustering-based open relation extraction (OpenRE) methods usually apply clustering algorithms on top of pre-trained language models. However, this practice has three drawbacks. First, embeddings from language models are high-dimensional \u2026"}, {"title": "IAPT: Instance-Aware Prompt Tuning for Large Language Models", "link": "https://aclanthology.org/2024.acl-long.771.pdf", "details": "W Zhu, A Tian, C Yin, Y Ni, X Wang, G Xie - Proceedings of the 62nd Annual Meeting \u2026, 2024", "abstract": "Soft prompt tuning is a widely studied parameter-efficient fine-tuning method. However, it has a clear drawback: many soft tokens must be inserted into the input sequences to guarantee downstream performance. As a result, soft prompt tuning is \u2026"}, {"title": "End-to-end Learning of Logical Rules for Enhancing Document-level Relation Extraction", "link": "https://aclanthology.org/2024.acl-long.391.pdf", "details": "K Qi, J Du, H Wan - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Document-level relation extraction (DocRE) aims to extract relations between entities in a whole document. One of the pivotal challenges of DocRE is to capture the intricate interdependencies between relations of entity pairs. Previous methods have \u2026"}, {"title": "A GPT-based EHR modeling system for unsupervised novel disease detection", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001242", "details": "B Hao, Y Hu, WG Adams, SA Assoumou, HE Hsu\u2026 - Journal of Biomedical \u2026, 2024", "abstract": "Abstract Objective To develop an Artificial Intelligence (AI)-based anomaly detection model as a complement of an \u201castute physician\u201d in detecting novel disease cases in a hospital and preventing emerging outbreaks. Methods Data included hospitalized \u2026"}, {"title": "DOSSIER: Fact checking in electronic health records while preserving patient privacy", "link": "https://www.amazon.science/publications/dossier-fact-checking-in-electronic-health-records-while-preserving-patient-privacy", "details": "H Zhang, S Nagesh, M Shyani, N Mishra - 2024", "abstract": "Given a particular claim about a specific document, the fact checking problem is to determine if the claim is true and, if so, provide corroborating evidence. The problem is motivated by contexts where a document is too lengthy to quickly read and find an \u2026"}, {"title": "On the role of the UMLS in supporting diagnosis generation differential diagnoses proposed by Large Language Models", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001254", "details": "M Afshar, Y Gao, D Gupta, E Croxford\u2026 - Journal of Biomedical \u2026, 2024", "abstract": "Objective: Traditional knowledge-based and machine learning diagnostic decision support systems have benefited from integrating the medical domain knowledge encoded in the Unified Medical Language System (UMLS). The emergence of Large \u2026"}, {"title": "LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin", "link": "https://aclanthology.org/2024.acl-long.106.pdf", "details": "S Dou, E Zhou, Y Liu, S Gao, W Shen, L Xiong, Y Zhou\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Substantially increasing instruction data is a direct solution to \u2026"}, {"title": "Continual Few-shot Relation Extraction via Adaptive Gradient Correction and Knowledge Decomposition", "link": "https://aclanthology.org/2024.findings-acl.702.pdf", "details": "J Hu, C Tan, J Xu, XK XiangyunKong - Findings of the Association for Computational \u2026, 2024", "abstract": "Continual few-shot relation extraction (CFRE) aims to continually learn new relations with limited samples. However, current methods neglect the instability of embeddings in the process of different task training, which leads to serious catastrophic forgetting \u2026"}]
