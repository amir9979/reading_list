[{"title": "Membership Inference Attacks against Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.02902", "details": "Z Li, Y Wu, Y Chen, F Tonin, EA Rocamora, V Cevher - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of \u2026"}, {"title": "Griffon-G: Bridging Vision-Language and Vision-Centric Tasks via Large Multimodal Models", "link": "https://arxiv.org/pdf/2410.16163", "details": "Y Zhan, H Zhao, Y Zhu, F Yang, M Tang, J Wang - arXiv preprint arXiv:2410.16163, 2024", "abstract": "Large Multimodal Models (LMMs) have achieved significant breakthroughs in various vision-language and vision-centric tasks based on auto-regressive modeling. However, these models typically focus on either vision-centric tasks, such as visual \u2026"}, {"title": "A Novel Interpretability Metric for Explaining Bias in Language Models: Applications on Multilingual Models from Southeast Asia", "link": "https://arxiv.org/pdf/2410.15464", "details": "LCL Gamboa, M Lee - arXiv preprint arXiv:2410.15464, 2024", "abstract": "Work on bias in pretrained language models (PLMs) focuses on bias evaluation and mitigation and fails to tackle the question of bias attribution and explainability. We propose a novel metric, the $\\textit {bias attribution score} $, which draws from \u2026"}, {"title": "Medical large language models are susceptible to targeted misinformation attacks", "link": "https://www.nature.com/articles/s41746-024-01282-7", "details": "T Han, S Nebelung, F Khader, T Wang\u2026 - NPJ Digital Medicine, 2024", "abstract": "Large language models (LLMs) have broad medical knowledge and can reason about medical information across many domains, holding promising potential for diverse medical applications in the near future. In this study, we demonstrate a \u2026"}]
