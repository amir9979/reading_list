[{"title": "Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning", "link": "https://arxiv.org/pdf/2505.13081", "details": "X Yang, J Lu, E Yu - arXiv preprint arXiv:2505.13081, 2025", "abstract": "This paper uncovers a critical yet overlooked phenomenon in multi-modal large language models (MLLMs): detrimental concept drift within chain-of-thought (CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where reasoning \u2026", "entry_id": "http://arxiv.org/abs/2505.13081v1", "updated": "2025-05-19 13:13:38", "published": "2025-05-19 13:13:38", "authors": "Xiaoyu Yang;Jie Lu;En Yu", "summary": "This paper uncovers a critical yet overlooked phenomenon in multi-modal large\nlanguage models (MLLMs): detrimental concept drift within chain-of-thought\n(CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where\nreasoning token distributions evolve unpredictably, thereby introducing\nsignificant biases in final predictions. To address this, we are pioneers in\nestablishing the theoretical bridge between concept drift theory and RFT\nprocesses by formalizing CoT's autoregressive token streams as non-stationary\ndistributions undergoing arbitrary temporal shifts. Leveraging this framework,\nwe propose a novel counterfact-aware RFT that systematically decouples\nbeneficial distribution adaptation from harmful concept drift through concept\ngraph-empowered LLM experts generating counterfactual reasoning trajectories.\nOur solution, Counterfactual Preference Optimization (CPO), enables stable RFT\nin non-stationary environments, particularly within the medical domain, through\ncustom-tuning of counterfactual-aware preference alignment. Extensive\nexperiments demonstrate our superior performance of robustness, generalization\nand coordination within RFT. Besides, we also contributed a large-scale dataset\nCXR-CounterFact (CCF), comprising 320,416 meticulously curated counterfactual\nreasoning trajectories derived from MIMIC-CXR. Our code and data are public.", "comment": "17 pages, 5figures", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CV", "links": "http://arxiv.org/abs/2505.13081v1;http://arxiv.org/pdf/2505.13081v1", "pdf_url": "http://arxiv.org/pdf/2505.13081v1"}, {"title": "A Deep Foundation Model for ECG Interpretation: Enabling Rare Disease Detection Through Transfer Learning", "link": "https://academic.oup.com/ehjdh/advance-article-pdf/doi/10.1093/ehjdh/ztaf051/63266097/ztaf051.pdf", "details": "S Hu, JP Barrios, GH Tison - European Heart Journal-Digital Health, 2025", "abstract": "In healthcare, scarcity of high-quality human-adjudicated labelled data may limit the potential of deep neural networks (DNNs). Foundation models provide an efficient starting point for deep learning that can facilitate effective DNN training with fewer \u2026"}, {"title": "Identifying signs and symptoms of pancreatic cancer: a population-based study using electronic health records and natural language processing", "link": "https://www.sciencedirect.com/science/article/pii/S1424390325000900", "details": "W Chen, F Xie, TQ Luong, J Chang, E Lustigova\u2026 - Pancreatology, 2025", "abstract": "ABSTRACT OBJECTIVES Patient-reported symptoms are often the trigger for diagnosis of pancreatic cancer (PC). We aimed to report the prevalence and lead time of symptoms of PC from electronic health records in a population-based sample \u2026"}, {"title": "Model Merging in Pre-training of Large Language Models", "link": "https://arxiv.org/pdf/2505.12082", "details": "Y Li, Y Ma, S Yan, C Zhang, J Liu, J Lu, Z Xu, M Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model \u2026", "entry_id": "http://arxiv.org/abs/2505.12082v3", "updated": "2025-05-22 09:35:43", "published": "2025-05-17 16:53:14", "authors": "Yunshui Li;Yiyuan Ma;Shen Yan;Chaoyi Zhang;Jing Liu;Jianqiao Lu;Ziwen Xu;Mengzhao Chen;Minrui Wang;Shiyi Zhan;Jin Ma;Xunhao Lai;Deyi Liu;Yao Luo;Xingyan Bin;Hongbin Ren;Mingji Han;Wenhao Hao;Bairen Yi;LingJun Liu;Bole Ma;Xiaoying Jia;Xun Zhou;Siyuan Qiao;Liang Xiang;Yonghui Wu", "summary": "Model merging has emerged as a promising technique for enhancing large\nlanguage models, though its application in large-scale pre-training remains\nrelatively unexplored. In this paper, we present a comprehensive investigation\nof model merging techniques during the pre-training process. Through extensive\nexperiments with both dense and Mixture-of-Experts (MoE) architectures ranging\nfrom millions to over 100 billion parameters, we demonstrate that merging\ncheckpoints trained with constant learning rates not only achieves significant\nperformance improvements but also enables accurate prediction of annealing\nbehavior. These improvements lead to both more efficient model development and\nsignificantly lower training costs. Our detailed ablation studies on merging\nstrategies and hyperparameters provide new insights into the underlying\nmechanisms while uncovering novel applications. Through comprehensive\nexperimental analysis, we offer the open-source community practical\npre-training guidelines for effective model merging.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.12082v3;http://arxiv.org/pdf/2505.12082v3", "pdf_url": "http://arxiv.org/pdf/2505.12082v3"}, {"title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example", "link": "https://arxiv.org/pdf/2504.20571", "details": "Y Wang, Q Yang, Z Zeng, L Ren, L Liu, B Peng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2. 5-Math \u2026", "entry_id": "http://arxiv.org/abs/2504.20571v1", "updated": "2025-04-29 09:24:30", "published": "2025-04-29 09:24:30", "authors": "Yiping Wang;Qing Yang;Zhiyuan Zeng;Liliang Ren;Lucas Liu;Baolin Peng;Hao Cheng;Xuehai He;Kuan Wang;Jianfeng Gao;Weizhu Chen;Shuohang Wang;Simon Shaolei Du;Yelong Shen", "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR", "comment": "28 pages, 12 figures, link: https://github.com/ypwang61/One-Shot-RLVR", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CL", "links": "http://arxiv.org/abs/2504.20571v1;http://arxiv.org/pdf/2504.20571v1", "pdf_url": "http://arxiv.org/pdf/2504.20571v1"}]
