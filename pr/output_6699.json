[{"title": "LOLA--An Open-Source Massively Multilingual Large Language Model", "link": "https://arxiv.org/pdf/2409.11272", "details": "N Srivastava, D Kuchelev, T Moteu, K Shetty, M Roeder\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper presents LOLA, a massively multilingual large language model trained on more than 160 languages using a sparse Mixture-of-Experts Transformer architecture. Our architectural and implementation choices address the challenge of \u2026"}]
