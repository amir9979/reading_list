[{"title": "How Well Can Vision Language Models See Image Details?", "link": "https://arxiv.org/pdf/2408.03940", "details": "C Gou, A Felemban, FF Khan, D Zhu, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Model-based Vision-Language Models (LLM-based VLMs) have demonstrated impressive results in various vision-language understanding tasks. However, how well these VLMs can see image detail beyond the semantic level \u2026"}, {"title": "A Survey on Employing Large Language Models for Text-to-SQL Tasks", "link": "https://arxiv.org/pdf/2407.15186", "details": "L Shi, Z Tang, Z Yang - arXiv preprint arXiv:2407.15186, 2024", "abstract": "The increasing volume of data stored in relational databases has led to the need for efficient querying and utilization of this data in various sectors. However, writing SQL queries requires specialized knowledge, which poses a challenge for non \u2026"}, {"title": "AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization", "link": "https://arxiv.org/pdf/2407.11591", "details": "A Afzal, R Chalumattu, F Matthes, LM Espuny - arXiv preprint arXiv:2407.11591, 2024", "abstract": "Despite the advances in the abstractive summarization task using Large Language Models (LLM), there is a lack of research that asses their abilities to easily adapt to different domains. We evaluate the domain adaptation abilities of a wide range of \u2026"}, {"title": "MindLLM: Lightweight large language model pre-training, evaluation and domain application", "link": "https://www.sciencedirect.com/science/article/pii/S2666651024000111", "details": "Y Yang, H Sun, J Li, R Liu, Y Li, Y Liu, Y Gao, H Huang - AI Open, 2024", "abstract": "Abstract Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is \u2026"}, {"title": "Token-Supervised Value Models for Enhancing Mathematical Reasoning Capabilities of Large Language Models", "link": "https://arxiv.org/pdf/2407.12863", "details": "JH Lee, JY Yang, B Heo, D Han, KM Yoo - arXiv preprint arXiv:2407.12863, 2024", "abstract": "Large Language Models (LLMs) have demonstrated impressive problem-solving capabilities in mathematics through step-by-step reasoning chains. However, they are susceptible to reasoning errors that impact the quality of subsequent reasoning \u2026"}, {"title": "Molecule Language Model with Augmented Pairs and Expertise Transfer", "link": "https://arxiv.org/pdf/2407.09043", "details": "N Lee, S Laghuvarapu, C Park, J Sun - arXiv preprint arXiv:2407.09043, 2024", "abstract": "Understanding the molecules and their textual descriptions via molecule language models (MoLM) recently got a surge of interest among researchers. However, unique challenges exist in the field of MoLM due to 1) a limited amount of molecule-text \u2026"}, {"title": "SubMerge: Merging Equivalent Subword Tokenizations for Subword Regularized Models in Neural Machine Translation", "link": "https://pubs.cs.uct.ac.za/id/eprint/1666/1/Submerge.pdf", "details": "H Song, F Meyer, R Dabre, H Tanaka, C Chu\u2026 - 2024", "abstract": "Subword regularized models leverage multiple subword tokenizations of one target sentence during training. Previous decoding algorithms select one tokenization during inference, leading to the underutilization of knowledge learned about multiple \u2026"}]
