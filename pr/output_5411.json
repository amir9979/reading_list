[{"title": "Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models", "link": "https://arxiv.org/pdf/2407.20271", "details": "H Tang, Y Liu, X Liu, K Zhang, Y Zhang, Q Liu, E Chen - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in machine learning, especially in Natural Language Processing (NLP), have led to the development of sophisticated models trained on vast datasets, but this progress has raised concerns about potential sensitive \u2026"}, {"title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "link": "https://arxiv.org/pdf/2407.21417", "details": "Z Wu, Y Zhang, P Qi, Y Xu, R Han, Y Zhang, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (ie, follow open-ended instructions) and \u2026"}, {"title": "Evaluating LLMs for Text-to-SQL Generation With Complex SQL Workload", "link": "https://arxiv.org/pdf/2407.19517", "details": "L Ma, K Pu, Y Zhu - arXiv preprint arXiv:2407.19517, 2024", "abstract": "This study presents a comparative analysis of the a complex SQL benchmark, TPC- DS, with two existing text-to-SQL benchmarks, BIRD and Spider. Our findings reveal that TPC-DS queries exhibit a significantly higher level of structural complexity \u2026"}, {"title": "Collaborative Training of Tiny-Large Vision Language Models", "link": "https://openreview.net/pdf%3Fid%3DUgy9DyhYYD", "details": "S Lu, L Guo, W Wang, Z Zhao, T Yue, J Liu, S Liu - ACM Multimedia 2024", "abstract": "In recent years, large vision language models (LVLMs) have significantly advanced artificial intelligence, especially in integrating visual and linguistic data for complex tasks like visual conversation, image captioning and visual question answering \u2026"}, {"title": "MixPrompt: Enhancing Generalizability and Adversarial Robustness for Vision-Language Models via Prompt Fusion", "link": "https://link.springer.com/chapter/10.1007/978-981-97-5606-3_28", "details": "H Fan, Z Ma, Y Li, R Tian, Y Chen, C Gao - International Conference on Intelligent \u2026, 2024", "abstract": "Abstract Pretrained Vision-Language Models (VLMs) like CLIP have exhibited remarkable capacities across downstream tasks, while their image encoders are vulnerable to adversarial examples. A recently introduced lightweight approach \u2026"}, {"title": "CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning", "link": "https://arxiv.org/pdf/2407.21011", "details": "Y Du, B Chang, NC Dvornek - arXiv preprint arXiv:2407.21011, 2024", "abstract": "Recent advancements in Contrastive Language-Image Pre-training (CLIP) have demonstrated notable success in self-supervised representation learning across various tasks. However, the existing CLIP-like approaches often demand extensive \u2026"}, {"title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT", "link": "https://arxiv.org/pdf/2407.21058", "details": "M Ali, S Panda, Q Shen, M Wick, A Kobren - arXiv preprint arXiv:2407.21058, 2024", "abstract": "In the current landscape of language model research, larger models, larger datasets and more compute seems to be the only way to advance towards intelligence. While there have been extensive studies of scaling laws and models' scaling behaviors, the \u2026"}, {"title": "Cognitive Assessment of Language Models", "link": "https://openreview.net/pdf%3Fid%3DpxRh1meUvN", "details": "D McDuff, D Munday, X Liu, I Galatzer-Levy - ICML 2024 Workshop on LLMs and Cognition", "abstract": "Large language models (LLMs) are a subclass of generative artificial intelligence that can interpret language inputs to generate novel responses. These capabilities are conceptualized as a significant step forward in artificial intelligence because the \u2026"}, {"title": "SSuieBERT: Domain Adaptation Model for Chinese Space Science Text Mining and Information Extraction", "link": "https://www.mdpi.com/2079-9292/13/15/2949", "details": "Y Liu, S Li, Y Deng, S Hao, L Wang - Electronics, 2024", "abstract": "With the continuous exploration of space science, a large number of domain-related materials and scientific literature are constantly generated, mostly in the form of text, which contains rich and unexplored domain knowledge. Natural language \u2026"}]
