[{"title": "Deep learning-based classification of early-stage mycosis fungoides and benign inflammatory dermatoses on hematoxylin and eosin-stained whole-slide images: a \u2026", "link": "https://www.sciencedirect.com/science/article/pii/S0022202X24021018", "details": "T Doeleman, S Brussee, LM Hondelink\u2026 - Journal of Investigative \u2026, 2024", "abstract": "The diagnosis of early-stage mycosis fungoides (MF) is challenging due to shared clinical and histopathological features with benign inflammatory dermatoses (BIDs). Recent evidence has shown that deep learning (DL) can assist pathologists in \u2026"}, {"title": "Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination", "link": "https://arxiv.org/pdf/2409.12746", "details": "ES Salido, R Morante, J Gonzalo, G Marco\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this article we present UNED-ACCESS 2024, a bilingual dataset that consists of 1003 multiple-choice questions of university entrance level exams in Spanish and English. Questions are originally formulated in Spanish and translated manually into \u2026"}, {"title": "Computational Pathology for Accurate Prediction of Breast Cancer Recurrence: Development and Validation of a Deep Learning-based Tool", "link": "https://arxiv.org/pdf/2409.15491", "details": "Z Su, Y Guo, R Wesolowski, G Tozbikian, NS O'Connell\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Accurate recurrence risk stratification is crucial for optimizing treatment plans for breast cancer patients. Current prognostic tools like Oncotype DX (ODX) offer valuable genomic insights for HR+/HER2-patients but are limited by cost and \u2026"}, {"title": "LOLA--An Open-Source Massively Multilingual Large Language Model", "link": "https://arxiv.org/pdf/2409.11272", "details": "N Srivastava, D Kuchelev, T Moteu, K Shetty, M Roeder\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper presents LOLA, a massively multilingual large language model trained on more than 160 languages using a sparse Mixture-of-Experts Transformer architecture. Our architectural and implementation choices address the challenge of \u2026"}]
