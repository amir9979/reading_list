[{"title": "Examining Linguistic Differences in Electronic Health Records for Diverse Patients With Diabetes: Natural Language Processing Analysis", "link": "https://medinform.jmir.org/2024/1/e50428/", "details": "I Bilotta, S Tonidandel, WR Liaw, E King, DN Carvajal\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background: Individuals from minoritized racial and ethnic backgrounds experience pernicious and pervasive health disparities that have emerged, in part, from clinician bias. Objective: We used a natural language processing approach to examine \u2026"}, {"title": "Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning", "link": "https://arxiv.org/pdf/2406.07081", "details": "M Cui, J Du, S Zhu, D Xiong - arXiv preprint arXiv:2406.07081, 2024", "abstract": "Large language models (LLMs) exhibit outstanding performance in machine translation via in-context learning. In contrast to sentence-level translation, document- level translation (DOCMT) by LLMs based on in-context learning faces two major \u2026"}, {"title": "Mixture of In-Context Prompters for Tabular PFNs", "link": "https://arxiv.org/pdf/2405.16156", "details": "D Xu, O Cirit, R Asadi, Y Sun, W Wang - arXiv preprint arXiv:2405.16156, 2024", "abstract": "Recent benchmarks found In-Context Learning (ICL) outperforms both deep learning and tree-based algorithms on small tabular datasets. However, on larger datasets, ICL for tabular learning cannot run without severely compromising performance, due \u2026"}, {"title": "CAM: A cross-lingual adaptation framework for low-resource language speech recognition", "link": "https://www.sciencedirect.com/science/article/pii/S1566253524002847", "details": "Q Hu, Y Zhang, X Zhang, Z Han, X Yu - Information Fusion, 2024", "abstract": "In this paper, a novel cross-lingual adaptation framework called CAM is presented for low-resource language speech recognition (LLSR). It is based on the recent popular adapter method. CAM is achieved by adapting self-supervised speech models \u2026"}, {"title": "Analyzing Effects of Learning Downstream Tasks on Moral Bias in Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.82.pdf", "details": "N Kiehne, A Ljapunov, M B\u00e4tje, WT Balke - \u2026 of the 2024 Joint International Conference \u2026, 2024", "abstract": "Pre-training and fine-tuning large language models (LMs) is currently the state-of-the- art methodology for enabling data-scarce downstream tasks. However, the derived models still tend to replicate and perpetuate social biases. To understand this \u2026"}]
