[{"title": "Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization", "link": "https://arxiv.org/pdf/2503.22577%3F", "details": "I Pikabea, I Lacunza, O Pareras, C Escolano\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Rapid advancements in Visual Language Models (VLMs) have transformed multimodal understanding but are often constrained by generating English responses regardless of the input language. This phenomenon has been termed as \u2026"}, {"title": "Vision-language foundation model for generalizable nasal disease diagnosis using unlabeled endoscopic records", "link": "https://www.sciencedirect.com/science/article/pii/S0031320325003061", "details": "X Liu, W Gong, X Chen, Z Li, Y Liu, L Wang, Q Liu\u2026 - Pattern Recognition, 2025", "abstract": "Medical artificial intelligence (AI) holds significant potential in identifying signs of health conditions in nasal endoscopic images, thereby accelerating the diagnosis of diseases and systemic disorders. However, the performance of AI models heavily \u2026"}, {"title": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection", "link": "https://arxiv.org/pdf/2504.00695", "details": "X Zhu, Z Gu, S Zheng, T Wang, T Li, H Feng, Y Xiao - arXiv preprint arXiv:2504.00695, 2025", "abstract": "Pre-training large language models (LLMs) necessitates enormous diverse textual corpora, making effective data selection a key challenge for balancing computational resources and model performance. Current methodologies primarily emphasize data \u2026"}, {"title": "QPP++ 2025: Query Performance Prediction and Its Applications in the Era of Large Language Models", "link": "https://link.springer.com/chapter/10.1007/978-3-031-88720-8_49", "details": "C Meng, G Faggioli, M Aliannejadi, N Ferro, J Mothe - European Conference on \u2026, 2025", "abstract": "Query performance prediction (QPP) is a key task in information retrieval (IR) and has been studied for over a decade. The task of (QPP) is defined as estimating search effectiveness without human relevance judgments. In this workshop, we aim \u2026"}, {"title": "ELOQUENT CLEF Shared Tasks for Evaluation of Generative Language Model Quality, 2025 Edition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-88720-8_56", "details": "J Karlgren, E Artemova, O Bojar, V Mikhailov\u2026 - European Conference on \u2026, 2025", "abstract": "The ELOQUENT lab for evaluation of generative language model quality and usefulness addresses high-level quality criteria through a set of open-ended shared tasks implemented, where possible, to leverage the ability of systems built on \u2026"}, {"title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?", "link": "https://arxiv.org/pdf/2504.00509", "details": "K Yan, Y Xu, Z Du, X Yao, Z Wang, X Guo, J Chen - arXiv preprint arXiv:2504.00509, 2025", "abstract": "The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' \u2026"}, {"title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.23064", "details": "Y Ren, K Tertikas, S Maiti, J Han, T Zhang, S S\u00fcsstrunk\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision-Language Models (LVLMs) struggle with puzzles, which require precise perception, rule comprehension, and logical reasoning. Assessing and enhancing their performance in this domain is crucial, as it reflects their ability to \u2026"}, {"title": "Beyond Standard MoE: Mixture of Latent Experts for Resource-Efficient Language Models", "link": "https://arxiv.org/pdf/2503.23100", "details": "Z Liu, H Wu, R She, X Fu, X Han, T Zhong, M Yuan - arXiv preprint arXiv:2503.23100, 2025", "abstract": "Mixture of Experts (MoE) has emerged as a pivotal architectural paradigm for efficient scaling of Large Language Models (LLMs), operating through selective activation of parameter subsets for each input token. Nevertheless, conventional MoE \u2026"}, {"title": "Synthetic Data Enhances Mathematical Reasoning of Language Models Based on Artificial Intelligence", "link": "https://www.itc.ktu.lt/index.php/ITC/article/view/39713/16892", "details": "Z Han, W Jiang - Information Technology and Control, 2025", "abstract": "Current large language models (LLMs) training involves extensive training data and computing resources to handle multiple natural language processing (NLP) tasks. This paper endeavors to assist individuals to compose feasible mathematical \u2026"}]
