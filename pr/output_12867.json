[{"title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers", "link": "https://arxiv.org/pdf/2501.16961%3F", "details": "M Raza, N Milic-Frayling - arXiv preprint arXiv:2501.16961, 2025", "abstract": "Robustness of reasoning remains a significant challenge for large language models, and addressing it is essential for the practical applicability of AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a novel approach that \u2026"}, {"title": "Vulnerability Mitigation for Safety-Aligned Language Models via Debiasing", "link": "https://arxiv.org/pdf/2502.02153", "details": "TQ Tran, A Wachi, R Sato, T Tanabe, Y Akimoto - arXiv preprint arXiv:2502.02153, 2025", "abstract": "Safety alignment is an essential research topic for real-world AI applications. Despite the multifaceted nature of safety and trustworthiness in AI, current safety alignment methods often focus on a comprehensive notion of safety. By carefully assessing \u2026"}, {"title": "Learning Conformal Abstention Policies for Adaptive Risk Management in Large Language and Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06884", "details": "S Tayebati, D Kumar, N Darabi, D Jayasuriya\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language and Vision-Language Models (LLMs/VLMs) are increasingly used in safety-critical applications, yet their opaque decision-making complicates risk assessment and reliability. Uncertainty quantification (UQ) helps assess prediction \u2026"}, {"title": "Scaling Pre-training to One Hundred Billion Data for Vision Language Models", "link": "https://arxiv.org/pdf/2502.07617", "details": "X Wang, I Alabdulmohsin, D Salz, Z Li, K Rong, X Zhai - arXiv preprint arXiv \u2026, 2025", "abstract": "We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric \u2026"}, {"title": "Tool Learning in the Wild: Empowering Language Models as Automatic Tool Agents", "link": "https://openreview.net/pdf%3Fid%3DT4wMdeFEjX", "details": "Z Shi, S Gao, L Yan, Y Feng, X Chen, Z Chen, D Yin\u2026 - THE WEB CONFERENCE 2025", "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a promising approach to extend their utility, enabling them to solve practical tasks. Previous methods manually parse tool documentation and create in-context \u2026"}, {"title": "Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models", "link": "https://arxiv.org/pdf/2501.18533%3F", "details": "Y Ding, L Li, B Cao, J Shao - arXiv preprint arXiv:2501.18533, 2025", "abstract": "Large Vision-Language Models (VLMs) have achieved remarkable performance across a wide range of tasks. However, their deployment in safety-critical domains poses significant challenges. Existing safety fine-tuning methods, which focus on \u2026"}, {"title": "Scaling Large Vision-Language Models for Enhanced Multimodal Comprehension In Biomedical Image Analysis", "link": "https://arxiv.org/pdf/2501.15370", "details": "R Umeike, N Getty, F Xia, R Stevens - arXiv preprint arXiv:2501.15370, 2025", "abstract": "Large language models (LLMs) have demonstrated immense capabilities in understanding textual data and are increasingly being adopted to help researchers accelerate scientific discovery through knowledge extraction (information retrieval) \u2026"}, {"title": "Personalization Toolkit: Training Free Personalization of Large Vision Language Models", "link": "https://arxiv.org/pdf/2502.02452%3F", "details": "S Seifi, V Dorovatas, DO Reino, R Aljundi - arXiv preprint arXiv:2502.02452, 2025", "abstract": "Large Vision Language Models (LVLMs) have significant potential to deliver personalized assistance by adapting to individual users' unique needs and preferences. Personalization of LVLMs is an emerging area that involves \u2026"}, {"title": "How Much Do Code Language Models Remember? An Investigation on Data Extraction Attacks before and after Fine-tuning", "link": "https://arxiv.org/pdf/2501.17501", "details": "F Salerno, A Al-Kaswan, M Izadi - arXiv preprint arXiv:2501.17501, 2025", "abstract": "Code language models, while widely popular, are often trained on unsanitized source code gathered from across the Internet. Previous work revealed that pre- trained models can remember the content of their training data and regurgitate them \u2026"}]
