[{"title": "Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving", "link": "https://arxiv.org/pdf/2503.21505", "details": "Y Li, M Tian, Z Lin, J Zhu, D Zhu, H Liu, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing benchmarks for Vision-Language Model (VLM) on autonomous driving (AD) primarily assess interpretability through open-form visual question answering (QA) within coarse-grained tasks, which remain insufficient to assess capabilities in \u2026"}, {"title": "Multi-Cue Adaptive Visual Token Pruning for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.08019", "details": "B Luan, W Zhou, H Feng, Z Wang, X Li, H Li - arXiv preprint arXiv:2503.08019, 2025", "abstract": "As the computational needs of Large Vision-Language Models (LVLMs) increase, visual token pruning has proven effective in improving inference speed and memory efficiency. Traditional pruning methods in LVLMs predominantly focus on attention \u2026"}, {"title": "UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces", "link": "https://arxiv.org/pdf/2503.06157", "details": "B Zhao, J Fang, Z Dai, Z Wang, J Zha, W Zhang, C Gao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large multimodal models exhibit remarkable intelligence, yet their embodied cognitive abilities during motion in open-ended urban 3D space remain to be explored. We introduce a benchmark to evaluate whether video-large language \u2026"}, {"title": "Inference retrieval-augmented multi-modal chain-of-thoughts reasoning for language models", "link": "https://ieeexplore.ieee.org/abstract/document/10888701/", "details": "Q He, S Qian, J Zhang, C Wang - ICASSP 2025-2025 IEEE International Conference \u2026, 2025", "abstract": "Recent advancements in Large Language Models (LLMs) have catalyzed the exploration of Chain of Thought (CoT) approaches, particularly in extending their application to multimodal tasks to enhance reasoning capabilities. However, current \u2026"}, {"title": "From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.06260", "details": "M Dai, J Sun, Z Zhao, S Liu, R Li, J Gao, X Li - arXiv preprint arXiv:2503.06260, 2025", "abstract": "Aligning large vision-language models (LVLMs) with human preferences is challenging due to the scarcity of fine-grained, high-quality, and multimodal preference data without human annotations. Existing methods relying on direct \u2026"}, {"title": "Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models", "link": "https://media.roboflow.com/rf100vl/rf100vl.pdf", "details": "P Robicheaux, M Popov, A Madan, I Robinson\u2026", "abstract": "Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to outof-distribution tasks \u2026"}, {"title": "Fine-grained knowledge fusion for retrieval-augmented medical visual question answering", "link": "https://www.sciencedirect.com/science/article/pii/S1566253525001320", "details": "X Liang, D Wang, B Jing, Z Jiao, R Li, R Liu, Q Miao\u2026 - Information Fusion, 2025", "abstract": "Given that medical image analysis often requires experts to recall typical symptoms from diagnostic archives or their own experience, implementing retrieval augmentation in multi-modal tasks like Medical Visual Question Answering \u2026"}, {"title": "Federated Continual Instruction Tuning", "link": "https://arxiv.org/pdf/2503.12897", "details": "H Guo, F Zeng, F Zhu, W Liu, DH Wang, J Xu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "A vast amount of instruction tuning data is crucial for the impressive performance of Large Multimodal Models (LMMs), but the associated computational costs and data collection demands during supervised fine-tuning make it impractical for most \u2026"}, {"title": "USP: Unified Self-Supervised Pretraining for Image Generation and Understanding", "link": "https://arxiv.org/pdf/2503.06132", "details": "X Chu, R Li, Y Wang - arXiv preprint arXiv:2503.06132, 2025", "abstract": "Recent studies have highlighted the interplay between diffusion models and representation learning. Intermediate representations from diffusion models can be leveraged for downstream visual tasks, while self-supervised vision models can \u2026"}]
