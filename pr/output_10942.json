[{"title": "FastVLM: Efficient Vision Encoding for Vision Language Models", "link": "https://arxiv.org/pdf/2412.13303", "details": "PKA Vasu, F Faghri, CL Li, C Koc, N True, A Antony\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high \u2026"}, {"title": "APOVIS: Automated pixel-level open-vocabulary instance segmentation through integration of pre-trained vision-language models and foundational segmentation \u2026", "link": "https://www.sciencedirect.com/science/article/pii/S026288562400489X", "details": "Q Ma, S Yang, L Zhang, Q Lan, D Yang, H Chen, Y Tan - Image and Vision \u2026, 2024", "abstract": "In recent years, substantial advancements have been achieved in vision-language integration and image segmentation, particularly through the use of pre-trained models like BERT and Vision Transformer (ViT). Within the domain of open \u2026"}, {"title": "HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.08378", "details": "S Zhu, W Dong, J Song, Y Guo, B Zheng - arXiv preprint arXiv:2412.08378, 2024", "abstract": "Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into \u2026"}, {"title": "Espresso: High Compression For Rich Extraction From Videos for Your Vision-Language Model", "link": "https://arxiv.org/pdf/2412.04729", "details": "KP Yu, A Dave, R Ambrus, J Mercat - arXiv preprint arXiv:2412.04729, 2024", "abstract": "Most of the current vision-language models (VLMs) for videos struggle to understand videos longer than a few seconds. This is primarily due to the fact that they do not scale to utilizing a large number of frames. In order to address this limitation, we \u2026"}, {"title": "Compositional Image Retrieval via Instruction-Aware Contrastive Learning", "link": "https://arxiv.org/pdf/2412.05756", "details": "W Zhong, W An, F Jiang, H Ma, Y Guo, J Huang - arXiv preprint arXiv:2412.05756, 2024", "abstract": "Composed Image Retrieval (CIR) involves retrieving a target image based on a composed query of an image paired with text that specifies modifications or changes to the visual reference. CIR is inherently an instruction-following task, as the model \u2026"}, {"title": "ConMix: Contrastive Learning with Mixup Augmentation for Dialogue Summarization", "link": "https://link.springer.com/chapter/10.1007/978-981-96-0847-8_18", "details": "Z Chen, J Xiao - International Conference on Advanced Data Mining \u2026, 2024", "abstract": "Seq2seq models have achieved remarkable performance on dialogue summarization, but the exposure bias problem still remains. Contrastive learning has been widely adopted to address this issue. However, previous contrastive learning \u2026"}, {"title": "SAT: Spatial Aptitude Training for Multimodal Language Models", "link": "https://arxiv.org/pdf/2412.07755", "details": "A Ray, J Duan, R Tan, D Bashkirova, R Hendrix\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Spatial perception is a fundamental component of intelligence. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only test for static spatial reasoning, such as categorizing the relative \u2026"}, {"title": "Qilin-Med-VL: Benchmarking Chinese Large Vision-Language Model for General Healthcare", "link": "https://www.researchgate.net/profile/Meng_Cao31/publication/386598823_Qilin-Med-VL_Benchmarking_Chinese_Large_Vision-Language_Model_for_General_Healthcare/links/6758096e3f7c7c7a83240bc7/Qilin-Med-VL-Benchmarking-Chinese-Large-Vision-Language-Model-for-General-Healthcare.pdf", "details": "J Liu, H Li, M Cao, Z Wang, Q Ye, D Chong, P Zhou\u2026", "abstract": "Abstract Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can \u2026"}, {"title": "HC-LLM: Historical-Constrained Large Language Models for Radiology Report Generation", "link": "https://arxiv.org/pdf/2412.11070", "details": "T Liu, J Wang, Y Hu, M Li, J Yi, X Chang, J Gao, B Yin - arXiv preprint arXiv \u2026, 2024", "abstract": "Radiology report generation (RRG) models typically focus on individual exams, often overlooking the integration of historical visual or textual data, which is crucial for patient follow-ups. Traditional methods usually struggle with long sequence \u2026"}]
