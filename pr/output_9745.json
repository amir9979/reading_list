[{"title": "Universal representations in cardiovascular ECG assessment: A self-supervised learning approach", "link": "https://www.sciencedirect.com/science/article/pii/S1386505624004052", "details": "ZY Liu, CH Lin, YC Hsu, JS Chen, PC Chang, MS Wen\u2026 - International Journal of \u2026, 2024", "abstract": "Background The 12-lead electrocardiogram (ECG) is an established modality for cardiovascular assessment. While deep learning algorithms have shown promising results for analyzing ECG data, the limited availability of labeled datasets hinders \u2026"}, {"title": "Sneaking Syntax into Transformer Language Models with Tree Regularization", "link": "https://arxiv.org/pdf/2411.18885", "details": "A Nandi, CD Manning, S Murty - arXiv preprint arXiv:2411.18885, 2024", "abstract": "While compositional accounts of human language understanding are based on a hierarchical tree-like process, neural models like transformers lack a direct inductive bias for such tree structures. Introducing syntactic inductive biases could unlock more \u2026"}, {"title": "Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass", "link": "https://arxiv.org/pdf/2411.05877%3F", "details": "T Chen, H Fang, P Xia, X Liu, B Van Durme\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LMs) are typically adapted to improve performance on new contexts (\\eg text prompts that define new tasks or domains) through fine-tuning or prompting. However, there is an accuracy compute tradeoff--fine-tuning incurs \u2026"}, {"title": "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.1249.pdf", "details": "X Lin, M Li, R Zemel, H Ji, SF Chang - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "Recently, enabling pretrained language models (PLMs) to perform zero-shot crossmodal tasks such as video question answering has been extensively studied. A popular approach is to learn a projection network that projects visual features into the \u2026"}, {"title": "Multimodal Whole Slide Foundation Model for Pathology", "link": "https://arxiv.org/pdf/2411.19666", "details": "T Ding, SJ Wagner, AH Song, RJ Chen, MY Lu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The field of computational pathology has been transformed with recent advances in foundation models that encode histopathology region-of-interests (ROIs) into versatile and transferable feature representations via self-supervised learning (SSL) \u2026"}, {"title": "Rephrasing Electronic Health Records for Pretraining Clinical Language Models", "link": "https://arxiv.org/pdf/2411.18940", "details": "J Liu, A Nguyen - arXiv preprint arXiv:2411.18940, 2024", "abstract": "Clinical language models are important for many applications in healthcare, but their development depends on access to extensive clinical text for pretraining. However, obtaining clinical notes from electronic health records (EHRs) at scale is challenging \u2026"}, {"title": "Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training", "link": "https://arxiv.org/pdf/2411.14318%3F", "details": "Z Luo, X Zhang, X Liu, H Li, Y Gong, C Qi, P Cheng - arXiv preprint arXiv:2411.14318, 2024", "abstract": "It is well-known that a diverse corpus is critical for training large language models, which are typically constructed from a mixture of various domains. In general, previous efforts resort to sampling training data from different domains with static \u2026"}]
