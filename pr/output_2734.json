[{"title": "Verbalized Machine Learning: Revisiting Machine Learning with Language Models", "link": "https://arxiv.org/pdf/2406.04344", "details": "TZ Xiao, R Bamler, B Sch\u00f6lkopf, W Liu - arXiv preprint arXiv:2406.04344, 2024", "abstract": "Motivated by the large progress made by large language models (LLMs), we introduce the framework of verbalized machine learning (VML). In contrast to conventional machine learning models that are typically optimized over a continuous \u2026"}, {"title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents", "link": "https://openreview.net/pdf%3Fid%3DDwTgy1hXXo", "details": "K Zhu, J Wang, Q Zhao, R Xu, X Xie - Forty-first International Conference on Machine \u2026", "abstract": "Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily \u2026"}, {"title": "Supportiveness-based Knowledge Rewriting for Retrieval-augmented Language Modeling", "link": "https://arxiv.org/pdf/2406.08116", "details": "Z Qiao, W Ye, Y Jiang, T Mo, P Xie, W Li, F Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Retrieval-augmented language models (RALMs) have recently shown great potential in mitigating the limitations of implicit knowledge in LLMs, such as untimely updating of the latest expertise and unreliable retention of long-tail knowledge. However \u2026"}, {"title": "Supplementary Material VILA: On Pre-training for Visual Language Models", "link": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Lin_VILA_On_Pre-training_CVPR_2024_supplemental.pdf", "details": "J Lin, H Yin, W Ping, P Molchanov, M Shoeybi, S Han", "abstract": "We used an in-house data blend for supervised finetuning/instruction tuning during the ablation study. We followed [5] to build the FLAN-style instructions from the training set of 18 visual language datasets, as shown in Table 1. We may see that \u2026"}, {"title": "On Trojans in Refined Language Models", "link": "https://arxiv.org/pdf/2406.07778", "details": "J Raghuram, G Kesidis, DJ Miller - arXiv preprint arXiv:2406.07778, 2024", "abstract": "A Trojan in a language model can be inserted when the model is refined for a particular application such as determining the sentiment of product reviews. In this paper, we clarify and empirically explore variations of the data-poisoning threat \u2026"}, {"title": "Accelerating Iterative Retrieval-augmented Language Model Serving with Speculation", "link": "https://openreview.net/pdf%3Fid%3DCDnv4vg02f", "details": "Z Zhang, A Zhu, L Yang, Y Xu, L Li, PM Phothilimthana\u2026 - Forty-first International Conference \u2026", "abstract": "This paper introduces RaLMSpec, a framework that accelerates iterative retrieval- augmented language model (RaLM) with* speculative retrieval* and* batched verification*. RaLMSpec further introduces several important systems optimizations \u2026"}, {"title": "Advancing DNA Language Models through Motif-Oriented Pre-Training with MoDNA", "link": "https://www.mdpi.com/2673-7426/4/2/85", "details": "W An, Y Guo, Y Bian, H Ma, J Yang, C Li, J Huang - BioMedInformatics, 2024", "abstract": "Acquiring meaningful representations of gene expression is essential for the accurate prediction of downstream regulatory tasks, such as identifying promoters and transcription factor binding sites. However, the current dependency on \u2026"}, {"title": "Querying as Prompt: Parameter-Efficient Learning for Multimodal Language Model", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Querying_as_Prompt_Parameter-Efficient_Learning_for_Multimodal_Language_Model_CVPR_2024_paper.pdf", "details": "T Liang, J Huang, M Kong, L Chen, Q Zhu - Proceedings of the IEEE/CVF Conference \u2026, 2024", "abstract": "Recent advancements in language models pre-trained on large-scale corpora have significantly propelled developments in the NLP domain and advanced progress in multimodal tasks. In this paper we propose a Parameter-Efficient multimodal \u2026"}, {"title": "Zyda: A 1.3 T Dataset for Open Language Modeling", "link": "https://arxiv.org/pdf/2406.01981", "details": "Y Tokpanov, B Millidge, P Glorioso, J Pilault, A Ibrahim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State- of-the-art language models, even at relatively smaller sizes, typically require training \u2026"}]
