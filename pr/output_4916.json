[{"title": "Fairness Definitions in Language Models Explained", "link": "https://arxiv.org/pdf/2407.18454", "details": "TV Doan, Z Chu, Z Wang, W Zhang - arXiv preprint arXiv:2407.18454, 2024", "abstract": "Language Models (LMs) have demonstrated exceptional performance across various Natural Language Processing (NLP) tasks. Despite these advancements, LMs can inherit and amplify societal biases related to sensitive attributes such as \u2026"}, {"title": "Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education", "link": "https://arxiv.org/pdf/2407.17022", "details": "S Kim, S Kim - arXiv preprint arXiv:2407.17022, 2024", "abstract": "Large language model (LLM)-based evaluation pipelines have demonstrated their capability to robustly evaluate machine-generated text. Extending this methodology to assess human-written text could significantly benefit educational settings by \u2026"}, {"title": "Coalitions of Large Language Models Increase the Robustness of AI Agents", "link": "https://arxiv.org/pdf/2408.01380", "details": "P Mangal, C Mak, T Kanakis, T Donovan, D Braines\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence of Large Language Models (LLMs) have fundamentally altered the way we interact with digital systems and have led to the pursuit of LLM powered AI agents to assist in daily workflows. LLMs, whilst powerful and capable of \u2026"}, {"title": "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models", "link": "https://arxiv.org/pdf/2407.17467", "details": "J Gu, Z Yang, C Ding, R Zhao, F Tan - arXiv preprint arXiv:2407.17467, 2024", "abstract": "Large Language Models (LLMs) excel in diverse tasks but often underperform in specialized fields due to limited domain-specific or proprietary corpus. Continual pre- training (CPT) enhances LLM capabilities by imbuing new domain-specific or \u2026"}, {"title": "Optimizing Tourism Accommodation Offers by Integrating Language Models and Knowledge Graph Technologies", "link": "https://www.mdpi.com/2078-2489/15/7/398", "details": "A Cadeddu, A Chessa, V De Leo, G Fenu, E Motta\u2026 - Information, 2024", "abstract": "Online platforms have become the primary means for travellers to search, compare, and book accommodations for their trips. Consequently, online platforms and revenue managers must acquire a comprehensive comprehension of these \u2026"}, {"title": "Fine-tuning language models for joint rewriting and completion of code with potential bugs", "link": "https://www.amazon.science/publications/fine-tuning-language-models-for-joint-rewriting-and-completion-of-code-with-potential-bugs", "details": "D Wang, J Zhao, H Pei, S Tan, S Zha - 2024", "abstract": "Handling drafty partial code remains a notable challenge in real-time code suggestion applications. Previous work has demonstrated shortcomings of large language models of code (CodeLLMs) in completing partial code with potential bugs \u2026"}, {"title": "Using Large Language Models for the Interpretation of Building Regulations", "link": "https://arxiv.org/pdf/2407.21060", "details": "S Fuchs, M Witbrock, J Dimyadi, R Amor - arXiv preprint arXiv:2407.21060, 2024", "abstract": "Compliance checking is an essential part of a construction project. The recent rapid uptake of building information models (BIM) in the construction industry has created more opportunities for automated compliance checking (ACC). BIM enables sharing \u2026"}, {"title": "Cool-Fusion: Fuse Large Language Models without Training", "link": "https://arxiv.org/pdf/2407.19807", "details": "C Liu, X Quan, Y Pan, L Lin, W Wu, X Chen - arXiv preprint arXiv:2407.19807, 2024", "abstract": "We focus on the problem of fusing two or more heterogeneous large language models (LLMs) to facilitate their complementary strengths. One of the challenges on model fusion is high computational load, ie to fine-tune or to align vocabularies via \u2026"}, {"title": "Mol2Lang-VLM: Vision-and Text-Guided Generative Pre-trained Language Models for Advancing Molecule Captioning through Multimodal Fusion", "link": "https://openreview.net/pdf%3Fid%3Dax8kYHfkNr", "details": "DT Tran, NT Pham, NDH Nguyen, B Manavalan - ACL 2024 Workshop Language+ \u2026", "abstract": "This paper introduces Mol2Lang-VLM, an enhanced method for refining generative pre-trained language models for molecule captioning using multimodal features to achieve more accurate caption generation. Our approach leverages the encoder and \u2026"}]
