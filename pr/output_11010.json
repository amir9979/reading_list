[{"title": "Multimodal Fact-Checking with Vision Language Models: A Probing Classifier based Solution with Embedding Strategies", "link": "https://arxiv.org/pdf/2412.05155%3F", "details": "RF Cekinel, P Karagoz, C Coltekin - arXiv preprint arXiv:2412.05155, 2024", "abstract": "This study evaluates the effectiveness of Vision Language Models (VLMs) in representing and utilizing multimodal content for fact-checking. To be more specific, we investigate whether incorporating multimodal content improves performance \u2026"}, {"title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators", "link": "https://arxiv.org/pdf/2412.02467%3F", "details": "T Afonja, HP Wang, R Kerkouche, M Fritz - arXiv preprint arXiv:2412.02467, 2024", "abstract": "Generating tabular data under differential privacy (DP) protection ensures theoretical privacy guarantees but poses challenges for training machine learning models, primarily due to the need to capture complex structures under noisy supervision \u2026"}, {"title": "Minerva LLMs: The first family of Large Language Models trained from scratch on Italian data", "link": "https://ceur-ws.org/Vol-3878/76_main_long.pdf", "details": "R Orlando, L Moroni, PLH Cabot, E Barba, S Conia\u2026 - Proc. of CLiC-it, 2024", "abstract": "The growing interest in Large Language Models (LLMs) has accelerated research efforts to adapt these models for various languages. Despite this, pretraining LLMs from scratch for non-English languages remains underexplored. This is the case for \u2026"}, {"title": "Enhancing Multi-Step Mathematical Reasoning in Large Language Models with Step-by-Step Similarity Prompts and Answer Voting", "link": "http://poster-openaccess.com/files/ICIC2024/818.pdf", "details": "Q Ye, X Ji, RH Hou, JP Liu, T Ruan", "abstract": "Complex reasoning problems, especially multi-step mathematical reasoning problems, are a difficult class of NLP tasks to solve. Existing methods such as Manual-CoT improve the accuracy of reasoning tasks by manually designing \u2026"}, {"title": "Attention-driven GUI Grounding: Leveraging Pretrained Multimodal Large Language Models without Fine-Tuning", "link": "https://arxiv.org/pdf/2412.10840", "details": "HM Xu, Q Chen, L Wang, L Liu - arXiv preprint arXiv:2412.10840, 2024", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have generated significant interest in their ability to autonomously interact with and interpret Graphical User Interfaces (GUIs). A major challenge in these systems is \u2026"}]
