[{"title": "Evidence-based multimodal fusion on structured EHRs and free-text notes for ICU outcome prediction", "link": "https://arxiv.org/pdf/2501.04389", "details": "Y Ruan, DJ Tan, SK Ng, L Huang, M Feng - arXiv preprint arXiv:2501.04389, 2025", "abstract": "Objective: Accurate Intensive Care Unit (ICU) outcome prediction is critical for improving patient treatment quality and ICU resource allocation. Existing research mainly focuses on structured data and lacks effective frameworks to integrate clinical \u2026"}, {"title": "Can Language Models Rival Mathematics Students? Evaluating Mathematical Reasoning through Textual Manipulation and Human Experiments", "link": "https://arxiv.org/pdf/2412.11908", "details": "A Nikolaiev, Y Stathopoulos, S Teufel - arXiv preprint arXiv:2412.11908, 2024", "abstract": "In this paper we look at the ability of recent large language models (LLMs) at solving mathematical problems in combinatorics. We compare models LLaMA-2, LLaMA-3.1, GPT-4, and Mixtral against each other and against human pupils and \u2026"}, {"title": "CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology", "link": "https://arxiv.org/pdf/2412.12077", "details": "Y Sun, Y Si, C Zhu, X Gong, K Zhang, P Chen, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence of large multimodal models (LMMs) has brought significant advancements to pathology. Previous research has primarily focused on separately training patch-level and whole-slide image (WSI)-level models, limiting the \u2026"}, {"title": "Sepllm: Accelerate large language models by compressing one segment into one separator", "link": "https://arxiv.org/pdf/2412.12094", "details": "G Chen, H Shi, J Li, Y Gao, X Ren, Y Chen, X Jiang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have exhibited exceptional performance across a spectrum of natural language processing tasks. However, their substantial sizes pose considerable challenges, particularly in computational demands and inference \u2026"}, {"title": "Eliciting Causal Abilities in Large Language Models for Reasoning Tasks", "link": "https://arxiv.org/pdf/2412.15314", "details": "Y Wang, Z Luo, J Wang, Z Zhou, Y Chen, B Han - arXiv preprint arXiv:2412.15314, 2024", "abstract": "Prompt optimization automatically refines prompting expressions, unlocking the full potential of LLMs in downstream tasks. However, current prompt optimization methods are costly to train and lack sufficient interpretability. This paper proposes \u2026"}]
