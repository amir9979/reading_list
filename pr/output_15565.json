[{"title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis", "link": "https://arxiv.org/pdf/2504.10352", "details": "Y Yang, S Liu, J Li, Y Hu, H Wu, H Wang, J Yu, L Meng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and \u2026"}, {"title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "link": "https://arxiv.org/pdf/2504.14366", "details": "P Haller, J Golde, A Akbik - arXiv preprint arXiv:2504.14366, 2025", "abstract": "Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures \u2026"}, {"title": "Reinforcement Learning from Human Feedback", "link": "https://arxiv.org/pdf/2504.12501", "details": "N Lambert - arXiv preprint arXiv:2504.12501, 2025", "abstract": "Reinforcement learning from human feedback (RLHF) has become an important technical and storytelling tool to deploy the latest machine learning systems. In this book, we hope to give a gentle introduction to the core methods for people with some \u2026"}, {"title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?", "link": "https://arxiv.org/pdf/2504.10000", "details": "Y Wang, J Guan, J Liang, R He - arXiv preprint arXiv:2504.10000, 2025", "abstract": "Multi-modal large language models (MLLMs) have made significant progress, yet their safety alignment remains limited. Typically, current open-source MLLMs rely on the alignment inherited from their language module to avoid harmful generations \u2026"}, {"title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer", "link": "https://arxiv.org/pdf/2504.10462", "details": "W Lei, J Wang, H Wang, X Li, JH Liew, J Feng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained \u2026"}, {"title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language Models", "link": "https://arxiv.org/pdf/2504.14245", "details": "Y Ji, Y Hong, J Zhan, H Chen, H Zhu, W Wang, L Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Progress in image generation raises significant public security concerns. We argue that fake image detection should not operate as a\" black box\". Instead, an ideal approach must ensure both strong generalization and transparency. Recent \u2026"}, {"title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data", "link": "https://arxiv.org/pdf/2504.09895", "details": "S Zhao, L Zhu, Y Yang - arXiv preprint arXiv:2504.09895, 2025", "abstract": "Large language models~(LLMs) are expected to be helpful, harmless, and honest. In various alignment scenarios, such as general human preference, safety, and confidence alignment, binary preference data collection and reward modeling are \u2026"}, {"title": "SaRO: Enhancing LLM Safety through Reasoning-based Alignment", "link": "https://arxiv.org/pdf/2504.09420", "details": "Y Mou, Y Luo, S Zhang, W Ye - arXiv preprint arXiv:2504.09420, 2025", "abstract": "Current safety alignment techniques for large language models (LLMs) face two key challenges:(1) under-generalization, which leaves models vulnerable to novel jailbreak attacks, and (2) over-alignment, which leads to the excessive refusal of \u2026"}, {"title": "Establishing Reliability Metrics for Reward Models in Large Language Models", "link": "https://arxiv.org/pdf/2504.14838", "details": "Y Chen, Y Liu, X Wang, Q Yu, G Huzhang, A Zeng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The reward model (RM) that represents human preferences plays a crucial role in optimizing the outputs of large language models (LLMs), eg, through reinforcement learning from human feedback (RLHF) or rejection sampling. However, a long \u2026"}]
