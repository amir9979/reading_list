[{"title": "Replicability measures for longitudinal information retrieval evaluation", "link": "https://arxiv.org/pdf/2409.05417", "details": "J Keller, T Breuer, P Schaer - International Conference of the Cross-Language \u2026, 2024", "abstract": "Abstract Information Retrieval (IR) systems are exposed to constant changes in most components. Documents are created, updated, or deleted, the information needs are changing, and even relevance might not be static. While it is generally expected that \u2026"}, {"title": "DetoxBench: Benchmarking Large Language Models for Multitask Fraud & Abuse Detection", "link": "https://arxiv.org/pdf/2409.06072", "details": "J Chakraborty, W Xia, A Majumder, D Ma, W Chaabene\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks. However, their practical application in high-stake domains, such as fraud and abuse detection, remains an area that requires further \u2026"}, {"title": "Towards Evaluating and Building Versatile Large Language Models for Medicine", "link": "https://arxiv.org/pdf/2408.12547", "details": "C Wu, P Qiu, J Liu, H Gu, N Li, Y Zhang, Y Wang, W Xie - arXiv preprint arXiv \u2026, 2024", "abstract": "In this study, we present MedS-Bench, a comprehensive benchmark designed to evaluate the performance of large language models (LLMs) in clinical contexts. Unlike existing benchmarks that focus on multiple-choice question answering, MedS \u2026"}, {"title": "Focused Large Language Models are Stable Many-Shot Learners", "link": "https://arxiv.org/pdf/2408.13987", "details": "P Yuan, S Feng, Y Li, X Wang, Y Zhang, C Tan, B Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL \u2026"}, {"title": "The representation landscape of few-shot learning and fine-tuning in large language models", "link": "https://arxiv.org/pdf/2409.03662", "details": "D Doimo, A Serra, A Ansuini, A Cazzaniga - arXiv preprint arXiv:2409.03662, 2024", "abstract": "In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to \u2026"}, {"title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding", "link": "https://arxiv.org/pdf/2408.15966", "details": "Y Tang, X Han, X Li, Q Yu, J Xu, Y Hao, L Hu, M Chen - arXiv preprint arXiv \u2026, 2024", "abstract": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world remains a significant challenge. Due to the lack of large-scale 3D-text pair datasets, the success of LLMs has yet to be replicated in 3D understanding. In this paper, we \u2026"}, {"title": "Medical Report Generation Is A Multi-label Classification Problem", "link": "https://arxiv.org/pdf/2409.00250", "details": "Y Fan, Z Yang, R Liu, M Li, X Chang - arXiv preprint arXiv:2409.00250, 2024", "abstract": "Medical report generation is a critical task in healthcare that involves the automatic creation of detailed and accurate descriptions from medical images. Traditionally, this task has been approached as a sequence generation problem, relying on vision \u2026"}]
