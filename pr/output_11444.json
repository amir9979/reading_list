[{"title": "STNMamba: Mamba-based Spatial-Temporal Normality Learning for Video Anomaly Detection", "link": "https://arxiv.org/pdf/2412.20084", "details": "Z Li, M Zhao, X Yang, Y Liu, J Sheng, X Zeng, T Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Video anomaly detection (VAD) has been extensively researched due to its potential for intelligent video systems. However, most existing methods based on CNNs and transformers still suffer from substantial computational burdens and have room for \u2026"}, {"title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining", "link": "https://arxiv.org/pdf/2501.00958", "details": "W Zhang, H Zhang, X Li, J Sun, Y Shen, W Lu, D Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge \u2026"}, {"title": "3VL: Using Trees to Improve Vision-Language Models' Interpretability", "link": "https://ieeexplore.ieee.org/abstract/document/10829542/", "details": "N Yellinek, L Karlinsky, R Giryes - IEEE Transactions on Image Processing, 2025", "abstract": "Vision-Language models (VLMs) have proven to be effective at aligning image and text representations, producing superior zero-shot results when transferred to many downstream tasks. However, these representations suffer from some key \u2026"}, {"title": "LoRACLR: Contrastive Adaptation for Customization of Diffusion Models", "link": "https://arxiv.org/pdf/2412.09622", "details": "E Simsar, T Hofmann, F Tombari, P Yanardag - arXiv preprint arXiv:2412.09622, 2024", "abstract": "Recent advances in text-to-image customization have enabled high-fidelity, context- rich generation of personalized images, allowing specific concepts to appear in a variety of scenarios. However, current methods struggle with combining multiple \u2026"}, {"title": "DiC: Rethinking Conv3x3 Designs in Diffusion Models", "link": "https://arxiv.org/pdf/2501.00603", "details": "Y Tian, J Han, C Wang, Y Liang, C Xu, H Chen - arXiv preprint arXiv:2501.00603, 2024", "abstract": "Diffusion models have shown exceptional performance in visual generation tasks. Recently, these models have shifted from traditional U-Shaped CNN-Attention hybrid structures to fully transformer-based isotropic architectures. While these transformers \u2026"}, {"title": "Enhancing Fine-Tuning Performance of Text-to-Image Diffusion Models for Few-Shot Image Generation Through", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3Dsuw5EQAAQBAJ%26oi%3Dfnd%26pg%3DPA133%26ots%3DNITWR-L7AX%26sig%3Dx7vKqM9t465noyOHh8nu6kLzBS8", "details": "YL Zhu, P Yang - Image and Graphics Technologies and Applications \u2026", "abstract": "Recent significant progress in the field of few-shot image generation has been achieved by fine-tuning pretrained text-to-image mod-els, notably methods such as Dreambooth and Textual Inversion. To enhance the performance of existing \u2026"}]
