[{"title": "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments", "link": "https://arxiv.org/pdf/2410.06420", "details": "S Ray, K Gupta, S Kundu, PA Kasat, S Aditya, P Goyal - arXiv preprint arXiv \u2026, 2024", "abstract": "The global shortage of healthcare workers has demanded the development of smart healthcare assistants, which can help monitor and alert healthcare workers when necessary. We examine the healthcare knowledge of existing Large Vision \u2026"}, {"title": "Exploring Pretraining via Active Forgetting for Improving Cross Lingual Transfer for Decoder Language Models", "link": "https://arxiv.org/pdf/2410.16168%3F", "details": "D Aggarwal, A Sathe, S Sitaram - arXiv preprint arXiv:2410.16168, 2024", "abstract": "Large Language Models (LLMs) demonstrate exceptional capabilities in a multitude of NLP tasks. However, the efficacy of such models to languages other than English is often limited. Prior works have shown that encoder-only models such as BERT or \u2026"}, {"title": "Reconstructing and analyzing the invariances of low\u2010dose CT image denoising networks", "link": "https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1002/mp.17413", "details": "E Eulig, F J\u00e4ger, J Maier, B Ommer, M Kachelrie\u00df - Medical Physics, 2024", "abstract": "Background Deep learning\u2010based methods led to significant advancements in many areas of medical imaging, most of which are concerned with the reduction of artifacts caused by motion, scatter, or noise. However, with most neural networks being black \u2026"}, {"title": "Demystifying Large Language Models for Medicine: A Primer", "link": "https://arxiv.org/pdf/2410.18856", "details": "Q Jin, N Wan, R Leaman, S Tian, Z Wang, Y Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) represent a transformative class of AI tools capable of revolutionizing various aspects of healthcare by generating human-like responses across diverse contexts and adapting to novel tasks following human instructions \u2026"}, {"title": "Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks", "link": "https://arxiv.org/pdf/2410.18387", "details": "L Wang, H Wang, H Yang, J Mao, Z Yang, J Shen, X Li - arXiv preprint arXiv \u2026, 2024", "abstract": "Several medical Multimodal Large Languange Models (MLLMs) have been developed to address tasks involving visual images with textual instructions across various medical modalities, achieving impressive results. Most current medical \u2026"}, {"title": "Aligning Large Language Models for Enhancing Psychiatric Interviews Through Symptom Delineation and Summarization: Pilot Study", "link": "https://formative.jmir.org/2024/1/e58418/", "details": "J So, J Chang, E Kim, J Na, JY Choi, J Sohn, BH Kim\u2026 - JMIR Formative Research, 2024", "abstract": "Background: Recent advancements in large language models (LLMs) have accelerated their use across various domains. Psychiatric interviews, which are goal- oriented and structured, represent a significantly underexplored area where LLMs \u2026"}, {"title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "link": "https://arxiv.org/pdf/2410.11829%3F", "details": "Y Cao, Y Liu, Z Chen, G Shi, W Wang, D Zhao, T Lu - arXiv preprint arXiv:2410.11829, 2024", "abstract": "Despite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating \u2026"}, {"title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models", "link": "https://arxiv.org/pdf/2410.06154", "details": "MJ Mirza, M Zhao, Z Mao, S Doveh, W Lin, P Gavrikov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task \u2026"}, {"title": "LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies", "link": "https://arxiv.org/pdf/2410.04749", "details": "A Hamza, YH Ahn, S Lee, ST Kim - arXiv preprint arXiv:2410.04749, 2024", "abstract": "Generating Natural Language Explanations (NLEs) for model predictions on medical images, particularly those depicting thoracic pathologies, remains a critical and challenging task. Existing methodologies often struggle due to general models' \u2026"}]
