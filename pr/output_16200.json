[{"title": "Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization", "link": "https://arxiv.org/pdf/2504.12083", "details": "P Sarkar, A Etemad - arXiv preprint arXiv:2504.12083, 2025", "abstract": "Despite recent advances in Large Video Language Models (LVLMs), they still struggle with fine-grained temporal understanding, hallucinate, and often make simple mistakes on even simple video question-answering tasks, all of which pose \u2026"}, {"title": "PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models", "link": "https://arxiv.org/pdf/2504.08966%3F", "details": "M Dhouib, D Buscaldi, S Vanier, A Shabou - arXiv preprint arXiv:2504.08966, 2025", "abstract": "Visual Language Models require substantial computational resources for inference due to the additional input tokens needed to represent visual information. However, these visual tokens often contain redundant and unimportant information, resulting in \u2026"}, {"title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models", "link": "https://arxiv.org/pdf/2504.12526%3F", "details": "J Zhang, T Zhu, C Luo, A Anandkumar - arXiv preprint arXiv:2504.12526, 2025", "abstract": "Long-context language models exhibit impressive performance but remain challenging to deploy due to high GPU memory demands during inference. We propose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that \u2026"}, {"title": "Empowering Agentic Video Analytics Systems with Video Language Models", "link": "https://arxiv.org/pdf/2505.00254", "details": "Y Yan, S Jiang, T Cao, Y Yang, Q Yang, Y Shu, Y Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "AI-driven video analytics has become increasingly pivotal across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of \u2026"}, {"title": "NNTile: a machine learning framework capable of training extremely large GPT language models on a single node", "link": "https://arxiv.org/pdf/2504.13236", "details": "A Mikhalev, A Katrutsa, K Sozykin, I Oseledets - arXiv preprint arXiv:2504.13236, 2025", "abstract": "This study presents an NNTile framework for training large deep neural networks in heterogeneous clusters. The NNTile is based on a StarPU library, which implements task-based parallelism and schedules all provided tasks onto all available \u2026"}, {"title": "Detoxifying language model outputs: combining multi-agent debates and reinforcement learning for improved summarization", "link": "https://www.researchgate.net/profile/Bharathi-Mohan-Gurusamy/publication/391274884_Detoxifying_language_model_outputs_combining_multi-agent_debates_and_reinforcement_learning_for_improved_summarization/links/6810f68860241d51401fd6e2/Detoxifying-language-model-outputs-combining-multi-agent-debates-and-reinforcement-learning-for-improved-summarization.pdf", "details": "GB Mohan, M Gayathri, RP Kumar - Language Resources and Evaluation, 2025", "abstract": "The increasing prevalence of online user generated content has raised serious concerns about toxic language, which reinforces societal biases and causes psychological harm. This study introduces a novel approach that combines multi \u2026"}, {"title": "Enhancing Multi-task Learning Capability of Medical Generalist Foundation Model via Image-centric Multi-annotation Data", "link": "https://arxiv.org/pdf/2504.09967", "details": "X Zhu, F Mo, Z Zhang, J Wang, Y Shi, M Wu, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The emergence of medical generalist foundation models has revolutionized conventional task-specific model development paradigms, aiming to better handle multiple tasks through joint training on large-scale medical datasets. However, recent \u2026"}, {"title": "The Framework and Implementation of Using Large Language Models to Answer Questions about Building Codes and Standards", "link": "https://ascelibrary.org/doi/abs/10.1061/JCCEE5.CPENG-6037", "details": "I Joffe, G Felobes, Y Elgouhari, M Talebi Kalaleh, Q Mei\u2026 - Journal of Computing in \u2026, 2025", "abstract": "Civil and structural engineering design projects are subject to strict regulations of relevant codes and standards to guarantee that certain standards of safety, reliability, and efficiency are met. However, ensuring that all engineering designs comply with \u2026"}, {"title": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items", "link": "https://arxiv.org/pdf/2504.11186", "details": "M Zou, S Srinivasan, TWS Lo, K Zou, GD Yang, X Ai\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in reasoning-focused large language models (LLMs) mark a shift from general LLMs toward models designed for complex decision-making, a crucial aspect in medicine. However, their performance in specialized domains like \u2026"}]
