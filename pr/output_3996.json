[{"title": "SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers", "link": "https://arxiv.org/pdf/2407.09413", "details": "S Pramanick, R Chellappa, S Venugopalan - arXiv preprint arXiv:2407.09413, 2024", "abstract": "Seeking answers to questions within long scientific research articles is a crucial area of study that aids readers in quickly addressing their inquiries. However, existing question-answering (QA) datasets based on scientific papers are limited in scale and \u2026"}, {"title": "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models", "link": "https://arxiv.org/pdf/2407.05131", "details": "P Xia, K Zhu, H Li, H Zhu, Y Li, G Li, L Zhang, H Yao - arXiv preprint arXiv:2407.05131, 2024", "abstract": "The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical \u2026"}, {"title": "DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning", "link": "https://arxiv.org/pdf/2406.17659", "details": "X Zhang, Z Altaweel, Y Hayamizu, Y Ding, S Amiri\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language models (VLMs) have been applied to robot task planning problems, where the robot receives a task in natural language and generates plans based on visual inputs. While current VLMs have demonstrated strong vision-language \u2026"}, {"title": "MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models", "link": "https://arxiv.org/pdf/2407.02775", "details": "Y Zhang, Z Yang, S Ji - arXiv preprint arXiv:2407.02775, 2024", "abstract": "Knowledge distillation is an effective technique for pre-trained language model compression. Although existing knowledge distillation methods perform well for the most typical model BERT, they could be further improved in two aspects: the relation \u2026"}, {"title": "Information Guided Regularization for Fine-tuning Language Models", "link": "https://arxiv.org/pdf/2406.14005", "details": "M Sharma, N Muralidhar, S Xu, RB Yosuf\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The pretraining-fine-tuning paradigm has been the de facto strategy for transfer learning in modern language modeling. With the understanding that task adaptation in LMs is often a function of parameters shared across tasks, we argue that a more \u2026"}, {"title": "Large Language Model as a Universal Clinical Multi-task Decoder", "link": "https://arxiv.org/pdf/2406.12738", "details": "Y Wu, H Song, J Zhang, X Wen, S Zheng, J Bian - arXiv preprint arXiv:2406.12738, 2024", "abstract": "The development of effective machine learning methodologies for enhancing the efficiency and accuracy of clinical systems is crucial. Despite significant research efforts, managing a plethora of diversified clinical tasks and adapting to emerging \u2026"}, {"title": "DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs", "link": "https://arxiv.org/pdf/2406.17158", "details": "VVD Prabhu, A Anand - arXiv preprint arXiv:2406.17158, 2024", "abstract": "Open-domain complex Question Answering (QA) is a difficult task with challenges in evidence retrieval and reasoning. The complexity of such questions could stem from questions being compositional, hybrid evidence, or ambiguity in questions. While \u2026"}, {"title": "A Knowledge Graph Embedding Model for Answering Factoid Entity Questions", "link": "https://dl.acm.org/doi/pdf/10.1145/3678003", "details": "P Jafarzadeh, F Ensan, M Ali Akbar Alavi\u2026 - ACM Transactions on \u2026, 2024", "abstract": "Factoid entity questions (FEQ), which seek answers in the form of a single entity from knowledge sources such as DBpedia and Wikidata, constitute a substantial portion of user queries in search engines. This paper introduces the Knowledge Graph \u2026"}, {"title": "Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation", "link": "https://arxiv.org/pdf/2407.03056", "details": "M Mistretta, A Baldrati, M Bertini, AD Bagdanov - arXiv preprint arXiv:2407.03056, 2024", "abstract": "Vision-Language Models (VLMs) demonstrate remarkable zero-shot generalization to unseen tasks, but fall short of the performance of supervised methods in generalizing to downstream tasks with limited data. Prompt learning is emerging as a \u2026"}]
