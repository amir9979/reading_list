[{"title": "SAIF: A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models", "link": "https://arxiv.org/pdf/2502.11356", "details": "Z He, H Zhao, Y Qiao, F Yang, A Payani, J Ma, M Du - arXiv preprint arXiv \u2026, 2025", "abstract": "The ability of large language models (LLMs) to follow instructions is crucial for their practical applications, yet the underlying mechanisms remain poorly understood. This paper presents a novel framework that leverages sparse autoencoders (SAE) to \u2026"}, {"title": "Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?", "link": "https://arxiv.org/pdf/2502.11895", "details": "J Nielsen, P Schneider-Kamp, L Galke - arXiv preprint arXiv:2502.11895, 2025", "abstract": "Large language models (LLMs) require immense resources for training and inference. Quantization, a technique that reduces the precision of model parameters, offers a promising solution for improving LLM efficiency and sustainability. While post \u2026"}, {"title": "KG-prompt: Interpretable knowledge graph prompt for pre-trained language models", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125001650", "details": "L Chen, J Liu, Y Duan, R Wang - Knowledge-Based Systems, 2025", "abstract": "Abstract Knowledge graphs (KGs) can provide rich factual knowledge for language models, enhancing reasoning ability and interpretability. However, existing knowledge injection methods usually ignore the structured information in KGs. Using \u2026"}, {"title": "How Linguistics Learned to Stop Worrying and Love the Language Models", "link": "https://arxiv.org/pdf/2501.17047%3F", "details": "R Futrell, K Mahowald - arXiv preprint arXiv:2501.17047, 2025", "abstract": "Language models can produce fluent, grammatical text. Nonetheless, some maintain that language models don't really learn language and also that, even if they did, that would not be informative for the study of human learning and processing. On the \u2026"}, {"title": "QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation", "link": "https://arxiv.org/pdf/2502.05178", "details": "Y Zhao, F Xue, S Reed, L Fan, Y Zhu, J Kautz, Z Yu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero- shot image understanding. QLIP trains a binary-spherical-quantization-based \u2026"}, {"title": "PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection", "link": "https://arxiv.org/pdf/2502.12119", "details": "J Bi, Y Wang, D Yan, X Xiao, A Hecker, V Tresp, Y Ma - arXiv preprint arXiv \u2026, 2025", "abstract": "Visual instruction tuning refines pre-trained Multimodal Large Language Models (MLLMs) to enhance their real-world task performance. However, the rapid expansion of visual instruction datasets introduces significant data redundancy \u2026"}, {"title": "Distraction is All You Need for Multimodal Large Language Model Jailbreaking", "link": "https://arxiv.org/pdf/2502.10794", "details": "Z Yang, J Fan, A Yan, E Gao, X Lin, T Li, C Dong - arXiv preprint arXiv:2502.10794, 2025", "abstract": "Multimodal Large Language Models (MLLMs) bridge the gap between visual and textual data, enabling a range of advanced applications. However, complex internal interactions among visual elements and their alignment with text can introduce \u2026"}, {"title": "Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs with Minimal Human Interventions", "link": "https://arxiv.org/pdf/2502.08657", "details": "J Xu, G Nan, S Guan, S Leng, Y Liu, Z Wang, Y Ma\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent AI agents, such as ChatGPT and LLaMA, primarily rely on instruction tuning and reinforcement learning to calibrate the output of large language models (LLMs) with human intentions, ensuring the outputs are harmless and helpful. Existing \u2026"}, {"title": "Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation", "link": "https://arxiv.org/pdf/2502.11306", "details": "H Nguyen, Z He, SA Gandre, U Pasupulety\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) often suffer from hallucination, generating factually incorrect or ungrounded content, which limits their reliability in high-stakes applications. A key factor contributing to hallucination is the use of hard labels during \u2026"}]
