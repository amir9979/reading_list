[{"title": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records", "link": "https://aclanthology.org/2024.cl4health-1.23.pdf", "details": "J Lov\u00f3n-Melgarejo, T Ben-Haddi, J Di Scala\u2026 - Proceedings of the First \u2026, 2024", "abstract": "The lack of standardized evaluation benchmarks in the medical domain for text inputs can be a barrier to widely adopting and leveraging the potential of natural language models for health-related downstream tasks. This paper revisited an \u2026"}, {"title": "A Survey on Vision-Language-Action Models for Embodied AI", "link": "https://arxiv.org/pdf/2405.14093", "details": "Y Ma, Z Song, Y Zhuang, J Hao, I King - arXiv preprint arXiv:2405.14093, 2024", "abstract": "Deep learning has demonstrated remarkable success across many domains, including computer vision, natural language processing, and reinforcement learning. Representative artificial neural networks in these fields span convolutional neural \u2026"}, {"title": "Unraveling Clinical Insights: A Lightweight and Interpretable Approach for Multimodal and Multilingual Knowledge Integration", "link": "https://aclanthology.org/2024.cl4health-1.24.pdf", "details": "K Uma, MF Moens - Proceedings of the First Workshop on Patient-Oriented \u2026, 2024", "abstract": "In recent years, the analysis of clinical texts has evolved significantly, driven by the emergence of language models like BERT such as PubMedBERT, and ClinicalBERT, which have been tailored for the (bio) medical domain that rely on \u2026"}, {"title": "Multi-modality Regional Alignment Network for Covid X-Ray Survival Prediction and Report Generation", "link": "https://arxiv.org/pdf/2405.14113", "details": "Z Zhong, J Li, J Sollee, S Collins, H Bai, P Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In response to the worldwide COVID-19 pandemic, advanced automated technologies have emerged as valuable tools to aid healthcare professionals in managing an increased workload by improving radiology report generation and \u2026"}, {"title": "Multimodal parallel attention network for medical image segmentation", "link": "https://www.sciencedirect.com/science/article/pii/S0262885624001732", "details": "Z Wang, W Wang, N Li, S Zhang, Q Chen, Z Jiang - Image and Vision Computing, 2024", "abstract": "Medical image segmentation is a crucial aspect of medical image processing, and has been widely used in the detection and clinical diagnosis for brain, lung, liver, heart and other diseases. In this paper, we propose a novel multimodal parallel \u2026"}, {"title": "Text-Free Multi-domain Graph Pre-training: Toward Graph Foundation Models", "link": "https://arxiv.org/pdf/2405.13934", "details": "X Yu, C Zhou, Y Fang, X Zhang - arXiv preprint arXiv:2405.13934, 2024", "abstract": "Given the ubiquity of graph data, it is intriguing to ask: Is it possible to train a graph foundation model on a broad range of graph data across diverse domains? A major hurdle toward this goal lies in the fact that graphs from different domains often exhibit \u2026"}, {"title": "Dynamic Knowledge Prompt for Chest X-ray Report Generation", "link": "https://aclanthology.org/2024.lrec-main.482.pdf", "details": "S Bu, Y Song, T Li, Z Dai - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Automatic generation of radiology reports can relieve the burden of radiologist. In the radiology library, the biased dataset and the sparse features of chest X-ray image make it difficult to generate reports. Many approaches strive to integrate prior \u2026"}, {"title": "Large Language Models Can Self-Correct with Minimal Effort", "link": "https://arxiv.org/pdf/2405.14092", "details": "Z Wu, Q Zeng, Z Zhang, Z Tan, C Shen, M Jiang - arXiv preprint arXiv:2405.14092, 2024", "abstract": "Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple \u2026"}, {"title": "Pre-training Concept Frequency is predictive of CLIP Zero-shot Performance", "link": "https://openreview.net/pdf%3Fid%3D55iCzZ1TtD", "details": "V Udandarao, A Prabhu, P Torr, A Bibi, S Albanie\u2026 - ICLR 2024 Workshop on \u2026", "abstract": "Web-crawled pre-training datasets are speculated to be key drivers of zero-shot generalization abilities of Vision-Language Models (VLMs) like CLIP, across a range of downstream classification and retrieval tasks, spanning diverse visual concepts \u2026"}]
