[{"title": "Self-Comparison for Dataset-Level Membership Inference in Large (Vision-) Language Models", "link": "https://arxiv.org/pdf/2410.13088", "details": "J Ren, K Chen, C Chen, V Sehwag, Y Xing, J Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have made significant advancements in a wide range of natural language processing and vision- language tasks. Access to large web-scale datasets has been a key factor in their \u2026"}, {"title": "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "link": "https://arxiv.org/pdf/2411.04118", "details": "DP Jeong, S Garg, ZC Lipton, M Oberst - arXiv preprint arXiv:2411.04118, 2024", "abstract": "Several recent works seek to develop foundation models specifically for medical applications, adapting general-purpose large language models (LLMs) and vision- language models (VLMs) via continued pretraining on publicly available biomedical \u2026"}, {"title": "Tuning Language Models by Mixture-of-Depths Ensemble", "link": "https://arxiv.org/pdf/2410.13077", "details": "H Luo, L Specia - arXiv preprint arXiv:2410.13077, 2024", "abstract": "Transformer-based Large Language Models (LLMs) traditionally rely on final-layer loss for training and final-layer representations for predictions, potentially overlooking the predictive power embedded in intermediate layers. Surprisingly, we \u2026"}, {"title": "Exploring the Benefits of Domain-Pretraining of Generative Large Language Models for Chemistry", "link": "https://arxiv.org/pdf/2411.03542", "details": "A Acharya, S Sharma, R Cosbey, M Subramanian\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "A proliferation of Large Language Models (the GPT series, BLOOM, LLaMA, and more) are driving forward novel development of multipurpose AI for a variety of tasks, particularly natural language processing (NLP) tasks. These models demonstrate \u2026"}, {"title": "Sleep apnea test prediction based on Electronic Health Records", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001552", "details": "LA Tahoun, AS Green, T Patalon, Y Dagan\u2026 - Journal of Biomedical \u2026, 2024", "abstract": "Abstract The identification of Obstructive Sleep Apnea (OSA) is done by a Polysomnography test which is often done in later ages. Being able to notify potential insured members at earlier ages is desirable. For that, we develop predictive models \u2026"}, {"title": "WizardArena: Post-training Large Language Models via Simulated Offline Chatbot Arena", "link": "https://openreview.net/pdf%3Fid%3DVHva3d836i", "details": "H Luo, Q Sun, C Xu, P Zhao, Q Lin, JG Lou, S Chen\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Recent work demonstrates that, post-training large language models with open- domain instruction following data have achieved colossal success. Simultaneously, human Chatbot Arena has emerged as one of the most reasonable benchmarks for \u2026"}, {"title": "Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.23114", "details": "J Wu, TT Chung, K Chen, DY Yeung - arXiv preprint arXiv:2410.23114, 2024", "abstract": "Despite the outstanding performance in vision-language reasoning, Large Vision- Language Models (LVLMs) might generate hallucinated contents that do not exist in the given image. Most existing LVLM hallucination benchmarks are constrained to \u2026"}, {"title": "Smoothie: Label Free Language Model Routing", "link": "https://openreview.net/pdf%3Fid%3DpPSWHsgqRp", "details": "N Guha, MF Chen, T Chow, IS Khare, C Re - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Large language models (LLMs) are increasingly used in applications where LLM inputs may span many different tasks. Recent work has found that the choice of LLM is consequential, and different LLMs may be good for different input samples. Prior \u2026"}, {"title": "Code-switching finetuning: Bridging multilingual pretrained language models for enhanced cross-lingual performance", "link": "https://www.sciencedirect.com/science/article/pii/S0952197624016907", "details": "C Zan, L Ding, L Shen, Y Cao, W Liu - Engineering Applications of Artificial \u2026, 2025", "abstract": "In recent years, the development of pre-trained models has significantly propelled advancements in natural language processing. However, multilingual sequence-to- sequence pretrained language models (Seq2Seq PLMs) are pretrained on a wide \u2026"}]
