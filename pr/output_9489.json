[{"title": "Adapting to Shifting Correlations with Unlabeled Data Calibration", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-73021-4_14.pdf", "details": "MR Sabuncu", "abstract": "Distribution shifts between sites can seriously degrade model performance since models are prone to exploiting unstable correlations. Thus, many methods try to find features that are stable across sites and discard unstable features. However \u2026"}, {"title": "Why These Documents? Explainable Generative Retrieval with Hierarchical Category Paths", "link": "https://arxiv.org/pdf/2411.05572", "details": "S Lee, R Heo, SK Kang, S Yoon, J Yeo, D Lee - arXiv preprint arXiv:2411.05572, 2024", "abstract": "Generative retrieval has recently emerged as a new alternative of traditional information retrieval approaches. However, existing generative retrieval methods directly decode docid when a query is given, making it impossible to provide users \u2026"}, {"title": "Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training", "link": "https://arxiv.org/pdf/2411.15207", "details": "A Bawazir, K Wu, W Li - arXiv preprint arXiv:2411.15207, 2024", "abstract": "Recent advancements in vision-language pre-training via contrastive learning have significantly improved performance across computer vision tasks. However, in the medical domain, obtaining multimodal data is often costly and challenging due to \u2026"}, {"title": "Generating Realistic Tabular Data with Large Language Models", "link": "https://arxiv.org/pdf/2410.21717", "details": "D Nguyen, S Gupta, K Do, T Nguyen, S Venkatesh - arXiv preprint arXiv:2410.21717, 2024", "abstract": "While most generative models show achievements in image data generation, few are developed for tabular data generation. Recently, due to success of large language models (LLM) in diverse tasks, they have also been used for tabular data generation \u2026"}, {"title": "Distributed, communication-efficient, and differentially private estimation of KL divergence", "link": "https://arxiv.org/pdf/2411.16478", "details": "M Scott, S Biswas, G Cormode, C Maple - arXiv preprint arXiv:2411.16478, 2024", "abstract": "A key task in managing distributed, sensitive data is to measure the extent to which a distribution changes. Understanding this drift can effectively support a variety of federated learning and analytics tasks. However, in many practical settings sharing \u2026"}, {"title": "Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models", "link": "https://arxiv.org/pdf/2411.00492%3F", "details": "DX Long, DN Yen, AT Luu, K Kawaguchi, MY Kan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple \u2026"}, {"title": "Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench", "link": "https://arxiv.org/pdf/2410.22108", "details": "Z Liu, G Dou, M Jia, Z Tan, Q Zeng, Y Yuan, M Jiang - arXiv preprint arXiv:2410.22108, 2024", "abstract": "Generative models such as Large Language Models (LLM) and Multimodal Large Language models (MLLMs) trained on massive web corpora can memorize and disclose individuals' confidential and private data, raising legal and ethical concerns \u2026"}, {"title": "Split and Merge: Aligning Position Biases in LLM-based Evaluators", "link": "https://aclanthology.org/2024.emnlp-main.621.pdf", "details": "Z Li, C Wang, P Ma, D Wu, S Wang, C Gao, Y Liu - Proceedings of the 2024 \u2026, 2024", "abstract": "Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate \u2026"}]
