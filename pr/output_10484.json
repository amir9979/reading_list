[{"title": "Efficient Fine-Tuning of Single-Cell Foundation Models Enables Zero-Shot Molecular Perturbation Prediction", "link": "https://arxiv.org/pdf/2412.13478", "details": "S Maleki, JC Huetter, KV Chuang, G Scalia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Predicting transcriptional responses to novel drugs provides a unique opportunity to accelerate biomedical research and advance drug discovery efforts. However, the inherent complexity and high dimensionality of cellular responses, combined with the \u2026"}, {"title": "Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with Neural Processes", "link": "https://arxiv.org/pdf/2412.13998", "details": "K Kobalczyk, C Fanconi, H Sun, M van der Schaar - arXiv preprint arXiv:2412.13998, 2024", "abstract": "As large language models (LLMs) become increasingly embedded in everyday applications, ensuring their alignment with the diverse preferences of individual users has become a critical challenge. Currently deployed approaches typically \u2026"}, {"title": "Curriculum Learning for Cross-Lingual Data-to-Text Generation With Noisy Data", "link": "https://arxiv.org/pdf/2412.13484", "details": "KA Hari, M Gupta, V Varma - arXiv preprint arXiv:2412.13484, 2024", "abstract": "Curriculum learning has been used to improve the quality of text generation systems by ordering the training samples according to a particular schedule in various tasks. In the context of data-to-text generation (DTG), previous studies used various \u2026"}, {"title": "Hybrid-LLM-GNN: Integrating Large Language Models and Graph Neural Networks for Enhanced Materials Property Prediction", "link": "https://pubs.rsc.org/en/content/articlepdf/2024/dd/d4dd00199k", "details": "Y Li, V Gupta, MNT Kilic, K Choudhary, D Wines\u2026 - Digital Discovery, 2024", "abstract": "Graph-centric learning has attracted significant interest in materials informatics. Accordingly, a family of graph-based machine learning models, primarily utilizing Graph Neural Networks (GNN), has been developed to provide accurate prediction \u2026"}, {"title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2411.14432", "details": "Y Dong, Z Liu, HL Sun, J Yang, W Hu, Y Rao, Z Liu - arXiv preprint arXiv:2411.14432, 2024", "abstract": "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high \u2026"}, {"title": "QAPyramid: Fine-grained Evaluation of Content Selection for Text Summarization", "link": "https://arxiv.org/pdf/2412.07096", "details": "S Zhang, D Wan, A Cattan, A Klein, I Dagan, M Bansal - arXiv preprint arXiv \u2026, 2024", "abstract": "How to properly conduct human evaluations for text summarization is a longstanding challenge. The Pyramid human evaluation protocol, which assesses content selection by breaking the reference summary into sub-units and verifying their \u2026"}, {"title": "Physics Reasoner: Knowledge-Augmented Reasoning for Solving Physics Problems with Large Language Models", "link": "https://arxiv.org/pdf/2412.13791", "details": "X Pang, R Hong, Z Zhou, F Lv, X Yang, Z Liang, B Han\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Physics problems constitute a significant aspect of reasoning, necessitating complicated reasoning ability and abundant physics knowledge. However, existing large language models (LLMs) frequently fail due to a lack of knowledge or incorrect \u2026"}, {"title": "Hint Marginalization for Improved Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2412.13292", "details": "S Pal, D Ch\u00e9telat, Y Zhang, M Coates - arXiv preprint arXiv:2412.13292, 2024", "abstract": "Large Language Models (LLMs) have exhibited an impressive capability to perform reasoning tasks, especially if they are encouraged to generate a sequence of intermediate steps. Reasoning performance can be improved by suitably combining \u2026"}, {"title": "MC-NEST--Enhancing Mathematical Reasoning in Large Language Models with a Monte Carlo Nash Equilibrium Self-Refine Tree", "link": "https://arxiv.org/pdf/2411.15645", "details": "G Rabby, F Keya, P Zamil, S Auer - arXiv preprint arXiv:2411.15645, 2024", "abstract": "Mathematical reasoning has proven to be a critical yet challenging task for large language models (LLMs), as they often struggle with complex multi-step problems. To address these limitations, we introduce the Monte Carlo Nash Equilibrium Self \u2026"}]
