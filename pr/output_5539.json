[{"title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge", "link": "https://arxiv.org/pdf/2407.19594", "details": "T Wu, W Yuan, O Golovneva, J Xu, Y Tian, J Jiao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can \u2026"}, {"title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk of Language Models", "link": "https://arxiv.org/pdf/2408.08926", "details": "AK Zhang, N Perry, R Dulepet, E Jones, JW Lin, J Ji\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have the potential to cause real- world impact. Policymakers, model providers, and other researchers in the AI and \u2026"}, {"title": "QPO: Query-dependent Prompt Optimization via Multi-Loop Offline Reinforcement Learning", "link": "https://arxiv.org/pdf/2408.10504", "details": "Y Kong, H Mao, Q Zhao, B Zhang, J Ruan, L Shen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Prompt engineering has demonstrated remarkable success in enhancing the performance of large language models (LLMs) across diverse tasks. However, most existing prompt optimization methods only focus on the task-level performance \u2026"}, {"title": "Explainable spatio-temporal graph evolution learning with applications to dynamic brain network analysis during development", "link": "https://www.sciencedirect.com/science/article/pii/S1053811924002684", "details": "L Chen, C Qiao, K Ren, G Qu, VD Calhoun\u2026 - NeuroImage, 2024", "abstract": "Modeling dynamic interactions among network components is crucial to uncovering the evolution mechanisms of complex networks. Recently, spatio-temporal graph learning methods have achieved noteworthy results in characterizing the dynamic \u2026"}, {"title": "Athena: Safe Autonomous Agents with Verbal Contrastive Learning", "link": "https://arxiv.org/pdf/2408.11021", "details": "T Sadhu, A Pesaranghader, Y Chen, DH Yi - arXiv preprint arXiv:2408.11021, 2024", "abstract": "Due to emergent capabilities, large language models (LLMs) have been utilized as language-based agents to perform a variety of tasks and make decisions with an increasing degree of autonomy. These autonomous agents can understand high \u2026"}, {"title": "Edge contrastive learning for link prediction", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324002061", "details": "L Liu, Q Xie, W Wen, J Zhu, M Peng - Information Processing & Management, 2024", "abstract": "Link prediction is a critical task within the realm of graph machine learning. While recent advancements mainly emphasize node representation learning, the rich information encapsulated within edges, proven advantageous in various graph \u2026"}, {"title": "Evaluating the necessity of the multiple metrics for assessing explainable AI: A critical examination", "link": "https://www.sciencedirect.com/science/article/pii/S0925231224010531", "details": "M Pawlicki, A Pawlicka, F Uccello, S Szelest\u2026 - Neurocomputing, 2024", "abstract": "This paper investigates the specific properties of Explainable Artificial Intelligence (xAI), particularly when implemented in AI/ML models across high-stakes sectors, in this case cybersecurity. The authors execute a comprehensive systematic review of \u2026"}, {"title": "SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models", "link": "https://arxiv.org/pdf/2408.10174", "details": "A Tang, L Shen, Y Luo, S Xie, H Hu, L Zhang, B Du\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Deep model training on extensive datasets is increasingly becoming cost-prohibitive, prompting the widespread adoption of deep model fusion techniques to leverage knowledge from pre-existing models. From simple weight averaging to more \u2026"}, {"title": "Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education", "link": "https://arxiv.org/pdf/2407.17022", "details": "S Kim, S Kim - arXiv preprint arXiv:2407.17022, 2024", "abstract": "Large language model (LLM)-based evaluation pipelines have demonstrated their capability to robustly evaluate machine-generated text. Extending this methodology to assess human-written text could significantly benefit educational settings by \u2026"}]
