[{"title": "LM2: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.920.pdf", "details": "G Juneja, S Dutta, T Chakraborty - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}, {"title": "SETLEXSEM CHALLENGE: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models", "link": "https://openreview.net/pdf%3Fid%3DMd1mEoPEaQ", "details": "NA Dronen, B Akhbari, M Gawali - The Thirty-eight Conference on Neural Information \u2026", "abstract": "Set theory is foundational to mathematics and, when sets are finite, to reasoning about the world. An intelligent system should perform set operations consistently, regardless of superficial variations in the operands. Initially designed for semantically \u2026"}, {"title": "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties", "link": "https://aclanthology.org/2024.emnlp-main.1137.pdf", "details": "K Yu, Z Zhang, F Hu, S Storks, J Chai - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "A major reason behind the recent success of large language models (LLMs) is their incontext learning capability, which makes it possible to rapidly adapt them to downstream textbased tasks by prompting them with a small number of relevant \u2026"}, {"title": "Improving Referring Ability for Biomedical Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.375.pdf", "details": "J Jiang, F Cheng, A Aizawa - Findings of the Association for Computational \u2026, 2024", "abstract": "Existing auto-regressive large language models (LLMs) are primarily trained using documents from general domains. In the biomedical domain, continual pre-training is a prevalent method for domain adaptation to inject professional knowledge into \u2026"}, {"title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models", "link": "https://aclanthology.org/2024.emnlp-main.566.pdf", "details": "XH Feng, C Chen, Y Li, Z Lin - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Pre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for \u2026"}, {"title": "Reducing Distraction in Long-Context Language Models by Focused Learning", "link": "https://arxiv.org/pdf/2411.05928", "details": "Z Wu, B Liu, R Yan, L Chen, T Delteil - arXiv preprint arXiv:2411.05928, 2024", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced their capacity to process long contexts. However, effectively utilizing this long context remains a challenge due to the issue of distraction, where irrelevant \u2026"}, {"title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "link": "https://arxiv.org/pdf/2411.08733%3F", "details": "S Singla, Z Wang, T Liu, A Ashfaq, Z Hu, EP Xing - arXiv preprint arXiv:2411.08733, 2024", "abstract": "Aligning Large Language Models (LLMs) traditionally relies on costly training and human preference annotations. Self-alignment seeks to reduce these expenses by enabling models to align themselves. To further lower costs and achieve alignment \u2026"}, {"title": "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "link": "https://aclanthology.org/2024.emnlp-main.565.pdf", "details": "J Li, H Zhang, F Zhang, TW Chang, K Kuang, L Chen\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Reinforcement learning from human feedback (RLHF) and AI-generated feedback (RLAIF) have become prominent techniques that significantly enhance the functionality of pre-trained language models (LMs). These methods harness \u2026"}, {"title": "Can Machine Unlearning Reduce Social Bias in Language Models?", "link": "https://aclanthology.org/2024.emnlp-industry.71.pdf", "details": "O Dige, D Arneja, TF Yau, Q Zhang, M Bolandraftar\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Mitigating bias in language models (LMs) has become a critical problem due to the widespread deployment of LMs in the industry and customer-facing applications. Numerous approaches revolve around data pre-processing and subsequent fine \u2026"}]
