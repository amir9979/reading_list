'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Refining Pre-trained Language Models for Domain Adapta'
[{"title": "Argumentative Large Language Models for Explainable and Contestable Decision-Making", "link": "https://arxiv.org/pdf/2405.02079", "details": "G Freedman, A Dejl, D Gorur, X Yin, A Rago, F Toni - arXiv preprint arXiv:2405.02079, 2024", "abstract": "The diversity of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them a promising candidate for use in decision-making. However, they are currently limited \u2026"}, {"title": "OpenEQA: Embodied Question Answering in the Era of Foundation Models", "link": "https://openreview.net/pdf%3Fid%3D7JIW6e1UJX", "details": "A Majumdar, A Ajay, X Zhang, P Putta, S Yenamandra\u2026 - 2nd Workshop on Mobile \u2026, 2024", "abstract": "We present a modern formulation of Embodied Question Answering (EQA) as the task of understanding an environment well enough to answer questions about it in natural language. An agent can achieve such an understanding by either drawing \u2026"}, {"title": "ATG: Benchmarking Automated Theorem Generation for Generative Language Models", "link": "https://eleanor-h.github.io/publication/confnaacl-2024-atg/confnaacl-2024-atg.pdf", "details": "X Lin, Q Cao, Y Huang, Z Yang, Z Liu, Z Li, X Liang15", "abstract": "Humans can develop new theorems to explore broader and more complex mathematical results. While current generative language models (LMs) have achieved significant improvement in automatically proving theorems, their ability to \u2026"}, {"title": "VI-OOD: A Unified Representation Learning Framework for Textual Out-of-distribution Detection", "link": "https://arxiv.org/pdf/2404.06217", "details": "LM Zhan, B Liu, XM Wu - arXiv preprint arXiv:2404.06217, 2024", "abstract": "Out-of-distribution (OOD) detection plays a crucial role in ensuring the safety and reliability of deep neural networks in various applications. While there has been a growing focus on OOD detection in visual data, the field of textual OOD detection has \u2026"}, {"title": "On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?", "link": "https://arxiv.org/pdf/2405.02266", "details": "M Zanella, IB Ayed - arXiv preprint arXiv:2405.02266, 2024", "abstract": "The development of large vision-language models, notably CLIP, has catalyzed research into effective adaptation techniques, with a particular focus on soft prompt tuning. Conjointly, test-time augmentation, which utilizes multiple augmented views \u2026"}, {"title": "Structural Pruning of Pre-trained Language Models via Neural Architecture Search", "link": "https://arxiv.org/pdf/2405.02267", "details": "A Klein, J Golebiowski, X Ma, V Perrone\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models (PLM), for example BERT or RoBERTa, mark the state- of-the-art for natural language understanding task when fine-tuned on labeled data. However, their large size poses challenges in deploying them for inference in real \u2026"}, {"title": "Text Quality-Based Pruning for Efficient Training of Language Models", "link": "https://arxiv.org/pdf/2405.01582", "details": "V Sharma, K Padthe, N Ardalani, K Tirumala, R Howes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent times training Language Models (LMs) have relied on computationally heavy training over massive datasets which makes this training process extremely laborious. In this paper we propose a novel method for numerically evaluating text \u2026"}, {"title": "Model & Data Insights using Pre-trained Language Models", "link": "https://openreview.net/pdf%3Fid%3DL5T3ZqsD0j", "details": "S Asgari, A Khani, AH Khasahmadi, A Sanghi\u2026 - ICLR 2024 Workshop on \u2026", "abstract": "We propose TExplain, using language models to interpret pre-trained image classifiers' features. Our approach connects the feature space of image classifiers with language models, generating explanatory sentences during inference. By \u2026"}, {"title": "Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case Study", "link": "https://arxiv.org/pdf/2404.08259", "details": "WH Her, U Kruschwitz - arXiv preprint arXiv:2404.08259, 2024", "abstract": "Machine Translation has made impressive progress in recent years offering close to human-level performance on many languages, but studies have primarily focused on high-resource languages with broad online presence and resources. With the help of \u2026"}]
