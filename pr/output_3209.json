[{"title": "Evaluating Copyright Takedown Methods for Language Models", "link": "https://arxiv.org/pdf/2406.18664", "details": "B Wei, W Shi, Y Huang, NA Smith, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) derive their capabilities from extensive training on diverse data, including potentially copyrighted material. These models can memorize and generate content similar to their training data, posing potential concerns. Therefore \u2026"}, {"title": "BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models", "link": "https://arxiv.org/pdf/2406.17092", "details": "Y Zeng, W Sun, TN Huynh, D Song, B Li, R Jia - arXiv preprint arXiv:2406.17092, 2024", "abstract": "Safety backdoor attacks in large language models (LLMs) enable the stealthy triggering of unsafe behaviors while evading detection during normal interactions. The high dimensionality of potential triggers in the token space and the diverse \u2026"}, {"title": "Lifelong Robot Library Learning: Bootstrapping Composable and Generalizable Skills for Embodied Control with Language Models", "link": "https://arxiv.org/pdf/2406.18746", "details": "G Tziafas, H Kasaei - arXiv preprint arXiv:2406.18746, 2024", "abstract": "Large Language Models (LLMs) have emerged as a new paradigm for embodied reasoning and control, most recently by generating robot policy code that utilizes a custom library of vision and control primitive skills. However, prior arts fix their skills \u2026"}, {"title": "Unlocking Continual Learning Abilities in Language Models", "link": "https://arxiv.org/pdf/2406.17245", "details": "W Du, S Cheng, T Luo, Z Qiu, Z Huang, KC Cheung\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) exhibit impressive performance and generalization capabilities. However, LMs struggle with the persistent challenge of catastrophic forgetting, which undermines their long-term sustainability in continual learning (CL) \u2026"}, {"title": "STAR: SocioTechnical Approach to Red Teaming Language Models", "link": "https://arxiv.org/pdf/2406.11757", "details": "L Weidinger, J Mellor, BG Pegueroles, N Marchal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This research introduces STAR, a sociotechnical framework that improves on current best practices for red teaming safety of large language models. STAR makes two key contributions: it enhances steerability by generating parameterised instructions for \u2026"}, {"title": "Mental Modeling of Reinforcement Learning Agents by Language Models", "link": "https://arxiv.org/pdf/2406.18505", "details": "W Lu, X Zhao, J Spisak, JH Lee, S Wermter - arXiv preprint arXiv:2406.18505, 2024", "abstract": "Can emergent language models faithfully model the intelligence of decision-making agents? Though modern language models exhibit already some reasoning ability, and theoretically can potentially express any probable distribution over tokens, it \u2026"}, {"title": "Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?", "link": "https://arxiv.org/pdf/2406.16316", "details": "Y Jinnai - arXiv preprint arXiv:2406.16316, 2024", "abstract": "Alignment of the language model with human preferences is a common approach to making a language model useful to end users. However, most alignment work is done in English, and human preference datasets are dominated by English \u2026"}, {"title": "How Truncating Weights Improves Reasoning in Language Models", "link": "https://arxiv.org/pdf/2406.03068", "details": "L Chen, J Bruna, A Bietti - arXiv preprint arXiv:2406.03068, 2024", "abstract": "In addition to the ability to generate fluent text in various languages, large language models have been successful at tasks that involve basic forms of logical\" reasoning\" over their context. Recent work found that selectively removing certain components \u2026"}, {"title": "Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models", "link": "https://arxiv.org/pdf/2406.04274", "details": "X Ji, S Kulkarni, M Wang, T Xie - arXiv preprint arXiv:2406.04274, 2024", "abstract": "This work studies the challenge of aligning large language models (LLMs) with offline preference data. We focus on alignment by Reinforcement Learning from Human Feedback (RLHF) in particular. While popular preference optimization \u2026"}]
