[{"title": "Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation on Language Models", "link": "https://arxiv.org/pdf/2412.14528", "details": "X Cui, M Zhu, Y Qin, L Xie, W Zhou, H Li - arXiv preprint arXiv:2412.14528, 2024", "abstract": "Knowledge distillation (KD) has become a prevalent technique for compressing large language models (LLMs). Existing KD methods are constrained by the need for identical tokenizers (ie, vocabularies) between teacher and student models, limiting \u2026"}, {"title": "Automated Machine Learning for Healthcare", "link": "https://link.springer.com/content/pdf/10.1007/978-981-97-8533-9.pdf%23page%3D33", "details": "MSRL Reddy, YA Sagar, N Jyothi, AN Rao, S Shilpa\u2026 - Cybernetics, Human Cognition \u2026", "abstract": "Abstract Automated Machine Learning (AutoML) has emerged as a promising approach to democratize machine learning techniques, enabling non-experts to build and deploy predictive models efficiently. In the healthcare domain, AutoML \u2026"}]
