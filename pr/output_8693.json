[{"title": "Which Client is Reliable?: A Reliable and Personalized Prompt-based Federated Learning for Medical Image Question Answering", "link": "https://arxiv.org/pdf/2410.17484", "details": "H Zhu, R Togo, T Ogawa, M Haseyama - arXiv preprint arXiv:2410.17484, 2024", "abstract": "Conventional medical artificial intelligence (AI) models face barriers in clinical application and ethical issues owing to their inability to handle the privacy-sensitive characteristics of medical data. We present a novel personalized federated learning \u2026"}, {"title": "Multifaceted Natural Language Processing Task\u2013Based Evaluation of Bidirectional Encoder Representations From Transformers Models for Bilingual (Korean and \u2026", "link": "https://medinform.jmir.org/2024/1/e52897/", "details": "K Kim, S Park, J Min, S Park, JY Kim, J Eun, K Jung\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background: The bidirectional encoder representations from transformers (BERT) model has attracted considerable attention in clinical applications, such as patient classification and disease prediction. However, current studies have typically \u2026"}, {"title": "Natural Language Inference Improves Compositionality in Vision-Language Models", "link": "https://arxiv.org/pdf/2410.22315%3F", "details": "P Cascante-Bonilla, Y Hou, YT Cao, H Daum\u00e9 III\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Compositional reasoning in Vision-Language Models (VLMs) remains challenging as these models often struggle to relate objects, attributes, and spatial relationships. Recent methods aim to address these limitations by relying on the semantics of the \u2026"}, {"title": "Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection", "link": "https://arxiv.org/pdf/2411.03359", "details": "G Yu, J Zhu, J Yao, B Han - arXiv preprint arXiv:2411.03359, 2024", "abstract": "Out-of-distribution (OOD) detection is crucial for deploying reliable machine learning models in open-world applications. Recent advances in CLIP-based OOD detection have shown promising results via regularizing prompt tuning with OOD features \u2026"}, {"title": "Retrieval In Decoder benefits generative models for explainable complex question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007573", "details": "J Feng, Q Wang, H Qiu, L Liu - Neural Networks, 2024", "abstract": "Abstract Large-scale Language Models (LLMs) utilizing the Chain-of-Thought prompting demonstrate exceptional performance in a variety of tasks. However, the persistence of factual hallucinations remains a significant challenge in practical \u2026"}, {"title": "Language-based reasoning graph neural network for commonsense question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007408", "details": "M Yang, Y Wang, Y Gu - Neural Networks, 2024", "abstract": "Abstract Language model (LM) has played an increasingly important role in the common-sense understanding and reasoning in the CSQA task (Common Sense Question Answering). However, due to the amount of model parameters, increasing \u2026"}, {"title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "link": "https://arxiv.org/pdf/2410.11829%3F", "details": "Y Cao, Y Liu, Z Chen, G Shi, W Wang, D Zhao, T Lu - arXiv preprint arXiv:2410.11829, 2024", "abstract": "Despite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating \u2026"}, {"title": "Enhancing the Completeness of Rationales for Multi-Step Question Answering", "link": "https://dl.acm.org/doi/abs/10.1145/3627673.3679660", "details": "S Xue, Z Huang, X Lin, J Liu, L Qin, T Su, H Liu, Q Liu - Proceedings of the 33rd ACM \u2026, 2024", "abstract": "Learning to answer multi-step complex questions requires machines to perform like a human to think and reason step by step, which is one of the core abilities of a question answering system. Recent advancements have revealed that large \u2026"}, {"title": "Prompt Learning for Few-Shot Question Answering via Self-Context Data Augmentation", "link": "https://ieeexplore.ieee.org/abstract/document/10723112/", "details": "JQ Qiu, CY Zhang, CLP Chen - IEEE Transactions on Artificial Intelligence, 2024", "abstract": "Pre-trained language models (PLMs) have shown remarkable performance on question answering (QA) tasks, but they usually require fine-tuning that depends on a substantial quantity of QA pairs. Therefore, improving the performance of PLMs in \u2026"}]
