[{"title": "Mastering Board Games by External and Internal Planning with Language Models", "link": "https://arxiv.org/pdf/2412.12119%3F", "details": "J Schultz, J Adamek, M Jusup, M Lanctot, M Kaisers\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While large language models perform well on a range of complex tasks (eg, text generation, question answering, summarization), robust multi-step planning and reasoning remains a considerable challenge for them. In this paper we show that \u2026"}, {"title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models", "link": "https://arxiv.org/pdf/2411.19477", "details": "Y Chen, X Pan, Y Li, B Ding, J Zhou - arXiv preprint arXiv:2411.19477, 2024", "abstract": "We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates $ N $ candidate solutions, and then chooses the \u2026"}, {"title": "TimeXAI: Concept-Based Counterfactual Explanations for Time Series", "link": "https://openreview.net/pdf%3Fid%3D2hS9zLkQQ8", "details": "K Oublal, D BENHAIEM, E LE BORGNE - AAAI 2025 Workshop on Artificial Intelligence with \u2026", "abstract": "Explaining AI systems operating on time series data is crucial in many decision- making areas, such as healthcare, energy, and public policy-making, which requires interpretable and transparent explanations to overcome the black-box nature of \u2026"}, {"title": "PETapter: Leveraging PET-style classification heads for modular few-shot parameter-efficient fine-tuning", "link": "https://arxiv.org/pdf/2412.04975%3F", "details": "J Rieger, M Ruckdeschel, G Wiedemann - arXiv preprint arXiv:2412.04975, 2024", "abstract": "Few-shot learning and parameter-efficient fine-tuning (PEFT) are crucial to overcome the challenges of data scarcity and ever growing language model sizes. This applies in particular to specialized scientific domains, where researchers might lack \u2026"}, {"title": "Large language models: game-changers in the healthcare industry", "link": "https://pubmed.ncbi.nlm.nih.gov/39674769/", "details": "B Dong, L Zhang, J Yuan, Y Chen, Q Li, L Shen - Science bulletin, 2024", "abstract": "Large language models: game-changers in the healthcare industry Large language models: game-changers in the healthcare industry Sci Bull (Beijing). 2024 Nov 26:S2095-9273(24)00847-8. doi: 10.1016/j.scib.2024.11.031. Online ahead of print. Authors Bin Dong 1 , Li Zhang \u2026"}, {"title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases", "link": "https://arxiv.org/pdf/2412.04862", "details": "LG Research, S An, K Bae, E Choi, K Choi, SJ Choi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8 B, and 2.4 B. These models feature several \u2026"}, {"title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents", "link": "https://arxiv.org/pdf/2412.00821", "details": "R Jaiswal, D Jain, HP Popat, A Anand\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in various reasoning tasks. However, they encounter significant challenges when it comes to scientific reasoning, particularly in physics, which requires not only mathematical \u2026"}, {"title": "Neural abstractive summarization: improvements at the sequence-level", "link": "https://dr.ntu.edu.sg/bitstream/10356/181414/2/Mathieu_Ravaut_s_PhD_Thesis_Edited_Final.pdf", "details": "M Ravaut - 2024", "abstract": "Automatic text summarization has made a fantastic leap forward in the last five to ten years, fueled by the rise of deep learning systems. Summarization at large consists in compressing an input text or series of texts (such as a scientific paper, news articles \u2026"}]
