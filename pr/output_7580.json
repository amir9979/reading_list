[{"title": "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "link": "https://arxiv.org/pdf/2410.06554", "details": "Y Chen, D Zhu, Y Sun, X Chen, W Zhang, X Shen - arXiv preprint arXiv:2410.06554, 2024", "abstract": "Reinforcement Learning from Human Feedback significantly enhances Natural Language Processing by aligning language models with human expectations. A critical factor in this alignment is the strength of reward models used during training \u2026"}, {"title": "Data Selection via Optimal Control for Language Models", "link": "https://arxiv.org/pdf/2410.07064", "details": "Y Gu, L Dong, H Wang, Y Hao, Q Dong, F Wei\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved \u2026"}, {"title": "KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks", "link": "https://arxiv.org/pdf/2410.06526", "details": "K Ma, X Du, Y Wang, H Zhang, Z Wen, X Qu, J Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we introduce Knowledge-Orthogonal Reasoning (KOR), which minimizes the impact of domain-specific knowledge for a more accurate evaluation of models' reasoning abilities in out-of-distribution scenarios. Based on this concept, we \u2026"}, {"title": "Can Language Models Induce Grammatical Knowledge from Indirect Evidence?", "link": "https://arxiv.org/pdf/2410.06022", "details": "M Oba, Y Oseki, A Fukatsu, A Haga, H Ouchi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "What kinds of and how much data is necessary for language models to induce grammatical knowledge to judge sentence acceptability? Recent language models still have much room for improvement in their data efficiency compared to humans \u2026"}, {"title": "Evolutionary Contrastive Distillation for Language Model Alignment", "link": "https://arxiv.org/pdf/2410.07513", "details": "J Katz-Samuels, Z Li, H Yun, P Nigam, Y Xu, V Petricek\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The ability of large language models (LLMs) to execute complex instructions is essential for their real-world applications. However, several recent studies indicate that LLMs struggle with challenging instructions. In this paper, we propose \u2026"}, {"title": "RAFT: Realistic Attacks to Fool Text Detectors", "link": "https://arxiv.org/pdf/2410.03658", "details": "J Wang, R Li, J Yang, C Mao - arXiv preprint arXiv:2410.03658, 2024", "abstract": "Large language models (LLMs) have exhibited remarkable fluency across various tasks. However, their unethical applications, such as disseminating disinformation, have become a growing concern. Although recent works have proposed a number of \u2026"}, {"title": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe", "link": "https://arxiv.org/pdf/2410.05248", "details": "Y Xiao, S Zhang, W Zhou, M Ghassemi, S Zhao - arXiv preprint arXiv:2410.05248, 2024", "abstract": "To induce desired behaviors in large language models (LLMs) for interaction-driven tasks, the instruction-tuning stage typically trains LLMs on instruction-response pairs using the next-token prediction (NTP) loss. Previous work aiming to improve \u2026"}, {"title": "$\\beta $-calibration of Language Model Confidence Scores for Generative QA", "link": "https://arxiv.org/pdf/2410.06615", "details": "P Manggala, A Mastakouri, E Kirschbaum\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim \u2026"}, {"title": "Steering Large Language Models between Code Execution and Textual Reasoning", "link": "https://arxiv.org/pdf/2410.03524", "details": "Y Chen, H Jhamtani, S Sharma, C Fan, C Wang - arXiv preprint arXiv:2410.03524, 2024", "abstract": "While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100 \u2026"}]
