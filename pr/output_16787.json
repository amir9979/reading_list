[{"title": "From Evaluation to Defense: Advancing Safety in Video Large Language Models", "link": "https://arxiv.org/pdf/2505.16643", "details": "Y Sun, P Jiang, C Liu, L Lin, Z Lu, H Xie - arXiv preprint arXiv:2505.16643, 2025", "abstract": "While the safety risks of image-based **large** **language** **models** have been extensively studied, their video-based counterparts (Video LLMs) remain critically under-examined. To systematically study this problem, we introduce \\textbf{VideoSafetyBench (VSB-77k) \u2026", "entry_id": "http://arxiv.org/abs/2505.16643v1", "updated": "2025-05-22 13:16:53", "published": "2025-05-22 13:16:53", "authors": "Yiwei Sun;Peiqi Jiang;Chuanbin Liu;Luohao Lin;Zhiying Lu;Hongtao Xie", "summary": "While the safety risks of image-based large language models have been\nextensively studied, their video-based counterparts (Video LLMs) remain\ncritically under-examined. To systematically study this problem, we introduce\n\\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse\nbenchmark for Video LLM safety}, which compromises 77,646 video-query pairs and\nspans 19 principal risk categories across 10 language communities. \\textit{We\nreveal that integrating video modality degrades safety performance by an\naverage of 42.3\\%, exposing systemic risks in multimodal attack exploitation.}\nTo address this vulnerability, we propose \\textbf{VideoSafety-R1}, a dual-stage\nframework achieving unprecedented safety gains through two innovations: (1)\nAlarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens\ninto visual and textual sequences, enabling explicit harm perception across\nmodalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances\ndefensive reasoning through dynamic policy optimization with rule-based rewards\nderived from dual-modality verification. These components synergize to shift\nsafety alignment from passive harm recognition to active reasoning. The\nresulting framework achieves a 65.1\\% improvement on VSB-Eval-HH, and improves\nby 59.1\\%, 44.3\\%, and 15.0\\% on the image safety datasets MMBench, VLGuard,\nand FigStep, respectively. \\textit{Our codes are available in the supplementary\nmaterials.} \\textcolor{red}{Warning: This paper contains examples of harmful\nlanguage and videos, and reader discretion is recommended.}", "comment": "49 pages, 12 figures, 17 tables", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.16643v1;http://arxiv.org/pdf/2505.16643v1", "pdf_url": "http://arxiv.org/pdf/2505.16643v1"}, {"title": "SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models", "link": "https://arxiv.org/pdf/2505.16003", "details": "R Daynauth, C Clarke, K Flautner, L Tang, J Mars - arXiv preprint arXiv:2505.16003, 2025", "abstract": "\u2026 Despite the rapid advancement of **large** **language** **models** (LLMs), reliably **evaluating** their outputs in a way that aligns with human \u2026 effective **evaluation** framework that uses entropy maximization to calibrate LLM-generated scores based on a small \u2026", "entry_id": "http://arxiv.org/abs/2505.16003v1", "updated": "2025-05-21 20:40:30", "published": "2025-05-21 20:40:30", "authors": "Roland Daynauth;Christopher Clarke;Krisztian Flautner;Lingjia Tang;Jason Mars", "summary": "The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for\nevaluating language models. Although several calibration techniques have been\nproposed to better align these evaluators with human judgment, prior studies\nfocus primarily on narrow, well-structured benchmarks. As a result, it remains\nunclear whether such calibrations generalize to real-world, open-ended tasks.\n  In this work, we show that SOTA calibrated evaluators often fail in these\nsettings, exhibiting weak or even negative correlation with human judgments. To\naddress this, we propose SLMEval, a novel and efficient calibration method\nbased on entropy maximization over a small amount of human preference data. By\nestimating a latent distribution over model quality and reweighting evaluator\nscores accordingly, SLMEval achieves strong correlation with human evaluations\nacross two real-world production use cases and the public benchmark. For\nexample, on one such task, SLMEval achieves a Spearman correlation of 0.57 with\nhuman judgments, while G-Eval yields a negative correlation. In addition,\nSLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated\nevaluators such as G-eval.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.16003v1;http://arxiv.org/pdf/2505.16003v1", "pdf_url": "http://arxiv.org/pdf/2505.16003v1"}, {"title": "EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios", "link": "https://arxiv.org/pdf/2505.16160", "details": "B Xu, Y Bai, H Sun, Y Lin, S Liu, X Liang, Y Li, Y Gao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 As **large** **language** **models** continue to advance, their application in educational contexts re\u2026 **evaluation** metrics that cover 12 critical aspects relevant to both teachers and students. We further apply human annotation to ensure the \u2026", "entry_id": "http://arxiv.org/abs/2505.16160v1", "updated": "2025-05-22 03:01:28", "published": "2025-05-22 03:01:28", "authors": "Bin Xu;Yu Bai;Huashan Sun;Yiguan Lin;Siming Liu;Xinyue Liang;Yaolin Li;Yang Gao;Heyan Huang", "summary": "As large language models continue to advance, their application in\neducational contexts remains underexplored and under-optimized. In this paper,\nwe address this gap by introducing the first diverse benchmark tailored for\neducational scenarios, incorporating synthetic data containing 9 major\nscenarios and over 4,000 distinct educational contexts. To enable comprehensive\nassessment, we propose a set of multi-dimensional evaluation metrics that cover\n12 critical aspects relevant to both teachers and students. We further apply\nhuman annotation to ensure the effectiveness of the model-generated evaluation\nresponses. Additionally, we succeed to train a relatively small-scale model on\nour constructed dataset and demonstrate that it can achieve performance\ncomparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on\nthe test set. Overall, this work provides a practical foundation for the\ndevelopment and evaluation of education-oriented language models. Code and data\nare released at https://github.com/ybai-nlp/EduBench.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.16160v1;http://arxiv.org/pdf/2505.16160v1", "pdf_url": "http://arxiv.org/pdf/2505.16160v1"}, {"title": "OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models", "link": "https://arxiv.org/pdf/2505.16036", "details": "BE \u00c7etin, Y \u00d6zen, EN Demiry\u0131lmaz, K Eng\u00fcr\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 open-source **large** **language** **models**. We employ the LLM-as-a-Judge approach as an evaluator for large-scale **evaluation**. Our motivation \u2026 Table 1: The statistics of our data collection to **evaluate** the ethical considerations of **large** **language** **models** \u2026", "entry_id": "http://arxiv.org/abs/2505.16036v1", "updated": "2025-05-21 21:31:35", "published": "2025-05-21 21:31:35", "authors": "Burak Erin\u00e7 \u00c7etin;Y\u0131ld\u0131r\u0131m \u00d6zen;Elif Naz Demiry\u0131lmaz;Kaan Eng\u00fcr;Cagri Toraman", "summary": "Generative large language models present significant potential but also raise\ncritical ethical concerns. Most studies focus on narrow ethical dimensions, and\nalso limited diversity of languages and models. To address these gaps, we\nconduct a broad ethical evaluation of 29 recent open-source large language\nmodels using a novel data collection including four ethical aspects:\nRobustness, reliability, safety, and fairness. We analyze model behavior in\nboth a commonly used language, English, and a low-resource language, Turkish.\nOur aim is to provide a comprehensive ethical assessment and guide safer model\ndevelopment by filling existing gaps in evaluation breadth, language coverage,\nand model diversity. Our experimental results, based on LLM-as-a-Judge, reveal\nthat optimization efforts for many open-source models appear to have\nprioritized safety and fairness, and demonstrated good robustness while\nreliability remains a concern. We demonstrate that ethical evaluation can be\neffectively conducted independently of the language used. In addition, models\nwith larger parameter counts tend to exhibit better ethical performance, with\nGemma and Qwen models demonstrating the most ethical behavior among those\nevaluated.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.16036v1;http://arxiv.org/pdf/2505.16036v1", "pdf_url": "http://arxiv.org/pdf/2505.16036v1"}, {"title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios", "link": "https://arxiv.org/pdf/2505.16944", "details": "Y Qi, H Peng, X Wang, A Xin, Y Liu, B Xu, L Hou, J Li - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Specifically, as shown in Figure 3, we define three **evaluation** modes based on constraint types: (1) Code **evaluation** , which is used for constraints that can be verified through simple and deterministic Python code (eg, keyword presence \u2026", "entry_id": "http://arxiv.org/abs/2505.16944v1", "updated": "2025-05-22 17:31:10", "published": "2025-05-22 17:31:10", "authors": "Yunjia Qi;Hao Peng;Xiaozhi Wang;Amy Xin;Youfeng Liu;Bin Xu;Lei Hou;Juanzi Li", "summary": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL", "links": "http://arxiv.org/abs/2505.16944v1;http://arxiv.org/pdf/2505.16944v1", "pdf_url": "http://arxiv.org/pdf/2505.16944v1"}, {"title": "MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models", "link": "https://arxiv.org/pdf/2505.16700", "details": "X Gao, S Xie, J Zhai, S Ma, C Shen - arXiv preprint arXiv:2505.16700, 2025", "abstract": "\u2026 We selected 7 representative **large** **language** **models** for **evaluation** , covering current mainstream commercial closed-source models (Claude 3.7 Sonnet [35], GPT-4o [36], GPT-4o-mini [37], and Gemini 2.5 Pro [38]) and open-source models (Llama 3.3 \u2026", "entry_id": "http://arxiv.org/abs/2505.16700v1", "updated": "2025-05-22 14:02:37", "published": "2025-05-22 14:02:37", "authors": "Xuanqi Gao;Siyi Xie;Juan Zhai;Shqing Ma;Chao Shen", "summary": "As Large Language Models (LLMs) evolve from passive text generators to active\nreasoning agents capable of tool interaction, the Model Context Protocol (MCP)\nhas emerged as a standardized framework for dynamic tool discovery and\norchestration. Despite widespread industry adoption, existing evaluation\nmethodologies fail to adequately assess tool utilization capabilities within\nthis new paradigm. This paper introduces MCP-RADAR, the first comprehensive\nbenchmark specifically designed to evaluate LLM performance in the MCP\nframework through a novel five-dimensional approach measuring: answer accuracy,\ntool selection efficiency, computational resource efficiency, parameter\nconstruction accuracy, and execution speed. Unlike conventional benchmarks that\nrely on subjective human evaluations or binary success metrics, MCP-RADAR\nemploys objective, quantifiable measurements across multiple task domains\nincluding software engineering, mathematical reasoning, and general\nproblem-solving. Our evaluations of leading commercial and open-source LLMs\nreveal distinctive capability profiles with significant trade-offs between\naccuracy, efficiency, and speed, challenging traditional single-metric\nperformance rankings. Besides, we provide valuable guidance for developers to\noptimize their tools for maximum model compatibility and effectiveness. While\nfocused on MCP due to its standardized approach, our methodology remains\napplicable across all LLM agent tool integration frameworks, providing valuable\ninsights for both LLM developers and tool creators to optimize the entire\nLLM-tool interaction ecosystem. The implementation, configurations, and\ndatasets used in our evaluation are publicly available at\nhttps://anonymous.4open.science/r/MCPRadar-B143.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.16700v1;http://arxiv.org/pdf/2505.16700v1", "pdf_url": "http://arxiv.org/pdf/2505.16700v1"}, {"title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery", "link": "https://arxiv.org/pdf/2505.16477", "details": "Y Zhang, SA Khan, A Mahmud, H Yang, A Lavin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With recent Nobel Prizes recognising AI contributions to science, **Large** **Language** **Models** (\u2026 In this contribution, we review how **Large** **Language** **Models** (LLMs) are redefining the \u2026 and alignment with human scientific goals, with clear **evaluation** \u2026", "entry_id": "http://arxiv.org/abs/2505.16477v1", "updated": "2025-05-22 10:05:48", "published": "2025-05-22 10:05:48", "authors": "Yanbo Zhang;Sumeer A. Khan;Adnan Mahmud;Huck Yang;Alexander Lavin;Michael Levin;Jeremy Frey;Jared Dunnmon;James Evans;Alan Bundy;Saso Dzeroski;Jesper Tegner;Hector Zenil", "summary": "With recent Nobel Prizes recognising AI contributions to science, Large\nLanguage Models (LLMs) are transforming scientific research by enhancing\nproductivity and reshaping the scientific method. LLMs are now involved in\nexperimental design, data analysis, and workflows, particularly in chemistry\nand biology. However, challenges such as hallucinations and reliability\npersist. In this contribution, we review how Large Language Models (LLMs) are\nredefining the scientific method and explore their potential applications\nacross different stages of the scientific cycle, from hypothesis testing to\ndiscovery. We conclude that, for LLMs to serve as relevant and effective\ncreative engines and productivity enhancers, their deep integration into all\nsteps of the scientific process should be pursued in collaboration and\nalignment with human scientific goals, with clear evaluation metrics. The\ntransition to AI-driven science raises ethical questions about creativity,\noversight, and responsibility. With careful guidance, LLMs could evolve into\ncreative engines, driving transformative breakthroughs across scientific\ndisciplines responsibly and effectively. However, the scientific community must\nalso decide how much it leaves to LLMs to drive science, even when associations\nwith 'reasoning', mostly currently undeserved, are made in exchange for the\npotential to explore hypothesis and solution regions that might otherwise\nremain unexplored by human exploration alone.", "comment": "45 pages", "journal_ref": "npj Artificial Intelligence, 2025", "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.16477v1;http://arxiv.org/pdf/2505.16477v1", "pdf_url": "http://arxiv.org/pdf/2505.16477v1"}, {"title": "Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models", "link": "https://arxiv.org/pdf/2505.16538", "details": "E Nie, H Schmid, H Sch\u00fctze - arXiv preprint arXiv:2505.16538, 2025", "abstract": "\u2026 We **evaluate** the impact of each method on the LCB benchmark. Our results (Table 4) demonstrate that Comparative Importance Selection achieves the most effective reduction in language confusion, substantially outperforming both frequency-based \u2026", "entry_id": "http://arxiv.org/abs/2505.16538v1", "updated": "2025-05-22 11:29:17", "published": "2025-05-22 11:29:17", "authors": "Ercong Nie;Helmut Schmid;Hinrich Sch\u00fctze", "summary": "Language confusion -- where large language models (LLMs) generate unintended\nlanguages against the user's need -- remains a critical challenge, especially\nfor English-centric models. We present the first mechanistic interpretability\n(MI) study of language confusion, combining behavioral benchmarking with\nneuron-level analysis. Using the Language Confusion Benchmark (LCB), we show\nthat confusion points (CPs) -- specific positions where language switches occur\n-- are central to this phenomenon. Through layer-wise analysis with TunedLens\nand targeted neuron attribution, we reveal that transition failures in the\nfinal layers drive confusion. We further demonstrate that editing a small set\nof critical neurons, identified via comparative analysis with\nmultilingual-tuned models, substantially mitigates confusion without harming\ngeneral competence or fluency. Our approach matches multilingual alignment in\nconfusion reduction for most languages and yields cleaner, higher-quality\noutputs. These findings provide new insights into the internal dynamics of LLMs\nand highlight neuron-level interventions as a promising direction for robust,\ninterpretable multilingual language modeling.", "comment": "16 pages, 5 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.16538v1;http://arxiv.org/pdf/2505.16538v1", "pdf_url": "http://arxiv.org/pdf/2505.16538v1"}, {"title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?", "link": "https://arxiv.org/pdf/2505.16998", "details": "J Jiang, J Wang, Y Yan, Y Liu, J Zhu, M Zhang, X Cai\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **evaluation** of LLMs utilizing various formal languages to tackle diverse logical reasoning problems. At first, we develop the **evaluation** \u2026 Secondly, we perform a thorough **evaluation** across these three dimensions (as detailed in Section 3). Many \u2026", "entry_id": "http://arxiv.org/abs/2505.16998v1", "updated": "2025-05-22 17:57:23", "published": "2025-05-22 17:57:23", "authors": "Jin Jiang;Jianing Wang;Yuchen Yan;Yang Liu;Jianhua Zhu;Mengdi Zhang;Xunliang Cai;Liangcai Gao", "summary": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.16998v1;http://arxiv.org/pdf/2505.16998v1", "pdf_url": "http://arxiv.org/pdf/2505.16998v1"}]
