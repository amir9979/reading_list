'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Detecting Edited Knowledge in Language Models](https:/'
[{"title": "Recall Them All: Retrieval-Augmented Language Models for Long Object List Extraction from Long Documents", "link": "https://arxiv.org/pdf/2405.02732", "details": "S Singhania, S Razniewski, G Weikum - arXiv preprint arXiv:2405.02732, 2024", "abstract": "Methods for relation extraction from text mostly focus on high precision, at the cost of limited recall. High recall is crucial, though, to populate long lists of object entities that stand in a specific relation with a given subject. Cues for relevant objects can be \u2026"}, {"title": "Addax: Memory-Efficient Fine-Tuning of Language Models with a Combination of Forward-Backward and Forward-Only Passes", "link": "https://openreview.net/pdf%3Fid%3DYtZv36CY5p", "details": "Z Li, X Zhang, M Razaviyayn - 5th Workshop on practical ML for limited/low resource \u2026", "abstract": "Fine-tuning language models (LMs) with first-order optimizers often demands excessive memory, limiting accessibility, while zeroth-order optimizers use less memory, but suffer from slow convergence depending on model size. We introduce a \u2026"}, {"title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2405.02659", "details": "T Zhang, D Li, Q Chen, C Wang, L Huang, H Xue, X He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Retrieval-augmented large language models (LLMs) leverage relevant content retrieved by information retrieval systems to generate correct responses, aiming to alleviate the hallucination problem. However, existing retriever-responder methods \u2026"}, {"title": "Redefining Information Retrieval of Structured Database via Large Language Models", "link": "https://arxiv.org/pdf/2405.05508", "details": "M Wang, Y Zhang, Q Zhao, J Yang, H Zhang - arXiv preprint arXiv:2405.05508, 2024", "abstract": "Retrieval augmentation is critical when Language Models (LMs) exploit non- parametric knowledge related to the query through external knowledge bases before reasoning. The retrieved information is incorporated into LMs as context alongside \u2026"}, {"title": "Backdoor Removal for Generative Large Language Models", "link": "https://arxiv.org/pdf/2405.07667", "details": "H Li, Y Chen, Z Zheng, Q Hu, C Chan, H Liu, Y Song - arXiv preprint arXiv \u2026, 2024", "abstract": "With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased \u2026"}, {"title": "NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli", "link": "https://arxiv.org/pdf/2405.02814", "details": "X Wang, C Li, Y Chang, J Wang, Y Wu - arXiv preprint arXiv:2405.02814, 2024", "abstract": "Large Language Models (LLMs) have become integral to a wide spectrum of applications, ranging from traditional computing tasks to advanced artificial intelligence (AI) applications. This widespread adoption has spurred extensive \u2026"}, {"title": "Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense", "link": "https://arxiv.org/pdf/2405.04655", "details": "S Shen, L Logeswaran, M Lee, H Lee, S Poria\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated substantial commonsense understanding through numerous benchmark evaluations. However, their understanding of cultural commonsense remains largely unexamined. In this paper \u2026"}, {"title": "BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models", "link": "https://arxiv.org/pdf/2405.04756", "details": "CF Luo, A Ghawanmeh, X Zhu, FK Khattak - arXiv preprint arXiv:2405.04756, 2024", "abstract": "Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly. The language model can also \u2026"}, {"title": "LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play", "link": "https://arxiv.org/pdf/2405.06373", "details": "LC Lu, SJ Chen, TM Pai, CH Yu, H Lee, SH Sun - arXiv preprint arXiv:2405.06373, 2024", "abstract": "Large language models (LLMs) have shown exceptional proficiency in natural language processing but often fall short of generating creative and original responses to open-ended questions. To enhance LLM creativity, our key insight is to \u2026"}]
