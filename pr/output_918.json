'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Construction of Domain-specified Japanese Large Langua'
[{"title": "RS-BERT: Pre-training radical enhanced sense embedding for Chinese word sense disambiguation", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324001006", "details": "X Zhou, H Huang, Z Chi, M Ren, Y Gao - Information Processing & Management, 2024", "abstract": "Word sense disambiguation is a crucial task to test whether a model can perform deep understanding. Nowadays, the rise of pre-trained language models facilitates substantial success in such tasks. However, most current pre-training tasks are taken \u2026"}, {"title": "Mirrored X-Net: Joint Classification and Contrastive Learning for Weakly Supervised GA Segmentation in SD-OCT\u2605", "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002589", "details": "Z Ji, X Ma, T Leng, DL Rubin, Q Chen - Pattern Recognition, 2024", "abstract": "Deep learning achieves impressive performance in medical image segmentation, but the training phase requires a large amount of annotated data with precise clinical definitions. Weakly supervised lesion segmentation aims to produce pixel-level \u2026"}]
