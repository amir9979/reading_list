[{"title": "Minerva: A Programmable Memory Test Benchmark for Language Models", "link": "https://arxiv.org/pdf/2502.03358%3F", "details": "M Xia, V Ruehle, S Rajmohan, R Shokri - arXiv preprint arXiv:2502.03358, 2025", "abstract": "How effectively can LLM-based AI assistants utilize their memory (context) to perform various tasks? Traditional data benchmarks, which are often manually crafted, suffer from several limitations: they are static, susceptible to overfitting, difficult to interpret \u2026"}, {"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models", "link": "https://arxiv.org/pdf/2502.03199%3F", "details": "J Wu, Y Shen, S Liu, Y Tang, S Song, X Wang, L Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the \u2026"}, {"title": "End-to-end Trajectory Generation-Contrasting Deep Generative Models and Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3716892", "details": "L Zhang, J Mbuya, L Zhao, D Pfoser, A Anastasopoulos - ACM Transactions on \u2026, 2025", "abstract": "Due to the limited availability of actual large-scale datasets, realistic synthetic trajectory data play a crucial role in various research domains, including spatiotemporal data mining and data management, and domain-driven research \u2026"}, {"title": "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting", "link": "https://arxiv.org/pdf/2502.04395", "details": "S Zhong, W Ruan, M Jin, H Li, Q Wen, Y Liang - arXiv preprint arXiv:2502.04395, 2025", "abstract": "Recent advancements in time series forecasting have explored augmenting models with text or vision modalities to improve accuracy. While text provides contextual understanding, it often lacks fine-grained temporal details. Conversely, vision \u2026"}, {"title": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs", "link": "https://arxiv.org/pdf/2502.14645", "details": "Y Wu, L Ding, L Shen, D Tao - arXiv preprint arXiv:2502.14645, 2025", "abstract": "Knowledge editing allows for efficient adaptation of large language models (LLMs) to new information or corrections without requiring full retraining. However, prior methods typically focus on either single-language editing or basic multilingual \u2026"}, {"title": "Evaluation of Large Language Models via Coupled Token Generation", "link": "https://arxiv.org/pdf/2502.01754", "details": "NC Benz, S Tsirtsis, E Straitouri, I Chatzi, AA Velasco\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "State of the art large language models rely on randomization to respond to a prompt. As an immediate consequence, a model may respond differently to the same prompt if asked multiple times. In this work, we argue that the evaluation and ranking of large \u2026"}, {"title": "Fact or Guesswork? Evaluating Large Language Model's Medical Knowledge with Structured One-Hop Judgment", "link": "https://arxiv.org/pdf/2502.14275", "details": "J Li, Y Wang, K Zhang, Y Cai, B Hooi, N Peng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have been widely adopted in various downstream task domains. However, their ability to directly recall and apply factual medical knowledge remains under-explored. Most existing medical QA benchmarks assess \u2026"}, {"title": "Knowing When to Stop: Dynamic Context Cutoff for Large Language Models", "link": "https://arxiv.org/pdf/2502.01025", "details": "R Xie, J Wang, P Rosu, C Deng, B Sun, Z Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) process entire input contexts indiscriminately, which is inefficient in cases where the information required to answer a query is localized within the context. We present dynamic context cutoff, a human-inspired method \u2026"}, {"title": "CoddLLM: Empowering Large Language Models for Data Analytics", "link": "https://arxiv.org/pdf/2502.00329", "details": "J Zhang, H Zhang, R Chakravarti, Y Hu, P Ng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have the potential to revolutionize data analytics by simplifying tasks such as data discovery and SQL query synthesis through natural language interactions. This work serves as a pivotal first step toward the \u2026"}]
