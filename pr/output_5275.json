[{"title": "A Grounded Preference Model for LLM Alignment", "link": "https://aclanthology.org/2024.findings-acl.10.pdf", "details": "T Naseem, G Xu, S Swaminathan, A Yehudai\u2026 - Findings of the Association \u2026, 2024", "abstract": "Despite LLMs' recent advancements, they still suffer from factual inconsistency and hallucination. An often-opted remedy is retrieval-augmented generation\u2013however, there is no guarantee that the model will strictly adhere to retrieved grounding \u2026"}, {"title": "Intravitreal Antiangiogenic Treatment for Diabetic Retinopathy: A Mexican Real-Life Scenario Experience", "link": "https://www.mdpi.com/2075-1729/14/8/976", "details": "S L\u00f3pez-Letayf, O Vivanco-Rojas, V Londo\u00f1o-Angarita\u2026 - Life, 2024", "abstract": "The objective of this study was to analyze the effectiveness of two intravitreal antiangiogenic drugs, ranibizumab and aflibercept, in a Mexican population over a period of 5 years, evaluating the improvement in visual acuity (VA) and central retinal \u2026"}, {"title": "Computer-aided detection of retinopathy of prematurity severity assessment via vessel tortuosity measurement in preterm infants' fundus images", "link": "https://www.nature.com/articles/s41433-024-03285-w", "details": "YP Huang, S Vadloori, EYC Kang, Y Fukushima\u2026 - Eye, 2024", "abstract": "Objective To develop a computer-aided diagnostic system for retinopathy of prematurity (ROP) disease using retinal vessel morphological features. Methods A total of 200 fundus images from 136 preterm infants with stage 1 to 3 ROP were \u2026"}, {"title": "Teaching Small Language Models to Reason for Knowledge-Intensive Multi-Hop Question Answering", "link": "https://aclanthology.org/2024.findings-acl.464.pdf", "details": "X Li, S He, F Lei, JY JunYang, T Su, K Liu, J Zhao - Findings of the Association for \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) can teach small language models (SLMs) to solve complex reasoning tasks (eg, mathematical question answering) by Chain-of- thought Distillation (CoTD). Specifically, CoTD fine-tunes SLMs by utilizing rationales \u2026"}, {"title": "Language Models Don't Learn the Physical Manifestation of Language", "link": "https://aclanthology.org/2024.acl-long.195.pdf", "details": "B Lee, J Lim - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "We argue that language-only models don't learn the physical manifestation of language. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-Test. These tasks highlight a \u2026"}, {"title": "Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination", "link": "https://aclanthology.org/2024.acl-long.490.pdf", "details": "N Yang, T Kang, SJ Choi, H Lee, K Jung - Proceedings of the 62nd Annual Meeting of \u2026, 2024", "abstract": "Instruction-following language models often show undesirable biases. These undesirable biases may be accelerated in the real-world usage of language models, where a wide range of instructions is used through zero-shot example prompting. To \u2026"}]
