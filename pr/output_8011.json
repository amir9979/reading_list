[{"title": "Analysing the Residual Stream of Language Models Under Knowledge Conflicts", "link": "https://arxiv.org/pdf/2410.16090", "details": "Y Zhao, X Du, G Hong, AP Gema, A Devoto, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model \u2026"}, {"title": "Rethinking Distance Metrics for Counterfactual Explainability", "link": "https://arxiv.org/pdf/2410.14522", "details": "JN Williams, A Katakkar, H Heidari, JZ Kolter - arXiv preprint arXiv:2410.14522, 2024", "abstract": "Counterfactual explanations have been a popular method of post-hoc explainability for a variety of settings in Machine Learning. Such methods focus on explaining classifiers by generating new data points that are similar to a given reference, while \u2026"}, {"title": "Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning", "link": "https://arxiv.org/pdf/2410.14464", "details": "J Tang, T Xia, Y Lu, C Mascolo, A Saeed - arXiv preprint arXiv:2410.14464, 2024", "abstract": "Electrocardiogram (ECG) interpretation requires specialized expertise, often involving synthesizing insights from ECG signals with complex clinical queries posed in natural language. The scarcity of labeled ECG data coupled with the diverse \u2026"}, {"title": "Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting Framework", "link": "https://arxiv.org/pdf/2409.16146%3F", "details": "L Chen, R Zhang, J Guo, Y Fan, X Cheng - arXiv preprint arXiv:2409.16146, 2024", "abstract": "Retrieval-augmented generation (RAG) has emerged as a popular solution to mitigate the hallucination issues of large language models. However, existing studies on RAG seldom address the issue of predictive uncertainty, ie, how likely it is \u2026"}, {"title": "Tackling domain generalization for out-of-distribution endoscopic imaging", "link": "https://arxiv.org/pdf/2410.14821", "details": "MA Teevno, G Ochoa-Ruiz, S Ali - arXiv preprint arXiv:2410.14821, 2024", "abstract": "While recent advances in deep learning (DL) for surgical scene segmentation have yielded promising results on single-center and single-imaging modality data, these methods usually do not generalize well to unseen distributions or modalities. Even \u2026"}, {"title": "Contrastive Learning to Improve Retrieval for Real-world Fact Checking", "link": "https://arxiv.org/pdf/2410.04657", "details": "A Sriram, F Xu, E Choi, G Durrett - arXiv preprint arXiv:2410.04657, 2024", "abstract": "Recent work on fact-checking addresses a realistic setting where models incorporate evidence retrieved from the web to decide the veracity of claims. A bottleneck in this pipeline is in retrieving relevant evidence: traditional methods may surface \u2026"}, {"title": "ComPO: Community Preferences for Language Model Personalization", "link": "https://arxiv.org/pdf/2410.16027", "details": "S Kumar, CY Park, Y Tsvetkov, NA Smith, H Hajishirzi - arXiv preprint arXiv \u2026, 2024", "abstract": "Conventional algorithms for training language models (LMs) with human feedback rely on preferences that are assumed to account for an\" average\" user, disregarding subjectivity and finer-grained variations. Recent studies have raised concerns that \u2026"}, {"title": "Harnessing Diversity for Important Data Selection in Pretraining Large Language Models", "link": "https://arxiv.org/pdf/2409.16986", "details": "C Zhang, H Zhong, K Zhang, C Chai, R Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Data selection is of great significance in pre-training large language models, given the variation in quality within the large-scale available training corpora. To achieve this, researchers are currently investigating the use of data influence to measure the \u2026"}, {"title": "Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents", "link": "https://arxiv.org/pdf/2409.17140", "details": "J Lu, Z Zhang, F Yang, J Zhang, L Wang, C Du, Q Lin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal large language models (MLLMs) have enabled LLM-based agents to directly interact with application user interfaces (UIs), enhancing agents' performance in complex tasks. However, these agents often suffer from high latency and low \u2026"}]
