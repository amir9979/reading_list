[{"title": "Causal Evaluation of Language Models", "link": "https://arxiv.org/pdf/2405.00622", "details": "S Chen, B Peng, M Chen, R Wang, M Xu, X Zeng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for \u2026"}, {"title": "A Lost Opportunity for Vision-Language Models: A Comparative Study of Online Test-time Adaptation for Vision-Language Models", "link": "https://arxiv.org/pdf/2405.14977", "details": "M D\u00f6bler, RA Marsden, T Raichle, B Yang - arXiv preprint arXiv:2405.14977, 2024", "abstract": "In the realm of deep learning, maintaining model robustness against distribution shifts is critical. This paper investigates test-time adaptation strategies for vision- language models, with a specific focus on CLIP and its variants. Through a \u2026"}, {"title": "KFEX-N: A table-text data question-answering model based on knowledge-fusion encoder and EX-N tree decoder", "link": "https://www.sciencedirect.com/science/article/pii/S0925231224005666", "details": "Y Tao, J Liu, H Li, W Cao, X Qin, Y Tian, Y Du - Neurocomputing, 2024", "abstract": "Answering questions about hybrid data combining tables and text is challenging. Recent research has employed encoder-tree decoder frameworks to simulate the reasoning process of arithmetic expressions for generating answers. However, this \u2026"}, {"title": "Zero-shot LLM-guided Counterfactual Generation for Text", "link": "https://arxiv.org/pdf/2405.04793", "details": "A Bhattacharjee, R Moraffah, J Garland, H Liu - arXiv preprint arXiv:2405.04793, 2024", "abstract": "Counterfactual examples are frequently used for model development and evaluation in many natural language processing (NLP) tasks. Although methods for automated counterfactual generation have been explored, such methods depend on models \u2026"}, {"title": "CHESS: Contextual Harnessing for Efficient SQL Synthesis", "link": "https://arxiv.org/pdf/2405.16755", "details": "S Talaei, M Pourreza, YC Chang, A Mirhoseini\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Utilizing large language models (LLMs) for transforming natural language questions into SQL queries (text-to-SQL) is a promising yet challenging approach, particularly when applied to real-world databases with complex and extensive schemas. In \u2026"}, {"title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective", "link": "https://arxiv.org/pdf/2405.16747", "details": "A Tomihari, I Sato - arXiv preprint arXiv:2405.16747, 2024", "abstract": "The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data. This success is largely \u2026"}, {"title": "SUTRA: Scalable Multilingual Language Model Architecture", "link": "https://arxiv.org/pdf/2405.06694", "details": "A Bendale, M Sapienza, S Ripplinger, S Gibbs, J Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we introduce SUTRA, multilingual Large Language Model architecture capable of understanding, reasoning, and generating text in over 50 languages. SUTRA's design uniquely decouples core conceptual understanding from language \u2026"}, {"title": "MCS-SQL: Leveraging Multiple Prompts and Multiple-Choice Selection For Text-to-SQL Generation", "link": "https://arxiv.org/pdf/2405.07467", "details": "D Lee, C Park, J Kim, H Park - arXiv preprint arXiv:2405.07467, 2024", "abstract": "Recent advancements in large language models (LLMs) have enabled in-context learning (ICL)-based methods that significantly outperform fine-tuning approaches for text-to-SQL tasks. However, their performance is still considerably lower than that \u2026"}, {"title": "What Variables Affect Out-Of-Distribution Generalization in Pretrained Models?", "link": "https://arxiv.org/pdf/2405.15018", "details": "MY Harun, K Lee, J Gallardo, G Krishnan, C Kanan - arXiv preprint arXiv:2405.15018, 2024", "abstract": "Embeddings produced by pre-trained deep neural networks (DNNs) are widely used; however, their efficacy for downstream tasks can vary widely. We study the factors influencing out-of-distribution (OOD) generalization of pre-trained DNN embeddings \u2026"}]
