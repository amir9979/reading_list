[{"title": "scSSL-Bench: Benchmarking Self-Supervised Learning for Single-Cell Data", "link": "https://arxiv.org/pdf/2506.10031", "details": "O Ovcharenko, F Barkmann, P Toma, I Daunhawer\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Self-supervised learning (SSL) has proven to be a powerful approach for extracting biologically meaningful representations from single-cell data. To advance our understanding of SSL methods applied to single-cell data, we present scSSL-Bench \u2026", "entry_id": "http://arxiv.org/abs/2506.10031v1", "updated": "2025-06-10 12:31:42", "published": "2025-06-10 12:31:42", "authors": "Olga Ovcharenko;Florian Barkmann;Philip Toma;Imant Daunhawer;Julia Vogt;Sebastian Schelter;Valentina Boeva", "summary": "Self-supervised learning (SSL) has proven to be a powerful approach for\nextracting biologically meaningful representations from single-cell data. To\nadvance our understanding of SSL methods applied to single-cell data, we\npresent scSSL-Bench, a comprehensive benchmark that evaluates nineteen SSL\nmethods. Our evaluation spans nine datasets and focuses on three common\ndownstream tasks: batch correction, cell type annotation, and missing modality\nprediction. Furthermore, we systematically assess various data augmentation\nstrategies. Our analysis reveals task-specific trade-offs: the specialized\nsingle-cell frameworks, scVI, CLAIRE, and the finetuned scGPT excel at\nuni-modal batch correction, while generic SSL methods, such as VICReg and\nSimCLR, demonstrate superior performance in cell typing and multi-modal data\nintegration. Random masking emerges as the most effective augmentation\ntechnique across all tasks, surpassing domain-specific augmentations. Notably,\nour results indicate the need for a specialized single-cell multi-modal data\nintegration framework. scSSL-Bench provides a standardized evaluation platform\nand concrete recommendations for applying SSL to single-cell analysis,\nadvancing the convergence of deep learning and single-cell genomics.", "comment": "Accepted at ICML 2025 (Spotlight)", "journal_ref": null, "primary_category": "q-bio.QM", "categories": "q-bio.QM;cs.LG", "links": "http://arxiv.org/abs/2506.10031v1;http://arxiv.org/pdf/2506.10031v1", "pdf_url": "http://arxiv.org/pdf/2506.10031v1"}]
