[{"title": "Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead $\\mathbf{\\texttt{O}}$ptimization", "link": "https://arxiv.org/pdf/2505.07675", "details": "S Kang, DB Lee, H Jang, SJ Hwang - arXiv preprint arXiv:2505.07675, 2025", "abstract": "Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resource \u2026", "entry_id": "http://arxiv.org/abs/2505.07675v1", "updated": "2025-05-12 15:39:51", "published": "2025-05-12 15:39:51", "authors": "Seongjae Kang;Dong Bok Lee;Hyungjoon Jang;Sung Ju Hwang", "summary": "Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\n$\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead\n$\\mathbf{\\texttt{O}}$ptimization ($\\mathbf{\\texttt{DHO}}$) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that $\\texttt{DHO}$ mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that $\\texttt{DHO}$\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters.", "comment": "41 pages, 19 figures, preprint", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2505.07675v1;http://arxiv.org/pdf/2505.07675v1", "pdf_url": "http://arxiv.org/pdf/2505.07675v1"}, {"title": "Out-of-Distribution Detection Using Knowledge Distillation", "link": "https://ieeexplore.ieee.org/abstract/document/11008432/", "details": "\u013d Kr\u00e1lik, P Tar\u00e1bek - 2025 35th International Conference Radioelektronika \u2026, 2025", "abstract": "Out-of-distribution (OOD) samples pose a critical challenge for deep neural networks deployed in real-world applications, as they can lead to unreliable or incorrect predictions. This paper introduces a novel method for OOD detection based on \u2026"}]
