[{"title": "HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/32171/34326", "details": "KHI Arif, JY Yoon, DS Nikolopoulos, H Vandierendonck\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "Abstract High-resolution Vision-Language Models (VLMs) are widely used in multimodal tasks to enhance accuracy by preserving detailed image information. However, these models often generate an excessive number of visual tokens due to \u2026"}, {"title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.23064", "details": "Y Ren, K Tertikas, S Maiti, J Han, T Zhang, S S\u00fcsstrunk\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision-Language Models (LVLMs) struggle with puzzles, which require precise perception, rule comprehension, and logical reasoning. Assessing and enhancing their performance in this domain is crucial, as it reflects their ability to \u2026"}, {"title": "Unified Knowledge Maintenance Pruning and Progressive Recovery with Weight Recalling for Large Vision-Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/32923/35078", "details": "Z Wu, J Chen, Y Wang - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Abstract Large Vision-Language Model (LVLM), leveraging Large Language Model (LLM) as the cognitive core, has recently become one of the most representative multimodal model paradigms. However, with the expansion of unimodal \u2026"}, {"title": "Towards Understanding How Knowledge Evolves in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2504.02862", "details": "S Wang, Y Zhang, Y Zhu, J Li, Z Wang, Y Liu, X Ji - arXiv preprint arXiv:2504.02862, 2025", "abstract": "Large Vision-Language Models (LVLMs) are gradually becoming the foundation for many artificial intelligence applications. However, understanding their internal working mechanisms has continued to puzzle researchers, which in turn limits the \u2026"}, {"title": "QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models", "link": "https://arxiv.org/pdf/2504.11038", "details": "Y Zhang, R Xie, J Chen, X Sun, Z Kang, Y Wang - arXiv preprint arXiv:2504.11038, 2025", "abstract": "In typical multimodal tasks, such as Visual Question Answering (VQA), adversarial attacks targeting a specific image and question can lead large vision-language models (LVLMs) to provide incorrect answers. However, it is common for a single \u2026"}, {"title": "R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning", "link": "https://arxiv.org/pdf/2504.11195", "details": "L Sheng, J Liang, Z Wang, R He - arXiv preprint arXiv:2504.11195, 2025", "abstract": "Vision-language models (VLMs), such as CLIP, have gained significant popularity as foundation models, with numerous fine-tuning methods developed to enhance performance on downstream tasks. However, due to their inherent vulnerability and \u2026"}, {"title": "VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering", "link": "https://arxiv.org/pdf/2504.08269", "details": "QZ Lim, CP Lee, KM Lim, KSM Anbananthen - arXiv preprint arXiv:2504.08269, 2025", "abstract": "The increasing availability of multimodal data across text, tables, and images presents new challenges for developing models capable of complex cross-modal reasoning. Existing methods for Multimodal Multi-hop Question Answering (MMQA) \u2026"}, {"title": "Beyond Standard MoE: Mixture of Latent Experts for Resource-Efficient Language Models", "link": "https://arxiv.org/pdf/2503.23100", "details": "Z Liu, H Wu, R She, X Fu, X Han, T Zhong, M Yuan - arXiv preprint arXiv:2503.23100, 2025", "abstract": "Mixture of Experts (MoE) has emerged as a pivotal architectural paradigm for efficient scaling of Large Language Models (LLMs), operating through selective activation of parameter subsets for each input token. Nevertheless, conventional MoE \u2026"}, {"title": "GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models", "link": "https://arxiv.org/pdf/2504.09696", "details": "J Zhang, C Zuo - arXiv preprint arXiv:2504.09696, 2025", "abstract": "Recent advances in R1-like reasoning models leveraging Group Relative Policy Optimization (GRPO) have significantly improved the performance of language models on mathematical reasoning tasks. However, current GRPO implementations \u2026"}]
