[{"title": "MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.17637", "details": "Z Liu, Y Zang, X Dong, P Zhang, Y Cao, H Duan, C He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs. This is typically achieved by using labeled datasets of chosen/rejected pairs and employing \u2026"}, {"title": "Self-Comparison for Dataset-Level Membership Inference in Large (Vision-) Language Models", "link": "https://arxiv.org/pdf/2410.13088", "details": "J Ren, K Chen, C Chen, V Sehwag, Y Xing, J Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have made significant advancements in a wide range of natural language processing and vision- language tasks. Access to large web-scale datasets has been a key factor in their \u2026"}, {"title": "RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models", "link": "https://arxiv.org/pdf/2411.04097", "details": "M Varma, JB Delbrouck, Z Chen, A Chaudhari\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) \u2026"}, {"title": "Large language models enabled multiagent ensemble method for efficient EHR data labeling", "link": "https://arxiv.org/pdf/2410.16543", "details": "J Huang, K Nezafati, I Villanueva-Miranda, Z Gu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study introduces a novel multiagent ensemble method powered by LLMs to address a key challenge in ML-data labeling, particularly in large-scale EHR datasets. Manual labeling of such datasets requires domain expertise and is labor \u2026"}, {"title": "Few-Shot Task Learning through Inverse Generative Modeling", "link": "https://openreview.net/pdf%3Fid%3DatIE6Npr5A", "details": "A Netanyahu, Y Du, A Bronars, J Pari, JB Tenenbaum\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Learning the intents of an agent, defined by its goals or motion style, is often extremely challenging from just a few examples. We refer to this problem as task concept learning and present our approach, Few-Shot Task Learning through \u2026"}, {"title": "Retrieval In Decoder benefits generative models for explainable complex question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007573", "details": "J Feng, Q Wang, H Qiu, L Liu - Neural Networks, 2024", "abstract": "Abstract Large-scale Language Models (LLMs) utilizing the Chain-of-Thought prompting demonstrate exceptional performance in a variety of tasks. However, the persistence of factual hallucinations remains a significant challenge in practical \u2026"}, {"title": "Improving Causal Inference of Large Language Models with SCM Tools", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9437-9_1", "details": "Z Hua, S Xing, H Jiang, C Wei, X Wang - CCF International Conference on Natural \u2026, 2024", "abstract": "Many previous studies have shown that Large Language Models (LLMs) are highly competent on many Natural Language Processing (NLP) tasks. However, a recent study showed the poor ability of LLMs to perform causal inference based on causal \u2026"}, {"title": "Interpretable Mesomorphic Neural Networks For Tabular Data", "link": "https://www.researchgate.net/profile/Arlind-Kadra/publication/385416359_Interpretable_Mesomorphic_Neural_Networks_For_Tabular_Data/links/67238fc25852dd723ca07a20/Interpretable-Mesomorphic-Neural-Networks-For-Tabular-Data.pdf", "details": "A Kadra, SP Arango, J Grabocka", "abstract": "Even though neural networks have been long deployed in applications involving tabular data, still existing neural architectures are not explainable by design. In this work, we propose a new class of interpretable neural networks for tabular data that \u2026"}, {"title": "Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning", "link": "https://arxiv.org/pdf/2410.10360", "details": "Y Xu, R Zhang, X Jiang, Y Feng, Y Xiao, X Ma, R Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Retrieval-Augmented Generation (RAG) offers an effective solution to the issues faced by Large Language Models (LLMs) in hallucination generation and knowledge obsolescence by incorporating externally retrieved knowledge. However, due to \u2026"}]
