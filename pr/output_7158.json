[{"title": "Robust image representations with counterfactual contrastive learning", "link": "https://arxiv.org/pdf/2409.10365", "details": "M Roschewitz, FDS Ribeiro, T Xia, G Khara, B Glocker - arXiv preprint arXiv \u2026, 2024", "abstract": "Contrastive pretraining can substantially increase model generalisation and downstream performance. However, the quality of the learned representations is highly dependent on the data augmentation strategy applied to generate positive \u2026"}, {"title": "Knowledge Planning in Large Language Models for Domain-Aligned Counseling Summarization", "link": "https://arxiv.org/pdf/2409.14907", "details": "A Srivastava, S Joshi, T Chakraborty, MS Akhtar - arXiv preprint arXiv:2409.14907, 2024", "abstract": "In mental health counseling, condensing dialogues into concise and relevant summaries (aka counseling notes) holds pivotal significance. Large Language Models (LLMs) exhibit remarkable capabilities in various generative tasks; however \u2026"}, {"title": "HelloBench: Evaluating Long Text Generation Capabilities of Large Language Models", "link": "https://arxiv.org/pdf/2409.16191", "details": "H Que, F Duan, L He, Y Mou, W Zhou, J Liu, W Rong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks (eg, long-context understanding), and many benchmarks have been proposed. However, we observe that long text generation capabilities are \u2026"}, {"title": "Tag Map: A Text-Based Map for Spatial Reasoning and Navigation with Large Language Models", "link": "https://arxiv.org/pdf/2409.15451", "details": "M Zhang, K Qu, V Patil, C Cadena, M Hutter - arXiv preprint arXiv:2409.15451, 2024", "abstract": "Large Language Models (LLM) have emerged as a tool for robots to generate task plans using common sense reasoning. For the LLM to generate actionable plans, scene context must be provided, often through a map. Recent works have shifted \u2026"}]
