[{"title": "Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?", "link": "https://arxiv.org/pdf/2407.17417", "details": "MA Panaitescu-Liess, Z Che, B An, Y Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in generating diverse and contextually rich text. However, concerns regarding copyright infringement arise as LLMs may inadvertently produce copyrighted material. In this \u2026"}, {"title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "link": "https://arxiv.org/pdf/2407.10817", "details": "T Vu, K Krishna, S Alzubi, C Tar, M Faruqui, YH Sung - arXiv preprint arXiv \u2026, 2024", "abstract": "As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their output due to the high costs of human evaluation. To make progress towards better LLM autoraters, we introduce FLAMe, a family of Foundational Large \u2026"}, {"title": "MLBMIKABR at \u201cDischarge Me!\u201d: Concept Based Clinical Text Description Generation", "link": "https://aclanthology.org/2024.bionlp-1.66.pdf", "details": "A Naskar, J Hocking, P Chondros, D Boyle, M Conway - Proceedings of the 23rd \u2026, 2024", "abstract": "This paper presents a method called Concept Based Description Generation, aimed at creating summaries (Brief Hospital Course and Discharge Instructions) using source (Discharge and Radiology) texts. We propose a rule-based approach for \u2026"}, {"title": "Fine-tuning Language Models for Triple Extraction with Data Augmentation", "link": "https://aclanthology.org/2024.kallm-1.12.pdf", "details": "Y Zhang, T Sadler, MR Taesiri, W Xu, M Reformat - Proceedings of the 1st Workshop \u2026, 2024", "abstract": "Advanced language models with impressive capabilities to process textual information can more effectively extract high-quality triples, which are the building blocks of knowledge graphs. Our work examines language models' abilities to extract \u2026"}, {"title": "Reinforcement Learning-Driven LLM Agent for Automated Attacks on LLMs", "link": "https://aclanthology.org/2024.privatenlp-1.17.pdf", "details": "X Wang, J Peng, K Xu, H Yao, T Chen - Proceedings of the Fifth Workshop on Privacy \u2026, 2024", "abstract": "Recently, there has been a growing focus on conducting attacks on large language models (LLMs) to assess LLMs' safety. Yet, existing attack methods face challenges, including the need to access model weights or merely ensuring LLMs output harmful \u2026"}, {"title": "Evaluating Large Language Models with fmeval", "link": "https://arxiv.org/pdf/2407.12872", "details": "P Schw\u00f6bel, L Franceschi, MB Zafar, K Vasist\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "fmeval is an open source library to evaluate large language models (LLMs) in a range of tasks. It helps practitioners evaluate their model for task performance and along multiple responsible AI dimensions. This paper presents the library and \u2026"}, {"title": "Multi-group Uncertainty Quantification for Long-form Text Generation", "link": "https://arxiv.org/pdf/2407.21057", "details": "T Liu, ZS Wu - arXiv preprint arXiv:2407.21057, 2024", "abstract": "While large language models are rapidly moving towards consumer-facing applications, they are often still prone to factual errors and hallucinations. In order to reduce the potential harms that may come from these errors, it is important for users \u2026"}, {"title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated", "link": "https://arxiv.org/pdf/2407.10969", "details": "H Wang, S Ma, R Wang, F Wei - arXiv preprint arXiv:2407.10969, 2024", "abstract": "We introduce, Q-Sparse, a simple yet effective approach to training sparsely- activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is \u2026"}, {"title": "DDK: Distilling Domain Knowledge for Efficient Large Language Models", "link": "https://arxiv.org/pdf/2407.16154", "details": "J Liu, C Zhang, J Guo, Y Zhang, H Que, K Deng, Z Bai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the advanced intelligence abilities of large language models (LLMs) in various applications, they still face significant computational and storage demands. Knowledge Distillation (KD) has emerged as an effective strategy to improve the \u2026"}]
