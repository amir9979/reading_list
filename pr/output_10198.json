[{"title": "BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models", "link": "https://arxiv.org/pdf/2411.15232", "details": "T Koleilat, H Asgariandehkordi, H Rivaz, Y Xiao - arXiv preprint arXiv:2411.15232, 2024", "abstract": "Recent advancements in vision-language models (VLMs), such as CLIP, have demonstrated substantial success in self-supervised representation learning for vision tasks. However, effectively adapting VLMs to downstream applications \u2026"}, {"title": "Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.06775", "details": "YL Lee, YH Tsai, WC Chiu - arXiv preprint arXiv:2412.06775, 2024", "abstract": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To \u2026"}, {"title": "segWCD: A new segmentation-based weak supervision neural network for building change detection", "link": "https://link.springer.com/article/10.1007/s10489-024-06003-x", "details": "Y Wu, X Zhang, X Zhao, Y Sun, T Li - Applied Intelligence, 2025", "abstract": "Manual annotation of changes in high-resolution remote sensing images is labor- intensive and limits advancements in change detection. We introduce the Segmentation-based Weakly Supervised Change Detection (segWCD) framework to \u2026"}, {"title": "Using Electronic Health Records to Evaluate Treatment Gaps and Disparities in Severe Hypertension", "link": "https://www.jacc.org/doi/full/10.1016/j.jacadv.2024.101428", "details": "Y Lu, Y Liu, C Kim, S Sussman, O Deshpande\u2026 - JACC: Advances, 2025", "abstract": "Methods We conducted a retrospective analysis using YNHHS EHR data from Connecticut's largest health care system, comprising five hospitals and an outpatient provider network. The Yale University Institutional Review Board exempted approval \u2026"}, {"title": "Lupus Alberto: A Transformer-Based Approach for SLE Information Extraction from Italian Clinical Reports", "link": "https://clic2024.ilc.cnr.it/wp-content/uploads/2024/12/59_main_long.pdf", "details": "L Lilli, L Antenucci, A Ortolan, SL Bosello\u2026 - 2024", "abstract": "Abstract Natural Language Processing (NLP) is widely used across several fields, such as in medicine, where information often originates from unstructured data sources. This creates the need for automated systems, in order to classify text and \u2026"}, {"title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models", "link": "https://arxiv.org/pdf/2412.03548", "details": "M Bigverdi, Z Luo, CY Hsieh, E Shen, D Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal language models (MLMs) still face challenges in fundamental visual perception tasks where specialized models excel. Tasks requiring reasoning about 3D structures benefit from depth estimation, and reasoning about 2D object \u2026"}, {"title": "Do Large Language Models have Shared Weaknesses in Medical Question Answering?", "link": "https://openreview.net/pdf%3Fid%3DZjQ04tsRQl", "details": "AM Bean, K Korgul, F Krones, R McCraith, A Mahdi - Advancements In Medical \u2026, 2024", "abstract": "Large language models (LLMs) have made rapid improvement on medical benchmarks, but their unreliability remains a persistent challenge for safe real-world uses. To design for the use LLMs as a category, rather than for specific models \u2026"}, {"title": "Reliability in AI-Assisted Critical Care: Assessing Large Language Model Robustness and Instruction Following for Cardiac Arrest Identification", "link": "https://openreview.net/pdf%3Fid%3DpsOWQZbI6Z", "details": "U Vurgun, S Hwang, DL Mowery - Advancements In Medical Foundation Models \u2026", "abstract": "This study systematically evaluates the performance, robustness, and instruction- following capabilities of large language models (LLMs) in identifying in-hospital cardiac arrest (IHCA) events. We assessed 51 open-source LLMs\u2014comprising 36 \u2026"}, {"title": "Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training", "link": "https://arxiv.org/pdf/2412.00440", "details": "H Wang, C Ju, W Lin, S Xiao, M Chen, Y Huang, C Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In rapidly evolving field of vision-language models (VLMs), contrastive language- image pre-training (CLIP) has made significant strides, becoming foundation for various downstream tasks. However, relying on one-to-one (image, text) contrastive \u2026"}]
