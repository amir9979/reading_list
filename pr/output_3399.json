[{"title": "HEST-1k: A Dataset for Spatial Transcriptomics and Histology Image Analysis", "link": "https://arxiv.org/pdf/2406.16192", "details": "G Jaume, P Doucet, AH Song, MY Lu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Spatial transcriptomics (ST) enables interrogating the molecular composition of tissue with ever-increasing resolution, depth, and sensitivity. However, costs, rapidly evolving technology, and lack of standards have constrained computational methods \u2026"}, {"title": "Prognostic Analysis Combining Histopathological Features and Clinical Information to Predict Colorectal Cancer Survival from Whole-Slide Images", "link": "https://link.springer.com/article/10.1007/s10620-024-08501-x", "details": "C Cai, Y Zhou, Y Jiao, L Li, J Xu - Digestive Diseases and Sciences, 2024", "abstract": "Background Colorectal cancer (CRC) is a malignant tumor within the digestive tract with both a high incidence rate and mortality. Early detection and intervention could improve patient clinical outcomes and survival. Methods This study computationally \u2026"}, {"title": "SCMIL: Sparse Context-aware Multiple Instance Learning for Predicting Cancer Survival Probability Distribution in Whole Slide Images", "link": "https://arxiv.org/pdf/2407.00664", "details": "Z Yang, H Liu, X Wang - arXiv preprint arXiv:2407.00664, 2024", "abstract": "Cancer survival prediction is a challenging task that involves analyzing of the tumor microenvironment within Whole Slide Image (WSI). Previous methods cannot effectively capture the intricate interaction features among instances within the local \u2026"}, {"title": "Mixture-of-Agents Enhances Large Language Model Capabilities", "link": "https://arxiv.org/pdf/2406.04692", "details": "J Wang, J Wang, B Athiwaratkun, C Zhang, J Zou - arXiv preprint arXiv:2406.04692, 2024", "abstract": "Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is \u2026"}, {"title": "MFC-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2406.11288", "details": "S Wang, H Lin, Z Luo, Z Ye, G Chen, J Ma - arXiv preprint arXiv:2406.11288, 2024", "abstract": "Large vision-language models (LVLMs) have significantly improved multimodal reasoning tasks, such as visual question answering and image captioning. These models embed multimodal facts within their parameters, rather than relying on \u2026"}, {"title": "Ensemble-based deep learning improves detection of invasive breast cancer in routine histopathology images", "link": "https://www.cell.com/heliyon/fulltext/S2405-8440\\(24\\)08923-0", "details": "L Solorzano, S Robertson, B Acs, J Hartman\u2026 - Heliyon, 2024", "abstract": "Accurate detection of invasive breast cancer (IC) can provide decision support to pathologists as well as improve downstream computational analyses, where detection of IC is a first step. Tissue containing IC is characterized by the presence of \u2026"}, {"title": "Training Compute-Optimal Protein Language Models", "link": "https://www.biorxiv.org/content/10.1101/2024.06.06.597716.full.pdf", "details": "X Cheng, B Chen, P Li, J Gong, J Tang, L Song - bioRxiv, 2024", "abstract": "We explore optimally training protein language models, an area of significant interest in biological research where guidance on best practices is limited. Most models are trained with extensive compute resources until performance gains plateau, focusing \u2026"}, {"title": "Merlin: A Vision Language Foundation Model for 3D Computed Tomography", "link": "https://arxiv.org/pdf/2406.06512", "details": "L Blankemeier, JP Cohen, A Kumar, D Van Veen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Over 85 million computed tomography (CT) scans are performed annually in the US, of which approximately one quarter focus on the abdomen. Given the current radiologist shortage, there is a large impetus to use artificial intelligence to alleviate \u2026"}, {"title": "Large Language Model-guided Document Selection", "link": "https://arxiv.org/pdf/2406.04638", "details": "X Kong, T Gunter, R Pang - arXiv preprint arXiv:2406.04638, 2024", "abstract": "Large Language Model (LLM) pre-training exhausts an ever growing compute budget, yet recent research has demonstrated that careful document selection enables comparable model quality with only a fraction of the FLOPs. Inspired by \u2026"}]
