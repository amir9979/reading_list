[{"title": "GenieBlue: Integrating both Linguistic and Multimodal Capabilities for Large Language Models on Mobile Devices", "link": "https://arxiv.org/pdf/2503.06019", "details": "X Lu, Y Chen, R Wu, H Gao, X Chen, X Yang, X Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled their deployment on mobile devices. However, challenges persist in maintaining strong language capabilities and ensuring hardware compatibility, both \u2026"}, {"title": "Adding Alignment Control to Language Models", "link": "https://arxiv.org/pdf/2503.04346", "details": "W Zhu, W Zhang, R Wang - arXiv preprint arXiv:2503.04346, 2025", "abstract": "Post-training alignment has increasingly become a crucial factor in enhancing the usability of language models (LMs). However, the strength of alignment varies depending on individual preferences. This paper proposes a method to incorporate \u2026"}, {"title": "Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation", "link": "https://arxiv.org/pdf/2502.19414", "details": "S Sinha, S Goel, P Kumaraguru, J Geiping, M Bethge\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "There is growing excitement about the potential of Language Models (LMs) to accelerate scientific discovery. Falsifying hypotheses is key to scientific progress, as it allows claims to be iteratively refined over time. This process requires significant \u2026"}, {"title": "ARMOR v0. 1: Empowering Autoregressive Multimodal Understanding Model with Interleaved Multimodal Generation via Asymmetric Synergy", "link": "https://arxiv.org/pdf/2503.06542", "details": "J Sun, Y Feng, C Li, F Zhang, Z Li, J Ai, S Zhou, Y Dai\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation \u2026"}, {"title": "IDEA Prune: An Integrated Enlarge-and-Prune Pipeline in Generative Language Model Pretraining", "link": "https://arxiv.org/pdf/2503.05920", "details": "Y Li, X Du, A Jaiswal, T Lei, T Zhao, C Wang, J Wang - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in large language models have intensified the need for efficient and deployable models within limited inference budgets. Structured pruning pipelines have shown promise in token efficiency compared to training target-size \u2026"}, {"title": "Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment", "link": "https://arxiv.org/pdf/2503.04647", "details": "W Yang, J Wu, C Wang, C Zong, J Zhang - arXiv preprint arXiv:2503.04647, 2025", "abstract": "Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences. While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is \u2026"}, {"title": "A Survey on Post-training of Large Language Models", "link": "https://arxiv.org/pdf/2503.06072", "details": "G Tie, Z Zhao, D Song, F Wei, R Zhou, Y Dai, W Yin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained \u2026"}, {"title": "DiffPO: Diffusion-styled Preference Optimization for Efficient Inference-Time Alignment of Large Language Models", "link": "https://arxiv.org/pdf/2503.04240", "details": "R Chen, W Chai, Z Yang, X Zhang, JT Zhou, T Quek\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Inference-time alignment provides an efficient alternative for aligning LLMs with humans. However, these approaches still face challenges, such as limited scalability due to policy-specific value functions and latency during the inference phase. In this \u2026"}, {"title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2503.06749", "details": "W Huang, B Jia, Z Zhai, S Cao, Z Ye, F Zhao, Y Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning \u2026"}]
