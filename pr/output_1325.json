'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Detecting Edited Knowledge in Language Models](https:/'
[{"title": "Recall Them All: Retrieval-Augmented Language Models for Long Object List Extraction from Long Documents", "link": "https://arxiv.org/pdf/2405.02732", "details": "S Singhania, S Razniewski, G Weikum - arXiv preprint arXiv:2405.02732, 2024", "abstract": "Methods for relation extraction from text mostly focus on high precision, at the cost of limited recall. High recall is crucial, though, to populate long lists of object entities that stand in a specific relation with a given subject. Cues for relevant objects can be \u2026"}, {"title": "Soft Preference Optimization: Aligning Language Models to Expert Distributions", "link": "https://arxiv.org/pdf/2405.00747", "details": "A Sharifnassab, S Ghiassian, S Salehkaleybar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We propose Soft Preference Optimization (SPO), a method for aligning generative models, such as Large Language Models (LLMs), with human preferences, without the need for a reward model. SPO optimizes model outputs directly over a preference \u2026"}, {"title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2405.02659", "details": "T Zhang, D Li, Q Chen, C Wang, L Huang, H Xue, X He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Retrieval-augmented large language models (LLMs) leverage relevant content retrieved by information retrieval systems to generate correct responses, aiming to alleviate the hallucination problem. However, existing retriever-responder methods \u2026"}, {"title": "A Framework for Real-time Safeguarding the Text Generation of Large Language", "link": "https://arxiv.org/pdf/2404.19048", "details": "X Dong, D Lin, S Wang, AE Hassan - arXiv preprint arXiv:2404.19048, 2024", "abstract": "Large Language Models (LLMs) have significantly advanced natural language processing (NLP) tasks but also pose ethical and societal risks due to their propensity to generate harmful content. To address this, various approaches have \u2026"}, {"title": "Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense", "link": "https://arxiv.org/pdf/2405.04655", "details": "S Shen, L Logeswaran, M Lee, H Lee, S Poria\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated substantial commonsense understanding through numerous benchmark evaluations. However, their understanding of cultural commonsense remains largely unexamined. In this paper \u2026"}, {"title": "BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models", "link": "https://arxiv.org/pdf/2405.04756", "details": "CF Luo, A Ghawanmeh, X Zhu, FK Khattak - arXiv preprint arXiv:2405.04756, 2024", "abstract": "Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly. The language model can also \u2026"}, {"title": "Markovian Agents for Truthful Language Modeling", "link": "https://arxiv.org/pdf/2404.18988", "details": "S Viteri, M Lamparth, P Chatain, C Barrett - arXiv preprint arXiv:2404.18988, 2024", "abstract": "Chain-of-Thought (CoT) reasoning could in principle enable a deeper understanding of a language model's (LM) internal reasoning. However, prior work suggests that some LMs answer questions similarly despite changes in their CoT, suggesting that \u2026"}, {"title": "CodeGRAG: Extracting Composed Syntax Graphs for Retrieval Augmented Cross-Lingual Code Generation", "link": "https://arxiv.org/pdf/2405.02355", "details": "K Du, R Rui, H Chai, L Fu, W Xia, Y Wang, R Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Utilizing large language models to generate codes has shown promising meaning in software development revolution. Despite the intelligence shown by the general large language models, their specificity in code generation can still be improved due \u2026"}, {"title": "Multi-hop Question Answering over Knowledge Graphs using Large Language Models", "link": "https://arxiv.org/pdf/2404.19234", "details": "A Chakraborty - arXiv preprint arXiv:2404.19234, 2024", "abstract": "Knowledge graphs (KGs) are large datasets with specific structures representing large knowledge bases (KB) where each node represents a key entity and relations amongst them are typed edges. Natural language queries formed to extract \u2026"}]
