[{"title": "Process-based self-rewarding language models", "link": "https://arxiv.org/pdf/2503.03746", "details": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' \u2026"}, {"title": "Training a Utility-based Retriever Through Shared Context Attribution for Retrieval-Augmented Language Models", "link": "https://arxiv.org/pdf/2504.00573", "details": "Y Xu, J Gao, X Yu, Y Xue, B Bi, H Shen, X Cheng - arXiv preprint arXiv:2504.00573, 2025", "abstract": "Retrieval-Augmented Language Models boost task performance, owing to the retriever that provides external knowledge. Although crucial, the retriever primarily focuses on semantics relevance, which may not always be effective for generation \u2026"}, {"title": "LaViC: Adapting Large Vision-Language Models to Visually-Aware Conversational Recommendation", "link": "https://arxiv.org/pdf/2503.23312", "details": "H Jeon, S Koide, Y Wang, Z He, J McAuley - arXiv preprint arXiv:2503.23312, 2025", "abstract": "Conversational recommender systems engage users in dialogues to refine their needs and provide more personalized suggestions. Although textual information suffices for many domains, visually driven categories such as fashion or home decor \u2026"}, {"title": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection", "link": "https://arxiv.org/pdf/2504.00695", "details": "X Zhu, Z Gu, S Zheng, T Wang, T Li, H Feng, Y Xiao - arXiv preprint arXiv:2504.00695, 2025", "abstract": "Pre-training large language models (LLMs) necessitates enormous diverse textual corpora, making effective data selection a key challenge for balancing computational resources and model performance. Current methodologies primarily emphasize data \u2026"}, {"title": "When Debate Fails: Bias Reinforcement in Large Language Models", "link": "https://arxiv.org/pdf/2503.16814", "details": "J Oh, M Jeong, J Ko, SY Yun - arXiv preprint arXiv:2503.16814, 2025", "abstract": "Large Language Models $($ LLMs $) $ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self \u2026"}, {"title": "Agentic Large Language Models, a survey", "link": "https://arxiv.org/pdf/2503.23037", "details": "A Plaat, M van Duijn, N van Stein, M Preuss\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "There is great interest in agentic LLMs, large language models that act as agents. We review the growing body of work in this area and provide a research agenda. Agentic LLMs are LLMs that (1) reason,(2) act, and (3) interact. We organize the \u2026"}, {"title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization", "link": "https://arxiv.org/pdf/2503.23733", "details": "Y Du, X Wang, C Chen, J Ye, Y Wang, P Li, M Yan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous \u2026"}, {"title": "Ethereum Price Prediction Employing Large Language Models for Short-term and Few-shot Forecasting", "link": "https://arxiv.org/pdf/2503.23190", "details": "E Makri, G Palaiokrassas, S Bouraga\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Cryptocurrencies have transformed financial markets with their innovative blockchain technology and volatile price movements, presenting both challenges and opportunities for predictive analytics. Ethereum, being one of the leading \u2026"}, {"title": "OUTLIER-AWARE PREFERENCE OPTIMIZATION FOR LARGE LANGUAGE MODELS", "link": "https://openreview.net/pdf%3Fid%3DYevRFGa9I7", "details": "P Srivastava, SS Nalli, A Deshpande, A Sharma - \u2026 in Foundation Models: The Next Frontier in \u2026", "abstract": "Aligning large language models (LLMs) to user preferences often relies on learning a reward model as a proxy from feedback. However, such reward models can fail on out-of-distribution examples and, if kept static, may reinforce incorrect preferences \u2026"}]
