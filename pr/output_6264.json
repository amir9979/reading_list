[{"title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.06663", "details": "K Sun, M Dredze - arXiv preprint arXiv:2408.06663, 2024", "abstract": "The development of large language models leads to the formation of a pre-train-then- align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream \u2026"}, {"title": "Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination", "link": "https://aclanthology.org/2024.acl-long.490.pdf", "details": "N Yang, T Kang, SJ Choi, H Lee, K Jung - Proceedings of the 62nd Annual Meeting of \u2026, 2024", "abstract": "Instruction-following language models often show undesirable biases. These undesirable biases may be accelerated in the real-world usage of language models, where a wide range of instructions is used through zero-shot example prompting. To \u2026"}, {"title": "Language Models Don't Learn the Physical Manifestation of Language", "link": "https://aclanthology.org/2024.acl-long.195.pdf", "details": "B Lee, J Lim - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "We argue that language-only models don't learn the physical manifestation of language. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-Test. These tasks highlight a \u2026"}, {"title": "LM-PUB-QUIZ: A Comprehensive Framework for Zero-Shot Evaluation of Relational Knowledge in Language Models", "link": "https://arxiv.org/pdf/2408.15729", "details": "M Ploner, J Wiland, S Pohl, A Akbik - arXiv preprint arXiv:2408.15729, 2024", "abstract": "Knowledge probing evaluates the extent to which a language model (LM) has acquired relational knowledge during its pre-training phase. It provides a cost- effective means of comparing LMs of different sizes and training setups and is useful \u2026"}, {"title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models", "link": "https://arxiv.org/pdf/2408.15313", "details": "W Zhang, PHS Torr, M Elhoseiny, A Bibi - arXiv preprint arXiv:2408.15313, 2024", "abstract": "Fine-tuning large language models (LLMs) on human preferences, typically through reinforcement learning from human feedback (RLHF), has proven successful in enhancing their capabilities. However, ensuring the safety of LLMs during the fine \u2026"}, {"title": "Path-Consistency: Prefix Enhancement for Efficient Inference in LLM", "link": "https://arxiv.org/pdf/2409.01281", "details": "J Zhu, Y Shen, J Zhao, A Zou - arXiv preprint arXiv:2409.01281, 2024", "abstract": "To enhance the reasoning capabilities of large language models (LLMs), self- consistency has gained significant popularity by combining multiple sampling with majority voting. However, the state-of-the-art self-consistency approaches consume \u2026"}, {"title": "Learning Global Controller in Latent Space for Parameter-Efficient Fine-Tuning", "link": "https://aclanthology.org/2024.acl-long.222.pdf", "details": "Z Tan, Y Shen, X Cheng, C Zong, W Zhang, J Shao\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "While large language models (LLMs) have showcased remarkable prowess in various natural language processing tasks, their training costs are exorbitant. Consequently, a plethora of parameter-efficient fine-tuning methods have emerged \u2026"}, {"title": "Exploring Reversal Mathematical Reasoning Ability for Large Language Models", "link": "https://aclanthology.org/2024.findings-acl.811.pdf", "details": "P Guo, W You, J Li, Y Bowen, M Zhang - Findings of the Association for \u2026, 2024", "abstract": "Large language models (LLMs) have presented remarkable capabilities in the wide range of natural language understanding and reasoning tasks. Despite their success, a few works indicate that LLMs suffer from the \u201creversal curse\u201d, in which \u2026"}, {"title": "Using Natural Language Explanations to Improve Robustness of In-context Learning", "link": "https://aclanthology.org/2024.acl-long.728.pdf", "details": "X He, Y Wu, OM Camburu, P Minervini, P Stenetorp - \u2026 of the 62nd Annual Meeting of \u2026, 2024", "abstract": "Recent studies demonstrated that large language models (LLMs) can excel in many tasks via in-context learning (ICL). However, recentworks show that ICL-prompted models tend to produce inaccurate results when presented with adversarial inputs. In \u2026"}]
