[{"title": "Evaluating the Meta-and Object-Level Reasoning of Large Language Models for Question Answering", "link": "https://arxiv.org/pdf/2502.10338", "details": "N Ferguson, L Guillou, A Bundy, K Nuamah - arXiv preprint arXiv:2502.10338, 2025", "abstract": "Large Language Models (LLMs) excel in natural language tasks but still face challenges in Question Answering (QA) tasks requiring complex, multi-step reasoning. We outline the types of reasoning required in some of these tasks, and \u2026"}, {"title": "Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs", "link": "https://arxiv.org/pdf/2502.10858", "details": "Z Wu, T Li, J Yang, M Zhan, X Zhu, L Feng - arXiv preprint arXiv:2502.10858, 2025", "abstract": "Deep iterative chain-of-thought (CoT) reasoning enables LLMs to tackle complex tasks by progressively activating relevant pre-trained knowledge. However, it faces challenges in ensuring continual improvement and determining a stopping criterion \u2026"}, {"title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models", "link": "https://arxiv.org/pdf/2502.10743", "details": "S Liu, H Wu, B He, Z Liu, X Han, M Yuan, L Song - arXiv preprint arXiv:2502.10743, 2025", "abstract": "Recent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they \u2026"}, {"title": "Tool Learning in the Wild: Empowering Language Models as Automatic Tool Agents", "link": "https://openreview.net/pdf%3Fid%3DT4wMdeFEjX", "details": "Z Shi, S Gao, L Yan, Y Feng, X Chen, Z Chen, D Yin\u2026 - THE WEB CONFERENCE 2025", "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a promising approach to extend their utility, enabling them to solve practical tasks. Previous methods manually parse tool documentation and create in-context \u2026"}, {"title": "Harnessing Language's Fractal Geometry with Recursive Inference Scaling", "link": "https://arxiv.org/pdf/2502.07503", "details": "I Alabdulmohsin, X Zhai - arXiv preprint arXiv:2502.07503, 2025", "abstract": "Recent research in language modeling reveals two scaling effects: the well-known improvement from increased training compute, and a lesser-known boost from applying more sophisticated or computationally intensive inference methods \u2026"}, {"title": "Injury degree appraisal of large language model based on retrieval-augmented generation and deep learning", "link": "https://www.sciencedirect.com/science/article/pii/S0160252725000032", "details": "F Zhang, Y Luo, Z Gao, A Han - International Journal of Law and Psychiatry, 2025", "abstract": "Abstract Large Language Models (LLMs) have shown impressive performance in various natural language processing tasks. However, their application in specialized domains like forensic injury appraisal remains challenging due to the lack of domain \u2026"}, {"title": "Personalization Toolkit: Training Free Personalization of Large Vision Language Models", "link": "https://arxiv.org/pdf/2502.02452%3F", "details": "S Seifi, V Dorovatas, DO Reino, R Aljundi - arXiv preprint arXiv:2502.02452, 2025", "abstract": "Large Vision Language Models (LVLMs) have significant potential to deliver personalized assistance by adapting to individual users' unique needs and preferences. Personalization of LVLMs is an emerging area that involves \u2026"}, {"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models", "link": "https://arxiv.org/pdf/2502.03199%3F", "details": "J Wu, Y Shen, S Liu, Y Tang, S Song, X Wang, L Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the \u2026"}, {"title": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques", "link": "https://arxiv.org/pdf/2501.14492", "details": "Z Tang, Z Li, Z Xiao, T Ding, R Sun, B Wang, D Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Critiques are important for enhancing the performance of Large Language Models (LLMs), enabling both self-improvement and constructive feedback for others by identifying flaws and suggesting improvements. However, evaluating the critique \u2026"}]
