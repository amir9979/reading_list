[{"title": "Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models", "link": "https://arxiv.org/pdf/2501.18533%3F", "details": "Y Ding, L Li, B Cao, J Shao - arXiv preprint arXiv:2501.18533, 2025", "abstract": "Large Vision-Language Models (VLMs) have achieved remarkable performance across a wide range of tasks. However, their deployment in safety-critical domains poses significant challenges. Existing safety fine-tuning methods, which focus on \u2026"}, {"title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features", "link": "https://arxiv.org/pdf/2502.14786", "details": "M Tschannen, A Gritsenko, X Wang, MF Naeem\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed \u2026"}, {"title": "ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model", "link": "https://arxiv.org/pdf/2502.14420", "details": "Z Zhou, Y Zhu, M Zhu, J Wen, N Liu, Z Xu, W Meng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world. Why can't large language models replicate this holistic understanding? Through a systematic analysis of existing training paradigms in \u2026"}, {"title": "On the Query Complexity of Verifier-Assisted Language Generation", "link": "https://arxiv.org/pdf/2502.12123", "details": "E Botta, Y Li, A Mehta, JT Ash, C Zhang, A Risteski - arXiv preprint arXiv:2502.12123, 2025", "abstract": "Recently, a plethora of works have proposed inference-time algorithms (eg best-of- n), which incorporate verifiers to assist the generation process. Their quality- efficiency trade-offs have been empirically benchmarked on a variety of constrained \u2026"}, {"title": "Explicitly unbiased large language models still form biased associations", "link": "https://www.pnas.org/doi/full/10.1073/pnas.2416228122", "details": "X Bai, A Wang, I Sucholutsky, TL Griffiths - Proceedings of the National Academy of \u2026, 2025", "abstract": "Large language models (LLMs) can pass explicit social bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: As LLMs become \u2026"}, {"title": "Analyzing patient perspectives with large language models: a cross-sectional study of sentiment and thematic classification on exception from informed consent", "link": "https://www.nature.com/articles/s41598-025-89996-w", "details": "AE Kornblith, C Singh, JC Innes, TP Chang\u2026 - Scientific Reports, 2025", "abstract": "Large language models (LLMs) can improve text analysis efficiency in healthcare. This study explores the application of LLMs to analyze patient perspectives within the exception from informed consent (EFIC) process, which waives consent in \u2026"}]
