[{"title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment", "link": "https://arxiv.org/pdf/2405.19332", "details": "S Zhang, D Yu, H Sharma, Z Yang, S Wang, H Hassan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed \u2026"}, {"title": "Calibrating Reasoning in Language Models with Internal Consistency", "link": "https://arxiv.org/pdf/2405.18711", "details": "Z Xie, J Guo, T Yu, S Li - arXiv preprint arXiv:2405.18711, 2024", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought (CoT) prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious \u2026"}, {"title": "A Philosophical Introduction to Language Models-Part II: The Way Forward", "link": "https://arxiv.org/pdf/2405.03207", "details": "R Milli\u00e8re, C Buckner - arXiv preprint arXiv:2405.03207, 2024", "abstract": "In this paper, the second of two companion pieces, we explore novel philosophical questions raised by recent progress in large language models (LLMs) that go beyond the classical debates covered in the first part. We focus particularly on issues \u2026"}, {"title": "Using Pre-Trained Language Models in an End-to-End Pipeline for Antithesis Detection", "link": "https://aclanthology.org/2024.lrec-main.1502.pdf", "details": "R K\u00fchn, K Saadi, J Mitrovi\u0107, M Granitzer - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Rhetorical figures play an important role in influencing readers and listeners. Some of these word constructs that deviate from the usual language structure are known to be persuasive\u2013antithesis is one of them. This figure combines parallel phrases with \u2026"}, {"title": "One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments", "link": "https://arxiv.org/pdf/2405.20202", "details": "K Yi, Y Xu, H Chang, C Tang, Y Meng, T Zhang, J Li - arXiv preprint arXiv:2405.20202, 2024", "abstract": "Large Language Models (LLMs) have advanced rapidly but face significant memory demands. While quantization has shown promise for LLMs, current methods typically require lengthy training to alleviate the performance degradation from quantization \u2026"}, {"title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models", "link": "https://arxiv.org/pdf/2405.16057", "details": "X Lu, A Zhou, Y Xu, R Zhang, P Gao, H Li - arXiv preprint arXiv:2405.16057, 2024", "abstract": "Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine- tuning and deployment. Current post-training pruning methods, while reducing the \u2026"}, {"title": "Kestrel: Point Grounding Multimodal LLM for Part-Aware 3D Vision-Language Understanding", "link": "https://arxiv.org/pdf/2405.18937", "details": "J Fei, M Ahmed, J Ding, EM Bakr, M Elhoseiny - arXiv preprint arXiv:2405.18937, 2024", "abstract": "While 3D MLLMs have achieved significant progress, they are restricted to object and scene understanding and struggle to understand 3D spatial structures at the part level. In this paper, we introduce Kestrel, representing a novel approach that \u2026"}, {"title": "Hyperbolic Pre-Trained Language Model", "link": "https://ieeexplore.ieee.org/abstract/document/10542420/", "details": "W Chen, X Han, Y Lin, K He, R Xie, J Zhou, Z Liu\u2026 - IEEE/ACM Transactions on \u2026, 2024", "abstract": "In recent years, we have witnessed significant improvements in pre-trained language models (PLM) brought about by the scaling of parameter sizes and data amounts. However, this also brings high computational and storage costs. In this paper, we \u2026"}, {"title": "TEII: Think, Explain, Interact and Iterate with Large Language Models to Solve Cross-lingual Emotion Detection", "link": "https://arxiv.org/pdf/2405.17129", "details": "L Cheng, Q Shao, C Zhao, S Bi, GA Levow - arXiv preprint arXiv:2405.17129, 2024", "abstract": "Cross-lingual emotion detection allows us to analyze global trends, public opinion, and social phenomena at scale. We participated in the Explainability of Cross-lingual Emotion Detection (EXALT) shared task, achieving an F1-score of 0.6046 on the \u2026"}]
