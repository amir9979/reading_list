'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [RIFF: Learning to Rephrase Inputs for Few-shot Fine-t'
[{"title": "Anatomical Structure-Guided Medical Vision-Language Pre-training", "link": "https://arxiv.org/html/2403.09294v1", "details": "Q Li, X Yan, J Xu, R Yuan, Y Zhang, R Feng, Q Shen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Learning medical visual representations through vision-language pre-training has reached remarkable progress. Despite the promising performance, it still faces challenges, ie, local alignment lacks interpretability and clinical relevance, and the \u2026"}, {"title": "Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods", "link": "https://arxiv.org/pdf/2403.00998", "details": "P Tsvilodub, H Wang, S Grosch, M Franke - arXiv preprint arXiv:2403.00998, 2024", "abstract": "This paper systematically compares different methods of deriving item-level predictions of language models for multiple-choice tasks. It compares scoring methods for answer options based on free generation of responses, various \u2026"}, {"title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models", "link": "https://arxiv.org/pdf/2403.08281", "details": "N Ding, Y Chen, G Cui, X Lv, R Xie, B Zhou, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three \u2026"}, {"title": "$\\texttt {COSMIC} $: Mutual Information for Task-Agnostic Summarization Evaluation", "link": "https://arxiv.org/pdf/2402.19457", "details": "M Darrin, P Formont, JCK Cheung, P Piantanida - arXiv preprint arXiv:2402.19457, 2024", "abstract": "Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks \u2026"}, {"title": "MagicClay: Sculpting Meshes With Generative Neural Fields", "link": "https://arxiv.org/pdf/2403.02460", "details": "A Barda, VG Kim, N Aigerman, AH Bermano, T Groueix - arXiv preprint arXiv \u2026, 2024", "abstract": "The recent developments in neural fields have brought phenomenal capabilities to the field of shape generation, but they lack crucial properties, such as incremental control-a fundamental requirement for artistic work. Triangular meshes, on the other \u2026"}, {"title": "KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models", "link": "https://arxiv.org/html/2403.07350v1", "details": "H Huang, H Zhong, Q Liu, S Wu, L Wang, T Tan - arXiv preprint arXiv:2403.07350, 2024", "abstract": "Currently, little research has been done on knowledge editing for Large Vision- Language Models (LVLMs). Editing LVLMs faces the challenge of effectively integrating diverse modalities (image and text) while ensuring coherent and \u2026"}, {"title": "Language models scale reliably with over-training and on downstream tasks", "link": "https://arxiv.org/pdf/2403.08540", "details": "SY Gadre, G Smyrnis, V Shankar, S Gururangan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute \u2026"}, {"title": "Generative Models for Complex Logical Reasoning over Knowledge Graphs", "link": "https://dl.acm.org/doi/pdf/10.1145/3616855.3635804", "details": "Y Liu, Y Cao, S Wang, Q Wang, G Bi - Proceedings of the 17th ACM International \u2026, 2024", "abstract": "Answering complex logical queries over knowledge graphs (KGs) is a fundamental yet challenging task. Recently, query representation has been a mainstream approach to complex logical reasoning, making the target answer and query closer \u2026"}, {"title": "Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Matching Framework", "link": "https://arxiv.org/html/2403.07636v1", "details": "MH Phan, Y Xie, Y Qi, L Liu, L Liu, B Zhang, Z Liao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical vision language pre-training (VLP) has emerged as a frontier of research, enabling zero-shot pathological recognition by comparing the query image with the textual descriptions for each disease. Due to the complex semantics of biomedical \u2026"}]
