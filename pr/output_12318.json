[{"title": "Language Models Encode the Value of Numbers Linearly", "link": "https://aclanthology.org/2025.coling-main.47.pdf", "details": "F Zhu, D Dai, Z Sui - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Large language models (LLMs) have exhibited impressive competence in various tasks, but their internal mechanisms on mathematical problems are still under- explored. In this paper, we study a fundamental question: how language models \u2026"}, {"title": "Factual Knowledge Assessment of Language Models Using Distractors", "link": "https://aclanthology.org/2025.coling-main.537.pdf", "details": "HA Khodja, F Bechet, Q Brabant, A Nasr, G Lecorv\u00e9 - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract Language models encode extensive factual knowledge within their parameters. The accurate assessment of this knowledge is crucial for understanding and improving these models. In the literature, factual knowledge assessment often \u2026"}, {"title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers", "link": "https://arxiv.org/pdf/2501.16961", "details": "M Raza, N Milic-Frayling - arXiv preprint arXiv:2501.16961, 2025", "abstract": "Robustness of reasoning remains a significant challenge for large language models, and addressing it is essential for the practical applicability of AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a novel approach that \u2026"}, {"title": "Global Semantic-Guided Sub-image Feature Weight Allocation in High-Resolution Large Vision-Language Models", "link": "https://arxiv.org/pdf/2501.14276", "details": "Y Liang, X Li, X Chen, H Chen, Y Zheng, C Lai, B Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As the demand for high-resolution image processing in Large Vision-Language Models (LVLMs) grows, sub-image partitioning has become a popular approach for mitigating visual information loss associated with fixed-resolution processing \u2026"}, {"title": "Advancing Math Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages", "link": "https://arxiv.org/pdf/2501.14002", "details": "Z Chen, T Liu, M Tian, Q Tong, W Luo, Z Liu - arXiv preprint arXiv:2501.14002, 2025", "abstract": "Advancements in LLMs have significantly expanded their capabilities across various domains. However, mathematical reasoning remains a challenging area, prompting the development of math-specific LLMs. These models typically follow a two-stage \u2026"}, {"title": "MME-Industry: A Cross-Industry Multimodal Evaluation Benchmark", "link": "https://arxiv.org/pdf/2501.16688", "details": "D Yi, G Zhu, C Ding, Z Li, D Yi, J Wang - arXiv preprint arXiv:2501.16688, 2025", "abstract": "With the rapid advancement of Multimodal Large Language Models (MLLMs), numerous evaluation benchmarks have emerged. However, comprehensive assessments of their performance across diverse industrial applications remain \u2026"}, {"title": "CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs", "link": "https://arxiv.org/pdf/2501.16629", "details": "J Fu, S Huangfu, H Fei, X Shen, B Hooi, X Qiu, SK Ng - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal Large Language Models (MLLMs) still struggle with hallucinations despite their impressive capabilities. Recent studies have attempted to mitigate this by applying Direct Preference Optimization (DPO) to multimodal scenarios using \u2026"}, {"title": "OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas", "link": "https://arxiv.org/pdf/2501.15427", "details": "X Wang, H Zhang, T Ge, W Yu, D Yu, D Yu - arXiv preprint arXiv:2501.15427, 2025", "abstract": "Customizable role-playing in large language models (LLMs), also known as character generalization, is gaining increasing attention for its versatility and cost- efficiency in developing and deploying role-playing dialogue agents. This study \u2026"}, {"title": "Redundancy Principles for MLLMs Benchmarks", "link": "https://arxiv.org/pdf/2501.13953", "details": "Z Zhang, X Zhao, X Fang, C Li, X Liu, X Min, H Duan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the rapid iteration of Multi-modality Large Language Models (MLLMs) and the evolving demands of the field, the number of benchmarks produced annually has surged into the hundreds. The rapid growth has inevitably led to significant \u2026"}]
