[{"title": "Language Models Don't Learn the Physical Manifestation of Language", "link": "https://aclanthology.org/2024.acl-long.195.pdf", "details": "B Lee, J Lim - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "We argue that language-only models don't learn the physical manifestation of language. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-Test. These tasks highlight a \u2026"}, {"title": "Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability", "link": "https://arxiv.org/pdf/2408.07852", "details": "J Hron, L Culp, G Elsayed, R Liu, B Adlam, M Bileschi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While many capabilities of language models (LMs) improve with increased training budget, the influence of scale on hallucinations is not yet fully understood. Hallucinations come in many forms, and there is no universally accepted definition \u2026"}, {"title": "DOSSIER: Fact checking in electronic health records while preserving patient privacy", "link": "https://www.amazon.science/publications/dossier-fact-checking-in-electronic-health-records-while-preserving-patient-privacy", "details": "H Zhang, S Nagesh, M Shyani, N Mishra - 2024", "abstract": "Given a particular claim about a specific document, the fact checking problem is to determine if the claim is true and, if so, provide corroborating evidence. The problem is motivated by contexts where a document is too lengthy to quickly read and find an \u2026"}, {"title": "Disease characteristics influence the privacy calculus to adopt electronic health records: A survey study in Germany", "link": "https://journals.sagepub.com/doi/pdf/10.1177/20552076241274245", "details": "N von Kalckreuth, MA Feufel - DIGITAL HEALTH, 2024", "abstract": "Background The electronic health record (EHR) is integral to improving healthcare efficiency and quality. Its successful implementation hinges on patient willingness to use it, particularly in Germany where concerns about data security and privacy \u2026"}, {"title": "Boosting entity recognition by leveraging cross-task domain models for weak supervision", "link": "https://www.amazon.science/publications/boosting-entity-recognition-by-leveraging-cross-task-domain-models-for-weak-supervision", "details": "S Agrawal, S Merugu, V Sembium - 2024", "abstract": "Entity Recognition (ER) is a common natural language processing task encountered in a number of real-world applications. For common domains and named entities such as places and organisations, there exists sufficient high quality annotated data \u2026"}, {"title": "Pre-training data selection for biomedical domain adaptation using journal impact metrics", "link": "https://arxiv.org/pdf/2409.02725", "details": "M La\u00ef-king, P Paroubek - arXiv preprint arXiv:2409.02725, 2024", "abstract": "Domain adaptation is a widely used method in natural language processing (NLP) to improve the performance of a language model within a specific domain. This method is particularly common in the biomedical domain, which sees regular publication of \u2026"}, {"title": "Large Language Models versus Classical Machine Learning: Performance in COVID-19 Mortality Prediction Using High-Dimensional Tabular Data", "link": "https://arxiv.org/pdf/2409.02136", "details": "M Ghaffarzadeh-Esfahani, M Ghaffarzadeh-Esfahani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Background: This study aimed to evaluate and compare the performance of classical machine learning models (CMLs) and large language models (LLMs) in predicting mortality associated with COVID-19 by utilizing a high-dimensional tabular dataset \u2026"}, {"title": "XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models", "link": "https://aclanthology.org/2024.bionlp-1.35.pdf", "details": "OC Thawakar, AM Shaker, SS Mullappilly, H Cholakkal\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "The latest breakthroughs in large language models (LLMs) and vision-language models (VLMs) have showcased promising capabilities toward performing a wide range of tasks. Such models are typically trained on massive datasets comprising \u2026"}, {"title": "The advantages of context specific language models: the case of the Erasmian Language Model", "link": "https://arxiv.org/pdf/2408.06931", "details": "J Gon\u00e7alves, N Jelicic, M Murgia, E Stamhuis - arXiv preprint arXiv:2408.06931, 2024", "abstract": "The current trend to improve language model performance seems to be based on scaling up with the number of parameters (eg the state of the art GPT4 model has approximately 1.7 trillion parameters) or the amount of training data fed into the \u2026"}]
