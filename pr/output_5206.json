[{"title": "FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt Optimization Across Tokenizers", "link": "https://arxiv.org/pdf/2408.04816", "details": "JN Williams, JZ Kolter - arXiv preprint arXiv:2408.04816, 2024", "abstract": "The widespread use of large language models has resulted in a multitude of tokenizers and embedding spaces, making knowledge transfer in prompt discovery tasks difficult. In this work, we propose FUSE (Flexible Unification of Semantic \u2026"}, {"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "link": "https://arxiv.org/pdf/2408.00724", "details": "Y Wu, Z Sun, S Li, S Welleck, Y Yang - arXiv preprint arXiv:2408.00724, 2024", "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth \u2026"}, {"title": "Steering Language Models with Game-Theoretic Solvers", "link": "https://openreview.net/pdf%3Fid%3D5QLtIodDmu", "details": "I Gemp, R Patel, Y Bachrach, M Lanctot, V Dasagi\u2026 - Agentic Markets Workshop at ICML \u2026", "abstract": "Mathematical models of strategic interactions among rational agents have long been studied in game theory. However the interactions studied are often over a small set of discrete actions which is very different from how humans communicate in natural \u2026"}, {"title": "Position Paper: Dual-System Language Models via Next-Action Prediction", "link": "https://openreview.net/pdf%3Fid%3D9ZVfz8DGC8", "details": "Z Du, WJ Su - ICML 2024 Workshop on LLMs and Cognition", "abstract": "In current Large Language Model (LLM) practices, each token is appended sequentially to the output. In contrast, humans are capable of revising and correcting what we write. Inspired by this gap, in this position paper, we propose a dual-system \u2026"}, {"title": "Boosting Large Language Models with Socratic Method for Conversational Mathematics Teaching", "link": "https://arxiv.org/pdf/2407.17349", "details": "Y Ding, H Hu, J Zhou, Q Chen, B Jiang, L He - arXiv preprint arXiv:2407.17349, 2024", "abstract": "With the introduction of large language models (LLMs), automatic math reasoning has seen tremendous success. However, current methods primarily focus on providing solutions or using techniques like Chain-of-Thought to enhance problem \u2026"}, {"title": "Order-Agnostic Data Augmentation for Few-Shot Named Entity Recognition", "link": "https://aclanthology.org/2024.acl-long.421/", "details": "H Wang, L Cheng, W Zhang, L Bing - Proceedings of the 62nd Annual Meeting of \u2026, 2024", "abstract": "Data augmentation (DA) methods have been proven to be effective for pre-trained language models (PLMs) in low-resource settings, including few-shot named entity recognition (NER). However, existing NER DA techniques either perform rule-based \u2026"}, {"title": "ArchCode: Incorporating Software Requirements in Code Generation with Large Language Models", "link": "https://arxiv.org/pdf/2408.00994", "details": "H Han, J Kim, J Yoo, Y Lee, S Hwang - arXiv preprint arXiv:2408.00994, 2024", "abstract": "This paper aims to extend the code generation capability of large language models (LLMs) to automatically manage comprehensive software requirements from given textual descriptions. Such requirements include both functional (ie achieving \u2026"}, {"title": "RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent", "link": "https://arxiv.org/pdf/2407.16667", "details": "H Xu, W Zhang, Z Wang, F Xiao, R Zheng, Y Feng, Z Ba\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot. These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of \u2026"}, {"title": "Thought-Like-Pro: Enhancing Reasoning of Large Language Models through Self-Driven Prolog-based Chain-of-Though", "link": "https://arxiv.org/pdf/2407.14562", "details": "X Tan, Y Deng, X Qiu, W Xu, C Qu, W Chu, Y Xu, Y Qi - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have shown exceptional performance as general- purpose assistants, excelling across a variety of reasoning tasks. This achievement represents a significant step toward achieving artificial general intelligence (AGI) \u2026"}]
