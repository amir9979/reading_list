[{"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "link": "https://arxiv.org/pdf/2410.05639", "details": "R Zhao, ZL Thai, Y Zhang, S Hu, Y Ba, J Zhou, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the \u2026"}, {"title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models", "link": "https://arxiv.org/pdf/2410.18785", "details": "Q Li, X Liu, Z Tang, P Dong, Z Li, X Pan, X Chu - arXiv preprint arXiv:2410.18785, 2024", "abstract": "Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality, with many methods excelling across these criteria. Some \u2026"}, {"title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition", "link": "https://arxiv.org/pdf/2410.13259", "details": "Q Yang, P Wang, LD Plonsky, FL Oswald, H Chen - arXiv preprint arXiv:2410.13259, 2024", "abstract": "We examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of \u2026"}, {"title": "Bypassing the Exponential Dependency: Looped Transformers Efficiently Learn In-context by Multi-step Gradient Descent", "link": "https://arxiv.org/pdf/2410.11268", "details": "B Chen, X Li, Y Liang, Z Shi, Z Song - arXiv preprint arXiv:2410.11268, 2024", "abstract": "In-context learning has been recognized as a key factor in the success of Large Language Models (LLMs). It refers to the model's ability to learn patterns on the fly from provided in-context examples in the prompt during inference. Previous studies \u2026"}, {"title": "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "link": "https://arxiv.org/pdf/2410.06554", "details": "Y Chen, D Zhu, Y Sun, X Chen, W Zhang, X Shen - arXiv preprint arXiv:2410.06554, 2024", "abstract": "Reinforcement Learning from Human Feedback significantly enhances Natural Language Processing by aligning language models with human expectations. A critical factor in this alignment is the strength of reward models used during training \u2026"}, {"title": "From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency", "link": "https://arxiv.org/pdf/2410.05459", "details": "K Wen, H Zhang, H Lin, J Zhang - arXiv preprint arXiv:2410.05459, 2024", "abstract": "Chain-of-thought (CoT) significantly enhances the reasoning performance of large language models (LLM). While current theoretical studies often attribute this improvement to increased expressiveness and computational capacity, we argue \u2026"}, {"title": "Moe++: Accelerating mixture-of-experts methods with zero-computation experts", "link": "https://arxiv.org/pdf/2410.07348", "details": "P Jin, B Zhu, L Yuan, S Yan - arXiv preprint arXiv:2410.07348, 2024", "abstract": "In this work, we aim to simultaneously enhance the effectiveness and efficiency of Mixture-of-Experts (MoE) methods. To achieve this, we propose MoE++, a general and heterogeneous MoE framework that integrates both Feed-Forward \u2026"}, {"title": "Probing-RAG: Self-Probing to Guide Language Models in Selective Document Retrieval", "link": "https://arxiv.org/pdf/2410.13339", "details": "I Baek, H Chang, B Kim, J Lee, H Lee - arXiv preprint arXiv:2410.13339, 2024", "abstract": "Retrieval-Augmented Generation (RAG) enhances language models by retrieving and incorporating relevant external knowledge. However, traditional retrieve-and- generate processes may not be optimized for real-world scenarios, where queries \u2026"}, {"title": "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes", "link": "https://arxiv.org/pdf/2410.05052", "details": "K Nishida, K Nishida, K Saito - arXiv preprint arXiv:2410.05052, 2024", "abstract": "Loss spikes, a phenomenon in which the loss value diverges suddenly, is a fundamental issue in the pre-training of large language models. This paper supposes that the non-uniformity of the norm of the parameters is one of the causes \u2026"}]
