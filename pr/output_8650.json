[{"title": "Improved Depth Estimation of Bayesian Neural Networks", "link": "https://arxiv.org/pdf/2410.10395", "details": "B van Erp, B de Vries - arXiv preprint arXiv:2410.10395, 2024", "abstract": "This paper proposes improvements over earlier work by Nazareth and Blei (2022) for estimating the depth of Bayesian neural networks. Here, we propose a discrete truncated normal distribution over the network depth to independently learn its mean \u2026"}, {"title": "Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models", "link": "https://arxiv.org/pdf/2410.09385", "details": "SK Bhethanabhotla, O Swelam, J Siems, D Salinas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces Mamba4Cast, a zero-shot foundation model for time series forecasting. Based on the Mamba architecture and inspired by Prior-data Fitted Networks (PFNs), Mamba4Cast generalizes robustly across diverse time series tasks \u2026"}, {"title": "Self-Supervised Learning of Disentangled Representations for Multivariate Time-Series", "link": "https://arxiv.org/pdf/2410.12606", "details": "C Chang, CT Chan, WY Wang, WC Peng, TF Chen - arXiv preprint arXiv:2410.12606, 2024", "abstract": "Multivariate time-series data in fields like healthcare and industry are informative but challenging due to high dimensionality and lack of labels. Recent self-supervised learning methods excel in learning rich representations without labels but struggle \u2026"}, {"title": "DyGraphformer: Transformer combining dynamic spatio-temporal graph network for multivariate time series forecasting", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007007", "details": "S Han, Y Xun, J Cai, H Yang, Y Li - Neural Networks, 2024", "abstract": "Transformer-based models demonstrate tremendous potential for Multivariate Time Series (MTS) forecasting due to their ability to capture long-term temporal dependencies by using the self-attention mechanism. However, effectively modeling \u2026"}, {"title": "Dynamic deep graph convolution with enhanced transformer networks for time series anomaly detection in IoT", "link": "https://link.springer.com/article/10.1007/s10586-024-04707-w", "details": "R Gao, Z Chen, X Wu, Y Yu, L Zhang - Cluster Computing, 2025", "abstract": "Anomaly detection of multi-time series data during the working process of Internet of Things systems that utilize sensors is one of the key aspects to prevent accidents in industrial information systems. The key challenge is to discover generalized normal \u2026"}, {"title": "Dynamic Contrastive Learning for Time Series Representation", "link": "https://arxiv.org/pdf/2410.15416", "details": "AK Shamba, K Bach, G Taylor - arXiv preprint arXiv:2410.15416, 2024", "abstract": "Understanding events in time series is an important task in a variety of contexts. However, human analysis and labeling are expensive and time-consuming. Therefore, it is advantageous to learn embeddings for moments in time series in an \u2026"}, {"title": "Posterior Inferred, Now What? Streamlining Prediction in Bayesian Deep Learning", "link": "https://openreview.net/pdf%3Fid%3Dcx9TXPTzt9", "details": "R Li, M Klasson, A Solin, M Trapp - NeurIPS 2024 Workshop on Bayesian Decision \u2026", "abstract": "The rising interest in Bayesian deep learning (BDL) has led to a plethora of methods for estimating the posterior distribution. However, efficient computation of inferences, such as predictions, has been largely overlooked with Monte Carlo integration \u2026"}, {"title": "Towards Personalized Federated Learning via Comprehensive Knowledge Distillation", "link": "https://arxiv.org/pdf/2411.03569", "details": "P Wang, B Liu, W Guo, Y Li, S Ge - arXiv preprint arXiv:2411.03569, 2024", "abstract": "Federated learning is a distributed machine learning paradigm designed to protect data privacy. However, data heterogeneity across various clients results in catastrophic forgetting, where the model rapidly forgets previous knowledge while \u2026"}]
