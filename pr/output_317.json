'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Language Models for Text Classification: Is In-Context'
[{"title": "Few-Shot Recalibration of Language Models", "link": "https://arxiv.org/pdf/2403.18286", "details": "XL Li, U Khandelwal, K Guu - arXiv preprint arXiv:2403.18286, 2024", "abstract": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over \u2026"}, {"title": "Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER", "link": "https://arxiv.org/pdf/2403.18025", "details": "M Abaho, D Bollegala, G Leeming, D Joyce, IE Buchan - arXiv preprint arXiv \u2026, 2024", "abstract": "Adapting language models (LMs) to novel domains is often achieved through fine- tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target \u2026"}, {"title": "BioMedLM: A 2.7 B Parameter Language Model Trained On Biomedical Text", "link": "https://arxiv.org/pdf/2403.18421", "details": "E Bolton, A Venigalla, M Yasunaga, D Hall, B Xiong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance on a wide variety of biomedical NLP tasks. However, these models have hundreds of billions of parameters, are computationally expensive to run \u2026"}, {"title": "Large-Scale Label Interpretation Learning for Few-Shot Named Entity Recognition", "link": "https://arxiv.org/html/2403.14222v1", "details": "J Golde, F Hamborg, A Akbik - arXiv preprint arXiv:2403.14222, 2024", "abstract": "Few-shot named entity recognition (NER) detects named entities within text using only a few annotated examples. One promising line of research is to leverage natural language descriptions of each entity type: the common label PER might, for example \u2026"}, {"title": "Naive Bayes-based Context Extension for Large Language Models", "link": "https://arxiv.org/html/2403.17552v1", "details": "J Su, M Ahmed, L Ao, M Zhu, Y Liu - arXiv preprint arXiv:2403.17552, 2024", "abstract": "Large Language Models (LLMs) have shown promising in-context learning abilities. However, conventional In-Context Learning (ICL) approaches are often impeded by length limitations of transformer architecture, which pose challenges when \u2026"}, {"title": "Enhancing patient representation learning from electronic health records through predicted family relations", "link": "https://www.medrxiv.org/content/10.1101/2024.03.12.24304163.full.pdf", "details": "X Huang, J Arora, AM Erzurumluoglu, D Lam\u2026 - medRxiv, 2024", "abstract": "Artificial intelligence and machine learning are powerful tools in analyzing electronic health records (EHRs) for healthcare research. Despite the recognized importance of family health history, in healthcare research individual patients are often treated as \u2026"}, {"title": "Temporal Cross-Attention for Dynamic Embedding and Tokenization of Multimodal Electronic Health Records", "link": "https://arxiv.org/pdf/2403.04012", "details": "Y Ma, S Kolla, D Kaliraman, V Nolan, Z Hu, Z Guan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The breadth, scale, and temporal granularity of modern electronic health records (EHR) systems offers great potential for estimating personalized and contextual patient health trajectories using sequential deep learning. However, learning useful \u2026"}, {"title": "Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware Classification", "link": "https://arxiv.org/html/2402.18502v1", "details": "G Chhikara, A Sharma, K Ghosh, A Chakraborty - arXiv preprint arXiv:2402.18502, 2024", "abstract": "Employing Large Language Models (LLM) in various downstream applications such as classification is crucial, especially for smaller companies lacking the expertise and resources required for fine-tuning a model. Fairness in LLMs helps ensure inclusivity \u2026"}, {"title": "Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models", "link": "https://arxiv.org/html/2403.13590v1", "details": "A Liusie, Y Fathullah, MJF Gales - arXiv preprint arXiv:2403.13590, 2024", "abstract": "Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities and versatility in NLP tasks, however they sometimes fail to maintain crucial invariances for specific tasks. One example is permutation sensitivity, where \u2026"}]
