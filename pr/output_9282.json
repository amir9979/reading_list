[{"title": "Eliciting Critical Reasoning in Retrieval-Augmented Language Models via Contrastive Explanations", "link": "https://arxiv.org/pdf/2410.22874", "details": "L Ranaldi, M Valentino, A Freitas - arXiv preprint arXiv:2410.22874, 2024", "abstract": "Retrieval-augmented generation (RAG) has emerged as a critical mechanism in contemporary NLP to support Large Language Models (LLMs) in systematically accessing richer factual context. However, the integration of RAG mechanisms brings \u2026"}, {"title": "Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training", "link": "https://arxiv.org/pdf/2411.14318", "details": "Z Luo, X Zhang, X Liu, H Li, Y Gong, C Qi, P Cheng - arXiv preprint arXiv:2411.14318, 2024", "abstract": "It is well-known that a diverse corpus is critical for training large language models, which are typically constructed from a mixture of various domains. In general, previous efforts resort to sampling training data from different domains with static \u2026"}, {"title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent", "link": "https://arxiv.org/pdf/2411.02265%3F", "details": "X Sun, Y Chen, Y Huang, R Xie, J Zhu, K Zhang, S Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we introduce Hunyuan-Large, which is currently the largest open- source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K \u2026"}, {"title": "Language-Emphasized Cross-Lingual In-Context Learning for Multilingual LLM", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9437-9_26", "details": "J Li, X Wei, X Wang, N Zhuang, L Wang, J Dang - CCF International Conference on \u2026, 2024", "abstract": "With the recent rise of large language models (LLMs), in-context learning (ICL) has shown remarkable performance, eliminating the need for fine-tuning parameters and reducing the reliance on extensive labeled data. However, the intricacies of cross \u2026"}, {"title": "Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2410.21728", "details": "K Luo, Z Ding, Z Weng, L Qiao, M Zhao, X Li, D Yin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Chain of Thought (CoT) prompting approaches have significantly consolidated the reasoning capabilities of large language models (LLMs), they still face limitations that require extensive human effort or have performance needs to be improved \u2026"}, {"title": "Sparsing Law: Towards Large Language Models with Greater Activation Sparsity", "link": "https://arxiv.org/pdf/2411.02335%3F", "details": "Y Luo, C Song, X Han, Y Chen, C Xiao, Z Liu, M Sun - arXiv preprint arXiv \u2026, 2024", "abstract": "Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting \u2026"}, {"title": "What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective", "link": "https://arxiv.org/pdf/2410.23743", "details": "M Li, Y Li, T Zhou - arXiv preprint arXiv:2410.23743, 2024", "abstract": "What makes a difference in the post-training of LLMs? We investigate the training patterns of different layers in large language models (LLMs), through the lens of gradient, when training with different responses and initial models. We are \u2026"}, {"title": "Defining and Evaluating Physical Safety for Large Language Models", "link": "https://arxiv.org/pdf/2411.02317%3F", "details": "YC Tang, PY Chen, TY Ho - arXiv preprint arXiv:2411.02317, 2024", "abstract": "Large Language Models (LLMs) are increasingly used to control robotic systems such as drones, but their risks of causing physical threats and harm in real-world applications remain unexplored. Our study addresses the critical gap in evaluating \u2026"}, {"title": "DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization", "link": "https://arxiv.org/pdf/2411.14055", "details": "H Deng, W Jiao, X Liu, M Zhang, Z Tu - arXiv preprint arXiv:2411.14055, 2024", "abstract": "Large language models (LLMs) deliver impressive results but face challenges from increasing model sizes and computational costs. Structured pruning reduces model size and speeds up inference but often causes uneven degradation across domains \u2026"}]
