'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Text Quality-Based Pruning for Efficient Training of L'
[{"title": "Gaze-infused BERT: Do human gaze signals help pre-trained language models?", "link": "https://link.springer.com/article/10.1007/s00521-024-09725-8", "details": "B Wang, B Liang, L Zhou, R Xu - Neural Computing and Applications, 2024", "abstract": "This research delves into the intricate connection between self-attention mechanisms in large-scale pre-trained language models, like BERT, and human gaze patterns, with the aim of harnessing gaze information to enhance the performance of natural \u2026"}, {"title": "Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback", "link": "https://arxiv.org/pdf/2404.14233", "details": "W Xiao, Z Huang, L Gan, W He, H Li, Z Yu, H Jiang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapidly developing Large Vision Language Models (LVLMs) have shown notable capabilities on a range of multi-modal tasks, but still face the hallucination phenomena where the generated texts do not align with the given contexts \u2026"}, {"title": "An in-depth evaluation of federated learning on biomedical natural language processing for information extraction", "link": "https://www.nature.com/articles/s41746-024-01126-4", "details": "L Peng, G Luo, S Zhou, J Chen, Z Xu, J Sun, R Zhang - npj Digital Medicine, 2024", "abstract": "Abstract Language models (LMs) such as BERT and GPT have revolutionized natural language processing (NLP). However, the medical field faces challenges in training LMs due to limited data access and privacy constraints imposed by \u2026"}, {"title": "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models", "link": "https://arxiv.org/pdf/2404.15846", "details": "Q He, J Zeng, Q He, J Liang, Y Xiao - arXiv preprint arXiv:2404.15846, 2024", "abstract": "It is imperative for Large language models (LLMs) to follow instructions with elaborate requirements (ie Complex Instructions Following). Yet, it remains under- explored how to enhance the ability of LLMs to follow complex instructions with \u2026"}, {"title": "ExcluIR: Exclusionary Neural Information Retrieval", "link": "https://arxiv.org/pdf/2404.17288", "details": "W Zhang, M Zhang, S Wu, J Pei, Z Ren, M de Rijke\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Exclusion is an important and universal linguistic skill that humans use to express what they do not want. However, in information retrieval community, there is little research on exclusionary retrieval, where users express what they do not want in \u2026"}, {"title": "Language model-based labeling of German thoracic radiology reports", "link": "https://www.thieme-connect.com/products/ejournals/html/10.1055/a-2287-5054", "details": "A Wollek, P Haitzer, T Sedlmeyr, S Hyska, J Rueckel\u2026 - R\u00f6Fo-Fortschritte auf dem \u2026, 2024", "abstract": "The aim of this study was to explore the potential of weak supervision in a deep learning-based label prediction model. The goal was to use this model to extract labels from German free-text thoracic radiology reports on chest X-ray images and for \u2026"}, {"title": "Meta In-Context Learning Makes Large Language Models Better Zero and Few-Shot Relation Extractors", "link": "https://arxiv.org/pdf/2404.17807", "details": "G Li, P Wang, J Liu, Y Guo, K Ji, Z Shang, Z Xu - arXiv preprint arXiv:2404.17807, 2024", "abstract": "Relation extraction (RE) is an important task that aims to identify the relationships between entities in texts. While large language models (LLMs) have revealed remarkable in-context learning (ICL) capability for general zero and few-shot \u2026"}, {"title": "Low-Cost Language Models: Survey and Performance Evaluation on Python Code Generation", "link": "https://arxiv.org/pdf/2404.11160", "details": "JL Espejel, MSY Alassan, M Bouhandi, W Dahhane\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have become the go-to solution for many Natural Language Processing (NLP) tasks due to their ability to tackle various problems and produce high-quality results. Specifically, they are increasingly used to automatically \u2026"}, {"title": "Towards Better Vision-Inspired Vision-Language Models", "link": "https://www.lamda.nju.edu.cn/caoyh/files/VIVL.pdf", "details": "YH Cao, K Ji, Z Huang, C Zheng, J Liu, J Wang, J Chen\u2026", "abstract": "Vision-language (VL) models have achieved unprecedented success recently, in which the connection module is the key to bridge the modality gap. Nevertheless, the abundant visual clues are not sufficiently exploited in most existing methods. On the \u2026"}]
