'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Fine-Tuning Language Models with Reward Learning on Po'
[{"title": "Learn\" No\" to Say\" Yes\" Better: Improving Vision-Language Models via Negations", "link": "https://arxiv.org/pdf/2403.20312", "details": "J Singh, I Shrivastava, M Vatsa, R Singh, A Bharati - arXiv preprint arXiv:2403.20312, 2024", "abstract": "Existing vision-language models (VLMs) treat text descriptions as a unit, confusing individual concepts in a prompt and impairing visual semantic matching and reasoning. An important aspect of reasoning in logic and language is negations. This \u2026"}, {"title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models", "link": "https://arxiv.org/pdf/2403.20262", "details": "T Thonet, J Rozen, L Besacier - arXiv preprint arXiv:2403.20262, 2024", "abstract": "Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities \u2026"}, {"title": "Emergent Abilities in Reduced-Scale Generative Language Models", "link": "https://arxiv.org/pdf/2404.02204", "details": "S Muckatira, V Deshpande, V Lialin, A Rumshisky - arXiv preprint arXiv:2404.02204, 2024", "abstract": "Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study \u2026"}, {"title": "Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models", "link": "https://arxiv.org/pdf/2403.16999", "details": "H Shao, S Qian, H Xiao, G Song, Z Zong, L Wang, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper presents Visual CoT, a novel pipeline that leverages the reasoning capabilities of multi-modal large language models (MLLMs) by incorporating visual Chain-of-Thought (CoT) reasoning. While MLLMs have shown promise in various \u2026"}, {"title": "Generative Language Models for Personalized Information Understanding", "link": "https://scholarworks.umass.edu/cgi/viewcontent.cgi%3Farticle%3D4123%26context%3Ddissertations_2", "details": "P Cai - 2024", "abstract": "A major challenge in information understanding stems from the diverse nature of the audience, where individuals possess varying preferences, experiences, educational and cultural backgrounds. Consequently, adopting a one-size-fits-all approach to \u2026"}, {"title": "Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning", "link": "https://arxiv.org/pdf/2404.00909", "details": "R Li, Y Wu, X He - arXiv preprint arXiv:2404.00909, 2024", "abstract": "Generative vision-language models (VLMs) have shown impressive performance in zero-shot vision-language tasks like image captioning and visual question answering. However, improving their zero-shot reasoning typically requires second \u2026"}, {"title": "Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models", "link": "https://arxiv.org/pdf/2403.17359", "details": "Z Pan, H Luo, M Li, H Liu - arXiv preprint arXiv:2403.17359, 2024", "abstract": "We present a Chain-of-Action (CoA) framework for multimodal and retrieval- augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications:(i) unfaithful hallucination that is \u2026"}, {"title": "Eyes Can Deceive: Benchmarking Counterfactual Reasoning Abilities of Multi-modal Large Language Models", "link": "https://arxiv.org/pdf/2404.12966", "details": "Y Li, W Tian, Y Jiao, J Chen, YG Jiang - arXiv preprint arXiv:2404.12966, 2024", "abstract": "Counterfactual reasoning, as a crucial manifestation of human intelligence, refers to making presuppositions based on established facts and extrapolating potential outcomes. Existing multimodal large language models (MLLMs) have exhibited \u2026"}, {"title": "Personalized Wireless Federated Learning for Large Language Models", "link": "https://arxiv.org/pdf/2404.13238", "details": "F Jiang, L Dong, S Tu, Y Peng, K Wang, K Yang, C Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have revolutionized natural language processing tasks. However, their deployment in wireless networks still face challenges, ie, a lack of privacy and security protection mechanisms. Federated Learning (FL) has \u2026"}]
