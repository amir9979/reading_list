[{"title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs", "link": "https://arxiv.org/pdf/2503.01743%3F", "details": "A Abouelenin, A Ashfaq, A Atkinson, H Awadalla\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent \u2026"}, {"title": "VLog: Video-Language Models by Generative Retrieval of Narration Vocabulary", "link": "https://arxiv.org/pdf/2503.09402", "details": "KQ Lin, MZ Shou - arXiv preprint arXiv:2503.09402, 2025", "abstract": "Human daily activities can be concisely narrated as sequences of routine events (eg, turning off an alarm) in video streams, forming an event vocabulary. Motivated by this, we introduce VLog, a novel video understanding framework that define video \u2026"}, {"title": "Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model for Advanced Medical Decision Support", "link": "https://arxiv.org/pdf/2502.18274%3F", "details": "G Wang, M Gao, S Yang, Y Zhang, L He, L Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs), particularly those with reasoning capabilities, have rapidly advanced in recent years, demonstrating significant potential across a wide range of applications. However, their deployment in healthcare, especially in disease \u2026"}, {"title": "Rethinking Data: Towards Better Performing Domain-Specific Small Language Models", "link": "https://arxiv.org/pdf/2503.01464", "details": "B Nazarov, D Frolova, Y Lubarsky, A Gaissinski\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Fine-tuning of Large Language Models (LLMs) for downstream tasks, performed on domain-specific data has shown significant promise. However, commercial use of such LLMs is limited by the high computational cost required for their deployment at \u2026"}, {"title": "Hierarchical Vision\u2013Language Pre-Training with Freezing Strategy for Multi-Level Semantic Alignment", "link": "https://www.mdpi.com/2079-9292/14/4/816", "details": "H Xie, Y Qin, S Ding - Electronics, 2025", "abstract": "Vision\u2013language pre-training (VLP) faces challenges in aligning hierarchical textual semantics (words/phrases/sentences) with multi-scale visual features (objects/relations/global context). We propose a hierarchical VLP model (HieVLP) \u2026"}, {"title": "Multilingual Language Model Pretraining using Machine-translated Data", "link": "https://arxiv.org/pdf/2502.13252", "details": "J Wang, Y Lu, M Weber, M Ryabinin, D Adelani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-resource languages such as English, enables the pretraining of high-quality large language models (LLMs). The same can not be said for most other languages as LLMs still underperform for non-English languages, likely due to a gap in the \u2026"}, {"title": "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability", "link": "https://arxiv.org/pdf/2503.09532", "details": "A Karvonen, C Rager, J Lin, C Tigges, J Bloom\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Sparse autoencoders (SAEs) are a popular technique for interpreting language model activations, and there is extensive recent work on improving SAE effectiveness. However, most prior work evaluates progress using unsupervised \u2026"}, {"title": "RetriEVAL: Evaluating Text Generation with Contextualized Lexical Match", "link": "https://dl.acm.org/doi/abs/10.1145/3701551.3703581", "details": "Z Li, X Li, C Tao, J Feng, T Shen, C Xu, H Wang\u2026 - Proceedings of the \u2026, 2025", "abstract": "Pre-trained language models have made significant advancements in text generation tasks. Nevertheless, evaluating the generated text with automatic metrics is still challenging. Compared with supervised metrics, unsupervised metrics which \u2026"}, {"title": "MedBot vs RealDoc: efficacy of large language modeling in physician-patient communication for rare diseases", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocaf034/8042190", "details": "MT Weber, R Noll, A Marchl, C Facchinello\u2026 - Journal of the American \u2026, 2025", "abstract": "Objectives This study assesses the abilities of 2 large language models (LLMs), GPT- 4 and BioMistral 7B, in responding to patient queries, particularly concerning rare diseases, and compares their performance with that of physicians. Materials and \u2026"}]
