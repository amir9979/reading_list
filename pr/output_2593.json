[{"title": "Measuring Retrieval Complexity in Question Answering Systems", "link": "https://arxiv.org/pdf/2406.03592", "details": "M Gabburo, NP Jedema, S Garg, LFR Ribeiro\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we investigate which questions are challenging for retrieval-based Question Answering (QA). We (i) propose retrieval complexity (RC), a novel metric conditioned on the completeness of retrieved documents, which measures the \u2026"}, {"title": "Multi-view Counterfactual Contrastive Learning for Fact-checking Fake News Detection", "link": "https://dl.acm.org/doi/abs/10.1145/3652583.3658087", "details": "Y Zhang, L Kong, S Tian, H Fei, C Xiang, H Wang\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Fact-checking fake news detection involves using verified accurate factual information in news reports as\" evidence\" to validate objective statement\" claim\". Existing works primarily focus on identifying critical elements within the evidence that \u2026"}, {"title": "AraMed: Arabic Medical Question Answering using Pretrained Transformer Language Models", "link": "https://aclanthology.org/2024.osact-1.6.pdf", "details": "A Alasmari, S Alhumoud, W Alshammari - Proceedings of the 6th Workshop on Open \u2026, 2024", "abstract": "Abstract Medical Question Answering systems have gained significant attention in recent years due to their potential to enhance medical decision-making and improve patient care. However, most of the research in this field has focused on English \u2026"}, {"title": "Clustering swap prediction for image-text pre-training", "link": "https://www.nature.com/articles/s41598-024-60832-x", "details": "S Fayou, HC Ngo, YW Sek, Z Meng - Scientific Reports, 2024", "abstract": "It is essential to delve into the strategy of multimodal model pre-training, which is an obvious impact on downstream tasks. Currently, clustering learning has achieved noteworthy benefits in multiple methods. However, due to the availability of open \u2026"}, {"title": "TAeKD: Teacher Assistant Enhanced Knowledge Distillation for Closed-Source Multilingual Neural Machine Translation", "link": "https://aclanthology.org/2024.lrec-main.1350.pdf", "details": "B Lv, X Liu, K Wei, P Luo, Y Yu - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Knowledge Distillation (KD) serves as an efficient method for transferring language knowledge from open-source large language models (LLMs) to more computationally efficient models. However, challenges arise when attempting to \u2026"}, {"title": "Exploring and Mitigating Shortcut Learning for Generative Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.602.pdf", "details": "Z Sun, Y Xiao, J Li, Y Ji, W Chen, M Zhang - Proceedings of the 2024 Joint \u2026, 2024", "abstract": "Recent generative large language models (LLMs) have exhibited incredible instruction-following capabilities while keeping strong task completion ability, even without task-specific fine-tuning. Some works attribute this to the bonus of the new \u2026"}, {"title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective", "link": "https://arxiv.org/pdf/2405.16747", "details": "A Tomihari, I Sato - arXiv preprint arXiv:2405.16747, 2024", "abstract": "The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data. This success is largely \u2026"}, {"title": "Before Generation, Align it! A Novel and Effective Strategy for Mitigating Hallucinations in Text-to-SQL Generation", "link": "https://arxiv.org/pdf/2405.15307", "details": "G Qu, J Li, B Li, B Qin, N Huo, C Ma, R Cheng - arXiv preprint arXiv:2405.15307, 2024", "abstract": "Large Language Models (LLMs) driven by In-Context Learning (ICL) have significantly improved the performance of text-to-SQL. Previous methods generally employ a two-stage reasoning framework, namely 1) schema linking and 2) logical \u2026"}, {"title": "TIQ: A Benchmark for Temporal Question Answering with Implicit Time Constraints", "link": "https://dl.acm.org/doi/pdf/10.1145/3589335.3651895", "details": "Z Jia, P Christmann, G Weikum - Companion Proceedings of the ACM on Web \u2026, 2024", "abstract": "Temporal question answering (QA) involves explicit (eg,\"... before 2024\") or implicit (eg,\"... during the Cold War period\") time constraints. Implicit constraints are more challenging; yet benchmarks for temporal QA largely disregard such questions. This \u2026"}]
