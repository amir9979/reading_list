[{"title": "Autonomous Data Selection with Language Models for Mathematical Texts", "link": "https://openreview.net/pdf%3Fid%3DbBF077z8LF", "details": "Y Zhang, Y Luo, Y Yuan, AC Yao - ICLR 2024 Workshop on Navigating and \u2026, 2024", "abstract": "To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or \u2026"}, {"title": "MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning", "link": "https://arxiv.org/pdf/2405.03000", "details": "W Shi, R Xu, Y Zhuang, Y Yu, H Wu, C Yang, MD Wang - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their improved capabilities in generation and reasoning, adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and corporate privacy. In this work, we propose MedAdapter, a unified \u2026"}, {"title": "StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation", "link": "https://arxiv.org/pdf/2405.01434", "details": "Y Zhou, D Zhou, MM Cheng, J Feng, Q Hou - arXiv preprint arXiv:2405.01434, 2024", "abstract": "For recent diffusion-based generative models, maintaining consistent content across a series of generated images, especially those containing subjects and complex details, presents a significant challenge. In this paper, we propose a new way of self \u2026"}, {"title": "Unifying 3D Vision-Language Understanding via Promptable Queries", "link": "https://arxiv.org/pdf/2405.11442", "details": "Z Zhu, Z Zhang, X Ma, X Niu, Y Chen, B Jia, Z Deng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "A unified model for 3D vision-language (3D-VL) understanding is expected to take various scene representations and perform a wide range of tasks in a 3D scene. However, a considerable gap exists between existing methods and such a unified \u2026"}, {"title": "Property Existence Inference against Generative Models", "link": "https://www.usenix.org/system/files/sec24fall-prepub-2868-wang-lijin.pdf", "details": "L Wang, J Wang, J Wan, L Long, Z Yang, Z Qin\u2026", "abstract": "Generative models have served as the backbone of versatile tools with a wide range of applications across various fields in recent years. However, it has been demonstrated that privacy concerns, such as membership information leakage of the \u2026"}, {"title": "Hallucinating for Diagnosing: One-Shot Medical Image Classification Leveraging Score-Based Generative Models", "link": "https://openreview.net/pdf%3Fid%3Dy5onNeda4Y", "details": "E Pachetti, S Colantonio - Medical Imaging with Deep Learning, 2024", "abstract": "Deep learning models in data-scarce domains, such as medical imaging, often suffer from poor performance due to the challenges of acquiring large amounts of labeled data. Few-shot learning offers a promising solution to this problem. This work \u2026"}, {"title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "link": "https://arxiv.org/pdf/2405.00402", "details": "L Ranaldi, A Freitas - arXiv preprint arXiv:2405.00402, 2024", "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches \u2026"}, {"title": "Is Less More? Quality, Quantity and Context in Idiom Processing with Natural Language Models", "link": "https://arxiv.org/pdf/2405.08497", "details": "A Knietaite, A Allsebrook, A Minkov, A Tomaszewski\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Compositionality in language models presents a problem when processing idiomatic expressions, as their meaning often cannot be directly derived from their individual parts. Although fine-tuning and other optimization strategies can be used to improve \u2026"}, {"title": "NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli", "link": "https://arxiv.org/pdf/2405.02814", "details": "X Wang, C Li, Y Chang, J Wang, Y Wu - arXiv preprint arXiv:2405.02814, 2024", "abstract": "Large Language Models (LLMs) have become integral to a wide spectrum of applications, ranging from traditional computing tasks to advanced artificial intelligence (AI) applications. This widespread adoption has spurred extensive \u2026"}]
