[{"title": "Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2407.00569", "details": "W Zhong, X Feng, L Zhao, Q Li, L Huang, Y Gu, W Ma\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations \u2026"}, {"title": "EgoVideo: Exploring Egocentric Foundation Model and Downstream Adaptation", "link": "https://arxiv.org/pdf/2406.18070", "details": "B Pei, G Chen, J Xu, Y He, Y Liu, K Pan, Y Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this report, we present our solutions to the EgoVis Challenges in CVPR 2024, including five tracks in the Ego4D challenge and three tracks in the EPIC-Kitchens challenge. Building upon the video-language two-tower model and leveraging our \u2026"}, {"title": "Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation", "link": "https://arxiv.org/pdf/2407.01220", "details": "Z Gao, L Li, L Jiao, F Liu, X Liu, W Ma, Y Guo, S Yang - arXiv preprint arXiv \u2026, 2024", "abstract": "Understanding 3D scenes is a crucial challenge in computer vision research with applications spanning multiple domains. Recent advancements in distilling 2D vision- language foundation models into neural fields, like NeRF and 3DGS, enables open \u2026"}, {"title": "Query-based Semantic Gaussian Field for Scene Representation in Reinforcement Learning", "link": "https://arxiv.org/pdf/2406.02370", "details": "J Wang, Z Zhang, Q Zhang, J Li, J Sun, M Sun, J He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Latent scene representation plays a significant role in training reinforcement learning (RL) agents. To obtain good latent vectors describing the scenes, recent works incorporate the 3D-aware latent-conditioned NeRF pipeline into scene \u2026"}, {"title": "Refusal in Language Models Is Mediated by a Single Direction", "link": "https://arxiv.org/pdf/2406.11717", "details": "A Arditi, O Obeso, A Syed, D Paleka, N Rimsky\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying \u2026"}, {"title": "Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training", "link": "https://arxiv.org/pdf/2406.03488", "details": "S Ao, W Zhao, X Han, C Yang, Z Liu, C Shi, M Sun - arXiv preprint arXiv:2406.03488, 2024", "abstract": "The emergence of large language models (LLMs) relies heavily on distributed training strategies, among which pipeline parallelism plays a crucial role. As LLMs' training sequence length extends to 32k or even 128k, the current pipeline parallel \u2026"}, {"title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback", "link": "https://openreview.net/pdf%3Fid%3DBOorDpKHiJ", "details": "G Cui, L Yuan, N Ding, G Yao, B He, W Zhu, Y Ni, G Xie\u2026 - Forty-first International \u2026, 2024", "abstract": "Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability \u2026"}]
