[{"title": "Personalizing Low-Rank Bayesian Neural Networks Via Federated Learning", "link": "https://arxiv.org/pdf/2410.14390", "details": "B Zhang, D Liu, O Simeone, G Wang, D Pezaros, G Zhu - arXiv preprint arXiv \u2026, 2024", "abstract": "To support real-world decision-making, it is crucial for models to be well-calibrated, ie, to assign reliable confidence estimates to their predictions. Uncertainty quantification is particularly important in personalized federated learning (PFL), as \u2026"}, {"title": "Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning Zero-Shot Models", "link": "https://openreview.net/pdf%3Fid%3Dp50Dyqk0GX", "details": "K Li, W Xie, Y Huang, D Deng, H Lanqing, Z Li, R Silva\u2026 - The Thirty-eighth Annual \u2026, 2024", "abstract": "Fine-tuning foundation models often compromises their robustness to distribution shifts. To remedy this, most robust fine-tuning methods aim to preserve the pre- trained features. However, not all pre-trained features are robust and those methods \u2026"}, {"title": "Self-supervised Hierarchical Representation for Medication Recommendation", "link": "https://arxiv.org/pdf/2411.03143", "details": "Y Liang, Y Liu, Y Dang, E Yang, G Guo, W Cai, J Zhao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medication recommender is to suggest appropriate medication combinations based on a patient's health history, eg, diagnoses and procedures. Existing works represent different diagnoses/procedures well separated by one-hot encodings. However, they \u2026"}, {"title": "Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered Clues", "link": "https://arxiv.org/pdf/2410.10700", "details": "Q Ren, H Li, D Liu, Z Xie, X Lu, Y Qiao, L Sha, J Yan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study exposes the safety vulnerabilities of Large Language Models (LLMs) in multi-turn interactions, where malicious users can obscure harmful intents across several queries. We introduce ActorAttack, a novel multi-turn attack method inspired \u2026"}, {"title": "Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs", "link": "https://arxiv.org/pdf/2410.10739", "details": "I Jindal, C Badrinath, P Bharti, L Vinay, SD Sharma - arXiv preprint arXiv:2410.10739, 2024", "abstract": "Large Language Models (LLMs) for public use require continuous pre-training to remain up-to-date with the latest data. The models also need to be fine-tuned with specific instructions to maintain their ability to follow instructions accurately. Typically \u2026"}, {"title": "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries", "link": "https://aclanthology.org/2024.findings-emnlp.578.pdf", "details": "B Fang, X Dai, S Karimi - Findings of the Association for Computational \u2026, 2024", "abstract": "Generating plain biomedical summaries with Large Language Models (LLMs) can enhance the accessibility of biomedical knowledge to the public. However, how faithful the generated summaries are remains an open yet critical question. To \u2026"}, {"title": "Who's Who: Large Language Models Meet Knowledge Conflicts in Practice", "link": "https://arxiv.org/pdf/2410.15737", "details": "QH Pham, H Ngo, AT Luu, DQ Nguyen - arXiv preprint arXiv:2410.15737, 2024", "abstract": "Retrieval-augmented generation (RAG) methods are viable solutions for addressing the static memory limits of pre-trained language models. Nevertheless, encountering conflicting sources of information within the retrieval context is an inevitable practical \u2026"}, {"title": "Order Matters: Exploring Order Sensitivity in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2410.16983%3F", "details": "Z Tan, X Chu, W Li, T Mo - arXiv preprint arXiv:2410.16983, 2024", "abstract": "Multimodal Large Language Models (MLLMs) utilize multimodal contexts consisting of text, images, or videos to solve various multimodal tasks. However, we find that changing the order of multimodal input can cause the model's performance to \u2026"}, {"title": "CLR-Bench: Evaluating Large Language Models in College-level Reasoning", "link": "https://arxiv.org/pdf/2410.17558", "details": "J Dong, Z Hong, Y Bei, F Huang, X Wang, X Huang - arXiv preprint arXiv:2410.17558, 2024", "abstract": "Large language models (LLMs) have demonstrated their remarkable performance across various language understanding tasks. While emerging benchmarks have been proposed to evaluate LLMs in various domains such as mathematics and \u2026"}]
