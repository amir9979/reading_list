[{"title": "B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability", "link": "https://arxiv.org/pdf/2502.12992", "details": "Y Wang, S Rao, JU Lee, M Jobanputra, V Demberg - arXiv preprint arXiv:2502.12992, 2025", "abstract": "Post-hoc explanation methods for black-box models often struggle with faithfulness and human interpretability due to the lack of explainability in current neural models. Meanwhile, B-cos networks have been introduced to improve model explainability \u2026"}, {"title": "Multilingual Language Model Pretraining using Machine-translated Data", "link": "https://arxiv.org/pdf/2502.13252", "details": "J Wang, Y Lu, M Weber, M Ryabinin, D Adelani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-resource languages such as English, enables the pretraining of high-quality large language models (LLMs). The same can not be said for most other languages as LLMs still underperform for non-English languages, likely due to a gap in the \u2026"}, {"title": "Adapting Generative Large Language Models for Information Extraction from Unstructured Electronic Health Records in Residential Aged Care: A Comparative \u2026", "link": "https://link.springer.com/article/10.1007/s41666-025-00190-z", "details": "D Vithanage, C Deng, L Wang, M Yin, M Alkhalaf\u2026 - Journal of Healthcare \u2026, 2025", "abstract": "Abstract Information extraction (IE) of unstructured electronic health records is challenging due to the semantic complexity of textual data. Generative large language models (LLMs) offer promising solutions to address this challenge \u2026"}, {"title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models", "link": "https://arxiv.org/pdf/2502.04728%3F", "details": "Z Yu, Y Yuan, TZ Xiao, FF Xia, J Fu, G Zhang, G Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language \u2026"}, {"title": "Evaluating and Enhancing Out-of-Domain Generalization of Task-Oriented Dialog Systems for Task Completion without Turn-level Dialog Annotations", "link": "https://arxiv.org/pdf/2502.13310", "details": "A Mosharrof, M Fereidouni, AB Siddique - arXiv preprint arXiv:2502.13310, 2025", "abstract": "Traditional task-oriented dialog (ToD) systems rely heavily on labor-intensive turn- level annotations, such as dialogue states and policy labels, for training. This work explores whether large language models (LLMs) can be fine-tuned solely on natural \u2026"}, {"title": "MuDAF: Long-Context Multi-Document Attention Focusing through Contrastive Learning on Attention Heads", "link": "https://arxiv.org/pdf/2502.13963", "details": "W Liu, N Wu, S Yang, W Ding, S Liang, M Gong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) frequently show distracted attention due to irrelevant information in the input, which severely impairs their long-context capabilities. Inspired by recent studies on the effectiveness of retrieval heads in long \u2026"}, {"title": "Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models", "link": "https://arxiv.org/pdf/2502.11881", "details": "H Kim, M Sclar, T Zhi-Xuan, L Ying, S Levine, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods-such \u2026"}]
