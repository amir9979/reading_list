[{"title": "Federated Koopman-Reservoir Learning for Large-Scale Multivariate Time-Series Anomaly Detection", "link": "https://arxiv.org/pdf/2503.11255", "details": "LT Le, TA Nguyen, H Shu, S Seneviratne, CS Hong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The proliferation of edge devices has dramatically increased the generation of multivariate time-series (MVTS) data, essential for applications from healthcare to smart cities. Such data streams, however, are vulnerable to anomalies that signal \u2026"}, {"title": "A Time Series Self-Supervised Contrastive Pre-Training Method with Data Augmentation using Discrepancy of Reconstruction Information Loss", "link": "https://ieeexplore.ieee.org/abstract/document/10912708/", "details": "Z Zhang, Y Han, B Ma, Z Geng - IEEE Transactions on Instrumentation and \u2026, 2025", "abstract": "Self-supervised pre-training has shown considerable advantages in multiple time series tasks. A widely used class of methods is based on contrastive learning and a key problem is how to construct augmented samples. At present, explicit \u2026"}, {"title": "Fusionformer: A Novel Adversarial Transformer Utilizing Fusion Attention for Multivariate Anomaly Detection", "link": "https://ieeexplore.ieee.org/abstract/document/10922726/", "details": "C Wang, Z Wang, H Dong, S Lauria, W Liu, Y Wang\u2026 - IEEE Transactions on \u2026, 2025", "abstract": "Multivariate time series forecasting (MTSF) is of significant importance in the enhancement and optimization of real-world applications. The task of MTSF poses substantial challenges due to the unpredictability of temporal patterns and the \u2026"}, {"title": "One-Class Classification Constraint in Reconstruction Networks for Multivariate Time Series Anomaly Detection", "link": "https://ieeexplore.ieee.org/abstract/document/10912658/", "details": "J Li, Z Yu, Q Jiang, Z Cao - IEEE Transactions on Instrumentation and \u2026, 2025", "abstract": "Detecting anomalies in multivariate time series (MTS) data is crucial for maintaining the stability of industrial manufacturing processes and biochemical operations. However, current methods often focus on capturing the normal patterns of training \u2026"}, {"title": "Series clustering and dynamic periodic patching-based transformer for multivariate time series forecasting", "link": "https://www.sciencedirect.com/science/article/pii/S1568494625002911", "details": "Y Wang, X Wu, J Zhang, W Wang, L Zheng, J Shang - Applied Soft Computing, 2025", "abstract": "Multivariate time series forecasting (MTSF) is widely employed in research-intensive domains, such as weather forecasting. Recently, Transformer-based models have outstanding ability to achieve SOTA performance, benefiting from its self-attention \u2026"}, {"title": "Tinyr1-32b-preview: Boosting accuracy with branch-merge distillation", "link": "https://arxiv.org/pdf/2503.04872", "details": "L Sun, G Zhao, X Jian, Y Wu, W Lin, Y Zhu, L Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The challenge of reducing the size of Large Language Models (LLMs) while maintaining their performance has gained significant attention. However, existing methods, such as model distillation and transfer learning, often fail to achieve high \u2026"}, {"title": "Contrastive Learning via Randomly Generated Deep Supervision", "link": "https://ieeexplore.ieee.org/abstract/document/10890867/", "details": "S Wang, Z Ma, KH Chan, Y Liu, T Tong, Q Gao, G Zhai\u2026 - ICASSP 2025-2025 IEEE \u2026, 2025", "abstract": "Unsupervised visual representation learning has gained significant attention in the computer vision community, driven by recent advancements in contrastive learning. Most existing contrastive learning frameworks rely on instance discrimination as a \u2026"}, {"title": "[TINY] Vision language models can implicitly quantify aleatoric uncertainty", "link": "https://openreview.net/pdf%3Fid%3DBkWVcXevTs", "details": "X Wang, E Nalisnick - \u2026 Workshop: Quantify Uncertainty and Hallucination in \u2026", "abstract": "Recent advances in vision language models (VLMs), such as GPT-4o, have revolutionized visual reasoning by enabling zero-shot task completion through natural language instructions. In this paper, we study VLMs' ability to detect input \u2026"}, {"title": "TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification", "link": "https://arxiv.org/pdf/2503.12206", "details": "A Munir, FZ Qureshi, MH Khan, M Ali - arXiv preprint arXiv:2503.12206, 2025", "abstract": "Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot performance on image classification. However, state-of-the-art methods often rely on fine-tuning techniques like prompt learning and adapter-based tuning to optimize \u2026"}]
