[{"title": "MedVLM: Medical Vision-Language Model for Consumer Devices", "link": "https://ieeexplore.ieee.org/abstract/document/10816095/", "details": "M Ayaz, M Khan, M Saqib, A Khelifi, M Sajjad\u2026 - IEEE Consumer Electronics \u2026, 2024", "abstract": "Generative Artificial Intelligence (GenAI) has enabled significant advancements in healthcare by supporting complex medical tasks through multimodal data processing. However, existing models often lack the adaptability required for diverse \u2026"}, {"title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding", "link": "https://arxiv.org/pdf/2412.16158%3F", "details": "C Tao, S Su, X Zhu, C Zhang, Z Chen, J Liu, W Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advance of Large Language Models (LLMs) has catalyzed the development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid modality-specific encoders, offer a promising alternative to the compositional ones \u2026"}, {"title": "Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models", "link": "https://arxiv.org/pdf/2412.18609%3F", "details": "J Yi, ST Wasim, Y Luo, M Naseer, J Gall - arXiv preprint arXiv:2412.18609, 2024", "abstract": "We present an efficient encoder-free approach for video-language understanding that achieves competitive performance while significantly reducing computational overhead. Current video-language models typically rely on heavyweight image \u2026"}, {"title": "Analysis and Visualization of Linguistic Structures in Large Language Models: Neural Representations of Verb-Particle Constructions in BERT", "link": "https://arxiv.org/pdf/2412.14670", "details": "H Kissane, A Schilling, P Krauss - arXiv preprint arXiv:2412.14670, 2024", "abstract": "This study investigates the internal representations of verb-particle combinations within transformer-based large language models (LLMs), specifically examining how these models capture lexical and syntactic nuances at different neural network \u2026"}, {"title": "Unleash the Power of Vision-Language Models by Visual Attention Prompt and Multi-modal Interaction", "link": "https://ieeexplore.ieee.org/abstract/document/10814093/", "details": "W Zhang, L Wu, Z Zhang, T Yu, C Ma, X Jin, X Yang\u2026 - IEEE Transactions on \u2026, 2024", "abstract": "Pre-trained vision-language models (VLMs) like CLIP [1], equipped with parameter- efficient tuning (PET) methods like prompting [2], have shown impressive knowledge transferability on new downstream tasks, but they are still prone to be limited by \u2026"}, {"title": "Do language models understand time?", "link": "https://arxiv.org/pdf/2412.13845", "details": "X Ding, L Wang - arXiv preprint arXiv:2412.13845, 2024", "abstract": "Large language models (LLMs) have revolutionized video-based computer vision applications, including action recognition, anomaly detection, and video summarization. Videos inherently pose unique challenges, combining spatial \u2026"}, {"title": "Language Models as Continuous Self-Evolving Data Engineers", "link": "https://arxiv.org/pdf/2412.15151%3F", "details": "P Wang, M Wang, Z Ma, X Yang, S Feng, D Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert \u2026"}, {"title": "LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding", "link": "https://arxiv.org/pdf/2501.08282", "details": "H Li, J Chen, Z Wei, S Huang, T Hui, J Gao, X Wei\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in multimodal large language models (MLLMs) have shown promising results, yet existing approaches struggle to effectively handle both temporal and spatial localization simultaneously. This challenge stems from two key \u2026"}, {"title": "Understanding Before Reasoning: Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting", "link": "https://arxiv.org/pdf/2501.04341%3F", "details": "DH Zhu, YJ Xiong, JC Zhang, XJ Xie, CM Xia - arXiv preprint arXiv:2501.04341, 2025", "abstract": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language Models (LLMs) to enhance complex reasoning. It guides LLMs to present multi-step reasoning, rather than generating the final answer directly. However, CoT \u2026"}]
