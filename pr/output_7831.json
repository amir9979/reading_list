[{"title": "CREAM: Consistency Regularized Self-Rewarding Language Models", "link": "https://arxiv.org/pdf/2410.12735", "details": "Z Wang, W He, Z Liang, X Zhang, C Bansal, Y Wei\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent self-rewarding large language models (LLM) have successfully applied LLM- as-a-Judge to iteratively improve the alignment performance without the need of human annotations for preference data. These methods commonly utilize the same \u2026"}, {"title": "Small Language Models: Survey, Measurements, and Insights", "link": "https://arxiv.org/pdf/2409.15790%3F", "details": "Z Lu, X Li, D Cai, R Yi, F Liu, X Zhang, ND Lane, M Xu - arXiv preprint arXiv \u2026, 2024", "abstract": "Small language models (SLMs), despite their widespread adoption in modern smart devices, have received significantly less academic attention compared to their large language model (LLM) counterparts, which are predominantly deployed in data \u2026"}, {"title": "Ascle\u2014A Python Natural Language Processing Toolkit for Medical Text Generation: Development and Evaluation Study", "link": "https://www.jmir.org/2024/1/e60601/", "details": "R Yang, Q Zeng, K You, Y Qiao, L Huang, CC Hsieh\u2026 - Journal of Medical Internet \u2026, 2024", "abstract": "Background Medical texts present significant domain-specific challenges, and manually curating these texts is a time-consuming and labor-intensive process. To address this, natural language processing (NLP) algorithms have been developed to \u2026"}, {"title": "Identifying Task Groupings for Multi-Task Learning Using Pointwise V-Usable Information", "link": "https://arxiv.org/pdf/2410.12774", "details": "Y Li, T Miller, S Bethard, G Savova - arXiv preprint arXiv:2410.12774, 2024", "abstract": "The success of multi-task learning can depend heavily on which tasks are grouped together. Naively grouping all tasks or a random set of tasks can result in negative transfer, with the multi-task models performing worse than single-task models \u2026"}, {"title": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt Optimization for Text Generation", "link": "https://arxiv.org/pdf/2410.02748%3F", "details": "H He, Q Liu, L Xu, C Shivade, Y Zhang, S Srinivasan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) can generate fluent summaries across domains using prompting techniques, reducing the need to train models for summarization applications. However, crafting effective prompts that guide LLMs to generate \u2026"}, {"title": "Language Models as Zero-shot Lossless Gradient Compressors: Towards General Neural Parameter Prior Models", "link": "https://arxiv.org/pdf/2409.17836%3F", "details": "HP Wang, M Fritz - arXiv preprint arXiv:2409.17836, 2024", "abstract": "Despite the widespread use of statistical prior models in various fields, such models for neural network gradients have long been overlooked. The inherent challenge stems from their high-dimensional structures and complex interdependencies, which \u2026"}, {"title": "POSIX: A Prompt Sensitivity Index For Large Language Models", "link": "https://arxiv.org/pdf/2410.02185", "details": "A Chatterjee, HK Renduchintala, S Bhatia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are found to be surprisingly sensitive to minor variations in prompts, often generating significantly divergent outputs in response to minor variations in the prompts, such as spelling \u2026"}, {"title": "Transformer-based active learning for multi-class text annotation and classification", "link": "https://journals.sagepub.com/doi/pdf/10.1177/20552076241287357", "details": "M Afzal, J Hussain, A Abbas, M Hussain, M Attique\u2026 - DIGITAL HEALTH, 2024", "abstract": "Objective Data-driven methodologies in healthcare necessitate labeled data for effective decision-making. However, medical data, particularly in unstructured formats, such as clinical notes, often lack explicit labels, making manual annotation \u2026"}, {"title": "Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers", "link": "https://arxiv.org/pdf/2410.02642", "details": "S Chen, BJ Guti\u00e9rrez, Y Su - arXiv preprint arXiv:2410.02642, 2024", "abstract": "Information retrieval (IR) systems have played a vital role in modern digital life and have cemented their continued usefulness in this new era of generative AI via retrieval-augmented generation. With strong language processing capabilities and \u2026"}]
