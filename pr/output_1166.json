'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [A Multi-view Molecular Pre-training with Generative Contrast'
[{"title": "SoftMCL: Soft Momentum Contrastive Learning for Fine-grained Sentiment-aware Pre-training", "link": "https://arxiv.org/pdf/2405.01827", "details": "J Wang, LC Yu, X Zhang - arXiv preprint arXiv:2405.01827, 2024", "abstract": "The pre-training for language models captures general language understanding but fails to distinguish the affective impact of a particular context to a specific word. Recent works have sought to introduce contrastive learning (CL) for sentiment-aware \u2026"}, {"title": "Oversampling effect in pretraining for bidirectional encoder representations from transformers (BERT) to localize medical BERT and enhance biomedical BERT", "link": "https://www.sciencedirect.com/science/article/pii/S0933365724001313", "details": "S Wada, T Takeda, K Okada, S Manabe, S Konishi\u2026 - Artificial Intelligence in \u2026, 2024", "abstract": "Background Pretraining large-scale neural language models on raw texts has made a significant contribution to improving transfer learning in natural language processing. With the introduction of transformer-based language models, such as \u2026"}, {"title": "Structural Pruning of Pre-trained Language Models via Neural Architecture Search", "link": "https://arxiv.org/pdf/2405.02267", "details": "A Klein, J Golebiowski, X Ma, V Perrone\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models (PLM), for example BERT or RoBERTa, mark the state- of-the-art for natural language understanding task when fine-tuned on labeled data. However, their large size poses challenges in deploying them for inference in real \u2026"}, {"title": "Language Models Still Struggle to Zero-shot Reason about Time Series", "link": "https://arxiv.org/pdf/2404.11757", "details": "MA Merrill, M Tan, V Gupta, T Hartvigsen, T Althoff - arXiv preprint arXiv:2404.11757, 2024", "abstract": "Time series are critical for decision-making in fields like finance and healthcare. Their importance has driven a recent influx of works passing time series into language models, leading to non-trivial forecasting on some datasets. But it remains \u2026"}, {"title": "MFNet: Meta\u2010learning based on frequency\u2010space mix for MRI segmentation in nasopharyngeal carcinoma", "link": "https://onlinelibrary.wiley.com/doi/pdf/10.1111/jcmm.18355", "details": "Y Li, Q Chen, H Li, S Wang, N Chen, T Han, K Wang\u2026 - Journal of Cellular and \u2026, 2024", "abstract": "Deep learning techniques have been applied to medical image segmentation and demonstrated expert\u2010level performance. Due to the poor generalization abilities of the models in the deployment in different centres, common solutions, such as transfer \u2026"}, {"title": "Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in Radiology with General-Domain Large Language Model", "link": "https://arxiv.org/pdf/2405.01591", "details": "S Cho, C Kim, J Lee, C Chilkunda, S Choi, JH Yoon - arXiv preprint arXiv:2405.01591, 2024", "abstract": "Recent advancements in Large Multimodal Models (LMMs) have attracted interest in their generalization capability with only a few samples in the prompt. This progress is particularly relevant to the medical domain, where the quality and sensitivity of data \u2026"}, {"title": "FITA: Fine-grained Image-Text Aligner for Radiology Report Generation", "link": "https://arxiv.org/pdf/2405.00962", "details": "H Yang, H Tang, X Li - arXiv preprint arXiv:2405.00962, 2024", "abstract": "Radiology report generation aims to automatically generate detailed and coherent descriptive reports alongside radiology images. Previous work mainly focused on refining fine-grained image features or leveraging external knowledge. However, the \u2026"}]
