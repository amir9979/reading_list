[{"title": "Few-shot Relation Extraction through Prompt with Relation Information and Multi-level Contrastive Learning", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10662978.pdf", "details": "Y Dong, R Yang, J Liu, X Qin - IEEE Access, 2024", "abstract": "Few-shot relation extraction uses only limited labeled data to predict relations between entities. Recently, several studies have introduced prompts to better guide models in understanding relations between entities. Although effective, these \u2026"}, {"title": "ARC: A Layer Replacement Compression Method Based on Fine-Grained Self-Attention Distillation for Compressing Pre-Trained Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10663832/", "details": "D Yu, L Qiu - IEEE Transactions on Emerging Topics in \u2026, 2024", "abstract": "The primary objective of model compression is to maintain the performance of the original model while reducing its size as much as possible. Knowledge distillation has become the mainstream method in the field of model compression due to its \u2026"}, {"title": "AlignRE: An Encoding and Semantic Alignment Approach for Zero-Shot Relation Extraction", "link": "https://aclanthology.org/2024.findings-acl.174.pdf", "details": "Z Li, F Zhang, J Cheng - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Zero-shot Relation Extraction (ZSRE) aims to predict unseen relations between entity pairs from input sentences. Existing prototype-based ZSRE methods encode relation descriptions into prototype embeddings and predict by measuring \u2026"}, {"title": "Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models", "link": "https://arxiv.org/pdf/2409.00084", "details": "SAA Safavi-Naini, S Ali, O Shahab, Z Shahhoseini\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Background and Aims: This study evaluates the medical reasoning performance of large language models (LLMs) and vision language models (VLMs) in gastroenterology. Methods: We used 300 gastroenterology board exam-style \u2026"}, {"title": "Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis", "link": "https://arxiv.org/pdf/2409.00106", "details": "A Nagar, S Jaiswal, C Tan - arXiv preprint arXiv:2409.00106, 2024", "abstract": "Vision-language models (VLMs) have shown impressive zero-and few-shot performance on real-world visual question answering (VQA) benchmarks, alluding to their capabilities as visual reasoning engines. However, the benchmarks being used \u2026"}, {"title": "Prompt Learning with Extended Kalman Filter for Pre-trained Language Models", "link": "https://www.ijcai.org/proceedings/2024/0492.pdf", "details": "Q Li, X Xie, C Wang, SK Zhou", "abstract": "Prompt learning has gained popularity as a means to leverage the knowledge embedded in pre-trained language models (PLMs) for NLP tasks while using a limited number of trainable parameters. While it has shown promise in tasks like \u2026"}, {"title": "How Does Diverse Interpretability of Textual Prompts Impact Medical Vision-Language Zero-Shot Tasks?", "link": "https://arxiv.org/pdf/2409.00543", "details": "S Wang, C Liu, R Arcucci - arXiv preprint arXiv:2409.00543, 2024", "abstract": "Recent advancements in medical vision-language pre-training (MedVLP) have significantly enhanced zero-shot medical vision tasks such as image classification by leveraging large-scale medical image-text pair pre-training. However, the \u2026"}]
