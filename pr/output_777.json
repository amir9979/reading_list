'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Consprompt: Exploiting Contrastive Samples for Few-Shot Prom'
[{"title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis", "link": "https://arxiv.org/pdf/2404.01204", "details": "C Yang, J Li, X Niu, X Du, S Gao, H Zhang, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Uncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining. The existing scaling law demonstrates the power- law correlation between pretraining loss and training flops, which serves as an \u2026"}]
