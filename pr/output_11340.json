[{"title": "A few-shot learning method based on knowledge graph in large language models", "link": "https://link.springer.com/article/10.1007/s41060-024-00699-3", "details": "FL Wang, D Shi, J Aguilar, X Cui - International Journal of Data Science and Analytics, 2024", "abstract": "The emergence of large language models has significantly transformed natural language processing and text generation. Fine-tuning these models for specific domains enables them to generate answers tailored to the unique requirements of \u2026"}, {"title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use", "link": "https://arxiv.org/pdf/2501.02506", "details": "J Ye, Z Du, X Yao, W Lin, Y Xu, Z Chen, Z Wang, S Zhu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To \u2026"}, {"title": "Stability Evaluation of Large Language Models via Distributional Perturbation Analysis", "link": "https://openreview.net/pdf%3Fid%3DcXADLtYkQq", "details": "J Liu, J Li, P Cui, J Blanchet - Red Teaming GenAI: What Can We Learn from \u2026", "abstract": "The performance of Large Language Models (LLMs) can degrade when exposed to shifts such as changes in language style or domain-specific knowledge that is underrepresented in the training data. To ensure robust deployment, we propose a \u2026"}]
