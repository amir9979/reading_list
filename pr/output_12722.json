[{"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}, {"title": "Teaching Language Models to Critique via Reinforcement Learning", "link": "https://arxiv.org/pdf/2502.03492", "details": "Z Xie, L Chen, W Mao, J Xu, L Kong - arXiv preprint arXiv:2502.03492, 2025", "abstract": "Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide accurate judgments and actionable suggestions. In this work, we \u2026"}, {"title": "Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective", "link": "https://arxiv.org/pdf/2502.01524%3F", "details": "X Ma, H Xie, SJ Qin - arXiv preprint arXiv:2502.01524, 2025", "abstract": "The integration of vision-language modalities has been a significant focus in multimodal learning, traditionally relying on Vision-Language Pretrained Models. However, with the advent of Large Language Models (LLMs), there has been a \u2026"}, {"title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning", "link": "https://arxiv.org/pdf/2501.18858", "details": "H Zhong, Y Yin, S Zhang, X Xu, Y Liu, Y Zuo, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, yet generating reliable reasoning processes remains a significant challenge. We present a unified probabilistic framework that formalizes \u2026"}, {"title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2501.14851", "details": "MK Chen, X Zhang, D Tao - arXiv preprint arXiv:2501.14851, 2025", "abstract": "Logical reasoning is a critical component of Large Language Models (LLMs), and substantial research efforts in recent years have aimed to enhance their deductive reasoning capabilities. However, existing deductive reasoning benchmarks, which \u2026"}, {"title": "LLM-BS: Enhancing Large Language Models for Recommendation through Exogenous Behavior-Semantics Integration", "link": "https://openreview.net/pdf%3Fid%3Drm07DoACiF", "details": "M Hong, Y Xia, Z Wang, J Zhu, Y Wang, S Cai, X Yang\u2026 - THE WEB CONFERENCE 2025", "abstract": "Large language models (LLMs) are increasingly leveraged as foundational backbones in the development of advanced recommender systems, offering enhanced capabilities through their extensive knowledge and reasoning. Existing \u2026"}, {"title": "How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence", "link": "https://arxiv.org/pdf/2502.00678", "details": "HK Choi, M Khanov, H Wei, Y Li - arXiv preprint arXiv:2502.00678, 2025", "abstract": "Dataset contamination, where evaluation datasets overlap with pre-training corpora, inflates performance metrics and undermines the reliability of model evaluations. Quantifying dataset contamination thus becomes essential to ensure that \u2026"}, {"title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models", "link": "https://arxiv.org/pdf/2501.17905", "details": "M Feng, J Wu, S Zhang, P Shao, R Jin, Z Wen, J Tao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have achieved significant progress across various domains, but their increasing scale results in high computational and memory costs. Recent studies have revealed that LLMs exhibit sparsity, providing the potential to \u2026"}, {"title": "Unlocking the Mysteries of OpenAI o1: A Survey of the Reasoning Abilities of Large Language Models", "link": "https://openreview.net/pdf%3Fid%3DJ0ADLa2rNp", "details": "G Wang, S Zhang, T Zhan, Z Shen, J Li, X Hu, X Sun\u2026", "abstract": "The release of OpenAI's o1 marks a significant milestone in AI, achieving proficiency comparable to PhD-level expertise in mathematics and coding. While o1 excels at solving complex reasoning tasks, it remains a closed-resource model, limiting its \u2026"}]
