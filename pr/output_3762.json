[{"title": "DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning", "link": "https://arxiv.org/pdf/2406.17659", "details": "X Zhang, Z Altaweel, Y Hayamizu, Y Ding, S Amiri\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language models (VLMs) have been applied to robot task planning problems, where the robot receives a task in natural language and generates plans based on visual inputs. While current VLMs have demonstrated strong vision-language \u2026"}, {"title": "GenderBias-\\emph {VL}: Benchmarking Gender Bias in Vision Language Models via Counterfactual Probing", "link": "https://arxiv.org/pdf/2407.00600", "details": "Y Xiao, A Liu, QJ Cheng, Z Yin, S Liang, J Li, J Shao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) have been widely adopted in various applications; however, they exhibit significant gender biases. Existing benchmarks primarily evaluate gender bias at the demographic group level, neglecting individual \u2026"}, {"title": "MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models", "link": "https://arxiv.org/pdf/2407.02775", "details": "Y Zhang, Z Yang, S Ji - arXiv preprint arXiv:2407.02775, 2024", "abstract": "Knowledge distillation is an effective technique for pre-trained language model compression. Although existing knowledge distillation methods perform well for the most typical model BERT, they could be further improved in two aspects: the relation \u2026"}, {"title": "Information Guided Regularization for Fine-tuning Language Models", "link": "https://arxiv.org/pdf/2406.14005", "details": "M Sharma, N Muralidhar, S Xu, RB Yosuf\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The pretraining-fine-tuning paradigm has been the de facto strategy for transfer learning in modern language modeling. With the understanding that task adaptation in LMs is often a function of parameters shared across tasks, we argue that a more \u2026"}, {"title": "ViGLUE: A Vietnamese General Language Understanding Benchmark and Analysis of Vietnamese Language Models", "link": "https://aclanthology.org/2024.findings-naacl.261.pdf", "details": "MN Tran, PV Nguyen, L Nguyen, D Dien - Findings of the Association for \u2026, 2024", "abstract": "As the number of language models has increased, various benchmarks have been suggested to assess the proficiency of the models in natural language understanding. However, there is a lack of such a benchmark in Vietnamese due to \u2026"}, {"title": "Large Language Model as a Universal Clinical Multi-task Decoder", "link": "https://arxiv.org/pdf/2406.12738", "details": "Y Wu, H Song, J Zhang, X Wen, S Zheng, J Bian - arXiv preprint arXiv:2406.12738, 2024", "abstract": "The development of effective machine learning methodologies for enhancing the efficiency and accuracy of clinical systems is crucial. Despite significant research efforts, managing a plethora of diversified clinical tasks and adapting to emerging \u2026"}, {"title": "Enhancing text-based knowledge graph completion with zero-shot large language models: A focus on semantic enhancement", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124007895", "details": "R Yang, J Zhu, J Man, L Fang, Y Zhou - Knowledge-Based Systems, 2024", "abstract": "The design and development of text-based knowledge graph completion (KGC) methods leveraging textual entity descriptions are at the forefront of research. These methods involve advanced optimization techniques such as soft prompts and \u2026"}, {"title": "Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation", "link": "https://arxiv.org/pdf/2407.03056", "details": "M Mistretta, A Baldrati, M Bertini, AD Bagdanov - arXiv preprint arXiv:2407.03056, 2024", "abstract": "Vision-Language Models (VLMs) demonstrate remarkable zero-shot generalization to unseen tasks, but fall short of the performance of supervised methods in generalizing to downstream tasks with limited data. Prompt learning is emerging as a \u2026"}, {"title": "ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks", "link": "https://aclanthology.org/2024.findings-naacl.85.pdf", "details": "X Yu, H Cheng, X Liu, D Roth, J Gao - Findings of the Association for Computational \u2026, 2024", "abstract": "Despite remarkable advancements in mitigating hallucinations in large language models (LLMs) by retrieval augmentation, it remains challenging to measure the reliability of LLMs using static question-answering (QA) data. Specifically, given the \u2026"}]
