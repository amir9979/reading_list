[{"title": "Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation", "link": "https://arxiv.org/pdf/2506.17088", "details": "J Cheng, T Su, J Yuan, G He, J Liu, X Tao, J Xie, H Li - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) often exhibit\\textit {hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of- Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step \u2026", "entry_id": "http://arxiv.org/abs/2506.17088v1", "updated": "2025-06-20 15:49:37", "published": "2025-06-20 15:49:37", "authors": "Jiahao Cheng;Tiancheng Su;Jia Yuan;Guoxiu He;Jiawei Liu;Xinqi Tao;Jingwen Xie;Huaxia Li", "summary": "Large Language Models (LLMs) often exhibit \\textit{hallucinations},\ngenerating factually incorrect or semantically irrelevant content in response\nto prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by\nencouraging step-by-step reasoning, but its impact on hallucination detection\nremains underexplored. To bridge this gap, we conduct a systematic empirical\nevaluation. We begin with a pilot experiment, revealing that CoT reasoning\nsignificantly affects the LLM's internal states and token probability\ndistributions. Building on this, we evaluate the impact of various CoT\nprompting methods on mainstream hallucination detection methods across both\ninstruction-tuned and reasoning-oriented LLMs. Specifically, we examine three\nkey dimensions: changes in hallucination score distributions, variations in\ndetection accuracy, and shifts in detection confidence. Our findings show that\nwhile CoT prompting helps reduce hallucination frequency, it also tends to\nobscure critical signals used for detection, impairing the effectiveness of\nvarious detection methods. Our study highlights an overlooked trade-off in the\nuse of reasoning. Code is publicly available at:\nhttps://anonymous.4open.science/r/cot-hallu-detect.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.17088v1;http://arxiv.org/pdf/2506.17088v1", "pdf_url": "http://arxiv.org/pdf/2506.17088v1"}, {"title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models", "link": "https://arxiv.org/pdf/2506.17585", "details": "Y Huang, S Chen, J Pei, M Zaheer, B Dhingra - arXiv preprint arXiv:2506.17585, 2025", "abstract": "Trustworthy language models should provide both correct and verifiable answers. While language models can sometimes attribute their outputs to pretraining data, their citations are often unreliable due to hallucination. As a result, current systems \u2026", "entry_id": "http://arxiv.org/abs/2506.17585v1", "updated": "2025-06-21 04:48:05", "published": "2025-06-21 04:48:05", "authors": "Yukun Huang;Sanxing Chen;Jian Pei;Manzil Zaheer;Bhuwan Dhingra", "summary": "Trustworthy language models should provide both correct and verifiable\nanswers. While language models can sometimes attribute their outputs to\npretraining data, their citations are often unreliable due to hallucination. As\na result, current systems insert citations by querying an external retriever at\ninference time, introducing latency, infrastructure dependence, and\nvulnerability to retrieval noise. We explore whether LLMs can be made to\nreliably attribute to the documents seen during (continual)\npretraining--without test-time retrieval--by revising the training process. To\nevaluate this, we release CitePretrainBench, a benchmark that mixes real-world\ncorpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and\nprobes both short-form (single fact) and long-form (multi-fact) citation tasks.\nOur approach follows a two-stage process: (1) continual pretraining to bind\nfacts to persistent document identifiers, and (2) instruction tuning to elicit\ncitation behavior. We find that simple Passive Indexing, which appends an\nidentifier to each document, helps memorize verbatim text but fails on\nparaphrased or compositional facts. Instead, we propose Active Indexing, which\ncontinually pretrains on synthetic QA pairs that (1) restate each fact in\ndiverse compositional forms, and (2) require bidirectional source-to-fact and\nfact-to-source generation, jointly teaching the model to generate content from\na cited source and to attribute its own answers. Experiments with Qwen2.5-7B\nand 3B show that Active Indexing consistently outperforms Passive Indexing\nacross all tasks and models, with citation precision gains up to 30.2 percent.\nOur ablation studies reveal that performance continues to improve as we scale\nthe amount of augmented data, showing a clear upward trend even at 16 times the\noriginal token count.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL;cs.LG", "links": "http://arxiv.org/abs/2506.17585v1;http://arxiv.org/pdf/2506.17585v1", "pdf_url": "http://arxiv.org/pdf/2506.17585v1"}, {"title": "PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries", "link": "https://arxiv.org/pdf/2506.18728", "details": "S Kolawole, K Santhanam, V Smith, P Thaker - arXiv preprint arXiv:2506.18728, 2025", "abstract": "LLM serving systems typically treat user prompts as monolithic inputs, optimizing inference through decoding tricks or inter-query batching. However, many real-world prompts contain latent semantic parallelism--decomposable structures where \u2026", "entry_id": "http://arxiv.org/abs/2506.18728v2", "updated": "2025-06-26 16:35:54", "published": "2025-06-23 15:05:54", "authors": "Steven Kolawole;Keshav Santhanam;Virginia Smith;Pratiksha Thaker", "summary": "LLM serving systems typically treat user prompts as monolithic inputs,\noptimizing inference through decoding tricks or inter-query batching. However,\nmany real-world prompts contain latent semantic parallelism--decomposable\nstructures where subtasks can be executed independently to reduce latency while\npreserving meaning. We introduce PARALLELPROMPT, the first benchmark for\nmeasuring intra-query parallelism in natural user prompts. Our dataset\ncomprises over 37,000 real-world prompts from public LLM chat logs, each\nannotated with a structured schema capturing task templates, shared context,\nand iteration inputs. These schemas are extracted using LLM-assisted prompting\nwith rule-based multilingual validation. To evaluate the benefits of\ndecomposition, we provide an execution suite that benchmarks serial vs.\nparallel strategies, measuring latency, structural adherence, and semantic\nfidelity. Our results show that intra-query parallelism can be successfully\nparsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks\nlike translation, comprehension, and comparative analysis, with minimal quality\ndegradation. By releasing this benchmark, curation pipeline, and evaluation\nsuite, we provide the first standardized testbed for studying structure-aware\nexecution in LLM serving pipelines.", "comment": "In Adaptive Foundation Models: Evolving AI for Personalized and\n  Efficient Learning", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2506.18728v2;http://arxiv.org/pdf/2506.18728v2", "pdf_url": "http://arxiv.org/pdf/2506.18728v2"}, {"title": "LLMs Can Reason Faster Only If We Let Them", "link": "https://openreview.net/pdf%3Fid%3DuTv5rOPZr4", "details": "B Sel, L Huang, N Ramakrishnan, R Jia, M Jin - Forty-second International Conference on \u2026", "abstract": "Large language models (LLMs) are making inroads into classical AI problems such as automated planning, yet key shortcomings continue to hamper their integration. Chain-of-Thought (CoT) struggles in complex multi-step reasoning, and Tree-of \u2026"}, {"title": "RiOT: Efficient Prompt Refinement with Residual Optimization Tree", "link": "https://arxiv.org/pdf/2506.16389", "details": "C Zhou, Z Shi, Y Yao, L Liang, H Chen, Q Zhang - arXiv preprint arXiv:2506.16389, 2025", "abstract": "Recent advancements in large language models (LLMs) have highlighted their potential across a variety of tasks, but their performance still heavily relies on the design of effective prompts. Existing methods for automatic prompt optimization face \u2026", "entry_id": "http://arxiv.org/abs/2506.16389v1", "updated": "2025-06-19 15:19:56", "published": "2025-06-19 15:19:56", "authors": "Chenyi Zhou;Zhengyan Shi;Yuan Yao;Lei Liang;Huajun Chen;Qiang Zhang", "summary": "Recent advancements in large language models (LLMs) have highlighted their\npotential across a variety of tasks, but their performance still heavily relies\non the design of effective prompts. Existing methods for automatic prompt\noptimization face two challenges: lack of diversity, limiting the exploration\nof valuable and innovative directions and semantic drift, where optimizations\nfor one task can degrade performance in others. To address these issues, we\npropose Residual Optimization Tree (RiOT), a novel framework for automatic\nprompt optimization. RiOT iteratively refines prompts through text gradients,\ngenerating multiple semantically diverse candidates at each step, and selects\nthe best prompt using perplexity. Additionally, RiOT incorporates the text\nresidual connection to mitigate semantic drift by selectively retaining\nbeneficial content across optimization iterations. A tree structure efficiently\nmanages the optimization process, ensuring scalability and flexibility.\nExtensive experiments across five benchmarks, covering commonsense,\nmathematical, logical, temporal, and semantic reasoning, demonstrate that RiOT\noutperforms both previous prompt optimization methods and manual prompting.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.16389v1;http://arxiv.org/pdf/2506.16389v1", "pdf_url": "http://arxiv.org/pdf/2506.16389v1"}, {"title": "Deep Research Agents: A Systematic Examination And Roadmap", "link": "https://arxiv.org/pdf/2506.18096", "details": "Y Huang, Y Chen, H Zhang, K Li, M Fang, L Yang, X Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The rapid progress of Large Language Models (LLMs) has given rise to a new category of autonomous AI systems, referred to as Deep Research (DR) agents. These agents are designed to tackle complex, multi-turn informational research tasks \u2026", "entry_id": "http://arxiv.org/abs/2506.18096v1", "updated": "2025-06-22 16:52:48", "published": "2025-06-22 16:52:48", "authors": "Yuxuan Huang;Yihang Chen;Haozheng Zhang;Kang Li;Meng Fang;Linyi Yang;Xiaoguang Li;Lifeng Shang;Songcen Xu;Jianye Hao;Kun Shao;Jun Wang", "summary": "The rapid progress of Large Language Models (LLMs) has given rise to a new\ncategory of autonomous AI systems, referred to as Deep Research (DR) agents.\nThese agents are designed to tackle complex, multi-turn informational research\ntasks by leveraging a combination of dynamic reasoning, adaptive long-horizon\nplanning, multi-hop information retrieval, iterative tool use, and the\ngeneration of structured analytical reports. In this paper, we conduct a\ndetailed analysis of the foundational technologies and architectural components\nthat constitute Deep Research agents. We begin by reviewing information\nacquisition strategies, contrasting API-based retrieval methods with\nbrowser-based exploration. We then examine modular tool-use frameworks,\nincluding code execution, multimodal input processing, and the integration of\nModel Context Protocols (MCPs) to support extensibility and ecosystem\ndevelopment. To systematize existing approaches, we propose a taxonomy that\ndifferentiates between static and dynamic workflows, and we classify agent\narchitectures based on planning strategies and agent composition, including\nsingle-agent and multi-agent configurations. We also provide a critical\nevaluation of current benchmarks, highlighting key limitations such as\nrestricted access to external knowledge, sequential execution inefficiencies,\nand misalignment between evaluation metrics and the practical objectives of DR\nagents. Finally, we outline open challenges and promising directions for future\nresearch. A curated and continuously updated repository of DR agent research is\navailable at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2506.18096v1;http://arxiv.org/pdf/2506.18096v1", "pdf_url": "http://arxiv.org/pdf/2506.18096v1"}, {"title": "EQuARX: Efficient Quantized AllReduce in XLA for Distributed Machine Learning Acceleration", "link": "https://arxiv.org/pdf/2506.17615", "details": "I Ahmed, C Schaefer, G Tabak, D Vnukov, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While Large Language Models (LLMs) have become highly influential, their enormous scale presents significant deployment challenges. Efficiently serving these models typically requires distributing them across numerous accelerator devices \u2026", "entry_id": "http://arxiv.org/abs/2506.17615v1", "updated": "2025-06-21 06:54:52", "published": "2025-06-21 06:54:52", "authors": "Ibrahim Ahmed;Clemens Schaefer;Gil Tabak;Denis Vnukov;Zenong Zhang;Felix chern;Anatoliy Yevtushenko;Andy Davis", "summary": "While Large Language Models (LLMs) have become highly influential, their\nenormous scale presents significant deployment challenges. Efficiently serving\nthese models typically requires distributing them across numerous accelerator\ndevices, which introduces substantial performance overhead from inter-device\ncommunication (collectives). While model quantization has been widely adopted\nto reduce the memory and compute requirements of LLM weights and activations\nwith minimal quality impact, applying quantization directly to collectives like\nAllReduce is inherently difficult due to the inter-device summation involved,\nwhich can lead to numerical instability or significant error accumulation. In\nthis work, we present a native dynamic block-wise efficient quantized AllReduce\nwithin the XLA compiler for TPUs (EQuARX). By using TPU-friendly quantization\nand deep pipelining of communication and compute, EQuARX with int8 precision\nachieves a 1.8X speedup over baseline BF16 AllReduce across various network\ntopologies. Furthermore, EQuARX accelerates the prefill stage of Gemma 3 27B by\n1.25X and Gemma 3 12B by 1.1X, respectively, with small to negligible impact on\nquality.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2506.17615v1;http://arxiv.org/pdf/2506.17615v1", "pdf_url": "http://arxiv.org/pdf/2506.17615v1"}, {"title": "Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models", "link": "https://arxiv.org/pdf/2505.23667", "details": "L Cao, J Xu, H Liu, J Wang, M Zhou, H Dong, S Han\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they \u2026", "entry_id": "http://arxiv.org/abs/2505.23667v2", "updated": "2025-05-31 07:32:09", "published": "2025-05-29 17:13:40", "authors": "Lang Cao;Jingxian Xu;Hanbing Liu;Jinyu Wang;Mengyu Zhou;Haoyu Dong;Shi Han;Dongmei Zhang", "summary": "Tables are a fundamental structure for organizing and analyzing data, making\neffective table understanding a critical capability for intelligent systems.\nWhile large language models (LMs) demonstrate strong general reasoning\nabilities, they continue to struggle with accurate numerical or symbolic\nreasoning over tabular data, especially in complex scenarios. Spreadsheet\nformulas provide a powerful and expressive medium for representing executable\nsymbolic operations, encoding rich reasoning patterns that remain largely\nunderutilized. In this paper, we propose Formula Tuning (Fortune), a\nreinforcement learning (RL) framework that trains LMs to generate executable\nspreadsheet formulas for question answering over general tabular data. Formula\nTuning reduces the reliance on supervised formula annotations by using binary\nanswer correctness as a reward signal, guiding the model to learn formula\nderivation through reasoning. We provide a theoretical analysis of its\nadvantages and demonstrate its effectiveness through extensive experiments on\nseven table reasoning benchmarks. Formula Tuning substantially enhances LM\nperformance, particularly on multi-step numerical and symbolic reasoning tasks,\nenabling a 7B model to outperform OpenAI o1 on table understanding. This\nhighlights the potential of formula-driven RL to advance symbolic table\nreasoning in LMs.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.23667v2;http://arxiv.org/pdf/2505.23667v2", "pdf_url": "http://arxiv.org/pdf/2505.23667v2"}, {"title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?", "link": "https://arxiv.org/pdf/2506.18183", "details": "Z Mei, C Zhang, T Yin, J Lidard, O Shorinwa\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models \u2026", "entry_id": "http://arxiv.org/abs/2506.18183v1", "updated": "2025-06-22 21:46:42", "published": "2025-06-22 21:46:42", "authors": "Zhiting Mei;Christina Zhang;Tenny Yin;Justin Lidard;Ola Shorinwa;Anirudha Majumdar", "summary": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL", "links": "http://arxiv.org/abs/2506.18183v1;http://arxiv.org/pdf/2506.18183v1", "pdf_url": "http://arxiv.org/pdf/2506.18183v1"}]
