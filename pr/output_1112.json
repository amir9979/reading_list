"*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Integrating learners' knowledge background to improve course"
[{"title": "Masked Modeling Duo: Towards a Universal Audio Pre-Training Framework", "link": "https://ieeexplore.ieee.org/iel7/6570655/6633080/10502167.pdf", "details": "D Niizumi, D Takeuchi, Y Ohishi, N Harada, K Kashino - IEEE/ACM Transactions on \u2026, 2024", "abstract": "Self-supervised learning (SSL) using masked prediction has made great strides in general-purpose audio representation. This study proposes Masked Modeling Duo (M2D), an improved masked prediction SSL, which learns by predicting \u2026"}, {"title": "Comprehensive Study on German Language Models for Clinical and Biomedical Text Understanding", "link": "https://arxiv.org/pdf/2404.05694", "details": "A Idrissi-Yaghir, A Dada, H Sch\u00e4fer, K Arzideh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in natural language processing (NLP) can be largely attributed to the advent of pre-trained language models such as BERT and RoBERTa. While these models demonstrate remarkable performance on general datasets, they can \u2026"}, {"title": "Stepwise incremental pretraining for integrating discriminative, restorative, and adversarial learning", "link": "https://www.sciencedirect.com/science/article/pii/S1361841524000847", "details": "Z Guo, NU Islam, MB Gotway, J Liang - Medical Image Analysis, 2024", "abstract": "We have developed a United framework that integrates three self-supervised learning (SSL) ingredients (discriminative, restorative, and adversarial learning), enabling collaborative learning among the three learning ingredients and yielding \u2026"}, {"title": "EventLens: Leveraging Event-Aware Pretraining and Cross-modal Linking Enhances Visual Commonsense Reasoning", "link": "https://arxiv.org/pdf/2404.13847", "details": "M Ma, Z Yu, Y Ma, G Li - arXiv preprint arXiv:2404.13847, 2024", "abstract": "Visual Commonsense Reasoning (VCR) is a cognitive task, challenging models to answer visual questions requiring human commonsense, and to provide rationales explaining why the answers are correct. With emergence of Large Language Models \u2026"}, {"title": "HLAT: High-quality Large Language Model Pre-trained on AWS Trainium", "link": "https://arxiv.org/pdf/2404.10630", "details": "H Fan, H Zhou, G Huang, P Raman, X Fu, G Gupta\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Getting large language models (LLMs) to perform well on the downstream tasks requires pre-training over trillions of tokens. This typically demands a large number of powerful computational devices in addition to a stable distributed training \u2026"}, {"title": "MetaRM: Shifted Distributions Alignment via Meta-Learning", "link": "https://arxiv.org/pdf/2405.00438", "details": "S Dou, Y Liu, E Zhou, T Li, H Jia, L Xiong, X Zhao, J Ye\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The success of Reinforcement Learning from Human Feedback (RLHF) in language model alignment is critically dependent on the capability of the reward model (RM). However, as the training process progresses, the output distribution of the policy \u2026"}, {"title": "Knowledge-augmented Methods for Natural Language Understanding", "link": "https://link.springer.com/chapter/10.1007/978-981-97-0747-8_3", "details": "M Jiang, BY Lin, S Wang, Y Xu, W Yu, C Zhu - Knowledge-augmented Methods for \u2026, 2024", "abstract": "This chapter delves into the emerging domain of knowledge-augmented natural language understanding (NLU), an essential aspect of natural language processing. The integration of external knowledge sources with pretrained language models is \u2026"}]
