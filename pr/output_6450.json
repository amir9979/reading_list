[{"title": "BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems", "link": "https://arxiv.org/pdf/2408.15971", "details": "W Wang, D Zhang, T Feng, B Wang, J Tang - arXiv preprint arXiv:2408.15971, 2024", "abstract": "Large Language Models (LLMs) are becoming increasingly powerful and capable of handling complex tasks, eg, building single agents and multi-agent systems. Compared to single agents, multi-agent systems have higher requirements for the \u2026"}, {"title": "Length Desensitization in Directed Preference Optimization", "link": "https://arxiv.org/pdf/2409.06411", "details": "W Liu, Y Bai, C Han, R Weng, J Xu, X Cao, J Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Direct Preference Optimization (DPO) is widely utilized in the Reinforcement Learning from Human Feedback (RLHF) phase to align Large Language Models (LLMs) with human preferences, thereby enhancing both their harmlessness and \u2026"}, {"title": "Factual and Tailored Recommendation Endorsements using Language Models and Reinforcement Learning", "link": "https://openreview.net/pdf%3Fid%3DxI8C7sfN1H", "details": "J Jeong, Y Chow, G Tennenholtz, CW Hsu\u2026 - First Conference on Language \u2026", "abstract": "Recommender systems (RSs) play a central role in matching candidate items to users based on their preferences. While traditional RSs rely on user feed-back signals, conversational RSs interact with users in natural language. In this work, we \u2026"}, {"title": "Language Models as Reasoners for Out-of-Distribution Detection", "link": "https://link.springer.com/chapter/10.1007/978-3-031-68738-9_30", "details": "K Kirchheim, F Ortmeier - \u2026 Conference on Computer Safety, Reliability, and \u2026, 2024", "abstract": "Deep neural networks (DNNs) are prone to making wrong predictions with high confidence for data that does not stem from their training distribution. Consequentially, out-of-distribution (OOD) detection is important in safety-critical \u2026"}, {"title": "Safety Layers of Aligned Large Language Models: The Key to LLM Security", "link": "https://arxiv.org/pdf/2408.17003", "details": "S Li, L Yao, L Zhang, Y Li - arXiv preprint arXiv:2408.17003, 2024", "abstract": "Aligned LLMs are highly secure, capable of recognizing and refusing to answer malicious questions. However, the role of internal parameters in maintaining this security is not well understood, further these models are vulnerable to security \u2026"}, {"title": "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models", "link": "https://arxiv.org/pdf/2408.14866", "details": "H Liu, Y Xie, Y Wang, M Shieh - arXiv preprint arXiv:2408.14866, 2024", "abstract": "Language Language Models (LLMs) face safety concerns due to potential misuse by malicious users. Recent red-teaming efforts have identified adversarial suffixes capable of jailbreaking LLMs using the gradient-based search algorithm Greedy \u2026"}, {"title": "Focused Large Language Models are Stable Many-Shot Learners", "link": "https://arxiv.org/pdf/2408.13987", "details": "P Yuan, S Feng, Y Li, X Wang, Y Zhang, C Tan, B Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL \u2026"}, {"title": "Language Models Pre-training", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-65647-7_2.pdf", "details": "U Kamath, K Keenan, G Somers, S Sorenson - Large Language Models: A Deep Dive \u2026, 2024", "abstract": "Pre-training forms the foundation for LLMs' capabilities. LLMs gain vital language comprehension and generative language skills by using large-scale datasets. The size and quality of these datasets are essential for maximizing LLMs' potential. It is \u2026"}, {"title": "Towards a Unified View of Preference Learning for Large Language Models: A Survey", "link": "https://arxiv.org/pdf/2409.02795", "details": "B Gao, F Song, Y Miao, Z Cai, Z Yang, L Chen, H Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of the crucial factors to achieve success is aligning the LLM's output with human preferences. This alignment process often requires only a small amount of data to \u2026"}]
