'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [FairPair: A Robust Evaluation of Biases in Language Mo'
[{"title": "CodecLM: Aligning Language Models with Tailored Synthetic Data", "link": "https://arxiv.org/pdf/2404.05875", "details": "Z Wang, CL Li, V Perot, LT Le, J Miao, Z Zhang, CY Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Instruction tuning has emerged as the key in aligning large language models (LLMs) with specific task instructions, thereby mitigating the discrepancy between the next- token prediction objective and users' actual goals. To reduce the labor and time cost \u2026"}, {"title": "Artificial intelligence\u2010assisted automated heart failure detection and classification from electronic health records", "link": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/ehf2.14828", "details": "MM Oo, C Gao, C Cole, Y Hummel, M Guignard\u2010Duff\u2026 - ESC Heart Failure, 2024", "abstract": "Aims Electronic health records (EHR) linked to Digital Imaging and Communications in Medicine (DICOM), biological specimens, and deep learning (DL) algorithms could potentially improve patient care through automated case detection and \u2026"}, {"title": "Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think", "link": "https://arxiv.org/pdf/2404.08382", "details": "X Wang, C Hu, B Ma, P R\u00f6ttger, B Plank - arXiv preprint arXiv:2404.08382, 2024", "abstract": "Multiple choice questions (MCQs) are commonly used to evaluate the capabilities of large language models (LLMs). One common way to evaluate the model response is to rank the candidate answers based on the log probability of the first token \u2026"}, {"title": "Calibrating Language Models With Adaptive Temperature Scaling", "link": "https://openreview.net/pdf%3Fid%3DBgfGqNpoMi", "details": "J Xie, AS Chen, Y Lee, E Mitchell, C Finn - ICLR 2024 Workshop on Secure and Trustworthy \u2026", "abstract": "The effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration\u2014how well their confidence scores reflect the probability of their outputs being correct. While \u2026"}, {"title": "Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language Selection and Cross-lingual Self-Distillation", "link": "https://arxiv.org/pdf/2404.08491", "details": "H Zhao, Z Cai, S Si, L Chen, Y He, K An, B Chang - arXiv preprint arXiv:2404.08491, 2024", "abstract": "Large-scale multilingual Pretrained Language Models (mPLMs) yield impressive performance on cross-language tasks, yet significant performance disparities exist across different languages within the same mPLM. Previous studies endeavored to \u2026"}, {"title": "Measuring Cross-lingual Transfer in Bytes", "link": "https://arxiv.org/pdf/2404.08191", "details": "LR de Souza, TS Almeida, R Lotufo, R Nogueira - arXiv preprint arXiv:2404.08191, 2024", "abstract": "Multilingual pretraining has been a successful solution to the challenges posed by the lack of resources for languages. These models can transfer knowledge to target languages with minimal or no examples. Recent research suggests that monolingual \u2026"}, {"title": "XNLIeu: a dataset for cross-lingual NLI in Basque", "link": "https://arxiv.org/pdf/2404.06996", "details": "M Heredia, J Etxaniz, M Zulaika, X Saralegi, J Barnes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "XNLI is a popular Natural Language Inference (NLI) benchmark widely used to evaluate cross-lingual Natural Language Understanding (NLU) capabilities across languages. In this paper, we expand XNLI to include Basque, a low-resource \u2026"}, {"title": "Language Imbalance Can Boost Cross-lingual Generalisation", "link": "https://arxiv.org/pdf/2404.07982", "details": "A Sch\u00e4fer, S Ravfogel, T Hofmann, T Pimentel, I Schlag - arXiv preprint arXiv \u2026, 2024", "abstract": "Multilinguality is crucial for extending recent advancements in language modelling to diverse linguistic communities. To maintain high performance while representing multiple languages, multilingual models ideally align representations, allowing what \u2026"}, {"title": "LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding", "link": "https://arxiv.org/pdf/2404.05225", "details": "C Luo, Y Shen, Z Zhu, Q Zheng, Z Yu, C Yao - arXiv preprint arXiv:2404.05225, 2024", "abstract": "Recently, leveraging large language models (LLMs) or multimodal large language models (MLLMs) for document understanding has been proven very promising. However, previous works that employ LLMs/MLLMs for document understanding \u2026"}]
