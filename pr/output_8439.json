[{"title": "On Unsupervised Prompt Learning for Classification with Black-box Language Models", "link": "https://arxiv.org/pdf/2410.03124", "details": "ZY Zhang, J Zhang, H Yao, G Niu, M Sugiyama - arXiv preprint arXiv:2410.03124, 2024", "abstract": "Large language models (LLMs) have achieved impressive success in text-formatted learning problems, and most popular LLMs have been deployed in a black-box fashion. Meanwhile, fine-tuning is usually necessary for a specific downstream task \u2026"}, {"title": "Unraveling cross-modality knowledge conflict in large vision-language models", "link": "https://arxiv.org/pdf/2410.03659", "details": "T Zhu, Q Liu, F Wang, Z Tu, M Chen - arXiv preprint arXiv:2410.03659, 2024", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities for capturing and reasoning over multimodal inputs. However, these models are prone to parametric knowledge conflicts, which arise from inconsistencies of \u2026"}, {"title": "DEPT: Decoupled Embeddings for Pre-training Language Models", "link": "https://arxiv.org/pdf/2410.05021", "details": "A Iacob, L Sani, M Kurmanji, WF Shen, X Qiu, D Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language Model pre-training benefits from a broader data mixture to enhance performance across domains and languages. However, training on such heterogeneous text corpora is complex, requiring extensive and cost-intensive \u2026"}, {"title": "CREAM: Consistency Regularized Self-Rewarding Language Models", "link": "https://arxiv.org/pdf/2410.12735%3F", "details": "Z Wang, W He, Z Liang, X Zhang, C Bansal, Y Wei\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent self-rewarding large language models (LLM) have successfully applied LLM- as-a-Judge to iteratively improve the alignment performance without the need of human annotations for preference data. These methods commonly utilize the same \u2026"}, {"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "link": "https://arxiv.org/pdf/2410.05639", "details": "R Zhao, ZL Thai, Y Zhang, S Hu, Y Ba, J Zhou, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the \u2026"}, {"title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks", "link": "https://arxiv.org/pdf/2410.05160%3F", "details": "Z Jiang, R Meng, X Yang, S Yavuz, Y Zhou, W Chen - arXiv preprint arXiv:2410.05160, 2024", "abstract": "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize \u2026"}, {"title": "NSSC: a neuro-symbolic AI system for enhancing accuracy of named entity recognition and linking from oncologic clinical notes", "link": "https://link.springer.com/article/10.1007/s11517-024-03227-4", "details": "\u00c1 Garc\u00eda-Barrag\u00e1n, A Sakor, ME Vidal, E Menasalvas\u2026 - Medical & Biological \u2026, 2024", "abstract": "Accurate recognition and linking of oncologic entities in clinical notes is essential for extracting insights across cancer research, patient care, clinical decision-making, and treatment optimization. We present the Neuro-Symbolic System for Cancer \u2026"}, {"title": "Sleep apnea test prediction based on Electronic Health Records", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001552", "details": "LA Tahoun, AS Green, T Patalon, Y Dagan\u2026 - Journal of Biomedical \u2026, 2024", "abstract": "Abstract The identification of Obstructive Sleep Apnea (OSA) is done by a Polysomnography test which is often done in later ages. Being able to notify potential insured members at earlier ages is desirable. For that, we develop predictive models \u2026"}, {"title": "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "link": "https://arxiv.org/pdf/2410.05077", "details": "FM Molfese, S Conia, R Orlando, R Navigli - arXiv preprint arXiv:2410.05077, 2024", "abstract": "Current Large Language Models (LLMs) have shown strong reasoning capabilities in commonsense question answering benchmarks, but the process underlying their success remains largely opaque. As a consequence, recent approaches have \u2026"}]
