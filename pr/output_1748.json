[{"title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "link": "https://arxiv.org/pdf/2405.00402", "details": "L Ranaldi, A Freitas - arXiv preprint arXiv:2405.00402, 2024", "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches \u2026"}, {"title": "Text-to-SQL Meets the Real-World", "link": "https://www.scitepress.org/Papers/2024/125552/125552.pdf", "details": "ER Nascimento, GM Garc\u0131a, L Feij\u00f3, WZ Victorio\u2026", "abstract": "Text-to-SQL refers to the task defined as \u201cgiven a relational database D and a natural language sentence S that describes a question on D, generate an SQL query Q over D that expresses S\u201d. Numerous tools have addressed this task with relative success \u2026"}, {"title": "Mitigating Overconfidence in Out-of-Distribution Detection by Capturing Extreme Activations", "link": "https://arxiv.org/pdf/2405.12658", "details": "M Azizmalayeri, A Abu-Hanna, G Cin\u00e0 - arXiv preprint arXiv:2405.12658, 2024", "abstract": "Detecting out-of-distribution (OOD) instances is crucial for the reliable deployment of machine learning models in real-world scenarios. OOD inputs are commonly expected to cause a more uncertain prediction in the primary task; however, there \u2026"}, {"title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning", "link": "https://arxiv.org/pdf/2404.17140", "details": "Y Zhang, M Khalifa, L Logeswaran, J Kim, M Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors. This work explores whether \u2026"}, {"title": "Path-Aware Cross-Attention Network for Question Answering", "link": "https://link.springer.com/chapter/10.1007/978-981-97-2253-2_9", "details": "Z Luo, Y Xiong, B Tang - Pacific-Asia Conference on Knowledge Discovery and \u2026, 2024", "abstract": "Abstract Reasoning is an essential ability in QA systems, and the integration of this ability into QA systems has been the subject of considerable research. A prevalent strategy involves incorporating domain knowledge graphs using Graph Neural \u2026"}, {"title": "Causal Evaluation of Language Models", "link": "https://arxiv.org/pdf/2405.00622", "details": "S Chen, B Peng, M Chen, R Wang, M Xu, X Zeng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for \u2026"}, {"title": "Vision Language Models in Autonomous Driving: A Survey and Outlook", "link": "https://ieeexplore.ieee.org/iel7/7274857/7448921/10531702.pdf", "details": "X Zhou, M Liu, E Yurtsever, BL Zagar, W Zimmer\u2026 - IEEE Transactions on \u2026, 2024", "abstract": "The applications of Vision-Language Models (VLMs) in the field of Autonomous Driving (AD) have attracted widespread attention due to their outstanding performance and the ability to leverage Large Language Models (LLMs). By \u2026"}, {"title": "Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models", "link": "https://arxiv.org/pdf/2405.12522", "details": "C O'Neill, T Bui - arXiv preprint arXiv:2405.12522, 2024", "abstract": "This paper introduces an efficient and robust method for discovering interpretable circuits in large language models using discrete sparse autoencoders. Our approach addresses key limitations of existing techniques, namely computational complexity \u2026"}, {"title": "Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation", "link": "https://arxiv.org/pdf/2404.17489", "details": "W Cui, R Hosseinzadeh, J Ma, T Wu, Y Sui, K Golestan - arXiv preprint arXiv \u2026, 2024", "abstract": "Contrastive learning is a model pre-training technique by first creating similar views of the original data, and then encouraging the data and its corresponding views to be close in the embedding space. Contrastive learning has witnessed success in image \u2026"}]
