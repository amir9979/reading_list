[{"title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models", "link": "https://arxiv.org/pdf/2501.05752", "details": "S Lee, H Park, J Kim, J Ok - arXiv preprint arXiv:2501.05752, 2025", "abstract": "Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer \u2026"}, {"title": "Eve: Efficient Multimodal Vision Language Models with Elastic Visual Experts", "link": "https://arxiv.org/pdf/2501.04322", "details": "M Rang, Z Bi, C Liu, Y Tang, K Han, Y Wang - arXiv preprint arXiv:2501.04322, 2025", "abstract": "Multimodal vision language models (VLMs) have made significant progress with the support of continuously increasing model sizes and data volumes. Running VLMs on edge devices has become a challenge for their widespread application. There are \u2026"}, {"title": "CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification", "link": "https://arxiv.org/pdf/2501.12266", "details": "C Patr\u00edcio, I Rio-Torto, JS Cardoso, LF Teixeira\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter \u2026"}, {"title": "Small Language Models (SLMs) Can Still Pack a Punch: A survey", "link": "https://arxiv.org/pdf/2501.05465", "details": "S Subramanian, V Elango, M Gungor - arXiv preprint arXiv:2501.05465, 2025", "abstract": "As foundation AI models continue to increase in size, an important question arises-is massive scale the only path forward? This survey of about 160 papers presents a family of Small Language Models (SLMs) in the 1 to 8 billion parameter range that \u2026"}, {"title": "KPL: Training-Free Medical Knowledge Mining of Vision-Language Models", "link": "https://arxiv.org/pdf/2501.11231", "details": "J Liu, T Hu, J Du, R Zhang, JT Zhou, Z Liu - arXiv preprint arXiv:2501.11231, 2025", "abstract": "Visual Language Models such as CLIP excel in image recognition due to extensive image-text pre-training. However, applying the CLIP inference in zero-shot classification, particularly for medical image diagnosis, faces challenges due to: 1) \u2026"}, {"title": "Are Vision-Language Models Truly Understanding Multi-vision Sensor?", "link": "https://arxiv.org/pdf/2412.20750", "details": "S Chung, Y Yu, Y Chee, SY Kim, BK Lee, YM Ro - arXiv preprint arXiv:2412.20750, 2024", "abstract": "Large-scale Vision-Language Models (VLMs) have advanced by aligning vision inputs with text, significantly improving performance in computer vision tasks. Moreover, for VLMs to be effectively utilized in real-world applications, an \u2026"}, {"title": "Reasoning Language Models: A Blueprint", "link": "https://arxiv.org/pdf/2501.11223", "details": "M Besta, J Barth, E Schreiber, A Kubicek, A Catarino\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have redefined AI's problem-solving capabilities by extending large language models \u2026"}, {"title": "Instruction-Guided Fusion of Multi-Layer Visual Features in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2501.08443", "details": "X Li, Y Zheng, H Chen, X Chen, Y Liang, C Lai - arXiv preprint arXiv:2501.08443, 2024", "abstract": "Large Vision-Language Models (LVLMs) have achieved significant success in multimodal tasks by combining pre-trained vision encoders and large language models. However, current LVLMs mainly rely on features from the final layers of the \u2026"}, {"title": "Bactrainus: Optimizing Large Language Models for Multi-hop Complex Question Answering Tasks", "link": "https://arxiv.org/pdf/2501.06286", "details": "I Barati, A Ghafouri, B Minaei-Bidgoli - arXiv preprint arXiv:2501.06286, 2025", "abstract": "In recent years, the use of large language models (LLMs) has significantly increased, and these models have demonstrated remarkable performance in a variety of general language tasks. However, the evaluation of their performance in domain \u2026"}]
