[{"title": "Universal Approximation Theory: The basic theory for large language models", "link": "https://arxiv.org/pdf/2407.00958", "details": "W Wang, Q Li - arXiv preprint arXiv:2407.00958, 2024", "abstract": "Language models have emerged as a critical area of focus in artificial intelligence, particularly with the introduction of groundbreaking innovations like ChatGPT. Large- scale Transformer networks have quickly become the leading approach for \u2026"}, {"title": "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models", "link": "https://arxiv.org/pdf/2407.01009", "details": "J Pan, Y Zhang, C Zhang, Z Liu, H Wang, H Li - arXiv preprint arXiv:2407.01009, 2024", "abstract": "Large language models (LLMs) have demonstrated emergent capabilities across diverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However, such a simple and fast COT approach often encounters limitations in dealing with \u2026"}, {"title": "PORT: Preference Optimization on Reasoning Traces", "link": "https://arxiv.org/pdf/2406.16061", "details": "S Lahlou, A Abubaker, H Hacid - arXiv preprint arXiv:2406.16061, 2024", "abstract": "Preference optimization methods have been successfully applied to improve not only the alignment of large language models (LLMs) with human values, but also specific natural language tasks such as summarization and stylistic continuations. This paper \u2026"}, {"title": "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training", "link": "https://arxiv.org/pdf/2406.16554", "details": "T Zhu, X Qu, D Dong, J Ruan, J Tong, C He, Y Cheng - arXiv preprint arXiv \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, training MoE from scratch in a large-scale setting still suffers from data-hungry and instability problems. Motivated \u2026"}, {"title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models", "link": "https://arxiv.org/pdf/2406.16714", "details": "J Cheng, Y Lu, X Gu, P Ke, X Liu, Y Dong, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Although Large Language Models (LLMs) are becoming increasingly powerful, they still exhibit significant but subtle weaknesses, such as mistakes in instruction- following or coding tasks. As these unexpected errors could lead to severe \u2026"}, {"title": "Concise and Precise Context Compression for Tool-Using Language Models", "link": "https://arxiv.org/pdf/2407.02043", "details": "Y Xu, Y Feng, H Mu, Y Hou, Y Li, X Wang, W Zhong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Through reading the documentation in the context, tool-using language models can dynamically extend their capability using external tools. The cost is that we have to input lengthy documentation every time the model needs to use the tool, occupying \u2026"}, {"title": "CELLO: Causal Evaluation of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2406.19131", "details": "M Chen, B Peng, Y Zhang, C Lu - arXiv preprint arXiv:2406.19131, 2024", "abstract": "Causal reasoning is fundamental to human intelligence and crucial for effective decision-making in real-world environments. Despite recent advancements in large vision-language models (LVLMs), their ability to comprehend causality remains \u2026"}, {"title": "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models", "link": "https://arxiv.org/pdf/2406.16635", "details": "Y Akhauri, AF AbouElhamayed, J Dotzel, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The high power consumption and latency-sensitive deployments of large language models (LLMs) have motivated techniques like quantization and sparsity. Contextual sparsity, where the sparsity pattern is input-dependent, is crucial in LLMs because \u2026"}, {"title": "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "link": "https://arxiv.org/pdf/2406.17681", "details": "K Qian, S Wan, C Tang, Y Wang, X Zhang, M Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As large language models achieve impressive scores on traditional benchmarks, an increasing number of researchers are becoming concerned about benchmark data leakage during pre-training, commonly known as the data contamination problem. To \u2026"}]
