'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Can 3D Vision-Language Models Truly Understand Natural'
[{"title": "Do Language Models Care About Text Quality? Evaluating Web-Crawled Corpora Across 11 Languages", "link": "https://arxiv.org/pdf/2403.08693", "details": "R van Noord, T Kuzman, P Rupnik, N Ljube\u0161i\u0107\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large, curated, web-crawled corpora play a vital role in training language models (LMs). They form the lion's share of the training data in virtually all recent LMs, such as the well-known GPT, LLaMA and XLM-RoBERTa models. However, despite this \u2026"}, {"title": "Automated Extraction of Stroke Severity from Unstructured Electronic Health Records using Natural Language Processing", "link": "https://www.medrxiv.org/content/10.1101/2024.03.08.24304011.full.pdf", "details": "M Bento Fernandes, B Westover, AB Singhal, SF Zafar - medRxiv, 2024", "abstract": "BACKGROUND: Multi-center electronic health records (EHR) can support quality improvement initiatives and comparative effectiveness research in stroke care. However, limitations of EHR-based research include challenges in abstracting key \u2026"}, {"title": "Third-Party Language Model Performance Prediction from Instruction", "link": "https://arxiv.org/pdf/2403.12413", "details": "R Nadkarni, Y Wang, NA Smith - arXiv preprint arXiv:2403.12413, 2024", "abstract": "Language model-based instruction-following systems have lately shown increasing performance on many benchmark tasks, demonstrating the capability of adapting to a broad variety of instructions. However, such systems are often not designed to be \u2026"}, {"title": "Scaling behavior of machine translation with large language models under prompt injection attacks", "link": "https://arxiv.org/pdf/2403.09832", "details": "Z Sun, AV Miceli-Barone - arXiv preprint arXiv:2403.09832, 2024", "abstract": "Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the \u2026"}, {"title": "Small Language Models are Good Too: An Empirical Study of Zero-Shot Classification", "link": "https://hal.science/hal-04519930/document", "details": "P Lepagnol, T Gerald, S Ghannay, C Servan, S Rosset - LREC-COLING 2024, 2024", "abstract": "This study is part of the debate on the efficiency of large versus small language models for text classification by prompting. We assess the performance of small language models in zero-shot text classification, challenging the prevailing \u2026"}]
