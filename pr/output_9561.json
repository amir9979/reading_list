[{"title": "Streamlining Prediction in Bayesian Deep Learning", "link": "https://arxiv.org/pdf/2411.18425", "details": "R Li, M Klasson, A Solin, M Trapp - arXiv preprint arXiv:2411.18425, 2024", "abstract": "The rising interest in Bayesian deep learning (BDL) has led to a plethora of methods for estimating the posterior distribution. However, efficient computation of inferences, such as predictions, has been largely overlooked with Monte Carlo integration \u2026"}, {"title": "Diffusion Self-Distillation for Zero-Shot Customized Image Generation", "link": "https://arxiv.org/pdf/2411.18616", "details": "S Cai, E Chan, Y Zhang, L Guibas, J Wu, G Wetzstein - arXiv preprint arXiv \u2026, 2024", "abstract": "Text-to-image diffusion models produce impressive results but are frustrating tools for artists who desire fine-grained control. For example, a common use case is to create images of a specific instance in novel contexts, ie,\" identity-preserving generation\" \u2026"}, {"title": "Robust Fine-tuning of Zero-shot Models via Variance Reduction", "link": "https://arxiv.org/pdf/2411.06966", "details": "B Zhu, J Cui, H Zhang - arXiv preprint arXiv:2411.06966, 2024", "abstract": "When fine-tuning zero-shot models like CLIP, our desideratum is for the fine-tuned model to excel in both in-distribution (ID) and out-of-distribution (OOD). Recently, ensemble-based models (ESM) have been shown to offer significant robustness \u2026"}, {"title": "Approximate attention with MLP: a pruning strategy for attention-based model in multivariate time series forecasting", "link": "https://arxiv.org/pdf/2410.24023", "details": "S Guo, J Deng, Y Wei, H Dou, F Shen, J Zhao - arXiv preprint arXiv:2410.24023, 2024", "abstract": "Attention-based architectures have become ubiquitous in time series forecasting tasks, including spatio-temporal (STF) and long-term time series forecasting (LTSF). Yet, our understanding of the reasons for their effectiveness remains limited. This \u2026"}, {"title": "VPformer: Multivariate Time Series Forecasting with Variable Correlation and Triple Patch Correlation Transformer", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9434-8_13", "details": "Z Wang, Y Huang, C Zhao, C Zhou - \u2026 on Natural Language Processing and Chinese \u2026, 2024", "abstract": "Time series forecasting is vital in industries like weather and transportation. However, Transformer models may face challenges capturing both variable and temporal correlations in multivariate forecasting, potentially hindering their \u2026"}]
