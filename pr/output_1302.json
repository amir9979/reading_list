'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Multi-Scale Dilated Convolution Network for Long-Term '
[{"title": "MAP: Model Aggregation and Personalization in Federated Learning with Incomplete Classes", "link": "https://arxiv.org/pdf/2404.09232", "details": "XC Li, S Song, Y Li, B Li, Y Shao, Y Yang, DC Zhan - IEEE Transactions on \u2026, 2024", "abstract": "In some real-world applications, data samples are usually distributed on local devices, where federated learning (FL) techniques are proposed to coordinate decentralized clients without directly sharing users' private data. FL commonly \u2026"}, {"title": "Eyes Can Deceive: Benchmarking Counterfactual Reasoning Abilities of Multi-modal Large Language Models", "link": "https://arxiv.org/pdf/2404.12966", "details": "Y Li, W Tian, Y Jiao, J Chen, YG Jiang - arXiv preprint arXiv:2404.12966, 2024", "abstract": "Counterfactual reasoning, as a crucial manifestation of human intelligence, refers to making presuppositions based on established facts and extrapolating potential outcomes. Existing multimodal large language models (MLLMs) have exhibited \u2026"}, {"title": "Understanding Multimodal Contrastive Learning Through Pointwise Mutual Information", "link": "https://arxiv.org/pdf/2404.19228", "details": "T Uesaka, T Suzuki, Y Takida, CH Lai, N Murata\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal representation learning to integrate different modalities, such as text, vision, and audio is important for real-world applications. The symmetric InfoNCE loss proposed in CLIP is a key concept in multimodal representation learning. In this \u2026"}, {"title": "Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case Study", "link": "https://arxiv.org/pdf/2404.08259", "details": "WH Her, U Kruschwitz - arXiv preprint arXiv:2404.08259, 2024", "abstract": "Machine Translation has made impressive progress in recent years offering close to human-level performance on many languages, but studies have primarily focused on high-resource languages with broad online presence and resources. With the help of \u2026"}, {"title": "Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning", "link": "https://arxiv.org/pdf/2405.05615", "details": "S Jie, Y Tang, N Ding, ZH Deng, K Han, Y Wang - arXiv preprint arXiv:2405.05615, 2024", "abstract": "Current solutions for efficiently constructing large vision-language (VL) models follow a two-step paradigm: projecting the output of pre-trained vision encoders to the input space of pre-trained language models as visual prompts; and then transferring the \u2026"}, {"title": "Enabling action crossmodality for a pretrained large language model", "link": "https://www.sciencedirect.com/science/article/pii/S2949719124000207", "details": "A Caesar, O \u00d6zdemir, C Weber, S Wermter - Natural Language Processing Journal, 2024", "abstract": "Natural language processing and vision tasks have seen large improvements recently through the rise of Transformer architectures. The high performing large language models (LLMs) benefit from large textual datasets that are numerously \u2026"}, {"title": "Measuring Cross-lingual Transfer in Bytes", "link": "https://arxiv.org/pdf/2404.08191", "details": "LR de Souza, TS Almeida, R Lotufo, R Nogueira - arXiv preprint arXiv:2404.08191, 2024", "abstract": "Multilingual pretraining has been a successful solution to the challenges posed by the lack of resources for languages. These models can transfer knowledge to target languages with minimal or no examples. Recent research suggests that monolingual \u2026"}]
