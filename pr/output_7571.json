[{"title": "EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.07133", "details": "R Zhao, H Yuan, Y Wei, S Zhang, Y Gu, L Ran, X Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only \u2026"}, {"title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate", "link": "https://arxiv.org/pdf/2410.07167", "details": "Q Huang, X Dong, P Zhang, Y Zang, Y Cao, J Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present the Modality Integration Rate (MIR), an effective, robust, and generalized metric to indicate the multi-modal pre-training quality of Large Vision Language Models (LVLMs). Large-scale pre-training plays a critical role in building capable \u2026"}, {"title": "Unsupervised multi-source domain adaptation via contrastive learning for EEG classification", "link": "https://www.sciencedirect.com/science/article/pii/S0957417424023194", "details": "C Xu, Y Song, Q Zheng, Q Wang, PA Heng - Expert Systems with Applications, 2024", "abstract": "Individual differences in electroencephalography (EEG) present significant challenges for subject-independent EEG classification in brain-computer interfaces (BCIs). Existing domain adaptation methods often address individual differences by \u2026"}, {"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "link": "https://arxiv.org/pdf/2410.05639", "details": "R Zhao, ZL Thai, Y Zhang, S Hu, Y Ba, J Zhou, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the \u2026"}, {"title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models", "link": "https://arxiv.org/pdf/2410.06154", "details": "MJ Mirza, M Zhao, Z Mao, S Doveh, W Lin, P Gavrikov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task \u2026"}, {"title": "Automatic structuring of radiology reports with on-premise open-source large language models", "link": "https://link.springer.com/article/10.1007/s00330-024-11074-y", "details": "P Wo\u017anicki, C Laqua, I Fiku, A Hekalo, D Truhn\u2026 - European Radiology, 2024", "abstract": "Objectives Structured reporting enhances comparability, readability, and content detail. Large language models (LLMs) could convert free text into structured data without disrupting radiologists' reporting workflow. This study evaluated an on \u2026"}, {"title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures", "link": "https://arxiv.org/pdf/2410.07698", "details": "Y Chen, Y Zhang, L Cao, K Yuan, Z Wen - arXiv preprint arXiv:2410.07698, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs when adapting large language models (LLMs) for downstream applications. However, traditional first-order (FO) fine-tuning algorithms incur substantial memory overhead \u2026"}, {"title": "Accelerated Preference Optimization for Large Language Model Alignment", "link": "https://arxiv.org/pdf/2410.06293", "details": "J He, H Yuan, Q Gu - arXiv preprint arXiv:2410.06293, 2024", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal tool for aligning large language models (LLMs) with human preferences. Direct Preference Optimization (DPO), one of the most popular approaches, formulates \u2026"}, {"title": "Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures", "link": "https://arxiv.org/pdf/2410.06672", "details": "J Wang, X Ge, W Shu, Q Tang, Y Zhou, Z He, X Qiu - arXiv preprint arXiv:2410.06672, 2024", "abstract": "The hypothesis of Universality in interpretability suggests that different neural networks may converge to implement similar algorithms on similar tasks. In this work, we investigate two mainstream architectures for language modeling, namely \u2026"}]
