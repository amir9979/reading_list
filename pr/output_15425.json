[{"title": "Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation", "link": "https://arxiv.org/pdf/2504.02438", "details": "C Cheng, J Guan, W Wu, R Yan - arXiv preprint arXiv:2504.02438, 2025", "abstract": "Long-form video processing fundamentally challenges vision-language models (VLMs) due to the high computational costs of handling extended temporal sequences. Existing token pruning and feature merging methods often sacrifice \u2026"}, {"title": "Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning", "link": "https://arxiv.org/pdf/2504.05632", "details": "S Kabra, A Jha, C Reddy - arXiv preprint arXiv:2504.05632, 2025", "abstract": "Recent advances in large-scale generative language models have shown that reasoning capabilities can significantly improve model performance across a variety of tasks. However, the impact of reasoning on a model's ability to mitigate \u2026"}, {"title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models", "link": "https://arxiv.org/pdf/2503.18931", "details": "Y Chen, L Meng, W Peng, Z Wu, YG Jiang - arXiv preprint arXiv:2503.18931, 2025", "abstract": "Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of \u2026"}, {"title": "Breaking the Encoder Barrier for Seamless Video-Language Understanding", "link": "https://arxiv.org/pdf/2503.18422", "details": "H Li, Y Zhang, L Guo, X Yue, J Liu - arXiv preprint arXiv:2503.18422, 2025", "abstract": "Most Video-Large Language Models (Video-LLMs) adopt an encoder-decoder framework, where a vision encoder extracts frame-wise features for processing by a language model. However, this approach incurs high computational costs \u2026"}, {"title": "Zero-shot 3D Scene Representation with Invertible Generative Neural Radiance Fields", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10967257.pdf", "details": "K Ko, S Kim, M Lee - IEEE Access, 2025", "abstract": "Generative Neural Radiance Fields (NeRFs) have recently enabled efficient synthesis of 3D scenes by training on unposed real image sets. However, existing methods for generating multi-view images of specific input images have limitations \u2026"}, {"title": "Knowledge-Instruct: Effective Continual Pre-training from Limited Data using Instructions", "link": "https://arxiv.org/pdf/2504.05571", "details": "O Ovadia, M Brief, R Lemberg, E Sheetrit - arXiv preprint arXiv:2504.05571, 2025", "abstract": "While Large Language Models (LLMs) acquire vast knowledge during pre-training, they often lack domain-specific, new, or niche information. Continual pre-training (CPT) attempts to address this gap but suffers from catastrophic forgetting and \u2026"}, {"title": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection", "link": "https://arxiv.org/pdf/2504.00695", "details": "X Zhu, Z Gu, S Zheng, T Wang, T Li, H Feng, Y Xiao - arXiv preprint arXiv:2504.00695, 2025", "abstract": "Pre-training large language models (LLMs) necessitates enormous diverse textual corpora, making effective data selection a key challenge for balancing computational resources and model performance. Current methodologies primarily emphasize data \u2026"}, {"title": "Using Large Language Models to Automate Data Extraction From Surgical Pathology Reports: Retrospective Cohort Study", "link": "https://formative.jmir.org/2025/1/e64544/", "details": "D Lee, A Vaid, KM Menon, R Freeman, DS Matteson\u2026 - JMIR Formative Research, 2025", "abstract": "Background: Popularized by ChatGPT, large language models (LLMs) are poised to transform the scalability of clinical natural language processing (NLP) downstream tasks such as medical question answering (MQA) and automated data extraction \u2026"}, {"title": "Slow-fast architecture for video multi-modal large language models", "link": "https://arxiv.org/pdf/2504.01328", "details": "M Shi, S Wang, CY Chen, J Jain, K Wang, J Xiong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using \u2026"}]
