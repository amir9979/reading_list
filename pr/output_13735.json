[{"title": "Code Summarization Beyond Function Level", "link": "https://arxiv.org/pdf/2502.16704", "details": "V Makharev, V Ivanov - arXiv preprint arXiv:2502.16704, 2025", "abstract": "Code summarization is a critical task in natural language processing and software engineering, which aims to generate concise descriptions of source code. Recent advancements have improved the quality of these summaries, enhancing code \u2026"}, {"title": "Comprehensive analysis of transparency and accessibility of chatgpt, deepseek, and other sota large language models", "link": "https://arxiv.org/pdf/2502.18505", "details": "R Sapkota, S Raza, M Karkee - arXiv preprint arXiv:2502.18505, 2025", "abstract": "Despite increasing discussions on open-source Artificial Intelligence (AI), existing research lacks a discussion on the transparency and accessibility of state-of-the-art (SoTA) Large Language Models (LLMs). The Open Source Initiative (OSI) has \u2026"}, {"title": "Anatomical grounding pre-training for medical phrase grounding", "link": "https://arxiv.org/pdf/2502.16585", "details": "W Zhang, S Chandra, A Nicolson - arXiv preprint arXiv:2502.16585, 2025", "abstract": "Medical Phrase Grounding (MPG) maps radiological findings described in medical reports to specific regions in medical images. The primary obstacle hindering progress in MPG is the scarcity of annotated data available for training and \u2026"}, {"title": "OPPA: OPtimizing PArallelism for Language Model Training", "link": "https://openreview.net/pdf%3Fid%3Dqsx5vPJlOx", "details": "A Hemachandra, Y Han, SK Ng, BKH Low - First Workshop on Scalable Optimization for \u2026", "abstract": "Training of modern large neural networks (NNs) is often done in parallel across multiple GPUs. While there are existing parallel training frameworks which easily allow NN training using multi-dimensional parallelism, the challenge remains in \u2026"}, {"title": "Explicitly unbiased large language models still form biased associations", "link": "https://www.pnas.org/doi/full/10.1073/pnas.2416228122", "details": "X Bai, A Wang, I Sucholutsky, TL Griffiths - Proceedings of the National Academy of \u2026, 2025", "abstract": "Large language models (LLMs) can pass explicit social bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: As LLMs become \u2026"}, {"title": "Hallucination Detection in Large Language Models with Metamorphic Relations", "link": "https://arxiv.org/pdf/2502.15844", "details": "B Yang, MAA Mamun, JM Zhang, G Uddin - arXiv preprint arXiv:2502.15844, 2025", "abstract": "Large Language Models (LLMs) are prone to hallucinations, eg, factually incorrect information, in their responses. These hallucinations present challenges for LLM- based applications that demand high factual accuracy. Existing hallucination \u2026"}, {"title": "SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2502.13059", "details": "X Cheng, W Zhang, S Zhang, J Yang, X Guan, X Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The increasing application of multi-modal large language models (MLLMs) across various sectors have spotlighted the essence of their output reliability and accuracy, particularly their ability to produce content grounded in factual information (eg \u2026"}, {"title": "VRoPE: Rotary Position Embedding for Video Large Language Models", "link": "https://arxiv.org/pdf/2502.11664", "details": "Z Liu, L Guo, Y Tang, J Cai, K Ma, X Chen, J Liu - arXiv preprint arXiv:2502.11664, 2025", "abstract": "Rotary Position Embedding (RoPE) has shown strong performance in text-based Large Language Models (LLMs), but extending it to video remains a challenge due to the intricate spatiotemporal structure of video frames. Existing adaptations, such as \u2026"}, {"title": "DR. GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting with Demonstration and Reasoning", "link": "https://arxiv.org/pdf/2502.11603", "details": "H Qiu, Y Xu, M Qiu, W Wang - arXiv preprint arXiv:2502.11603, 2025", "abstract": "Large Language Models (LLMs) exhibit strong natural language processing capabilities but also inherit and amplify societal biases, including gender bias, raising fairness concerns. Existing debiasing methods face significant limitations \u2026"}]
