[{"title": "Identification and multiply robust estimation of causal effects via instrumental variables from an auxiliary heterogeneous population", "link": "https://arxiv.org/pdf/2407.18166", "details": "W Li, J Liu, P Ding, Z Geng - arXiv preprint arXiv:2407.18166, 2024", "abstract": "Evaluating causal effects in a primary population of interest with unmeasured confounders is challenging. Although instrumental variables (IVs) are widely used to address unmeasured confounding, they may not always be available in the primary \u2026"}, {"title": "IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization", "link": "https://arxiv.org/pdf/2407.10486", "details": "J Cao, D Jiao, Q Yan, W Zhang, S Tang, Y Zhuang - arXiv preprint arXiv:2407.10486, 2024", "abstract": "Query-focused summarization (QFS) aims to produce summaries that answer particular questions of interest, enabling greater user control and personalization. With the advent of large language models (LLMs), shows their impressive capability \u2026"}, {"title": "BAPO: Base-Anchored Preference Optimization for Personalized Alignment in Large Language Models", "link": "https://arxiv.org/pdf/2407.00693", "details": "G Lee, M Jeong, Y Kim, H Jung, J Oh, S Kim, SY Yun - arXiv preprint arXiv \u2026, 2024", "abstract": "While learning to align Large Language Models (LLMs) with human preferences has shown remarkable success, aligning these models to meet the diverse user preferences presents further challenges in preserving previous knowledge. This \u2026"}, {"title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs", "link": "https://arxiv.org/pdf/2407.00653", "details": "Y Zhang, X Wang, J Liang, S Xia, L Chen, Y Xiao - arXiv preprint arXiv:2407.00653, 2024", "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new \u2026"}, {"title": "Interpretable Differential Diagnosis with Dual-Inference Large Language Models", "link": "https://arxiv.org/pdf/2407.07330", "details": "S Zhou, S Ding, J Wang, M Lin, GB Melton, R Zhang - arXiv preprint arXiv:2407.07330, 2024", "abstract": "Methodological advancements to automate the generation of differential diagnosis (DDx) to predict a list of potential diseases as differentials given patients' symptom descriptions are critical to clinical reasoning and applications such as decision \u2026"}, {"title": "Universal Approximation Theory: The basic theory for large language models", "link": "https://arxiv.org/pdf/2407.00958", "details": "W Wang, Q Li - arXiv preprint arXiv:2407.00958, 2024", "abstract": "Language models have emerged as a critical area of focus in artificial intelligence, particularly with the introduction of groundbreaking innovations like ChatGPT. Large- scale Transformer networks have quickly become the leading approach for \u2026"}, {"title": "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models", "link": "https://arxiv.org/pdf/2407.01906", "details": "Z Wang, D Chen, D Dai, R Xu, Z Li, Y Wu - arXiv preprint arXiv:2407.01906, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) is crucial for customizing Large Language Models (LLMs) with constrained resources. Although there have been various PEFT methods for dense-architecture LLMs, PEFT for sparse-architecture LLMs is still \u2026"}, {"title": "Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models", "link": "https://arxiv.org/pdf/2407.10873", "details": "R Zhang, F Liu, X Lin, Z Wang, Z Lu, Q Zhang - arXiv preprint arXiv:2407.10873, 2024", "abstract": "Automated heuristic design (AHD) has gained considerable attention for its potential to automate the development of effective heuristics. The recent advent of large language models (LLMs) has paved a new avenue for AHD, with initial efforts \u2026"}, {"title": "Planning with Large Language Models for Conversational Agents", "link": "https://arxiv.org/pdf/2407.03884", "details": "Z Li, J Peng, Y Wang, T Shen, M Zhang, L Su, S Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Controllability and proactivity are crucial properties of autonomous conversational agents (CAs). Controllability requires the CAs to follow the standard operating procedures (SOPs), such as verifying identity before activating credit cards \u2026"}]
