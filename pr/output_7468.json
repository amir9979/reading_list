[{"title": "Rethinking Fair Representation Learning for Performance-Sensitive Tasks", "link": "https://arxiv.org/pdf/2410.04120", "details": "C Jones, FS Ribeiro, M Roschewitz, DC Castro\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We investigate the prominent class of fair representation learning methods for bias mitigation. Using causal reasoning to define and formalise different sources of dataset bias, we reveal important implicit assumptions inherent to these methods. We \u2026"}, {"title": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models", "link": "https://arxiv.org/pdf/2410.02355", "details": "J Fang, H Jiang, K Wang, Y Ma, X Wang, X He, T Chua - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) often exhibit hallucinations due to incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating \u2026"}, {"title": "AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text", "link": "https://arxiv.org/pdf/2410.04265", "details": "X Lu, M Sclar, S Hallinan, N Mireshghallah, J Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Creativity has long been considered one of the most difficult aspect of human intelligence for AI to mimic. However, the rise of Large Language Models (LLMs), like ChatGPT, has raised questions about whether AI can match or even surpass human \u2026"}, {"title": "Are Expert-Level Language Models Expert-Level Annotators?", "link": "https://arxiv.org/pdf/2410.03254", "details": "YM Tseng, WL Chen, CC Chen, HH Chen - arXiv preprint arXiv:2410.03254, 2024", "abstract": "Data annotation refers to the labeling or tagging of textual data with relevant information. A large body of works have reported positive results on leveraging LLMs as an alternative to human annotators. However, existing studies focus on classic \u2026"}, {"title": "DEPT: Decoupled Embeddings for Pre-training Language Models", "link": "https://arxiv.org/pdf/2410.05021", "details": "A Iacob, L Sani, M Kurmanji, WF Shen, X Qiu, D Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language Model pre-training benefits from a broader data mixture to enhance performance across domains and languages. However, training on such heterogeneous text corpora is complex, requiring extensive and cost-intensive \u2026"}, {"title": "Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation for Logical Reading Comprehension", "link": "https://arxiv.org/pdf/2409.14495", "details": "C Wang, P Jian, Y Zhen - arXiv preprint arXiv:2409.14495, 2024", "abstract": "Logical reading comprehension is a challenging task that entails grasping the underlying semantics of text and applying reasoning to deduce the correct answer. Prior researches have primarily focused on enhancing logical reasoning capabilities \u2026"}, {"title": "Unsupervised Domain Adaptation Using Soft-Labeled Contrastive Learning with Reversed Monte Carlo Method for Cardiac Image Segmentation", "link": "https://papers.miccai.org/miccai-2024/paper/1593_paper.pdf", "details": "M Gu, M Thies, S Mei, F Wagner, M Fan, Y Sun, Z Pan\u2026 - International Conference on \u2026, 2024", "abstract": "Recent unsupervised domain adaptation methods in medical image segmentation adopt centroid/prototypical contrastive learning (CL) to match the source and target features for their excellent ability of representation learning and semantic feature \u2026"}, {"title": "Explainable Diagnosis Prediction through Neuro-Symbolic Integration", "link": "https://arxiv.org/pdf/2410.01855", "details": "Q Lu, R Li, E Sagheb, A Wen, J Wang, L Wang, JW Fan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Diagnosis prediction is a critical task in healthcare, where timely and accurate identification of medical conditions can significantly impact patient outcomes. Traditional machine learning and deep learning models have achieved notable \u2026"}, {"title": "Reliable machine learning models in genomic medicine using conformal prediction", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/09/19/2024.09.09.24312995.full.pdf", "details": "C Papangelou, K Kyriakidis, P Natsiavas, I Chouvarda\u2026 - medRxiv, 2024", "abstract": "Machine learning and genomic medicine are the mainstays of research in delivering personalized healthcare services for disease diagnosis, risk stratification, tailored treatment, and prediction of adverse effects. However, potential prediction errors in \u2026"}]
