[{"title": "Designing Retrieval-Augmented Language Models for Clinical Decision Support", "link": "https://link.springer.com/chapter/10.1007/978-3-031-63592-2_13", "details": "K Quigley, T Koker, J Taylor, V Mancuso, L Brattain - AI for Health Equity and Fairness \u2026, 2024", "abstract": "Ever-increasing demands for physician expertise drive the need for trustworthy point- of-care tools that can help aid decision-making in all clinical settings. Retrieval- augmented language models carry potential to relieve the information burden on \u2026"}, {"title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism", "link": "https://arxiv.org/pdf/2408.10473", "details": "G Li, X Zhao, L Liu, Z Li, D Li, L Tian, J He, A Sirasao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational \u2026"}, {"title": "The Need for Continuous Evaluation of Artificial Intelligence Prediction Algorithms", "link": "https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2823641", "details": "NH Shah, MA Pfeffer, M Ghassemi - JAMA Network Open, 2024", "abstract": "Rakers et al8 conducted a much-needed systematic review of the availability of evidence for artificial intelligence prediction algorithms (AIPAs) in primary care. Their review found 43 such prediction algorithms, of which 25 were commercially \u2026"}, {"title": "An open-source framework for end-to-end analysis of electronic health record data", "link": "https://www.nature.com/articles/s41591-024-03214-0", "details": "L Heumos, P Ehmele, T Treis, J Upmeier zu Belzen\u2026 - Nature Medicine, 2024", "abstract": "With progressive digitalization of healthcare systems worldwide, large-scale collection of electronic health records (EHRs) has become commonplace. However, an extensible framework for comprehensive exploratory analysis that accounts for \u2026"}, {"title": "Skipformer: Evolving Beyond Blocks for Extensively Searching On-Device Language Models with Learnable Attention Window", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10666862.pdf", "details": "M Bodenham, J Kung - IEEE Access, 2024", "abstract": "Deployment of language models to resource-constrained edge devices is an uphill battle against their ever-increasing size. The task transferability of language models makes deployment to the edge an attractive application. Prior neural architecture \u2026"}, {"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "link": "https://arxiv.org/pdf/2408.12337", "details": "KS Phogat, SA Puranam, S Dasaratha, C Harsha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain \u2026"}, {"title": "Unraveling the Inner Workings of Massive Language Models: Architecture, Training, and Linguistic Capacities", "link": "https://www.igi-global.com/chapter/unraveling-the-inner-workings-of-massive-language-models/354398", "details": "CVS Babu, CSA Anniyappa - Challenges in Large Language Model Development \u2026, 2024", "abstract": "This study explores the evolution of language models, emphasizing the shift from traditional statistical methods to advanced neural networks, particularly the transformer architecture. It aims to understand the impact of these advancements on \u2026"}, {"title": "Leveraging Unstructured Text Data for Federated Instruction Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2409.07136", "details": "R Ye, R Ge, Y Fengting, J Chai, Y Wang, S Chen - arXiv preprint arXiv:2409.07136, 2024", "abstract": "Federated instruction tuning enables multiple clients to collaboratively fine-tune a shared large language model (LLM) that can follow humans' instructions without directly sharing raw data. However, existing literature impractically requires that all \u2026"}, {"title": "DetoxBench: Benchmarking Large Language Models for Multitask Fraud & Abuse Detection", "link": "https://arxiv.org/pdf/2409.06072", "details": "J Chakraborty, W Xia, A Majumder, D Ma, W Chaabene\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks. However, their practical application in high-stake domains, such as fraud and abuse detection, remains an area that requires further \u2026"}]
