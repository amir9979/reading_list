[{"title": "Client-Centered Federated Learning for Heterogeneous EHRs: Use Fewer Participants to Achieve the Same Performance", "link": "https://www.researchgate.net/profile/Jiyoun-Kim-16/publication/388919565_Client-Centered_Federated_Learning_for_Heterogeneous_EHRs_Use_Fewer_Participants_to_Achieve_the_Same_Performance/links/67aca7bc207c0c20fa857389/Client-Centered-Federated-Learning-for-Heterogeneous-EHRs-Use-Fewer-Participants-to-Achieve-the-Same-Performance.pdf", "details": "J Kim, J Kim, K Hur, E Choi", "abstract": "The increasing volume of electronic health records (EHRs) presents the opportunity to improve the accuracy and robustness of models in clinical prediction tasks. Unlike traditional centralized approaches, federated learning enables training on data from \u2026"}, {"title": "When Evolution Strategy Meets Language Models Tuning", "link": "https://aclanthology.org/2025.coling-main.357.pdf", "details": "B Huang, Y Jiang, M Chen, Y Wang, H Chen, W Wang - Proceedings of the 31st \u2026, 2025", "abstract": "Supervised Fine-tuning has been pivotal in training autoregressive language models, yet it introduces exposure bias. To mitigate this, Post Fine-tuning, including on-policy and off-policy methods, has emerged as a solution to enhance models \u2026"}, {"title": "Reinforced Lifelong Editing for Language Models", "link": "https://arxiv.org/pdf/2502.05759", "details": "Z Li, H Jiang, H Chen, B Bi, Z Zhou, F Sun, J Fang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) acquire information from pre-training corpora, but their stored knowledge can become inaccurate or outdated over time. Model editing addresses this challenge by modifying model parameters without retraining, and \u2026"}]
