[{"title": "Role of Natural Language Processing in Automatic Detection of Unexpected Findings in Radiology Reports: A Comparative Study of RoBERTa, CNN, and ChatGPT", "link": "https://www.academicradiology.org/article/S1076-6332\\(24\\)00562-2/fulltext", "details": "P L\u00f3pez-\u00dabeda, T Mart\u00edn-Noguerol, J Escart\u00edn, A Luna - Academic Radiology, 2024", "abstract": "Rationale and Objectives Large Language Models can capture the context of radiological reports, offering high accuracy in detecting unexpected findings. We aim to fine-tune a Robustly Optimized BERT Pretraining Approach (RoBERTa) model for \u2026"}, {"title": "Overview of the 9th Social Media Mining for Health Applications (# SMM4H) Shared Tasks at ACL 2024\u2013Large Language Models and Generalizability for Social \u2026", "link": "https://aclanthology.org/2024.smm4h-1.40.pdf", "details": "D Xu, G Garcia, L Raithel, P Thomas, R Roller\u2026 - Proceedings of The 9th \u2026, 2024", "abstract": "For the past nine years, the Social Media Mining for Health Applications (# SMM4H) shared tasks have promoted community-driven development and evaluation of advanced natural language processing systems to detect, extract, and normalize \u2026"}, {"title": "Improving Self-training with Prototypical Learning for Source-Free Domain Adaptation on Clinical Text", "link": "https://aclanthology.org/2024.bionlp-1.1/", "details": "S Shimizu, S Yada, L Raithel, E Aramaki - Proceedings of the 23rd Workshop on \u2026, 2024", "abstract": "Abstract Domain adaptation is crucial in the clinical domain since the performance of a model trained on one domain (source) degrades seriously when applied to another domain (target). However, conventional domain adaptation methods often cannot be \u2026"}]
