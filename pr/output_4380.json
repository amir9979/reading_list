[{"title": "Can Large Language Models Automatically Jailbreak GPT-4V?", "link": "https://arxiv.org/pdf/2407.16686", "details": "Y Wu, Y Huang, Y Liu, X Li, P Zhou, L Sun - arXiv preprint arXiv:2407.16686, 2024", "abstract": "GPT-4V has attracted considerable attention due to its extraordinary capacity for integrating and processing multimodal information. At the same time, its ability of face recognition raises new safety concerns of privacy leakage. Despite researchers' \u2026"}, {"title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models", "link": "https://arxiv.org/pdf/2407.04693", "details": "Y Gu, Z Ji, W Zhang, C Lyu, D Lin, K Chen - arXiv preprint arXiv:2407.04693, 2024", "abstract": "Large language models (LLMs) exhibit hallucinations in long-form question- answering tasks across various domains and wide applications. Current hallucination detection and mitigation datasets are limited in domains and sizes \u2026"}, {"title": "Interpretable Differential Diagnosis with Dual-Inference Large Language Models", "link": "https://arxiv.org/pdf/2407.07330", "details": "S Zhou, S Ding, J Wang, M Lin, GB Melton, R Zhang - arXiv preprint arXiv:2407.07330, 2024", "abstract": "Methodological advancements to automate the generation of differential diagnosis (DDx) to predict a list of potential diseases as differentials given patients' symptom descriptions are critical to clinical reasoning and applications such as decision \u2026"}, {"title": "LICO: Large Language Models for In-Context Molecular Optimization", "link": "https://arxiv.org/pdf/2406.18851", "details": "T Nguyen, A Grover - arXiv preprint arXiv:2406.18851, 2024", "abstract": "Optimizing black-box functions is a fundamental problem in science and engineering. To solve this problem, many approaches learn a surrogate function that estimates the underlying objective from limited historical evaluations. Large \u2026"}, {"title": "Direct Preference Knowledge Distillation for Large Language Models", "link": "https://arxiv.org/pdf/2406.19774", "details": "Y Li, Y Gu, L Dong, D Wang, Y Cheng, F Wei - arXiv preprint arXiv:2406.19774, 2024", "abstract": "In the field of large language models (LLMs), Knowledge Distillation (KD) is a critical technique for transferring capabilities from teacher models to student models. However, existing KD methods face limitations and challenges in distillation of LLMs \u2026"}]
