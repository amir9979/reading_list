'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Simplifying Multimodality: Unimodal Approach to Multim'
[{"title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies", "link": "https://arxiv.org/pdf/2404.06395", "details": "S Hu, Y Tu, X Han, C He, G Cui, X Long, Z Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This \u2026"}, {"title": "What matters when building vision-language models?", "link": "https://arxiv.org/pdf/2405.02246", "details": "H Lauren\u00e7on, L Tronchon, M Cord, V Sanh - arXiv preprint arXiv:2405.02246, 2024", "abstract": "The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding \u2026"}, {"title": "On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?", "link": "https://arxiv.org/pdf/2405.02266", "details": "M Zanella, IB Ayed - arXiv preprint arXiv:2405.02266, 2024", "abstract": "The development of large vision-language models, notably CLIP, has catalyzed research into effective adaptation techniques, with a particular focus on soft prompt tuning. Conjointly, test-time augmentation, which utilizes multiple augmented views \u2026"}, {"title": "Unifying Scene Representation and Hand-Eye Calibration with 3D Foundation Models", "link": "https://arxiv.org/pdf/2404.11683", "details": "W Zhi, H Tang, T Zhang, M Johnson-Roberson - arXiv preprint arXiv:2404.11683, 2024", "abstract": "Representing the environment is a central challenge in robotics, and is essential for effective decision-making. Traditionally, before capturing images with a manipulator- mounted camera, users need to calibrate the camera using a specific external \u2026"}, {"title": "Improving the spatial resolution of solar images using super-resolution diffusion generative adversarial networks", "link": "https://www.aanda.org/articles/aa/pdf/forth/aa49100-23.pdf", "details": "W Song, Y Ma, H Sun, X Zhao, G Lin - 2024", "abstract": "Context. High-spatial-resolution solar images contribute to the study of small-scale structures on the Sun. The Helioseismic and Magnetic Imager (HMI) conducts continuous full-disk observations of the Sun at a fixed cadence, accumulating a \u2026"}, {"title": "FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations", "link": "https://arxiv.org/pdf/2404.06619", "details": "J Dwivedi-Yu, R Dwivedi, T Schick - arXiv preprint arXiv:2404.06619, 2024", "abstract": "The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes \u2026"}, {"title": "Refining Pre-trained Language Models for Domain Adaptation with Entity-Aware Discriminative and Contrastive Learning", "link": "https://epubs.siam.org/doi/pdf/10.1137/1.9781611978032.48", "details": "J Yang, X Hu, Y Shen, G xiao - Proceedings of the 2024 SIAM International \u2026, 2024", "abstract": "With the rapid advancement of pre-trained language models (PLMs), the adaptation of these models to specialized domains has emerged as an essential area of research. However, PLMs encounter substantial challenges when deployed in highly \u2026"}, {"title": "HyFit: Hybrid Fine-Tuning With Diverse Sampling for Abstractive Summarization", "link": "https://ieeexplore.ieee.org/abstract/document/10496256/", "details": "S Zhao, Y Cheng, Y Zhang, J Chen, Z Duan, Y Sun\u2026 - IEEE Transactions on Big \u2026, 2024", "abstract": "Abstractive summarization has made significant progress in recent years, which aims to generate a concise and coherent summary that contains the most important facts from the source document. Current fine-tuning approaches based on pre-training \u2026"}, {"title": "NGLUEni: Benchmarking and Adapting Pretrained Language Models for Nguni Languages", "link": "https://openreview.net/pdf%3Fid%3DDAirSCt8kO", "details": "F Meyer, H Song, A Chakrabarty, J Buys, R Dabre\u2026 - 5th Workshop on African Natural \u2026", "abstract": "The Nguni languages have over 20 million home language speakers in South Africa. There has been considerable growth in datasets for Nguni languages, but no analysis of performance of NLP models for these languages has been reported \u2026"}]
