[{"title": "Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports", "link": "https://arxiv.org/pdf/2506.16247", "details": "A Bhattacharya, T Rehman, DK Sanyal\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The findings section of a radiology report is often detailed and lengthy, whereas the impression section is comparatively more compact and captures key diagnostic conclusions. This research explores the use of advanced abstractive summarization \u2026", "entry_id": "http://arxiv.org/abs/2506.16247v1", "updated": "2025-06-19 12:07:17", "published": "2025-06-19 12:07:17", "authors": "Anindita Bhattacharya;Tohida Rehman;Debarshi Kumar Sanyal;Samiran Chattopadhyay", "summary": "The findings section of a radiology report is often detailed and lengthy,\nwhereas the impression section is comparatively more compact and captures key\ndiagnostic conclusions. This research explores the use of advanced abstractive\nsummarization models to generate the concise impression from the findings\nsection of a radiology report. We have used the publicly available MIMIC-CXR\ndataset. A comparative analysis is conducted on leading pre-trained and\nopen-source large language models, including T5-base, BART-base,\nPEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, and a custom Pointer Generator Network\nwith a coverage mechanism. To ensure a thorough assessment, multiple evaluation\nmetrics are employed, including ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and\nBERTScore. By analyzing the performance of these models, this study identifies\ntheir respective strengths and limitations in the summarization of medical\ntext. The findings of this paper provide helpful information for medical\nprofessionals who need automated summarization solutions in the healthcare\nsector.", "comment": "14 pages, 2 figures, 6 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.16247v1;http://arxiv.org/pdf/2506.16247v1", "pdf_url": "http://arxiv.org/pdf/2506.16247v1"}, {"title": "Can Common VLMs Rival Medical VLMs? Evaluation and Strategic Insights", "link": "https://arxiv.org/pdf/2506.17337", "details": "Y Zhong, R Jin, X Li, Q Dou - arXiv preprint arXiv:2506.17337, 2025", "abstract": "Medical vision-language models (VLMs) leverage large-scale pretraining for diverse imaging tasks but require substantial computational and data resources. Meanwhile, common or general-purpose VLMs (eg, CLIP, LLaVA), though not trained for medical \u2026", "entry_id": "http://arxiv.org/abs/2506.17337v1", "updated": "2025-06-19 07:59:00", "published": "2025-06-19 07:59:00", "authors": "Yuan Zhong;Ruinan Jin;Xiaoxiao Li;Qi Dou", "summary": "Medical vision-language models (VLMs) leverage large-scale pretraining for\ndiverse imaging tasks but require substantial computational and data resources.\nMeanwhile, common or general-purpose VLMs (e.g., CLIP, LLaVA), though not\ntrained for medical use, show promise with fine-tuning. This raises a key\nquestion: Can efficient fine-tuned common VLMs rival generalist medical VLMs\nfor solving specific medical imaging tasks? This study systematically evaluates\ncommon and medical VLMs across disease diagnosis and visual question answering\n(VQA). Using CLIP-based and LLaVA-based models, we examine (1) off-the-shelf\nperformance gaps in in-domain (ID) settings, (2) whether fine-tuning bridges\nthese gaps, and (3) generalization to out-of-domain (OOD) tasks on unseen\nmedical modalities. While medical-specific pretraining provides advantages in\nID settings, common VLMs match or surpass medical-specific models after\nlightweight fine-tuning, with LoRA-based adaptation proving highly effective\namong different tasks. In OOD tasks, common VLMs demonstrate strong\nadaptability in some tasks, challenging the assumption that medical-specific\npre-training is essential. These findings suggest that leveraging common VLMs\nwith fine-tuning offers a scalable and cost-effective alternative to developing\nlarge-scale medical VLMs, providing crucial insights for future research in the\nmedical imaging field.", "comment": null, "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2506.17337v1;http://arxiv.org/pdf/2506.17337v1", "pdf_url": "http://arxiv.org/pdf/2506.17337v1"}, {"title": "Multimodal Medical Image Binding via Shared Text Embeddings", "link": "https://arxiv.org/pdf/2506.18072", "details": "Y Liu, S Xi, S Liu, H Ding, C Jin, C Yang, J He, Y Shen - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical image analysis increasingly relies on the integration of multiple imaging modalities to capture complementary anatomical and functional information, enabling more accurate diagnosis and treatment planning. Achieving aligned feature \u2026", "entry_id": "http://arxiv.org/abs/2506.18072v1", "updated": "2025-06-22 15:39:25", "published": "2025-06-22 15:39:25", "authors": "Yunhao Liu;Suyang Xi;Shiqi Liu;Hong Ding;Chicheng Jin;Chenxi Yang;Junjun He;Yiqing Shen", "summary": "Medical image analysis increasingly relies on the integration of multiple\nimaging modalities to capture complementary anatomical and functional\ninformation, enabling more accurate diagnosis and treatment planning. Achieving\naligned feature representations across these diverse modalities is therefore\nimportant for effective multimodal analysis. While contrastive language-image\npre-training (CLIP) and its variant have enabled image-text alignments, they\nrequire explicitly paired data between arbitrary two modalities, which is\ndifficult to acquire in medical contexts. To address the gap, we present\nMultimodal Medical Image Binding with Text (M\\textsuperscript{3}Bind), a novel\npre-training framework that enables seamless alignment of multiple medical\nimaging modalities through a shared text representation space without requiring\nexplicit paired data between any two medical image modalities. Specifically,\nbased on the insight that different images can naturally bind with text,\nM\\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text\nmodels to align their modality-specific text embedding space while preserving\ntheir original image-text alignments. Subsequently, we distill these\nmodality-specific text encoders into a unified model, creating a shared text\nembedding space. Experiments on X-ray, CT, retina, ECG, and pathological images\non multiple downstream tasks demonstrate that M\\textsuperscript{3}Bind achieves\nstate-of-the-art performance in zero-shot, few-shot classification and\ncross-modal retrieval tasks compared to its CLIP-like counterparts. These\nresults validate M\\textsuperscript{3}Bind's effectiveness in achieving\ncross-image-modal alignment for medical analysis.", "comment": "10 pages, 3 figures", "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2506.18072v1;http://arxiv.org/pdf/2506.18072v1", "pdf_url": "http://arxiv.org/pdf/2506.18072v1"}, {"title": "Deep Learning-based Alignment Measurement in Knee Radiographs", "link": "https://arxiv.org/pdf/2506.18209", "details": "Z Hu, D Cullen, P Thompson, D Johnson, C Bian\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Radiographic knee alignment (KA) measurement is important for predicting joint health and surgical outcomes after total knee replacement. Traditional methods for KA measurements are manual, time-consuming and require long-leg radiographs \u2026", "entry_id": "http://arxiv.org/abs/2506.18209v1", "updated": "2025-06-22 23:57:46", "published": "2025-06-22 23:57:46", "authors": "Zhisen Hu;Dominic Cullen;Peter Thompson;David Johnson;Chang Bian;Aleksei Tiulpin;Timothy Cootes;Claudia Lindner", "summary": "Radiographic knee alignment (KA) measurement is important for predicting\njoint health and surgical outcomes after total knee replacement. Traditional\nmethods for KA measurements are manual, time-consuming and require long-leg\nradiographs. This study proposes a deep learning-based method to measure KA in\nanteroposterior knee radiographs via automatically localized knee anatomical\nlandmarks. Our method builds on hourglass networks and incorporates an\nattention gate structure to enhance robustness and focus on key anatomical\nfeatures. To our knowledge, this is the first deep learning-based method to\nlocalize over 100 knee anatomical landmarks to fully outline the knee shape\nwhile integrating KA measurements on both pre-operative and post-operative\nimages. It provides highly accurate and reliable anatomical varus/valgus KA\nmeasurements using the anatomical tibiofemoral angle, achieving mean absolute\ndifferences ~1{\\deg} when compared to clinical ground truth measurements.\nAgreement between automated and clinical measurements was excellent\npre-operatively (intra-class correlation coefficient (ICC) = 0.97) and good\npost-operatively (ICC = 0.86). Our findings demonstrate that KA assessment can\nbe automated with high accuracy, creating opportunities for digitally enhanced\nclinical workflows.", "comment": "Accepted to MICCAI 2025", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2506.18209v1;http://arxiv.org/pdf/2506.18209v1", "pdf_url": "http://arxiv.org/pdf/2506.18209v1"}]
