[{"title": "Scaling Laws for Upcycling Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2502.03009", "details": "SP Liew, T Kato, S Takase - arXiv preprint arXiv:2502.03009, 2025", "abstract": "Pretraining large language models (LLMs) is resource-intensive, often requiring months of training time even with high-end GPU clusters. There are two approaches of mitigating such computational demands: reusing smaller models to train larger \u2026"}, {"title": "Language Models are Few-Shot Graders", "link": "https://arxiv.org/pdf/2502.13337", "details": "C Zhao, M Silva, S Poulsen - arXiv preprint arXiv:2502.13337, 2025", "abstract": "Providing evaluations to student work is a critical component of effective student learning, and automating its process can significantly reduce the workload on human graders. Automatic Short Answer Grading (ASAG) systems, enabled by \u2026"}, {"title": "Towards Statistical Factuality Guarantee for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2502.20560", "details": "Z Li, C Yan, NJ Jackson, W Cui, B Li, J Zhang, BA Malin - arXiv preprint arXiv \u2026, 2025", "abstract": "Advancements in Large Vision-Language Models (LVLMs) have demonstrated promising performance in a variety of vision-language tasks involving image- conditioned free-form text generation. However, growing concerns about \u2026"}, {"title": "$ Q\\sharp $: Provably Optimal Distributional RL for LLM Post-Training", "link": "https://arxiv.org/pdf/2502.20548", "details": "JP Zhou, K Wang, J Chang, Z Gao, N Kallus\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reinforcement learning (RL) post-training is crucial for LLM alignment and reasoning, but existing policy-based methods, such as PPO and DPO, can fall short of fixing shortcuts inherited from pre-training. In this work, we introduce $ Q\\sharp $, a \u2026"}, {"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models", "link": "https://arxiv.org/pdf/2502.03199%3F", "details": "J Wu, Y Shen, S Liu, Y Tang, S Song, X Wang, L Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the \u2026"}, {"title": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization", "link": "https://arxiv.org/pdf/2502.04306%3F", "details": "Y Wang, L Yang, G Li, M Wang, B Aragam - arXiv preprint arXiv:2502.04306, 2025", "abstract": "Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods \u2026"}, {"title": "Leveraging Large Language Models for Building Interpretable Rule-Based Data-to-Text Systems", "link": "https://arxiv.org/pdf/2502.20609", "details": "M Lango, O Dusek - arXiv preprint arXiv:2502.20609, 2025", "abstract": "We introduce a simple approach that uses a large language model (LLM) to automatically implement a fully interpretable rule-based data-to-text system in pure Python. Experimental evaluation on the WebNLG dataset showed that such a \u2026"}, {"title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training", "link": "https://arxiv.org/pdf/2502.06589", "details": "Y Zhuang, J Yang, H Jiang, X Liu, K Cheng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce \u2026"}, {"title": "A Training Data Recipe to Accelerate A* Search with Large Language Models", "link": "http://www.boyangli.org/paper/Gupta-EMNLP-2024.pdf", "details": "D Gupta, B Li", "abstract": "Abstract Combining Large Language Models (LLMs) with heuristic search algorithms like A* holds the promise of enhanced LLM reasoning and scalable inference. To accelerate training and reduce computational demands, we investigate the coreset \u2026"}]
