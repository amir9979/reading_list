[{"title": "MindLLM: Lightweight large language model pre-training, evaluation and domain application", "link": "https://www.sciencedirect.com/science/article/pii/S2666651024000111", "details": "Y Yang, H Sun, J Li, R Liu, Y Li, Y Liu, Y Gao, H Huang - AI Open, 2024", "abstract": "Abstract Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is \u2026"}, {"title": "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws", "link": "https://arxiv.org/pdf/2408.08310", "details": "R Li, Y Wei, M Zhang, N Yu, H Hu, H Peng - arXiv preprint arXiv:2408.08310, 2024", "abstract": "High-quality data is crucial for the pre-training performance of large language models. Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity. In \u2026"}, {"title": "Fairness Definitions in Language Models Explained", "link": "https://arxiv.org/pdf/2407.18454", "details": "TV Doan, Z Chu, Z Wang, W Zhang - arXiv preprint arXiv:2407.18454, 2024", "abstract": "Language Models (LMs) have demonstrated exceptional performance across various Natural Language Processing (NLP) tasks. Despite these advancements, LMs can inherit and amplify societal biases related to sensitive attributes such as \u2026"}, {"title": "Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning", "link": "https://arxiv.org/pdf/2407.18248", "details": "T Wang, S Li, W Lu - arXiv preprint arXiv:2407.18248, 2024", "abstract": "Effective training of language models (LMs) for mathematical reasoning tasks demands high-quality supervised fine-tuning data. Besides obtaining annotations from human experts, a common alternative is sampling from larger and more \u2026"}, {"title": "Leveraging LLM Reasoning Enhances Personalized Recommender Systems", "link": "https://arxiv.org/pdf/2408.00802", "details": "AY Tsai, A Kraft, L Jin, C Cai, A Hosseini, T Xu, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements have showcased the potential of Large Language Models (LLMs) in executing reasoning tasks, particularly facilitated by Chain-of-Thought (CoT) prompting. While tasks like arithmetic reasoning involve clear, definitive \u2026"}, {"title": "How rationals boost textual entailment modeling: Insights from large language models", "link": "https://www.sciencedirect.com/science/article/pii/S0045790624004440", "details": "DH Pham, T Le, HT Nguyen - Computers and Electrical Engineering, 2024", "abstract": "This study introduces an innovative methodology for rationale-based distillation in textual entailment. Central to our methodology is the use of diverse and deep rationale types generated by large language models, eliminating the need for explicit \u2026"}, {"title": "Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2408.02032", "details": "F Huo, W Xu, Z Zhang, H Wang, Z Chen, P Zhao - arXiv preprint arXiv:2408.02032, 2024", "abstract": "While Large Vision-Language Models (LVLMs) have rapidly advanced in recent years, the prevalent issue known as thehallucination'problem has emerged as a significant bottleneck, hindering their real-world deployments. Existing methods \u2026"}, {"title": "Detecting and Understanding Vulnerabilities in Language Models via Mechanistic Interpretability", "link": "https://arxiv.org/pdf/2407.19842", "details": "J Garc\u00eda-Carrasco, A Mat\u00e9, J Trujillo - arXiv preprint arXiv:2407.19842, 2024", "abstract": "Large Language Models (LLMs), characterized by being trained on broad amounts of data in a self-supervised manner, have shown impressive performance across a wide range of tasks. Indeed, their generative abilities have aroused interest on the \u2026"}, {"title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models", "link": "https://arxiv.org/pdf/2408.03907", "details": "SH Kumar, S Sahay, S Mazumder, E Okur\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have excelled at language understanding and generating human-level text. However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users \u2026"}]
