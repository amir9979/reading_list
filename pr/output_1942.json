[{"title": "WISER: Weak supervISion and supErvised Representation learning to improve drug response prediction in cancer", "link": "https://arxiv.org/pdf/2405.04078", "details": "K Shubham, A Jayagopal, SM Danish, P AP, V Rajan - arXiv preprint arXiv \u2026, 2024", "abstract": "Cancer, a leading cause of death globally, occurs due to genomic changes and manifests heterogeneously across patients. To advance research on personalized treatment strategies, the effectiveness of various drugs on cells derived from cancers \u2026"}, {"title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models", "link": "https://arxiv.org/pdf/2405.01535", "details": "S Kim, J Suk, S Longpre, BY Lin, J Shin, S Welleck\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source \u2026"}, {"title": "Autonomous Data Selection with Language Models for Mathematical Texts", "link": "https://openreview.net/pdf%3Fid%3DbBF077z8LF", "details": "Y Zhang, Y Luo, Y Yuan, AC Yao - ICLR 2024 Workshop on Navigating and \u2026, 2024", "abstract": "To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or \u2026"}, {"title": "Infusing internalized knowledge of language models into hybrid prompts for knowledgeable dialogue generation", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124005082", "details": "J Bai, Z Yan, S Zhang, J Yang, H Guo, Z Li - Knowledge-Based Systems, 2024", "abstract": "Existing knowledge-grounded dialogue (KGD) systems access the knowledge from an external knowledge base, then generate the context-coherent response accordingly. However, the knowledge access capability is constrained to the scale of \u2026"}, {"title": "Causal Evaluation of Language Models", "link": "https://arxiv.org/pdf/2405.00622", "details": "S Chen, B Peng, M Chen, R Wang, M Xu, X Zeng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for \u2026"}, {"title": "Almanac Copilot: Towards Autonomous Electronic Health Record Navigation", "link": "https://arxiv.org/pdf/2405.07896", "details": "C Zakka, J Cho, G Fahed, R Shad, M Moor, R Fong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Clinicians spend large amounts of time on clinical documentation, and inefficiencies impact quality of care and increase clinician burnout. Despite the promise of electronic medical records (EMR), the transition from paper-based records has been \u2026"}, {"title": "Annot-Mix: Learning with Noisy Class Labels from Multiple Annotators via a Mixup Extension", "link": "https://arxiv.org/pdf/2405.03386", "details": "M Herde, L L\u00fchrs, D Huseljic, B Sick - arXiv preprint arXiv:2405.03386, 2024", "abstract": "Training with noisy class labels impairs neural networks' generalization performance. In this context, mixup is a popular regularization technique to improve training robustness by making memorizing false class labels more difficult. However, mixup \u2026"}, {"title": "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models", "link": "https://arxiv.org/pdf/2405.06134", "details": "V Raina, R Ma, C McGhee, K Knill, M Gales - arXiv preprint arXiv:2405.06134, 2024", "abstract": "Recent developments in large speech foundation models like Whisper have led to their widespread use in many automatic speech recognition (ASR) applications. These systems incorporatespecial tokens' in their vocabulary, such as $\\texttt {< \u2026"}, {"title": "Recall Them All: Retrieval-Augmented Language Models for Long Object List Extraction from Long Documents", "link": "https://arxiv.org/pdf/2405.02732", "details": "S Singhania, S Razniewski, G Weikum - arXiv preprint arXiv:2405.02732, 2024", "abstract": "Methods for relation extraction from text mostly focus on high precision, at the cost of limited recall. High recall is crucial, though, to populate long lists of object entities that stand in a specific relation with a given subject. Cues for relevant objects can be \u2026"}]
