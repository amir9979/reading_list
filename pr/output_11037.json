[{"title": "HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.08378", "details": "S Zhu, W Dong, J Song, Y Guo, B Zheng - arXiv preprint arXiv:2412.08378, 2024", "abstract": "Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into \u2026"}, {"title": "CPLLM: Clinical prediction with large language models", "link": "https://journals.plos.org/digitalhealth/article%3Fid%3D10.1371/journal.pdig.0000680", "details": "O Ben Shoham, N Rappoport - PLOS Digital Health, 2024", "abstract": "We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for predicting clinical disease and readmission. We utilized quantization and fine-tuned the LLM using \u2026"}, {"title": "On the Role of Surrogates in Conformal Inference of Individual Causal Effects", "link": "https://arxiv.org/pdf/2412.12365%3F", "details": "C Gao, PB Gilbert, L Han - arXiv preprint arXiv:2412.12365, 2024", "abstract": "Learning the Individual Treatment Effect (ITE) is essential for personalized decision making, yet causal inference has traditionally focused on aggregated treatment effects. While integrating conformal prediction with causal inference can provide \u2026"}, {"title": "Aligning Crowd-Sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10818581/", "details": "MF Wong, CW Tan - IEEE Transactions on Big Data, 2024", "abstract": "This paper studies how AI-assisted programming and large language models (LLM) improve software developers' ability via AI tools (LLM agents) like Github Copilot and Amazon CodeWhisperer, while integrating human feedback to enhance \u2026"}, {"title": "Training large language models to reason in a continuous latent space", "link": "https://arxiv.org/pdf/2412.06769%3F", "details": "S Hao, S Sukhbaatar, DJ Su, X Li, Z Hu, J Weston\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) are restricted to reason in the\" language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may \u2026"}, {"title": "Copyright-Protected Language Generation via Adaptive Model Fusion", "link": "https://arxiv.org/pdf/2412.06619%3F", "details": "J Abad, K Donhauser, F Pinto, F Yang - arXiv preprint arXiv:2412.06619, 2024", "abstract": "The risk of language models reproducing copyrighted material from their training data has led to the development of various protective measures. Among these, inference-time strategies that impose constraints via post-processing have shown \u2026"}, {"title": "Minerva LLMs: The first family of Large Language Models trained from scratch on Italian data", "link": "https://ceur-ws.org/Vol-3878/76_main_long.pdf", "details": "R Orlando, L Moroni, PLH Cabot, E Barba, S Conia\u2026 - Proc. of CLiC-it, 2024", "abstract": "The growing interest in Large Language Models (LLMs) has accelerated research efforts to adapt these models for various languages. Despite this, pretraining LLMs from scratch for non-English languages remains underexplored. This is the case for \u2026"}, {"title": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models", "link": "https://arxiv.org/pdf/2412.02830", "details": "H Tran, Z Yao, J Wang, Y Zhang, Z Yang, H Yu - arXiv preprint arXiv:2412.02830, 2024", "abstract": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a versatile extension to the mutual reasoning framework (rStar), aimed at enhancing reasoning accuracy and factual integrity across large language models (LLMs) for \u2026"}, {"title": "LLM2: Let Large Language Models Harness System 2 Reasoning", "link": "https://arxiv.org/pdf/2412.20372", "details": "C Yang, C Shi, S Li, B Shui, Y Yang, W Lam - arXiv preprint arXiv:2412.20372, 2024", "abstract": "Large language models (LLMs) have exhibited impressive capabilities across a myriad of tasks, yet they occasionally yield undesirable outputs. We posit that these limitations are rooted in the foundational autoregressive architecture of LLMs, which \u2026"}]
