[{"title": "Balrog: Benchmarking agentic llm and vlm reasoning on games", "link": "https://arxiv.org/pdf/2411.13543", "details": "D Paglieri, B Cupia\u0142, S Coward, U Piterbarg, M Wolczyk\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities; however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require \u2026"}, {"title": "Is Small Really Beautiful for Central Bank Communication? Evaluating Language Models for Finance: Llama-3-70B, GPT-4, FinBERT-FOMC, FinBERT, and VADER", "link": "https://dl.acm.org/doi/pdf/10.1145/3677052.3698675", "details": "W Kim, J Sp\u00f6rer, CL Lee, S Handschuh - Proceedings of the 5th ACM International \u2026, 2024", "abstract": "This study compares the sentiment detection capabilities of language models for the domain of central bank communication, particularly the official statements released by the US Federal Open Market Committee (FOMC). While previous studies have \u2026"}, {"title": "Warmstarting for Scaling Language Models", "link": "https://arxiv.org/pdf/2411.07340%3F", "details": "N Mallik, M Janowski, J Hog, H Rakotoarison, A Klein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling model sizes to scale performance has worked remarkably well for the current large language models paradigm. The research and empirical findings of various scaling studies led to novel scaling results and laws that guides subsequent \u2026"}, {"title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2411.14432", "details": "Y Dong, Z Liu, HL Sun, J Yang, W Hu, Y Rao, Z Liu - arXiv preprint arXiv:2411.14432, 2024", "abstract": "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high \u2026"}, {"title": "Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training", "link": "https://arxiv.org/pdf/2412.00440", "details": "H Wang, C Ju, W Lin, S Xiao, M Chen, Y Huang, C Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In rapidly evolving field of vision-language models (VLMs), contrastive language- image pre-training (CLIP) has made significant strides, becoming foundation for various downstream tasks. However, relying on one-to-one (image, text) contrastive \u2026"}, {"title": "ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?", "link": "https://arxiv.org/pdf/2412.02368", "details": "L Zhang, S Eger, Y Cheng, W Zhai, J Belouadi, C Leiter\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal large language models (LLMs) have demonstrated impressive capabilities in generating high-quality images from textual instructions. However, their performance in generating scientific images--a critical application for \u2026"}, {"title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models", "link": "https://arxiv.org/pdf/2411.19477", "details": "Y Chen, X Pan, Y Li, B Ding, J Zhou - arXiv preprint arXiv:2411.19477, 2024", "abstract": "We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates $ N $ candidate solutions, and then chooses the \u2026"}, {"title": "Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training", "link": "https://arxiv.org/pdf/2411.14318%3F", "details": "Z Luo, X Zhang, X Liu, H Li, Y Gong, C Qi, P Cheng - arXiv preprint arXiv:2411.14318, 2024", "abstract": "It is well-known that a diverse corpus is critical for training large language models, which are typically constructed from a mixture of various domains. In general, previous efforts resort to sampling training data from different domains with static \u2026"}, {"title": "On Domain-Specific Post-Training for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2411.19930%3F", "details": "D Cheng, S Huang, Z Zhu, X Zhang, WX Zhao, Z Luan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent years have witnessed the rapid development of general multimodal large language models (MLLMs). However, adapting general MLLMs to specific domains, such as scientific fields and industrial applications, remains less explored. This paper \u2026"}]
