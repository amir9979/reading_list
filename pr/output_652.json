'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Evaluating General Vision-Language Models for Clinical'
[{"title": "Dialogue for Prompting: A Policy-Gradient-Based Discrete Prompt Generation for Few-Shot Learning", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29809/31402", "details": "C Li, X Liu, Y Wang, D Li, Y Lan, C Shen - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Prompt-based pre-trained language models (PLMs) paradigm has succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base \u2026"}, {"title": "Improving Language Model Reasoning with Self-motivated Learning", "link": "https://arxiv.org/pdf/2404.07017", "details": "Y Feng, Y Xu, L Qin, Y Wang, W Che - arXiv preprint arXiv:2404.07017, 2024", "abstract": "Large-scale high-quality training data is important for improving the performance of models. After trained with data that has rationales (reasoning steps), models gain reasoning capability. However, the dataset with high-quality rationales is relatively \u2026"}]
