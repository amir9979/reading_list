'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Fine-Tuning Language Models with Reward Learning on Po'
[{"title": "Learn\" No\" to Say\" Yes\" Better: Improving Vision-Language Models via Negations", "link": "https://arxiv.org/pdf/2403.20312", "details": "J Singh, I Shrivastava, M Vatsa, R Singh, A Bharati - arXiv preprint arXiv:2403.20312, 2024", "abstract": "Existing vision-language models (VLMs) treat text descriptions as a unit, confusing individual concepts in a prompt and impairing visual semantic matching and reasoning. An important aspect of reasoning in logic and language is negations. This \u2026"}, {"title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models", "link": "https://arxiv.org/pdf/2403.20262", "details": "T Thonet, J Rozen, L Besacier - arXiv preprint arXiv:2403.20262, 2024", "abstract": "Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities \u2026"}, {"title": "Generative Language Models for Personalized Information Understanding", "link": "https://scholarworks.umass.edu/cgi/viewcontent.cgi%3Farticle%3D4123%26context%3Ddissertations_2", "details": "P Cai - 2024", "abstract": "A major challenge in information understanding stems from the diverse nature of the audience, where individuals possess varying preferences, experiences, educational and cultural backgrounds. Consequently, adopting a one-size-fits-all approach to \u2026"}, {"title": "Tree-of-Reasoning Question Decomposition for Complex Question Answering with Large Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29928/31621", "details": "K Zhang, J Zeng, F Meng, Y Wang, S Sun, L Bai\u2026 - Proceedings of the AAAI \u2026, 2024", "abstract": "Large language models (LLMs) have recently demonstrated remarkable performance across various natual language processing tasks. In the field of multi- hop reasoning, the Chain-of-thought (CoT) prompt method has emerged as a \u2026"}, {"title": "Developing Healthcare Language Model Embedding Spaces", "link": "https://arxiv.org/pdf/2403.19802", "details": "N Taylor, D Schofield, A Kormilitzin, DW Joyce\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained Large Language Models (LLMs) often struggle on out-of-domain datasets like healthcare focused text. We explore specialized pre-training to adapt smaller LLMs to different healthcare datasets. Three methods are assessed \u2026"}, {"title": "Text clustering with LLM embeddings", "link": "https://arxiv.org/pdf/2403.15112", "details": "A Petukhova, JP Matos-Carvalho, N Fachada - arXiv preprint arXiv:2403.15112, 2024", "abstract": "Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings-particularly those used \u2026"}, {"title": "Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models", "link": "https://arxiv.org/pdf/2403.16999", "details": "H Shao, S Qian, H Xiao, G Song, Z Zong, L Wang, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper presents Visual CoT, a novel pipeline that leverages the reasoning capabilities of multi-modal large language models (MLLMs) by incorporating visual Chain-of-Thought (CoT) reasoning. While MLLMs have shown promise in various \u2026"}, {"title": "Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering", "link": "https://arxiv.org/pdf/2403.14783", "details": "B Jiang, Z Zhuang, SS Shivakumar, D Roth, CJ Taylor - arXiv preprint arXiv \u2026, 2024", "abstract": "This work explores the zero-shot capabilities of foundation models in Visual Question Answering (VQA) tasks. We propose an adaptive multi-agent system, named Multi- Agent VQA, to overcome the limitations of foundation models in object detection and \u2026"}, {"title": "Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models", "link": "https://arxiv.org/pdf/2403.17359", "details": "Z Pan, H Luo, M Li, H Liu - arXiv preprint arXiv:2403.17359, 2024", "abstract": "We present a Chain-of-Action (CoA) framework for multimodal and retrieval- augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications:(i) unfaithful hallucination that is \u2026"}]
