'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Path-Aware Cross-Attention Network for Question Answering](h'
[{"title": "Rethinking Personalized Client Collaboration in Federated Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10517642/", "details": "L Wu, S Guo, Y Ding, J Wang, W Xu, Y Zhan\u2026 - IEEE Transactions on \u2026, 2024", "abstract": "Federated Learning (FL) has gained considerable attention recently, as it allows clients to cooperatively train a global machine learning model without sharing raw data. However, its performance can be compromised due to the high heterogeneity \u2026"}, {"title": "Eyes Can Deceive: Benchmarking Counterfactual Reasoning Abilities of Multi-modal Large Language Models", "link": "https://arxiv.org/pdf/2404.12966", "details": "Y Li, W Tian, Y Jiao, J Chen, YG Jiang - arXiv preprint arXiv:2404.12966, 2024", "abstract": "Counterfactual reasoning, as a crucial manifestation of human intelligence, refers to making presuppositions based on established facts and extrapolating potential outcomes. Existing multimodal large language models (MLLMs) have exhibited \u2026"}, {"title": "Understanding Multimodal Contrastive Learning Through Pointwise Mutual Information", "link": "https://arxiv.org/pdf/2404.19228", "details": "T Uesaka, T Suzuki, Y Takida, CH Lai, N Murata\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal representation learning to integrate different modalities, such as text, vision, and audio is important for real-world applications. The symmetric InfoNCE loss proposed in CLIP is a key concept in multimodal representation learning. In this \u2026"}, {"title": "Enabling action crossmodality for a pretrained large language model", "link": "https://www.sciencedirect.com/science/article/pii/S2949719124000207", "details": "A Caesar, O \u00d6zdemir, C Weber, S Wermter - Natural Language Processing Journal, 2024", "abstract": "Natural language processing and vision tasks have seen large improvements recently through the rise of Transformer architectures. The high performing large language models (LLMs) benefit from large textual datasets that are numerously \u2026"}]
