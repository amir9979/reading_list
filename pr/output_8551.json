[{"title": "Belief in the Machine: Investigating Epistemological Blind Spots of Language Models", "link": "https://arxiv.org/pdf/2410.21195", "details": "M Suzgun, T Gur, F Bianchi, DE Ho, T Icard, D Jurafsky\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As language models (LMs) become integral to fields like healthcare, law, and journalism, their ability to differentiate between fact, belief, and knowledge is essential for reliable decision-making. Failure to grasp these distinctions can lead to \u2026"}, {"title": "Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language Models", "link": "https://arxiv.org/pdf/2410.20745%3F", "details": "Y Jin, Z Li, C Zhang, T Cao, Y Gao, P Jayarao, M Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Online shopping is a complex multi-task, few-shot learning problem with a wide and evolving range of entities, relations, and tasks. However, existing models and benchmarks are commonly tailored to specific tasks, falling short of capturing the full \u2026"}, {"title": "Constraint Back-translation Improves Complex Instruction Following of Large Language Models", "link": "https://arxiv.org/pdf/2410.24175", "details": "Y Qi, H Peng, X Wang, B Xu, L Hou, J Li - arXiv preprint arXiv:2410.24175, 2024", "abstract": "Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs \u2026"}, {"title": "SWITCH: Studying with Teacher for Knowledge Distillation of Large Language Models", "link": "https://arxiv.org/pdf/2410.19503", "details": "J Koo, Y Hwang, Y Kim, T Kang, H Bae, K Jung - arXiv preprint arXiv:2410.19503, 2024", "abstract": "Despite the success of Large Language Models (LLMs), they still face challenges related to high inference costs and memory requirements. To address these issues, Knowledge Distillation (KD) has emerged as a popular method for model \u2026"}, {"title": "Dynamic Strategy Planning for Efficient Question Answering with Large Language Models", "link": "https://arxiv.org/pdf/2410.23511", "details": "T Parekh, P Prakash, A Radovic, A Shekher\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Research has shown the effectiveness of reasoning (eg, Chain-of-Thought), planning (eg, SelfAsk), and retrieval augmented generation strategies to improve the performance of Large Language Models (LLMs) on various tasks, such as question \u2026"}, {"title": "Flaming-hot Initiation with Regular Execution Sampling for Large Language Models", "link": "https://arxiv.org/pdf/2410.21236", "details": "W Chen, Z Zhang, G Liu, R Zheng, W Shi, C Dun, Z Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Since the release of ChatGPT, large language models (LLMs) have demonstrated remarkable capabilities across various domains. A key challenge in developing these general capabilities is efficiently sourcing diverse, high-quality data. This \u2026"}, {"title": "UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2411.01703", "details": "S Oh, Y Jin, M Sharma, D Kim, E Ma, G Verma\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal large language models (MLLMs) have revolutionized vision-language understanding but are vulnerable to multimodal jailbreak attacks, where adversaries meticulously craft inputs to elicit harmful or inappropriate responses. We propose \u2026"}, {"title": "Pretraining and finetuning language models on geospatial networks for accurate address matching", "link": "https://www.amazon.science/publications/pretraining-and-finetuning-language-models-on-geospatial-networks-for-accurate-address-matching", "details": "S Maheshwary, A Paul, S Sohoney - 2024", "abstract": "We propose a novel framework for pretraining and fine-tuning language models with the goal of determining whether two addresses represent the same physical building. Address matching and building authoritative address catalogues are important to \u2026"}, {"title": "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "link": "https://arxiv.org/pdf/2410.20008", "details": "Z Zhao, Y Ziser, SB Cohen - arXiv preprint arXiv:2410.20008, 2024", "abstract": "Fine-tuning pre-trained large language models (LLMs) on a diverse array of tasks has become a common approach for building models that can solve various natural language processing (NLP) tasks. However, where and to what extent these models \u2026"}]
