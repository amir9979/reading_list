[{"title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining", "link": "https://arxiv.org/pdf/2501.00958", "details": "W Zhang, H Zhang, X Li, J Sun, Y Shen, W Lu, D Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge \u2026"}, {"title": "Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models", "link": "https://arxiv.org/pdf/2412.18609%3F", "details": "J Yi, ST Wasim, Y Luo, M Naseer, J Gall - arXiv preprint arXiv:2412.18609, 2024", "abstract": "We present an efficient encoder-free approach for video-language understanding that achieves competitive performance while significantly reducing computational overhead. Current video-language models typically rely on heavyweight image \u2026"}, {"title": "Unifying Specialized Visual Encoders for Video Language Models", "link": "https://arxiv.org/pdf/2501.01426", "details": "J Chung, T Zhu, MG Saez-Diez, JC Niebles, H Zhou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of \u2026"}, {"title": "Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach", "link": "https://arxiv.org/pdf/2412.18108", "details": "J Bi, J Guo, Y Tang, LB Wen, Z Liu, C Xu - arXiv preprint arXiv:2412.18108, 2024", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated remarkable progress in visual understanding. This impressive leap raises a compelling question: how can language models, initially trained solely on \u2026"}, {"title": "Bridged Semantic Alignment for Zero-shot 3D Medical Image Diagnosis", "link": "https://arxiv.org/pdf/2501.03565", "details": "H Lai, Z Jiang, Q Yao, R Wang, Z He, X Tao, W Wei\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "3D medical images such as Computed tomography (CT) are widely used in clinical practice, offering a great potential for automatic diagnosis. Supervised learning- based approaches have achieved significant progress but rely heavily on extensive \u2026"}, {"title": "MMFactory: A Universal Solution Search Engine for Vision-Language Tasks", "link": "https://arxiv.org/pdf/2412.18072", "details": "WC Fan, T Rahman, L Sigal - arXiv preprint arXiv:2412.18072, 2024", "abstract": "With advances in foundational and vision-language models, and effective fine-tuning techniques, a large number of both general and special-purpose models have been developed for a variety of visual tasks. Despite the flexibility and accessibility of these \u2026"}, {"title": "ICONS: Influence Consensus for Vision-Language Data Selection", "link": "https://arxiv.org/pdf/2501.00654", "details": "X Wu, M Xia, R Shao, Z Deng, PW Koh, O Russakovsky - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual Instruction Tuning typically requires a large amount of vision-language training data. This data often containing redundant information that increases computational costs without proportional performance gains. In this work, we \u2026"}, {"title": "Valley2: Exploring Multimodal Models with Scalable Vision-Language Design", "link": "https://arxiv.org/pdf/2501.05901", "details": "Z Wu, Z Chen, R Luo, C Zhang, Y Gao, Z He, X Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, vision-language models have made remarkable progress, demonstrating outstanding capabilities in various tasks such as image captioning and video understanding. We introduce Valley2, a novel multimodal large language model \u2026"}, {"title": "Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment", "link": "https://arxiv.org/pdf/2412.19326", "details": "Z Yan, Z Li, Y He, C Wang, K Li, X Li, X Zeng, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Current multimodal large language models (MLLMs) struggle with fine-grained or precise understanding of visuals though they give comprehensive perception and reasoning in a spectrum of vision applications. Recent studies either develop tool \u2026"}]
