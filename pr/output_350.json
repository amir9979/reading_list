'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Smaller Language Models are Better Zero-shot Machine-G'
[{"title": "PE-Ynet: a novel attention-based multi-task model for pulmonary embolism detection using CT pulmonary angiography (CTPA) scan images", "link": "https://link.springer.com/article/10.1007/s13246-024-01410-3", "details": "GR Hemalakshmi, M Murugappan, MY Sikkandar\u2026 - Physical and Engineering \u2026, 2024", "abstract": "Pulmonary Embolism (PE) has diverse manifestations with different etiologies such as venous thromboembolism, septic embolism, and paradoxical embolism. In this study, a novel attention-based multi-task model is proposed for PE segmentation and \u2026"}, {"title": "RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models", "link": "https://arxiv.org/html/2403.02271v1", "details": "S Najafi, A Fyshe - arXiv preprint arXiv:2403.02271, 2024", "abstract": "Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter- efficient fine-tuning methods that optimize input prompts or adjust a small number of \u2026"}, {"title": "A Comparison of Parameter-Efficient ASR Domain Adaptation Methods for Universal Speech and Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10445894/", "details": "KC Sim, Z Huo, T Munkhdalai, N Siddhartha, A Stooke\u2026 - ICASSP 2024-2024 IEEE \u2026, 2024", "abstract": "A recent paradigm shift in artificial intelligence has seen the rise of foundation models, such as the large language models and the universal speech models. With billions of model parameters and trained with a wide range of data, these foundation \u2026"}, {"title": "A Novel Corpus of Annotated Medical Imaging Reports and Information Extraction Results Using BERT-based Language Models", "link": "https://arxiv.org/pdf/2403.18975", "details": "N Park, K Lybarger, GK Ramachandran, S Lewis\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical imaging is critical to the diagnosis, surveillance, and treatment of many health conditions, including oncological, neurological, cardiovascular, and musculoskeletal disorders, among others. Radiologists interpret these complex \u2026"}, {"title": "Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records", "link": "https://arxiv.org/pdf/2403.09226", "details": "A Ziletti, L D'Ambrosi - arXiv preprint arXiv:2403.09226, 2024", "abstract": "Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. Querying these databases to answer epidemiological questions is challenging due to the intricacy of medical \u2026"}, {"title": "DCS: Debiased Contrastive Learning with Weak Supervision for Time Series Classification", "link": "https://ieeexplore.ieee.org/abstract/document/10446381/", "details": "R Cai, L Peng, Z Lu, K Zhang, Y Liu - \u2026 2024-2024 IEEE International Conference on \u2026, 2024", "abstract": "Self-supervised contrastive learning (SSCL) has performed excellently on time series classification tasks. Most SSCL-based classification algorithms generate positive and negative samples in the time or frequency domains, focusing on mining similarities \u2026"}, {"title": "Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity", "link": "https://arxiv.org/pdf/2403.05257", "details": "S Hoshino, A Kato, S Murakami, P Zhang - arXiv preprint arXiv:2403.05257, 2024", "abstract": "Learning better sentence embeddings leads to improved performance for natural language understanding tasks including semantic textual similarity (STS) and natural language inference (NLI). As prior studies leverage large-scale labeled NLI datasets \u2026"}, {"title": "HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild", "link": "https://arxiv.org/pdf/2403.04307", "details": "Z Zhu, Z Sun, Y Yang - arXiv preprint arXiv:2403.04307, 2024", "abstract": "Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains. Recent benchmarks designed to assess LLM hallucinations within conventional NLP tasks, such as knowledge-intensive question \u2026"}, {"title": "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models", "link": "https://arxiv.org/html/2403.02178v1", "details": "C Chen, X Wang, TE Lin, A Lv, Y Wu, X Gao, JR Wen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains. Earlier fine- tuning approaches sought to mitigate this by leveraging more precise supervisory \u2026"}]
