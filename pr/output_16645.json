[{"title": "OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning", "link": "https://arxiv.org/pdf/2505.11031", "details": "X Zhang, H Lai, Q Meng, J Bos - arXiv preprint arXiv:2505.11031, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) have demonstrated remarkable capabilities across a range of natural language processing tasks, yet their \u2026 ONTOURL, the first comprehensive benchmark designed to systematically **evaluate** LLMs\u2019 proficiency in \u2026", "entry_id": "http://arxiv.org/abs/2505.11031v2", "updated": "2025-05-19 08:19:47", "published": "2025-05-16 09:26:06", "authors": "Xiao Zhang;Huiyuan Lai;Qianru Meng;Johan Bos", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of natural language processing tasks, yet their ability to process\nstructured symbolic knowledge remains underexplored. To address this gap, we\npropose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the\nfirst comprehensive benchmark designed to systematically evaluate LLMs'\nproficiency in handling ontologies -- formal, symbolic representations of\ndomain knowledge through concepts, relationships, and instances. Based on the\nproposed taxonomy, OntoURL systematically assesses three dimensions:\nunderstanding, reasoning, and learning through 15 distinct tasks comprising\n58,981 questions derived from 40 ontologies across 8 domains. Experiments with\n20 open-source LLMs reveal significant performance differences across models,\ntasks, and domains, with current LLMs showing proficiency in understanding\nontological knowledge but substantial weaknesses in reasoning and learning\ntasks. These findings highlight fundamental limitations in LLMs' capability to\nprocess symbolic knowledge and establish OntoURL as a critical benchmark for\nadvancing the integration of LLMs with formal knowledge representations.", "comment": "Paper submitted to NeurIPS 2025 dataset and benchmark track", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.11031v2;http://arxiv.org/pdf/2505.11031v2", "pdf_url": "http://arxiv.org/pdf/2505.11031v2"}, {"title": "Disentangling Reasoning and Knowledge in Medical Large Language Models", "link": "https://arxiv.org/pdf/2505.11462", "details": "R Thapa, Q Wu, K Wu, H Zhang, A Zhang, E Wu, H Ye\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **evaluation** pipeline for benchmarking biomedical reasoning in LLMs. We begin by curating and cleaning the **evaluation** datasets, followed by two complementary **evaluation** \u2026 traces to **evaluate** backtracking and self-correction. An overview of the \u2026", "entry_id": "http://arxiv.org/abs/2505.11462v1", "updated": "2025-05-16 17:16:27", "published": "2025-05-16 17:16:27", "authors": "Rahul Thapa;Qingyang Wu;Kevin Wu;Harrison Zhang;Angela Zhang;Eric Wu;Haotian Ye;Suhana Bedi;Nevin Aresh;Joseph Boen;Shriya Reddy;Ben Athiwaratkun;Shuaiwen Leon Song;James Zou", "summary": "Medical reasoning in large language models (LLMs) aims to emulate clinicians'\ndiagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and\nPubMedQA often mix reasoning with factual recall. We address this by separating\n11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using\na PubMedBERT classifier that reaches 81 percent accuracy, comparable to human\nperformance. Our analysis shows that only 32.8 percent of questions require\ncomplex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)\nand general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent\ngaps between knowledge and reasoning performance. For example, m1 scores 60.5\non knowledge but only 47.1 on reasoning. In adversarial tests where models are\nmisled with incorrect initial reasoning, biomedical models degrade sharply,\nwhile larger or RL-trained general models show more robustness. To address\nthis, we train BioMed-R1 using fine-tuning and reinforcement learning on\nreasoning-heavy examples. It achieves the strongest performance among similarly\nsized models. Further gains may come from incorporating clinical case reports\nand training with adversarial and backtracking scenarios.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.11462v1;http://arxiv.org/pdf/2505.11462v1", "pdf_url": "http://arxiv.org/pdf/2505.11462v1"}, {"title": "EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2505.11405", "details": "B Xing, X Liu, G Zhao, C Liu, X Fu, H K\u00e4lvi\u00e4inen - arXiv preprint arXiv:2505.11405, 2025", "abstract": "\u2026 To reduce these **evaluation** biases, we propose EmotionHallucer with an adversarial **evaluation** framework inspired by [75, 77]. For each **evaluation** instance, we construct a pair of complementary questions: a basic question, which tests the \u2026", "entry_id": "http://arxiv.org/abs/2505.11405v1", "updated": "2025-05-16 16:14:08", "published": "2025-05-16 16:14:08", "authors": "Bohao Xing;Xin Liu;Guoying Zhao;Chengyu Liu;Xiaolan Fu;Heikki K\u00e4lvi\u00e4inen", "summary": "Emotion understanding is a critical yet challenging task. Recent advances in\nMultimodal Large Language Models (MLLMs) have significantly enhanced their\ncapabilities in this area. However, MLLMs often suffer from hallucinations,\ngenerating irrelevant or nonsensical content. To the best of our knowledge,\ndespite the importance of this issue, there has been no dedicated effort to\nevaluate emotion-related hallucinations in MLLMs. In this work, we introduce\nEmotionHallucer, the first benchmark for detecting and analyzing emotion\nhallucinations in MLLMs. Unlike humans, whose emotion understanding stems from\nthe interplay of biology and social learning, MLLMs rely solely on data-driven\nlearning and lack innate emotional instincts. Fortunately, emotion psychology\nprovides a solid foundation of knowledge about human emotions. Building on\nthis, we assess emotion hallucinations from two dimensions: emotion psychology\nknowledge and real-world multimodal perception. To support robust evaluation,\nwe utilize an adversarial binary question-answer (QA) framework, which employs\ncarefully crafted basic and hallucinated pairs to assess the emotion\nhallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on\nEmotionHallucer, we reveal that: i) most current models exhibit substantial\nissues with emotion hallucinations; ii) closed-source models outperform\nopen-source ones in detecting emotion hallucinations, and reasoning capability\nprovides additional advantages; iii) existing models perform better in emotion\npsychology knowledge than in multimodal emotion perception. As a byproduct,\nthese findings inspire us to propose the PEP-MEK framework, which yields an\naverage improvement of 9.90% in emotion hallucination detection across selected\nmodels. Resources will be available at\nhttps://github.com/xxtars/EmotionHallucer.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.CL", "links": "http://arxiv.org/abs/2505.11405v1;http://arxiv.org/pdf/2505.11405v1", "pdf_url": "http://arxiv.org/pdf/2505.11405v1"}, {"title": "Phare: A Safety Probe for Large Language Models", "link": "https://arxiv.org/pdf/2505.11365", "details": "PL Jeune, B Mal\u00e9sieux, W Xiao, M Dora - arXiv preprint arXiv:2505.11365, 2025", "abstract": "\u2026 Ensuring the safety of **large** **language** **models** (LLMs) is critical for responsible deployment, yet existing **evaluations** often prioritize \u2026 **Large** **Language** **Models** (LLMs) have become foundational tools in artificial intelligence, enabling breakthroughs in \u2026", "entry_id": "http://arxiv.org/abs/2505.11365v2", "updated": "2025-05-19 09:01:44", "published": "2025-05-16 15:31:08", "authors": "Pierre Le Jeune;Beno\u00eet Mal\u00e9zieux;Weixuan Xiao;Matteo Dora", "summary": "Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.", "comment": null, "journal_ref": null, "primary_category": "cs.CY", "categories": "cs.CY;cs.AI;cs.CL;cs.CR", "links": "http://arxiv.org/abs/2505.11365v2;http://arxiv.org/pdf/2505.11365v2", "pdf_url": "http://arxiv.org/pdf/2505.11365v2"}, {"title": "Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models", "link": "https://arxiv.org/pdf/2505.11341", "details": "BC Figueras, R Agerri - arXiv preprint arXiv:2505.11341, 2025", "abstract": "\u2026 We also investigate automatic **evaluation** methods and identify a reference-based technique using **large** **language** **models** (LLMs) as the strategy that best correlates with human judgments. Our zero-shot **evaluation** of 11 LLMs establishes a strong \u2026", "entry_id": "http://arxiv.org/abs/2505.11341v2", "updated": "2025-05-20 10:15:19", "published": "2025-05-16 15:08:04", "authors": "Banca Calvo Figueras;Rodrigo Agerri", "summary": "The task of Critical Questions Generation (CQs-Gen) aims to foster critical\nthinking by enabling systems to generate questions that expose underlying\nassumptions and challenge the validity of argumentative reasoning structures.\nDespite growing interest in this area, progress has been hindered by the lack\nof suitable datasets and automatic evaluation standards. This paper presents a\ncomprehensive approach to support the development and benchmarking of systems\nfor this task. We construct the first large-scale dataset including $~$5K\nmanually annotated questions. We also investigate automatic evaluation methods\nand propose a reference-based technique using large language models (LLMs) as\nthe strategy that best correlates with human judgments. Our zero-shot\nevaluation of 11 LLMs establishes a strong baseline while showcasing the\ndifficulty of the task. Data and code plus a public leaderboard are provided to\nencourage further research not only in terms of model performance, but also to\nexplore the practical benefits of CQs-Gen for both automated reasoning and\nhuman critical thinking.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.11341v2;http://arxiv.org/pdf/2505.11341v2", "pdf_url": "http://arxiv.org/pdf/2505.11341v2"}, {"title": "Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models", "link": "https://arxiv.org/pdf/2505.11010", "details": "J Wu, C Wang, TH Su, J Yang, H Lin, C Zhang, M Peng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 This section compares Review-Instruct with several state-of-the-art **large** **language** **models** (LLMs), all initialized from the Llama2-13b base \u2026 For the MT-Bench **evaluation** , we report scores for the first and second turns, as well as the average \u2026", "entry_id": "http://arxiv.org/abs/2505.11010v1", "updated": "2025-05-16 08:59:07", "published": "2025-05-16 08:59:07", "authors": "Jiangxu Wu;Cong Wang;TianHuang Su;Jun Yang;Haozhi Lin;Chao Zhang;Ming Peng;Kai Shi;SongPan Yang;BinQing Pan;ZiXian Li;Ni Yang;ZhenYu Yang", "summary": "The effectiveness of large language models (LLMs) in conversational AI is\nhindered by their reliance on single-turn supervised fine-tuning (SFT) data,\nwhich limits contextual coherence in multi-turn dialogues. Existing methods for\ngenerating multi-turn dialogue data struggle to ensure both diversity and\nquality in instructions. To address this, we propose Review-Instruct, a novel\nframework that synthesizes multi-turn conversations through an iterative\n\"Ask-Respond-Review\" process involving three agent roles: a Candidate, multiple\nReviewers, and a Chairman. The framework iteratively refines instructions by\nincorporating Reviewer feedback, enhancing dialogue diversity and difficulty.\nWe construct a multi-turn dataset using the Alpaca dataset and fine-tune the\nLLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate\nsignificant improvements, achieving absolute gains of 2.9\\% on MMLU-Pro and 2\\%\non MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B.\nAblation studies confirm the critical role of the Review stage and the use of\nmultiple Reviewers in boosting instruction diversity and difficulty. Our work\nhighlights the potential of review-driven, multi-agent frameworks for\ngenerating high-quality conversational data at scale.", "comment": "ACL2025 Accepted", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.11010v1;http://arxiv.org/pdf/2505.11010v1", "pdf_url": "http://arxiv.org/pdf/2505.11010v1"}, {"title": "Scaling Reasoning can Improve Factuality in Large Language Models", "link": "https://arxiv.org/pdf/2505.11140", "details": "M Zhang, J Bjerva, R Biswas - arXiv preprint arXiv:2505.11140, 2025", "abstract": "\u2026 We **evaluate** reasoning traces using three methods: (1) Exact Match, checking if the \\boxed{} answer exactly matches or is a sub-phrase of any gold answer; (2) Semantic Match, accepting answers with a maximum similarity score  >0.5; and (3) \u2026", "entry_id": "http://arxiv.org/abs/2505.11140v1", "updated": "2025-05-16 11:39:33", "published": "2025-05-16 11:39:33", "authors": "Mike Zhang;Johannes Bjerva;Russa Biswas", "summary": "Recent studies on large language model (LLM) reasoning capabilities have\ndemonstrated promising improvements in model performance by leveraging a\nlengthy thinking process and additional computational resources during\ninference, primarily in tasks involving mathematical reasoning (Muennighoff et\nal., 2025). However, it remains uncertain if longer reasoning chains inherently\nenhance factual accuracy, particularly beyond mathematical contexts. In this\nwork, we thoroughly examine LLM reasoning within complex open-domain\nquestion-answering (QA) scenarios. We initially distill reasoning traces from\nadvanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then\nfine-tune a variety of models ranging from smaller, instruction-tuned variants\nto larger architectures based on Qwen2.5. To enrich reasoning traces, we\nintroduce factual information from knowledge graphs in the form of paths into\nour reasoning traces. Our experimental setup includes four baseline approaches\nand six different instruction-tuned models evaluated across a benchmark of six\ndatasets, encompassing over 22.6K questions. Overall, we carry out 168\nexperimental runs and analyze approximately 1.7 million reasoning traces. Our\nfindings indicate that, within a single run, smaller reasoning models achieve\nnoticeable improvements in factual accuracy compared to their original\ninstruction-tuned counterparts. Moreover, our analysis demonstrates that adding\ntest-time compute and token budgets factual accuracy consistently improves by\n2-8%, further confirming the effectiveness of test-time scaling for enhancing\nperformance and consequently improving reasoning accuracy in open-domain QA\ntasks. We release all the experimental artifacts for further research.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.11140v1;http://arxiv.org/pdf/2505.11140v1", "pdf_url": "http://arxiv.org/pdf/2505.11140v1"}, {"title": "MatTools: Benchmarking Large Language Models for Materials Science Tools", "link": "https://arxiv.org/pdf/2505.10852", "details": "S Liu, J Xu, B Ye, B Hu, DJ Srolovitz, T Wen - arXiv preprint arXiv:2505.10852, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) are increasingly applied to materials science questions, \u2026 Here, we propose a benchmark application to **evaluate** the proficiency of LLMs to answer \u2026 Our **evaluation** of diverse LLMs yields three key insights: (1) \u2026", "entry_id": "http://arxiv.org/abs/2505.10852v1", "updated": "2025-05-16 04:43:05", "published": "2025-05-16 04:43:05", "authors": "Siyu Liu;Jiamin Xu;Beilin Ye;Bo Hu;David J. Srolovitz;Tongqi Wen", "summary": "Large language models (LLMs) are increasingly applied to materials science\nquestions, including literature comprehension, property prediction, materials\ndiscovery and alloy design. At the same time, a wide range of physics-based\ncomputational approaches have been developed in which materials properties can\nbe calculated. Here, we propose a benchmark application to evaluate the\nproficiency of LLMs to answer materials science questions through the\ngeneration and safe execution of codes based on such physics-based\ncomputational materials science packages. MatTools is built on two\ncomplementary components: a materials simulation tool question-answer (QA)\nbenchmark and a real-world tool-usage benchmark. We designed an automated\nmethodology to efficiently collect real-world materials science tool-use\nexamples. The QA benchmark, derived from the pymatgen (Python Materials\nGenomics) codebase and documentation, comprises 69,225 QA pairs that assess the\nability of an LLM to understand materials science tools. The real-world\nbenchmark contains 49 tasks (138 subtasks) requiring the generation of\nfunctional Python code for materials property calculations. Our evaluation of\ndiverse LLMs yields three key insights: (1)Generalists outshine\nspecialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a\nstandardized framework for assessing and improving LLM capabilities for\nmaterials science tool applications, facilitating the development of more\neffective AI systems for materials science and general scientific research.", "comment": "27 pages, 23 figures", "journal_ref": null, "primary_category": "cond-mat.mtrl-sci", "categories": "cond-mat.mtrl-sci;cs.CL;cs.DB", "links": "http://arxiv.org/abs/2505.10852v1;http://arxiv.org/pdf/2505.10852v1", "pdf_url": "http://arxiv.org/pdf/2505.10852v1"}, {"title": "Search-augmented **Large Language Models** : Transforming Internet Search-a Systematic Literature Review", "link": "https://aisel.aisnet.org/ecis2025/hci/hci/4/", "details": "B Wildhaber, A G\u00f6ldi, R Rietsche - 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) have revolutionized natural language processing but often rely on outdated information, limiting their \u2026 We conduct a systematic literature review, categorizing findings into two main streams: **evaluation** \u2026"}]
