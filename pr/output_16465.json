[{"title": "Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models", "link": "https://arxiv.org/pdf/2505.09062", "details": "J Zhao, Y Song, E Cohen - arXiv preprint arXiv:2505.09062, 2025", "abstract": "Recent advancements in source code summarization have leveraged transformer- based pre-trained models, including Large Language Models of Code (LLMCs), to automate and improve the generation of code summaries. However, existing \u2026"}, {"title": "Analyzing and Improving Coherence of Large Language Models in Question Answering", "link": "https://aclanthology.org/2025.naacl-long.588.pdf", "details": "I Lauriola, S Campese, A Moschitti - Proceedings of the 2025 Conference of the \u2026, 2025", "abstract": "Large language models (LLMs) have recently revolutionized natural language processing. These models, however, often suffer from instability or lack of coherence, that is the ability of the models to generate semantically equivalent outputs when \u2026"}, {"title": "PLHF: Prompt Optimization with Few-Shot Human Feedback", "link": "https://arxiv.org/pdf/2505.07886", "details": "CP Yang, K Zheng, SD Lin - arXiv preprint arXiv:2505.07886, 2025", "abstract": "Automatic prompt optimization frameworks are developed to obtain suitable prompts for large language models (LLMs) with respect to desired output quality metrics. Although existing approaches can handle conventional tasks such as fixed-solution \u2026"}]
