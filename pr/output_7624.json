[{"title": "Scaling Parameter-Constrained Language Models with Quality Data", "link": "https://arxiv.org/pdf/2410.03083", "details": "E Chang, M Paltenghi, Y Li, PJ Lin, C Zhao, P Huber\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws in language modeling traditionally quantify training loss as a function of dataset size and model parameters, providing compute-optimal estimates but often neglecting the impact of data quality on model generalization. In this paper, we \u2026"}, {"title": "Are Expert-Level Language Models Expert-Level Annotators?", "link": "https://arxiv.org/pdf/2410.03254", "details": "YM Tseng, WL Chen, CC Chen, HH Chen - arXiv preprint arXiv:2410.03254, 2024", "abstract": "Data annotation refers to the labeling or tagging of textual data with relevant information. A large body of works have reported positive results on leveraging LLMs as an alternative to human annotators. However, existing studies focus on classic \u2026"}, {"title": "Erasing Conceptual Knowledge from Language Models", "link": "https://arxiv.org/pdf/2410.02760", "details": "R Gandikota, S Feucht, S Marks, D Bau - arXiv preprint arXiv:2410.02760, 2024", "abstract": "Concept erasure in language models has traditionally lacked a comprehensive evaluation framework, leading to incomplete assessments of effectiveness of erasure methods. We propose an evaluation paradigm centered on three critical criteria \u2026"}, {"title": "General Preference Modeling with Preference Representations for Aligning Language Models", "link": "https://arxiv.org/pdf/2410.02197%3F", "details": "Y Zhang, G Zhang, Y Wu, K Xu, Q Gu - arXiv preprint arXiv:2410.02197, 2024", "abstract": "Modeling human preferences is crucial for aligning foundation models with human values. Traditional reward modeling methods, such as the Bradley-Terry (BT) reward model, fall short in expressiveness, particularly in addressing intransitive \u2026"}, {"title": "Law of the Weakest Link: Cross Capabilities of Large Language Models", "link": "https://arxiv.org/pdf/2409.19951", "details": "M Zhong, A Zhang, X Wang, R Hou, W Xiong, C Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real \u2026"}, {"title": "PALM: Predicting Actions through Language Models", "link": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10743.pdf", "details": "S Kim, D Huang, Y Xian, O Hilliges, L Van Gool\u2026 - European Conference on \u2026, 2025", "abstract": "Understanding human activity is a crucial yet intricate task in egocentric vision, a field that focuses on capturing visual perspectives from the camera wearer's viewpoint. Traditional methods heavily rely on representation learning that is trained on a large \u2026"}, {"title": "Data-Centric AI Governance: Addressing the Limitations of Model-Focused Policies", "link": "https://arxiv.org/pdf/2409.17216", "details": "R Gupta, L Walker, R Corona, S Fu, S Petryk\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Current regulations on powerful AI capabilities are narrowly focused on\" foundation\" or\" frontier\" models. However, these terms are vague and inconsistently defined, leading to an unstable foundation for governance efforts. Critically, policy debates \u2026"}, {"title": "POSIX: A Prompt Sensitivity Index For Large Language Models", "link": "https://arxiv.org/pdf/2410.02185", "details": "A Chatterjee, HK Renduchintala, S Bhatia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are found to be surprisingly sensitive to minor variations in prompts, often generating significantly divergent outputs in response to minor variations in the prompts, such as spelling \u2026"}, {"title": "EMMA: Efficient Visual Alignment in Multi-Modal LLMs", "link": "https://arxiv.org/pdf/2410.02080%3F", "details": "S Ghazanfari, A Araujo, P Krishnamurthy, S Garg\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multi-modal Large Language Models (MLLMs) have recently exhibited impressive general-purpose capabilities by leveraging vision foundation models to encode the core concepts of images into representations. These are then combined with \u2026"}]
