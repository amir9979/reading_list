[{"title": "CPLLM: Clinical prediction with large language models", "link": "https://journals.plos.org/digitalhealth/article%3Fid%3D10.1371/journal.pdig.0000680", "details": "O Ben Shoham, N Rappoport - PLOS Digital Health, 2024", "abstract": "We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for predicting clinical disease and readmission. We utilized quantization and fine-tuned the LLM using \u2026"}, {"title": "On the Adversarial Robustness of Instruction-Tuned Large Language Models for Code", "link": "https://arxiv.org/pdf/2411.19508", "details": "MI Hossen, X Hei - arXiv preprint arXiv:2411.19508, 2024", "abstract": "The advent of instruction-tuned Large Language Models designed for coding tasks (Code LLMs) has transformed software engineering practices. However, their robustness against various input challenges remains a critical concern. This study \u2026"}, {"title": "PrivAgent: Agentic-based Red-teaming for LLM Privacy Leakage", "link": "https://arxiv.org/pdf/2412.05734", "details": "Y Nie, Z Wang, Y Yu, X Wu, X Zhao, W Guo, D Song - arXiv preprint arXiv:2412.05734, 2024", "abstract": "Recent studies have discovered that LLMs have serious privacy leakage concerns, where an LLM may be fooled into outputting private information under carefully crafted adversarial prompts. These risks include leaking system prompts, personally \u2026"}]
