[{"title": "MFC-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2406.11288", "details": "S Wang, H Lin, Z Luo, Z Ye, G Chen, J Ma - arXiv preprint arXiv:2406.11288, 2024", "abstract": "Large vision-language models (LVLMs) have significantly improved multimodal reasoning tasks, such as visual question answering and image captioning. These models embed multimodal facts within their parameters, rather than relying on \u2026"}, {"title": "PORT: Preference Optimization on Reasoning Traces", "link": "https://arxiv.org/pdf/2406.16061", "details": "S Lahlou, A Abubaker, H Hacid - arXiv preprint arXiv:2406.16061, 2024", "abstract": "Preference optimization methods have been successfully applied to improve not only the alignment of large language models (LLMs) with human values, but also specific natural language tasks such as summarization and stylistic continuations. This paper \u2026"}, {"title": "Show, Don't Tell: Aligning Language Models with Demonstrated Feedback", "link": "https://arxiv.org/pdf/2406.00888", "details": "O Shaikh, M Lam, J Hejna, Y Shao, M Bernstein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large \u2026"}, {"title": "ICLEval: Evaluating In-Context Learning Ability of Large Language Models", "link": "https://arxiv.org/pdf/2406.14955", "details": "W Chen, Y Lin, ZH Zhou, HY Huang, Y Jia, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) is a critical capability of Large Language Models (LLMs) as it empowers them to comprehend and reason across interconnected inputs. Evaluating the ICL ability of LLMs can enhance their utilization and deepen our \u2026"}, {"title": "Direct Alignment of Language Models via Quality-Aware Self-Refinement", "link": "https://arxiv.org/pdf/2405.21040", "details": "R Yu, Y Wang, X Jiao, Y Zhang, JT Kwok - arXiv preprint arXiv:2405.21040, 2024", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been commonly used to align the behaviors of Large Language Models (LLMs) with human preferences. Recently, a popular alternative is Direct Policy Optimization (DPO), which replaces \u2026"}, {"title": "ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models", "link": "https://arxiv.org/pdf/2406.13342", "details": "H Jo, H Lee, T Park - arXiv preprint arXiv:2406.13342, 2024", "abstract": "The recent advancements in large language models (LLMs) have brought significant progress in solving NLP tasks. Notably, in-context learning (ICL) is the key enabling mechanism for LLMs to understand specific tasks and grasping nuances. In this \u2026"}, {"title": "BERTs are Generative In-Context Learners", "link": "https://arxiv.org/pdf/2406.04823", "details": "D Samuel - arXiv preprint arXiv:2406.04823, 2024", "abstract": "This paper explores the in-context learning capabilities of masked language models, challenging the common view that this ability does not'emerge'in them. We present an embarrassingly simple inference technique that enables DeBERTa to operate as \u2026"}, {"title": "Generate Subgoal Images before Act: Unlocking the Chain-of-Thought Reasoning in Diffusion Model for Robot Manipulation with Multimodal Prompts", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ni_Generate_Subgoal_Images_before_Act_Unlocking_the_Chain-of-Thought_Reasoning_in_CVPR_2024_paper.pdf", "details": "F Ni, J Hao, S Wu, L Kou, J Liu, Y Zheng, B Wang\u2026 - Proceedings of the IEEE \u2026, 2024", "abstract": "Robotics agents often struggle to understand and follow the multi-modal prompts in complex manipulation scenes which are challenging to be sufficiently and accurately described by text alone. Moreover for long-horizon manipulation tasks the deviation \u2026"}, {"title": "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training", "link": "https://arxiv.org/pdf/2406.16554", "details": "T Zhu, X Qu, D Dong, J Ruan, J Tong, C He, Y Cheng - arXiv preprint arXiv \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, training MoE from scratch in a large-scale setting still suffers from data-hungry and instability problems. Motivated \u2026"}]
