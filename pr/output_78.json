'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\r\n\r\n---\r\n### \r\n\r\n### \r\n\r\n### [PDF] [Enhancing Chest X-Ray Representation Learning w'
[{"title": "Enhancing Chest X-Ray Representation Learning with Masked Siamese Networks and Electronic Health Records", "link": "https://openreview.net/pdf%3Fid%3DcpCMoTvHnt", "details": "S Shurrab, A Guerra-Manzanares, F Shamout - Medical Imaging with Deep Learning, 2024", "abstract": "Self-supervised learning methods for medical imaging primarily rely on image data for pretraining. While this approach delivers promising results, incorporating associated information from Electronic Health Records (EHR) can enhance the \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models", "link": "https://arxiv.org/pdf/2403.02436", "details": "X Lu, Y Zhao, B Qin - arXiv preprint arXiv:2403.02436, 2024", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Optimizing Pulmonary Embolism detection through diverse UNET architectural variations", "link": "http://137.117.138.59/bitstream/handle/123456789/5508/1571005180.pdf%3Fsequence%3D1%26isAllowed%3Dy", "details": "R Vadhera, M Sharma - International Journal of Computing and Digital Systems, 2024", "abstract": "A Pulmonary Embolism (PE) occurs when a blood clot, usually originating from the deep veins in the legs (deep vein thrombosis), travels to the lungs and becomes lodged in a blood vessel, obstructing blood flow. This can lead to serious \u2026"}, {"title": "Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning", "link": "https://arxiv.org/pdf/2402.11537", "details": "Y Zhao, L Du, X Ding, K Xiong, Z Sun, J Shi, T Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of \u2026"}, {"title": "Exploring the Adversarial Capabilities of Large Language Models", "link": "https://arxiv.org/pdf/2402.09132", "details": "L Struppek, MH Le, D Hintersdorf, K Kersting - arXiv preprint arXiv:2402.09132, 2024", "abstract": "The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation capabilities, offering great potential for both industry and research. While previous research delved into the \u2026"}, {"title": "ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling", "link": "https://arxiv.org/html/2402.13542v1", "details": "L Zhang, Y Yu, K Wang, C Zhang - arXiv preprint arXiv:2402.13542, 2024", "abstract": "Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge \u2026"}, {"title": "Robustness to Subpopulation Shift with Domain Label Noise via Regularized Annotation of Domains", "link": "https://arxiv.org/pdf/2402.11039", "details": "N Stromberg, R Ayyagari, M Welfert, S Koyejo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing methods for last layer retraining that aim to optimize worst-group accuracy (WGA) rely heavily on well-annotated groups in the training data. We show, both in theory and practice, that annotation-based data augmentations using either \u2026"}, {"title": "Data Engineering for Scaling Language Models to 128K Context", "link": "https://arxiv.org/pdf/2402.10171", "details": "Y Fu, R Panda, X Niu, X Yue, H Hajishirzi, Y Kim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular\\textit {the ability to utilize information at arbitrary input \u2026"}, {"title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains", "link": "https://arxiv.org/pdf/2402.10373.pdf%3Ftrk%3Dpublic_post_comment-text", "details": "Y Labrak, A Bazoge, E Morin, PA Gourraud, M Rouvier\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for \u2026"}, {"title": "Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods", "link": "https://arxiv.org/pdf/2403.00998", "details": "P Tsvilodub, H Wang, S Grosch, M Franke - arXiv preprint arXiv:2403.00998, 2024", "abstract": "This paper systematically compares different methods of deriving item-level predictions of language models for multiple-choice tasks. It compares scoring methods for answer options based on free generation of responses, various \u2026"}]
