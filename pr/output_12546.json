[{"title": "Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences", "link": "https://arxiv.org/pdf/2502.01126", "details": "V Shrivastava, A Kumar, P Liang - arXiv preprint arXiv:2502.01126, 2025", "abstract": "Language models (LMs) should provide reliable confidence estimates to help users detect mistakes in their outputs and defer to human experts when necessary. Asking a language model to assess its confidence (\" Score your confidence from 0-1.\") is a \u2026"}, {"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}, {"title": "DarkMind: Latent Chain-of-Thought Backdoor in Customized LLMs", "link": "https://arxiv.org/pdf/2501.18617", "details": "Z Guo, R Tourani - arXiv preprint arXiv:2501.18617, 2025", "abstract": "With the growing demand for personalized AI solutions, customized LLMs have become a preferred choice for businesses and individuals, driving the deployment of millions of AI agents across various platforms, eg, GPT Store hosts over 3 million \u2026"}, {"title": "Scaling Laws for Differentially Private Language Models", "link": "https://arxiv.org/pdf/2501.18914", "details": "R McKenna, Y Huang, A Sinha, B Balle, Z Charles\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Scaling laws have emerged as important components of large language model (LLM) training as they can predict performance gains through scale, and provide guidance on important hyper-parameter choices that would otherwise be expensive \u2026"}, {"title": "FIRE: Flexible Integration of Data Quality Ratings for Effective Pre-Training", "link": "https://arxiv.org/pdf/2502.00761", "details": "L Xu, X Zhang, F Duan, S Wang, J Wang, X Cai - arXiv preprint arXiv:2502.00761, 2025", "abstract": "Selecting high-quality data can significantly improve the pre-training efficiency of large language models (LLMs). Existing methods often rely on heuristic techniques and single quality signals, limiting their ability to comprehensively evaluate data \u2026"}, {"title": "Word-level Cross-lingual Structure in Large Language Models", "link": "https://aclanthology.org/2025.coling-main.138.pdf", "details": "Z Feng, H Cao, W Xu, T Zhao - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Abstract Large Language Models (LLMs) have demonstrated exceptional performance across a broad spectrum of cross-lingual Natural Language Processing (NLP) tasks. However, previous methods predominantly focus on leveraging parallel \u2026"}, {"title": "Knowing When to Stop: Dynamic Context Cutoff for Large Language Models", "link": "https://arxiv.org/pdf/2502.01025", "details": "R Xie, J Wang, P Rosu, C Deng, B Sun, Z Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) process entire input contexts indiscriminately, which is inefficient in cases where the information required to answer a query is localized within the context. We present dynamic context cutoff, a human-inspired method \u2026"}, {"title": "Calling a Spade a Heart: Gaslighting Multimodal Large Language Models via Negation", "link": "https://arxiv.org/pdf/2501.19017", "details": "B Zhu, Y Gui, J Chen, CW Ngo, EP Lim - arXiv preprint arXiv:2501.19017, 2025", "abstract": "Multimodal Large Language Models (MLLMs) have exhibited remarkable advancements in integrating different modalities, excelling in complex understanding and generation tasks. Despite their success, MLLMs remain vulnerable to \u2026"}, {"title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language Models", "link": "https://arxiv.org/pdf/2501.19392", "details": "A Shutova, V Malinovskii, V Egiazarian, D Kuznedelev\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Efficient real-world deployments of large language models (LLMs) rely on Key-Value (KV) caching for processing and generating long outputs, reducing the need for repetitive computation. For large contexts, Key-Value caches can take up tens of \u2026"}]
