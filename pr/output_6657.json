[{"title": "KiL 2024: 4th International Workshop on Knowledge-infused Learning (Towards Consistent, Reliable, Explainable, and Safe LLMs)", "link": "https://dl.acm.org/doi/abs/10.1145/3637528.3671495", "details": "M Gaur, E Tsamoura, E Raff, N Vedula\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "The Knowledge-infused Learning Workshop is a recurring event in ACM's KDD Conference that gathers the research community on knowledge graphs and knowledge-enabled learning, grounded neurosymbolic AI, explainable and safe AI \u2026"}, {"title": "InfoDisent: Explainability of Image Classification Models by Information Disentanglement", "link": "https://arxiv.org/pdf/2409.10329", "details": "\u0141 Struski, J Tabor - arXiv preprint arXiv:2409.10329, 2024", "abstract": "Understanding the decisions made by image classification networks is a critical area of research in deep learning. This task is traditionally divided into two distinct approaches: post-hoc methods and intrinsic methods. Post-hoc methods, such as \u2026"}, {"title": "Towards understanding evolution of science through language model series", "link": "https://arxiv.org/pdf/2409.09636", "details": "J Dong, Z Lyu, Q Ke - arXiv preprint arXiv:2409.09636, 2024", "abstract": "We introduce AnnualBERT, a series of language models designed specifically to capture the temporal evolution of scientific text. Deviating from the prevailing paradigms of subword tokenizations and\" one model to rule them all\", AnnualBERT \u2026"}]
