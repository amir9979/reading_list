[{"title": "Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?", "link": "https://arxiv.org/pdf/2410.13523", "details": "C Liu, Z Wan, H Wang, Y Chen, T Qaiser, C Jin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical Vision-Language Pre-training (MedVLP) has made significant progress in enabling zero-shot tasks for medical image understanding. However, training MedVLP models typically requires large-scale datasets with paired, high-quality \u2026"}, {"title": "FedSpaLLM: Federated Pruning of Large Language Models", "link": "https://arxiv.org/pdf/2410.14852", "details": "G Bai, Y Li, Z Li, L Zhao, K Kim - arXiv preprint arXiv:2410.14852, 2024", "abstract": "Large Language Models (LLMs) achieve state-of-the-art performance but are challenging to deploy due to their high computational and storage demands. Pruning can reduce model size, yet existing methods assume public access to calibration \u2026"}, {"title": "Multi-domain improves classification in out-of-distribution and data-limited scenarios for medical image analysis", "link": "https://www.nature.com/articles/s41598-024-73561-y", "details": "E Ozkan, X Boix - Scientific Reports, 2024", "abstract": "Current machine learning methods for medical image analysis primarily focus on developing models tailored for their specific tasks, utilizing data within their target domain. These specialized models tend to be data-hungry and often exhibit \u2026"}, {"title": "Cal-dpo: Calibrated direct preference optimization for language model alignment", "link": "https://openreview.net/pdf%3Fid%3D57OQXxbTbY", "details": "T Xiao, Y Yuan, H Zhu, M Li, VG Honavar - The Thirty-eighth Annual Conference on \u2026, 2024", "abstract": "We study the problem of aligning large language models (LLMs) with human preference data. Contrastive preference optimization has shown promising results in aligning LLMs with available preference data by optimizing the implicit reward \u2026"}, {"title": "Scaling Up Membership Inference: When and How Attacks Succeed on Large Language Models", "link": "https://arxiv.org/pdf/2411.00154", "details": "H Puerto, M Gubri, S Yun, SJ Oh - arXiv preprint arXiv:2411.00154, 2024", "abstract": "Membership inference attacks (MIA) attempt to verify the membership of a given data sample in the training set for a model. MIA has become relevant in recent years, following the rapid development of large language models (LLM). Many are \u2026"}, {"title": "NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples", "link": "https://arxiv.org/pdf/2410.14669%3F", "details": "B Li, Z Lin, W Peng, JD Nyandwi, D Jiang, Z Ma\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language models (VLMs) have made significant progress in recent visual- question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that \u2026"}, {"title": "Early Fusion Helps Vision Language Action Models Generalize Better", "link": "https://openreview.net/pdf%3Fid%3D8VhxFsvZVD", "details": "H Huang, F Liu, L Fu, T Wu, M Mukadam, J Malik\u2026 - Workshop on Language and \u2026", "abstract": "Recent advances in Vision-Language-Action (VLA) models can enable robots to perform a wide range of tasks based on language or goal-based instructions. These VLA models typically encode text and images into disjoint tokens, generating actions \u2026"}, {"title": "Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench", "link": "https://arxiv.org/pdf/2410.22108", "details": "Z Liu, G Dou, M Jia, Z Tan, Q Zeng, Y Yuan, M Jiang - arXiv preprint arXiv:2410.22108, 2024", "abstract": "Generative models such as Large Language Models (LLM) and Multimodal Large Language models (MLLMs) trained on massive web corpora can memorize and disclose individuals' confidential and private data, raising legal and ethical concerns \u2026"}, {"title": "Skills-in-Context: Unlocking Compositionality in Large Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.812.pdf", "details": "J Chen, X Pan, D Yu, K Song, X Wang, D Yu, J Chen - Findings of the Association for \u2026, 2024", "abstract": "We investigate how to elicit compositional generalization capabilities in large language models (LLMs). Compositional generalization empowers LLMs to solve complex problems by combining foundational skills, a critical reasoning ability akin to \u2026"}]
