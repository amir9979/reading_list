[{"title": "Multifaceted Natural Language Processing Task\u2013Based Evaluation of Bidirectional Encoder Representations From Transformers Models for Bilingual (Korean and \u2026", "link": "https://medinform.jmir.org/2024/1/e52897/", "details": "K Kim, S Park, J Min, S Park, JY Kim, J Eun, K Jung\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background: The bidirectional encoder representations from transformers (BERT) model has attracted considerable attention in clinical applications, such as patient classification and disease prediction. However, current studies have typically \u2026"}, {"title": "Natural Language Inference Improves Compositionality in Vision-Language Models", "link": "https://arxiv.org/pdf/2410.22315%3F", "details": "P Cascante-Bonilla, Y Hou, YT Cao, H Daum\u00e9 III\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Compositional reasoning in Vision-Language Models (VLMs) remains challenging as these models often struggle to relate objects, attributes, and spatial relationships. Recent methods aim to address these limitations by relying on the semantics of the \u2026"}, {"title": "Retrieval In Decoder benefits generative models for explainable complex question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007573", "details": "J Feng, Q Wang, H Qiu, L Liu - Neural Networks, 2024", "abstract": "Abstract Large-scale Language Models (LLMs) utilizing the Chain-of-Thought prompting demonstrate exceptional performance in a variety of tasks. However, the persistence of factual hallucinations remains a significant challenge in practical \u2026"}, {"title": "Language-based reasoning graph neural network for commonsense question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007408", "details": "M Yang, Y Wang, Y Gu - Neural Networks, 2024", "abstract": "Abstract Language model (LM) has played an increasingly important role in the common-sense understanding and reasoning in the CSQA task (Common Sense Question Answering). However, due to the amount of model parameters, increasing \u2026"}, {"title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "link": "https://arxiv.org/pdf/2410.11829%3F", "details": "Y Cao, Y Liu, Z Chen, G Shi, W Wang, D Zhao, T Lu - arXiv preprint arXiv:2410.11829, 2024", "abstract": "Despite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating \u2026"}, {"title": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models", "link": "https://arxiv.org/pdf/2410.06741", "details": "Z Gong, H Yu, C Liao, B Liu, C Chen, J Li - arXiv preprint arXiv:2410.06741, 2024", "abstract": "Multi-task learning (MTL) benefits the fine-tuning of large language models (LLMs) by providing a single model with improved performance and generalization ability across tasks, presenting a resource-efficient alternative to developing separate \u2026"}, {"title": "VERIFAI: TOWARDS AN OPEN-SOURCE SCIENTIFIC GENERATIVE QUESTION-ANSWERING SYSTEM WITH REFERENCED AND VERIFIABLE ANSWERS", "link": "https://amslaurea.unibo.it/32899/1/Thesis.pdf", "details": "L Cassano, P Torroni, DN Milosevic", "abstract": "This research investigates the effectiveness of transformer-based models in mitigating hallucinations within the biomedical domain, a crucial area in natural language processing (NLP). Hallucinations occur when language models generate \u2026"}, {"title": "Controlled Low-Rank Adaptation with Subspace Regularization for Continued Training on Large Language Models", "link": "https://arxiv.org/pdf/2410.16801", "details": "Y Lu, B Qian, C Yuan, H Jiang, X Wang - arXiv preprint arXiv:2410.16801, 2024", "abstract": "Large language models (LLMs) exhibit remarkable capabilities in natural language processing but face catastrophic forgetting when learning new tasks, where adaptation to a new domain leads to a substantial decline in performance on \u2026"}, {"title": "On the Regularization of Learnable Embeddings for Time Series Processing", "link": "https://arxiv.org/pdf/2410.14630", "details": "L Butera, G De Felice, A Cini, C Alippi - arXiv preprint arXiv:2410.14630, 2024", "abstract": "In processing multiple time series, accounting for the individual features of each sequence can be challenging. To address this, modern deep learning methods for time series analysis combine a shared (global) model with local layers, specific to \u2026"}]
