[{"title": "Scalable Multi-Domain Adaptation of Language Models using Modular Experts", "link": "https://arxiv.org/pdf/2410.10181", "details": "P Schafhalter, S Liao, Y Zhou, CK Yeh, A Kandoor\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Domain-specific adaptation is critical to maximizing the performance of pre-trained language models (PLMs) on one or multiple targeted tasks, especially under resource-constrained use cases, such as edge devices. However, existing methods \u2026"}, {"title": "MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models", "link": "https://arxiv.org/pdf/2410.09733", "details": "H Hua, Y Tang, Z Zeng, L Cao, Z Yang, H He, C Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video \u2026"}, {"title": "Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models", "link": "https://arxiv.org/pdf/2410.09047", "details": "Q Liu, C Shang, L Liu, N Pappas, J Ma, NA John\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be degraded by the integration of the vision module compared to its LLM backbone. We investigate this phenomenon, dubbed as''safety alignment degradation''in this paper \u2026"}, {"title": "ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context Information in Multi-Turn Multimodal Medical Dialogue", "link": "https://arxiv.org/pdf/2409.17610", "details": "Z Li, C Zou, S Ma, Z Yang, C Du, Y Tang, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rocketing prosperity of large language models (LLMs) in recent years has boosted the prevalence of vision-language models (VLMs) in the medical sector. In our online medical consultation scenario, a doctor responds to the texts and images \u2026"}, {"title": "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "link": "https://arxiv.org/pdf/2410.05077", "details": "FM Molfese, S Conia, R Orlando, R Navigli - arXiv preprint arXiv:2410.05077, 2024", "abstract": "Current Large Language Models (LLMs) have shown strong reasoning capabilities in commonsense question answering benchmarks, but the process underlying their success remains largely opaque. As a consequence, recent approaches have \u2026"}, {"title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ", "link": "https://arxiv.org/pdf/2410.09613", "details": "A Poulis, E Tsalapati, M Koubarakis - arXiv preprint arXiv:2410.09613, 2024", "abstract": "Recent advancements in transformer-based language models have sparked research into their logical reasoning capabilities. Most of the benchmarks used to evaluate these models are simple: generated from short (fragments of) first-order \u2026"}, {"title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models", "link": "https://arxiv.org/pdf/2410.06154", "details": "MJ Mirza, M Zhao, Z Mao, S Doveh, W Lin, P Gavrikov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task \u2026"}, {"title": "From One to Zero: RAG-IM Adapts Language Models for Interpretable Zero-Shot Predictions on Clinical Tabular Data", "link": "https://openreview.net/pdf%3Fid%3DBnKvIn8JKl", "details": "S Mahbub, C Ellington, S Alinejad, K Wen, Y Luo\u2026 - NeurIPS 2024 Third Table \u2026", "abstract": "Clinical machine learning models, often learned from tabular data, must adapt to new settings such as different hospitals, clinicians, or patient populations. These differing environments present related but subtly distinct tasks, where diseases and medical \u2026"}, {"title": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models", "link": "https://arxiv.org/pdf/2410.06741", "details": "Z Gong, H Yu, C Liao, B Liu, C Chen, J Li - arXiv preprint arXiv:2410.06741, 2024", "abstract": "Multi-task learning (MTL) benefits the fine-tuning of large language models (LLMs) by providing a single model with improved performance and generalization ability across tasks, presenting a resource-efficient alternative to developing separate \u2026"}]
