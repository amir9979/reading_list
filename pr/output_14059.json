[{"title": "Reasoning-Grounded Natural Language Explanations for Language Models", "link": "https://arxiv.org/pdf/2503.11248", "details": "V Cahlik, R Alves, P Kordik - arXiv preprint arXiv:2503.11248, 2025", "abstract": "We propose a large language model explainability technique for obtaining faithful natural language explanations by grounding the explanations in a reasoning process. When converted to a sequence of tokens, the outputs of the reasoning \u2026"}, {"title": "Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection", "link": "https://arxiv.org/pdf/2503.11794", "details": "B Li, F Wang, W Zhou, N Xu, B Zhou, S Zhang, H Poon\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-Language Models (VLMs) leverage aligned visual encoders to transform images into visual tokens, allowing them to be processed similarly to text by the backbone large language model (LLM). This unified input paradigm enables VLMs to \u2026"}, {"title": "Improving Foundation Models on Electronic Health Records", "link": "https://udspace.udel.edu/bitstreams/581bb829-eb4b-43de-b937-9e9637390a98/download", "details": "R Poulain - 2025", "abstract": "Recent advances in foundation models have opened up new possibilities for healthcare applications, particularly by utilizing transformer-based models to take advantage of the longitudinal nature of both natural language and electronic health \u2026"}, {"title": "Process-based self-rewarding language models", "link": "https://arxiv.org/pdf/2503.03746", "details": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' \u2026"}, {"title": "Language Models Predict Empathy Gaps Between Social In-groups and Out-groups", "link": "https://arxiv.org/pdf/2503.01030", "details": "Y Hou, H Daum\u00e9 III, R Rudinger - arXiv preprint arXiv:2503.01030, 2025", "abstract": "Studies of human psychology have demonstrated that people are more motivated to extend empathy to in-group members than out-group members (Cikara et al., 2011). In this study, we investigate how this aspect of intergroup relations in humans is \u2026"}, {"title": "Capturing Nuanced Preferences: Preference-Aligned Distillation for Small Language Models", "link": "https://arxiv.org/pdf/2502.14272", "details": "Y Gu, J Li, S Huang, X Zou, Z Li, X Hu - arXiv preprint arXiv:2502.14272, 2025", "abstract": "Aligning small language models (SLMs) with human values typically involves distilling preference knowledge from large language models (LLMs). However, existing distillation methods model preference knowledge in teacher LLMs by \u2026"}, {"title": "Llm post-training: A deep dive into reasoning large language models", "link": "https://arxiv.org/pdf/2502.21321", "details": "K Kumar, T Ashraf, O Thawakar, RM Anwer\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have transformed the natural language processing landscape and brought to life diverse applications. Pretraining on vast web-scale data has laid the foundation for these models, yet the research community is now \u2026"}, {"title": "Enhancing zero-shot learning in medical imaging: integrating clip with advanced techniques for improved chest x-ray analysis", "link": "https://arxiv.org/pdf/2503.13134", "details": "P Bhardwaj, S Bhat, A Maier - arXiv preprint arXiv:2503.13134, 2025", "abstract": "Due to the large volume of medical imaging data, advanced AI methodologies are needed to assist radiologists in diagnosing thoracic diseases from chest X-rays (CXRs). Existing deep learning models often require large, labeled datasets, which \u2026"}]
