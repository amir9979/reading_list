[{"title": "Focus Directions Make Your Language Models Pay More Attention to Relevant Contexts", "link": "https://arxiv.org/pdf/2503.23306", "details": "Y Zhu, R Li, D Wang, D Haehn, X Liang - arXiv preprint arXiv:2503.23306, 2025", "abstract": "Long-context large language models (LLMs) are prone to be distracted by irrelevant contexts. The reason for distraction remains poorly understood. In this paper, we first identify the contextual heads, a special group of attention heads that control the \u2026"}, {"title": "Style over Substance: Distilled Language Models Reason Via Stylistic Replication", "link": "https://arxiv.org/pdf/2504.01738", "details": "P Lippmann, J Yang - arXiv preprint arXiv:2504.01738, 2025", "abstract": "Specialized reasoning language models (RLMs) have demonstrated that scaling test- time computation through detailed reasoning traces significantly enhances performance. Although these traces effectively facilitate knowledge distillation into \u2026"}, {"title": "Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image Analysis", "link": "https://arxiv.org/pdf/2503.20047%3F", "details": "Y Xin, GC Ates, K Gong, W Shao - arXiv preprint arXiv:2503.20047, 2025", "abstract": "Vision-language models (VLMs) have shown promise in 2D medical image analysis, but extending them to 3D remains challenging due to the high computational demands of volumetric data and the difficulty of aligning 3D spatial features with \u2026"}, {"title": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection", "link": "https://arxiv.org/pdf/2504.00695", "details": "X Zhu, Z Gu, S Zheng, T Wang, T Li, H Feng, Y Xiao - arXiv preprint arXiv:2504.00695, 2025", "abstract": "Pre-training large language models (LLMs) necessitates enormous diverse textual corpora, making effective data selection a key challenge for balancing computational resources and model performance. Current methodologies primarily emphasize data \u2026"}, {"title": "FLUE: Streamlined Uncertainty Estimation for Large Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/33840/35995", "details": "S Gao, T Gong, Z Lin, R Xu, H Zhou, J Li - Proceedings of the AAAI Conference on \u2026, 2025", "abstract": "Uncertainty estimation is essential for practical applications such as decision- making, risk assessment, and human-AI collaboration. However, Uncertainty estimation in open-ended question-answering (QA) tasks presents unique \u2026"}, {"title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking", "link": "https://arxiv.org/pdf/2503.19855%3F", "details": "X Tian, S Zhao, H Wang, S Chen, Y Ji, Y Peng, H Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite \u2026"}, {"title": "A Survey on Mixture of Experts in Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10937907/", "details": "W Cai, J Jiang, F Wang, J Tang, S Kim, J Huang - IEEE Transactions on Knowledge \u2026, 2025", "abstract": "Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size \u2026"}, {"title": "Large language models are human-like annotators", "link": "https://link.springer.com/chapter/10.1007/978-3-031-88720-8_45", "details": "M Marreddy, SR Oota, M Gupta - European Conference on Information Retrieval, 2025", "abstract": "Large Language Models Are Human-Like Annotators | SpringerLink Skip to main content Advertisement Springer Nature Link Account Menu Find a journal Publish with us Track your research Search Cart 1.Home 2.Advances in Information \u2026"}, {"title": "CSPO: chain-structured prompt optimisation for large language models", "link": "https://www.inderscienceonline.com/doi/abs/10.1504/IJAHUC.2025.145202", "details": "J Wang, S Lin, X Xue, S Chen, Z Tang - International Journal of Ad Hoc and \u2026, 2025", "abstract": "Large language models (LLMs) show promise in improving content distribution in mobile communication networks, but their performance heavily depends on input prompts. Manually crafting effective prompts for complex tasks is time-consuming \u2026"}]
