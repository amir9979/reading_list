'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Occupation Recognition and Exploitation in Rheumatolog'
[{"title": "Comparing natural language processing representations of coded disease sequences for prediction in electronic health records", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae091/7667337", "details": "T Beaney, S Jha, A Alaa, A Smith, J Clarke\u2026 - Journal of the American \u2026, 2024", "abstract": "Objective Natural language processing (NLP) algorithms are increasingly being applied to obtain unsupervised representations of electronic health record (EHR) data, but their comparative performance at predicting clinical endpoints remains \u2026"}, {"title": "Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge", "link": "https://arxiv.org/pdf/2405.05253", "details": "C Koutcheme, N Dainese, S Sarsa, A Hellas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have shown great potential for the automatic generation of feedback in a wide range of computing contexts. However, concerns have been voiced around the privacy and ethical implications of sending student \u2026"}, {"title": "Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation", "link": "https://arxiv.org/pdf/2404.17489", "details": "W Cui, R Hosseinzadeh, J Ma, T Wu, Y Sui, K Golestan - arXiv preprint arXiv \u2026, 2024", "abstract": "Contrastive learning is a model pre-training technique by first creating similar views of the original data, and then encouraging the data and its corresponding views to be close in the embedding space. Contrastive learning has witnessed success in image \u2026"}, {"title": "BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models", "link": "https://arxiv.org/pdf/2405.04756", "details": "CF Luo, A Ghawanmeh, X Zhu, FK Khattak - arXiv preprint arXiv:2405.04756, 2024", "abstract": "Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly. The language model can also \u2026"}, {"title": "XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples", "link": "https://arxiv.org/pdf/2405.05116", "details": "P Lin, AFT Martins, H Sch\u00fctze - arXiv preprint arXiv:2405.05116, 2024", "abstract": "Recent studies have shown that leveraging off-the-shelf or fine-tuned retrievers, capable of retrieving high-quality in-context examples, significantly improves in- context learning of English. However, adapting these methods to other languages \u2026"}]
