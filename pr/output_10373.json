[{"title": "How Private are Language Models in Abstractive Summarization?", "link": "https://arxiv.org/pdf/2412.12040", "details": "A Hughes, N Aletras, N Ma - arXiv preprint arXiv:2412.12040, 2024", "abstract": "Language models (LMs) have shown outstanding performance in text summarization including sensitive domains such as medicine and law. In these settings, it is important that personally identifying information (PII) included in the source \u2026"}, {"title": "Minimax Regret Estimation for Generalizing Heterogeneous Treatment Effects with Multisite Data", "link": "https://arxiv.org/pdf/2412.11136", "details": "Y Zhang, M Huang, K Imai - arXiv preprint arXiv:2412.11136, 2024", "abstract": "To test scientific theories and develop individualized treatment rules, researchers often wish to learn heterogeneous treatment effects that can be consistently found across diverse populations and contexts. We consider the problem of generalizing \u2026"}, {"title": "Political-LLM: Large Language Models in Political Science", "link": "https://arxiv.org/pdf/2412.06864", "details": "L Li, J Li, C Chen, F Gui, H Yang, C Yu, Z Wang, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, large language models (LLMs) have been widely adopted in political science tasks such as election prediction, sentiment analysis, policy impact assessment, and misinformation detection. Meanwhile, the need to systematically \u2026"}, {"title": "A few-shot learning method based on knowledge graph in large language models", "link": "https://link.springer.com/article/10.1007/s41060-024-00699-3", "details": "FL Wang, D Shi, J Aguilar, X Cui - International Journal of Data Science and Analytics, 2024", "abstract": "The emergence of large language models has significantly transformed natural language processing and text generation. Fine-tuning these models for specific domains enables them to generate answers tailored to the unique requirements of \u2026"}, {"title": "Partial Identifiability in Inverse Reinforcement Learning For Agents With Non-Exponential Discounting", "link": "https://arxiv.org/pdf/2412.11155", "details": "J Skalse, A Abate - arXiv preprint arXiv:2412.11155, 2024", "abstract": "The aim of inverse reinforcement learning (IRL) is to infer an agent's preferences from observing their behaviour. Usually, preferences are modelled as a reward function, $ R $, and behaviour is modelled as a policy, $\\pi $. One of the central \u2026"}, {"title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2411.14432", "details": "Y Dong, Z Liu, HL Sun, J Yang, W Hu, Y Rao, Z Liu - arXiv preprint arXiv:2411.14432, 2024", "abstract": "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high \u2026"}, {"title": "The Superalignment of Superhuman Intelligence with Large Language Models", "link": "https://arxiv.org/pdf/2412.11145", "details": "M Huang, Y Wang, S Cui, P Ke, J Tang - arXiv preprint arXiv:2412.11145, 2024", "abstract": "We have witnessed superhuman intelligence thanks to the fast development of large language models and multimodal language models. As the application of such superhuman models becomes more and more common, a critical question rises \u2026"}, {"title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation", "link": "https://arxiv.org/pdf/2412.11919", "details": "X Li, J Jin, Y Zhou, Y Wu, Z Li, Q Ye, Z Dou - arXiv preprint arXiv:2412.11919, 2024", "abstract": "Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations. Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several \u2026"}, {"title": "Minerva LLMs: The First Family of Large Language Models Trained from Scratch on Italian Data", "link": "https://clic2024.ilc.cnr.it/wp-content/uploads/2024/12/76_main_long.pdf", "details": "R Orlando, L Moroni, PLH Cabot, E Barba, S Conia\u2026 - Proceedings of the Tenth \u2026, 2024", "abstract": "The growing interest in Large Language Models (LLMs) has accelerated research efforts to adapt these models for various languages. Despite this, pretraining LLMs from scratch for non-English languages remains underexplored. This is the case for \u2026"}]
