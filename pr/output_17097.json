[{"title": "Generating Diverse Training Samples for Relation Extraction with Large Language Models", "link": "https://arxiv.org/pdf/2505.23108", "details": "Z Li, H Dai, P Li - arXiv preprint arXiv:2505.23108, 2025", "abstract": "Using Large Language Models (LLMs) to generate training data can potentially be a preferable way to improve zero or few-shot NLP tasks. However, many problems remain to be investigated for this direction. For the task of Relation Extraction (RE) \u2026", "entry_id": "http://arxiv.org/abs/2505.23108v1", "updated": "2025-05-29 05:21:54", "published": "2025-05-29 05:21:54", "authors": "Zexuan Li;Hongliang Dai;Piji Li", "summary": "Using Large Language Models (LLMs) to generate training data can potentially\nbe a preferable way to improve zero or few-shot NLP tasks. However, many\nproblems remain to be investigated for this direction. For the task of Relation\nExtraction (RE), we find that samples generated by directly prompting LLMs may\neasily have high structural similarities with each other. They tend to use a\nlimited variety of phrasing while expressing the relation between a pair of\nentities. Therefore, in this paper, we study how to effectively improve the\ndiversity of the training samples generated with LLMs for RE, while also\nmaintaining their correctness. We first try to make the LLMs produce dissimilar\nsamples by directly giving instructions in In-Context Learning (ICL) prompts.\nThen, we propose an approach to fine-tune LLMs for diversity training sample\ngeneration through Direct Preference Optimization (DPO). Our experiments on\ncommonly used RE datasets show that both attempts can improve the quality of\nthe generated training data. We also find that comparing with directly\nperforming RE with an LLM, training a non-LLM RE model with its generated\nsamples may lead to better performance.", "comment": "ACL2025 Main", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.23108v1;http://arxiv.org/pdf/2505.23108v1", "pdf_url": "http://arxiv.org/pdf/2505.23108v1"}, {"title": "Talent or Luck? Evaluating Attribution Bias in Large Language Models", "link": "https://arxiv.org/pdf/2505.22910", "details": "C Raj, M Banerjee, A Caliskan, A Anastasopoulos\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "When a student fails an exam, do we tend to blame their effort or the test's difficulty? Attribution, defined as how reasons are assigned to event outcomes, shapes perceptions, reinforces stereotypes, and influences decisions. Attribution Theory in \u2026", "entry_id": "http://arxiv.org/abs/2505.22910v1", "updated": "2025-05-28 22:18:46", "published": "2025-05-28 22:18:46", "authors": "Chahat Raj;Mahika Banerjee;Aylin Caliskan;Antonios Anastasopoulos;Ziwei Zhu", "summary": "When a student fails an exam, do we tend to blame their effort or the test's\ndifficulty? Attribution, defined as how reasons are assigned to event outcomes,\nshapes perceptions, reinforces stereotypes, and influences decisions.\nAttribution Theory in social psychology explains how humans assign\nresponsibility for events using implicit cognition, attributing causes to\ninternal (e.g., effort, ability) or external (e.g., task difficulty, luck)\nfactors. LLMs' attribution of event outcomes based on demographics carries\nimportant fairness implications. Most works exploring social biases in LLMs\nfocus on surface-level associations or isolated stereotypes. This work proposes\na cognitively grounded bias evaluation framework to identify how models'\nreasoning disparities channelize biases toward demographic groups.", "comment": "18 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.22910v1;http://arxiv.org/pdf/2505.22910v1", "pdf_url": "http://arxiv.org/pdf/2505.22910v1"}, {"title": "Comparative performance of large language models in structuring head CT radiology reports: multi-institutional validation study in Japan", "link": "https://link.springer.com/article/10.1007/s11604-025-01799-1", "details": "H Takita, SL Walston, Y Mitsuyama, K Watanabe\u2026 - Japanese Journal of \u2026, 2025", "abstract": "Purpose To compare the diagnostic performance of three proprietary large language models (LLMs)\u2014Claude, GPT, and Gemini\u2014in structuring free-text Japanese radiology reports for intracranial hemorrhage and skull fractures, and to assess the \u2026"}]
