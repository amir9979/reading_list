'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [CATS: Contextually-Aware Thresholding for Sparsity in '
[{"title": "Model Reconstruction Using Counterfactual Explanations: Mitigating the Decision Boundary Shift", "link": "https://arxiv.org/pdf/2405.05369", "details": "P Dissanayake, S Dutta - arXiv preprint arXiv:2405.05369, 2024", "abstract": "Counterfactual explanations find ways of achieving a favorable model outcome with minimum input perturbation. However, counterfactual explanations can also be exploited to steal the model by strategically training a surrogate model to give similar \u2026"}, {"title": "Calibrating Language Models With Adaptive Temperature Scaling", "link": "https://openreview.net/pdf%3Fid%3DBgfGqNpoMi", "details": "J Xie, AS Chen, Y Lee, E Mitchell, C Finn - ICLR 2024 Workshop on Secure and Trustworthy \u2026", "abstract": "The effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration\u2014how well their confidence scores reflect the probability of their outputs being correct. While \u2026"}, {"title": "Phi-3 technical report: A highly capable language model locally on your phone", "link": "https://arxiv.org/pdf/2404.14219%3Ftrk%3Dpublic_post_comment-text", "details": "M Abdin, SA Jacobs, AA Awan, J Aneja, A Awadallah\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT \u2026"}, {"title": "Foundational challenges in assuring alignment and safety of large language models", "link": "https://arxiv.org/pdf/2404.09932", "details": "U Anwar, A Saparov, J Rando, D Paleka, M Turpin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment \u2026"}, {"title": "Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity from Large Language Models", "link": "https://arxiv.org/pdf/2404.09170", "details": "X Chen, S Zhou, K Liang, X Liu - arXiv preprint arXiv:2404.09170, 2024", "abstract": "Chain of thought finetuning aims to endow small student models with reasoning capacity to improve their performance towards a specific task by allowing them to imitate the reasoning procedure of large language models (LLMs) beyond simply \u2026"}, {"title": "On Prompt-Driven Safeguarding for Large Language Models", "link": "https://openreview.net/pdf%3Fid%3DlFwf7bnpUs", "details": "C Zheng, F Yin, H Zhou, F Meng, J Zhou, KW Chang\u2026 - ICLR 2024 Workshop on Secure \u2026", "abstract": "Prepending model inputs with safety prompts is a common practice for safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not been revealed \u2026"}, {"title": "Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding", "link": "https://arxiv.org/pdf/2405.02750", "details": "Z Zhao, E Monti, J Lehmann, H Assem - arXiv preprint arXiv:2405.02750, 2024", "abstract": "Large language models (LLMs) tend to inadequately integrate input context during text generation, relying excessively on encoded prior knowledge in model parameters, potentially resulting in generated text with factual inconsistencies or \u2026"}]
