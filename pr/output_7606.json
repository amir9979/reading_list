[{"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "link": "https://arxiv.org/pdf/2410.05639", "details": "R Zhao, ZL Thai, Y Zhang, S Hu, Y Ba, J Zhou, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the \u2026"}, {"title": "FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs", "link": "https://arxiv.org/pdf/2409.13612", "details": "B Yan, Z Zhang, L Jing, E Hossain, X Du - arXiv preprint arXiv:2409.13612, 2024", "abstract": "The rapid development of Large Vision-Language Models (LVLMs) often comes with widespread hallucination issues, making cost-effective and comprehensive assessments increasingly vital. Current approaches mainly rely on costly annotations \u2026"}, {"title": "Do Vision-Language Models Really Understand Visual Language?", "link": "https://arxiv.org/pdf/2410.00193", "details": "B Giledereli, Y Hou, Y Tu, M Sachan - arXiv preprint arXiv:2410.00193, 2024", "abstract": "Visual language is a system of communication that conveys information through symbols, shapes, and spatial arrangements. Diagrams are a typical example of a visual language depicting complex concepts and their relationships in the form of an \u2026"}, {"title": "CAST: Cross-modal Alignment Similarity Test for Vision Language Models", "link": "https://arxiv.org/pdf/2409.11007", "details": "G Dagan, O Loginova, A Batra - arXiv preprint arXiv:2409.11007, 2024", "abstract": "Vision Language Models (VLMs) are typically evaluated with Visual Question Answering (VQA) tasks which assess a model's understanding of scenes. Good VQA performance is taken as evidence that the model will perform well on a broader \u2026"}, {"title": "Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models", "link": "https://arxiv.org/pdf/2409.12512", "details": "J Rao, X Liu, Z Lin, L Ding, J Li, D Tao - arXiv preprint arXiv:2409.12512, 2024", "abstract": "Knowledge distillation (KD) is a technique that compresses large teacher models by training smaller student models to mimic them. The success of KD in auto-regressive language models mainly relies on Reverse KL for mode-seeking and student \u2026"}, {"title": "Neural-Symbolic Collaborative Distillation: Advancing Small Language Models for Complex Reasoning Tasks", "link": "https://arxiv.org/pdf/2409.13203", "details": "H Liao, S He, Y Xu, Y Zhang, K Liu, J Zhao - arXiv preprint arXiv:2409.13203, 2024", "abstract": "In this paper, we propose $\\textbf {Ne} $ ural-$\\textbf {Sy} $ mbolic $\\textbf {C} $ ollaborative $\\textbf {D} $ istillation ($\\textbf {NesyCD} $), a novel knowledge distillation method for learning the complex reasoning abilities of Large Language \u2026"}, {"title": "Training language models to self-correct via reinforcement learning", "link": "https://arxiv.org/pdf/2409.12917", "details": "A Kumar, V Zhuang, R Agarwal, Y Su, JD Co-Reyes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a \u2026"}, {"title": "Zero-Shot Multi-Hop Question Answering via Monte-Carlo Tree Search with Large Language Models", "link": "https://arxiv.org/pdf/2409.19382", "details": "S Lee, J Shin, Y Ahn, S Seo, O Kwon, KE Kim - arXiv preprint arXiv:2409.19382, 2024", "abstract": "Recent advances in large language models (LLMs) have significantly impacted the domain of multi-hop question answering (MHQA), where systems are required to aggregate information and infer answers from disparate pieces of text. However, the \u2026"}, {"title": "Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering", "link": "https://arxiv.org/pdf/2410.04752", "details": "Z Wang, L Xia, W Wang, X Du - arXiv preprint arXiv:2410.04752, 2024", "abstract": "As an essential task in information extraction (IE), Event-Event Causal Relation Extraction (ECRE) aims to identify and classify the causal relationships between event mentions in natural language texts. However, existing research on ECRE has \u2026"}]
