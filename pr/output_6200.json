[{"title": "Data-Centric Explainable Debiasing for Improving Fairness in Pre-trained Language Models", "link": "https://aclanthology.org/2024.findings-acl.226.pdf", "details": "Y Li, M Du, R Song, X Wang, Y Wang - Findings of the Association for Computational \u2026, 2024", "abstract": "Human-like social bias of pre-trained language models (PLMs) on downstream tasks have attracted increasing attention. The potential flaws in the training data are the main factor that causes unfairness in PLMs. Existing data-centric debiasing \u2026"}, {"title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk of Language Models", "link": "https://arxiv.org/pdf/2408.08926", "details": "AK Zhang, N Perry, R Dulepet, E Jones, JW Lin, J Ji\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have the potential to cause real- world impact. Policymakers, model providers, and other researchers in the AI and \u2026"}, {"title": "TabSAL: Synthesizing tabular data with small agent assisted language models", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124010724", "details": "J Li, R Qian, Y Tan, Z Li, L Chen, S Liu, J Wu, H Chai - Knowledge-Based Systems, 2024", "abstract": "Tabular data are widely used in machine-learning tasks because of their prevalence in various fields; however, the potential risks of data breaches in tabular data and privacy protection regulations render such data almost unavailable. Tabular data \u2026"}, {"title": "Effective prompt extraction from language models", "link": "https://openreview.net/pdf%3Fid%3D0o95CVdNuz", "details": "Y Zhang, N Carlini, D Ippolito - First Conference on Language Modeling, 2024", "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden \u2026"}, {"title": "Symbolic Working Memory Enhances Language Models for Complex Rule Application", "link": "https://arxiv.org/pdf/2408.13654", "details": "S Wang, Z Wei, Y Choi, X Ren - arXiv preprint arXiv:2408.13654, 2024", "abstract": "Large Language Models (LLMs) have shown remarkable reasoning performance but struggle with multi-step deductive reasoning involving a series of rule application steps, especially when rules are presented non-sequentially. Our preliminary \u2026"}, {"title": "FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt Optimization Across Tokenizers", "link": "https://arxiv.org/pdf/2408.04816", "details": "JN Williams, JZ Kolter - arXiv preprint arXiv:2408.04816, 2024", "abstract": "The widespread use of large language models has resulted in a multitude of tokenizers and embedding spaces, making knowledge transfer in prompt discovery tasks difficult. In this work, we propose FUSE (Flexible Unification of Semantic \u2026"}, {"title": "Unleash the power of pre-trained language models for irregularly sampled time series", "link": "https://arxiv.org/pdf/2408.08328", "details": "W Zhang, C Yin, H Liu, H Xiong - arXiv preprint arXiv:2408.08328, 2024", "abstract": "Pre-trained Language Models (PLMs), such as ChatGPT, have significantly advanced the field of natural language processing. This progress has inspired a series of innovative studies that explore the adaptation of PLMs to time series \u2026"}, {"title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models", "link": "https://arxiv.org/pdf/2408.15313", "details": "W Zhang, PHS Torr, M Elhoseiny, A Bibi - arXiv preprint arXiv:2408.15313, 2024", "abstract": "Fine-tuning large language models (LLMs) on human preferences, typically through reinforcement learning from human feedback (RLHF), has proven successful in enhancing their capabilities. However, ensuring the safety of LLMs during the fine \u2026"}, {"title": "Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language Models", "link": "https://arxiv.org/pdf/2408.14853", "details": "Y Du, Z Li, P Cheng, X Wan, A Gao - arXiv preprint arXiv:2408.14853, 2024", "abstract": "Large Language Models (LLMs) have become a focal point in the rapidly evolving field of artificial intelligence. However, a critical concern is the presence of toxic content within the pre-training corpus of these models, which can lead to the \u2026"}]
