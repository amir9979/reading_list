[{"title": "A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models", "link": "https://arxiv.org/pdf/2502.13942", "details": "H Huang, S Yuan, Y Hao, C Wen, Y Fang - arXiv preprint arXiv:2502.13942, 2025", "abstract": "A large-scale vision and language model that has been pretrained on massive data encodes visual and linguistic prior, which makes it easier to generate images and language that are more natural and realistic. Despite this, there is still a significant \u2026"}, {"title": "Layer by Layer: Uncovering Hidden Representations in Language Models", "link": "https://arxiv.org/pdf/2502.02013", "details": "O Skean, MR Arefin, D Zhao, N Patel, J Naghiyev\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "From extracting features to generating text, the outputs of large language models (LLMs) typically rely on their final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that \u2026"}, {"title": "Promote, Suppress, Iterate: How Language Models Answer One-to-Many Factual Queries", "link": "https://arxiv.org/pdf/2502.20475", "details": "TL Yan, R Jia - arXiv preprint arXiv:2502.20475, 2025", "abstract": "To answer one-to-many factual queries (eg, listing cities of a country), a language model (LM) must simultaneously recall knowledge and avoid repeating previous answers. How are these two subtasks implemented and integrated internally? Across \u2026"}, {"title": "Scalable Language Models with Posterior Inference of Latent Thought Vectors", "link": "https://arxiv.org/pdf/2502.01567%3F", "details": "D Kong, M Zhao, D Xu, B Pang, S Wang, E Honig, Z Si\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We propose a novel family of language models, Latent-Thought Language Models (LTMs), which incorporate explicit latent thought vectors that follow an explicit prior model in latent space. These latent thought vectors guide the autoregressive \u2026"}, {"title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "link": "https://arxiv.org/pdf/2502.21321", "details": "K Kumar, T Ashraf, O Thawakar, RM Anwer\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have transformed the natural language processing landscape and brought to life diverse applications. Pretraining on vast web-scale data has laid the foundation for these models, yet the research community is now \u2026"}, {"title": "FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge", "link": "https://arxiv.org/pdf/2502.19207", "details": "N Yang, M Kim, S Yoon, J Shin, K Jung - arXiv preprint arXiv:2502.19207, 2025", "abstract": "Various studies have attempted to remove sensitive or private knowledge from a language model to prevent its unauthorized exposure. However, prior studies have overlooked the complex and interconnected nature of knowledge, where related \u2026"}, {"title": "KG-prompt: Interpretable knowledge graph prompt for pre-trained language models", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125001650", "details": "L Chen, J Liu, Y Duan, R Wang - Knowledge-Based Systems, 2025", "abstract": "Abstract Knowledge graphs (KGs) can provide rich factual knowledge for language models, enhancing reasoning ability and interpretability. However, existing knowledge injection methods usually ignore the structured information in KGs. Using \u2026"}, {"title": "MeMo: Towards Language Models with Associative Memory Mechanisms", "link": "https://arxiv.org/pdf/2502.12851", "details": "FM Zanzotto, ES Ruzzetti, GA Xompero, L Ranaldi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Memorization is a fundamental ability of Transformer-based Large Language Models, achieved through learning. In this paper, we propose a paradigm shift by designing an architecture to memorize text directly, bearing in mind the principle that \u2026"}, {"title": "Leveraging language models for automated distribution of review notes in animated productions", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225002929", "details": "D Garc\u00e9s, M Santos, D Fern\u00e1ndez-Llorca - Neurocomputing, 2025", "abstract": "During the production of an animated film, professionals at the animation studio prepare thousands of notes. These notes describe improvements and corrections identified by supervisors and directors during daily meetings where the film's \u2026"}]
