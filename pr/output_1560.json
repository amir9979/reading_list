'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Text summarization with ChatGPT for drug labeling doc'
[{"title": "BANF: Band-limited Neural Fields for Levels of Detail Reconstruction", "link": "https://arxiv.org/pdf/2404.13024", "details": "A Shabanov, S Govindarajan, C Reading, L Goli\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Largely due to their implicit nature, neural fields lack a direct mechanism for filtering, as Fourier analysis from discrete signal processing is not directly applicable to these representations. Effective filtering of neural fields is critical to enable level-of-detail \u2026"}, {"title": "Vision\u2013language foundation model for echocardiogram interpretation", "link": "https://www.nature.com/articles/s41591-024-02959-y", "details": "M Christensen, M Vukadinovic, N Yuan, D Ouyang - Nature Medicine, 2024", "abstract": "The development of robust artificial intelligence models for echocardiography has been limited by the availability of annotated clinical data. Here, to address this challenge and improve the performance of cardiac imaging models, we developed \u2026"}, {"title": "PRISM: A Multi-Modal Generative Foundation Model for Slide-Level Histopathology", "link": "https://arxiv.org/pdf/2405.10254", "details": "G Shaikovski, A Casson, K Severson, E Zimmermann\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Foundation models in computational pathology promise to unlock the development of new clinical decision support systems and models for precision medicine. However, there is a mismatch between most clinical analysis, which is defined at the \u2026"}, {"title": "SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2405.08317", "details": "R Peri, SM Jayanthi, S Ronanki, A Bhatia, K Mundnich\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Integrated Speech and Large Language Models (SLMs) that can follow speech instructions and generate relevant text responses have gained popularity lately. However, the safety and robustness of these models remains largely unclear. In this \u2026"}, {"title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning", "link": "https://arxiv.org/abs/2405.10292", "details": "Y Zhai, H Bai, Z Lin, J Pan, S Tong, Y Zhou, A Suhr\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision-language models (VLMs) fine-tuned on specialized visual instruction- following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently \u2026"}, {"title": "Align vision-language semantics by multi-task learning for multi-modal summarization", "link": "https://link.springer.com/article/10.1007/s00521-024-09908-3", "details": "C Cui, X Liang, S Wu, Z Li - Neural Computing and Applications, 2024", "abstract": "Most current multi-modal summarization methods follow a cascaded manner, where an off-the-shelf object detector is first used to extract visual features. After that, these visual features are fused with language representations for the decoder to generate \u2026"}, {"title": "HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models", "link": "https://arxiv.org/abs/2405.10299", "details": "RS Sukthanker, A Zela, B Staffler, JKH Franke, F Hutter - arXiv preprint arXiv \u2026, 2024", "abstract": "The expanding size of language models has created the necessity for a comprehensive examination across various dimensions that reflect the desiderata with respect to the tradeoffs between various hardware metrics, such as latency \u2026"}, {"title": "EfficientGS: Streamlining Gaussian Splatting for Large-Scale High-Resolution Scene Representation", "link": "https://arxiv.org/pdf/2404.12777", "details": "W Liu, T Guan, B Zhu, L Ju, Z Song, D Li, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the domain of 3D scene representation, 3D Gaussian Splatting (3DGS) has emerged as a pivotal technology. However, its application to large-scale, high- resolution scenes (exceeding 4k $\\times $4 k pixels) is hindered by the excessive \u2026"}, {"title": "When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models", "link": "https://arxiv.org/pdf/2405.10255", "details": "X Ma, Y Bhalgat, B Smart, S Chen, X Li, J Ding, J Gu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As large language models (LLMs) evolve, their integration with 3D spatial data (3D- LLMs) has seen rapid progress, offering unprecedented capabilities for understanding and interacting with physical spaces. This survey provides a \u2026"}]
