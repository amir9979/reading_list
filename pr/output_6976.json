[{"title": "An adapted large language model facilitates multiple medical tasks in diabetes care", "link": "https://arxiv.org/pdf/2409.13191", "details": "L Wei, Z Ying, M He, Y Chen, Q Yang, Y Hong, J Lu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Diabetes is a chronic disease that poses a significant global health burden, and optimizing diabetes management requires multi-stakeholder collaboration. Large language models (LLMs) have shown promise in various healthcare scenarios, but \u2026"}, {"title": "A review of graph neural networks and pretrained language models for knowledge graph reasoning", "link": "https://www.sciencedirect.com/science/article/pii/S092523122401261X", "details": "J Ma, B Liu, K Li, C Li, F Zhang, X Luo, Y Qiao - Neurocomputing, 2024", "abstract": "Abstract Knowledge Graph (KG) stores human knowledge facts in an intuitive graphical structure but faces challenges such as incomplete construction or inability to handle new knowledge. Knowledge Graph Reasoning (KGR) can make KGs more \u2026"}, {"title": "MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large Language Model", "link": "https://arxiv.org/abs/2409.13729", "details": "Z Yang, J Chen, Z Du, W Yu, W Wang, W Hong, Z Jiang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated significant capabilities in mathematical reasoning, particularly with text-based mathematical problems. However, current multi-modal large language models (MLLMs), especially those \u2026"}, {"title": "Investigating Layer Importance in Large Language Models", "link": "https://arxiv.org/pdf/2409.14381", "details": "Y Zhang, Y Dong, K Kawaguchi - arXiv preprint arXiv:2409.14381, 2024", "abstract": "Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in \u2026"}, {"title": "Instruction-tuned Large Language Models for Machine Translation in the Medical Domain", "link": "https://arxiv.org/pdf/2408.16440", "details": "M Rios - arXiv preprint arXiv:2408.16440, 2024", "abstract": "Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains. However, in specialised domains (eg medical) LLMs have shown lower performance compared to standard \u2026"}, {"title": "Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style", "link": "https://arxiv.org/pdf/2409.10955", "details": "Y Li, K Zhou, Q Qiao, B Nguyen, Q Wang, Q Li - arXiv preprint arXiv:2409.10955, 2024", "abstract": "Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by incorporating external information into the response generation process. However, how context-faithful LLMs are and what factors influence LLMs' context \u2026"}, {"title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "link": "https://arxiv.org/pdf/2409.13853", "details": "Z Wang, R Bao, Y Wu, J Taylor, C Xiao, F Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pretrained large language models (LLMs) have revolutionized natural language processing (NLP) tasks such as summarization, question answering, and translation. However, LLMs pose significant security risks due to their tendency to memorize \u2026"}]
