[{"title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models", "link": "https://arxiv.org/pdf/2502.10250", "details": "GK Kumar, I Chaabane, K Wu - arXiv preprint arXiv:2502.10250, 2025", "abstract": "Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved \u2026"}, {"title": "KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan", "link": "https://arxiv.org/pdf/2502.12829", "details": "M Togmanov, N Mukhituly, D Turmakhan, J Mansurov\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite having a population of twenty million, Kazakhstan's culture and language remain underrepresented in the field of natural language processing. Although large language models (LLMs) continue to advance worldwide, progress in Kazakh \u2026"}, {"title": "Evaluating the Paperclip Maximizer: Are RL-Based Language Models More Likely to Pursue Instrumental Goals?", "link": "https://arxiv.org/pdf/2502.12206", "details": "Y He, Y Li, J Wu, Y Sui, Y Chen, B Hooi - arXiv preprint arXiv:2502.12206, 2025", "abstract": "As large language models (LLMs) continue to evolve, ensuring their alignment with human goals and values remains a pressing challenge. A key concern is\\textit {instrumental convergence}, where an AI system, in optimizing for a given objective \u2026"}, {"title": "Scalable Language Models with Posterior Inference of Latent Thought Vectors", "link": "https://arxiv.org/pdf/2502.01567%3F", "details": "D Kong, M Zhao, D Xu, B Pang, S Wang, E Honig, Z Si\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We propose a novel family of language models, Latent-Thought Language Models (LTMs), which incorporate explicit latent thought vectors that follow an explicit prior model in latent space. These latent thought vectors guide the autoregressive \u2026"}, {"title": "The Multilingual Mind: A Survey of Multilingual Reasoning in Language Models", "link": "https://arxiv.org/pdf/2502.09457", "details": "A Ghosh, D Datta, S Saha, C Agarwal - arXiv preprint arXiv:2502.09457, 2025", "abstract": "While reasoning and multilingual capabilities in Language Models (LMs) have achieved remarkable progress in recent years, their integration into a unified paradigm, multilingual reasoning, is at a nascent stage. Multilingual reasoning \u2026"}, {"title": "MeMo: Towards Language Models with Associative Memory Mechanisms", "link": "https://arxiv.org/pdf/2502.12851", "details": "FM Zanzotto, ES Ruzzetti, GA Xompero, L Ranaldi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Memorization is a fundamental ability of Transformer-based Large Language Models, achieved through learning. In this paper, we propose a paradigm shift by designing an architecture to memorize text directly, bearing in mind the principle that \u2026"}, {"title": "Evaluating Language Models on Grooming Risk Estimation Using Fuzzy Theory", "link": "https://arxiv.org/pdf/2502.12563", "details": "G Bihani, T Ringenberg, J Rayz - arXiv preprint arXiv:2502.12563, 2025", "abstract": "Encoding implicit language presents a challenge for language models, especially in high-risk domains where maintaining high precision is important. Automated detection of online child grooming is one such critical domain, where predators \u2026"}, {"title": "Leveraging language models for automated distribution of review notes in animated productions", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225002929", "details": "D Garc\u00e9s, M Santos, D Fern\u00e1ndez-Llorca - Neurocomputing, 2025", "abstract": "During the production of an animated film, professionals at the animation studio prepare thousands of notes. These notes describe improvements and corrections identified by supervisors and directors during daily meetings where the film's \u2026"}, {"title": "PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery", "link": "https://arxiv.org/pdf/2502.12594", "details": "B He, L Yin, HL Zhen, X Zhang, M Yuan, C Ma - arXiv preprint arXiv:2502.12594, 2025", "abstract": "Model pruning is an effective approach for compressing large language models. However, this process often leads to significant degradation of model capabilities. While post-training techniques such as instruction tuning are commonly employed to \u2026"}]
