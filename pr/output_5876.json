[{"title": "Symbolic Working Memory Enhances Language Models for Complex Rule Application", "link": "https://arxiv.org/pdf/2408.13654", "details": "S Wang, Z Wei, Y Choi, X Ren - arXiv preprint arXiv:2408.13654, 2024", "abstract": "Large Language Models (LLMs) have shown remarkable reasoning performance but struggle with multi-step deductive reasoning involving a series of rule application steps, especially when rules are presented non-sequentially. Our preliminary \u2026"}, {"title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "link": "https://arxiv.org/pdf/2407.21417", "details": "Z Wu, Y Zhang, P Qi, Y Xu, R Han, Y Zhang, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (ie, follow open-ended instructions) and \u2026"}, {"title": "Making Long-Context Language Models Better Multi-Hop Reasoners", "link": "https://arxiv.org/pdf/2408.03246", "details": "Y Li, S Liang, MR Lyu, L Wang - arXiv preprint arXiv:2408.03246, 2024", "abstract": "Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased \u2026"}, {"title": "Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language Models", "link": "https://arxiv.org/pdf/2408.14853", "details": "Y Du, Z Li, P Cheng, X Wan, A Gao - arXiv preprint arXiv:2408.14853, 2024", "abstract": "Large Language Models (LLMs) have become a focal point in the rapidly evolving field of artificial intelligence. However, a critical concern is the presence of toxic content within the pre-training corpus of these models, which can lead to the \u2026"}, {"title": "CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning", "link": "https://arxiv.org/pdf/2407.21011", "details": "Y Du, B Chang, NC Dvornek - arXiv preprint arXiv:2407.21011, 2024", "abstract": "Recent advancements in Contrastive Language-Image Pre-training (CLIP) have demonstrated notable success in self-supervised representation learning across various tasks. However, the existing CLIP-like approaches often demand extensive \u2026"}, {"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "link": "https://arxiv.org/pdf/2408.00724", "details": "Y Wu, Z Sun, S Li, S Welleck, Y Yang - arXiv preprint arXiv:2408.00724, 2024", "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth \u2026"}, {"title": "KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models", "link": "https://arxiv.org/pdf/2408.03297", "details": "R Zhang, Y Xu, Y Xiao, R Zhu, X Jiang, X Chu, J Zhao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks \u2026"}, {"title": "Towards Case-based Interpretability for Medical Federated Learning", "link": "https://arxiv.org/pdf/2408.13626", "details": "L Latorre, L Petrychenko, R Beets-Tan, T Kopytova\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We explore deep generative models to generate case-based explanations in a medical federated learning setting. Explaining AI model decisions through case- based interpretability is paramount to increasing trust and allowing widespread \u2026"}, {"title": "Ensembling disentangled domain-specific prompts for domain generalization", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124009924", "details": "F Xu, S Deng, T Jia, X Yu, D Chen - Knowledge-Based Systems, 2024", "abstract": "Abstract Domain generalization (DG) is a challenging problem because we cannot access any unseen target domain data during training. Recent emergence of vision- language models (VLMs) has inspired researchers to improve the DG performance \u2026"}]
