[{"title": "Self-supervised analogical learning using language models", "link": "https://arxiv.org/pdf/2502.00996", "details": "B Zhou, S Jain, Y Zhang, Q Ning, S Wang, Y Benajiba\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can \u2026"}, {"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}, {"title": "DesCLIP: Robust Continual Adaptation via General Attribute Descriptions for Pretrained Vision-Language Models", "link": "https://arxiv.org/pdf/2502.00618", "details": "C He, Z Qiu, F Meng, L Xu, Q Wu, H Li - arXiv preprint arXiv:2502.00618, 2025", "abstract": "Continual adaptation of vision-language models (VLMs) focuses on leveraging cross- modal pretrained knowledge to incrementally adapt for expanding downstream tasks and datasets, while tackling the challenge of knowledge forgetting. Existing research \u2026"}, {"title": "Mordal: Automated Pretrained Model Selection for Vision Language Models", "link": "https://arxiv.org/pdf/2502.00241", "details": "S He, I Jang, M Chowdhury - arXiv preprint arXiv:2502.00241, 2025", "abstract": "Incorporating multiple modalities into large language models (LLMs) is a powerful way to enhance their understanding of non-textual data, enabling them to perform multimodal tasks. Vision language models (VLMs) form the fastest growing category \u2026"}, {"title": "Evaluating Small Language Models for News Summarization: Implications and Factors Influencing Performance", "link": "https://arxiv.org/pdf/2502.00641", "details": "B Xu, Y Chen, Z Wen, W Liu, B He - arXiv preprint arXiv:2502.00641, 2025", "abstract": "The increasing demand for efficient summarization tools in resource-constrained environments highlights the need for effective solutions. While large language models (LLMs) deliver superior summarization quality, their high computational \u2026"}, {"title": "Sibyl: Empowering Empathetic Dialogue Generation in Large Language Models via Sensible and Visionary Commonsense Inference", "link": "https://aclanthology.org/2025.coling-main.10.pdf", "details": "L Wang, J Li, C Yang, Z Lin, H Tang, H Liu, Y Cao\u2026 - Proceedings of the 31st \u2026, 2025", "abstract": "Recently, there has been a heightened interest in building chatbots based on Large Language Models (LLMs) to emulate human-like qualities in multi-turn conversations. Despite having access to commonsense knowledge to better \u2026"}, {"title": "Generalized Simple Graphical Rules for Assessing Selection Bias", "link": "https://arxiv.org/pdf/2502.00924", "details": "Y Zhang, H Lu - arXiv preprint arXiv:2502.00924, 2025", "abstract": "Selection bias is a major obstacle toward valid causal inference in epidemiology. Over the past decade, several simple graphical rules based on causal diagrams have been proposed as the sufficient identification conditions for addressing \u2026"}, {"title": "HaluCheck: Explainable and verifiable automation for detecting hallucinations in LLM responses", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425003343", "details": "S Heo, S Son, H Park - Expert Systems with Applications, 2025", "abstract": "Large language models have become integral to various aspects of modern life, but a critical challenge persists: hallucinations. This work contributes to expert systems research by providing a systematic framework for enhancing AI reliability and \u2026"}, {"title": "Leveraging large language models for abstractive summarization of Italian legal news", "link": "https://link.springer.com/article/10.1007/s10506-025-09431-3", "details": "I Benedetto, L Cagliero, M Ferro, F Tarasconi, C Bernini\u2026 - Artificial Intelligence and \u2026, 2025", "abstract": "Condensing the key message conveyed by a long document into an informative summary is particularly helpful to lawyers and legal experts. State-of-the-art approaches to legal document summarization rely on Language Models (LMs) and \u2026"}]
