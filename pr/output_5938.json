[{"title": "POA: Pre-training Once for Models of All Sizes", "link": "https://arxiv.org/pdf/2408.01031", "details": "Y Zhang, X Guo, J Lao, L Yu, L Ru, J Wang, G Ye, H He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large-scale self-supervised pre-training has paved the way for one foundation model to handle many different vision tasks. Most pre-training methodologies train a single model of a certain size at one time. Nevertheless, various computation or \u2026"}, {"title": "M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation", "link": "https://arxiv.org/pdf/2408.16213", "details": "J Park, S Kim, B Yoon, J Hyun, K Choi - arXiv preprint arXiv:2408.16213, 2024", "abstract": "The rapid evolution of artificial intelligence, especially in large language models (LLMs), has significantly impacted various domains, including healthcare. In chest X- ray (CXR) analysis, previous studies have employed LLMs, but with limitations: either \u2026"}, {"title": "Cross-Modal Learning for Chemistry Property Prediction: Large Language Models Meet Graph Machine Learning", "link": "https://arxiv.org/pdf/2408.14964", "details": "SS Srinivas, V Runkana - arXiv preprint arXiv:2408.14964, 2024", "abstract": "In the field of chemistry, the objective is to create novel molecules with desired properties, facilitating accurate property predictions for applications such as material design and drug screening. However, existing graph deep learning methods face \u2026"}, {"title": "Deconstructing reasoning paths and attending to semantic guidance for document-level relation extraction", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124009626", "details": "Y Zhong, B Shen, T Wang - Knowledge-Based Systems, 2024", "abstract": "Extracting relations from a document is more challenging than from a sentence, due to the involvement of more entities and complex contextual information. To capture long-distance contextual dependencies, graph networks are widely used in \u2026"}, {"title": "ReXamine-Global: A Framework for Uncovering Inconsistencies in Radiology Report Generation Metrics", "link": "https://arxiv.org/pdf/2408.16208", "details": "O Banerjee, A Saenz, K Wu, W Clements, A Zia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Given the rapidly expanding capabilities of generative AI models for radiology, there is a need for robust metrics that can accurately measure the quality of AI-generated radiology reports across diverse hospitals. We develop ReXamine-Global, a LLM \u2026"}, {"title": "Geometric View of Soft Decorrelation in Self-Supervised Learning", "link": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671914", "details": "Y Zhang, H Zhu, Z Song, Y Chen, X Fu, Z Meng\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "Contrastive learning, a form of Self-Supervised Learning (SSL), typically consists of an alignment term and a regularization term. The alignment term minimizes the distance between the embeddings of a positive pair, while the regularization term \u2026"}]
