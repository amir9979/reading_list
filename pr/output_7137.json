[{"title": "Mutual Prompt Leaning for Vision Language Models", "link": "https://link.springer.com/article/10.1007/s11263-024-02243-z", "details": "S Long, Z Zhao, J Yuan, Z Tan, J Liu, J Feng, S Wang\u2026 - International Journal of \u2026, 2024", "abstract": "Large pre-trained vision language models (VLMs) have demonstrated impressive representation learning capabilities, but their transferability across various downstream tasks heavily relies on prompt learning. Since VLMs consist of text and \u2026"}, {"title": "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness", "link": "https://arxiv.org/pdf/2409.17791", "details": "J Li, H Huang, Y Zhang, P Xu, X Chen, R Song, L Shi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, there has been significant interest in replacing the reward model in Reinforcement Learning with Human Feedback (RLHF) methods for Large Language Models (LLMs), such as Direct Preference Optimization (DPO) and its \u2026"}, {"title": "Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation", "link": "https://arxiv.org/pdf/2409.17313", "details": "Z Wang, M Wu, Y Cao, Y Ma, M Chen, T Tuytelaars - arXiv preprint arXiv:2409.17313, 2024", "abstract": "This study presents a novel evaluation framework for the Vision-Language Navigation (VLN) task. It aims to diagnose current models for various instruction categories at a finer-grained level. The framework is structured around the context \u2026"}, {"title": "EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language Models", "link": "https://arxiv.org/pdf/2409.17892", "details": "S Ji, Z Li, I Paul, J Paavola, P Lin, P Chen, D O'Brien\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we introduce EMMA-500, a large-scale multilingual language model continue-trained on texts across 546 languages designed for enhanced multilingual performance, focusing on improving language coverage for low-resource languages \u2026"}, {"title": "Interpreting and Controlling Linguistic Features in Multilingual Language Models", "link": "https://dspace.cuni.cz/bitstream/handle/20.500.11956/192821/140123221.pdf%3Fsequence%3D1", "details": "T Limisiewicz - 2024", "abstract": "Language models based on neural networks have become the foundation for solving diverse tasks, yet their inner workings remain opaque. This dissertation investigates which components of language models are crucial for representing and processing \u2026"}, {"title": "Enhancing Polyglot Voices by Leveraging Cross-Lingual Fine-Tuning in Any-to-One Voice Conversion", "link": "https://arxiv.org/pdf/2409.17387", "details": "G Ruggiero, M Testa, J Van de Walle, L Di Caro - arXiv preprint arXiv:2409.17387, 2024", "abstract": "The creation of artificial polyglot voices remains a challenging task, despite considerable progress in recent years. This paper investigates self-supervised learning for voice conversion to create native-sounding polyglot voices. We introduce \u2026"}, {"title": "Weighted Cross-entropy for Low-Resource Languages in Multilingual Speech Recognition", "link": "https://arxiv.org/pdf/2409.16954", "details": "A Pi\u00f1eiro-Mart\u00edn, C Garc\u00eda-Mateo, L Doc\u00edo-Fern\u00e1ndez\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper addresses the challenge of integrating low-resource languages into multilingual automatic speech recognition (ASR) systems. We introduce a novel application of weighted cross-entropy, typically used for unbalanced datasets, to \u2026"}]
