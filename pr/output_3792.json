[{"title": "Interpretable Differential Diagnosis with Dual-Inference Large Language Models", "link": "https://arxiv.org/pdf/2407.07330", "details": "S Zhou, S Ding, J Wang, M Lin, GB Melton, R Zhang - arXiv preprint arXiv:2407.07330, 2024", "abstract": "Methodological advancements to automate the generation of differential diagnosis (DDx) to predict a list of potential diseases as differentials given patients' symptom descriptions are critical to clinical reasoning and applications such as decision \u2026"}, {"title": "How Do Large Language Models Acquire Factual Knowledge During Pretraining?", "link": "https://arxiv.org/pdf/2406.11813", "details": "H Chang, J Park, S Ye, S Yang, Y Seo, DS Chang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the recent observation that large language models (LLMs) can store substantial factual knowledge, there is a limited understanding of the mechanisms of how they acquire factual knowledge through pretraining. This work addresses this \u2026"}, {"title": "Developing a natural language processing system using transformer-based models for adverse drug event detection in electronic health records", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/07/10/2024.07.09.24310100.full.pdf", "details": "J Wu, X Ruan, E McNeer, KM Rossow, L Choi - medRxiv, 2024", "abstract": "Objective: To develop a transformer-based natural language processing (NLP) system for detecting adverse drug events (ADEs) from clinical notes in electronic health records (EHRs). Materials and Methods: We fine-tuned BERT Short-Formers \u2026"}, {"title": "EM_Mixers at MEDIQA-CORR 2024: Knowledge-Enhanced Few-Shot In-Context Learning for Medical Error Detection and Correction", "link": "https://aclanthology.org/2024.clinicalnlp-1.56.pdf", "details": "S Rajwal, E Agichtein, A Sarker - Proceedings of the 6th Clinical Natural Language \u2026, 2024", "abstract": "This paper describes our submission to MEDIQA-CORR 2024 shared task for automatic identification and correction of medical errors in a given clinical text. We report results from two approaches: the first uses a few-shot in-context learning (ICL) \u2026"}, {"title": "Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language Models", "link": "https://arxiv.org/pdf/2407.07263", "details": "J Parmar, S Satheesh, M Patwary, M Shoeybi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As language models have scaled both their number of parameters and pretraining dataset sizes, the computational cost for pretraining has become intractable except for the most well-resourced teams. This increasing cost makes it ever more important \u2026"}, {"title": "On Speeding Up Language Model Evaluation", "link": "https://arxiv.org/pdf/2407.06172", "details": "JP Zhou, CK Belardi, R Wu, T Zhang, CP Gomes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) currently dominate the field of natural language processing (NLP), representing the state-of-the-art across a diverse array of tasks. Developing a model of this nature, from training to inference, requires making \u2026"}]
