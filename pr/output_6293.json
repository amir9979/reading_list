[{"title": "DP-FedEwc: Differentially private federated elastic weight consolidation for model personalization", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124010359", "details": "J Liang, S Su - Knowledge-Based Systems, 2024", "abstract": "Federated learning (FL) has become a prevalent paradigm for training a model collaboratively on multiple clients with the coordination of a central server. As traditional FL suffers from client-drift due to data heterogeneity across clients, many \u2026"}, {"title": "Advancing entity alignment with dangling cases: a structure-aware approach through optimal transport learning and contrastive learning", "link": "https://link.springer.com/article/10.1007/s00521-024-10276-1", "details": "J Xu, Y Li, X Xie, N Hu, Y Li, HT Zheng, Y Jiang - Neural Computing and Applications, 2024", "abstract": "Entity alignment (EA) aims to discover the equivalent entities in different knowledge graphs (KGs), which plays an important role in knowledge engineering. Recently, EA with dangling entities has been proposed as a more realistic setting, which assumes \u2026"}, {"title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.06663", "details": "K Sun, M Dredze - arXiv preprint arXiv:2408.06663, 2024", "abstract": "The development of large language models leads to the formation of a pre-train-then- align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream \u2026"}, {"title": "Large Language Models Can Learn Representation in Natural Language", "link": "https://aclanthology.org/2024.findings-acl.542.pdf", "details": "Y Guo, Y Liang, D Zhao, N Duan - Findings of the Association for Computational \u2026, 2024", "abstract": "One major challenge for Large Language Models (LLMs) is completing complex tasks involving multiple entities, such as tool APIs. To tackle this, one approach is to retrieve relevant entities to enhance LLMs in task completion. A crucial issue here is \u2026"}, {"title": "Race, Gender, and Donor Heart Acceptance\u2014Reply", "link": "https://jamanetwork.com/journals/jama/article-abstract/2822179", "details": "K Breathett, SM Knapp - JAMA", "abstract": "In Reply Ms Majeed and colleagues pose important questions regarding our study. 1 First, the COVID-19 pandemic had several possible implications for transplant care. To explore these implications, we reexamined the number of donor heart offers until \u2026"}, {"title": "Leveraging Variation Theory in Counterfactual Data Augmentation for Optimized Active Learning", "link": "https://ui.adsabs.harvard.edu/abs/2024arXiv240803819A/abstract", "details": "S Araya Gebreegziabher, K Ai, Z Zhang, EL Glassman\u2026 - arXiv e-prints, 2024", "abstract": "Active Learning (AL) allows models to learn interactively from user feedback. This paper introduces a counterfactual data augmentation approach to AL, particularly addressing the selection of datapoints for user querying, a pivotal concern in \u2026"}, {"title": "KoCommonGEN v2: A Benchmark for Navigating Korean Commonsense Reasoning Challenges in Large Language Models", "link": "https://aclanthology.org/2024.findings-acl.141.pdf", "details": "J Seo, J Lee, C Park, ST Hong, S Lee, HS Lim - Findings of the Association for \u2026, 2024", "abstract": "The evolution of large language models (LLMs) has culminated in a multitask model paradigm where prompts drive the generation of user-specific outputs. However, this advancement has revealed a critical challenge: LLMs frequently produce outputs \u2026"}, {"title": "Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models", "link": "https://aclanthology.org/2024.acl-long.525.pdf", "details": "Y Sun, C Liu, K Zhou, J Huang, R Song, WX Zhao\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Humans often interact with large language models (LLMs) in multi-turn interaction to obtain desired answers or more information. However, most existing studies overlook the multi-turn instruction following ability of LLMs, in terms of training dataset, training \u2026"}, {"title": "T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step", "link": "https://aclanthology.org/2024.acl-long.515.pdf", "details": "Z Chen, W Du, W Zhang, K Liu, J Liu, M Zheng, J Zhuo\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Large language models (LLMs) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool utilization capability of LLMs is still under-explored. In contrast \u2026"}]
