[{"title": "Large Language Models Often Know When They Are Being Evaluated", "link": "https://arxiv.org/pdf/2505.23836", "details": "J Needham, G Edkins, G Pimpale, H Bartsch\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 , it may also enable the model to distinguish an **evaluation** setting from a real deployment \u2013 a phenomenon we call **evaluation** awareness. \u2026 Some interactions with **large** **language** **models** are with real users and others are automated for the \u2026", "entry_id": "http://arxiv.org/abs/2505.23836v1", "updated": "2025-05-28 12:03:09", "published": "2025-05-28 12:03:09", "authors": "Joe Needham;Giles Edkins;Govind Pimpale;Henning Bartsch;Marius Hobbhahn", "summary": "If AI models can detect when they are being evaluated, the effectiveness of\nevaluations might be compromised. For example, models could have systematically\ndifferent behavior during evaluations, leading to less reliable benchmarks for\ndeployment and governance decisions. We investigate whether frontier language\nmodels can accurately classify transcripts based on whether they originate from\nevaluations or real-world deployment, a capability we call evaluation\nawareness. To achieve this, we construct a diverse benchmark of 1,000 prompts\nand transcripts from 61 distinct datasets. These span public benchmarks (e.g.,\nMMLU, SWEBench), real-world deployment interactions, and agent trajectories\nfrom scaffolding frameworks (e.g., web-browsing agents). Frontier models\nclearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches\nan AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of\n$0.92$). Furthermore, both AI models and humans are better at identifying\nevaluations in agentic settings compared to chat settings. Additionally, we\ntest whether models can identify the purpose of the evaluation. Under\nmultiple-choice and open-ended questioning, AI models far outperform random\nchance in identifying what an evaluation is testing for. Our results indicate\nthat frontier models already exhibit a substantial, though not yet superhuman,\nlevel of evaluation-awareness. We recommend tracking this capability in future\nmodels.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.23836v1;http://arxiv.org/pdf/2505.23836v1", "pdf_url": "http://arxiv.org/pdf/2505.23836v1"}, {"title": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks", "link": "https://arxiv.org/pdf/2505.23802", "details": "S Bedi, H Cui, M Fuentes, A Unell, M Wornow\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While **large** **language** **models** (LLMs) achieve near-perfect scores on medical licensing exams, these **evaluations** inadequately reflect the complexity and diversity of real-world clinical practice. We introduce MedHELM, an extensible **evaluation** \u2026", "entry_id": "http://arxiv.org/abs/2505.23802v2", "updated": "2025-06-02 04:19:10", "published": "2025-05-26 22:55:49", "authors": "Suhana Bedi;Hejie Cui;Miguel Fuentes;Alyssa Unell;Michael Wornow;Juan M. Banda;Nikesh Kotecha;Timothy Keyes;Yifan Mai;Mert Oez;Hao Qiu;Shrey Jain;Leonardo Schettini;Mehr Kashyap;Jason Alan Fries;Akshay Swaminathan;Philip Chung;Fateme Nateghi;Asad Aali;Ashwin Nayak;Shivam Vedak;Sneha S. Jain;Birju Patel;Oluseyi Fayanju;Shreya Shah;Ethan Goh;Dong-han Yao;Brian Soetikno;Eduardo Reis;Sergios Gatidis;Vasu Divi;Robson Capasso;Rachna Saralkar;Chia-Chun Chiang;Jenelle Jindal;Tho Pham;Faraz Ghoddusi;Steven Lin;Albert S. Chiou;Christy Hong;Mohana Roy;Michael F. Gensheimer;Hinesh Patel;Kevin Schulman;Dev Dash;Danton Char;Lance Downing;Francois Grolleau;Kameron Black;Bethel Mieso;Aydin Zahedivash;Wen-wai Yim;Harshita Sharma;Tony Lee;Hannah Kirsch;Jennifer Lee;Nerissa Ambers;Carlene Lugtu;Aditya Sharma;Bilal Mawji;Alex Alekseyev;Vicky Zhou;Vikas Kakkar;Jarrod Helzer;Anurang Revri;Yair Bannett;Roxana Daneshjou;Jonathan Chen;Emily Alsentzer;Keith Morse;Nirmal Ravi;Nima Aghaeepour;Vanessa Kennedy;Akshay Chaudhari;Thomas Wang;Sanmi Koyejo;Matthew P. Lungren;Eric Horvitz;Percy Liang;Mike Pfeffer;Nigam H. Shah", "summary": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.23802v2;http://arxiv.org/pdf/2505.23802v2", "pdf_url": "http://arxiv.org/pdf/2505.23802v2"}, {"title": "USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2505.23793", "details": "B Zheng, G Chen, H Zhong, Q Teng, Y Tan, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 In this paper, we present USB-SafeBench, a unified benchmark for **evaluating** the safety of multimodal **large** **language** **models** (MLLMs). It enables reliable safety assessment through a single, comprehensive dataset. USB-SafeBench offers broad \u2026", "entry_id": "http://arxiv.org/abs/2505.23793v1", "updated": "2025-05-26 08:39:14", "published": "2025-05-26 08:39:14", "authors": "Baolin Zheng;Guanlin Chen;Hongqiong Zhong;Qingyang Teng;Yingshui Tan;Zhendong Liu;Weixun Wang;Jiaheng Liu;Jian Yang;Huiyun Jing;Jincheng Wei;Wenbo Su;Xiaoyong Zhu;Bo Zheng;Kaifu Zhang", "summary": "Despite their remarkable achievements and widespread adoption, Multimodal\nLarge Language Models (MLLMs) have revealed significant security\nvulnerabilities, highlighting the urgent need for robust safety evaluation\nbenchmarks. Existing MLLM safety benchmarks, however, fall short in terms of\ndata quality and coverge, and modal risk combinations, resulting in inflated\nand contradictory evaluation results, which hinders the discovery and\ngovernance of security concerns. Besides, we argue that vulnerabilities to\nharmful queries and oversensitivity to harmless ones should be considered\nsimultaneously in MLLMs safety evaluation, whereas these were previously\nconsidered separately. In this paper, to address these shortcomings, we\nintroduce Unified Safety Benchmarks (USB), which is one of the most\ncomprehensive evaluation benchmarks in MLLM safety. Our benchmark features\nhigh-quality queries, extensive risk categories, comprehensive modal\ncombinations, and encompasses both vulnerability and oversensitivity\nevaluations. From the perspective of two key dimensions: risk categories and\nmodality combinations, we demonstrate that the available benchmarks -- even the\nunion of the vast majority of them -- are far from being truly comprehensive.\nTo bridge this gap, we design a sophisticated data synthesis pipeline that\ngenerates extensive, high-quality complementary data addressing previously\nunexplored aspects. By combining open-source datasets with our synthetic data,\nour benchmark provides 4 distinct modality combinations for each of the 61 risk\nsub-categories, covering both English and Chinese across both vulnerability and\noversensitivity dimensions.", "comment": null, "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR;cs.AI", "links": "http://arxiv.org/abs/2505.23793v1;http://arxiv.org/pdf/2505.23793v1", "pdf_url": "http://arxiv.org/pdf/2505.23793v1"}, {"title": "Revisiting Uncertainty Estimation and Calibration of Large Language Models", "link": "https://arxiv.org/pdf/2505.23854", "details": "L Tao, YF Yeh, M Dong, T Huang, P Torr, C Xu - arXiv preprint arXiv:2505.23854, 2025", "abstract": "\u2026 2.2 **Evaluation** of Uncertainty Estimation To assess the quality of uncertainty estimation in **large** **language** **models** (LLMs), we consider two widely adopted **evaluation** tasks: uncertainty calibration and uncertainty-based selective \u2026", "entry_id": "http://arxiv.org/abs/2505.23854v1", "updated": "2025-05-29 02:04:49", "published": "2025-05-29 02:04:49", "authors": "Linwei Tao;Yi-Fan Yeh;Minjing Dong;Tao Huang;Philip Torr;Chang Xu", "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\napplications, robust uncertainty estimation is essential for ensuring the safe\nand trustworthy deployment of LLMs. We present the most comprehensive study to\ndate of uncertainty estimation in LLMs, evaluating 80 models spanning open- and\nclosed-source families, dense and Mixture-of-Experts (MoE) architectures,\nreasoning and non-reasoning modes, quantization variants and parameter scales\nfrom 0.6B to 671B. Focusing on three representative black-box single-pass\nmethods, including token probability-based uncertainty (TPU), numerical verbal\nuncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically\nevaluate uncertainty calibration and selective classification using the\nchallenging MMLU-Pro benchmark, which covers both reasoning-intensive and\nknowledge-based tasks. Our results show that LVU consistently outperforms TPU\nand NVU, offering stronger calibration and discrimination while being more\ninterpretable. We also find that high accuracy does not imply reliable\nuncertainty, and that model scale, post-training, reasoning ability and\nquantization all influence estimation performance. Notably, LLMs exhibit better\nuncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good\ncalibration does not necessarily translate to effective error ranking. These\nfindings highlight the need for multi-perspective evaluation and position LVU\nas a practical tool for improving the reliability of LLMs in real-world\nsettings.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.23854v1;http://arxiv.org/pdf/2505.23854v1", "pdf_url": "http://arxiv.org/pdf/2505.23854v1"}, {"title": "Disentangling Language and Culture for Evaluating Multilingual Large Language Models", "link": "https://arxiv.org/pdf/2505.24635", "details": "J Ying, W Tang, Y Zhao, Y Cao, Y Rong, W Zhang - arXiv preprint arXiv:2505.24635, 2025", "abstract": "\u2026 To comprehensively **evaluate** multilingual capability, especially considering the real-world usage, we propose a Dual **Evaluation** framework in this paper, which decomposes the multilingual capability **evaluation** along two critical dimensions: (1) \u2026", "entry_id": "http://arxiv.org/abs/2505.24635v1", "updated": "2025-05-30 14:25:45", "published": "2025-05-30 14:25:45", "authors": "Jiahao Ying;Wei Tang;Yiran Zhao;Yixin Cao;Yu Rong;Wenxuan Zhang", "summary": "This paper introduces a Dual Evaluation Framework to comprehensively assess\nthe multilingual capabilities of LLMs. By decomposing the evaluation along the\ndimensions of linguistic medium and cultural context, this framework enables a\nnuanced analysis of LLMs' ability to process questions within both native and\ncross-cultural contexts cross-lingually. Extensive evaluations are conducted on\na wide range of models, revealing a notable \"CulturalLinguistic Synergy\"\nphenomenon, where models exhibit better performance when questions are\nculturally aligned with the language. This phenomenon is further explored\nthrough interpretability probing, which shows that a higher proportion of\nspecific neurons are activated in a language's cultural context. This\nactivation proportion could serve as a potential indicator for evaluating\nmultilingual performance during model training. Our findings challenge the\nprevailing notion that LLMs, primarily trained on English data, perform\nuniformly across languages and highlight the necessity of culturally and\nlinguistically model evaluations. Our code can be found at\nhttps://yingjiahao14. github.io/Dual-Evaluation/.", "comment": "Accepted to ACL 2025 (Main Conference)", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.24635v1;http://arxiv.org/pdf/2505.24635v1", "pdf_url": "http://arxiv.org/pdf/2505.24635v1"}, {"title": "A structured review of **large language models** in metaheuristic optimisation", "link": "https://www.sciencedirect.com/science/article/pii/S2772662225000438", "details": "R Ghanbarzadeh, S Mirjalili - Decision Analytics Journal, 2025", "abstract": "\u2026 The findings demonstrate that **large** **language** **models** support the automation of \u2026 These issues highlight the need for more robust **evaluation** frameworks and benchmarking \u2026 and practitioners aiming to integrate **large** **language** **models** into \u2026"}, {"title": "Applying **large language models** to sanitize self-disclosure in user-generated content", "link": "https://www.sciencedirect.com/science/article/pii/S1568494625006222", "details": "C Alfieri, GL Scoccia, S Ganesh, N Sadeh - Applied Soft Computing, 2025", "abstract": "The rise of e-commerce and social networking platforms has led to an increase in the disclosure of personal health information within user-generated content. This study investigates the application of **large** **language** **models** (LLMs) to detect and \u2026"}, {"title": "HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America", "link": "https://arxiv.org/pdf/2505.24712", "details": "G Ivetta, MJ Gomez, S Martinelli, P Palombini\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Most resources for **evaluating** social biases in **Large** **Language** **Models** are developed without co-design from the communities affected by these biases, and rarely involve participatory approaches. We introduce HESEIA, a dataset of 46,499 \u2026", "entry_id": "http://arxiv.org/abs/2505.24712v1", "updated": "2025-05-30 15:32:48", "published": "2025-05-30 15:32:48", "authors": "Guido Ivetta;Marcos J. Gomez;Sof\u00eda Martinelli;Pietro Palombini;M. Emilia Echeveste;Nair Carolina Mazzeo;Beatriz Busaniche;Luciana Benotti", "summary": "Most resources for evaluating social biases in Large Language Models are\ndeveloped without co-design from the communities affected by these biases, and\nrarely involve participatory approaches. We introduce HESEIA, a dataset of\n46,499 sentences created in a professional development course. The course\ninvolved 370 high-school teachers and 5,370 students from 189 Latin-American\nschools. Unlike existing benchmarks, HESEIA captures intersectional biases\nacross multiple demographic axes and school subjects. It reflects local\ncontexts through the lived experience and pedagogical expertise of educators.\nTeachers used minimal pairs to create sentences that express stereotypes\nrelevant to their school subjects and communities. We show the dataset\ndiversity in term of demographic axes represented and also in terms of the\nknowledge areas included. We demonstrate that the dataset contains more\nstereotypes unrecognized by current LLMs than previous datasets. HESEIA is\navailable to support bias assessments grounded in educational communities.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.CY", "links": "http://arxiv.org/abs/2505.24712v1;http://arxiv.org/pdf/2505.24712v1", "pdf_url": "http://arxiv.org/pdf/2505.24712v1"}, {"title": "Benchmarking Large Language Models for Cryptanalysis and Mismatched-Generalization", "link": "https://arxiv.org/pdf/2505.24621", "details": "U Maskey, C Zhu, U Naseem - arXiv preprint arXiv:2505.24621, 2025", "abstract": "\u2026 However, cryptanalysis\u2014a critical area for data security and encryption\u2014has not yet been thoroughly explored in LLM **evaluations**. To address this gap, we **evaluate** cryptanalytic potential of state-of-the-art LLMs on encrypted texts generated using a \u2026", "entry_id": "http://arxiv.org/abs/2505.24621v1", "updated": "2025-05-30 14:12:07", "published": "2025-05-30 14:12:07", "authors": "Utsav Maskey;Chencheng Zhu;Usman Naseem", "summary": "Recent advancements in Large Language Models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis a critical area for data security and\nencryption has not yet been thoroughly explored in LLM evaluations. To address\nthis gap, we evaluate cryptanalytic potential of state of the art LLMs on\nencrypted texts generated using a range of cryptographic algorithms. We\nintroduce a novel benchmark dataset comprising diverse plain texts spanning\nvarious domains, lengths, writing styles, and topics paired with their\nencrypted versions. Using zero-shot and few shot settings, we assess multiple\nLLMs for decryption accuracy and semantic comprehension across different\nencryption schemes. Our findings reveal key insights into the strengths and\nlimitations of LLMs in side-channel communication while raising concerns about\ntheir susceptibility to jailbreaking attacks. This research highlights the\ndual-use nature of LLMs in security contexts and contributes to the ongoing\ndiscussion on AI safety and security.", "comment": "Preprint", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.24621v1;http://arxiv.org/pdf/2505.24621v1", "pdf_url": "http://arxiv.org/pdf/2505.24621v1"}]
