'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Relation Extraction Using Large Language Models: A Cas'
[{"title": "Calibration and Uncertainty Estimation Challenges in Self-Supervised Chest X-ray Pathology Classification Models", "link": "https://openreview.net/pdf%3Fid%3DPfCY5BLNHc", "details": "J Xu, P Rajpurkar - Medical Imaging with Deep Learning, 2024", "abstract": "Uncertainty quantification is crucial for the safe deployment of AI systems in clinical radiology. We analyze the calibration of CheXzero (Tiu et al., 2022), a high- performance self-supervised model for chest X-ray pathology detection, on two \u2026"}, {"title": "Observation, Analysis, and Solution: Exploring Strong Lightweight Vision Transformers via Masked Image Modeling Pre-Training", "link": "https://arxiv.org/pdf/2404.12210", "details": "J Gao, S Lin, S Wang, Y Kou, Z Li, L Li, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Masked image modeling (MIM) pre-training for large-scale vision transformers (ViTs) in computer vision has enabled promising downstream performance on top of the learned self-supervised ViT features. In this paper, we question if the extremely \u2026"}, {"title": "Masked Modeling Duo: Towards a Universal Audio Pre-Training Framework", "link": "https://ieeexplore.ieee.org/iel7/6570655/6633080/10502167.pdf", "details": "D Niizumi, D Takeuchi, Y Ohishi, N Harada, K Kashino - IEEE/ACM Transactions on \u2026, 2024", "abstract": "Self-supervised learning (SSL) using masked prediction has made great strides in general-purpose audio representation. This study proposes Masked Modeling Duo (M2D), an improved masked prediction SSL, which learns by predicting \u2026"}, {"title": "Language Models Still Struggle to Zero-shot Reason about Time Series", "link": "https://arxiv.org/pdf/2404.11757", "details": "MA Merrill, M Tan, V Gupta, T Hartvigsen, T Althoff - arXiv preprint arXiv:2404.11757, 2024", "abstract": "Time series are critical for decision-making in fields like finance and healthcare. Their importance has driven a recent influx of works passing time series into language models, leading to non-trivial forecasting on some datasets. But it remains \u2026"}, {"title": "Training a high-performance retinal foundation model with half-the-data and 400 times less compute", "link": "https://arxiv.org/pdf/2405.00117", "details": "J Engelmann, MO Bernabeu - arXiv preprint arXiv:2405.00117, 2024", "abstract": "Artificial Intelligence holds tremendous potential in medicine, but is traditionally limited by the lack of massive datasets to train models on. Foundation models, pre- trained models that can be adapted to downstream tasks with small datasets, could \u2026"}, {"title": "MetaRM: Shifted Distributions Alignment via Meta-Learning", "link": "https://arxiv.org/pdf/2405.00438", "details": "S Dou, Y Liu, E Zhou, T Li, H Jia, L Xiong, X Zhao, J Ye\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The success of Reinforcement Learning from Human Feedback (RLHF) in language model alignment is critically dependent on the capability of the reward model (RM). However, as the training process progresses, the output distribution of the policy \u2026"}, {"title": "Diluie: constructing diverse demonstrations of in-context learning with large language model for unified information extraction", "link": "https://link.springer.com/article/10.1007/s00521-024-09728-5", "details": "Q Guo, Y Guo, J Zhao - Neural Computing and Applications, 2024", "abstract": "Large language models (LLMs) have demonstrated promising in-context learning capabilities, especially with instructive prompts. However, recent studies have shown that existing large models still face challenges in specific information extraction (IE) \u2026"}, {"title": "Expert Insight-Enhanced Follow-up Chest X-Ray Summary Generation", "link": "https://arxiv.org/pdf/2405.00344", "details": "Z Wang, K Lee, Q Deng, TY So, WH Chiu, B Zhou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "A chest X-ray radiology report describes abnormal findings not only from X-ray obtained at current examination, but also findings on disease progression or change in device placement with reference to the X-ray from previous examination. Majority \u2026"}, {"title": "Check for updates Knowledge Graph Embeddings for Multi-lingual Structured Representations of Radiology Reports", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DUeoEEQAAQBAJ%26oi%3Dfnd%26pg%3DPA84%26ots%3Dit5-T0X2jD%26sig%3Dm1PlMX6-Hfd8Rj-w9DA0AeytJ7Q", "details": "T van Sonsbeek, X Zhen, M Worring - \u2026 Labelling, and Imperfections: Third MICCAI Workshop \u2026", "abstract": "The way we analyse clinical texts has undergone major changes over the last years. The introduction of language models such as BERT led to adaptations for the (bio) medical domain like PubMedBERT and ClinicalBERT. These models rely on large \u2026"}]
