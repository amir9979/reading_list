[{"title": "MCM: Multi-layer Concept Map for Efficient Concept Learning from Masked Images", "link": "https://arxiv.org/pdf/2502.00266", "details": "Y Sun, L Mi, I Fujisawa, R Kanai - arXiv preprint arXiv:2502.00266, 2025", "abstract": "Masking strategies commonly employed in natural language processing are still underexplored in vision tasks such as concept learning, where conventional methods typically rely on full images. However, using masked images diversifies \u2026"}, {"title": "Multi-Grained Contrastive Learning for Text-supervised Open-vocabulary Semantic Segmentation", "link": "https://dl.acm.org/doi/pdf/10.1145/3711868", "details": "Y Liu, P Ge, G Wang, Q Liu, D Huang - ACM Transactions on Multimedia Computing \u2026, 2025", "abstract": "Learning open-vocabulary semantic segmentation (OVSS) from text supervision has recently received increasing attention for its promising potential in real-world applications. However, only with image-level supervision, it struggles to achieve \u2026"}, {"title": "RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning", "link": "https://arxiv.org/pdf/2502.00848", "details": "Y Lyu, X Zheng, L Jiang, Y Yan, X Zou, H Zhou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent text-to-image generative models, eg, Stable Diffusion V3 and Flux, have achieved notable progress. However, these models are strongly restricted to their limited knowledge, aka, their own fixed parameters, that are trained with closed \u2026"}, {"title": "Harmonic Loss Trains Interpretable AI Models", "link": "https://arxiv.org/pdf/2502.01628", "details": "DD Baek, Z Liu, R Tyagi, M Tegmark - arXiv preprint arXiv:2502.01628, 2025", "abstract": "In this paper, we introduce** harmonic loss** as an alternative to the standard cross- entropy loss for training neural networks and large language models (LLMs). Harmonic loss enables improved interpretability and faster convergence, owing to its \u2026"}, {"title": "Denoising Score Matching with Random Features: Insights on Diffusion Models from Precise Learning Curves", "link": "https://arxiv.org/pdf/2502.00336", "details": "AJ George, R Veiga, N Macris - arXiv preprint arXiv:2502.00336, 2025", "abstract": "We derive asymptotically precise expressions for test and train errors of denoising score matching (DSM) in generative diffusion models. The score function is parameterized by random features neural networks, with the target distribution being \u2026"}, {"title": "Pre-Trained Vision-Language Model Selection and Reuse for Downstream Tasks", "link": "https://arxiv.org/pdf/2501.18271%3F", "details": "HZ Tan, Z Zhou, LZ Guo, YF Li - arXiv preprint arXiv:2501.18271, 2025", "abstract": "Pre-trained Vision-Language Models (VLMs) are becoming increasingly popular across various visual tasks, and several open-sourced VLM variants have been released. However, selecting the best-performing pre-trained VLM for a specific \u2026"}, {"title": "DebiasPI: Inference-time Debiasing by Prompt Iteration of a Text-to-Image Generative Model", "link": "https://arxiv.org/pdf/2501.18642", "details": "S Bonna, YC Huang, E Novozhilova, S Paik, Z Shan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Ethical intervention prompting has emerged as a tool to counter demographic biases of text-to-image generative AI models. Existing solutions either require to retrain the model or struggle to generate images that reflect desired distributions on gender and \u2026"}, {"title": "Interpretability of AI race detection model in medical imaging with saliency methods", "link": "https://www.sciencedirect.com/science/article/pii/S2001037025000066", "details": "S Konate, L Lebrat, R Santa Cruz, JW Gichoya, B Price\u2026 - Computational and \u2026, 2025", "abstract": "Deep neural networks (DNNs) are powerful tools for classifying images. Using these convolutional models for medical images is challenging due to their complexity and large number of parameters, making it hard to find clinically meaningful explanations \u2026"}]
