[{"title": "KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models", "link": "https://arxiv.org/pdf/2408.03297", "details": "R Zhang, Y Xu, Y Xiao, R Zhu, X Jiang, X Chu, J Zhao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks \u2026"}, {"title": "Evaluating Language Models for Efficient Code Generation", "link": "https://arxiv.org/pdf/2408.06450", "details": "J Liu, S Xie, J Wang, Y Wei, Y Ding, L Zhang - arXiv preprint arXiv:2408.06450, 2024", "abstract": "We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. Traditional coding benchmarks often fail to provide reliable insights into code \u2026"}, {"title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Trustworthy Response Generation in Chinese", "link": "https://dl.acm.org/doi/pdf/10.1145/3686807", "details": "H Wang, S Zhao, Z Qiang, Z Li, C Liu, N Xi, Y Du, B Qin\u2026 - ACM Transactions on \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to \u2026"}, {"title": "In2Core: Leveraging Influence Functions for Coreset Selection in Instruction Finetuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.03560", "details": "AS Joaquin, B Wang, Z Liu, N Asher, B Lim, P Muller\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite advancements, fine-tuning Large Language Models (LLMs) remains costly due to the extensive parameter count and substantial data requirements for model generalization. Accessibility to computing resources remains a barrier for the open \u2026"}, {"title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.06663", "details": "K Sun, M Dredze - arXiv preprint arXiv:2408.06663, 2024", "abstract": "The development of large language models leads to the formation of a pre-train-then- align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream \u2026"}, {"title": "SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models", "link": "https://arxiv.org/pdf/2408.02632", "details": "M Diao, R Li, S Liu, G Liao, J Wang, X Cai, W Xu - arXiv preprint arXiv:2408.02632, 2024", "abstract": "As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to \u2026"}, {"title": "EXAONE 3.0 7.8 B Instruction Tuned Language Model", "link": "https://arxiv.org/pdf/2408.03541", "details": "LG Research, S An, K Bae, E Choi, SJ Choi, Y Choi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce EXAONE 3.0 instruction-tuned language model, the first open model in the family of Large Language Models (LLMs) developed by LG AI Research. Among different model sizes, we publicly release the 7.8 B instruction-tuned model to \u2026"}, {"title": "Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?", "link": "https://arxiv.org/pdf/2408.02651", "details": "MB Karkevandi, N Vishwamitra, P Najafirad - arXiv preprint arXiv:2408.02651, 2024", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in natural language tasks, but their safety and morality remain contentious due to their training on internet text corpora. To address these concerns, alignment techniques \u2026"}, {"title": "Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models", "link": "https://arxiv.org/pdf/2408.02416", "details": "Z Liang, H Hu, Q Ye, Y Xiao, H Li - arXiv preprint arXiv:2408.02416, 2024", "abstract": "The drastic increase of large language models'(LLMs) parameters has led to a new research direction of fine-tuning-free downstream customization by prompts, ie, task descriptions. While these prompt-based services (eg OpenAI's GPTs) play an \u2026"}]
