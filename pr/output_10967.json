[{"title": "Single-View Graph Contrastive Learning with Soft Neighborhood Awareness", "link": "https://arxiv.org/pdf/2412.09261", "details": "Q Sun, C Chen, Z Qiao, X Zheng, K Wang - arXiv preprint arXiv:2412.09261, 2024", "abstract": "Most graph contrastive learning (GCL) methods heavily rely on cross-view contrast, thus facing several concomitant challenges, such as the complexity of designing effective augmentations, the potential for information loss between views, and \u2026"}, {"title": "Equivariant Representation Learning for Augmentation-based Self-Supervised Learning via Image Reconstruction", "link": "https://arxiv.org/pdf/2412.03314", "details": "Q Wang, K Krajsek, H Scharr - arXiv preprint arXiv:2412.03314, 2024", "abstract": "Augmentation-based self-supervised learning methods have shown remarkable success in self-supervised visual representation learning, excelling in learning invariant features but often neglecting equivariant ones. This limitation reduces the \u2026"}, {"title": "DebiasDiff: Debiasing Text-to-image Diffusion Models with Self-discovering Latent Attribute Directions", "link": "https://arxiv.org/pdf/2412.18810", "details": "Y Jiang, W Li, Y Zhang, M Cai, X Yue - arXiv preprint arXiv:2412.18810, 2024", "abstract": "While Diffusion Models (DM) exhibit remarkable performance across various image generative tasks, they nonetheless reflect the inherent bias presented in the training set. As DMs are now widely used in real-world applications, these biases could \u2026"}, {"title": "Normalizing Flows are Capable Generative Models", "link": "https://arxiv.org/pdf/2412.06329", "details": "S Zhai, R Zhang, P Nakkiran, D Berthelot, J Gu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Normalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density estimation and generative modeling tasks, but have received relatively little attention in recent years. In this \u2026"}, {"title": "Let the Rule Speak: Enhancing In-context Learning Debiasing with Interpretability", "link": "https://arxiv.org/pdf/2412.19018", "details": "R Lin, Y You - arXiv preprint arXiv:2412.19018, 2024", "abstract": "In-context learning, which allows large language models to perform diverse tasks with a few demonstrations, is found to have imbalanced per-class prediction accuracy on multi-class text classification. Although notable output correction \u2026"}, {"title": "Self-supervised disentangled representation learning of artistic style through Neural Style Transfer", "link": "https://andrewjohngilbert.github.io/aladinnst/assets/ALADIN-NST_Self-supervised_disentangled_representation_learning_of_artistic_Paper.pdf", "details": "D Ruta, GC Tarr\u00e9s, A Black, A Gilbert, J Collomosse", "abstract": "We present a new method for learning a fine-grained representation of visual style. Representation learning aims to discover individual salient features of a domain in a compact and descriptive form that strongly identifies the unique characteristics of that \u2026"}, {"title": "GUESS: Generative Uncertainty Ensemble for Self Supervision", "link": "https://arxiv.org/pdf/2412.02896", "details": "S Mohamadi, G Doretto, DA Adjeroh - arXiv preprint arXiv:2412.02896, 2024", "abstract": "Self-supervised learning (SSL) frameworks consist of pretext task, and loss function aiming to learn useful general features from unlabeled data. The basic idea of most SSL baselines revolves around enforcing the invariance to a variety of data \u2026"}, {"title": "GLL: A Differentiable Graph Learning Layer for Neural Networks", "link": "https://arxiv.org/pdf/2412.08016", "details": "J Brown, B Chen, H Hardiman-Mostow, J Calder\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Standard deep learning architectures used for classification generate label predictions with a projection head and softmax activation function. Although successful, these methods fail to leverage the relational information between \u2026"}, {"title": "COSMOS: Cross-Modality Self-Distillation for Vision Language Pre-training", "link": "https://arxiv.org/pdf/2412.01814%3F", "details": "S Kim, R Xiao, MI Georgescu, S Alaniz, Z Akata - arXiv preprint arXiv:2412.01814, 2024", "abstract": "Vision-Language Models (VLMs) trained with contrastive loss have achieved significant advancements in various vision and language tasks. However, the global nature of contrastive loss makes VLMs focus predominantly on foreground objects \u2026"}]
