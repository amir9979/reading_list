[{"title": "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "link": "https://aclanthology.org/2024.emnlp-main.852.pdf", "details": "L Chen, R Zheng, B Wang, S Jin, C Huang, J Ye\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract Reinforcement Learning from Human Feedback (RLHF) is a crucial approach to aligning language models with human values and intentions. A fundamental challenge in this method lies in ensuring that the reward model \u2026"}, {"title": "RedPajama: an Open Dataset for Training Large Language Models", "link": "https://arxiv.org/pdf/2411.12372", "details": "M Weber, D Fu, Q Anthony, Y Oren, S Adams\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top \u2026"}, {"title": "Measuring short-form factuality in large language models", "link": "https://arxiv.org/pdf/2411.04368", "details": "J Wei, N Karina, HW Chung, YJ Jiao, S Papay\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present SimpleQA, a benchmark that evaluates the ability of language models to answer short, fact-seeking questions. We prioritized two properties in designing this eval. First, SimpleQA is challenging, as it is adversarially collected against GPT-4 \u2026"}]
