[{"title": "DianGPT: Bridging Precision and Efficiency for a Domain-Specific Question-Answering System", "link": "https://www.computer.org/csdl/proceedings-article/hpsc/2025/966300a072/27HvuWNqGdO", "details": "X Chen, Y Jiang, H Pei, X Hei - 2025 IEEE 11th International Conference on High \u2026, 2025", "abstract": "\u2026 -chat) initially achieved a score of 3.087 for **LLM** **evaluation** and 0.137 for Rouge-L F1. After filtering the dataset and fine-tuning the model for 23 hours on an L20 48GB GPU, the performance improved to 3.273 for **LLM** **evaluation** and 0.164 for Rouge-L \u2026"}, {"title": "Supervisor's statement of a final thesis", "link": "https://dspace.cvut.cz/bitstream/handle/10467/123992/F8-BP-2025-Holub-Ondrej-ctufit-thesis.pdf%3Fsequence%3D-1", "details": "RA da Silva Alves, O Holub - Artificial Intelligence, 2025", "abstract": "This thesis presents a system for generating reflection questions in educational settings using large language models (LLMs). The system uses the Socratic method in a multi-round dialogue between two separate LLM instances each with its own \u2026"}, {"title": "Limitations of Scientific Articles and Navigated Future Directions with LLM and RAG", "link": "https://search.proquest.com/openview/9fa08cf90c8a42aeb232e7ad729c7ee1/1%3Fpq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy", "details": "I Al Azher - 2025", "abstract": "Traditional Topic Modeling approaches, as well as zero-shot, few-shot, and fine-tuned Large Language Models (LLMs), have struggled to generate topics alongside relevant text from diverse sources, particularly sections such as Limitations. This \u2026"}]
