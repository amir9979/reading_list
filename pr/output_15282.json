[{"title": "FUSION: Fully Integration of Vision-Language Representations for Deep Cross-Modal Understanding", "link": "https://arxiv.org/pdf/2504.09925", "details": "Z Liu, M Liu, J Chen, J Xu, B Cui, C He, W Zhang - arXiv preprint arXiv:2504.09925, 2025", "abstract": "We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our \u2026"}, {"title": "Enhancing Multi-task Learning Capability of Medical Generalist Foundation Model via Image-centric Multi-annotation Data", "link": "https://arxiv.org/pdf/2504.09967", "details": "X Zhu, F Mo, Z Zhang, J Wang, Y Shi, M Wu, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The emergence of medical generalist foundation models has revolutionized conventional task-specific model development paradigms, aiming to better handle multiple tasks through joint training on large-scale medical datasets. However, recent \u2026"}, {"title": "Large Language Models Meet Contrastive Learning: Zero-Shot Emotion Recognition Across Languages", "link": "https://arxiv.org/pdf/2503.21806%3F", "details": "H Zou, F Lv, D Zheng, ES Chng, D Rajan - arXiv preprint arXiv:2503.21806, 2025", "abstract": "Multilingual speech emotion recognition aims to estimate a speaker's emotional state using a contactless method across different languages. However, variability in voice characteristics and linguistic diversity poses significant challenges for zero-shot \u2026"}, {"title": "Foundation models for electronic health records: representation dynamics and transferability", "link": "https://arxiv.org/pdf/2504.10422", "details": "MC Burkhart, B Ramadan, Z Liao, K Chhikara\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Foundation models (FMs) trained on electronic health records (EHRs) have shown strong performance on a range of clinical prediction tasks. However, adapting these models to local health systems remains challenging due to limited data availability \u2026"}, {"title": "Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families", "link": "https://arxiv.org/pdf/2504.10340", "details": "S Noroozizadeh, S Kumar, JC Weiss - arXiv preprint arXiv:2504.10340, 2025", "abstract": "Clinical case reports encode rich, temporal patient trajectories that are often underexploited by traditional machine learning methods relying on structured data. In this work, we introduce the forecasting problem from textual time series, where \u2026"}, {"title": "Probing the Symbolic Logical Reasoning Ability of Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3729238", "details": "J Ji, Z Li, S Xu, W Hua, J Tan, H Gong, Y Zhang - ACM Transactions on Intelligent Systems \u2026", "abstract": "Large Language Models (LLMs) have achieved significant successes in various research domains by learning the relationship between words. However, while these models are capable of making predictions and inferences based on the learned \u2026"}]
