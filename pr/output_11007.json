[{"title": "Low-Rank Adaptation with Task-Relevant Feature Enhancement for Fine-tuning Language Models", "link": "https://arxiv.org/pdf/2412.09827", "details": "C Li, C Ding, K Luan, X Di - arXiv preprint arXiv:2412.09827, 2024", "abstract": "Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. LoRA is one of the most widely used methods, which assumes that the optimization process is essentially low \u2026"}, {"title": "FastVLM: Efficient Vision Encoding for Vision Language Models", "link": "https://arxiv.org/pdf/2412.13303", "details": "PKA Vasu, F Faghri, CL Li, C Koc, N True, A Antony\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high \u2026"}, {"title": "Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models", "link": "https://arxiv.org/pdf/2412.14133", "details": "I Cohen, D Gottesman, M Geva, R Giryes - arXiv preprint arXiv:2412.14133, 2024", "abstract": "Vision-language models (VLMs) excel at extracting and reasoning about information from images. Yet, their capacity to leverage internal knowledge about specific entities remains underexplored. This work investigates the disparity in model performance \u2026"}, {"title": "HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.08378", "details": "S Zhu, W Dong, J Song, Y Guo, B Zheng - arXiv preprint arXiv:2412.08378, 2024", "abstract": "Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into \u2026"}, {"title": "Unsupervised Video Summarization via Iterative Training and Simplified GAN", "link": "https://openaccess.thecvf.com/content/ACCV2024/papers/Li_Unsupervised_Video_Summarization_via_Iterative_Training_and_Simplified_GAN_ACCV_2024_paper.pdf", "details": "H Li, D Klabjan, J Utke - Proceedings of the Asian Conference on Computer \u2026, 2024", "abstract": "This paper introduces a new, unsupervised method for auto-matic video summarization using ideas from generative adversarial net-works but eliminating the discriminator, having a simple loss function, and separating training of different parts \u2026"}, {"title": "HandsOnVLM: Vision-Language Models for Hand-Object Interaction Prediction", "link": "https://arxiv.org/pdf/2412.13187", "details": "C Bao, J Xu, X Wang, A Gupta, H Bharadhwaj - arXiv preprint arXiv:2412.13187, 2024", "abstract": "How can we predict future interaction trajectories of human hands in a scene given high-level colloquial task specifications in the form of natural language? In this paper, we extend the classic hand trajectory prediction task to two tasks involving \u2026"}, {"title": "Espresso: High Compression For Rich Extraction From Videos for Your Vision-Language Model", "link": "https://arxiv.org/pdf/2412.04729", "details": "KP Yu, A Dave, R Ambrus, J Mercat - arXiv preprint arXiv:2412.04729, 2024", "abstract": "Most of the current vision-language models (VLMs) for videos struggle to understand videos longer than a few seconds. This is primarily due to the fact that they do not scale to utilizing a large number of frames. In order to address this limitation, we \u2026"}, {"title": "Do language models understand time?", "link": "https://arxiv.org/pdf/2412.13845", "details": "X Ding, L Wang - arXiv preprint arXiv:2412.13845, 2024", "abstract": "Large language models (LLMs) have revolutionized video-based computer vision applications, including action recognition, anomaly detection, and video summarization. Videos inherently pose unique challenges, combining spatial \u2026"}, {"title": "SAT: Spatial Aptitude Training for Multimodal Language Models", "link": "https://arxiv.org/pdf/2412.07755", "details": "A Ray, J Duan, R Tan, D Bashkirova, R Hendrix\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Spatial perception is a fundamental component of intelligence. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only test for static spatial reasoning, such as categorizing the relative \u2026"}]
