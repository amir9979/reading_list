[{"title": "Tuning Language Models by Mixture-of-Depths Ensemble", "link": "https://arxiv.org/pdf/2410.13077", "details": "H Luo, L Specia - arXiv preprint arXiv:2410.13077, 2024", "abstract": "Transformer-based Large Language Models (LLMs) traditionally rely on final-layer loss for training and final-layer representations for predictions, potentially overlooking the predictive power embedded in intermediate layers. Surprisingly, we \u2026"}, {"title": "MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.17637", "details": "Z Liu, Y Zang, X Dong, P Zhang, Y Cao, H Duan, C He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs. This is typically achieved by using labeled datasets of chosen/rejected pairs and employing \u2026"}, {"title": "Are Expert-Level Language Models Expert-Level Annotators?", "link": "https://arxiv.org/pdf/2410.03254", "details": "YM Tseng, WL Chen, CC Chen, HH Chen - arXiv preprint arXiv:2410.03254, 2024", "abstract": "Data annotation refers to the labeling or tagging of textual data with relevant information. A large body of works have reported positive results on leveraging LLMs as an alternative to human annotators. However, existing studies focus on classic \u2026"}, {"title": "Manual Verbalizer Enrichment for Few-Shot Text Classification", "link": "https://arxiv.org/pdf/2410.06173", "details": "QA Nguyen, N Tomeh, M Lebbah, T Charnois, H Azzag\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the continuous development of pre-trained language models, prompt-based training becomes a well-adopted paradigm that drastically improves the exploitation of models for many natural language processing tasks. Prompting also shows great \u2026"}, {"title": "Verifiable, Debuggable, and Repairable Commonsense Logical Reasoning via LLM-based Theory Resolution", "link": "https://ssanner.github.io/papers/emnlp24_llmtres.pdf", "details": "A Toroghi, W Guo, A Pesaranghader, S Sanner - The 2024 Conference on Empirical \u2026, 2024", "abstract": "Abstract Recent advances in Large Language Models (LLM) have led to substantial interest in their application to commonsense reasoning tasks. Despite their potential, LLMs are susceptible to reasoning errors and hallucinations that may be harmful in \u2026"}, {"title": "NSSC: a neuro-symbolic AI system for enhancing accuracy of named entity recognition and linking from oncologic clinical notes", "link": "https://link.springer.com/article/10.1007/s11517-024-03227-4", "details": "\u00c1 Garc\u00eda-Barrag\u00e1n, A Sakor, ME Vidal, E Menasalvas\u2026 - Medical & Biological \u2026, 2024", "abstract": "Accurate recognition and linking of oncologic entities in clinical notes is essential for extracting insights across cancer research, patient care, clinical decision-making, and treatment optimization. We present the Neuro-Symbolic System for Cancer \u2026"}, {"title": "Large language models enabled multiagent ensemble method for efficient EHR data labeling", "link": "https://arxiv.org/pdf/2410.16543", "details": "J Huang, K Nezafati, I Villanueva-Miranda, Z Gu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study introduces a novel multiagent ensemble method powered by LLMs to address a key challenge in ML-data labeling, particularly in large-scale EHR datasets. Manual labeling of such datasets requires domain expertise and is labor \u2026"}, {"title": "LiP-LLM: Integrating Linear Programming and dependency graph with Large Language Models for multi-robot task planning", "link": "https://arxiv.org/pdf/2410.21040", "details": "K Obata, T Aoki, T Horii, T Taniguchi, T Nagai - arXiv preprint arXiv:2410.21040, 2024", "abstract": "This study proposes LiP-LLM: integrating linear programming and dependency graph with large language models (LLMs) for multi-robot task planning. In order for multiple robots to perform tasks more efficiently, it is necessary to manage the \u2026"}, {"title": "Retrieval In Decoder benefits generative models for explainable complex question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007573", "details": "J Feng, Q Wang, H Qiu, L Liu - Neural Networks, 2024", "abstract": "Abstract Large-scale Language Models (LLMs) utilizing the Chain-of-Thought prompting demonstrate exceptional performance in a variety of tasks. However, the persistence of factual hallucinations remains a significant challenge in practical \u2026"}]
