[{"title": "Continual Multimodal Contrastive Learning", "link": "https://arxiv.org/pdf/2503.14963", "details": "X Liu, X Xia, SK Ng, TS Chua - arXiv preprint arXiv:2503.14963, 2025", "abstract": "Multimodal contrastive learning (MCL) advances in aligning different modalities and generating multimodal representations in a joint space. By leveraging contrastive learning across diverse modalities, large-scale multimodal data enhances \u2026"}, {"title": "Semantic Retrieval Augmented Contrastive Learning for Sequential Recommendation", "link": "https://arxiv.org/pdf/2503.04162", "details": "Z Cui, Y Weng, X Tang, X Zhang, D Liu, S Li, P Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Sequential recommendation aims to model user preferences based on historical behavior sequences, which is crucial for various online platforms. Data sparsity remains a significant challenge in this area as most users have limited interactions \u2026"}, {"title": "Medical foundation large language models for comprehensive text analysis and beyond", "link": "https://www.nature.com/articles/s41746-025-01533-1", "details": "Q Xie, Q Chen, A Chen, C Peng, Y Hu, F Lin, X Peng\u2026 - npj Digital Medicine, 2025", "abstract": "Recent advancements in large language models (LLMs) show significant potential in medical applications but are hindered by limited specialized medical knowledge. We present Me-LLaMA, a family of open-source medical LLMs integrating extensive \u2026"}, {"title": "Enhancing diagnostic capability with multi-agents conversational large language models", "link": "https://www.nature.com/articles/s41746-025-01550-0", "details": "X Chen, H Yi, M You, WZ Liu, L Wang, H Li, X Zhang\u2026 - npj Digital Medicine, 2025", "abstract": "Abstract Large Language Models (LLMs) show promise in healthcare tasks but face challenges in complex medical scenarios. We developed a Multi-Agent Conversation (MAC) framework for disease diagnosis, inspired by clinical Multi-Disciplinary Team \u2026"}, {"title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures", "link": "https://arxiv.org/pdf/2503.01496", "details": "D Lan, W Sun, J Hu, J Du, Y Cheng - arXiv preprint arXiv:2503.01496, 2025", "abstract": "Transformers with linear recurrent modeling offer linear-time training and constant- memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky \u2026"}, {"title": "Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding", "link": "https://arxiv.org/pdf/2503.01422", "details": "Y Wang, P Zhang, S Huang, B Yang, Z Zhang, F Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Test-time scaling improves large language model performance by adding extra compute during decoding. Best-of-N (BoN) sampling serves as a common scaling technique, broadening the search space for finding better solutions from the model \u2026"}]
