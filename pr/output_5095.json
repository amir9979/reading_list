[{"title": "Skip\\n: A simple method to reduce hallucination in large vision-language models", "link": "https://oar.a-star.edu.sg/storage/d/d66g61dkp7/nn-bias-in-visual-language-models-camera-ready.pdf", "details": "Z Han, Z Bai, H Mei, Q Xu, C Zhang, MZ Shou - arXiv preprint arXiv:2402.01345, 2024", "abstract": "Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination \u2026"}, {"title": "On Pre-training of Multimodal Language Models Customized for Chart Understanding", "link": "https://arxiv.org/pdf/2407.14506", "details": "WC Fan, YC Chen, M Liu, L Yuan, L Sigal - arXiv preprint arXiv:2407.14506, 2024", "abstract": "Recent studies customizing Multimodal Large Language Models (MLLMs) for domain-specific tasks have yielded promising results, especially in the field of scientific chart comprehension. These studies generally utilize visual instruction \u2026"}, {"title": "EPFL-MAKE at \u201cDischarge Me!\u201d: An LLM System for Automatically Generating Discharge Summaries of Clinical Electronic Health Record", "link": "https://aclanthology.org/2024.bionlp-1.61.pdf", "details": "H Wu, P Boulenger, A Faure, B C\u00e9spedes, F Boukil\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "This paper presents our contribution to the Streamlining Discharge Documentation shared task organized as part of the ACL'24 workshop. We propose MEDISCHARGE (Meditron-7B Based Medical Summary Generation System for Discharge Me), an \u2026"}, {"title": "Enhancing Healthcare through Large Language Models: A Study on Medical Question Answering", "link": "https://arxiv.org/pdf/2408.04138", "details": "H Yu, C Yu, Z Wang, D Zou, H Qin - arXiv preprint arXiv:2408.04138, 2024", "abstract": "In recent years, the application of Large Language Models (LLMs) in healthcare has shown significant promise in improving the accessibility and dissemination of medical knowledge. This paper presents a detailed study of various LLMs trained on \u2026"}, {"title": "KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models", "link": "https://arxiv.org/pdf/2408.03297", "details": "R Zhang, Y Xu, Y Xiao, R Zhu, X Jiang, X Chu, J Zhao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks \u2026"}, {"title": "Using Large Language Models to Evaluate Biomedical Query-Focused Summarisation", "link": "https://aclanthology.org/2024.bionlp-1.18.pdf", "details": "H Hijazi, D Molla, V Nguyen, S Karimi - Proceedings of the 23rd Workshop on \u2026, 2024", "abstract": "Biomedical question-answering systems remain popular for biomedical experts interacting with the literature to answer their medical questions. However, these systems are difficult to evaluate in the absence of costly human experts. Therefore \u2026"}, {"title": "3D Question Answering with Scene Graph Reasoning", "link": "https://openreview.net/pdf%3Fid%3DqmyPQ3XbBZ", "details": "Z Wu, H Li, G Chen, Z Yu, X Gu, Y Wang - ACM Multimedia 2024", "abstract": "3DQA has gained considerable attention due to its enhanced spatial understanding capabilities compared to image-based VQA. However, existing 3DQA methods have explicitly focused on integrating text and color-coded point cloud features, thereby \u2026"}, {"title": "Patch-Level Training for Large Language Models", "link": "https://arxiv.org/pdf/2407.12665", "details": "C Shao, F Meng, J Zhou - arXiv preprint arXiv:2407.12665, 2024", "abstract": "As Large Language Models (LLMs) achieve remarkable progress in language understanding and generation, their training efficiency has become a critical concern. Traditionally, LLMs are trained to predict the next token in a sequence \u2026"}, {"title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2407.11681", "details": "H Cheng, M Zhang, JQ Shi - arXiv preprint arXiv:2407.11681, 2024", "abstract": "As Large Language Models (LLMs) grow dramatically in size, there is an increasing trend in compressing and speeding up these models. Previous studies have highlighted the usefulness of gradients for importance scoring in neural network \u2026"}]
