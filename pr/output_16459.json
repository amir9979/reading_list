[{"title": "Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records", "link": "https://arxiv.org/pdf/2505.09435", "details": "Y He, Y Zhu, P Fu, R Yang, T Chen, Z Wang, Q Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Pre-training on image-text colonoscopy records offers substantial potential for improving endoscopic image analysis, but faces challenges including non- informative background images, complex medical terminology, and ambiguous multi \u2026"}, {"title": "A Particle Swarm Optimization\u2010Based Approach Coupled With Large Language Models for Prompt Optimization", "link": "https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.70049", "details": "PC Hsieh, WP Lee - Expert Systems, 2025", "abstract": "Large language models (LLMs) have been developing rapidly to attract significant attention these days. These models have exhibited remarkable abilities in achieving various natural language processing (NLP) tasks, but the performance depends \u2026"}, {"title": "PLHF: Prompt Optimization with Few-Shot Human Feedback", "link": "https://arxiv.org/pdf/2505.07886", "details": "CP Yang, K Zheng, SD Lin - arXiv preprint arXiv:2505.07886, 2025", "abstract": "Automatic prompt optimization frameworks are developed to obtain suitable prompts for large language models (LLMs) with respect to desired output quality metrics. Although existing approaches can handle conventional tasks such as fixed-solution \u2026"}, {"title": "Pushing the boundary on Natural Language Inference", "link": "https://arxiv.org/pdf/2504.18376%3F", "details": "P Miralles-Gonz\u00e1lez, J Huertas-Tato, A Mart\u00edn\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Natural Language Inference (NLI) is a central task in natural language understanding with applications in fact-checking, question answering, and information retrieval. Despite its importance, current NLI systems heavily rely on \u2026"}]
