[{"title": "Self-Supervised Learning-Based Time Series Classification via Hierarchical Sparse Convolutional Masked-Autoencoder", "link": "https://ieeexplore.ieee.org/iel8/8782710/9006934/10614789.pdf", "details": "T Yu, K Xu, X Wang, B Ding, D Feng - IEEE Open Journal of Signal Processing, 2024", "abstract": "In recent years, the use of time series analysis has become widespread, prompting researchers to explore methods to improve classification. Time series self-supervised learning has emerged as a significant area of study, aiming to uncover patterns in \u2026"}, {"title": "Towards Robustness Prompt Tuning with Fully Test-Time Adaptation for CLIP's Zero-Shot Generalization", "link": "https://openreview.net/pdf%3Fid%3DBVFAVis7ui", "details": "R Wang, H Zuo, Z Fang, J Lu - ACM Multimedia 2024", "abstract": "In the field of Vision-Language Models (VLM), the Contrastive Language-Image Pretraining (CLIP) model has yielded outstanding performance on many downstream tasks through prompt tuning. By integrating image and text representations, CLIP \u2026"}, {"title": "Multi-view Self-Supervised Contrastive Learning for Multivariate Time Series", "link": "https://openreview.net/pdf%3Fid%3DBgjLy3chju", "details": "Y Wu, X Meng, Y He, J Zhang, H Zhang, Y Dong, D Lu - ACM Multimedia 2024", "abstract": "Learning semantic-rich representations from unlabeled time series data with intricate dynamics is a notable challenge. Traditional contrastive learning techniques predominantly focus on segment-level augmentations through time slicing, a practice \u2026"}, {"title": "Sparse transformer with local and seasonal adaptation for multivariate time series forecasting", "link": "https://www.nature.com/articles/s41598-024-66886-1", "details": "Y Zhang, R Wu, SM Dascalu, FC Harris Jr - Scientific Reports, 2024", "abstract": "Transformers have achieved remarkable performance in multivariate time series (MTS) forecasting due to their capability to capture long-term dependencies. However, the canonical attention mechanism has two key limitations:(1) its quadratic \u2026"}, {"title": "Learning by imitating the classics: Mitigating class imbalance in federated learning via simulated centralized learning", "link": "https://www.sciencedirect.com/science/article/pii/S0957417424016221", "details": "G Zhu, X Liu, J Niu, Y Wei, S Tang, J Zhang - Expert Systems with Applications, 2024", "abstract": "Federated learning (FL) is a distributed machine learning framework in which multiple clients update their local models in parallel and then aggregate them to generate a global model. However, when local data on different clients are class \u2026"}, {"title": "Extending Binary Neural Networks to Bayesian Neural Networks with Probabilistic Interpretation of Binary Weights", "link": "https://www.jstage.jst.go.jp/article/transinf/E107.D/8/E107.D_2023LOP0009/_pdf", "details": "T SAITO, K ANDO, T ASAI - IEICE TRANSACTIONS on Information and Systems, 2024", "abstract": "Neural networks (NNs) fail to perform well or make excessive predictions when predicting out-of-distribution or unseen datasets. In contrast, Bayesian neural networks (BNNs) can quantify the uncertainty of their inference to solve this problem \u2026"}, {"title": "FMamba: Mamba based on Fast-attention for Multivariate Time-series Forecasting", "link": "https://arxiv.org/pdf/2407.14814", "details": "S Ma, Y Kang, P Bai, YB Zhao - arXiv preprint arXiv:2407.14814, 2024", "abstract": "In multivariate time-series forecasting (MTSF), extracting the temporal correlations of the input sequences is crucial. While popular Transformer-based predictive models can perform well, their quadratic computational complexity results in inefficiency and \u2026"}]
