[{"title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2411.14432", "details": "Y Dong, Z Liu, HL Sun, J Yang, W Hu, Y Rao, Z Liu - arXiv preprint arXiv:2411.14432, 2024", "abstract": "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high \u2026"}, {"title": "ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?", "link": "https://arxiv.org/pdf/2412.02368", "details": "L Zhang, S Eger, Y Cheng, W Zhai, J Belouadi, C Leiter\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal large language models (LLMs) have demonstrated impressive capabilities in generating high-quality images from textual instructions. However, their performance in generating scientific images--a critical application for \u2026"}, {"title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models", "link": "https://arxiv.org/pdf/2411.19477", "details": "Y Chen, X Pan, Y Li, B Ding, J Zhou - arXiv preprint arXiv:2411.19477, 2024", "abstract": "We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates $ N $ candidate solutions, and then chooses the \u2026"}, {"title": "Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training", "link": "https://arxiv.org/pdf/2411.14318%3F", "details": "Z Luo, X Zhang, X Liu, H Li, Y Gong, C Qi, P Cheng - arXiv preprint arXiv:2411.14318, 2024", "abstract": "It is well-known that a diverse corpus is critical for training large language models, which are typically constructed from a mixture of various domains. In general, previous efforts resort to sampling training data from different domains with static \u2026"}, {"title": "On Domain-Specific Post-Training for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2411.19930%3F", "details": "D Cheng, S Huang, Z Zhu, X Zhang, WX Zhao, Z Luan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent years have witnessed the rapid development of general multimodal large language models (MLLMs). However, adapting general MLLMs to specific domains, such as scientific fields and industrial applications, remains less explored. This paper \u2026"}, {"title": "MIND: Effective Incorrect Assignment Detection through a Multi-Modal Structure-Enhanced Language Model", "link": "https://arxiv.org/pdf/2412.03930", "details": "Y Pang, B Chen, F Zhang, Y Rao, J Tang - arXiv preprint arXiv:2412.03930, 2024", "abstract": "The rapid growth of academic publications has exacerbated the issue of author name ambiguity in online digital libraries. Despite advances in name disambiguation algorithms, cumulative errors continue to undermine the reliability of academic \u2026"}, {"title": "Training Agents with Weakly Supervised Feedback from Large Language Models", "link": "https://arxiv.org/pdf/2411.19547", "details": "D Gong, P Lu, Z Wang, M Zhou, X He - arXiv preprint arXiv:2411.19547, 2024", "abstract": "Large Language Models (LLMs) offer a promising basis for creating agents that can tackle complex tasks through iterative environmental interaction. Existing methods either require these agents to mimic expert-provided trajectories or rely on definitive \u2026"}, {"title": "Can Large Language Models Serve as Evaluators for Code Summarization?", "link": "https://arxiv.org/pdf/2412.01333", "details": "Y Wu, Y Wan, Z Chu, W Zhao, Y Liu, H Zhang, X Shi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Code summarization facilitates program comprehension and software maintenance by converting code snippets into natural-language descriptions. Over the years, numerous methods have been developed for this task, but a key challenge remains \u2026"}, {"title": "The performance of the LSTM-based code generated by Large Language Models (LLMs) in forecasting time series data", "link": "https://www.sciencedirect.com/science/article/pii/S2949719124000682", "details": "S Gopali, S Siami-Namini, F Abri, AS Namin - Natural Language Processing Journal, 2024", "abstract": "Generative AI, and in particular Large Language Models (LLMs), have gained substantial momentum due to their wide applications in various disciplines. While the use of these game changing technologies in generating textual information has \u2026"}]
