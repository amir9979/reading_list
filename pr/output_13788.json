[{"title": "Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding", "link": "https://arxiv.org/pdf/2502.09906", "details": "TD Truong, HQ Nguyen, XB Nguyen, A Dowling, X Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual \u2026"}, {"title": "Hierarchical Vision\u2013Language Pre-Training with Freezing Strategy for Multi-Level Semantic Alignment", "link": "https://www.mdpi.com/2079-9292/14/4/816", "details": "H Xie, Y Qin, S Ding - Electronics, 2025", "abstract": "Vision\u2013language pre-training (VLP) faces challenges in aligning hierarchical textual semantics (words/phrases/sentences) with multi-scale visual features (objects/relations/global context). We propose a hierarchical VLP model (HieVLP) \u2026"}, {"title": "Multilingual Language Model Pretraining using Machine-translated Data", "link": "https://arxiv.org/pdf/2502.13252", "details": "J Wang, Y Lu, M Weber, M Ryabinin, D Adelani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-resource languages such as English, enables the pretraining of high-quality large language models (LLMs). The same can not be said for most other languages as LLMs still underperform for non-English languages, likely due to a gap in the \u2026"}, {"title": "AIDE: Agentically Improve Visual Language Model with Domain Experts", "link": "https://arxiv.org/pdf/2502.09051", "details": "MC Chiu, F Liu, K Sapra, A Tao, Y Jacoob, X Ma, Z Yu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The enhancement of Visual Language Models (VLMs) has traditionally relied on knowledge distillation from larger, more capable models. This dependence creates a fundamental bottleneck for improving state-of-the-art systems, particularly when no \u2026"}, {"title": "Enhancing Chest X-ray Classification through Knowledge Injection in Cross-Modality Learning", "link": "https://arxiv.org/pdf/2502.13447", "details": "Y Yan, B Yue, Q Li, M Huang, J Chen, Z Lan - ICASSP 2025-2025 IEEE International \u2026, 2025", "abstract": "The integration of artificial intelligence in medical imaging has shown tremendous potential, yet the relationship between pre-trained knowledge and performance in cross-modality learning remains unclear. This study investigates how explicitly \u2026"}, {"title": "ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language Interactions for Computer-Aided Diagnosis", "link": "https://arxiv.org/pdf/2502.10620", "details": "X Li, X Hou, Z Huang, Y Gan - arXiv preprint arXiv:2502.10620, 2025", "abstract": "Recent advancements in large language models (LLMs) have demonstrated extraordinary comprehension capabilities with remarkable breakthroughs on various vision-language tasks. However, the application of LLMs in generating reliable \u2026"}, {"title": "Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models", "link": "https://arxiv.org/pdf/2502.11559", "details": "Y Xu, C Fu, L Xiong, S Yang, W Wang - arXiv preprint arXiv:2502.11559, 2025", "abstract": "Pre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias \u2026"}, {"title": "Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models", "link": "https://arxiv.org/pdf/2502.08130", "details": "S Gupta, Y Nandwani, A Yehudai, D Khandelwal\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task \u2026"}, {"title": "Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation", "link": "https://arxiv.org/pdf/2502.15040", "details": "YW Chu, K Zhang, C Malon, MR Min - arXiv preprint arXiv:2502.15040, 2025", "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive performance in vision and text tasks. However, hallucination remains a major challenge, especially in fields like healthcare where details are critical. In this work, we show \u2026"}]
