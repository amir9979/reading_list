'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Achieving> 97% on GSM8K: Deeply Understanding the Prob'
[{"title": "General Purpose Verification for Chain of Thought Prompting", "link": "https://arxiv.org/pdf/2405.00204", "details": "R Vacareanu, A Pratik, E Spiliopoulou, Z Qi, G Paolini\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Many of the recent capabilities demonstrated by Large Language Models (LLMs) arise primarily from their ability to exploit contextual information. In this paper, we explore ways to improve reasoning capabilities of LLMs through (1) exploration of \u2026"}, {"title": "HFT: Half Fine-Tuning for Large Language Models", "link": "https://arxiv.org/pdf/2404.18466", "details": "T Hui, Z Zhang, S Wang, W Xu, Y Sun, H Wu - arXiv preprint arXiv:2404.18466, 2024", "abstract": "Large language models (LLMs) with one or more fine-tuning phases have become a necessary step to unlock various capabilities, enabling LLMs to follow natural language instructions or align with human preferences. However, it carries the risk of \u2026"}, {"title": "A comparison of chain-of-thought reasoning strategies across datasets and models", "link": "https://peerj.com/articles/cs-1999/", "details": "K Hebenstreit, R Praas, LP Kiesewetter, M Samwald - PeerJ Computer Science, 2024", "abstract": "Emergent chain-of-thought (CoT) reasoning capabilities promise to improve the performance and explainability of large language models (LLMs). However, uncertainties remain about how reasoning strategies formulated for previous model \u2026"}, {"title": "Soft Preference Optimization: Aligning Language Models to Expert Distributions", "link": "https://arxiv.org/pdf/2405.00747", "details": "A Sharifnassab, S Ghiassian, S Salehkaleybar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We propose Soft Preference Optimization (SPO), a method for aligning generative models, such as Large Language Models (LLMs), with human preferences, without the need for a reward model. SPO optimizes model outputs directly over a preference \u2026"}, {"title": "Retrieval Augmented Generation for Domain-specific Question Answering", "link": "https://arxiv.org/pdf/2404.14760", "details": "S Sharma, DS Yoon, F Dernoncourt, D Sultania\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Question answering (QA) has become an important application in the advanced development of large language models. General pre-trained large language models for question-answering are not trained to properly understand the knowledge or \u2026"}, {"title": "GRAMMAR: Grounded and Modular Evaluation of Domain-Specific Retrieval-Augmented Language Models", "link": "https://arxiv.org/pdf/2404.19232", "details": "X Li, M Liu, S Gao - arXiv preprint arXiv:2404.19232, 2024", "abstract": "Retrieval-augmented Generation (RAG) systems have been actively studied and deployed across various industries to query on domain-specific knowledge base. However, evaluating these systems presents unique challenges due to the scarcity of \u2026"}, {"title": "Fleet of Agents: Coordinated Problem Solving with Large Language Models using Genetic Particle Filtering", "link": "https://arxiv.org/pdf/2405.06691", "details": "A Arora, L Klein, N Potatmitis, R Aydin, C Gulcehre\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have significantly evolved, moving from simple output generation to complex reasoning and from stand-alone usage to being embedded into broader frameworks. In this paper, we introduce Fleet of Agents \u2026"}, {"title": "OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework", "link": "https://arxiv.org/pdf/2404.14619%3Ftrk%3Dpublic_post_comment-text", "details": "S Mehta, MH Sekhavat, Q Cao, M Horton, Y Jin, C Sun\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we \u2026"}, {"title": "A Human-Computer Collaborative Tool for Training a Single Large Language Model Agent into a Network through Few Examples", "link": "https://arxiv.org/pdf/2404.15974", "details": "L Pan, Y Li, C Yu, Y Shi - arXiv preprint arXiv:2404.15974, 2024", "abstract": "The capabilities of a single large language model (LLM) agent for solving a complex task are limited. Connecting multiple LLM agents to a network can effectively improve overall performance. However, building an LLM agent network (LAN) requires a \u2026"}]
