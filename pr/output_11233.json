[{"title": "Implementing Trust in Non-Small Cell Lung Cancer Diagnosis with a Conformalized Uncertainty-Aware AI Framework in Whole-Slide Images", "link": "https://www.medrxiv.org/content/10.1101/2024.12.27.24319715.full.pdf", "details": "X Zhang, T Wang, C Yan, F Najdawi, K Zhou, Y Ma\u2026 - medRxiv, 2024", "abstract": "Ensuring trustworthiness is fundamental to the development of artificial intelligence (AI) that is considered societally responsible, particularly in cancer diagnostics, where a misdiagnosis can have dire consequences. Current digital pathology AI \u2026"}, {"title": "Real-world Deployment and Evaluation of PErioperative AI CHatbot (PEACH)--a Large Language Model Chatbot for Perioperative Medicine", "link": "https://arxiv.org/pdf/2412.18096", "details": "YH Ke, L Jin, K Elangovan, BWX Ong, CY Oh, J Sim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) are emerging as powerful tools in healthcare, particularly for complex, domain-specific tasks. This study describes the development and evaluation of the PErioperative AI CHatbot (PEACH), a secure LLM-based \u2026"}, {"title": "Language Models as Continuous Self-Evolving Data Engineers", "link": "https://arxiv.org/pdf/2412.15151%3F", "details": "P Wang, M Wang, Z Ma, X Yang, S Feng, D Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert \u2026"}, {"title": "Knowledge Graph as Pre-training Corpus for Structural Reasoning via Multi-hop Linearization", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10817607.pdf", "details": "W Kim, H Jung, W Kim - IEEE Access, 2024", "abstract": "Large language models have demonstrated exceptional performance across various natural language processing tasks. However, their reliance on unstructured text corpora for pre-training limits their effectiveness in tasks requiring structured \u2026"}, {"title": "Template Matters: Understanding the Role of Instruction Templates in Multimodal Language Model Evaluation and Training", "link": "https://arxiv.org/pdf/2412.08307", "details": "S Wang, L Song, J Zhang, R Shimizu, A Luo, L Yao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Current multimodal language models (MLMs) evaluation and training approaches overlook the influence of instruction format, presenting an elephant-in-the-room problem. Previous research deals with this problem by manually crafting instructions \u2026"}]
