[{"title": "Regularized Training with Generated Datasets for Name-Only Transfer of Vision-Language Models", "link": "https://arxiv.org/pdf/2406.05432", "details": "M Park, S Park, J Yun, J Choo - Gen, 2024", "abstract": "Recent advancements in text-to-image generation have inspired researchers to generate datasets tailored for perception models using generative models, which prove particularly valuable in scenarios where real-world data is limited. In this study \u2026"}, {"title": "Measuring Retrieval Complexity in Question Answering Systems", "link": "https://arxiv.org/pdf/2406.03592", "details": "M Gabburo, NP Jedema, S Garg, LFR Ribeiro\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we investigate which questions are challenging for retrieval-based Question Answering (QA). We (i) propose retrieval complexity (RC), a novel metric conditioned on the completeness of retrieved documents, which measures the \u2026"}, {"title": "Federated Contrastive Learning for Personalized Semantic Communication", "link": "https://arxiv.org/pdf/2406.09182", "details": "Y Wang, W Ni, W Yi, X Xu, P Zhang, A Nallanathan - arXiv preprint arXiv:2406.09182, 2024", "abstract": "In this letter, we design a federated contrastive learning (FedCL) framework aimed at supporting personalized semantic communication. Our FedCL enables collaborative training of local semantic encoders across multiple clients and a global semantic \u2026"}, {"title": "Adaptive Hyper-graph Aggregation for Modality-Agnostic Federated Learning", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_Adaptive_Hyper-graph_Aggregation_for_Modality-Agnostic_Federated_Learning_CVPR_2024_paper.pdf", "details": "F Qi, S Li - Proceedings of the IEEE/CVF Conference on Computer \u2026, 2024", "abstract": "Abstract In Federated Learning (FL) the issue of statistical data heterogeneity has been a significant challenge to the field's ongoing development. This problem is further exacerbated when clients' data vary in modalities. In response to these issues \u2026"}, {"title": "FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2405.18218", "details": "Y Zhang, Y Li, X Wang, Q Shen, B Plank, B Bischl\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Overparametrized transformer networks are the state-of-the-art architecture for Large Language Models (LLMs). However, such models contain billions of parameters making large compute a necessity, while raising environmental concerns. To \u2026"}, {"title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via Adaptive Heads Fusion", "link": "https://arxiv.org/pdf/2406.06567", "details": "Y Chen, L Zhang, J Shang, Z Zhang, T Liu, S Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) with billions of parameters demonstrate impressive performance. However, the widely used Multi-Head Attention (MHA) in LLMs incurs substantial computational and memory costs during inference. While some efforts \u2026"}, {"title": "LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation", "link": "https://arxiv.org/pdf/2406.02876", "details": "Z Sun, Y Liu, F Meng, J Xu, Y Chen, J Zhou - arXiv preprint arXiv:2406.02876, 2024", "abstract": "Multilingual neural machine translation models generally distinguish translation directions by the language tag (LT) in front of the source or target sentences. However, current LT strategies cannot indicate the desired target language as \u2026"}, {"title": "Source-Free Unsupervised Domain Adaptation for Question Answering via Prompt-Assisted Self-learning", "link": "https://aclanthology.org/2024.findings-naacl.44/", "details": "M Yin, B Wang, C Ling - Findings of the Association for Computational \u2026, 2024", "abstract": "This work addresses source-free domain adaptation (SFDA) for Question Answering (QA), wherein a model trained on a source domain is adapted to unlabeled target domains without additional source data. Existing SFDA methods only focus on the \u2026"}, {"title": "MedExQA: Medical Question Answering Benchmark with Multiple Explanations", "link": "https://arxiv.org/pdf/2406.06331", "details": "Y Kim, J Wu, Y Abdulle, H Wu - arXiv e-prints, 2024", "abstract": "This paper introduces MedExQA, a novel benchmark in medical question-answering, to evaluate large language models'(LLMs) understanding of medical knowledge through explanations. By constructing datasets across five distinct medical \u2026"}]
