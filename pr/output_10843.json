[{"title": "segWCD: A new segmentation-based weak supervision neural network for building change detection", "link": "https://link.springer.com/article/10.1007/s10489-024-06003-x", "details": "Y Wu, X Zhang, X Zhao, Y Sun, T Li - Applied Intelligence, 2025", "abstract": "Manual annotation of changes in high-resolution remote sensing images is labor- intensive and limits advancements in change detection. We introduce the Segmentation-based Weakly Supervised Change Detection (segWCD) framework to \u2026"}, {"title": "Reusing routine electronic health record data for nationwide COVID-19 surveillance in nursing homes: barriers, facilitators, and lessons learned", "link": "https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-024-02818-3", "details": "Y Wieland-Jorna, RA Verheij, AL Francke, R Coppen\u2026 - BMC Medical Informatics \u2026, 2024", "abstract": "At the beginning of the COVID-19 pandemic in 2020, little was known about the spread of COVID-19 in Dutch nursing homes while older people were particularly at risk of severe symptoms. Therefore, attempts were made to develop a nationwide \u2026"}, {"title": "Mastering Board Games by External and Internal Planning with Language Models", "link": "https://arxiv.org/pdf/2412.12119%3F", "details": "J Schultz, J Adamek, M Jusup, M Lanctot, M Kaisers\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While large language models perform well on a range of complex tasks (eg, text generation, question answering, summarization), robust multi-step planning and reasoning remains a considerable challenge for them. In this paper we show that \u2026"}, {"title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models", "link": "https://arxiv.org/pdf/2412.03548", "details": "M Bigverdi, Z Luo, CY Hsieh, E Shen, D Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal language models (MLMs) still face challenges in fundamental visual perception tasks where specialized models excel. Tasks requiring reasoning about 3D structures benefit from depth estimation, and reasoning about 2D object \u2026"}, {"title": "Training Agents with Weakly Supervised Feedback from Large Language Models", "link": "https://arxiv.org/pdf/2411.19547", "details": "D Gong, P Lu, Z Wang, M Zhou, X He - arXiv preprint arXiv:2411.19547, 2024", "abstract": "Large Language Models (LLMs) offer a promising basis for creating agents that can tackle complex tasks through iterative environmental interaction. Existing methods either require these agents to mimic expert-provided trajectories or rely on definitive \u2026"}, {"title": "SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models", "link": "https://arxiv.org/pdf/2412.11605", "details": "J Cheng, X Liu, C Wang, X Gu, Y Lu, D Zhang, Y Dong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Instruction-following is a fundamental capability of language models, requiring the model to recognize even the most subtle requirements in the instructions and accurately reflect them in its output. Such an ability is well-suited for and often \u2026"}, {"title": "M $^ 3$ PC: Test-time Model Predictive Control for Pretrained Masked Trajectory Model", "link": "https://arxiv.org/pdf/2412.05675", "details": "K Wen, Y Hu, Y Mu, L Ke - arXiv preprint arXiv:2412.05675, 2024", "abstract": "Recent work in Offline Reinforcement Learning (RL) has shown that a unified Transformer trained under a masked auto-encoding objective can effectively capture the relationships between different modalities (eg, states, actions, rewards) within \u2026"}]
