'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Self-Refine Instruction-Tuning for Aligning Reasoning '
[{"title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2404.05567", "details": "B Pan, Y Shen, H Liu, M Mishra, G Zhang, A Oliva\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) language models can reduce computational costs by 2- 4$\\times $ compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios. However, MoE models generally \u2026"}, {"title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models", "link": "https://arxiv.org/pdf/2405.01535", "details": "S Kim, J Suk, S Longpre, BY Lin, J Shin, S Welleck\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source \u2026"}, {"title": "Infusing internalized knowledge of language models into hybrid prompts for knowledgeable dialogue generation", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124005082", "details": "J Bai, Z Yan, S Zhang, J Yang, H Guo, Z Li - Knowledge-Based Systems, 2024", "abstract": "Existing knowledge-grounded dialogue (KGD) systems access the knowledge from an external knowledge base, then generate the context-coherent response accordingly. However, the knowledge access capability is constrained to the scale of \u2026"}, {"title": "FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations", "link": "https://arxiv.org/pdf/2404.06619", "details": "J Dwivedi-Yu, R Dwivedi, T Schick - arXiv preprint arXiv:2404.06619, 2024", "abstract": "The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes \u2026"}, {"title": "Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning", "link": "https://arxiv.org/pdf/2404.03891", "details": "G Choi, H Ahn - arXiv preprint arXiv:2404.03891, 2024", "abstract": "In robotics, the use of Large Language Models (LLMs) is becoming prevalent, especially for understanding human commands. In particular, LLMs are utilized as domain-agnostic task planners for high-level human commands. LLMs are capable \u2026"}, {"title": "EventLens: Leveraging Event-Aware Pretraining and Cross-modal Linking Enhances Visual Commonsense Reasoning", "link": "https://arxiv.org/pdf/2404.13847", "details": "M Ma, Z Yu, Y Ma, G Li - arXiv preprint arXiv:2404.13847, 2024", "abstract": "Visual Commonsense Reasoning (VCR) is a cognitive task, challenging models to answer visual questions requiring human commonsense, and to provide rationales explaining why the answers are correct. With emergence of Large Language Models \u2026"}, {"title": "On the Surprising Efficacy of Distillation as an Alternative to Pre-Training Small Models", "link": "https://arxiv.org/pdf/2404.03263", "details": "S Farhat, D Chen - arXiv preprint arXiv:2404.03263, 2024", "abstract": "In this paper, we propose that small models may not need to absorb the cost of pre- training to reap its benefits. Instead, they can capitalize on the astonishing results achieved by modern, enormous models to a surprising degree. We observe that \u2026"}, {"title": "Measuring Cross-lingual Transfer in Bytes", "link": "https://arxiv.org/pdf/2404.08191", "details": "LR de Souza, TS Almeida, R Lotufo, R Nogueira - arXiv preprint arXiv:2404.08191, 2024", "abstract": "Multilingual pretraining has been a successful solution to the challenges posed by the lack of resources for languages. These models can transfer knowledge to target languages with minimal or no examples. Recent research suggests that monolingual \u2026"}]
