[{"title": "LLM-IE: a python package for biomedical generative information extraction with large language models", "link": "https://academic.oup.com/jamiaopen/article/8/2/ooaf012/8071856", "details": "E Hsu, K Roberts - JAMIA open, 2025", "abstract": "Objectives Despite the recent adoption of large language models (LLMs) for biomedical information extraction (IE), challenges in prompt engineering and algorithms persist, with no dedicated software available. To address this, we \u2026"}, {"title": "Patient and clinician acceptability of automated extraction of social drivers of health from clinical notes in primary care", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocaf046/8078597", "details": "SJ Xie, C Spice, P Wedgeworth, R Langevin\u2026 - Journal of the American \u2026, 2025", "abstract": "Abstract Objective Artificial Intelligence (AI)-based approaches for extracting Social Drivers of Health (SDoH) from clinical notes offer healthcare systems an efficient way to identify patients' social needs, yet we know little about the acceptability of this \u2026"}, {"title": "How Do Multilingual Language Models Remember Facts?", "link": "https://infoscience.epfl.ch/bitstreams/08d1b43b-16d0-45e5-b76f-7b979ac12950/download", "details": "C Fierro, N Foroutan, D Elliott, A S\u00f8gaard - arXiv preprint arXiv:2410.14387, 2025", "abstract": "Abstract Large Language Models (LLMs) store and retrieve vast amounts of factual knowledge acquired during pre-training. Prior research has localized and identified mechanisms behind knowledge recall; however, it has only focused on English \u2026"}, {"title": "Language Models as Knowledge Bases?", "link": "https://scispace.com/pdf/language-models-as-knowledge-bases-2bs7zawas9.pdf", "details": "FPT Rockt\u00e4schel, P Lewis, A Bakhtin, Y Wu\u2026", "abstract": "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the \u2026"}, {"title": "Mmrag: Multi-mode retrieval-augmented generation with large language models for biomedical in-context learning", "link": "https://arxiv.org/pdf/2502.15954", "details": "Z Zhan, J Wang, S Zhou, J Deng, R Zhang - arXiv preprint arXiv:2502.15954, 2025", "abstract": "Objective: To optimize in-context learning in biomedical natural language processing by improving example selection. Methods: We introduce a novel multi-mode retrieval- augmented generation (MMRAG) framework, which integrates four retrieval \u2026"}, {"title": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning", "link": "https://arxiv.org/pdf/2502.18439", "details": "C Park, S Han, X Guo, A Ozdaglar, K Zhang, JK Kim - arXiv preprint arXiv:2502.18439, 2025", "abstract": "Leveraging multiple large language models (LLMs) to build collaborative multi- agentic workflows has demonstrated significant potential. However, most previous studies focus on prompting the out-of-the-box LLMs, relying on their innate capability \u2026"}, {"title": "Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI", "link": "https://arxiv.org/pdf/2502.16691", "details": "E Noh, J Baek - arXiv preprint arXiv:2502.16691, 2025", "abstract": "Recent research has increasingly focused on training large language models (LLMs) using federated learning, known as FedLLM. However, responsible AI (RAI), which aims to ensure safe responses, remains underexplored in the context of FedLLM. In \u2026"}, {"title": "MedTransTab: Advancing Medical Cross-Table Tabular Data Generation", "link": "https://dl.acm.org/doi/abs/10.1145/3701551.3703501", "details": "Y Chen, Q Guo, S You, Z Li - Proceedings of the Eighteenth ACM International \u2026, 2025", "abstract": "In medical research, clinical trials are pivotal. While prospective clinical research provides a systematic approach to collecting patient data, it grapples with challenges like long durations, increased costs, and most crucially, data scarcity. To address \u2026"}]
