[{"title": "LONG EXPOSURE: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity", "link": "https://www.computer.org/csdl/proceedings-article/sc/2024/529100b176/21HUWjOHh3q", "details": "T Wang, K Li, Z Hao, D Bai, J Ren, Y Zhang, T Cao\u2026 - 2024 SC24: International \u2026, 2024", "abstract": "The adaptation of pre-trained large language models (LLMs) to diverse downstream tasks via fine-tuning is critical for numerous applications. However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques presents significant challenges in \u2026"}, {"title": "The Radiance of Neural Fields: Democratizing Photorealistic and Dynamic Robotic Simulation", "link": "https://arxiv.org/pdf/2411.16940", "details": "G Nuthall, R Bowden, O Mendez - arXiv preprint arXiv:2411.16940, 2024", "abstract": "As robots increasingly coexist with humans, they must navigate complex, dynamic environments rich in visual information and implicit social dynamics, like when to yield or move through crowds. Addressing these challenges requires significant \u2026"}, {"title": "Multifaceted Natural Language Processing Task\u2013Based Evaluation of Bidirectional Encoder Representations From Transformers Models for Bilingual (Korean and \u2026", "link": "https://medinform.jmir.org/2024/1/e52897/", "details": "K Kim, S Park, J Min, S Park, JY Kim, J Eun, K Jung\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background: The bidirectional encoder representations from transformers (BERT) model has attracted considerable attention in clinical applications, such as patient classification and disease prediction. However, current studies have typically \u2026"}, {"title": "Label correlated contrastive learning for medical report generation", "link": "https://www.sciencedirect.com/science/article/pii/S0169260724004759", "details": "X Liu, J Xin, B Dai, Q Shen, Z Huang, Z Wang - Computer Methods and Programs in \u2026, 2024", "abstract": "Abstract Background and Objective: Automatic generation of medical reports reduces both the burden on radiologists and the possibility of errors due to the inexperience of radiologists. The model that utilizes attention mechanism and contrastive learning \u2026"}, {"title": "ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for Large Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3DPyTf2jj0SH", "details": "S Liu, K Ying, H Zhang, Y Yang, Y Lin, T Zhang, C Li\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Multi-turn visual conversation is an important ability of real-world AI assistants. However, the related evaluation benchmark is missed. This paper presents ConvBench, a multi-turn conversation benchmark with hierarchical capabilities \u2026"}, {"title": "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.1249.pdf", "details": "X Lin, M Li, R Zemel, H Ji, SF Chang - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "Recently, enabling pretrained language models (PLMs) to perform zero-shot crossmodal tasks such as video question answering has been extensively studied. A popular approach is to learn a projection network that projects visual features into the \u2026"}, {"title": "What's in the Image? A Deep-Dive into the Vision of Vision Language Models", "link": "https://arxiv.org/abs/2411.17491", "details": "O Kaduri, S Bagon, T Dekel - arXiv preprint arXiv:2411.17491, 2024", "abstract": "Vision-Language Models (VLMs) have recently demonstrated remarkable capabilities in comprehending complex visual content. However, the mechanisms underlying how VLMs process visual information remain largely unexplored. In this \u2026"}, {"title": "MWVOS: Mask-Free Weakly Supervised Video Object Segmentation via promptable foundation model", "link": "https://www.sciencedirect.com/science/article/pii/S0031320324008513", "details": "Z Zhang, S Zhang, Z Dai, Z Dong, S Zhu - Pattern Recognition, 2024", "abstract": "The current state-of-the-art techniques for video object segmentation necessitate extensive training on video datasets with mask annotations, thereby constraining their ability to transfer zero-shot learning to new image distributions and tasks \u2026"}, {"title": "Classification Done Right for Vision-Language Pre-Training", "link": "https://openreview.net/pdf%3Fid%3DHd2EOwKItm", "details": "Z Huang, Q Ye, B Kang, J Feng, H Fan - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data. Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised \u2026"}]
