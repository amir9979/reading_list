[{"title": "Analysing the Residual Stream of Language Models Under Knowledge Conflicts", "link": "https://arxiv.org/pdf/2410.16090", "details": "Y Zhao, X Du, G Hong, AP Gema, A Devoto, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model \u2026"}, {"title": "Not All Votes Count! Programs as Verifiers Improve Self-Consistency of Language Models for Math Reasoning", "link": "https://arxiv.org/pdf/2410.12608%3F", "details": "VYH Toh, D Ghosal, S Poria - arXiv preprint arXiv:2410.12608, 2024", "abstract": "Large language models (LLMs) have shown increasing proficiency in solving mathematical reasoning problems. However, many current open-source LLMs often still make calculation and semantic understanding errors in their intermediate \u2026"}, {"title": "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models", "link": "https://arxiv.org/pdf/2410.17389", "details": "M Lin, S Shi, Y Guo, B Chalaki, V Tadiparthi, EM Pari\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The correct specification of reward models is a well-known challenge in reinforcement learning. Hand-crafted reward functions often lead to inefficient or suboptimal policies and may not be aligned with user values. Reinforcement \u2026"}, {"title": "Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in Language Models", "link": "https://openreview.net/pdf%3Fid%3DHOLs697aIx", "details": "A Dutta, YC Hsiao - NeurIPS 2024 Workshop on Open-World Agents", "abstract": "We propose a novel in-context learning algorithm for building autonomous decision- making language agents. The language agent continuously attempts to solve the same task by reasoning, acting, observing and then self-correcting each time the task \u2026"}, {"title": "CLR-Bench: Evaluating Large Language Models in College-level Reasoning", "link": "https://arxiv.org/pdf/2410.17558", "details": "J Dong, Z Hong, Y Bei, F Huang, X Wang, X Huang - arXiv preprint arXiv:2410.17558, 2024", "abstract": "Large language models (LLMs) have demonstrated their remarkable performance across various language understanding tasks. While emerging benchmarks have been proposed to evaluate LLMs in various domains such as mathematics and \u2026"}, {"title": "LLaVA-KD: A Framework of Distilling Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2410.16236", "details": "Y Cai, J Zhang, H He, X He, A Tong, Z Gan, C Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The success of Large Language Models (LLM) has led researchers to explore Multimodal Large Language Models (MLLM) for unified visual and linguistic understanding. However, the increasing model size and computational complexity of \u2026"}, {"title": "Table-LLM-Specialist: Language Model Specialists for Tables using Iterative Generator-Validator Fine-tuning", "link": "https://arxiv.org/pdf/2410.12164", "details": "J Xing, Y He, M Zhou, H Dong, S Han, D Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we propose Table-LLM-Specialist, or Table-Specialist for short, as a new self-trained fine-tuning paradigm specifically designed for table tasks. Our insight is that for each table task, there often exist two dual versions of the same task, one \u2026"}, {"title": "Negative-Prompt-driven Alignment for Generative Language Model", "link": "https://arxiv.org/pdf/2410.12194", "details": "S Qiao, N Xv, B Liu, X Geng - arXiv preprint arXiv:2410.12194, 2024", "abstract": "Large language models have achieved remarkable capabilities, but aligning their outputs with human values and preferences remains a significant challenge. Existing alignment methods primarily focus on positive examples while overlooking the \u2026"}, {"title": "Cross-lingual Transfer of Reward Models in Multilingual Alignment", "link": "https://arxiv.org/pdf/2410.18027", "details": "J Hong, N Lee, R Mart\u00ednez-Casta\u00f1o, C Rodr\u00edguez\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Reinforcement learning with human feedback (RLHF) is shown to largely benefit from precise reward models (RMs). However, recent studies in reward modeling schemes are skewed towards English, limiting the applicability of RLHF in \u2026"}]
