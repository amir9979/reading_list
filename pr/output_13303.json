[{"title": "AnyEdit: Edit Any Knowledge Encoded in Language Models", "link": "https://arxiv.org/pdf/2502.05628", "details": "H Jiang, J Fang, N Zhang, G Ma, M Wan, X Wang, X He\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) often produce incorrect or outdated information, necessitating efficient and precise knowledge updates. Current model editing methods, however, struggle with long-form knowledge in diverse formats, such as \u2026"}, {"title": "Self-Consistency of the Internal Reward Models Improves Self-Rewarding Language Models", "link": "https://arxiv.org/pdf/2502.08922", "details": "X Zhou, Y Guo, R Ma, T Gui, Q Zhang, X Huang - arXiv preprint arXiv:2502.08922, 2025", "abstract": "Aligning Large Language Models (LLMs) with human preferences is crucial for their deployment in real-world applications. Recent advancements in Self-Rewarding Language Models suggest that an LLM can use its internal reward models (such as \u2026"}, {"title": "BARE: Combining Base and Instruction-Tuned Language Models for Better Synthetic Data Generation", "link": "https://arxiv.org/pdf/2502.01697", "details": "A Zhu, P Asawa, JQ Davis, L Chen, I Stoica\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. A common assumption about synthetic data is that sampling from instruct-tuned models \u2026"}, {"title": "CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport Alignment for Language Models with Different Tokenizers", "link": "https://arxiv.org/pdf/2502.16806", "details": "AD Le, T Vu, NL Hai, NTN Diep, LN Van, T Le\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) achieve state-of-the-art performance across various NLP tasks but face deployment challenges due to high computational costs and memory constraints. Knowledge distillation (KD) is a promising solution, transferring \u2026"}, {"title": "Cost-Efficient Domain-Adaptive Pretraining of Language Models for Optoelectronics Applications", "link": "https://pubs.acs.org/doi/full/10.1021/acs.jcim.4c02029", "details": "D Huang, JM Cole - Journal of Chemical Information and Modeling, 2025", "abstract": "Pretrained language models have demonstrated strong capability and versatility in natural language processing (NLP) tasks, and they have important applications in optoelectronics research, such as data mining and topic modeling. Many language \u2026"}, {"title": "Revealing the Pragmatic Dilemma for Moral Reasoning Acquisition in Language Models", "link": "https://arxiv.org/pdf/2502.16600", "details": "G Liu, L Jiang, X Zhang, KM Johnson - arXiv preprint arXiv:2502.16600, 2025", "abstract": "Ensuring that Large Language Models (LLMs) return just responses which adhere to societal values is crucial for their broader application. Prior research has shown that LLMs often fail to perform satisfactorily on tasks requiring moral cognizance, such as \u2026"}, {"title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models", "link": "https://arxiv.org/pdf/2502.17387", "details": "A Albalak, D Phung, N Lile, R Rafailov, K Gandhi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written \u2026"}, {"title": "Detecting Benchmark Contamination Through Watermarking", "link": "https://arxiv.org/pdf/2502.17259", "details": "T Sander, P Fernandez, S Mahloujifar, A Durmus\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking \u2026"}, {"title": "Position: Contextual Integrity Washing for Language Models", "link": "https://arxiv.org/pdf/2501.19173", "details": "Y Shvartzshnaider, V Duddu - arXiv preprint arXiv:2501.19173, 2025", "abstract": "Machine learning community is discovering Contextual Integrity (CI) as a useful framework to assess the privacy implications of large language models (LLMs). This is an encouraging development. The CI theory emphasizes sharing information in \u2026"}]
