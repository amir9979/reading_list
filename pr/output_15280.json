[{"title": "HalluShift: Measuring Distribution Shifts towards Hallucination Detection in LLMs", "link": "https://arxiv.org/pdf/2504.09482", "details": "S Dasgupta, S Nath, A Basu, P Shamsolmoali, S Das - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have recently garnered widespread attention due to their adeptness at generating innovative responses to the given prompts across a multitude of domains. However, LLMs often suffer from the inherent limitation of \u2026"}, {"title": "DICE: A Framework for Dimensional and Contextual Evaluation of Language Models", "link": "https://arxiv.org/pdf/2504.10359", "details": "A Shrivastava, PA Aoyagui - arXiv preprint arXiv:2504.10359, 2025", "abstract": "Language models (LMs) are increasingly being integrated into a wide range of applications, yet the modern evaluation paradigm does not sufficiently reflect how they are actually being used. Current evaluations rely on benchmarks that often lack \u2026"}, {"title": "Where do Large Vision-Language Models Look at when Answering Questions?", "link": "https://arxiv.org/pdf/2503.13891", "details": "X Xing, CW Kuo, L Fuxin, Y Niu, F Chen, M Li, Y Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision-Language Models (LVLMs) have shown promising performance in vision-language understanding and reasoning tasks. However, their visual understanding behaviors remain underexplored. A fundamental question arises: to \u2026"}, {"title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing", "link": "https://arxiv.org/pdf/2503.15815%3F", "details": "VA Dasu, V Gupta, S Tizpaz-Niari, G Tan - arXiv preprint arXiv:2503.15815, 2025", "abstract": "This paper explores pruning attention heads as a post-processing bias mitigation method for large language models (LLMs). Modern AI systems such as LLMs are expanding into sensitive social contexts where fairness concerns become especially \u2026"}, {"title": "Learning to Erase Private Knowledge from Multi-Documents for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2504.09910", "details": "Y Wang, H Zhang, L Pang, Y Tong, B Guo, H Zheng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-Augmented Generation (RAG) is a promising technique for applying LLMs to proprietary domains. However, retrieved documents may contain sensitive knowledge, posing risks of privacy leakage in generative results. Thus, effectively \u2026"}, {"title": "Large Language Model Empowered Recommendation Meets All-domain Continual Pre-Training", "link": "https://arxiv.org/pdf/2504.08949", "details": "H Ma, Y Ma, R Xie, L Meng, J Shen, X Sun, Z Kang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent research efforts have investigated how to integrate Large Language Models (LLMs) into recommendation, capitalizing on their semantic comprehension and open-world knowledge for user behavior understanding. These approaches \u2026"}, {"title": "Enhancing NER Performance in Low-Resource Pakistani Languages using Cross-Lingual Data Augmentation", "link": "https://arxiv.org/pdf/2504.08792", "details": "T Ehsan, T Solorio - arXiv preprint arXiv:2504.08792, 2025", "abstract": "Named Entity Recognition (NER), a fundamental task in Natural Language Processing (NLP), has shown significant advancements for high-resource languages. However, due to a lack of annotated datasets and limited representation \u2026"}, {"title": "ELOQUENT CLEF Shared Tasks for Evaluation of Generative Language Model Quality, 2025 Edition", "link": "https://dl.acm.org/doi/abs/10.1007/978-3-031-88720-8_56", "details": "J Karlgren, E Artemova, O Bojar, V Mikhailov\u2026 - European Conference on \u2026, 2025", "abstract": "The ELOQUENT lab for evaluation of generative language model quality and usefulness addresses high-level quality criteria through a set of open-ended shared tasks implemented, where possible, to leverage the ability of systems built on \u2026"}, {"title": "ExpertRAG: Efficient RAG with Mixture of Experts--Optimizing Context Retrieval for Adaptive LLM Responses", "link": "https://arxiv.org/pdf/2504.08744", "details": "E Gumaan - arXiv preprint arXiv:2504.08744, 2025", "abstract": "ExpertRAG is a novel theoretical framework that integrates Mixture-of-Experts (MoE) architectures with Retrieval Augmented Generation (RAG) to advance the efficiency and accuracy of knowledge-intensive language modeling. We propose a dynamic \u2026"}]
