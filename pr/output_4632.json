[{"title": "Graph-based Unsupervised Disentangled Representation Learning via Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2407.18999", "details": "B Xie, Q Chen, Y Wang, Z Zhang, X Jin, W Zeng - arXiv preprint arXiv:2407.18999, 2024", "abstract": "Disentangled representation learning (DRL) aims to identify and decompose underlying factors behind observations, thus facilitating data perception and generation. However, current DRL approaches often rely on the unrealistic \u2026"}, {"title": "Hard Prompts Made Interpretable: Sparse Entropy Regularization for Prompt Tuning with RL", "link": "https://arxiv.org/pdf/2407.14733", "details": "Y Choi, S Bae, S Ban, M Jeong, C Zhang, L Song\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the advent of foundation models, prompt tuning has positioned itself as an important technique for directing model behaviors and eliciting desired responses. Prompt tuning regards selecting appropriate keywords included into the input \u2026"}, {"title": "Physics-Informed Geometric Operators to Support Surrogate, Dimension Reduction and Generative Models for Engineering Design", "link": "https://arxiv.org/pdf/2407.07611", "details": "S Khan, Z Masood, M Usama, K Kostas, P Kaklis - arXiv preprint arXiv:2407.07611, 2024", "abstract": "In this work, we propose a set of physics-informed geometric operators (GOs) to enrich the geometric data provided for training surrogate/discriminative models, dimension reduction, and generative models, typically employed for performance \u2026"}, {"title": "Unsupervised Dense Prediction using Differentiable Normalized Cuts", "link": "https://csyanbin.github.io/papers/ECCV2024_DiffNCuts.pdf", "details": "Y Liu, S Gould", "abstract": "With the emergent attentive property of self-supervised Vision Transformer (ViT), Normalized Cuts (NCut) has resurfaced as a powerful tool for unsupervised dense prediction. However, the pre-trained ViT backbone (eg, DINO) is frozen in existing \u2026"}, {"title": "Interdependence-Adaptive Mutual Information Maximization for Graph Contrastive Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10596072/", "details": "Q Sun, K Wang, W Zhang, P Cheng, X Lin - IEEE Transactions on Knowledge and \u2026, 2024", "abstract": "Despite remarkable advancements in graph contrastive learning techniques, the identification of interdependent relationships when maximizing cross-view mutual information remains a challenging issue, primarily due to the complexity of graph \u2026"}, {"title": "Vision-Language Generative Model for View-Specific Chest X-ray Generation", "link": "https://proceedings.mlr.press/v248/lee24a.html", "details": "H Lee, W Kim, JH Kim, T Kim, J Kim, L Sunwoo, E Choi - Conference on Health \u2026, 2024", "abstract": "Synthetic medical data generation has opened up new possibilities in the healthcare domain, offering a powerful tool for simulating clinical scenarios, enhancing diagnostic and treatment quality, gaining granular medical knowledge, and \u2026"}, {"title": "Revisiting the robustness of post-hoc interpretability methods", "link": "https://arxiv.org/pdf/2407.19683", "details": "J Wei, H Turb\u00e9, G Mengaldo - arXiv preprint arXiv:2407.19683, 2024", "abstract": "Post-hoc interpretability methods play a critical role in explainable artificial intelligence (XAI), as they pinpoint portions of data that a trained deep learning model deemed important to make a decision. However, different post-hoc \u2026"}, {"title": "Towards Efficient Large-Scale Language-3D Representation Learning", "link": "https://openreview.net/pdf%3Fid%3DGtHx6P7HP9", "details": "S Mo, X Xu, T Wang, A Torralba, S Li - Workshop on Efficient Systems for Foundation Models \u2026", "abstract": "Recent years have seen significant advancements in large-scale representation learning of 2D vision and language tasks. However, the efficacy of such cross-modal training on large-scale 3D objects with other modalities (such as text and images) \u2026"}]
