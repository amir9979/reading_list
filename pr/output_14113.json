[{"title": "Optimizing generative AI by backpropagating language model feedback", "link": "https://www.nature.com/articles/s41586-025-08661-4", "details": "M Yuksekgonul, F Bianchi, J Boen, S Liu, P Lu\u2026 - Nature, 2025", "abstract": "Recent breakthroughs in artificial intelligence (AI) are increasingly driven by systems orchestrating multiple large language models (LLMs) and other specialized tools, such as search engines and simulators. So far, these systems are primarily \u2026"}, {"title": "MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for Inpatient Pathways", "link": "https://arxiv.org/pdf/2503.13205", "details": "Z Chen, Z Peng, X Liang, C Wang, P Liang, L Zeng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Inpatient pathways demand complex clinical decision-making based on comprehensive patient information, posing critical challenges for clinicians. Despite advancements in large language models (LLMs) in medical applications, limited \u2026"}, {"title": "Diff-ZsVQA: Zero-shot Visual Question Answering with Frozen Large Language Models Using Diffusion Model", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425005731", "details": "Q Xu, J Li, Y Tian, L Zhou, F Zhang, R Huang - Expert Systems with Applications, 2025", "abstract": "Abstract Visual Question Answering (VQA) methods leveraging Large Language Models (LLMs) aim to enhance performance in few/zero-shot scenarios. While the results attained by these approaches were outstanding, there remains scope for \u2026"}, {"title": "Leveraging shared feature representation in cross-domain alignment of decision thresholds for electronic health records data.", "link": "https://openreview.net/pdf%3Fid%3DLe4uBO1jB5", "details": "E Gal, A Thakur, S Molaei, A Soltan, DA Clifton - ICLR 2025 Workshop on Deep Generative \u2026", "abstract": "The real-world deployment of clinical machine learning models requires adaptability to distributional shifts caused by variations in the patient population and data acquisition mechanisms. However, distributional shifts are known to significantly \u2026"}, {"title": "Towards better understanding of program-of-thought reasoning in cross-lingual and multilingual environments", "link": "https://arxiv.org/pdf/2502.17956", "details": "P Payoungkhamdee, P Tuchinda, J Baek\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multi-step reasoning is essential for large language models (LLMs), yet multilingual performance remains challenging. While Chain-of-Thought (CoT) prompting improves reasoning, it struggles with non-English languages due to the \u2026"}, {"title": "An astronomical question answering dataset for evaluating large language models", "link": "https://www.nature.com/articles/s41597-025-04613-9", "details": "J Li, F Zhao, P Chen, J Xie, X Zhang, H Li, M Chen\u2026 - Scientific Data, 2025", "abstract": "Large language models (LLMs) have recently demonstrated exceptional capabilities across a variety of linguistic tasks including question answering (QA). However, it remains challenging to assess their performance in astronomical QA due to the lack \u2026"}, {"title": "Understanding the Logical Capabilities of Large Language Models via Out-of-Context Representation Learning", "link": "https://arxiv.org/pdf/2503.10408", "details": "J Shaki, E La Malfa, M Wooldridge, S Kraus - arXiv preprint arXiv:2503.10408, 2025", "abstract": "We study the capabilities of Large Language Models (LLM) on binary relations, a ubiquitous concept in math employed in most reasoning, math and logic benchmarks. This work focuses on equality, inequality, and inclusion, along with the \u2026"}]
