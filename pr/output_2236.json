[{"title": "A fine-grained self-adapting prompt learning approach for few-shot learning with pre-trained language models", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124006026", "details": "X Chen, T Liu, P Fournier-Viger, B Zhang, G Long\u2026 - Knowledge-Based Systems, 2024", "abstract": "Pre-trained language models have demonstrated remarkable performance in few- shot learning through the emergence of \u201cprompt-based learning\u201d methods, where the performance of these tasks highly rely on the quality of prompts. Existing prompt \u2026"}, {"title": "AsCL: An Asymmetry-sensitive Contrastive Learning Method for Image-Text Retrieval with Cross-Modal Fusion", "link": "https://arxiv.org/pdf/2405.10029", "details": "Z Gong, C Mai, Y Huang - arXiv preprint arXiv:2405.10029, 2024", "abstract": "The image-text retrieval task aims to retrieve relevant information from a given image or text. The main challenge is to unify multimodal representation and distinguish fine- grained differences across modalities, thereby finding similar contents and filtering \u2026"}, {"title": "Prompt Tuning for Few-shot Relation Extraction via Modeling Global and Local Graphs", "link": "https://aclanthology.org/2024.lrec-main.1158.pdf", "details": "Z Zhang, Y Yang, B Chen - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Recently, prompt-tuning has achieved very significant results for few-shot tasks. The core idea of prompt-tuning is to insert prompt templates into the input, thus converting the classification task into a masked language modeling problem. However, for few \u2026"}]
