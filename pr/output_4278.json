[{"title": "On Pre-training of Multimodal Language Models Customized for Chart Understanding", "link": "https://arxiv.org/pdf/2407.14506", "details": "WC Fan, YC Chen, M Liu, L Yuan, L Sigal - arXiv preprint arXiv:2407.14506, 2024", "abstract": "Recent studies customizing Multimodal Large Language Models (MLLMs) for domain-specific tasks have yielded promising results, especially in the field of scientific chart comprehension. These studies generally utilize visual instruction \u2026"}, {"title": "TCohPrompt: task-coherent prompt-oriented fine-tuning for relation extraction", "link": "https://link.springer.com/article/10.1007/s40747-024-01563-4", "details": "J Long, Z Yin, C Liu, W Huang - Complex & Intelligent Systems, 2024", "abstract": "Prompt-tuning has emerged as a promising approach for improving the performance of classification tasks by converting them into masked language modeling problems through the insertion of text templates. Despite its considerable success, applying \u2026"}, {"title": "Large Language Models for Tabular Data: Progresses and Future Directions", "link": "https://dl.acm.org/doi/pdf/10.1145/3626772.3661384", "details": "H Dong, Z Wang - Proceedings of the 47th International ACM SIGIR \u2026, 2024", "abstract": "Tables contain a significant portion of the world's structured information. The ability to efficiently and accurately understand, process, reason about, analyze, and generate tabular data is critical for achieving Artificial General Intelligence (AGI) systems \u2026"}, {"title": "Large Language Models are Interpretable Learners", "link": "https://arxiv.org/pdf/2406.17224", "details": "R Wang, S Si, F Yu, D Wiesmann, CJ Hsieh, I Dhillon - arXiv preprint arXiv \u2026, 2024", "abstract": "The trade-off between expressiveness and interpretability remains a core challenge when building human-centric predictive models for classification and decision- making. While symbolic rules offer interpretability, they often lack expressiveness \u2026"}, {"title": "Source Code Summarization in the Era of Large Language Models", "link": "https://arxiv.org/pdf/2407.07959", "details": "W Sun, Y Miao, Y Li, H Zhang, C Fang, Y Liu, G Deng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To support software developers in understanding and maintaining programs, various automatic (source) code summarization techniques have been proposed to generate a concise natural language summary (ie, comment) for a given code snippet \u2026"}, {"title": "Entropy-Based Decoding for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2406.17519", "details": "Z Qiu, Z Ou, B Wu, J Li, A Liu, I King - arXiv preprint arXiv:2406.17519, 2024", "abstract": "Augmenting Large Language Models (LLMs) with retrieved external knowledge has proven effective for improving the factual accuracy of generated responses. Despite their success, retrieval-augmented LLMs still face the distractibility issue, where the \u2026"}, {"title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models", "link": "https://arxiv.org/pdf/2407.04693", "details": "Y Gu, Z Ji, W Zhang, C Lyu, D Lin, K Chen - arXiv preprint arXiv:2407.04693, 2024", "abstract": "Large language models (LLMs) exhibit hallucinations in long-form question- answering tasks across various domains and wide applications. Current hallucination detection and mitigation datasets are limited in domains and sizes \u2026"}]
