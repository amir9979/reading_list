[{"title": "THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2405.05256", "details": "P Kaul, Z Li, H Yang, Y Dukler, A Swaminathan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term\" Type I hallucinations\". Instead, they focus on \u2026"}, {"title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision", "link": "https://arxiv.org/pdf/2405.04086", "details": "Y Tong, S Wang, D Li, Y Wang, S Han, Z Lin, C Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations \u2026"}, {"title": "On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?", "link": "https://arxiv.org/pdf/2405.02266", "details": "M Zanella, IB Ayed - arXiv preprint arXiv:2405.02266, 2024", "abstract": "The development of large vision-language models, notably CLIP, has catalyzed research into effective adaptation techniques, with a particular focus on soft prompt tuning. Conjointly, test-time augmentation, which utilizes multiple augmented views \u2026"}, {"title": "Enhancing Data Quality in Federated Fine-Tuning of Large Language Models", "link": "https://openreview.net/pdf%3Fid%3DaHD3WJ7gQ5", "details": "W Zhao, Y Du, ND Lane, S Chen, Y Wang - ICLR 2024 Workshop on Navigating and \u2026", "abstract": "In the current landscape of large language model training, there is a significant reliance on public domain data, which is nearing exhaustion according to recent research. To further scale up, it is crucial to incorporate collaboration among multiple \u2026"}, {"title": "Argumentative Large Language Models for Explainable and Contestable Decision-Making", "link": "https://arxiv.org/pdf/2405.02079", "details": "G Freedman, A Dejl, D Gorur, X Yin, A Rago, F Toni - arXiv preprint arXiv:2405.02079, 2024", "abstract": "The diversity of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them a promising candidate for use in decision-making. However, they are currently limited \u2026"}, {"title": "Interpretable Multi-task Learning with Shared Variable Embeddings", "link": "https://arxiv.org/pdf/2405.06330", "details": "M \u017belaszczyk, J Ma\u0144dziuk - arXiv preprint arXiv:2405.06330, 2024", "abstract": "This paper proposes a general interpretable predictive system with shared information. The system is able to perform predictions in a multi-task setting where distinct tasks are not bound to have the same input/output structure. Embeddings of \u2026"}, {"title": "Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning", "link": "https://arxiv.org/pdf/2405.05615", "details": "S Jie, Y Tang, N Ding, ZH Deng, K Han, Y Wang - arXiv preprint arXiv:2405.05615, 2024", "abstract": "Current solutions for efficiently constructing large vision-language (VL) models follow a two-step paradigm: projecting the output of pre-trained vision encoders to the input space of pre-trained language models as visual prompts; and then transferring the \u2026"}, {"title": "Model & Data Insights using Pre-trained Language Models", "link": "https://openreview.net/pdf%3Fid%3DL5T3ZqsD0j", "details": "S Asgari, A Khani, AH Khasahmadi, A Sanghi\u2026 - ICLR 2024 Workshop on \u2026", "abstract": "We propose TExplain, using language models to interpret pre-trained image classifiers' features. Our approach connects the feature space of image classifiers with language models, generating explanatory sentences during inference. By \u2026"}, {"title": "TIQ: A Benchmark for Temporal Question Answering with Implicit Time Constraints", "link": "https://dl.acm.org/doi/pdf/10.1145/3589335.3651895", "details": "Z Jia, P Christmann, G Weikum - Companion Proceedings of the ACM on Web \u2026, 2024", "abstract": "Temporal question answering (QA) involves explicit (eg,\"... before 2024\") or implicit (eg,\"... during the Cold War period\") time constraints. Implicit constraints are more challenging; yet benchmarks for temporal QA largely disregard such questions. This \u2026"}]
