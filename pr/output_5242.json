[{"title": "Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models", "link": "https://aclanthology.org/2024.acl-long.404.pdf", "details": "Q Ai, J Li, J Dai, J Zhou, L Liu, H Jiang, S Shi - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields. In this paper, we propose a new paradigm for interactive and instructional graph data \u2026"}, {"title": "VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models", "link": "https://aclanthology.org/2024.acl-long.658.pdf", "details": "Q Cao, J Cheng, X Liang, L Lin - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Despite the significant success of large vision-language models (LVLMs), some studies have revealed that LVLMs suffer from the hallucination problem, where the LVLMs' response contains descriptions of non-existent objects. Although various \u2026"}, {"title": "InstructEval: Instruction-Tuned Text Evaluator from Human Preference", "link": "https://aclanthology.org/2024.findings-acl.799.pdf", "details": "W Wu, W Li, X Xiao, J Liu, S Li - Findings of the Association for Computational \u2026, 2024", "abstract": "This paper explores to construct a general text evaluator based on open-source Large Language Models (LLMs), a domain predominantly occupied by commercial counterparts such as GPT-4. Recognizing the limitations of open-source models like \u2026"}, {"title": "Llama2Vec: Unsupervised Adaptation of Large Language Models for Dense Retrieval", "link": "https://aclanthology.org/2024.acl-long.191.pdf", "details": "C Li, Z Liu, S Xiao, Y Shao, D Lian - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Dense retrieval calls for discriminative embeddings to represent the semantic relationship between query and document. It may benefit from the using of large language models (LLMs), given LLMs' strong capability on semantic understanding \u2026"}, {"title": "Domain Adaptation for Subjective Induction Questions Answering on Products by Adversarial Disentangled Learning", "link": "https://aclanthology.org/2024.acl-long.491.pdf", "details": "Y Zhang, J Yu, Y Rao, L Zheng, Q Su, H Zhu, J Yin - \u2026 of the 62nd Annual Meeting of \u2026, 2024", "abstract": "This paper focuses on answering subjective questions about products. Different from the factoid question with a single answer span, this subjective one involves multiple viewpoints. For example, the question of 'how the phone's battery is?'not only \u2026"}, {"title": "On the role of the UMLS in supporting diagnosis generation differential diagnoses proposed by Large Language Models", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001254", "details": "M Afshar, Y Gao, D Gupta, E Croxford\u2026 - Journal of Biomedical \u2026, 2024", "abstract": "Objective: Traditional knowledge-based and machine learning diagnostic decision support systems have benefited from integrating the medical domain knowledge encoded in the Unified Medical Language System (UMLS). The emergence of Large \u2026"}, {"title": "Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination", "link": "https://aclanthology.org/2024.acl-long.490.pdf", "details": "N Yang, T Kang, SJ Choi, H Lee, K Jung - Proceedings of the 62nd Annual Meeting of \u2026, 2024", "abstract": "Instruction-following language models often show undesirable biases. These undesirable biases may be accelerated in the real-world usage of language models, where a wide range of instructions is used through zero-shot example prompting. To \u2026"}, {"title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios", "link": "https://aclanthology.org/2024.findings-acl.230.pdf", "details": "L Lin, J Fu, P Liu, Q Li, Y Gong, J Wan, F Zhang\u2026 - Findings of the Association \u2026, 2024", "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local \u2026"}, {"title": "Text2DB: Integration-Aware Information Extraction with Large Language Model Agents", "link": "https://aclanthology.org/2024.findings-acl.12.pdf", "details": "Y Jiao, S Li, S Zhou, H Ji, J Han - Findings of the Association for Computational \u2026, 2024", "abstract": "The task of information extraction (IE) is to extract structured knowledge from text. However, it is often not straightforward to utilize IE output due to the mismatch between the IE ontology and the downstream application needs. We propose a new \u2026"}]
