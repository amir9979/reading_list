[{"title": "How to Train Long-Context Language Models (Effectively)", "link": "https://arxiv.org/pdf/2410.02660%3F", "details": "T Gao, A Wettig, H Yen, D Chen - arXiv preprint arXiv:2410.02660, 2024", "abstract": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development--Instead of perplexity or simple \u2026"}, {"title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models", "link": "https://arxiv.org/pdf/2410.18252", "details": "M Noukhovitch, S Huang, S Xhonneux, A Hosseini\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this \u2026"}, {"title": "Fine-Tuning Pre-trained Language Models for Robust Causal Representation Learning", "link": "https://arxiv.org/pdf/2410.14375", "details": "J Yu, Y Zhou, Y He, NL Zhang, R Silva - arXiv preprint arXiv:2410.14375, 2024", "abstract": "The fine-tuning of pre-trained language models (PLMs) has been shown to be effective across various domains. By using domain-specific supervised data, the general-purpose representation derived from PLMs can be transformed into a \u2026"}, {"title": "Tuning Language Models by Mixture-of-Depths Ensemble", "link": "https://arxiv.org/pdf/2410.13077", "details": "H Luo, L Specia - arXiv preprint arXiv:2410.13077, 2024", "abstract": "Transformer-based Large Language Models (LLMs) traditionally rely on final-layer loss for training and final-layer representations for predictions, potentially overlooking the predictive power embedded in intermediate layers. Surprisingly, we \u2026"}, {"title": "IPO: Interpretable Prompt Optimization for Vision-Language Models", "link": "https://arxiv.org/pdf/2410.15397", "details": "Y Du, W Sun, CGM Snoek - arXiv preprint arXiv:2410.15397, 2024", "abstract": "Pre-trained vision-language models like CLIP have remarkably adapted to various downstream tasks. Nonetheless, their performance heavily depends on the specificity of the input text prompts, which requires skillful prompt template \u2026"}, {"title": "Manual Verbalizer Enrichment for Few-Shot Text Classification", "link": "https://arxiv.org/pdf/2410.06173", "details": "QA Nguyen, N Tomeh, M Lebbah, T Charnois, H Azzag\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the continuous development of pre-trained language models, prompt-based training becomes a well-adopted paradigm that drastically improves the exploitation of models for many natural language processing tasks. Prompting also shows great \u2026"}, {"title": "Verifiable, Debuggable, and Repairable Commonsense Logical Reasoning via LLM-based Theory Resolution", "link": "https://ssanner.github.io/papers/emnlp24_llmtres.pdf", "details": "A Toroghi, W Guo, A Pesaranghader, S Sanner - The 2024 Conference on Empirical \u2026, 2024", "abstract": "Abstract Recent advances in Large Language Models (LLM) have led to substantial interest in their application to commonsense reasoning tasks. Despite their potential, LLMs are susceptible to reasoning errors and hallucinations that may be harmful in \u2026"}, {"title": "Large language models enabled multiagent ensemble method for efficient EHR data labeling", "link": "https://arxiv.org/pdf/2410.16543", "details": "J Huang, K Nezafati, I Villanueva-Miranda, Z Gu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study introduces a novel multiagent ensemble method powered by LLMs to address a key challenge in ML-data labeling, particularly in large-scale EHR datasets. Manual labeling of such datasets requires domain expertise and is labor \u2026"}, {"title": "Sample Feature Enhancement Model Based on Heterogeneous Graph Representation Learning for Few-shot Relation Classification", "link": "https://www.sciencedirect.com/science/article/pii/S002002552401497X", "details": "Z Xing, Y Ye, R Song, Y Teng, Z Li, J Liu - Information Sciences, 2024", "abstract": "Abstract Few-Shot Relation Classification (FSRC) aims to predict novel relationships by learning from limited samples. Graph Neural Network (GNN) approaches for FSRC constructs data as graphs, effectively capturing sample features through graph \u2026"}]
