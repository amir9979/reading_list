[{"title": "Achieving GPT-4o level performance in astronomy with a specialized 8B-parameter large language model", "link": "https://www.nature.com/articles/s41598-025-97131-y", "details": "T de Haan, YS Ting, T Ghosal, TD Nguyen\u2026 - Scientific Reports, 2025", "abstract": "Abstract AstroSage-Llama-3.1-8B is a domain-specialized natural-language AI assistant tailored for research in astronomy, astrophysics, cosmology, and astronomical instrumentation. Trained on the complete collection of astronomy \u2026"}, {"title": "PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning", "link": "https://arxiv.org/pdf/2505.09519", "details": "Z Li, Y Su, N Collier - arXiv preprint arXiv:2505.09519, 2025", "abstract": "Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting large language models, yet existing approaches exhibit counter-intuitive phenomena: integrating router into prompt tuning (PT) increases training efficiency \u2026"}, {"title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging", "link": "https://arxiv.org/pdf/2505.09316", "details": "H Qian, Z Liu - arXiv preprint arXiv:2505.09316, 2025", "abstract": "Augmenting large language models (LLMs) with external retrieval has become a standard method to address their inherent knowledge cutoff limitations. However, traditional retrieval-augmented generation methods employ static, pre-inference \u2026"}]
