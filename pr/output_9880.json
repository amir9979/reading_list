[{"title": "Evaluating Language Models as Synthetic Data Generators", "link": "https://arxiv.org/pdf/2412.03679", "details": "S Kim, J Suk, X Yue, V Viswanathan, S Lee, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data \u2026"}, {"title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification", "link": "https://arxiv.org/pdf/2412.00876", "details": "W Huang, Z Zhai, Y Shen, S Cao, F Zhao, X Xu, Z Ye\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision understanding, reasoning, and interaction. However, the inference computation and memory increase progressively with the generation of output tokens \u2026"}, {"title": "Visual cot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought reasoning", "link": "https://openreview.net/pdf%3Fid%3DaXeiCbMFFJ", "details": "H Shao, S Qian, H Xiao, G Song, Z Zong, L Wang, Y Liu\u2026 - The Thirty-eight Conference \u2026, 2024", "abstract": "Multi-Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However, they often lack interpretability and struggle with complex visual inputs, especially when the resolution of the input image \u2026"}, {"title": "TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation", "link": "https://arxiv.org/pdf/2412.03069", "details": "L Qu, H Zhang, Y Liu, X Wang, Y Jiang, Y Gao, H Ye\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present TokenFlow, a novel unified image tokenizer that bridges the long- standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder \u2026"}, {"title": "Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination", "link": "https://arxiv.org/pdf/2411.12591", "details": "H Zheng, T Xu, H Sun, S Pu, R Chen, L Sun - arXiv preprint arXiv:2411.12591, 2024", "abstract": "Multimodal large language models (MLLMs) have advanced the integration of visual and linguistic modalities, establishing themselves as the dominant paradigm for visual-language tasks. Current approaches like chain of thought (CoT) reasoning \u2026"}, {"title": "All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages", "link": "https://arxiv.org/pdf/2411.16508", "details": "A Vayani, D Dissanayake, H Watawana, N Ahsan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing Large Multimodal Models (LMMs) generally focus on only a few regions and languages. As LMMs continue to improve, it is increasingly important to ensure they understand cultural contexts, respect local sensitivities, and support low-resource \u2026"}, {"title": "Accelerating Multimodel Large Language Models by Searching Optimal Vision Token Reduction", "link": "https://arxiv.org/pdf/2412.00556", "details": "S Zhao, Z Wang, F Juefei-Xu, X Xia, M Liu, X Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Prevailing Multimodal Large Language Models (MLLMs) encode the input image (s) as vision tokens and feed them into the language backbone, similar to how Large Language Models (LLMs) process the text tokens. However, the number of vision \u2026"}, {"title": "Evaluating and Advancing Multimodal Large Language Models in Ability Lens", "link": "https://arxiv.org/pdf/2411.14725", "details": "F Chen, C Gou, J Liu, Y Yang, Z Li, J Zhang, Z Sun\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As multimodal large language models (MLLMs) advance rapidly, rigorous evaluation has become essential, providing further guidance for their development. In this work, we focus on a unified and robust evaluation of\\textbf {vision perception} abilities, the \u2026"}, {"title": "Enhancing Fairness in LLM Evaluations: Unveiling and Mitigating Biases in Standard-Answer-Based Evaluations", "link": "https://ojs.aaai.org/index.php/AAAI-SS/article/download/31771/33938", "details": "T Jiao, J Zhang, K Xu, R Li, X Du, S Wang, Z Song - Proceedings of the AAAI \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) are recognized for their effectiveness in comparing two answers. However, LLMs can still exhibit biases when comparing one answer to a standard answer, particularly in real-world scenarios like new employee \u2026"}]
