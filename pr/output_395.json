'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [HaluEval-Wild: Evaluating Hallucinations of Language M'
[{"title": "An Efficient Approach for Studying Cross-Lingual Transfer in Multilingual Language Models", "link": "https://arxiv.org/pdf/2403.20088", "details": "F Faisal, A Anastasopoulos - arXiv preprint arXiv:2403.20088, 2024", "abstract": "The capacity and effectiveness of pre-trained multilingual models (MLMs) for zero- shot cross-lingual transfer is well established. However, phenomena of positive or negative transfer, and the effect of language choice still need to be fully understood \u2026"}, {"title": "ZOOM: Learning Video Mirror Detection with Extremely-Weak Supervision", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/28450/28877", "details": "K Xu, TW Siu, RWH Lau - Proceedings of the AAAI Conference on Artificial \u2026, 2024", "abstract": "Mirror detection is an active research topic in computer vision. However, all existing mirror detectors learn mirror representations from large-scale pixel-wise datasets, which are tedious and expensive to obtain. Although weakly-supervised learning has \u2026"}, {"title": "Identifying data-driven subtypes of major depressive disorder with electronic health records", "link": "https://www.sciencedirect.com/science/article/pii/S0165032724005858", "details": "A Sharma, PF Verhaak, TH McCoy, RH Perlis\u2026 - Journal of Affective \u2026, 2024", "abstract": "Background Efforts to reduce the heterogeneity of major depressive disorder (MDD) by identifying subtypes have not yet facilitated treatment personalization or investigation of biology, so novel approaches merit consideration. Methods We \u2026"}, {"title": "Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records", "link": "https://arxiv.org/pdf/2403.09226", "details": "A Ziletti, L D'Ambrosi - arXiv preprint arXiv:2403.09226, 2024", "abstract": "Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. Querying these databases to answer epidemiological questions is challenging due to the intricacy of medical \u2026"}, {"title": "Automated Extraction of Stroke Severity from Unstructured Electronic Health Records using Natural Language Processing", "link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10980121/", "details": "M Fernandes, MB Westover, AB Singhal, SF Zafar - medRxiv", "abstract": "BACKGROUND Multi-center electronic health records (EHR) can support quality improvement initiatives and comparative effectiveness research in stroke care. However, limitations of EHR-based research include challenges in abstracting key \u2026"}, {"title": "Concept-aware Data Construction Improves In-context Learning of Language Models", "link": "https://arxiv.org/pdf/2403.09703", "details": "M \u0160tef\u00e1nik, M Kadl\u010d\u00edk, P Sojka - arXiv preprint arXiv:2403.09703, 2024", "abstract": "Many recent language models (LMs) are capable of in-context learning (ICL), manifested in the LMs' ability to perform a new task solely from natural-language instruction. Previous work curating in-context learners assumes that ICL emerges \u2026"}, {"title": "IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators", "link": "https://arxiv.org/pdf/2403.03894", "details": "I Paul, J Luo, G Glava\u0161, I Gurevych - arXiv preprint arXiv:2403.03894, 2024", "abstract": "Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (ie, LMs for code generation) such as cross-lingual transfer \u2026"}, {"title": "Algorithmic progress in language models", "link": "https://arxiv.org/html/2403.05812v1", "details": "A Ho, T Besiroglu, E Erdil, D Owen, R Rahman, ZC Guo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We investigate the rate at which algorithms for pre-training language models have improved since the advent of deep learning. Using a dataset of over 200 language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we find that \u2026"}, {"title": "Common 7B Language Models Already Possess Strong Math Capabilities", "link": "https://arxiv.org/html/2403.04706v1", "details": "C Li, W Wang, J Hu, Y Wei, N Zheng, H Hu, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits \u2026"}]
