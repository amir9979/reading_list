[{"title": "Dense Communication between Language Models", "link": "https://arxiv.org/pdf/2505.12741", "details": "S Wu, Y Wang, Q Yao - arXiv preprint arXiv:2505.12741, 2025", "abstract": "As higher-level intelligence emerges from the combination of modular components with lower-level intelligence, many works combines Large Language Models (LLMs) for collective intelligence. Such combination is achieved by building communications \u2026", "entry_id": "http://arxiv.org/abs/2505.12741v1", "updated": "2025-05-19 05:56:06", "published": "2025-05-19 05:56:06", "authors": "Shiguang Wu;Yaqing Wang;Quanming Yao", "summary": "As higher-level intelligence emerges from the combination of modular\ncomponents with lower-level intelligence, many works combines Large Language\nModels (LLMs) for collective intelligence. Such combination is achieved by\nbuilding communications among LLMs. While current systems primarily facilitate\nsuch communication through natural language, this paper proposes a novel\nparadigm of direct dense vector communication between LLMs. Our approach\neliminates the unnecessary embedding and de-embedding steps when LLM interact\nwith another, enabling more efficient information transfer, fully\ndifferentiable optimization pathways, and exploration of capabilities beyond\nhuman heuristics. We use such stripped LLMs as vertexes and optimizable seq2seq\nmodules as edges to construct LMNet, with similar structure as MLPs. By\nutilizing smaller pre-trained LLMs as vertexes, we train a LMNet that achieves\ncomparable performance with LLMs in similar size with only less than 0.1%\ntraining cost. This offers a new perspective on scaling for general\nintelligence rather than training a monolithic LLM from scratch. Besides, the\nproposed method can be used for other applications, like customizing LLM with\nlimited data, showing its versatility.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.12741v1;http://arxiv.org/pdf/2505.12741v1", "pdf_url": "http://arxiv.org/pdf/2505.12741v1"}, {"title": "COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3717867.3717923", "details": "P Govil, H Jain, V Bonagiri, A Chadha, P Kumaraguru\u2026 - Proceedings of the 17th \u2026, 2025", "abstract": "Large Language Models (LLMs) often inherit biases from the web data they are trained on, which contains stereotypes and prejudices. Current methods for evaluating and mitigating these biases rely on bias-benchmark datasets. These \u2026"}, {"title": "Do Language Models Use Their Depth Efficiently?", "link": "https://arxiv.org/pdf/2505.13898", "details": "R Csord\u00e1s, CD Manning, C Potts - arXiv preprint arXiv:2505.13898, 2025", "abstract": "Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible \u2026", "entry_id": "http://arxiv.org/abs/2505.13898v1", "updated": "2025-05-20 04:00:56", "published": "2025-05-20 04:00:56", "authors": "R\u00f3bert Csord\u00e1s;Christopher D. Manning;Christopher Potts", "summary": "Modern LLMs are increasingly deep, and depth correlates with performance,\nalbeit with diminishing returns. However, do these models use their depth\nefficiently? Do they compose more features to create higher-order computations\nthat are impossible in shallow models, or do they merely spread the same kinds\nof computation out over more layers? To address these questions, we analyze the\nresidual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,\ncomparing the output of the sublayers to the residual stream reveals that\nlayers in the second half contribute much less than those in the first half,\nwith a clear phase transition between the two halves. Second, skipping layers\nin the second half has a much smaller effect on future computations and output\npredictions. Third, for multihop tasks, we are unable to find evidence that\nmodels are using increased depth to compose subresults in examples involving\nmany hops. Fourth, we seek to directly address whether deeper models are using\ntheir additional layers to perform new kinds of computation. To do this, we\ntrain linear maps from the residual stream of a shallow model to a deeper one.\nWe find that layers with the same relative depth map best to each other,\nsuggesting that the larger model simply spreads the same computations out over\nits many layers. All this evidence suggests that deeper models are not using\ntheir depth to learn new kinds of computation, but only using the greater depth\nto perform more fine-grained adjustments to the residual. This may help explain\nwhy increasing scale leads to diminishing returns for stacked Transformer\narchitectures.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.NE", "links": "http://arxiv.org/abs/2505.13898v1;http://arxiv.org/pdf/2505.13898v1", "pdf_url": "http://arxiv.org/pdf/2505.13898v1"}, {"title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models", "link": "https://arxiv.org/pdf/2505.14103", "details": "G Chen, F Song, Z Zhao, X Jia, Y Liu, Y Qiao, W Zhang - arXiv preprint arXiv \u2026, 2025", "abstract": "Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first \u2026", "entry_id": "http://arxiv.org/abs/2505.14103v2", "updated": "2025-05-21 03:36:20", "published": "2025-05-20 09:10:45", "authors": "Guangke Chen;Fu Song;Zhe Zhao;Xiaojun Jia;Yang Liu;Yanchen Qiao;Weizhe Zhang", "summary": "Jailbreak attacks to Large audio-language models (LALMs) are studied\nrecently, but they achieve suboptimal effectiveness, applicability, and\npracticability, particularly, assuming that the adversary can fully manipulate\nuser prompts. In this work, we first conduct an extensive experiment showing\nthat advanced text jailbreak attacks cannot be easily ported to end-to-end\nLALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a\nnovel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio\ndoes not need to align with user prompts in the time axis by crafting suffixal\njailbreak audios; (2) universality: a single jailbreak perturbation is\neffective for different prompts by incorporating multiple prompts into\nperturbation generation; (3) stealthiness: the malicious intent of jailbreak\naudios will not raise the awareness of victims by proposing various intent\nconcealment strategies; and (4) over-the-air robustness: the jailbreak audios\nremain effective when being played over the air by incorporating the\nreverberation distortion effect with room impulse response into the generation\nof the perturbations. In contrast, all prior audio jailbreak attacks cannot\noffer asynchrony, universality, stealthiness, or over-the-air robustness.\nMoreover, AudioJailbreak is also applicable to the adversary who cannot fully\nmanipulate user prompts, thus has a much broader attack scenario. Extensive\nexperiments with thus far the most LALMs demonstrate the high effectiveness of\nAudioJailbreak. We highlight that our work peeks into the security implications\nof audio jailbreak attacks against LALMs, and realistically fosters improving\ntheir security robustness. The implementation and audio samples are available\nat our website https://audiojailbreak.github.io/AudioJailbreak.", "comment": null, "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR;cs.AI;cs.LG;cs.SD;eess.AS", "links": "http://arxiv.org/abs/2505.14103v2;http://arxiv.org/pdf/2505.14103v2", "pdf_url": "http://arxiv.org/pdf/2505.14103v2"}, {"title": "Search-Based Correction of Reasoning Chains for Language Models", "link": "https://arxiv.org/pdf/2505.11824", "details": "M Kim, JP Falet, OE Richardson, X Chen, M Jain\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Chain-of-Thought (CoT) reasoning has advanced the capabilities and transparency of language models (LMs); however, reasoning chains can contain inaccurate statements that reduce performance and trustworthiness. To address this, we \u2026", "entry_id": "http://arxiv.org/abs/2505.11824v1", "updated": "2025-05-17 04:16:36", "published": "2025-05-17 04:16:36", "authors": "Minsu Kim;Jean-Pierre Falet;Oliver E. Richardson;Xiaoyin Chen;Moksh Jain;Sungjin Ahn;Sungsoo Ahn;Yoshua Bengio", "summary": "Chain-of-Thought (CoT) reasoning has advanced the capabilities and\ntransparency of language models (LMs); however, reasoning chains can contain\ninaccurate statements that reduce performance and trustworthiness. To address\nthis, we introduce a new self-correction framework that augments each reasoning\nstep in a CoT with a latent variable indicating its veracity, enabling modeling\nof all possible truth assignments rather than assuming correctness throughout.\nTo efficiently explore this expanded space, we introduce Search Corrector, a\ndiscrete search algorithm over boolean-valued veracity assignments. It\nefficiently performs otherwise intractable inference in the posterior\ndistribution over veracity assignments by leveraging the LM's joint likelihood\nover veracity and the final answer as a proxy reward. This efficient\ninference-time correction method facilitates supervised fine-tuning of an\nAmortized Corrector by providing pseudo-labels for veracity. The Amortized\nCorrector generalizes self-correction, enabling accurate zero-shot veracity\ninference in novel contexts. Empirical results demonstrate that Search\nCorrector reliably identifies errors in logical (ProntoQA) and mathematical\nreasoning (GSM8K) benchmarks. The Amortized Corrector achieves comparable\nzero-shot accuracy and improves final answer accuracy by up to 25%.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.11824v1;http://arxiv.org/pdf/2505.11824v1", "pdf_url": "http://arxiv.org/pdf/2505.11824v1"}, {"title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation", "link": "https://arxiv.org/pdf/2505.11754", "details": "W Huang, P Vougiouklis, M Lapata, JZ Pan - arXiv preprint arXiv:2505.11754, 2025", "abstract": "Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant \u2026", "entry_id": "http://arxiv.org/abs/2505.11754v1", "updated": "2025-05-16 23:29:47", "published": "2025-05-16 23:29:47", "authors": "Wenyu Huang;Pavlos Vougiouklis;Mirella Lapata;Jeff Z. Pan", "summary": "Multi-hop Question Answering (MHQA) adds layers of complexity to question\nanswering, making it more challenging. When Language Models (LMs) are prompted\nwith multiple search results, they are tasked not only with retrieving relevant\ninformation but also employing multi-hop reasoning across the information\nsources. Although LMs perform well on traditional question-answering tasks, the\ncausal mask can hinder their capacity to reason across complex contexts. In\nthis paper, we explore how LMs respond to multi-hop questions by permuting\nsearch results (retrieved documents) under various configurations. Our study\nreveals interesting findings as follows: 1) Encoder-decoder models, such as the\nones in the Flan-T5 family, generally outperform causal decoder-only LMs in\nMHQA tasks, despite being significantly smaller in size; 2) altering the order\nof gold documents reveals distinct trends in both Flan T5 models and fine-tuned\ndecoder-only models, with optimal performance observed when the document order\naligns with the reasoning chain order; 3) enhancing causal decoder-only models\nwith bi-directional attention by modifying the causal mask can effectively\nboost their end performance. In addition to the above, we conduct a thorough\ninvestigation of the distribution of LM attention weights in the context of\nMHQA. Our experiments reveal that attention weights tend to peak at higher\nvalues when the resulting answer is correct. We leverage this finding to\nheuristically improve LMs' performance on this task. Our code is publicly\navailable at https://github.com/hwy9855/MultiHopQA-Reasoning.", "comment": "ACL 2025 main", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.11754v1;http://arxiv.org/pdf/2505.11754v1", "pdf_url": "http://arxiv.org/pdf/2505.11754v1"}, {"title": "Do different prompting methods yield a common task representation in language models?", "link": "https://arxiv.org/pdf/2505.12075", "details": "G Davidson, TM Gureckis, BM Lake, A Williams - arXiv preprint arXiv:2505.12075, 2025", "abstract": "Demonstrations and instructions are two primary approaches for prompting language models to perform in-context learning (ICL) tasks. Do identical tasks elicited in different ways result in similar representations of the task? An improved \u2026", "entry_id": "http://arxiv.org/abs/2505.12075v2", "updated": "2025-05-21 22:58:38", "published": "2025-05-17 16:28:33", "authors": "Guy Davidson;Todd M. Gureckis;Brenden M. Lake;Adina Williams", "summary": "Demonstrations and instructions are two primary approaches for prompting\nlanguage models to perform in-context learning (ICL) tasks. Do identical tasks\nelicited in different ways result in similar representations of the task? An\nimproved understanding of task representation mechanisms would offer\ninterpretability insights and may aid in steering models. We study this through\n\\textit{function vectors} (FVs), recently proposed as a mechanism to extract\nfew-shot ICL task representations. We generalize FVs to alternative task\npresentations, focusing on short textual instruction prompts, and successfully\nextract instruction function vectors that promote zero-shot task accuracy. We\nfind evidence that demonstration- and instruction-based function vectors\nleverage different model components, and offer several controls to dissociate\ntheir contributions to task performance. Our results suggest that different\ntask promptings forms do not induce a common task representation through FVs\nbut elicit different, partly overlapping mechanisms. Our findings offer\nprincipled support to the practice of combining instructions and task\ndemonstrations, imply challenges in universally monitoring task inference\nacross presentation forms, and encourage further examinations of LLM task\ninference mechanisms.", "comment": "9 pages, 4 figures; under review", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.12075v2;http://arxiv.org/pdf/2505.12075v2", "pdf_url": "http://arxiv.org/pdf/2505.12075v2"}, {"title": "Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE", "link": "https://arxiv.org/pdf/2505.12533", "details": "V Arzt, A Hanbury, M Wiegand, G Recski, T Blevins - arXiv preprint arXiv:2505.12533, 2025", "abstract": "Analysing the generalisation capabilities of relation extraction (RE) models is crucial for assessing whether they learn robust relational patterns or rely on spurious correlations. Our cross-dataset experiments find that RE models struggle with \u2026", "entry_id": "http://arxiv.org/abs/2505.12533v1", "updated": "2025-05-18 20:22:14", "published": "2025-05-18 20:22:14", "authors": "Varvara Arzt;Allan Hanbury;Michael Wiegand;G\u00e1bor Recski;Terra Blevins", "summary": "Analysing the generalisation capabilities of relation extraction (RE) models\nis crucial for assessing whether they learn robust relational patterns or rely\non spurious correlations. Our cross-dataset experiments find that RE models\nstruggle with unseen data, even within similar domains. Notably, higher\nintra-dataset performance does not indicate better transferability, instead\noften signaling overfitting to dataset-specific artefacts. Our results also\nshow that data quality, rather than lexical similarity, is key to robust\ntransfer, and the choice of optimal adaptation strategy depends on the quality\nof data available: while fine-tuning yields the best cross-dataset performance\nwith high-quality data, few-shot in-context learning (ICL) is more effective\nwith noisier data. However, even in these cases, zero-shot baselines\noccasionally outperform all cross-dataset results. Structural issues in RE\nbenchmarks, such as single-relation per sample constraints and non-standardised\nnegative class definitions, further hinder model transferability.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.12533v1;http://arxiv.org/pdf/2505.12533v1", "pdf_url": "http://arxiv.org/pdf/2505.12533v1"}, {"title": "Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce", "link": "https://arxiv.org/pdf/2505.12244", "details": "H Wang, Z Zhu, F Shi - arXiv preprint arXiv:2505.12244, 2025", "abstract": "Autoregressive neural language models (LMs) generate a probability distribution over tokens at each time step given a prompt. In this work, we attempt to systematically understand the probability distributions that LMs can produce \u2026", "entry_id": "http://arxiv.org/abs/2505.12244v1", "updated": "2025-05-18 05:49:48", "published": "2025-05-18 05:49:48", "authors": "Haojin Wang;Zining Zhu;Freda Shi", "summary": "Autoregressive neural language models (LMs) generate a probability\ndistribution over tokens at each time step given a prompt. In this work, we\nattempt to systematically understand the probability distributions that LMs can\nproduce, showing that some distributions are significantly harder to elicit\nthan others. Specifically, for any target next-token distribution over the\nvocabulary, we attempt to find a prompt that induces the LM to output a\ndistribution as close as possible to the target, using either soft or hard\ngradient-based prompt tuning. We find that (1) in general, distributions with\nvery low or very high entropy are easier to approximate than those with\nmoderate entropy; (2) among distributions with the same entropy, those\ncontaining ''outlier tokens'' are easier to approximate; (3) target\ndistributions generated by LMs -- even LMs with different tokenizers -- are\neasier to approximate than randomly chosen targets. These results offer\ninsights into the expressiveness of LMs and the challenges of using them as\nprobability distribution proposers.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.12244v1;http://arxiv.org/pdf/2505.12244v1", "pdf_url": "http://arxiv.org/pdf/2505.12244v1"}]
