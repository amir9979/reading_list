[{"title": "\" See the World, Discover Knowledge\": A Chinese Factuality Evaluation for Large Vision Language Models", "link": "https://arxiv.org/pdf/2502.11718", "details": "J Gu, Y Wang, P Bu, C Wang, Z Wang, T Song, D Wei\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The evaluation of factual accuracy in large vision language models (LVLMs) has lagged behind their rapid development, making it challenging to fully reflect these models' knowledge capacity and reliability. In this paper, we introduce the first \u2026"}, {"title": "Multilingual Language Model Pretraining using Machine-translated Data", "link": "https://arxiv.org/pdf/2502.13252", "details": "J Wang, Y Lu, M Weber, M Ryabinin, D Adelani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-resource languages such as English, enables the pretraining of high-quality large language models (LLMs). The same can not be said for most other languages as LLMs still underperform for non-English languages, likely due to a gap in the \u2026"}, {"title": "FedBM: Stealing knowledge from pre-trained language models for heterogeneous federated learning", "link": "https://arxiv.org/pdf/2502.16832", "details": "M Zhu, Q Yang, Z Gao, Y Yuan, J Liu - Medical Image Analysis, 2025", "abstract": "Federated learning (FL) has shown great potential in medical image computing since it provides a decentralized learning paradigm that allows multiple clients to train a model collaboratively without privacy leakage. However, current studies have shown \u2026"}, {"title": "Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models", "link": "https://arxiv.org/pdf/2502.15964", "details": "A Narayan, D Biderman, S Eyuboglu, A May\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We investigate an emerging setup in which a small, on-device language model (LM) with access to local data communicates with a frontier, cloud-hosted LM to solve real- world tasks involving financial, medical, and scientific reasoning over long \u2026"}, {"title": "Transfer-Prompting: Enhancing Cross-Task Adaptation in Large Language Models via Dual-Stage Prompts Optimization", "link": "https://arxiv.org/pdf/2502.14211", "details": "Y Chang, Y Chang, Y Wu - arXiv preprint arXiv:2502.14211, 2025", "abstract": "Large language models (LLMs) face significant challenges when balancing multiple high-level objectives, such as generating coherent, relevant, and high-quality responses while maintaining efficient task adaptation across diverse tasks. To \u2026"}, {"title": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs", "link": "https://arxiv.org/pdf/2502.14645", "details": "Y Wu, L Ding, L Shen, D Tao - arXiv preprint arXiv:2502.14645, 2025", "abstract": "Knowledge editing allows for efficient adaptation of large language models (LLMs) to new information or corrections without requiring full retraining. However, prior methods typically focus on either single-language editing or basic multilingual \u2026"}, {"title": "Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration", "link": "https://openreview.net/pdf%3Fid%3DPp90xRxITT", "details": "Q Zhu, R Zhao, H Yan, Y He, Y Chen, L Gui - Workshop on Reasoning and Planning for \u2026", "abstract": "Large Language Models (LLMs) struggle with reasoning due to limited diversity and inefficient search. We propose an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) \u2026"}, {"title": "Efficient Response Generation Method Selection for Fine-Tuning Large Language Models", "link": "https://arxiv.org/pdf/2502.11779", "details": "X Ren, Q Chen, L Liu - arXiv preprint arXiv:2502.11779, 2025", "abstract": "The training data for fine-tuning large language models (LLMs) is typically structured as input-output pairs. However, for many tasks, there can be multiple equally valid output variations for the same input. Recent studies have observed that the choice of \u2026"}, {"title": "Fact or Guesswork? Evaluating Large Language Model's Medical Knowledge with Structured One-Hop Judgment", "link": "https://arxiv.org/pdf/2502.14275", "details": "J Li, Y Wang, K Zhang, Y Cai, B Hooi, N Peng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have been widely adopted in various downstream task domains. However, their ability to directly recall and apply factual medical knowledge remains under-explored. Most existing medical QA benchmarks assess \u2026"}]
