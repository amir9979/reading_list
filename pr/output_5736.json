[{"title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "link": "https://arxiv.org/pdf/2407.21417", "details": "Z Wu, Y Zhang, P Qi, Y Xu, R Han, Y Zhang, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (ie, follow open-ended instructions) and \u2026"}, {"title": "An Evidence-Based Framework For Heterogeneous Electronic Health Records: A Case Study In Mortality Prediction", "link": "https://link.springer.com/chapter/10.1007/978-3-031-67977-3_9", "details": "Y Ruan, L Huang, Q Xu, M Feng - International Conference on Belief Functions, 2024", "abstract": "Abstract Electronic Health Records (EHRs), characterized by their centralization of patient comprehensive disease and history information, hold significant promise to improve healthcare quality and efficiency. However, the heterogeneous nature of \u2026"}, {"title": "Optimizing Clinical Trial Eligibility Design Using Natural Language Processing Models and Real-World Data: Algorithm Development and Validation", "link": "https://ai.jmir.org/2024/1/e50800", "details": "K Lee, Z Liu, Y Mai, T Jun, M Ma, T Wang, L Ai, E Calay\u2026 - JMIR AI, 2024", "abstract": "Background Clinical trials are vital for developing new therapies but can also delay drug development. Efficient trial data management, optimized trial protocol, and accurate patient identification are critical for reducing trial timelines. Natural \u2026"}, {"title": "Making Long-Context Language Models Better Multi-Hop Reasoners", "link": "https://arxiv.org/pdf/2408.03246", "details": "Y Li, S Liang, MR Lyu, L Wang - arXiv preprint arXiv:2408.03246, 2024", "abstract": "Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased \u2026"}, {"title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios", "link": "https://aclanthology.org/2024.findings-acl.230.pdf", "details": "L Lin, J Fu, P Liu, Q Li, Y Gong, J Wan, F Zhang\u2026 - Findings of the Association \u2026, 2024", "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local \u2026"}, {"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "link": "https://arxiv.org/pdf/2408.00724", "details": "Y Wu, Z Sun, S Li, S Welleck, Y Yang - arXiv preprint arXiv:2408.00724, 2024", "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth \u2026"}, {"title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Trustworthy Response Generation in Chinese", "link": "https://dl.acm.org/doi/pdf/10.1145/3686807", "details": "H Wang, S Zhao, Z Qiang, Z Li, C Liu, N Xi, Y Du, B Qin\u2026 - ACM Transactions on \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to \u2026"}, {"title": "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "link": "https://arxiv.org/pdf/2408.02103", "details": "P Wang, X Wang, C Lou, S Mao, P Xie, Y Jiang - arXiv preprint arXiv:2408.02103, 2024", "abstract": "In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language \u2026"}, {"title": "Multi-modal Concept Alignment Pre-training for Generative Medical Visual Question Answering", "link": "https://aclanthology.org/2024.findings-acl.319.pdf", "details": "Q Yan, J Duan, J Wang - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Medical Visual Question Answering (Med-VQA) seeks to accurately respond to queries regarding medical images, a task particularly challenging for open-ended questions. This study unveils the Multi-modal Concept Alignment Pre-training \u2026"}]
