[{"title": "ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models", "link": "https://arxiv.org/pdf/2412.07012", "details": "J Zhang, L Xue, L Song, J Wang, W Huang, M Shu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the rise of multimodal applications, instruction data has become critical for training multimodal language models capable of understanding complex image- based queries. Existing practices rely on powerful but costly large language models \u2026"}, {"title": "Automatic Database Configuration Debugging using Retrieval-Augmented Language Models", "link": "https://arxiv.org/pdf/2412.07548", "details": "S Chen, J Fan, B Wu, N Tang, C Deng, P Wang, Y Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Database management system (DBMS) configuration debugging, eg, diagnosing poorly configured DBMS knobs and generating troubleshooting recommendations, is crucial in optimizing DBMS performance. However, the configuration debugging \u2026"}, {"title": "SpecFuse: Ensembling Large Language Models via Next-Segment Prediction", "link": "https://arxiv.org/pdf/2412.07380", "details": "B Lv, C Tang, Y Zhang, X Liu, Y Yu, P Luo - arXiv preprint arXiv:2412.07380, 2024", "abstract": "Ensembles of generative large language models (LLMs) can integrate the strengths of different LLMs to compensate for the limitations of individual models. However, recent work has focused on training an additional fusion model to combine complete \u2026"}, {"title": "Fully Open Source Moxin-7B Technical Report", "link": "https://arxiv.org/pdf/2412.06845", "details": "P Zhao, X Shen, Z Kong, Y Shen, SE Chang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have \u2026"}, {"title": "Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2412.05934", "details": "M Teng, J Xiaojun, D Ranjie, L Xinfeng, H Yihao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the rapid advancement of multimodal large language models (MLLMs), concerns regarding their security have increasingly captured the attention of both academia and industry. Although MLLMs are vulnerable to jailbreak attacks \u2026"}, {"title": "ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?", "link": "https://arxiv.org/pdf/2412.02368", "details": "L Zhang, S Eger, Y Cheng, W Zhai, J Belouadi, C Leiter\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal large language models (LLMs) have demonstrated impressive capabilities in generating high-quality images from textual instructions. However, their performance in generating scientific images--a critical application for \u2026"}, {"title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases", "link": "https://arxiv.org/pdf/2412.04862", "details": "LG Research, S An, K Bae, E Choi, K Choi, SJ Choi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8 B, and 2.4 B. These models feature several \u2026"}, {"title": "Temporal Insight Enhancement: Mitigating Temporal Hallucination in Video Understanding by Multimodal Large Language Models", "link": "https://link.springer.com/chapter/10.1007/978-3-031-78183-4_29", "details": "L Sun, L Wang, J Sun, T Okatani - International Conference on Pattern Recognition, 2025", "abstract": "Abstract Recent advancements in Multimodal Large Language Models (MLLMs) have enabled to process diverse input modalities, leading to significantly better understanding of multimedia contents. However, understanding videos is still difficult \u2026"}, {"title": "Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach", "link": "https://arxiv.org/pdf/2411.17760", "details": "S Deng, W Zhao, YJ Li, K Wan, D Miranda, A Kale\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-improvement in multimodal large language models (MLLMs) is crucial for enhancing their reliability and robustness. However, current methods often rely heavily on MLLMs themselves as judges, leading to high computational costs and \u2026"}]
