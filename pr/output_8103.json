[{"title": "Scalable Multi-Domain Adaptation of Language Models using Modular Experts", "link": "https://arxiv.org/pdf/2410.10181", "details": "P Schafhalter, S Liao, Y Zhou, CK Yeh, A Kandoor\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Domain-specific adaptation is critical to maximizing the performance of pre-trained language models (PLMs) on one or multiple targeted tasks, especially under resource-constrained use cases, such as edge devices. However, existing methods \u2026"}, {"title": "MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.17637", "details": "Z Liu, Y Zang, X Dong, P Zhang, Y Cao, H Duan, C He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs. This is typically achieved by using labeled datasets of chosen/rejected pairs and employing \u2026"}, {"title": "Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models", "link": "https://arxiv.org/pdf/2410.09047%3F", "details": "Q Liu, C Shang, L Liu, N Pappas, J Ma, NA John\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be degraded by the integration of the vision module compared to its LLM backbone. We investigate this phenomenon, dubbed as''safety alignment degradation''in this paper \u2026"}, {"title": "RE-tune: Incremental Fine Tuning of Biomedical Vision-Language Models for Multi-label Chest X-ray Classification", "link": "https://arxiv.org/pdf/2410.17827", "details": "M Mistretta, AD Bagdanov - arXiv preprint arXiv:2410.17827, 2024", "abstract": "In this paper we introduce RE-tune, a novel approach for fine-tuning pre-trained Multimodal Biomedical Vision-Language models (VLMs) in Incremental Learning scenarios for multi-label chest disease diagnosis. RE-tune freezes the backbones \u2026"}, {"title": "Preserving Generalization of Language models in Few-shot Continual Relation Extraction", "link": "https://arxiv.org/pdf/2410.00334", "details": "Q Tran, NX Thanh, NH Anh, NL Hai, T Le, L Van Ngo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Few-shot Continual Relations Extraction (FCRE) is an emerging and dynamic area of study where models can sequentially integrate knowledge from new relations with limited labeled data while circumventing catastrophic forgetting and preserving prior \u2026"}, {"title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ", "link": "https://arxiv.org/pdf/2410.09613", "details": "A Poulis, E Tsalapati, M Koubarakis - arXiv preprint arXiv:2410.09613, 2024", "abstract": "Recent advancements in transformer-based language models have sparked research into their logical reasoning capabilities. Most of the benchmarks used to evaluate these models are simple: generated from short (fragments of) first-order \u2026"}, {"title": "Which Client is Reliable?: A Reliable and Personalized Prompt-based Federated Learning for Medical Image Question Answering", "link": "https://arxiv.org/pdf/2410.17484", "details": "H Zhu, R Togo, T Ogawa, M Haseyama - arXiv preprint arXiv:2410.17484, 2024", "abstract": "Conventional medical artificial intelligence (AI) models face barriers in clinical application and ethical issues owing to their inability to handle the privacy-sensitive characteristics of medical data. We present a novel personalized federated learning \u2026"}, {"title": "A few-shot Label Unlearning in Vertical Federated Learning", "link": "https://arxiv.org/pdf/2410.10922", "details": "H Gu, HX Tae, CS Chan, L Fan - arXiv preprint arXiv:2410.10922, 2024", "abstract": "This paper addresses the critical challenge of unlearning in Vertical Federated Learning (VFL), an area that has received limited attention compared to horizontal federated learning. We introduce the first approach specifically designed to tackle \u2026"}, {"title": "Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment", "link": "https://arxiv.org/pdf/2410.14148", "details": "C Cui, A Zhang, Y Zhou, Z Chen, G Deng, H Yao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The recent advancements in large language models (LLMs) and pre-trained vision models have accelerated the development of vision-language large models (VLLMs), enhancing the interaction between visual and linguistic modalities. Despite \u2026"}]
