[{"title": "Mapping Bias in Vision Language Models: Signposts, Pitfalls, and the Road Ahead", "link": "https://arxiv.org/pdf/2410.13146", "details": "K Sasse, S Chen, J Pond, D Bitterman, J Osborne - arXiv preprint arXiv:2410.13146, 2024", "abstract": "As Vision Language Models (VLMs) gain widespread use, their fairness remains under-explored. In this paper, we analyze demographic biases across five models and six datasets. We find that portrait datasets like UTKFace and CelebA are the best \u2026"}, {"title": "Drivedreamer4d: World models are effective data machines for 4d driving scene representation", "link": "https://arxiv.org/pdf/2410.13571", "details": "G Zhao, C Ni, X Wang, Z Zhu, G Huang, X Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Closed-loop simulation is essential for advancing end-to-end autonomous driving systems. Contemporary sensor simulation methods, such as NeRF and 3DGS, rely predominantly on conditions closely aligned with training data distributions, which \u2026"}, {"title": "Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?", "link": "https://arxiv.org/pdf/2410.13523", "details": "C Liu, Z Wan, H Wang, Y Chen, T Qaiser, C Jin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical Vision-Language Pre-training (MedVLP) has made significant progress in enabling zero-shot tasks for medical image understanding. However, training MedVLP models typically requires large-scale datasets with paired, high-quality \u2026"}, {"title": "Code-switching finetuning: Bridging multilingual pretrained language models for enhanced cross-lingual performance", "link": "https://www.sciencedirect.com/science/article/pii/S0952197624016907", "details": "C Zan, L Ding, L Shen, Y Cao, W Liu - Engineering Applications of Artificial \u2026, 2025", "abstract": "In recent years, the development of pre-trained models has significantly propelled advancements in natural language processing. However, multilingual sequence-to- sequence pretrained language models (Seq2Seq PLMs) are pretrained on a wide \u2026"}, {"title": "Domain Adaptation with a Single Vision-Language Embedding", "link": "https://arxiv.org/pdf/2410.21361", "details": "M Fahes, TH Vu, A Bursuc, P P\u00e9rez, R de Charette - arXiv preprint arXiv:2410.21361, 2024", "abstract": "Domain adaptation has been extensively investigated in computer vision but still requires access to target data at the training time, which might be difficult to obtain in some uncommon conditions. In this paper, we present a new framework for domain \u2026"}, {"title": "Improving Multi-modal Large Language Model through Boosting Vision Capabilities", "link": "https://arxiv.org/pdf/2410.13733", "details": "Y Sun, H Zhang, Q Chen, X Zhang, N Sang, G Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We focus on improving the visual understanding capability for boosting the vision- language models. We propose\\textbf {Arcana}, a multiModal language model, which introduces two crucial techniques. First, we present Multimodal LoRA (MM-LoRA), a \u2026"}, {"title": "ComPO: Community Preferences for Language Model Personalization", "link": "https://arxiv.org/pdf/2410.16027", "details": "S Kumar, CY Park, Y Tsvetkov, NA Smith, H Hajishirzi - arXiv preprint arXiv \u2026, 2024", "abstract": "Conventional algorithms for training language models (LMs) with human feedback rely on preferences that are assumed to account for an\" average\" user, disregarding subjectivity and finer-grained variations. Recent studies have raised concerns that \u2026"}, {"title": "A Comparative Study of Recent Large Language Models on Generating Hospital Discharge Summaries for Lung Cancer Patients", "link": "https://arxiv.org/pdf/2411.03805", "details": "Y Li, F Li, K Roberts, L Cui, C Tao, H Xu - arXiv preprint arXiv:2411.03805, 2024", "abstract": "Generating discharge summaries is a crucial yet time-consuming task in clinical practice, essential for conveying pertinent patient information and facilitating continuity of care. Recent advancements in large language models (LLMs) have \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers", "link": "https://openreview.net/pdf%3Fid%3D67tRrjgzsh", "details": "X Lu, Y Zhao, B Qin, L Huo, Q Yang, D Xu - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}]
