[{"title": "Evolutionary Contrastive Distillation for Language Model Alignment", "link": "https://arxiv.org/pdf/2410.07513", "details": "J Katz-Samuels, Z Li, H Yun, P Nigam, Y Xu, V Petricek\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The ability of large language models (LLMs) to execute complex instructions is essential for their real-world applications. However, several recent studies indicate that LLMs struggle with challenging instructions. In this paper, we propose \u2026"}, {"title": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe", "link": "https://arxiv.org/pdf/2410.05248", "details": "Y Xiao, S Zhang, W Zhou, M Ghassemi, S Zhao - arXiv preprint arXiv:2410.05248, 2024", "abstract": "To induce desired behaviors in large language models (LLMs) for interaction-driven tasks, the instruction-tuning stage typically trains LLMs on instruction-response pairs using the next-token prediction (NTP) loss. Previous work aiming to improve \u2026"}, {"title": "Quantifying Generalization Complexity for Large Language Models", "link": "https://arxiv.org/pdf/2410.01769%3F", "details": "Z Qi, H Luo, X Huang, Z Zhao, Y Jiang, X Fan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating \u2026"}, {"title": "$\\beta $-calibration of Language Model Confidence Scores for Generative QA", "link": "https://arxiv.org/pdf/2410.06615", "details": "P Manggala, A Mastakouri, E Kirschbaum\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim \u2026"}, {"title": "Steering Large Language Models between Code Execution and Textual Reasoning", "link": "https://arxiv.org/pdf/2410.03524%3F", "details": "Y Chen, H Jhamtani, S Sharma, C Fan, C Wang - arXiv preprint arXiv:2410.03524, 2024", "abstract": "While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100 \u2026"}, {"title": "Falcon Mamba: The First Competitive Attention-free 7B Language Model", "link": "https://arxiv.org/pdf/2410.05355", "details": "J Zuo, M Velikanov, DE Rhaiem, I Chahed, Y Belkada\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this technical report, we present Falcon Mamba 7B, a new base large language model based on the novel Mamba architecture. Falcon Mamba 7B is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model \u2026"}, {"title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining", "link": "https://arxiv.org/pdf/2410.08102", "details": "T Bai, L Yang, ZH Wong, J Peng, X Zhuang, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these \u2026"}, {"title": "Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning", "link": "https://arxiv.org/pdf/2410.06304", "details": "R Li, Z Luo, X Du - arXiv preprint arXiv:2410.06304, 2024", "abstract": "Hallucinations in large language models (LLMs) pose significant challenges in tasks requiring complex multi-step reasoning, such as mathematical problem-solving. Existing approaches primarily detect the presence of hallucinations but lack a \u2026"}, {"title": "Harnessing Task Overload for Scalable Jailbreak Attacks on Large Language Models", "link": "https://arxiv.org/pdf/2410.04190", "details": "Y Dong, G Shen, D Zhao, X He, Y Zeng - arXiv preprint arXiv:2410.04190, 2024", "abstract": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks that bypass their safety mechanisms. Existing attack methods are fixed or specifically tailored for certain models and cannot flexibly adjust attack strength, which is critical for \u2026"}]
