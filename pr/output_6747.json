[{"title": "Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs", "link": "https://arxiv.org/pdf/2408.14397", "details": "X Zhang, JN Acosta, HY Zhou, P Rajpurkar - arXiv preprint arXiv:2408.14397, 2024", "abstract": "Recent advancements in artificial intelligence have significantly improved the automatic generation of radiology reports. However, existing evaluation methods fail to reveal the models' understanding of radiological images and their capacity to \u2026"}, {"title": "Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on Generated Data", "link": "https://arxiv.org/pdf/2409.11423", "details": "A Akkus, M Li, J Chu, M Backes, Y Zhang, S Sav - arXiv preprint arXiv:2409.11423, 2024", "abstract": "Large language models (LLMs) have shown considerable success in a range of domain-specific tasks, especially after fine-tuning. However, fine-tuning with real- world data usually leads to privacy risks, particularly when the fine-tuning samples \u2026"}, {"title": "The Factuality of Large Language Models in the Legal Domain", "link": "https://arxiv.org/pdf/2409.11798", "details": "RE Hamdani, T Bonald, F Malliaros, N Holzenberger\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper investigates the factuality of large language models (LLMs) as knowledge bases in the legal domain, in a realistic usage scenario: we allow for acceptable variations in the answer, and let the model abstain from answering when uncertain \u2026"}, {"title": "Towards Assurance of LLM Adversarial Robustness using Ontology-Driven Argumentation", "link": "https://www.researchgate.net/profile/Tomas_Bueno_Momcilovic/publication/380922433_Towards_Assurance_of_LLM_Adversarial_Robustness_using_Ontology-Driven_Argumentation/links/66e074ddfa5e11512cb0ea5b/Towards-Assurance-of-LLM-Adversarial-Robustness-using-Ontology-Driven-Argumentation.pdf", "details": "TB Momcilovic, B Buesser, G Zizzo, M Purcell, D Balta - 2024", "abstract": "Despite the impressive adaptability of large language models (LLMs), challenges remain in ensuring their security, transparency, and interpretability. Given their susceptibility to adversarial attacks, LLMs need to be defended with an evolving \u2026"}]
