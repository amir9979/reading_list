[{"title": "Deep Learning for Structured Data: Weak Supervision and Interpretability", "link": "https://etda.libraries.psu.edu/files/final_submissions/31291", "details": "T Zhao - 2025", "abstract": "Modern machine learning excels at modeling statistical associations and distributions of observational data, showing strong performance across many tasks. However, the application of ML algorithms faces two distinct challenges:(1) in many \u2026"}, {"title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models", "link": "https://arxiv.org/pdf/2501.05752", "details": "S Lee, H Park, J Kim, J Ok - arXiv preprint arXiv:2501.05752, 2025", "abstract": "Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer \u2026"}, {"title": "DnDScore: Decontextualization and Decomposition for Factuality Verification in Long-Form Text Generation", "link": "https://arxiv.org/pdf/2412.13175", "details": "M Wanner, B Van Durme, M Dredze - arXiv preprint arXiv:2412.13175, 2024", "abstract": "The decompose-then-verify strategy for verification of Large Language Model (LLM) generations decomposes claims that are then independently verified. Decontextualization augments text (claims) to ensure it can be verified outside of the \u2026"}, {"title": "SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models", "link": "https://arxiv.org/pdf/2412.11605", "details": "J Cheng, X Liu, C Wang, X Gu, Y Lu, D Zhang, Y Dong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Instruction-following is a fundamental capability of language models, requiring the model to recognize even the most subtle requirements in the instructions and accurately reflect them in its output. Such an ability is well-suited for and often \u2026"}, {"title": "Using Large Language Models to Promote Health Equity", "link": "https://ai.nejm.org/doi/full/10.1056/AIp2400889", "details": "E Pierson, D Shanmugam, R Movva, J Kleinberg\u2026 - NEJM AI, 2025", "abstract": "While the discussion about the effects of large language models (LLMs) on health equity has been largely cautionary, LLMs also present significant opportunities for improving health equity. We highlight three such opportunities: improving the \u2026"}, {"title": "Cascaded Self-Evaluation Augmented Training for Efficient Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2501.05662", "details": "Z Lv, W Wang, J Wang, S Zhang, F Wu - arXiv preprint arXiv:2501.05662, 2025", "abstract": "Efficient Multimodal Large Language Models (EMLLMs) have rapidly advanced recently. Incorporating Chain-of-Thought (CoT) reasoning and step-by-step self- evaluation has improved their performance. However, limited parameters often \u2026"}]
