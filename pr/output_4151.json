[{"title": "Scaling Retrieval-Based Language Models with a Trillion-Token Datastore", "link": "https://arxiv.org/pdf/2407.12854", "details": "R Shao, J He, A Asai, W Shi, T Dettmers, S Min\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws with respect to the amount of training data and the number of parameters allow us to predict the cost-benefit trade-offs of pretraining language models (LMs) in different configurations. In this paper, we consider another \u2026"}, {"title": "Fuzzy Multi-view Graph Learning on Sparse Electronic Health Records", "link": "https://ieeexplore.ieee.org/abstract/document/10572354/", "details": "T Tang, Z Han, S Yu, A Bagirov, Q Zhang - IEEE Transactions on Fuzzy Systems, 2024", "abstract": "Extracting latent disease patterns from electronic health records (EHRs) is a crucial solution for disease analysis, significantly facilitating healthcare decision-making. Multiview learning presents itself as a promising approach that offers a \u2026"}, {"title": "Exploring Universal Intrinsic Task Subspace for Few-shot Learning via Prompt Tuning", "link": "https://ieeexplore.ieee.org/iel8/6570655/6633080/10603438.pdf", "details": "Y Qin, X Wang, Y Su, Y Lin, N Ding, J Yi, W Chen, Z Liu\u2026 - IEEE/ACM Transactions on \u2026, 2024", "abstract": "Why can pre-trained language models (PLMs) learn universal representations and effectively adapt to broad NLP tasks differing a lot superficially? In this work, we empirically find evidence indicating that the adaptations of PLMs to various fewshot \u2026"}, {"title": "Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination", "link": "https://ieeecai.org/2024/wp-content/pdfs/540900a486/540900a486.pdf", "details": "X Zhong, K Batmanghelich, L Sun", "abstract": "Vision-language models pre-trained on large scale of unlabeled biomedical images and associated reports learn generalizable semantic representations. These multi- modal representations can benefit various downstream tasks in the biomedical \u2026"}, {"title": "Mental Modeling of Reinforcement Learning Agents by Language Models", "link": "https://arxiv.org/pdf/2406.18505", "details": "W Lu, X Zhao, J Spisak, JH Lee, S Wermter - arXiv preprint arXiv:2406.18505, 2024", "abstract": "Can emergent language models faithfully model the intelligence of decision-making agents? Though modern language models exhibit already some reasoning ability, and theoretically can potentially express any probable distribution over tokens, it \u2026"}, {"title": "JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models", "link": "https://arxiv.org/pdf/2407.01599", "details": "H Jin, L Hu, X Li, P Zhang, C Chen, J Zhuang, H Wang - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid evolution of artificial intelligence (AI) through developments in Large Language Models (LLMs) and Vision-Language Models (VLMs) has brought significant advancements across various technological domains. While these models \u2026"}, {"title": "Language models, like humans, show content effects on reasoning tasks", "link": "https://academic.oup.com/pnasnexus/article/3/7/pgae233/7712372", "details": "AK Lampinen, I Dasgupta, SCY Chan, HR Sheahan\u2026 - PNAS nexus, 2024", "abstract": "Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks but exhibit many imperfections. However, human abstract reasoning is also imperfect. Human \u2026"}, {"title": "Suri: Multi-constraint Instruction Following for Long-form Text Generation", "link": "https://arxiv.org/pdf/2406.19371", "details": "CM Pham, S Sun, M Iyyer - arXiv preprint arXiv:2406.19371, 2024", "abstract": "Existing research on instruction following largely focuses on tasks with simple instructions and short responses. In this work, we explore multi-constraint instruction following for generating long-form text. We create Suri, a dataset with 20K human \u2026"}, {"title": "Aligning Language Models with the Human World", "link": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi%3Farticle%3D1241%26context%3Ddissertations", "details": "R LIU - 2024", "abstract": "Abstract The field of Natural Language Processing (NLP) has undergone a significant transformation with the emergence of large language models (LMs). These models have enabled the development of human-like conversational \u2026"}]
