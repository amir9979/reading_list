'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Autonomous Data Selection with Language Models for Mat'
[{"title": "Causal Evaluation of Language Models", "link": "https://arxiv.org/pdf/2405.00622", "details": "S Chen, B Peng, M Chen, R Wang, M Xu, X Zeng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for \u2026"}, {"title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "link": "https://arxiv.org/pdf/2405.00402", "details": "L Ranaldi, A Freitas - arXiv preprint arXiv:2405.00402, 2024", "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches \u2026"}, {"title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision", "link": "https://arxiv.org/pdf/2405.04086", "details": "Y Tong, S Wang, D Li, Y Wang, S Han, Z Lin, C Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations \u2026"}, {"title": "LMD3: Language Model Data Density Dependence", "link": "https://arxiv.org/pdf/2405.06331", "details": "J Kirchenbauer, G Honke, G Somepalli, J Geiping\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We develop a methodology for analyzing language model task performance at the individual example level based on training data density estimation. Experiments with paraphrasing as a controlled intervention on finetuning data demonstrate that \u2026"}, {"title": "HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models", "link": "https://arxiv.org/abs/2405.10299", "details": "RS Sukthanker, A Zela, B Staffler, JKH Franke, F Hutter - arXiv preprint arXiv \u2026, 2024", "abstract": "The expanding size of language models has created the necessity for a comprehensive examination across various dimensions that reflect the desiderata with respect to the tradeoffs between various hardware metrics, such as latency \u2026"}, {"title": "Model & Data Insights using Pre-trained Language Models", "link": "https://openreview.net/pdf%3Fid%3DL5T3ZqsD0j", "details": "S Asgari, A Khani, AH Khasahmadi, A Sanghi\u2026 - ICLR 2024 Workshop on \u2026", "abstract": "We propose TExplain, using language models to interpret pre-trained image classifiers' features. Our approach connects the feature space of image classifiers with language models, generating explanatory sentences during inference. By \u2026"}, {"title": "SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2405.08317", "details": "R Peri, SM Jayanthi, S Ronanki, A Bhatia, K Mundnich\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Integrated Speech and Large Language Models (SLMs) that can follow speech instructions and generate relevant text responses have gained popularity lately. However, the safety and robustness of these models remains largely unclear. In this \u2026"}, {"title": "Beyond Helpfulness and Harmlessness: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning", "link": "https://arxiv.org/pdf/2405.02501", "details": "HK Choi, Y Li - arXiv preprint arXiv:2405.02501, 2024", "abstract": "Large Language Models (LLMs) are trained on massive text corpora, which are encoded with diverse personality traits. This triggers an interesting goal of eliciting a desired personality trait from the LLM, and probing its behavioral preferences \u2026"}, {"title": "CodeGRAG: Extracting Composed Syntax Graphs for Retrieval Augmented Cross-Lingual Code Generation", "link": "https://arxiv.org/pdf/2405.02355", "details": "K Du, R Rui, H Chai, L Fu, W Xia, Y Wang, R Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Utilizing large language models to generate codes has shown promising meaning in software development revolution. Despite the intelligence shown by the general large language models, their specificity in code generation can still be improved due \u2026"}]
