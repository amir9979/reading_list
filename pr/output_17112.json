[{"title": "HistoChat: Instruction-tuning multimodal vision language assistant for colorectal histopathology on limited data", "link": "https://www.cell.com/patterns/fulltext/S2666-3899\\(25\\)00132-1", "details": "U Afzaal, Z Su, U Sajjad, T Stack, H Lu, S Niu\u2026 - Patterns, 2025", "abstract": "\u2026 This study explores how **large** **language** **models** (LLMs) and multimodal LLMs (MLLMs) can improve histopathological analysis by using **medical** data to aid diagnostics. However, challenges such as data quality and availability limit their effectiveness. To \u2026"}, {"title": "TransBERT: Leveraging Automatic Translation for Domain-Specific Knowledge Transfer", "link": "https://access.archive-ouverte.unige.ch/access/metadata/1a375d95-21e4-479a-960d-db5856596bee/download", "details": "DEG HES-SO\u2013HAUTE \u00c9COLE, DE GENEVE", "abstract": "\u2026 , creating multilingual models, and comparing the results with **Large** **Language** **Models** (LLMs). The chapter wraps up by \u2026 **Question** **Answering** (QA) involves locating **answers** to **questions** within a sequence. Various methods can be used for \u2026"}, {"title": "Optimizing Pre-Trained Natural Language Transformers to Discover Domain Specific Incidental Findings From Radiology Documents", "link": "https://search.proquest.com/openview/5c505120b2ba558afa887fe4042eaa2b/1%3Fpq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy", "details": "R Carroll - 2025", "abstract": "This dissertation investigates the effectiveness of three pre-training approaches for identifying incidental **clinical** findings in radiology reports. The study compares a publicly available pretrained transformer model, the augmentation of this model's \u2026"}, {"title": "Towards Pragmatic Time Series Intelligence", "link": "https://kilthub.cmu.edu/articles/thesis/Towards_Pragmatic_Time_Series_Intelligence/29119580/1/files/54713414.pdf", "details": "M Goswami - 2025", "abstract": "\u2026 Arvind\u2013thank you for introducing me to **large** **language** **models** and for always being a patient listener. Without your intellectual guidance, JoLT \u2026 The final chapter shifts our focus to **large** **language** **models** (LLMs) and their surprising effectiveness \u2026"}, {"title": "Maximizing Learning Efficiency With Limited Labeled Data: Applications to Healthcare and Education", "link": "https://search.proquest.com/openview/c1ef1da9529643e0cfa09d92008bb2eb/1%3Fpq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy", "details": "S Enayati - 2025", "abstract": "\u2026 leverage **large** **language** **models** (LLMs) to learn the scoring patterns of teachers accurately, offering a reliable tool for automated narrative scoring. This approach reduces the subjectivity and resource requirements of manual scoring, providing a \u2026"}]
