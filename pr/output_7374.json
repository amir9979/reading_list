[{"title": "Reconstructing and analyzing the invariances of low\u2010dose CT image denoising networks", "link": "https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1002/mp.17413", "details": "E Eulig, F J\u00e4ger, J Maier, B Ommer, M Kachelrie\u00df - Medical Physics, 2024", "abstract": "Background Deep learning\u2010based methods led to significant advancements in many areas of medical imaging, most of which are concerned with the reduction of artifacts caused by motion, scatter, or noise. However, with most neural networks being black \u2026"}, {"title": "Multi-objective Evolution of Heuristic Using Large Language Model", "link": "https://arxiv.org/pdf/2409.16867", "details": "S Yao, F Liu, X Lin, Z Lu, Z Wang, Q Zhang - arXiv preprint arXiv:2409.16867, 2024", "abstract": "Heuristics are commonly used to tackle diverse search and optimization problems. Design heuristics usually require tedious manual crafting with domain knowledge. Recent works have incorporated large language models (LLMs) into automatic \u2026"}, {"title": "HunFlair2 in a cross-corpus evaluation of biomedical named entity recognition and normalization tools", "link": "https://academic.oup.com/bioinformatics/advance-article-pdf/doi/10.1093/bioinformatics/btae564/59212710/btae564.pdf", "details": "M S\u00e4nger, S Garda, XD Wang, L Weber-Genzel\u2026 - Bioinformatics, 2024", "abstract": "Motivation With the exponential growth of the life sciences literature, biomedical text mining (BTM) has become an essential technology for accelerating the extraction of insights from publications. The identification of entities in texts, such as diseases or \u2026"}, {"title": "\" Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree\": Zero-Shot Decision Tree Induction and Embedding with Large Language Models", "link": "https://arxiv.org/pdf/2409.18594", "details": "R Knauer, M Koddenbrock, R Wallsberger, NM Brisson\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) provide powerful means to leverage prior knowledge for predictive modeling when data is limited. In this work, we demonstrate how LLMs can use their compressed world knowledge to generate intrinsically interpretable \u2026"}, {"title": "Extractive Summarization via Fine-grained Semantic Tuple Extraction", "link": "https://aclanthology.org/2024.inlg-main.10.pdf", "details": "Y Ge, S Jeoung, J Diesner - Proceedings of the 17th International Natural \u2026, 2024", "abstract": "Traditional extractive summarization treats the task as sentence-level classification and requires a fixed number of sentences for extraction. However, this rigid constraint on the number of sentences to extract may hinder model generalization \u2026"}, {"title": "HeadCT-ONE: Enabling Granular and Controllable Automated Evaluation of Head CT Radiology Report Generation", "link": "https://arxiv.org/pdf/2409.13038", "details": "JN Acosta, X Zhang, S Dogra, HY Zhou, S Payabvash\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Head CT Ontology Normalized Evaluation (HeadCT-ONE), a metric for evaluating head CT report generation through ontology-normalized entity and relation extraction. HeadCT-ONE enhances current information extraction derived \u2026"}, {"title": "Debiasing large language models: research opportunities", "link": "https://www.tandfonline.com/doi/pdf/10.1080/03036758.2024.2398567", "details": "V Yogarajan, G Dobbie, TT Keegan - Journal of the Royal Society of New Zealand, 2024", "abstract": "Large language models (LLMs) are powerful decision-making tools widely adopted in healthcare, finance, and transportation. Embracing the opportunities and innovations of LLMs is inevitable. However, LLMs inherit stereotypes \u2026"}, {"title": "ASR Error Correction using Large Language Models", "link": "https://arxiv.org/pdf/2409.09554", "details": "R Ma, M Qian, M Gales, K Knill - arXiv preprint arXiv:2409.09554, 2024", "abstract": "Error correction (EC) models play a crucial role in refining Automatic Speech Recognition (ASR) transcriptions, enhancing the readability and quality of transcriptions. Without requiring access to the underlying code or model weights, EC \u2026"}, {"title": "Enhancing disease detection in radiology reports through fine-tuning lightweight LLM on weak labels", "link": "https://arxiv.org/pdf/2409.16563", "details": "Y Wei, X Wang, H Ong, Y Zhou, A Flanders, G Shih\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite significant progress in applying large language models (LLMs) to the medical domain, several limitations still prevent them from practical applications. Among these are the constraints on model size and the lack of cohort-specific \u2026"}]
