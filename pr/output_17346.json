[{"title": "Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models", "link": "https://arxiv.org/pdf/2506.04689", "details": "T Nguyen, Y Li, O Golovneva, L Zettlemoyer, S Oh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Scaling laws predict that the performance of large language models improves with increasing model size and data size. In practice, pre-training has been relying on massive web crawls, using almost all data sources publicly available on the internet \u2026", "entry_id": "http://arxiv.org/abs/2506.04689v1", "updated": "2025-06-05 07:12:12", "published": "2025-06-05 07:12:12", "authors": "Thao Nguyen;Yang Li;Olga Golovneva;Luke Zettlemoyer;Sewoong Oh;Ludwig Schmidt;Xian Li", "summary": "Scaling laws predict that the performance of large language models improves\nwith increasing model size and data size. In practice, pre-training has been\nrelying on massive web crawls, using almost all data sources publicly available\non the internet so far. However, this pool of natural data does not grow at the\nsame rate as the compute supply. Furthermore, the availability of high-quality\ntexts is even more limited: data filtering pipelines often remove up to 99% of\nthe initial web scrapes to achieve state-of-the-art. To address the \"data wall\"\nof pre-training scaling, our work explores ways to transform and recycle data\ndiscarded in existing filtering processes. We propose REWIRE, REcycling the Web\nwith guIded REwrite, a method to enrich low-quality documents so that they\ncould become useful for training. This in turn allows us to increase the\nrepresentation of synthetic data in the final pre-training set. Experiments at\n1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw\ntexts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points\nimprovement respectively across 22 diverse tasks, compared to training on only\nfiltered web data. Training on the raw-synthetic data mix is also more\neffective than having access to 2x web data. Through further analysis, we\ndemonstrate that about 82% of the mixed in texts come from transforming\nlower-quality documents that would otherwise be discarded. REWIRE also\noutperforms related approaches of generating synthetic data, including\nWikipedia-style paraphrasing, question-answer synthesizing and knowledge\nextraction. These results suggest that recycling web texts holds the potential\nfor being a simple and effective approach for scaling pre-training data.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2506.04689v1;http://arxiv.org/pdf/2506.04689v1", "pdf_url": "http://arxiv.org/pdf/2506.04689v1"}, {"title": "Membership Inference Attacks on Sequence Models", "link": "https://arxiv.org/pdf/2506.05126", "details": "L Rossi, M Aerni, J Zhang, F Tram\u00e8r - arXiv preprint arXiv:2506.05126, 2025", "abstract": "Sequence models, such as Large Language Models (LLMs) and autoregressive image generators, have a tendency to memorize and inadvertently leak sensitive information. While this tendency has critical legal implications, existing tools are \u2026", "entry_id": "http://arxiv.org/abs/2506.05126v1", "updated": "2025-06-05 15:13:57", "published": "2025-06-05 15:13:57", "authors": "Lorenzo Rossi;Michael Aerni;Jie Zhang;Florian Tram\u00e8r", "summary": "Sequence models, such as Large Language Models (LLMs) and autoregressive\nimage generators, have a tendency to memorize and inadvertently leak sensitive\ninformation. While this tendency has critical legal implications, existing\ntools are insufficient to audit the resulting risks. We hypothesize that those\ntools' shortcomings are due to mismatched assumptions. Thus, we argue that\neffectively measuring privacy leakage in sequence models requires leveraging\nthe correlations inherent in sequential generation. To illustrate this, we\nadapt a state-of-the-art membership inference attack to explicitly model\nwithin-sequence correlations, thereby demonstrating how a strong existing\nattack can be naturally extended to suit the structure of sequence models.\nThrough a case study, we show that our adaptations consistently improve the\neffectiveness of memorization audits without introducing additional\ncomputational costs. Our work hence serves as an important stepping stone\ntoward reliable memorization audits for large sequence models.", "comment": "Accepted to the 8th Deep Learning Security and Privacy Workshop\n  (DLSP) workshop (best paper award)", "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR;cs.LG", "links": "http://arxiv.org/abs/2506.05126v1;http://arxiv.org/pdf/2506.05126v1", "pdf_url": "http://arxiv.org/pdf/2506.05126v1"}, {"title": "CONFIDENCE IS ALL YOU NEED: FEW-SHOT RL FINE-TUNING OF LANGUAGE MODELS", "link": "https://www.researchgate.net/profile/Li-Pengyi-2/publication/392439026_Confidence_Is_All_You_Need_Few-Shot_RL_Fine-Tuning_of_Language_Models/links/6841fc4ec33afe388aca60f1/Confidence-Is-All-You-Need-Few-Shot-RL-Fine-Tuning-of-Language-Models.pdf", "details": "P Li, S AIRI, M Skripkin, A Zubrey, A Kuznetsov\u2026", "abstract": "Large language models (LLMs) excel at reasoning, yet post-training remains critical for aligning their behavior with task goals. Existing reinforcement learning (RL) methods often depend on costly human annotations or external reward models. We \u2026"}, {"title": "Benchmarking and Advancing Large Language Models for Local Life Services", "link": "https://arxiv.org/pdf/2506.02720", "details": "X Lan, J Feng, J Lei, X Shi, Y Li - arXiv preprint arXiv:2506.02720, 2025", "abstract": "Large language models (LLMs) have exhibited remarkable capabilities and achieved significant breakthroughs across various domains, leading to their widespread adoption in recent years. Building on this progress, we investigate their \u2026", "entry_id": "http://arxiv.org/abs/2506.02720v1", "updated": "2025-06-03 10:18:19", "published": "2025-06-03 10:18:19", "authors": "Xiaochong Lan;Jie Feng;Jiahuan Lei;Xinlei Shi;Yong Li", "summary": "Large language models (LLMs) have exhibited remarkable capabilities and\nachieved significant breakthroughs across various domains, leading to their\nwidespread adoption in recent years. Building on this progress, we investigate\ntheir potential in the realm of local life services. In this study, we\nestablish a comprehensive benchmark and systematically evaluate the performance\nof diverse LLMs across a wide range of tasks relevant to local life services.\nTo further enhance their effectiveness, we explore two key approaches: model\nfine-tuning and agent-based workflows. Our findings reveal that even a\nrelatively compact 7B model can attain performance levels comparable to a much\nlarger 72B model, effectively balancing inference cost and model capability.\nThis optimization greatly enhances the feasibility and efficiency of deploying\nLLMs in real-world online services, making them more practical and accessible\nfor local life applications.", "comment": "KDD 2025", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL", "links": "http://arxiv.org/abs/2506.02720v1;http://arxiv.org/pdf/2506.02720v1", "pdf_url": "http://arxiv.org/pdf/2506.02720v1"}, {"title": "Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation", "link": "https://arxiv.org/pdf/2506.02648", "details": "Y Yang, MK Chen, Q Liu, M Hu, Q Chen, G Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in large language models (LLMs) have demonstrated impressive reasoning capacities that mirror human-like thinking. However, whether LLMs possess genuine fluid intelligence (ie, the ability to reason abstractly and generalize \u2026", "entry_id": "http://arxiv.org/abs/2506.02648v1", "updated": "2025-06-03 09:01:08", "published": "2025-06-03 09:01:08", "authors": "Yue Yang;MingKang Chen;Qihua Liu;Mengkang Hu;Qiguang Chen;Gengrui Zhang;Shuyue Hu;Guangtao Zhai;Yu Qiao;Yu Wang;Wenqi Shao;Ping Luo", "summary": "Recent advances in large language models (LLMs) have demonstrated impressive\nreasoning capacities that mirror human-like thinking. However, whether LLMs\npossess genuine fluid intelligence (i.e., the ability to reason abstractly and\ngeneralize rules in novel situations) remains an open question. Existing\nreasoning benchmarks either focus on domain-specific knowledge (crystallized\nintelligence) or lack interpretability. To address these limitations, we\npropose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a\nhierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning\ntasks organized across four cognitive levels, with each task featuring multiple\ndynamic variants that test the same underlying latent rule. This design enables\nfine-grained, interpretable, and reliable assessments of fluid intelligence. We\nevaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o,\nClaude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1).\nExperimental results reveal that although most LLMs achieve competent and\nrobust performance in low-level cognition, they struggle with high-level\ncognition and exhibit limited generalization as task complexity grows. Our\nfindings highlight the gap between current LLMs and true human-like fluid\nintelligence and offer a new path for systematically tracking reasoning\nprogress in LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2506.02648v1;http://arxiv.org/pdf/2506.02648v1", "pdf_url": "http://arxiv.org/pdf/2506.02648v1"}, {"title": "Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes", "link": "https://arxiv.org/pdf/2506.01512", "details": "M Li, M Vrazitulis, D Schlangen - arXiv preprint arXiv:2506.01512, 2025", "abstract": "Rational speakers are supposed to know what they know and what they do not know, and to generate expressions matching the strength of evidence. In contrast, it is still a challenge for current large language models to generate corresponding utterances \u2026", "entry_id": "http://arxiv.org/abs/2506.01512v1", "updated": "2025-06-02 10:19:42", "published": "2025-06-02 10:19:42", "authors": "Meng Li;Michael Vrazitulis;David Schlangen", "summary": "Rational speakers are supposed to know what they know and what they do not\nknow, and to generate expressions matching the strength of evidence. In\ncontrast, it is still a challenge for current large language models to generate\ncorresponding utterances based on the assessment of facts and confidence in an\nuncertain real-world environment. While it has recently become popular to\nestimate and calibrate confidence of LLMs with verbalized uncertainty, what is\nlacking is a careful examination of the linguistic knowledge of uncertainty\nencoded in the latent space of LLMs. In this paper, we draw on typological\nframeworks of epistemic expressions to evaluate LLMs' knowledge of epistemic\nmodality, using controlled stories. Our experiments show that the performance\nof LLMs in generating epistemic expressions is limited and not robust, and\nhence the expressions of uncertainty generated by LLMs are not always reliable.\nTo build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge\nof epistemic modality in LLMs.", "comment": "accepted by ACL 2025 (main)", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.01512v1;http://arxiv.org/pdf/2506.01512v1", "pdf_url": "http://arxiv.org/pdf/2506.01512v1"}, {"title": "MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework", "link": "https://arxiv.org/pdf/2506.02460", "details": "Y Qi, Z Lyu, M Yang, Y Wang, L Bai, L Cui - arXiv preprint arXiv:2506.02460, 2025", "abstract": "As large language models (LLMs) are increasingly applied across various domains, enhancing safety while maintaining the helpfulness of LLMs has become a critical challenge. Recent studies solve this problem through safety-constrained online \u2026", "entry_id": "http://arxiv.org/abs/2506.02460v1", "updated": "2025-06-03 05:23:09", "published": "2025-06-03 05:23:09", "authors": "Yupeng Qi;Ziyu Lyu;Min Yang;Yanlin Wang;Lu Bai;Lixin Cui", "summary": "As large language models (LLMs) are increasingly applied across various\ndomains, enhancing safety while maintaining the helpfulness of LLMs has become\na critical challenge. Recent studies solve this problem through\nsafety-constrained online preference optimization or safety-constrained offline\npreference optimization. However, the safety-constrained online methods often\nsuffer from excessive safety, which might reduce helpfulness, while the\nsafety-constrained offline methods perform poorly in adaptively balancing\nsafety and helpfulness. To address these limitations, we propose MidPO, a\n\\textbf{\\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness\n\\textbf{\\underline{d}}ual \\textbf{\\underline{P}}reference\n\\textbf{\\underline{O}}ptimization. Firstly, MidPO devises single-preference\nenhanced direct preference optimization approach to transform the base model\ninto two independent experts, termed safety and helpfulness experts, and\nfine-tunes the two independent experts for optimal safety or helpfulness\nperformance. Secondly, to achieve an effective balance between safety and\nhelpfulness, MidPO incorporates the two experts into the MoE framework and\ndesigns a dynamic routing mechanism to allocate contributions from each expert\nadaptively. We conduct quantitative and qualitative experiments on three\npopular datasets to demonstrate the proposed MidPO significantly outperforms\nstate-of-the-art approaches in both safety and helpfulness. The code and models\nwill be released.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.02460v1;http://arxiv.org/pdf/2506.02460v1", "pdf_url": "http://arxiv.org/pdf/2506.02460v1"}, {"title": "VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning", "link": "https://arxiv.org/pdf/2506.02537", "details": "H Yan, H Zheng, H Wang, L Yin, X Liu, Z Cao, X Su\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent strides in multimodal large language models (MLLMs) have significantly advanced their performance in many reasoning tasks. However, Abstract Visual Reasoning (AVR) remains a critical challenge, primarily due to limitations in \u2026", "entry_id": "http://arxiv.org/abs/2506.02537v1", "updated": "2025-06-03 07:24:00", "published": "2025-06-03 07:24:00", "authors": "Hao Yan;Handong Zheng;Hao Wang;Liang Yin;Xingchen Liu;Zhenbiao Cao;Xinxing Su;Zihao Chen;Jihao Wu;Minghui Liao;Chao Weng;Wei Chen;Yuliang Liu;Xiang Bai", "summary": "Recent strides in multimodal large language models (MLLMs) have significantly\nadvanced their performance in many reasoning tasks. However, Abstract Visual\nReasoning (AVR) remains a critical challenge, primarily due to limitations in\nperceiving abstract graphics. To tackle this issue, we investigate the\nbottlenecks in current MLLMs and synthesize training data to improve their\nabstract visual perception. First, we propose VisuRiddles, a benchmark for AVR,\nfeaturing tasks meticulously constructed to assess models' reasoning capacities\nacross five core dimensions and two high-level reasoning categories. Second, we\nintroduce the Perceptual Riddle Synthesizer (PRS), an automated framework for\ngenerating riddles with fine-grained perceptual descriptions. PRS not only\ngenerates valuable training data for abstract graphics but also provides\nfine-grained perceptual description, crucially allowing for supervision over\nintermediate reasoning stages and thereby improving both training efficacy and\nmodel interpretability. Our extensive experimental results on VisuRiddles\nempirically validate that fine-grained visual perception is the principal\nbottleneck and our synthesis framework markedly enhances the performance of\ncontemporary MLLMs on these challenging tasks. Our code and dataset will be\nreleased at https://github.com/yh-hust/VisuRiddles", "comment": "13 pages, 4 figures", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2506.02537v1;http://arxiv.org/pdf/2506.02537v1", "pdf_url": "http://arxiv.org/pdf/2506.02537v1"}, {"title": "Evaluating Vision-Language and Large Language Models for Automated Student Assessment in Indonesian Classrooms", "link": "https://arxiv.org/pdf/2506.04822", "details": "N Aisyah, MDA Kautsar, A Hidayat, R Chowdhury\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Although vision-language and large language models (VLM and LLM) offer promising opportunities for AI-driven educational assessment, their effectiveness in real-world classroom settings, particularly in underrepresented educational contexts \u2026", "entry_id": "http://arxiv.org/abs/2506.04822v1", "updated": "2025-06-05 09:41:09", "published": "2025-06-05 09:41:09", "authors": "Nurul Aisyah;Muhammad Dehan Al Kautsar;Arif Hidayat;Raqib Chowdhury;Fajri Koto", "summary": "Although vision-language and large language models (VLM and LLM) offer\npromising opportunities for AI-driven educational assessment, their\neffectiveness in real-world classroom settings, particularly in\nunderrepresented educational contexts, remains underexplored. In this study, we\nevaluated the performance of a state-of-the-art VLM and several LLMs on 646\nhandwritten exam responses from grade 4 students in six Indonesian schools,\ncovering two subjects: Mathematics and English. These sheets contain more than\n14K student answers that span multiple choice, short answer, and essay\nquestions. Assessment tasks include grading these responses and generating\npersonalized feedback. Our findings show that the VLM often struggles to\naccurately recognize student handwriting, leading to error propagation in\ndownstream LLM grading. Nevertheless, LLM-generated feedback retains some\nutility, even when derived from imperfect input, although limitations in\npersonalization and contextual relevance persist.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.04822v1;http://arxiv.org/pdf/2506.04822v1", "pdf_url": "http://arxiv.org/pdf/2506.04822v1"}]
