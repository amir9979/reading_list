'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Transparent deep learning to identify autism spectrum'
[{"title": "Scaling Properties of Speech Language Models", "link": "https://arxiv.org/pdf/2404.00685", "details": "S Cuervo, R Marxer - arXiv preprint arXiv:2404.00685, 2024", "abstract": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language \u2026"}, {"title": "Measuring Cross-lingual Transfer in Bytes", "link": "https://arxiv.org/pdf/2404.08191", "details": "LR de Souza, TS Almeida, R Lotufo, R Nogueira - arXiv preprint arXiv:2404.08191, 2024", "abstract": "Multilingual pretraining has been a successful solution to the challenges posed by the lack of resources for languages. These models can transfer knowledge to target languages with minimal or no examples. Recent research suggests that monolingual \u2026"}, {"title": "Digital Forgetting in Large Language Models: A Survey of Unlearning Methods", "link": "https://arxiv.org/pdf/2404.02062", "details": "A Blanco-Justicia, N Jebreel, B Manzanares\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The objective of digital forgetting is, given a model with undesirable knowledge or behavior, obtain a new model where the detected issues are no longer present. The motivations for forgetting include privacy protection, copyright protection, elimination \u2026"}]
