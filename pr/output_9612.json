[{"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers", "link": "https://openreview.net/pdf%3Fid%3D67tRrjgzsh", "details": "X Lu, Y Zhao, B Qin, L Huo, Q Yang, D Xu - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "LL\\\" aMmlein: Compact and Competitive German-Only Language Models from Scratch", "link": "https://arxiv.org/pdf/2411.11171", "details": "J Pfister, J Wunderle, A Hotho - arXiv preprint arXiv:2411.11171, 2024", "abstract": "We create two German-only decoder models, LL\\\" aMmlein 120M and 1B, transparently from scratch and publish them, along with the training data, for the German NLP research community to use. The model training involved several key \u2026"}, {"title": "RedPajama: an Open Dataset for Training Large Language Models", "link": "https://arxiv.org/pdf/2411.12372%3F", "details": "M Weber, D Fu, Q Anthony, Y Oren, S Adams\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top \u2026"}, {"title": "metaTextGrad: Learning to learn with language models as optimizers", "link": "https://openreview.net/pdf%3Fid%3DyzieYIT9hu", "details": "G Xu, M Yuksekgonul, C Guestrin, J Zou - Adaptive Foundation Models: Evolving AI for \u2026", "abstract": "Large language models (LLMs) are increasingly used in learning algorithms, evaluations, and optimization tasks. Recent studies have shown that incorporating self-criticism into LLMs can significantly enhance model performance, with \u2026"}, {"title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step", "link": "https://arxiv.org/pdf/2411.10440%3F", "details": "G Xu, P Jin, L Hao, Y Song, L Sun, L Yuan - arXiv preprint arXiv:2411.10440, 2024", "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to \u2026"}, {"title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models", "link": "https://arxiv.org/pdf/2411.09595", "details": "Z Wang, J Lorraine, Y Wang, H Su, J Zhu, S Fidler\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This work explores expanding the capabilities of large language models (LLMs) pretrained on text to generate 3D meshes within a unified model. This offers key advantages of (1) leveraging spatial knowledge already embedded in LLMs, derived \u2026"}, {"title": "Hidden in Plain Sight: Evaluating Abstract Shape Recognition in Vision-Language Models", "link": "https://arxiv.org/pdf/2411.06287", "details": "A Hemmat, A Davies, TA Lamb, J Yuan, P Torr\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the importance of shape perception in human vision, early neural image classifiers relied less on shape information for object recognition than other (often spurious) features. While recent research suggests that current large Vision \u2026"}, {"title": "Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity", "link": "https://openreview.net/pdf%3Fid%3D4TlUE0ufiz", "details": "K Liang, Z Zhang, JF Fisac - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots \u2026"}, {"title": "Aligner: Efficient alignment by learning to correct", "link": "https://openreview.net/pdf%3Fid%3Dkq166jACVP", "details": "J Ji, B Chen, H Lou, D Hong, B Zhang, X Pan, T Qiu\u2026 - The Thirty-eighth Annual \u2026, 2024", "abstract": "With the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment \u2026"}]
