[{"title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models", "link": "https://openreview.net/pdf%3Fid%3DLC1QAqhePv", "details": "D Zhang, Z Hu, S Zhoubian, Z Du, K Yang, Z Wang\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving \u2026"}, {"title": "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties", "link": "https://aclanthology.org/2024.emnlp-main.1137.pdf", "details": "K Yu, Z Zhang, F Hu, S Storks, J Chai - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "A major reason behind the recent success of large language models (LLMs) is their incontext learning capability, which makes it possible to rapidly adapt them to downstream textbased tasks by prompting them with a small number of relevant \u2026"}, {"title": "FoPru: Focal Pruning for Efficient Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.14164", "details": "L Jiang, W Huang, T Liu, Y Zeng, J Li, L Cheng, X Xu - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) represent a significant advancement toward achieving superior multimodal capabilities by enabling powerful Large Language Models (LLMs) to understand visual input. Typically, LVLMs utilize visual \u2026"}, {"title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step", "link": "https://arxiv.org/pdf/2411.10440%3F", "details": "G Xu, P Jin, L Hao, Y Song, L Sun, L Yuan - arXiv preprint arXiv:2411.10440, 2024", "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to \u2026"}, {"title": "A cascaded retrieval-while-reasoning multi-document comprehension framework with incremental attention for medical question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0957417424025685", "details": "J Liu, J Ren, R Bai, Z Zhang, Z Lu - Expert Systems with Applications, 2024", "abstract": "Abstract Clinical Machine Reading Comprehension (MRC) is challenging due to the need for medical expertise and comprehensive reasoning chains for diagnosis. This paper introduces a novel cascade retrieval-while-reasoning framework for clinical \u2026"}, {"title": "Faithscore: Fine-grained evaluations of hallucinations in large vision-language models", "link": "https://aclanthology.org/2024.findings-emnlp.290.pdf", "details": "L Jing, R Li, Y Chen, X Du - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract We introduce FaithScore (Faithfulness to Atomic Image Facts Score), a reference-free and fine-grained evaluation metric that measures the faithfulness of the generated free-form answers from large vision-language models (LVLMs). The \u2026"}, {"title": "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.884.pdf", "details": "P Gao, T Yamasaki, K Imoto - Findings of the Association for Computational \u2026, 2024", "abstract": "We propose VE-KD, a novel method that balances knowledge distillation and vocabulary expansion with the aim of training efficient domain-specific language models. Compared with traditional pre-training approaches, VE-KD exhibits \u2026"}, {"title": "Task Arithmetic Through The Lens Of One-Shot Federated Learning", "link": "https://arxiv.org/pdf/2411.18607%3F", "details": "Z Tao, I Mason, S Kulkarni, X Boix - arXiv preprint arXiv:2411.18607, 2024", "abstract": "Task Arithmetic is a model merging technique that enables the combination of multiple models' capabilities into a single model through simple arithmetic in the weight space, without the need for additional fine-tuning or access to the original \u2026"}, {"title": "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.1052.pdf", "details": "W Zhai, A Zubiaga, B Liu, CJ Sun, Y Zhao - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "The fusion of language models (LMs) and knowledge graphs (KGs) is widely used in commonsense question answering, but generating faithful explanations remains challenging. Current methods often overlook path decoding faithfulness, leading to \u2026"}]
