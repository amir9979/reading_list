[{"title": "DiffuSETS: 12-lead ECG Generation Conditioned on Clinical Text Reports and Patient-Specific Information", "link": "https://arxiv.org/pdf/2501.05932", "details": "Y Lai, J Chen, D Zhang, Y Wang, S Geng, H Li, S Hong - arXiv preprint arXiv \u2026, 2025", "abstract": "Heart disease remains a significant threat to human health. As a non-invasive diagnostic tool, the electrocardiogram (ECG) is one of the most widely used methods for cardiac screening. However, the scarcity of high-quality ECG data, driven by \u2026"}, {"title": "Cascaded Self-Evaluation Augmented Training for Efficient Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2501.05662", "details": "Z Lv, W Wang, J Wang, S Zhang, F Wu - arXiv preprint arXiv:2501.05662, 2025", "abstract": "Efficient Multimodal Large Language Models (EMLLMs) have rapidly advanced recently. Incorporating Chain-of-Thought (CoT) reasoning and step-by-step self- evaluation has improved their performance. However, limited parameters often \u2026"}, {"title": "Backdoor Token Unlearning: Exposing and Defending Backdoors in Pretrained Language Models", "link": "https://arxiv.org/pdf/2501.03272", "details": "P Jiang, X Lyu, Y Li, J Ma - arXiv preprint arXiv:2501.03272, 2025", "abstract": "Supervised fine-tuning has become the predominant method for adapting large pretrained models to downstream tasks. However, recent studies have revealed that these models are vulnerable to backdoor attacks, where even a small number of \u2026"}]
