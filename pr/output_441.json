'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [DeViDe: Faceted medical knowledge for improved medica'
[{"title": "Adapting transformer-based language models for heart disease detection and risk factors extraction", "link": "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00903-y", "details": "EH Houssein, RE Mohamed, G Hu, AA Ali - Journal of Big Data, 2024", "abstract": "Efficiently treating cardiac patients before the onset of a heart attack relies on the precise prediction of heart disease. Identifying and detecting the risk factors for heart disease such as diabetes mellitus, Coronary Artery Disease (CAD), hyperlipidemia \u2026"}, {"title": "Uncertainty in Language Models: Assessment through Rank-Calibration", "link": "https://arxiv.org/html/2404.03163v1", "details": "X Huang, S Li, M Yu, M Sesia, H Hassani, I Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition \u2026"}, {"title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models", "link": "https://arxiv.org/html/2404.02936v1", "details": "J Zhang, J Sun, E Yeats, Y Ouyang, M Kuo, J Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. The current state-of-the-art approach, Min-K \u2026"}, {"title": "Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification", "link": "https://ui.adsabs.harvard.edu/abs/2024arXiv240304024B/abstract", "details": "R Bigolin Lanfredi, P Mukherjee, R Summers - arXiv e-prints, 2024", "abstract": "In chest X-ray (CXR) image analysis, rule-based systems are usually employed to extract labels from reports, but concerns exist about label quality. These datasets typically offer only presence labels, sometimes with binary uncertainty indicators \u2026"}, {"title": "Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts", "link": "https://arxiv.org/html/2403.12918v1", "details": "SA Somayajula, Y Liang, A Singh, L Zhang, P Xie - arXiv preprint arXiv:2403.12918, 2024", "abstract": "Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle \u2026"}, {"title": "An Incomplete Loop: Deductive, Inductive, and Abductive Learning in Large Language Models", "link": "https://arxiv.org/pdf/2404.03028", "details": "E Liu, G Neubig, J Andreas - arXiv preprint arXiv:2404.03028, 2024", "abstract": "Modern language models (LMs) can learn to perform new tasks in different ways: in instruction following, the target task is described explicitly in natural language; in few- shot prompting, the task is specified implicitly with a small number of examples; in \u2026"}, {"title": "A Novel Corpus of Annotated Medical Imaging Reports and Information Extraction Results Using BERT-based Language Models", "link": "https://arxiv.org/pdf/2403.18975", "details": "N Park, K Lybarger, GK Ramachandran, S Lewis\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical imaging is critical to the diagnosis, surveillance, and treatment of many health conditions, including oncological, neurological, cardiovascular, and musculoskeletal disorders, among others. Radiologists interpret these complex \u2026"}]
