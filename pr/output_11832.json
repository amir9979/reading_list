[{"title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models", "link": "https://arxiv.org/pdf/2501.02955", "details": "W Hong, Y Cheng, Z Yang, W Wang, L Wang, X Gu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In recent years, vision language models (VLMs) have made significant advancements in video understanding. However, a crucial capability-fine-grained motion comprehension-remains under-explored in current benchmarks. To address \u2026"}, {"title": "Open-source small language models for personal medical assistant chatbots", "link": "https://www.sciencedirect.com/science/article/pii/S2666521224000644", "details": "M Magnini, G Aguzzi, S Montagna - Intelligence-Based Medicine, 2025", "abstract": "Medical chatbots are becoming essential components of telemedicine applications as tools to assist patients in the self-management of their conditions. This trend is particularly driven by advancements in natural language processing techniques with \u2026"}, {"title": "Disentangled Contrastive Learning from Synthetic Matching Pairs for Targeted Chest X-ray Generation", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10844299.pdf", "details": "E Kim, S Lee, KM Lee - IEEE Access, 2025", "abstract": "Disentangled generation enables the synthesis of images with explicit control over disentangled attributes. However, traditional generative models often struggle to independently disentangle these attributes while maintaining the ability to generate \u2026"}, {"title": "Mee-SLAM: Memory efficient endoscopic RGB SLAM with implicit scene representation", "link": "https://www.sciencedirect.com/science/article/pii/S0957417424031026", "details": "Y Zhou, T Li, Y Dai, J Zhang - Expert Systems with Applications, 2025", "abstract": "Endoscopic dense simultaneous localization and mapping (SLAM) plays a critical role in robot assisted surgery. Recently, SLAM systems based on neural implicit representation have demonstrated superior localization and real-time mapping \u2026"}, {"title": "Autonomous International Classification of Diseases Coding Using Pretrained Language Models and Advanced Prompt Learning Techniques: Evaluation of an \u2026", "link": "https://medinform.jmir.org/2025/1/e63020/", "details": "Y Zhuang, J Zhang, X Li, C Liu, Y Yu, W Dong, K He - JMIR Medical Informatics, 2025", "abstract": "Background: Machine learning models can reduce the burden on doctors by converting medical records into International Classification of Diseases (ICD) codes in real time, thereby enhancing the efficiency of diagnosis and treatment. However, it \u2026"}, {"title": "Contrastive Learning in Multimodal Generative Models: Towards Interpretable and Robust 3D Image Synthesis", "link": "https://www.researchgate.net/profile/Andrew-Johnson-159/publication/387511076_Contrastive_Learning_in_Multimodal_Generative_Models_Towards_Interpretable_and_Robust_3D_Image_Synthesis/links/67720b6d117f340ec3e516d7/Contrastive-Learning-in-Multimodal-Generative-Models-Towards-Interpretable-and-Robust-3D-Image-Synthesis.pdf", "details": "A Johnson, R Lewis, D Jackson", "abstract": "Generative models have become a transformative technology in artificial intelligence, enabling the synthesis of high-quality data in domains such as computer vision, natural language processing, and multimodal understanding. Among these \u2026"}, {"title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining", "link": "https://arxiv.org/pdf/2501.00958", "details": "W Zhang, H Zhang, X Li, J Sun, Y Shen, W Lu, D Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge \u2026"}, {"title": "An Empirical Study of Autoregressive Pre-training from Videos", "link": "https://arxiv.org/pdf/2501.05453", "details": "J Rajasegaran, I Radosavovic, R Ravishankar\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We empirically study autoregressive pre-training from videos. To perform our study, we construct a series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict \u2026"}, {"title": "Zero-Shot Strategies for Length-Controllable Summarization", "link": "https://arxiv.org/pdf/2501.00233", "details": "F Retkowski, A Waibel - arXiv preprint arXiv:2501.00233, 2024", "abstract": "Large language models (LLMs) struggle with precise length control, particularly in zero-shot settings. We conduct a comprehensive study evaluating LLMs' length control capabilities across multiple measures and propose practical methods to \u2026"}]
