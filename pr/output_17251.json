[{"title": "VISTA3D: A unified segmentation foundation model for 3D medical imaging", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/He_VISTA3D_A_Unified_Segmentation_Foundation_Model_For_3D_Medical_Imaging_CVPR_2025_paper.pdf", "details": "Y He, P Guo, Y Tang, A Myronenko, V Nath, Z Xu\u2026 - Proceedings of the \u2026, 2025", "abstract": "Foundation models for interactive segmentation in 2D natural images and videos have sparked significant interest in building 3D foundation models for medical imaging. However, the domain gaps and clinical use cases for 3D medical imaging \u2026"}, {"title": "Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models", "link": "https://ieeexplore.ieee.org/iel8/42/4359023/11021425.pdf", "details": "C Lian, HY Zhou, D Liang, J Qin, L Wang - IEEE Transactions on Medical Imaging, 2025", "abstract": "Medical vision-language alignment through cross-modal contrastive learning shows promising performance in image-text matching tasks, such as retrieval and zero-shot classification. However, conventional cross-modal contrastive learning (CLIP-based) \u2026"}, {"title": "Special Topic Burnout: Ambient Artificial Intelligence Scribes in Pediatric Primary Care: A Mixed Methods Study", "link": "https://www.thieme-connect.com/products/ejournals/abstract/10.1055/a-2625-0750", "details": "N Rabbani, M Ray, E Verhagen, J Hatoun, L Patane\u2026 - Applied Clinical Informatics, 2025", "abstract": "Objective: Quantify the effect of ambient artificial intelligence (AI) scribe technology on work experience, clinical operations, and patient experience in pediatric primary care. Methods: We conducted a 12-week study of 39 clinicians within a large \u2026"}, {"title": "Tomographic Foundation Model -- FORCE: Flow-Oriented Reconstruction Conditioning Engine", "link": "https://arxiv.org/pdf/2506.02149", "details": "W Xia, C Niu, G Wang - arXiv preprint arXiv:2506.02149, 2025", "abstract": "Computed tomography (CT) is a major medical imaging modality. Clinical CT scenarios, such as low-dose screening, sparse-view scanning, and metal implants, often lead to severe noise and artifacts in reconstructed images, requiring improved \u2026", "entry_id": "http://arxiv.org/abs/2506.02149v1", "updated": "2025-06-02 18:25:12", "published": "2025-06-02 18:25:12", "authors": "Wenjun Xia;Chuang Niu;Ge Wang", "summary": "Computed tomography (CT) is a major medical imaging modality. Clinical CT\nscenarios, such as low-dose screening, sparse-view scanning, and metal\nimplants, often lead to severe noise and artifacts in reconstructed images,\nrequiring improved reconstruction techniques. The introduction of deep learning\nhas significantly advanced CT image reconstruction. However, obtaining paired\ntraining data remains rather challenging due to patient motion and other\nconstraints. Although deep learning methods can still perform well with\napproximately paired data, they inherently carry the risk of hallucination due\nto data inconsistencies and model instability. In this paper, we integrate the\ndata fidelity with the state-of-the-art generative AI model, referred to as the\nPoisson flow generative model (PFGM) with a generalized version PFGM++, and\npropose a novel CT framework: Flow-Oriented Reconstruction Conditioning Engine\n(FORCE). In our experiments, the proposed method shows superior performance in\nvarious CT imaging tasks, outperforming existing unsupervised reconstruction\napproaches.", "comment": null, "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.LG;eess.SP", "links": "http://arxiv.org/abs/2506.02149v1;http://arxiv.org/pdf/2506.02149v1", "pdf_url": "http://arxiv.org/pdf/2506.02149v1"}, {"title": "CineMA: A Foundation Model for Cine Cardiac MRI", "link": "https://arxiv.org/pdf/2506.00679", "details": "Y Fu, W Yi, C Manisty, AN Bhuva, TA Treibel, JC Moon\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Cardiac magnetic resonance (CMR) is a key investigation in clinical cardiovascular medicine and has been used extensively in population research. However, extracting clinically important measurements such as ejection fraction for diagnosing \u2026", "entry_id": "http://arxiv.org/abs/2506.00679v1", "updated": "2025-05-31 19:12:34", "published": "2025-05-31 19:12:34", "authors": "Yunguan Fu;Weixi Yi;Charlotte Manisty;Anish N Bhuva;Thomas A Treibel;James C Moon;Matthew J Clarkson;Rhodri Huw Davies;Yipeng Hu", "summary": "Cardiac magnetic resonance (CMR) is a key investigation in clinical\ncardiovascular medicine and has been used extensively in population research.\nHowever, extracting clinically important measurements such as ejection fraction\nfor diagnosing cardiovascular diseases remains time-consuming and subjective.\nWe developed CineMA, a foundation AI model automating these tasks with limited\nlabels. CineMA is a self-supervised autoencoder model trained on 74,916 cine\nCMR studies to reconstruct images from masked inputs. After fine-tuning, it was\nevaluated across eight datasets on 23 tasks from four categories: ventricle and\nmyocardium segmentation, left and right ventricle ejection fraction\ncalculation, disease detection and classification, and landmark localisation.\nCineMA is the first foundation model for cine CMR to match or outperform\nconvolutional neural networks (CNNs). CineMA demonstrated greater label\nefficiency than CNNs, achieving comparable or better performance with fewer\nannotations. This reduces the burden of clinician labelling and supports\nreplacing task-specific training with fine-tuning foundation models in future\ncardiac imaging applications. Models and code for pre-training and fine-tuning\nare available at https://github.com/mathpluscode/CineMA, democratising access\nto high-performance models that otherwise require substantial computational\nresources, promoting reproducibility and accelerating clinical translation.", "comment": null, "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2506.00679v1;http://arxiv.org/pdf/2506.00679v1", "pdf_url": "http://arxiv.org/pdf/2506.00679v1"}, {"title": "Robust multi\u2010coil MRI reconstruction via self\u2010supervised denoising", "link": "https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.30591", "details": "A Aali, M Arvinte, S Kumar, YI Arefeen, JI Tamir - Magnetic Resonance in Medicine", "abstract": "Purpose To examine the effect of incorporating self\u2010supervised denoising as a pre\u2010 processing step for training deep learning (DL) based reconstruction methods on data corrupted by Gaussian noise. K\u2010space data employed for training are typically \u2026"}, {"title": "The Impact Label Noise and Choice of Threshold has on Cross-Entropy and Soft-Dice in Image Segmentation", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Nordstrom_The_Impact_Label_Noise_and_Choice_of_Threshold_has_on_CVPR_2025_paper.pdf", "details": "M Nordstr\u00f6m, A Maki, H Hult - Proceedings of the Computer Vision and Pattern \u2026, 2025", "abstract": "In image segmentation and specifically in medical image segmentation, the soft-Dice loss is often chosen instead of the more traditional cross-entropy loss to improve performance with respect to the Dice metric. Experimental work supporting this claim \u2026"}, {"title": "DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?", "link": "https://arxiv.org/pdf/2505.24173", "details": "T Zhou, Y Xu, Y Zhu, C Xiao, H Bian, L Wei, X Zhang - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) exhibit strong zero-shot generalization on natural images and show early promise in interpretable medical image analysis. However, existing benchmarks do not systematically evaluate whether these models truly \u2026", "entry_id": "http://arxiv.org/abs/2505.24173v1", "updated": "2025-05-30 03:33:25", "published": "2025-05-30 03:33:25", "authors": "Tianhong Zhou;Yin Xu;Yingtao Zhu;Chuxi Xiao;Haiyang Bian;Lei Wei;Xuegong Zhang", "summary": "Vision-language models (VLMs) exhibit strong zero-shot generalization on\nnatural images and show early promise in interpretable medical image analysis.\nHowever, existing benchmarks do not systematically evaluate whether these\nmodels truly reason like human clinicians or merely imitate superficial\npatterns. To address this gap, we propose DrVD-Bench, the first multimodal\nbenchmark for clinical visual reasoning. DrVD-Bench consists of three modules:\nVisual Evidence Comprehension, Reasoning Trajectory Assessment, and Report\nGeneration Evaluation, comprising a total of 7,789 image-question pairs. Our\nbenchmark covers 20 task types, 17 diagnostic categories, and five imaging\nmodalities-CT, MRI, ultrasound, radiography, and pathology. DrVD-Bench is\nexplicitly structured to reflect the clinical reasoning workflow from modality\nrecognition to lesion identification and diagnosis. We benchmark 19 VLMs,\nincluding general-purpose and medical-specific, open-source and proprietary\nmodels, and observe that performance drops sharply as reasoning complexity\nincreases. While some models begin to exhibit traces of human-like reasoning,\nthey often still rely on shortcut correlations rather than grounded visual\nunderstanding. DrVD-Bench offers a rigorous and structured evaluation framework\nto guide the development of clinically trustworthy VLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.24173v1;http://arxiv.org/pdf/2505.24173v1", "pdf_url": "http://arxiv.org/pdf/2505.24173v1"}, {"title": "Anatomical Consistency and Adaptive Prior-informed Transformation for Multi-contrast MR Image Synthesis via Diffusion Model", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Shin_Anatomical_Consistency_and_Adaptive_Prior-informed_Transformation_for_Multi-contrast_MR_Image_CVPR_2025_paper.pdf", "details": "Y Shin, Y Lee, H Jang, G Son, H Kim, D Hwang - \u2026 of the Computer Vision and Pattern \u2026, 2025", "abstract": "Multi-contrast magnetic resonance (MR) images offer critical diagnostic information but are limited by long scan times and high cost. While diffusion models (DMs) excel in medical image synthesis, they often struggle to maintain anatomical consistency \u2026"}]
