[{"title": "Stratified Prediction-Powered Inference for Effective Hybrid Evaluation of Language Models", "link": "https://openreview.net/pdf%3Fid%3D8CBcdDQFDQ", "details": "A Fisch, J Maynez, RA Hofer, B Dhingra, A Globerson\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Prediction-powered inference (PPI) is a method that improves statistical estimates based on limited human-labeled data. PPI achieves this by combining small amounts of human-labeled data with larger amounts of data labeled by a reasonably accurate \u2026"}, {"title": "Classification Done Right for Vision-Language Pre-Training", "link": "https://openreview.net/pdf%3Fid%3DHd2EOwKItm", "details": "Z Huang, Q Ye, B Kang, J Feng, H Fan - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data. Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised \u2026"}, {"title": "GraphVis: Boosting LLMs with Visual Knowledge Graph Integration", "link": "https://openreview.net/pdf%3Fid%3DhaVPmN8UGi", "details": "Y Deng, C Ye, Z Huang, MD Ma, Y Kou, W Wang - The Thirty-eighth Annual Conference on \u2026", "abstract": "The rapid evolution of large language models (LLMs) has expanded their capabilities across various data modalities, extending from well-established image data to increasingly popular graph data. Given the limitation of LLMs in \u2026"}, {"title": "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models", "link": "https://aclanthology.org/2024.emnlp-main.210.pdf", "details": "J Wang, F Mo, W Ma, P Sun, M Zhang, JY Nie - \u2026 of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Large language models (LLMs) are essential tools that users employ across various scenarios, so evaluating their performance and guiding users in selecting the suitable service is important. Although many benchmarks exist, they mainly focus on \u2026"}, {"title": "Guided Profile Generation Improves Personalization with Large Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.231.pdf", "details": "J Zhang - Findings of the Association for Computational \u2026, 2024", "abstract": "In modern commercial systems, including Recommendation, Ranking, and E- Commerce platforms, there is a trend towards improving customer experiences by incorporating Personalization context as input into Large Language Models (LLM) \u2026"}, {"title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "link": "https://openreview.net/pdf%3Fid%3DIRXyPm9IPW", "details": "X Wu, S Huang, G Wang, J Xiong, F Wei - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts. Despite these advances \u2026"}, {"title": "BAdam: A memory efficient full parameter optimization method for large language models", "link": "https://openreview.net/pdf%3Fid%3D0uXtFk5KNJ", "details": "Q Luo, H Yu, X Li - The Thirty-eighth Annual Conference on Neural \u2026, 2024", "abstract": "This work presents BAdam, an optimization method that leverages the block coordinate descent (BCD) framework with Adam's update rule. BAdam offers a memory efficient approach to the full parameter finetuning of large language models \u2026"}, {"title": "PhoneLM: an Efficient and Capable Small Language Model Family through Principled Pre-training", "link": "https://arxiv.org/pdf/2411.05046", "details": "R Yi, X Li, W Xie, Z Lu, C Wang, A Zhou, S Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The interest in developing small language models (SLM) for on-device deployment is fast growing. However, the existing SLM design hardly considers the device hardware characteristics. Instead, this work presents a simple yet effective principle \u2026"}, {"title": "Bias Amplification in Language Model Evolution: An Iterated Learning Perspective", "link": "https://openreview.net/pdf%3Fid%3DBSYn7ah4KX", "details": "Y Ren, S Guo, L Qiu, B Wang, DJ Sutherland - The Thirty-eighth Annual Conference on \u2026", "abstract": "With the widespread adoption of Large Language Models (LLMs), the prevalence of iterative interactions among these models is anticipated to increase. Notably, recent advancements in multi-round on-policy self-improving methods allow LLMs to \u2026"}]
