[{"title": "Extracting Systemic Anticancer Therapy and Response Information From Clinical Notes Following the RECIST Definition", "link": "https://ascopubs.org/doi/abs/10.1200/CCI.23.00166", "details": "X Zuo, A Kumar, S Shen, J Li, G Cong, E Jin, Q Chen\u2026 - JCO Clinical Cancer \u2026, 2024", "abstract": "PURPOSE The RECIST guidelines provide a standardized approach for evaluating the response of cancer to treatment, allowing for consistent comparison of treatment efficacy across different therapies and patients. However, collecting such information \u2026"}, {"title": "$\\texttt {MoE-RBench} $: Towards Building Reliable Language Models with Sparse Mixture-of-Experts", "link": "https://openreview.net/pdf%3Fid%3DLyJ85kgHFe", "details": "G Chen, X Zhao, T Chen, Y Cheng - Forty-first International Conference on Machine \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new \u2026"}, {"title": "ViGLUE: A Vietnamese General Language Understanding Benchmark and Analysis of Vietnamese Language Models", "link": "https://aclanthology.org/2024.findings-naacl.261.pdf", "details": "MN Tran, PV Nguyen, L Nguyen, D Dien - Findings of the Association for \u2026, 2024", "abstract": "As the number of language models has increased, various benchmarks have been suggested to assess the proficiency of the models in natural language understanding. However, there is a lack of such a benchmark in Vietnamese due to \u2026"}, {"title": "P3Sum: Preserving Author's Perspective in News Summarization with Diffusion Language Models", "link": "https://aclanthology.org/2024.naacl-long.119.pdf", "details": "Y Liu, S Feng, X Han, V Balachandran, CY Park\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "In this work, we take a first step towards designing summarization systems that are faithful to the author's intent, not only the semantic content of the article. Focusing on a case study of preserving political perspectives in news summarization, we find that \u2026"}, {"title": "PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models", "link": "https://aclanthology.org/2024.naacl-long.336.pdf", "details": "HJ Kim, YJ Kim, JY Bak - Proceedings of the 2024 Conference of the North \u2026, 2024", "abstract": "Pre-trained language models (PLMs) show impressive performance in various downstream NLP tasks. However, pre-training large language models demands substantial memory and training compute. Furthermore, due to the substantial \u2026"}, {"title": "Language Models can be Deductive Solvers", "link": "https://aclanthology.org/2024.findings-naacl.254.pdf", "details": "J Feng, R Xu, J Hao, H Sharma, Y Shen, D Zhao\u2026 - Findings of the Association \u2026, 2024", "abstract": "Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit \u2026"}, {"title": "No Context Needed: Contextual Quandary In Idiomatic Reasoning With Pre-Trained Language Models", "link": "https://aclanthology.org/2024.naacl-long.272.pdf", "details": "K Cheng, S Bhat - Proceedings of the 2024 Conference of the North \u2026, 2024", "abstract": "Abstract Reasoning in the presence of idiomatic expressions (IEs) remains a challenging frontier in natural language understanding (NLU). Unlike standard text, the non-compositional nature of an IE makes it difficult for model comprehension, as \u2026"}, {"title": "Analysing zero-shot temporal relation extraction on clinical notes using temporal consistency", "link": "https://arxiv.org/abs/2406.11486", "details": "V Kougia, A Sedova, A Stephan, K Zaporojets, B Roth - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper presents the first study for temporal relation extraction in a zero-shot setting focusing on biomedical text. We employ two types of prompts and five LLMs (GPT-3.5, Mixtral, Llama 2, Gemma, and PMC-LLaMA) to obtain responses about the \u2026"}, {"title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models", "link": "https://aclanthology.org/2024.findings-naacl.18.pdf", "details": "Z Su, Z Lin, B Baixue, H Chen, S Hu, W Zhou, G Ding\u2026 - Findings of the Association \u2026, 2024", "abstract": "Generative language models are usually pre-trained on large text corpus via predicting the next token (ie, sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language \u2026"}]
