[{"title": "Accelerating Multimodel Large Language Models by Searching Optimal Vision Token Reduction", "link": "https://arxiv.org/pdf/2412.00556", "details": "S Zhao, Z Wang, F Juefei-Xu, X Xia, M Liu, X Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Prevailing Multimodal Large Language Models (MLLMs) encode the input image (s) as vision tokens and feed them into the language backbone, similar to how Large Language Models (LLMs) process the text tokens. However, the number of vision \u2026"}, {"title": "Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2412.05934", "details": "M Teng, J Xiaojun, D Ranjie, L Xinfeng, H Yihao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the rapid advancement of multimodal large language models (MLLMs), concerns regarding their security have increasingly captured the attention of both academia and industry. Although MLLMs are vulnerable to jailbreak attacks \u2026"}, {"title": "ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?", "link": "https://arxiv.org/pdf/2412.02368", "details": "L Zhang, S Eger, Y Cheng, W Zhai, J Belouadi, C Leiter\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal large language models (LLMs) have demonstrated impressive capabilities in generating high-quality images from textual instructions. However, their performance in generating scientific images--a critical application for \u2026"}, {"title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases", "link": "https://arxiv.org/pdf/2412.04862", "details": "LG Research, S An, K Bae, E Choi, K Choi, SJ Choi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8 B, and 2.4 B. These models feature several \u2026"}, {"title": "Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective", "link": "https://arxiv.org/pdf/2412.00554", "details": "Y Zhou, B Di Eugenio, L Cheng - arXiv preprint arXiv:2412.00554, 2024", "abstract": "This paper studies the performance of large language models (LLMs), particularly regarding demographic fairness, in solving real-world healthcare tasks. We evaluate state-of-the-art LLMs with three prevalent learning frameworks across six diverse \u2026"}, {"title": "Temporal Insight Enhancement: Mitigating Temporal Hallucination in Video Understanding by Multimodal Large Language Models", "link": "https://link.springer.com/chapter/10.1007/978-3-031-78183-4_29", "details": "L Sun, L Wang, J Sun, T Okatani - International Conference on Pattern Recognition, 2025", "abstract": "Abstract Recent advancements in Multimodal Large Language Models (MLLMs) have enabled to process diverse input modalities, leading to significantly better understanding of multimedia contents. However, understanding videos is still difficult \u2026"}, {"title": "Minerva LLMs: The First Family of Large Language Models Trained from Scratch on Italian Data", "link": "https://clic2024.ilc.cnr.it/wp-content/uploads/2024/12/76_main_long.pdf", "details": "R Orlando, L Moroni, PLH Cabot, E Barba, S Conia\u2026 - Proc. of CLiC-it, 2024", "abstract": "The growing interest in Large Language Models (LLMs) has accelerated research efforts to adapt these models for various languages. Despite this, pretraining LLMs from scratch for non-English languages remains underexplored. This is the case for \u2026"}, {"title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models", "link": "https://arxiv.org/pdf/2411.19477", "details": "Y Chen, X Pan, Y Li, B Ding, J Zhou - arXiv preprint arXiv:2411.19477, 2024", "abstract": "We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates $ N $ candidate solutions, and then chooses the \u2026"}, {"title": "Enhancing Visual Reasoning with Autonomous Imagination in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2411.18142", "details": "J Liu, Y Li, B Xiao, Y Jian, Z Qin, T Shao, YX Ding\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "There have been recent efforts to extend the Chain-of-Thought (CoT) paradigm to Multimodal Large Language Models (MLLMs) by finding visual clues in the input scene, advancing the visual reasoning ability of MLLMs. However, current \u2026"}]
