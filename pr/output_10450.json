[{"title": "BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models", "link": "https://arxiv.org/pdf/2411.15232", "details": "T Koleilat, H Asgariandehkordi, H Rivaz, Y Xiao - arXiv preprint arXiv:2411.15232, 2024", "abstract": "Recent advancements in vision-language models (VLMs), such as CLIP, have demonstrated substantial success in self-supervised representation learning for vision tasks. However, effectively adapting VLMs to downstream applications \u2026"}, {"title": "Early detection of heart failure using in-patient longitudinal electronic health records", "link": "https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0314145", "details": "I Drozdov, B Szubert, C Murphy, K Brooksbank\u2026 - PloS one, 2024", "abstract": "Heart Failure (HF) is common, with worldwide prevalence of 1%-3% and a lifetime risk of 20% for individuals 40 years or older. Despite its considerable health economic burden, techniques for early detection of HF in the general population are \u2026"}, {"title": "Generating synthetic clinical text with local large language models to identify misdiagnosed limb fractures in radiology reports", "link": "https://www.sciencedirect.com/science/article/pii/S0933365724002690", "details": "J Liu, B Koopman, NJ Brown, K Chu, A Nguyen - Artificial Intelligence in Medicine, 2024", "abstract": "Large language models (LLMs) demonstrate impressive capabilities in generating human-like content and have much potential to improve the performance and efficiency of healthcare. An important application of LLMs is to generate synthetic \u2026"}, {"title": "iPrOp: Interactive Prompt Optimization for Large Language Models with a Human in the Loop", "link": "https://arxiv.org/pdf/2412.12644", "details": "J Li, R Klinger - arXiv preprint arXiv:2412.12644, 2024", "abstract": "Prompt engineering has made significant contributions to the era of large language models, yet its effectiveness depends on the skills of a prompt author. Automatic prompt optimization can support the prompt development process, but requires \u2026"}, {"title": "Core Context Aware Attention for Long Context Language Modeling", "link": "https://arxiv.org/pdf/2412.12465", "details": "Y Chen, Z You, S Zhang, H Li, Y Li, Y Wang, M Tan - arXiv preprint arXiv:2412.12465, 2024", "abstract": "Transformer-based Large Language Models (LLMs) have exhibited remarkable success in various natural language processing tasks primarily attributed to self- attention mechanism, which requires a token to consider all preceding tokens as its \u2026"}]
