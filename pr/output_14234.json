[{"title": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates", "link": "https://arxiv.org/pdf/2503.16334", "details": "Y Shen, L Huang - arXiv preprint arXiv:2503.16334, 2025", "abstract": "Recent findings reveal that much of the knowledge in a Transformer-based Large Language Model (LLM) is encoded in its feed-forward (FFN) layers, where each FNN layer can be interpreted as the summation of sub-updates, each corresponding to a \u2026"}, {"title": "Stackelberg Game Preference Optimization for Data-Efficient Alignment of Language Models", "link": "https://arxiv.org/pdf/2502.18099", "details": "X Chu, Z Zhang, T Jia, Y Jin - arXiv preprint arXiv:2502.18099, 2025", "abstract": "Aligning language models with human preferences is critical for real-world deployment, but existing methods often require large amounts of high-quality human annotations. Aiming at a data-efficient alignment method, we propose Stackelberg \u2026"}, {"title": "Growing a Twig to Accelerate Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.14075", "details": "Z Shao, M Wang, Z Yu, W Pan, Y Yang, T Wei, H Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large vision-language models (VLMs) have demonstrated remarkable capabilities in open-world multimodal understanding, yet their high computational overheads pose great challenges for practical deployment. Some recent works have proposed \u2026"}, {"title": "Auditing language models for hidden objectives", "link": "https://arxiv.org/pdf/2503.10965%3F", "details": "S Marks, J Treutlein, T Bricken, J Lindsey, J Marcus\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We study the feasibility of conducting alignment audits: investigations into whether models have undesired objectives. As a testbed, we train a language model with a hidden objective. Our training pipeline first teaches the model about exploitable \u2026"}, {"title": "Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment", "link": "https://arxiv.org/pdf/2503.04647", "details": "W Yang, J Wu, C Wang, C Zong, J Zhang - arXiv preprint arXiv:2503.04647, 2025", "abstract": "Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences. While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is \u2026"}, {"title": "An astronomical question answering dataset for evaluating large language models", "link": "https://www.nature.com/articles/s41597-025-04613-9", "details": "J Li, F Zhao, P Chen, J Xie, X Zhang, H Li, M Chen\u2026 - Scientific Data, 2025", "abstract": "Large language models (LLMs) have recently demonstrated exceptional capabilities across a variety of linguistic tasks including question answering (QA). However, it remains challenging to assess their performance in astronomical QA due to the lack \u2026"}, {"title": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning", "link": "https://arxiv.org/pdf/2502.18439", "details": "C Park, S Han, X Guo, A Ozdaglar, K Zhang, JK Kim - arXiv preprint arXiv:2502.18439, 2025", "abstract": "Leveraging multiple large language models (LLMs) to build collaborative multi- agentic workflows has demonstrated significant potential. However, most previous studies focus on prompting the out-of-the-box LLMs, relying on their innate capability \u2026"}, {"title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for Multilingual Medical Question Answering", "link": "https://arxiv.org/pdf/2503.16131", "details": "F Li, Y Chen, H Liu, R Yang, H Yuan, Y Jiang, T Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have shown remarkable progress in medical question answering (QA), yet their effectiveness remains predominantly limited to English due to imbalanced multilingual training data and scarce medical resources \u2026"}, {"title": "ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2502.19409", "details": "DS Villegas, I Ziegler, D Elliott - arXiv preprint arXiv:2502.19409, 2025", "abstract": "Reasoning over sequences of images remains a challenge for multimodal large language models (MLLMs). While recent models incorporate multi-image data during pre-training, they still struggle to recognize sequential structures, often treating \u2026"}]
