[{"title": "Seal: Advancing Speech Language Models to be Few-Shot Learners", "link": "https://arxiv.org/pdf/2407.14875", "details": "S Lei, L Liu, J Yang, Y Jiao, Y Yang, Y Yang, X Guo - arXiv preprint arXiv:2407.14875, 2024", "abstract": "Existing auto-regressive language models have demonstrated a remarkable capability to perform a new task with just a few examples in prompt, without requiring any additional training. In order to extend this capability to a multi-modal setting (ie \u2026"}, {"title": "Evaluating Behaviors of General Purpose Language Models in a Pedagogical Context", "link": "https://link.springer.com/chapter/10.1007/978-3-031-64299-9_4", "details": "S Karumbaiah, A Ganesh, A Bharadwaj, L Anderson - International Conference on \u2026, 2024", "abstract": "Abstract General-purpose Language Models (LMs) bypass the need for task-specific model training by allowing textual prompts to specify a downstream task (eg, assessment, feedback generation). One of the main benefits of using a prompt-based \u2026"}, {"title": "Extracting and Encoding: Leveraging Large Language Models and Medical Knowledge to Enhance Radiological Text Representation", "link": "https://arxiv.org/pdf/2407.01948", "details": "P Messina, R Vidal, D Parra, \u00c1 Soto, V Araujo - arXiv preprint arXiv:2407.01948, 2024", "abstract": "Advancing representation learning in specialized fields like medicine remains challenging due to the scarcity of expert annotations for text and images. To tackle this issue, we present a novel two-stage framework designed to extract high-quality \u2026"}, {"title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application", "link": "https://arxiv.org/pdf/2407.01885", "details": "C Yang, W Lu, Y Zhu, Y Wang, Q Chen, C Gao, B Yan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have showcased exceptional capabilities in various domains, attracting significant interest from both academia and industry. Despite their impressive performance, the substantial size and computational demands of LLMs \u2026"}]
