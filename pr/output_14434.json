[{"title": "Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models", "link": "https://arxiv.org/pdf/2503.15888", "details": "B Bi, S Liu, Y Wang, Y Xu, J Fang, L Mei, X Cheng - arXiv preprint arXiv:2503.15888, 2025", "abstract": "Retrieval-Augmented Generation (RAG) mitigates hallucinations in Large Language Models (LLMs) by integrating external knowledge. However, conflicts between parametric knowledge and retrieved context pose challenges, particularly when \u2026"}, {"title": "White-box structure analysis of pre-trained language models of code for effective attacking", "link": "https://www.sciencedirect.com/science/article/pii/S0950584925000692", "details": "C Liu, X Ren, Y Xue - Information and Software Technology, 2025", "abstract": "Context: Pre-trained language models of code (PLMs-C for short) have dramatically improved the state-of-the-art on various programming language processing tasks. Objective: Due to these well-performed models being easily disturbed by slight \u2026"}, {"title": "Qwen2. 5-Omni Technical Report", "link": "https://arxiv.org/pdf/2503.20215", "details": "J Xu, Z Guo, J He, H Hu, T He, S Bai, K Chen, J Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this report, we present Qwen2. 5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming \u2026"}, {"title": "Safe RLHF-V: Safe Reinforcement Learning from Human Feedback in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2503.17682", "details": "J Ji, X Chen, R Pan, H Zhu, C Zhang, J Li, D Hong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal large language models (MLLMs) are critical for developing general- purpose AI assistants, yet they face growing safety risks. How can we ensure that MLLMs are safely aligned to prevent undesired behaviors such as discrimination \u2026"}, {"title": "QA-Calibration of language model confidence scores", "link": "https://www.amazon.science/publications/qa-calibration-of-language-model-confidence-scores", "details": "A Mastakouri, E Kirschbaum, S Kasiviswanathan\u2026 - 2025", "abstract": "To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim \u2026"}, {"title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models", "link": "https://arxiv.org/pdf/2503.18931", "details": "Y Chen, L Meng, W Peng, Z Wu, YG Jiang - arXiv preprint arXiv:2503.18931, 2025", "abstract": "Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of \u2026"}, {"title": "R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization", "link": "https://arxiv.org/pdf/2503.12937%3F", "details": "J Zhang, J Huang, H Yao, S Liu, X Zhang, S Lu, D Tao - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised fine- tuning on high-quality chain-of-thought reasoning data, which often leads models to merely imitate successful reasoning paths without understanding what the wrong \u2026"}, {"title": "Corrective In-Context Learning: Evaluating Self-Correction in Large Language Models", "link": "https://arxiv.org/pdf/2503.16022", "details": "M Sanz-Guerrero, K von der Wense - arXiv preprint arXiv:2503.16022, 2025", "abstract": "In-context learning (ICL) has transformed the use of large language models (LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled examples without finetuning. Despite its effectiveness, ICL is prone to errors, especially for \u2026"}, {"title": "ViLBench: A Suite for Vision-Language Process Reward Modeling", "link": "https://arxiv.org/pdf/2503.20271", "details": "H Tu, W Feng, H Chen, H Liu, X Tang, C Xie - arXiv preprint arXiv:2503.20271, 2025", "abstract": "Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on \u2026"}]
