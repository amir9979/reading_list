[{"title": "Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization", "link": "https://arxiv.org/pdf/2410.09302", "details": "G Liu, K Ji, R Zheng, Z Wu, C Dun, Q Gu, L Yan - arXiv preprint arXiv:2410.09302, 2024", "abstract": "Reinforcement Learning (RL) plays a crucial role in aligning large language models (LLMs) with human preferences and improving their ability to perform complex tasks. However, current approaches either require significant computational resources due \u2026"}, {"title": "Are Expert-Level Language Models Expert-Level Annotators?", "link": "https://arxiv.org/pdf/2410.03254", "details": "YM Tseng, WL Chen, CC Chen, HH Chen - arXiv preprint arXiv:2410.03254, 2024", "abstract": "Data annotation refers to the labeling or tagging of textual data with relevant information. A large body of works have reported positive results on leveraging LLMs as an alternative to human annotators. However, existing studies focus on classic \u2026"}, {"title": "Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models", "link": "https://arxiv.org/pdf/2410.02681%3F", "details": "S Wang, Y Li, H Wei - arXiv preprint arXiv:2410.02681, 2024", "abstract": "Confidence calibration is critical for the safe deployment of machine learning models in the real world. However, such issue in vision-language models like CLIP, particularly after fine-tuning, has not been fully addressed. In this work, we \u2026"}, {"title": "Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models", "link": "https://arxiv.org/pdf/2410.12011", "details": "K Tatariya, V Araujo, T Bauwens, M de Lhoneux - arXiv preprint arXiv:2410.12011, 2024", "abstract": "Pixel-based language models have emerged as a compelling alternative to subword- based language modelling, particularly because they can represent virtually any script. PIXEL, a canonical example of such a model, is a vision transformer that has \u2026"}, {"title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ", "link": "https://arxiv.org/pdf/2410.09613", "details": "A Poulis, E Tsalapati, M Koubarakis - arXiv preprint arXiv:2410.09613, 2024", "abstract": "Recent advancements in transformer-based language models have sparked research into their logical reasoning capabilities. Most of the benchmarks used to evaluate these models are simple: generated from short (fragments of) first-order \u2026"}, {"title": "Medqa-cs: Benchmarking large language models clinical skills using an ai-sce framework", "link": "https://arxiv.org/pdf/2410.01553%3F", "details": "Z Yao, Z Zhang, C Tang, X Bian, Y Zhao, Z Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Artificial intelligence (AI) and large language models (LLMs) in healthcare require advanced clinical skills (CS), yet current benchmarks fail to evaluate these comprehensively. We introduce MedQA-CS, an AI-SCE framework inspired by \u2026"}, {"title": "Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback", "link": "https://arxiv.org/pdf/2410.05434", "details": "S Choudhury, P Sodhi - arXiv preprint arXiv:2410.05434, 2024", "abstract": "While large language models (LLMs) show impressive decision-making abilities, current methods lack a mechanism for automatic self-improvement from errors during task execution. We propose LEAP, an iterative fine-tuning framework that continually \u2026"}, {"title": "TABCF: Counterfactual Explanations for Tabular Data Using a Transformer-Based VAE", "link": "https://arxiv.org/pdf/2410.10463", "details": "E Panagiotou, M Heurich, T Landgraf, E Ntoutsi - arXiv preprint arXiv:2410.10463, 2024", "abstract": "In the field of Explainable AI (XAI), counterfactual (CF) explanations are one prominent method to interpret a black-box model by suggesting changes to the input that would alter a prediction. In real-world applications, the input is predominantly in \u2026"}, {"title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models", "link": "https://arxiv.org/pdf/2410.01335", "details": "L Bandarkar, B Muller, P Yuvraj, R Hou, N Singhal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large \u2026"}]
