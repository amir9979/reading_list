[{"title": "Efficient Vision-Language Pre-training by Cluster Masking", "link": "https://arxiv.org/pdf/2405.08815", "details": "Z Wei, Z Pan, A Owens - arXiv preprint arXiv:2405.08815, 2024", "abstract": "We propose a simple strategy for masking image patches during visual-language contrastive learning that improves the quality of the learned representations and the training speed. During each iteration of training, we randomly mask clusters of \u2026"}, {"title": "HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models", "link": "https://arxiv.org/pdf/2405.10299", "details": "RS Sukthanker, A Zela, B Staffler, JKH Franke, F Hutter - arXiv preprint arXiv \u2026, 2024", "abstract": "The expanding size of language models has created the necessity for a comprehensive examination across various dimensions that reflect the desiderata with respect to the tradeoffs between various hardware metrics, such as latency \u2026"}, {"title": "Align vision-language semantics by multi-task learning for multi-modal summarization", "link": "https://link.springer.com/article/10.1007/s00521-024-09908-3", "details": "C Cui, X Liang, S Wu, Z Li - Neural Computing and Applications, 2024", "abstract": "Most current multi-modal summarization methods follow a cascaded manner, where an off-the-shelf object detector is first used to extract visual features. After that, these visual features are fused with language representations for the decoder to generate \u2026"}, {"title": "MEDVOC: Vocabulary Adaptation for Fine-tuning Pre-trained Language Models on Medical Text Summarization", "link": "https://arxiv.org/pdf/2405.04163", "details": "G Balde, S Roy, M Mondal, N Ganguly - arXiv preprint arXiv:2405.04163, 2024", "abstract": "This work presents a dynamic vocabulary adaptation strategy, MEDVOC, for fine- tuning pre-trained language models (PLMs) like BertSumAbs, BART, and PEGASUS for improved medical text summarization. In contrast to existing domain adaptation \u2026"}, {"title": "Structural Pruning of Pre-trained Language Models via Neural Architecture Search", "link": "https://arxiv.org/pdf/2405.02267", "details": "A Klein, J Golebiowski, X Ma, V Perrone\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models (PLM), for example BERT or RoBERTa, mark the state- of-the-art for natural language understanding task when fine-tuned on labeled data. However, their large size poses challenges in deploying them for inference in real \u2026"}, {"title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision", "link": "https://arxiv.org/pdf/2405.04086", "details": "Y Tong, S Wang, D Li, Y Wang, S Han, Z Lin, C Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations \u2026"}, {"title": "Joint semi-supervised and contrastive learning enables zero-shot domain-adaptation and multi-domain segmentation", "link": "https://arxiv.org/pdf/2405.05336", "details": "A Gomariz, Y Kikuchi, YY Li, T Albrecht, A Maunz\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their effectiveness, current deep learning models face challenges with images coming from different domains with varying appearance and content. We introduce SegCLR, a versatile framework designed to segment volumetric images \u2026"}, {"title": "Bridge and Hint: Extending Pre-trained Language Models for Long-Range Code", "link": "https://arxiv.org/pdf/2405.11233", "details": "Y Chen, C Gao, Z Yang, H Zhang, Q Liao - arXiv preprint arXiv:2405.11233, 2024", "abstract": "In the field of code intelligence, effectively modeling long-range code poses a significant challenge. Existing pre-trained language models (PLMs) such as UniXcoder have achieved remarkable success, but they still face difficulties with long \u2026"}, {"title": "Modeling Caption Diversity in Contrastive Vision-Language Pretraining", "link": "https://arxiv.org/pdf/2405.00740", "details": "S Lavoie, P Kirichenko, M Ibrahim, M Assran\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "There are a thousand ways to caption an image. Contrastive Language Pretraining (CLIP) on the other hand, works by mapping an image and its caption to a single vector--limiting how well CLIP-like models can represent the diverse ways to \u2026"}]
