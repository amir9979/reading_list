[{"title": "Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance", "link": "https://arxiv.org/pdf/2406.00644", "details": "J Li, T Su, B Zhao, F Lv, Q Wang, N Navab, Y Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Automatic report generation has arisen as a significant research area in computer- aided diagnosis, aiming to alleviate the burden on clinicians by generating reports automatically based on medical images. In this work, we propose a novel framework \u2026"}, {"title": "Compressing large language models by joint sparsification and quantization", "link": "https://openreview.net/pdf%3Fid%3DsCGRhnuMUJ", "details": "J Guo, J Wu, Z Wang, J Liu, G Yang, Y Ding, R Gong\u2026 - Forty-first International \u2026, 2024", "abstract": "In this paper, we introduce a novel model compression technique named Joint Sparsification and Quantization (JSQ), explicitly tailored for large language models (LLMs). Traditional methods employ either sparsification or quantization individually \u2026"}, {"title": "Improved few-shot jailbreaking can circumvent aligned language models and their defenses", "link": "https://arxiv.org/pdf/2406.01288", "details": "X Zheng, T Pang, C Du, Q Liu, J Jiang, M Lin - arXiv preprint arXiv:2406.01288, 2024", "abstract": "Recently, Anil et al.(2024) show that many-shot (up to hundreds of) demonstrations can jailbreak state-of-the-art LLMs by exploiting their long-context capability. Nevertheless, is it possible to use few-shot demonstrations to efficiently jailbreak \u2026"}, {"title": "Large Language Models are Zero-Shot Next Location Predictors", "link": "https://arxiv.org/pdf/2405.20962", "details": "C Beneduce, B Lepri, M Luca - arXiv preprint arXiv:2405.20962, 2024", "abstract": "Predicting the locations an individual will visit in the future is crucial for solving many societal issues like disease diffusion and reduction of pollution among many others. The models designed to tackle next-location prediction, however, require a \u2026"}, {"title": "BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation", "link": "https://arxiv.org/pdf/2405.17039", "details": "C Jia, P Wang, Z Li, YC Li, Z Zhang, N Tang, Y Yu - arXiv preprint arXiv:2405.17039, 2024", "abstract": "Large language models (LLMs) have catalyzed a paradigm shift in natural language processing, yet their limited controllability poses a significant challenge for downstream applications. We aim to address this by drawing inspiration from the \u2026"}]
