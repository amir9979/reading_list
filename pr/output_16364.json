[{"title": "Evaluation and Bias Analysis of Large Language Models in Generating Synthetic Electronic Health Records: Comparative Study", "link": "https://www.jmir.org/2025/1/e65317/", "details": "R Huang, H Wu, Y Yuan, Y Xu, H Qian, C Zhang, X Wei\u2026 - Journal of Medical Internet \u2026, 2025", "abstract": "Background Synthetic electronic health records (EHRs) generated by large language models (LLMs) offer potential for clinical education and model training while addressing privacy concerns. However, performance variations and demographic \u2026"}, {"title": "Validation of a software application using electronic health records for automatic detection of community onset sepsis", "link": "https://www.nature.com/articles/s41598-025-99879-9", "details": "C Dur\u00e9, S Jonmarker, E Joelsson-Alm, H Nordqvist\u2026 - Scientific Reports, 2025", "abstract": "Our aim was to design and validate a software application, based on the Sepsis-3 criteria, capable of retrospectively identifying community-onset sepsis among emergency department patients requiring hospital admission. The application was \u2026"}, {"title": "MiMo: Unlocking the Reasoning Potential of Language Model--From Pretraining to Posttraining", "link": "https://arxiv.org/pdf/2505.07608", "details": "B Xia, B Shen, D Zhu, D Zhang, G Wang, H Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing \u2026"}, {"title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models", "link": "https://arxiv.org/pdf/2505.07591", "details": "J Ye, C Huang, Z Chen, W Fu, C Yang, L Yang, Y Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of \u2026"}]
