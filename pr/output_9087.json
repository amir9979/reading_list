[{"title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step", "link": "https://arxiv.org/pdf/2411.10440", "details": "G Xu, P Jin, L Hao, Y Song, L Sun, L Yuan - arXiv preprint arXiv:2411.10440, 2024", "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to \u2026"}, {"title": "Can Language Models Learn to Skip Steps?", "link": "https://arxiv.org/pdf/2411.01855%3F", "details": "T Liu, Q Guo, X Hu, C Jiayang, Y Zhang, X Qiu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Trained on vast corpora of human language, language models demonstrate emergent human-like reasoning abilities. Yet they are still far from true intelligence, which opens up intriguing opportunities to explore the parallels of humans and \u2026"}, {"title": "Information Extraction from Clinical Notes: Are We Ready to Switch to Large Language Models?", "link": "https://arxiv.org/pdf/2411.10020", "details": "Y Hu, X Zuo, Y Zhou, X Peng, J Huang, VK Keloth\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Backgrounds: Information extraction (IE) is critical in clinical natural language processing (NLP). While large language models (LLMs) excel on generative tasks, their performance on extractive tasks remains debated. Methods: We investigated \u2026"}, {"title": "Measuring Non-Adversarial Reproduction of Training Data in Large Language Models", "link": "https://arxiv.org/pdf/2411.10242", "details": "M Aerni, J Rando, E Debenedetti, N Carlini, D Ippolito\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models memorize parts of their training data. Memorizing short snippets and facts is required to answer questions about the world and to be fluent in any language. But models have also been shown to reproduce long verbatim \u2026"}, {"title": "Memorize and rank: Elevating large language models for clinical diagnosis prediction", "link": "https://openreview.net/pdf%3Fid%3DbO5UYHs6sn", "details": "MD Ma, X Wang, Y Xiao, A Cuturrufo, VS Nori\u2026 - GenAI for Health: Potential \u2026, 2024", "abstract": "Clinical diagnosis prediction models, when provided with a patient's medical history, aim to detect potential diseases early, facilitating timely intervention and improving prognostic outcomes. However, the inherent scarcity of patient data and large \u2026"}, {"title": "Retrieval In Decoder benefits generative models for explainable complex question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007573", "details": "J Feng, Q Wang, H Qiu, L Liu - Neural Networks, 2024", "abstract": "Abstract Large-scale Language Models (LLMs) utilizing the Chain-of-Thought prompting demonstrate exceptional performance in a variety of tasks. However, the persistence of factual hallucinations remains a significant challenge in practical \u2026"}, {"title": "LongReward: Improving Long-context Large Language Models with AI Feedback", "link": "https://arxiv.org/pdf/2410.21252", "details": "J Zhang, Z Hou, X Lv, S Cao, Z Hou, Y Niu, L Hou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT \u2026"}, {"title": "Flaming-hot Initiation with Regular Execution Sampling for Large Language Models", "link": "https://arxiv.org/pdf/2410.21236", "details": "W Chen, Z Zhang, G Liu, R Zheng, W Shi, C Dun, Z Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Since the release of ChatGPT, large language models (LLMs) have demonstrated remarkable capabilities across various domains. A key challenge in developing these general capabilities is efficiently sourcing diverse, high-quality data. This \u2026"}, {"title": "CoCoP: Enhancing Text Classification with LLM through Code Completion Prompt", "link": "https://arxiv.org/pdf/2411.08979", "details": "MM Mohajeri, MJ Dousti, MN Ahmadabadi - arXiv preprint arXiv:2411.08979, 2024", "abstract": "Text classification is a fundamental task in natural language processing (NLP), and large language models (LLMs) have demonstrated their capability to perform this task across various domains. However, the performance of LLMs heavily depends on \u2026"}]
