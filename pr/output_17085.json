[{"title": "TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning", "link": "https://arxiv.org/pdf/2505.23719", "details": "A Auer, P Podest, D Klotz, S B\u00f6ck, G Klambauer\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In-context learning, the ability of large language models to perform tasks using only examples provided in the prompt, has recently been adapted for time series forecasting. This paradigm enables zero-shot prediction, where past values serve as \u2026", "entry_id": "http://arxiv.org/abs/2505.23719v1", "updated": "2025-05-29 17:52:10", "published": "2025-05-29 17:52:10", "authors": "Andreas Auer;Patrick Podest;Daniel Klotz;Sebastian B\u00f6ck;G\u00fcnter Klambauer;Sepp Hochreiter", "summary": "In-context learning, the ability of large language models to perform tasks\nusing only examples provided in the prompt, has recently been adapted for time\nseries forecasting. This paradigm enables zero-shot prediction, where past\nvalues serve as context for forecasting future values, making powerful\nforecasting tools accessible to non-experts and increasing the performance when\ntraining data are scarce. Most existing zero-shot forecasting approaches rely\non transformer architectures, which, despite their success in language, often\nfall short of expectations in time series forecasting, where recurrent models\nlike LSTMs frequently have the edge. Conversely, while LSTMs are well-suited\nfor time series modeling due to their state-tracking capabilities, they lack\nstrong in-context learning abilities. We introduce TiRex that closes this gap\nby leveraging xLSTM, an enhanced LSTM with competitive in-context learning\nskills. Unlike transformers, state-space models, or parallelizable RNNs such as\nRWKV, TiRex retains state-tracking, a critical property for long-horizon\nforecasting. To further facilitate its state-tracking ability, we propose a\ntraining-time masking strategy called CPM. TiRex sets a new state of the art in\nzero-shot time series forecasting on the HuggingFace benchmarks GiftEval and\nChronos-ZS, outperforming significantly larger models including TabPFN-TS\n(Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce)\nacross both short- and long-term forecasts.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.23719v1;http://arxiv.org/pdf/2505.23719v1", "pdf_url": "http://arxiv.org/pdf/2505.23719v1"}, {"title": "Uncertainty-aware traffic accident risk prediction via multi-view hypergraph contrastive learning", "link": "https://www.sciencedirect.com/science/article/pii/S156625352500404X", "details": "Y Zhang, G Shen, W Zhang, K Ning, R Jiang, X Kong - Information Fusion, 2025", "abstract": "Traffic accident prediction is crucial for maintaining safety in smart cities. Accurate prediction can significantly reduce casualties and economic losses, while alleviating public concerns about urban safety. However, achieving this is challenging. First \u2026"}, {"title": "Bayesian deep learning with particle flow using the exponential family of probability densities", "link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13479/1347909/Bayesian-deep-learning-with-particle-flow-using-the-exponential-family/10.1117/12.3050158.short", "details": "F Daum, L Dai, J Huang, A Noushin - Signal Processing, Sensor/Information Fusion \u2026, 2025", "abstract": "We derive a new theory of Bayesian deep learning with particle flow that exploits the exponential family of probability densities. This work applies the theory that was developed to solve the Zakai equation for nonlinear filtering and design a fixed finite \u2026"}, {"title": "A temporal dependency preserving approach for anomaly detection on multivariate time series", "link": "https://link.springer.com/article/10.1007/s11280-025-01346-y", "details": "SE Benkabou, K Benabdeslem, DEK Mansouri\u2026 - World Wide Web, 2025", "abstract": "Multivariate time series present significant methodological challenges for anomaly detection, primarily due to the intricate nature of their temporal dependencies and the dynamic interplay among variables. These complexities render traditional methods \u2026"}, {"title": "Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead $\\mathbf{\\texttt{O}}$ptimization", "link": "https://arxiv.org/pdf/2505.07675", "details": "S Kang, DB Lee, H Jang, SJ Hwang - arXiv preprint arXiv:2505.07675, 2025", "abstract": "Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resource \u2026", "entry_id": "http://arxiv.org/abs/2505.07675v1", "updated": "2025-05-12 15:39:51", "published": "2025-05-12 15:39:51", "authors": "Seongjae Kang;Dong Bok Lee;Hyungjoon Jang;Sung Ju Hwang", "summary": "Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\n$\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead\n$\\mathbf{\\texttt{O}}$ptimization ($\\mathbf{\\texttt{DHO}}$) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that $\\texttt{DHO}$ mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that $\\texttt{DHO}$\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters.", "comment": "41 pages, 19 figures, preprint", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2505.07675v1;http://arxiv.org/pdf/2505.07675v1", "pdf_url": "http://arxiv.org/pdf/2505.07675v1"}, {"title": "Out-of-Distribution Detection Using Knowledge Distillation", "link": "https://ieeexplore.ieee.org/abstract/document/11008432/", "details": "\u013d Kr\u00e1lik, P Tar\u00e1bek - 2025 35th International Conference Radioelektronika \u2026, 2025", "abstract": "Out-of-distribution (OOD) samples pose a critical challenge for deep neural networks deployed in real-world applications, as they can lead to unreliable or incorrect predictions. This paper introduces a novel method for OOD detection based on \u2026"}]
