[{"title": "How Well Can Vision Language Models See Image Details?", "link": "https://arxiv.org/pdf/2408.03940", "details": "C Gou, A Felemban, FF Khan, D Zhu, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Model-based Vision-Language Models (LLM-based VLMs) have demonstrated impressive results in various vision-language understanding tasks. However, how well these VLMs can see image detail beyond the semantic level \u2026"}, {"title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios", "link": "https://aclanthology.org/2024.findings-acl.230.pdf", "details": "L Lin, J Fu, P Liu, Q Li, Y Gong, J Wan, F Zhang\u2026 - Findings of the Association \u2026, 2024", "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local \u2026"}, {"title": "Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination", "link": "https://aclanthology.org/2024.acl-long.490.pdf", "details": "N Yang, T Kang, SJ Choi, H Lee, K Jung - Proceedings of the 62nd Annual Meeting of \u2026, 2024", "abstract": "Instruction-following language models often show undesirable biases. These undesirable biases may be accelerated in the real-world usage of language models, where a wide range of instructions is used through zero-shot example prompting. To \u2026"}, {"title": "Contrastive Learning on Medical Intents for Sequential Prescription Recommendation", "link": "https://arxiv.org/pdf/2408.10259", "details": "AH Moghaddam, MN Kerdabadi, M Liu, Z Yao - arXiv preprint arXiv:2408.10259, 2024", "abstract": "Recent advancements in sequential modeling applied to Electronic Health Records (EHR) have greatly influenced prescription recommender systems. While the recent literature on drug recommendation has shown promising performance, the study of \u2026"}, {"title": "MedSyn: LLM-based Synthetic Medical Text Generation Framework", "link": "https://arxiv.org/pdf/2408.02056", "details": "G Kumichev, P Blinov, Y Kuzkina, V Goncharov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating synthetic text addresses the challenge of data availability in privacy- sensitive domains such as healthcare. This study explores the applicability of synthetic data in real-world medical settings. We introduce MedSyn, a novel medical \u2026"}, {"title": "MixPrompt: Enhancing Generalizability and Adversarial Robustness for Vision-Language Models via Prompt Fusion", "link": "https://link.springer.com/chapter/10.1007/978-981-97-5606-3_28", "details": "H Fan, Z Ma, Y Li, R Tian, Y Chen, C Gao - International Conference on Intelligent \u2026, 2024", "abstract": "Abstract Pretrained Vision-Language Models (VLMs) like CLIP have exhibited remarkable capacities across downstream tasks, while their image encoders are vulnerable to adversarial examples. A recently introduced lightweight approach \u2026"}, {"title": "CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning", "link": "https://arxiv.org/pdf/2407.21011", "details": "Y Du, B Chang, NC Dvornek - arXiv preprint arXiv:2407.21011, 2024", "abstract": "Recent advancements in Contrastive Language-Image Pre-training (CLIP) have demonstrated notable success in self-supervised representation learning across various tasks. However, the existing CLIP-like approaches often demand extensive \u2026"}, {"title": "Entity Retrieval for Answering Entity-Centric Questions", "link": "https://arxiv.org/pdf/2408.02795", "details": "HS Shavarani, A Sarkar - arXiv preprint arXiv:2408.02795, 2024", "abstract": "The similarity between the question and indexed documents is a crucial factor in document retrieval for retrieval-augmented question answering. Although this is typically the only method for obtaining the relevant documents, it is not the sole \u2026"}, {"title": "EUDA: An Efficient Unsupervised Domain Adaptation via Self-Supervised Vision Transformer", "link": "https://arxiv.org/pdf/2407.21311", "details": "A Abedi, QM Wu, N Zhang, F Pourpanah - arXiv preprint arXiv:2407.21311, 2024", "abstract": "Unsupervised domain adaptation (UDA) aims to mitigate the domain shift issue, where the distribution of training (source) data differs from that of testing (target) data. Many models have been developed to tackle this problem, and recently vision \u2026"}]
