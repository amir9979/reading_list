[{"title": "Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning", "link": "https://arxiv.org/pdf/2502.19634", "details": "J Pan, C Liu, J Wu, F Liu, J Zhu, HB Li, C Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show \u2026"}, {"title": "Towards Statistical Factuality Guarantee for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2502.20560", "details": "Z Li, C Yan, NJ Jackson, W Cui, B Li, J Zhang, BA Malin - arXiv preprint arXiv \u2026, 2025", "abstract": "Advancements in Large Vision-Language Models (LVLMs) have demonstrated promising performance in a variety of vision-language tasks involving image- conditioned free-form text generation. However, growing concerns about \u2026"}, {"title": "Abn-BLIP: Abnormality-aligned Bootstrapping Language-Image Pre-training for Pulmonary Embolism Diagnosis and Report Generation from CTPA", "link": "https://arxiv.org/pdf/2503.02034", "details": "Z Zhong, Y Wang, L Bi, Z Ma, SH Ahn, CJ Mullin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical imaging plays a pivotal role in modern healthcare, with computed tomography pulmonary angiography (CTPA) being a critical tool for diagnosing pulmonary embolism and other thoracic conditions. However, the complexity of \u2026"}, {"title": "MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data with Vision Generation Task using Discrete Visual Representations", "link": "https://arxiv.org/pdf/2503.01019", "details": "Z Zhang, Y Yu, Y Chen, X Yang, SY Yeo - arXiv preprint arXiv:2503.01019, 2025", "abstract": "Despite significant progress in Vision-Language Pre-training (VLP), current approaches predominantly emphasize feature extraction and cross-modal comprehension, with limited attention to generating or transforming visual content \u2026"}, {"title": "Continual Pre-training of MoEs: How robust is your router?", "link": "https://arxiv.org/pdf/2503.05029", "details": "B Th\u00e9rien, C\u00c9 Joseph, Z Sarwar, A Panda, A Das\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Sparsely-activated Mixture of Experts (MoE) transformers are promising architectures for foundation models. Compared to dense transformers that require the same amount of floating point operations (FLOPs) per forward pass, MoEs benefit from \u2026"}, {"title": "CoCa-CXR: Contrastive Captioners Learn Strong Temporal Structures for Chest X-Ray Vision-Language Understanding", "link": "https://arxiv.org/pdf/2502.20509", "details": "Y Chen, S Xu, A Sellergren, Y Matias, A Hassidim\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models have proven to be of great benefit for medical image analysis since they learn rich semantics from both images and reports. Prior efforts have focused on better alignment of image and text representations to enhance \u2026"}, {"title": "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions", "link": "https://arxiv.org/pdf/2503.03278", "details": "J Li, C Liu, W Bai, R Arcucci, CI Bercea, JA Schnabel - arXiv preprint arXiv \u2026, 2025", "abstract": "Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains \u2026"}, {"title": "Multidimensional Consistency Improves Reasoning in Language Models", "link": "https://arxiv.org/pdf/2503.02670", "details": "H Lai, X Zhang, M Nissim - arXiv preprint arXiv:2503.02670, 2025", "abstract": "While Large language models (LLMs) have proved able to address some complex reasoning tasks, we also know that they are highly sensitive to input variation, which can lead to different solution paths and final answers. Answer consistency across \u2026"}, {"title": "Pathology Report Generation and Multimodal Representation Learning for Cutaneous Melanocytic Lesions", "link": "https://arxiv.org/pdf/2502.19293%3F", "details": "RT Lucassen, SPJ Moonemans, T van de Luijtgaarden\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Millions of melanocytic skin lesions are examined by pathologists each year, the majority of which concern common nevi (ie, ordinary moles). While most of these lesions can be diagnosed in seconds, writing the corresponding pathology report is \u2026"}]
