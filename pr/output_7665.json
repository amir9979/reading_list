[{"title": "Efficient Fine-Tuning for Low-Resource Tibetan Pre-trained Language Models", "link": "https://link.springer.com/chapter/10.1007/978-3-031-72350-6_28", "details": "M Zhou, Z Daiqing, N Qun, T Nyima - International Conference on Artificial Neural \u2026, 2024", "abstract": "For low-resource languages like Tibetan, the availability of pre-trained language models (PLMs) is severely limited both in quantity and performance. Therefore, it is crucial to explore the optimization of these limited PLMs. In this paper, leveraging the \u2026"}, {"title": "Retrieval-Pretrained Transformer: Long-range Language Modeling with Self-retrieval", "link": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00693/124629", "details": "O Rubin, J Berant - Transactions of the Association for Computational \u2026, 2024", "abstract": "Retrieval-augmented language models (LMs) have received much attention recently. However, typically the retriever is not trained jointly as a native component of the LM, but added post-hoc to an already-pretrained LM, which limits the ability of the LM and \u2026"}, {"title": "Comparative Analysis of Large Language Models in Chinese Medical Named Entity Recognition", "link": "https://www.mdpi.com/2306-5354/11/10/982", "details": "Z Zhu, Q Zhao, J Li, Y Ge, X Ding, T Gu, J Zou, S Lv\u2026 - Bioengineering, 2024", "abstract": "The emergence of large language models (LLMs) has provided robust support for application tasks across various domains, such as name entity recognition (NER) in the general domain. However, due to the particularity of the medical domain, the \u2026"}, {"title": "Debiasing large language models: research opportunities", "link": "https://www.tandfonline.com/doi/pdf/10.1080/03036758.2024.2398567", "details": "V Yogarajan, G Dobbie, TT Keegan - Journal of the Royal Society of New Zealand, 2024", "abstract": "Large language models (LLMs) are powerful decision-making tools widely adopted in healthcare, finance, and transportation. Embracing the opportunities and innovations of LLMs is inevitable. However, LLMs inherit stereotypes \u2026"}, {"title": "Revisiting In-context Learning Inference Circuit in Large Language Models", "link": "https://arxiv.org/pdf/2410.04468", "details": "H Cho, M Kato, Y Sakai, N Inoue - arXiv preprint arXiv:2410.04468, 2024", "abstract": "In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the \u2026"}, {"title": "The Role of Deductive and Inductive Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2410.02892", "details": "C Cai, X Zhao, H Liu, Z Jiang, T Zhang, Z Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have achieved substantial progress in artificial intelligence, particularly in reasoning tasks. However, their reliance on static prompt structures, coupled with limited dynamic reasoning capabilities, often constrains their \u2026"}, {"title": "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models", "link": "https://arxiv.org/pdf/2410.02308", "details": "R Meng, Y Liu, L Tu, D He, Y Zhou, S Yavuz - arXiv preprint arXiv:2410.02308, 2024", "abstract": "Phrases are fundamental linguistic units through which humans convey semantics. This study critically examines the capacity of API-based large language models (LLMs) to comprehend phrase semantics, utilizing three human-annotated datasets \u2026"}, {"title": "Evidence and axial attention guided Document-level Relation Extraction", "link": "https://www.sciencedirect.com/science/article/pii/S0885230824001116", "details": "J Yuan, H Leng, Y Qian, J Chen, M Ma, S Hou - Computer Speech & Language, 2024", "abstract": "Abstract Document-level Relation Extraction (DocRE) aims to identify semantic relations among multiple entity pairs within a document. Most of the previous DocRE methods take the entire document as input. However, for human annotators, a small \u2026"}]
