[{"title": "Can Language Models Reason about Individualistic Human Values and Preferences?", "link": "https://arxiv.org/pdf/2410.03868", "details": "L Jiang, T Sorensen, S Levine, Y Choi - arXiv preprint arXiv:2410.03868, 2024", "abstract": "Recent calls for pluralistic alignment emphasize that AI systems should address the diverse needs of all people. Yet, efforts in this space often require sorting people into fixed buckets of pre-specified diversity-defining dimensions (eg, demographics \u2026"}, {"title": "FLIP: Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction", "link": "https://dl.acm.org/doi/abs/10.1145/3640457.3688106", "details": "H Wang, J Lin, X Li, B Chen, C Zhu, R Tang, W Zhang\u2026 - Proceedings of the 18th \u2026, 2024", "abstract": "Click-through rate (CTR) prediction plays as a core function module in various personalized online services. The traditional ID-based models for CTR prediction take as inputs the one-hot encoded ID features of tabular modality, which capture the \u2026"}, {"title": "Instructing and Prompting Large Language Models for Explainable Cross-domain Recommendations", "link": "https://dl.acm.org/doi/abs/10.1145/3640457.3688137", "details": "A Petruzzelli, C Musto, L Laraspata, I Rinaldi\u2026 - Proceedings of the 18th \u2026, 2024", "abstract": "In this paper, we present a strategy to provide users with explainable cross-domain recommendations (CDR) that exploits large language models (LLMs). Generally speaking, CDR is a task that is hard to tackle, mainly due to data sparsity issues \u2026"}, {"title": "Harnessing Shared Relations via Multimodal Mixup Contrastive Learning for Multimodal Classification", "link": "https://arxiv.org/pdf/2409.17777", "details": "R Kumar, R Singhal, P Kulkarni, D Mehta, K Jadhav - arXiv preprint arXiv:2409.17777, 2024", "abstract": "Deep multimodal learning has shown remarkable success by leveraging contrastive learning to capture explicit one-to-one relations across modalities. However, real- world data often exhibits shared relations beyond simple pairwise associations. We \u2026"}, {"title": "Explainable Multi-agent Network for Multi-classification in Small Tabular Data", "link": "https://link.springer.com/chapter/10.1007/978-3-031-65038-3_5", "details": "M Bouskri, A Idrissi - Modern Artificial Intelligence and Data Science 2024 \u2026, 2024", "abstract": "Deep learning has gained tremendous success in recent years in most data types, but still outperformed by classical machine learning on tabular datasets, the most common data type, especially smaller ones. An explainable deep learning model \u2026"}, {"title": "Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on Generated Data", "link": "https://arxiv.org/pdf/2409.11423", "details": "A Akkus, M Li, J Chu, M Backes, Y Zhang, S Sav - arXiv preprint arXiv:2409.11423, 2024", "abstract": "Large language models (LLMs) have shown considerable success in a range of domain-specific tasks, especially after fine-tuning. However, fine-tuning with real- world data usually leads to privacy risks, particularly when the fine-tuning samples \u2026"}, {"title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks", "link": "https://arxiv.org/pdf/2409.07353", "details": "MZ Hossain, A Imteaj - arXiv preprint arXiv:2409.07353, 2024", "abstract": "Large Vision-Language Models (LVLMs), trained on multimodal big datasets, have significantly advanced AI by excelling in vision-language tasks. However, these models remain vulnerable to adversarial attacks, particularly jailbreak attacks, which \u2026"}, {"title": "Towards understanding evolution of science through language model series", "link": "https://arxiv.org/pdf/2409.09636", "details": "J Dong, Z Lyu, Q Ke - arXiv preprint arXiv:2409.09636, 2024", "abstract": "We introduce AnnualBERT, a series of language models designed specifically to capture the temporal evolution of scientific text. Deviating from the prevailing paradigms of subword tokenizations and\" one model to rule them all\", AnnualBERT \u2026"}, {"title": "A Comprehensive Analysis of Memorization in Large Language Models", "link": "https://aclanthology.org/2024.inlg-main.45.pdf", "details": "H Kiyomaru, I Sugiura, D Kawahara, S Kurohashi - Proceedings of the 17th \u2026, 2024", "abstract": "This paper presents a comprehensive study that investigates memorization in large language models (LLMs) from multiple perspectives. Experiments are conducted with the Pythia and LLM-jp model suites, both of which offer LLMs with over 10B \u2026"}]
