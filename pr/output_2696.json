[{"title": "Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI", "link": "https://openreview.net/pdf%3Fid%3DPrmxFWI1Fr", "details": "T Papamarkou, M Skoularidou, K Palla, L Aitchison\u2026 - Forty-first International \u2026, 2024", "abstract": "In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of \u2026"}, {"title": "ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification", "link": "https://arxiv.org/pdf/2405.14608", "details": "XM Le, L Luo, U Aickelin, MT Tran - arXiv preprint arXiv:2405.14608, 2024", "abstract": "Multivariate time series classification (MTSC) has attracted significant research attention due to its diverse real-world applications. Recently, exploiting transformers for MTSC has achieved state-of-the-art performance. However, existing methods \u2026"}, {"title": "Posterior Inference on Shallow Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance", "link": "https://openreview.net/pdf%3Fid%3DJ97bdMR7Lv", "details": "J Loria, A Bhadra - The 40th Conference on Uncertainty in Artificial \u2026", "abstract": "From the classical and influential works of Neal (1996), it is known that the infinite width scaling limit of a Bayesian neural network with one hidden layer is a Gaussian process, when the network weights have bounded prior variance. Neal's result has \u2026"}, {"title": "Gaussian processes based data augmentation and expected signature for time series classification", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10546274.pdf", "details": "F Triggiano, M Romito - IEEE Access, 2024", "abstract": "Time series classification tasks play a crucial role in extracting relevant information from data equipped with a temporal structure. In various scientific domains, such as biology or finance, this kind of data comes from complex and hardly predictable \u2026"}, {"title": "Zero-Shot Out-of-Distribution Detection with Outlier Label Exposure", "link": "https://arxiv.org/pdf/2406.01170", "details": "C Ding, G Pang - arXiv preprint arXiv:2406.01170, 2024", "abstract": "As vision-language models like CLIP are widely applied to zero-shot tasks and gain remarkable performance on in-distribution (ID) data, detecting and rejecting out-of- distribution (OOD) inputs in the zero-shot setting have become crucial for ensuring \u2026"}, {"title": "Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models", "link": "https://arxiv.org/pdf/2406.04320", "details": "A Behrouz, M Santacatterina, R Zabih - arXiv preprint arXiv:2406.04320, 2024", "abstract": "Modeling multivariate time series is a well-established problem with a wide range of applications from healthcare to financial markets. Traditional State Space Models (SSMs) are classical approaches for univariate time series modeling due to their \u2026"}, {"title": "Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation", "link": "https://arxiv.org/pdf/2406.06890", "details": "Y Zhai, K Lin, Z Yang, L Li, J Wang, CC Lin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Image diffusion distillation achieves high-fidelity generation with very few sampling steps. However, applying these techniques directly to video diffusion often results in unsatisfactory frame quality due to the limited visual quality in public video datasets \u2026"}, {"title": "Multivariate Time Series Modeling and Forecasting with Parallelized Convolution and Decomposed Sparse-Transformer", "link": "https://ieeexplore.ieee.org/abstract/document/10552140/", "details": "S Ma, YB Zhao, Y Kang, P Bai - IEEE Transactions on Artificial Intelligence, 2024", "abstract": "Many real-world scenarios require accurate predictions of time series, especially in the case of long sequence time-series forecasting (LSTF), such as predicting traffic flow and electricity consumption. However, existing time series prediction models \u2026"}, {"title": "SDformer: Transformer with Spectral Filter and Dynamic Attention for Multivariate Time Series Long-term Forecasting", "link": "https://gengyulyu.github.io/homepage/assets/pdf/SDformer.pdf", "details": "Z Zhou, G Lyu, Y Huang, Z Wang, Z Jia, Z Yang", "abstract": "Transformer has gained widespread adoption in modeling time series due to the exceptional ability of its self-attention mechanism in capturing long-range dependencies. However, when processing time series data with numerous variates \u2026"}]
