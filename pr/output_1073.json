'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Generalized Contrastive Learning for Multi-Modal Retri'
[{"title": "Empirical Analysis of Dialogue Relation Extraction with Large Language Models", "link": "https://arxiv.org/pdf/2404.17802", "details": "G Li, Z Xu, Z Shang, J Liu, K Ji, Y Guo - arXiv preprint arXiv:2404.17802, 2024", "abstract": "Dialogue relation extraction (DRE) aims to extract relations between two arguments within a dialogue, which is more challenging than standard RE due to the higher person pronoun frequency and lower information density in dialogues. However \u2026"}, {"title": "Advancing human-centric AI for robust X-ray analysis through holistic self-supervised learning", "link": "https://arxiv.org/pdf/2405.01469", "details": "T Moutakanni, P Bojanowski, G Chassagnon\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "AI Foundation models are gaining traction in various applications, including medical fields like radiology. However, medical foundation models are often tested on limited tasks, leaving their generalisability and biases unexplored. We present RayDINO, a \u2026"}, {"title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies", "link": "https://arxiv.org/pdf/2404.06395", "details": "S Hu, Y Tu, X Han, C He, G Cui, X Long, Z Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This \u2026"}, {"title": "CausalBench: A Comprehensive Benchmark for Causal Learning Capability of Large Language Models", "link": "https://arxiv.org/pdf/2404.06349", "details": "Y Zhou, X Wu, B Huang, J Wu, L Feng, KC Tan - arXiv preprint arXiv:2404.06349, 2024", "abstract": "Causality reveals fundamental principles behind data distributions in real-world scenarios, and the capability of large language models (LLMs) to understand causality directly impacts their efficacy across explaining outputs, adapting to new \u2026"}, {"title": "FREE: Faster and Better Data-Free Meta-Learning", "link": "https://arxiv.org/pdf/2405.00984", "details": "Y Wei, Z Hu, Z Wang, L Shen, C Yuan, D Tao - arXiv preprint arXiv:2405.00984, 2024", "abstract": "Data-Free Meta-Learning (DFML) aims to extract knowledge from a collection of pre- trained models without requiring the original data, presenting practical benefits in contexts constrained by data privacy concerns. Current DFML methods primarily \u2026"}, {"title": "Medical Image Analysis with Vision Transformers for Downstream Tasks and Clinical Report Generation", "link": "https://www.taylorfrancis.com/chapters/edit/10.1201/9781003407959-19/medical-image-analysis-vision-transformers-downstream-tasks-clinical-report-generation-evans-kotei-ramkumar-thirunavukarasu", "details": "E Kotei, R Thirunavukarasu - Intelligent Systems and Sustainable Computational \u2026", "abstract": "Transformer neural networks have performed so well in language processing tasks leading to the introduction of the vision transformer (ViT) network, an alternative technique to handle vision applications. The outstanding performance of the \u2026"}]
