[{"title": "Improving Context-Aware Preference Modeling for Language Models", "link": "https://arxiv.org/pdf/2407.14916", "details": "S Pitis, Z Xiao, NL Roux, A Sordoni - arXiv preprint arXiv:2407.14916, 2024", "abstract": "While finetuning language models from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where \u2026"}, {"title": "Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation with Small Language Models", "link": "https://arxiv.org/pdf/2407.16565", "details": "I Buhnila, A Sinha, M Constant - arXiv preprint arXiv:2407.16565, 2024", "abstract": "Recent surge in the accessibility of large language models (LLMs) to the general population can lead to untrackable use of such models for medical-related recommendations. Language generation via LLMs models has two key problems \u2026"}, {"title": "Toward User-centered Explainable Displays for Complex Machine Learning Models in Healthcare: A Case Study of Heart Disease Prediction", "link": "https://journals.sagepub.com/doi/abs/10.1177/10711813241264252", "details": "JK Nuamah - Proceedings of the Human Factors and Ergonomics \u2026, 2024", "abstract": "The current approaches to explaining black box machine learning models have primarily been based on the intuition of model developers, rather than being informed by end-user needs or existing literature. Our goal is to utilize existing \u2026"}, {"title": "Looking at Model Debiasing through the Lens of Anomaly Detection", "link": "https://arxiv.org/pdf/2407.17449", "details": "VP Pastore, M Ciranni, D Marinelli, F Odone, V Murino - arXiv preprint arXiv \u2026, 2024", "abstract": "It is widely recognized that deep neural networks are sensitive to bias in the data. This means that during training these models are likely to learn spurious correlations between data and labels, resulting in limited generalization abilities and low \u2026"}, {"title": "Learn to Preserve and Diversify: Parameter-Efficient Group with Orthogonal Regularization for Domain Generalization", "link": "https://arxiv.org/pdf/2407.15085", "details": "J Hu, J Zhang, L Qi, Y Shi, Y Gao - arXiv preprint arXiv:2407.15085, 2024", "abstract": "Domain generalization (DG) aims to avoid the performance degradation of the model when the distribution shift between the limited training data and unseen test data occurs. Recently, foundation models with enormous parameters have been pre \u2026"}, {"title": "Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?", "link": "https://arxiv.org/pdf/2407.17417", "details": "MA Panaitescu-Liess, Z Che, B An, Y Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in generating diverse and contextually rich text. However, concerns regarding copyright infringement arise as LLMs may inadvertently produce copyrighted material. In this \u2026"}, {"title": "How Useful is Continued Pre-Training for Generative Unsupervised Domain Adaptation?", "link": "https://aclanthology.org/2024.repl4nlp-1.9.pdf", "details": "R Uppaal, Y Li, J Hu - Proceedings of the 9th Workshop on Representation \u2026, 2024", "abstract": "Recent breakthroughs in scale have enabled the emergence of powerful generative language models, and the ability to fine-tune these models on various tasks by casting them into prompts or instructions. In this landscape, the problem of \u2026"}, {"title": "Fair Federated Learning with Biased Vision-Language Models", "link": "https://aclanthology.org/2024.findings-acl.595.pdf", "details": "H Zeng, Z Yue, Y Zhang, L Shang, D Wang - Findings of the Association for \u2026, 2024", "abstract": "Existing literature that integrates CLIP into federated learning (FL) largely ignores the inherent group unfairness within CLIP and its ethical implications on FL applications. Furthermore, such CLIP bias may be amplified in FL, due to the unique issue of data \u2026"}, {"title": "AbdomenAtlas: A large-scale, detailed-annotated, & multi-center dataset for efficient transfer learning and open algorithmic benchmarking", "link": "https://arxiv.org/pdf/2407.16697", "details": "W Li, C Qu, X Chen, PRAS Bassi, Y Shi, Y Lai, Q Yu\u2026 - Medical Image Analysis, 2024", "abstract": "We introduce the largest abdominal CT dataset (termed AbdomenAtlas) of 20,460 three-dimensional CT volumes sourced from 112 hospitals across diverse populations, geographies, and facilities. AbdomenAtlas provides 673 K high-quality \u2026"}]
