[{"title": "Every Expert Matters: Towards Effective Knowledge Distillation for Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2502.12947", "details": "G Kim, G Chu, E Yang - arXiv preprint arXiv:2502.12947, 2025", "abstract": "With the emergence of Mixture-of-Experts (MoE), the efficient scaling of model size has accelerated the development of large language models in recent years. However, their high memory requirements prevent their use in resource-constrained \u2026"}, {"title": "Self-supervised analogical learning using language models", "link": "https://arxiv.org/pdf/2502.00996", "details": "B Zhou, S Jain, Y Zhang, Q Ning, S Wang, Y Benajiba\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can \u2026"}, {"title": "Noise is an Efficient Learner for Zero-Shot Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06019", "details": "R Imam, A Hanif, J Zhang, KW Dawoud\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, test-time adaptation has garnered attention as a method for tuning models without labeled data. The conventional modus operandi for adapting pre-trained vision-language models (VLMs) during test-time primarily focuses on tuning \u2026"}, {"title": "Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images", "link": "https://arxiv.org/pdf/2502.13928", "details": "S Wu, FY Sun, K Wen, N Haber - arXiv preprint arXiv:2502.13928, 2025", "abstract": "Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises \u2026"}, {"title": "CSP-DCPE: Category-Specific Prompt with Deep Contextual Prompt Enhancement for Vision\u2013Language Models", "link": "https://www.mdpi.com/2079-9292/14/4/673", "details": "C Wu, Y Wu, Q Xu, X Zi - Electronics, 2025", "abstract": "Recently, prompt learning has emerged as a viable technique for fine-tuning pre- trained vision\u2013language models (VLMs). The use of prompts allows pre-trained VLMs to be quickly adapted to specific downstream tasks, bypassing the necessity to \u2026"}, {"title": "Mordal: Automated Pretrained Model Selection for Vision Language Models", "link": "https://arxiv.org/pdf/2502.00241", "details": "S He, I Jang, M Chowdhury - arXiv preprint arXiv:2502.00241, 2025", "abstract": "Incorporating multiple modalities into large language models (LLMs) is a powerful way to enhance their understanding of non-textual data, enabling them to perform multimodal tasks. Vision language models (VLMs) form the fastest growing category \u2026"}, {"title": "Optimizing Temperature for Language Models with Multi-Sample Inference", "link": "https://arxiv.org/pdf/2502.05234", "details": "W Du, Y Yang, S Welleck - arXiv preprint arXiv:2502.05234, 2025", "abstract": "Multi-sample aggregation strategies, such as majority voting and best-of-N sampling, are widely used in contemporary large language models (LLMs) to enhance predictive accuracy across various tasks. A key challenge in this process is \u2026"}, {"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}, {"title": "TabSD: Large Free-Form Table Question Answering with SQL-Based Table Decomposition", "link": "https://arxiv.org/pdf/2502.13422", "details": "Y Wang, J Gan, J Qi - arXiv preprint arXiv:2502.13422, 2025", "abstract": "Question answering on free-form tables (TableQA) is challenging due to the absence of predefined schemas and the presence of noise in large tables. While Large Language Models (LLMs) have shown promise in TableQA, they struggle with large \u2026"}]
