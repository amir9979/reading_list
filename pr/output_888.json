'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [MFORT-QA: Multi-hop Few-shot Open Rich Table Question '
[{"title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "link": "https://arxiv.org/pdf/2404.14219", "details": "M Abdin, SA Jacobs, AA Awan, J Aneja, A Awadallah\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT \u2026"}, {"title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI", "link": "https://arxiv.org/pdf/2404.16006", "details": "K Ying, F Meng, J Wang, Z Li, H Lin, Y Yang, H Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal \u2026"}, {"title": "VLRM: Vision-Language Models act as Reward Models for Image Captioning", "link": "https://arxiv.org/pdf/2404.01911", "details": "M Dzabraev, A Kunitsyn, A Ivaniuta - arXiv preprint arXiv:2404.01911, 2024", "abstract": "In this work, we present an unsupervised method for enhancing an image captioning model (in our case, BLIP2) using reinforcement learning and vision-language models like CLIP and BLIP2-ITM as reward models. The RL-tuned model is able to \u2026"}, {"title": "Conceptual and Unbiased Reasoning in Language Models", "link": "https://arxiv.org/pdf/2404.00205", "details": "B Zhou, H Zhang, S Chen, D Yu, H Wang, B Peng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Conceptual reasoning, the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on large language models' capability to perform conceptual reasoning. In this work, we \u2026"}, {"title": "Let's Think Dot by Dot: Hidden Computation in Transformer Language Models", "link": "https://arxiv.org/pdf/2404.15758", "details": "J Pfau, W Merrill, SR Bowman - arXiv preprint arXiv:2404.15758, 2024", "abstract": "Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater \u2026"}, {"title": "Prototype Similarity Distillation for Communication-Efficient Federated Unsupervised Representation Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10495206/", "details": "C Zhang, Y Xie, T Chen, W Mao, B Yu - IEEE Transactions on Knowledge and Data \u2026, 2024", "abstract": "Federated unsupervised representation learning aims at leveraging unlabeled data from multiple parties to learn visual representations without compromising the data privacy and tackle the non-IID challenge by aligning diverse representation spaces \u2026"}, {"title": "MAP: Model Aggregation and Personalization in Federated Learning with Incomplete Classes", "link": "https://arxiv.org/pdf/2404.09232", "details": "XC Li, S Song, Y Li, B Li, Y Shao, Y Yang, DC Zhan - IEEE Transactions on \u2026, 2024", "abstract": "In some real-world applications, data samples are usually distributed on local devices, where federated learning (FL) techniques are proposed to coordinate decentralized clients without directly sharing users' private data. FL commonly \u2026"}, {"title": "Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class", "link": "https://arxiv.org/pdf/2404.16717", "details": "M Moayeri, M Rabbat, M Ibrahim, D Bouchacourt - arXiv preprint arXiv:2404.16717, 2024", "abstract": "Vision-language models enable open-world classification of objects without the need for any retraining. While this zero-shot paradigm marks a significant advance, even today's best models exhibit skewed performance when objects are dissimilar from \u2026"}, {"title": "Large Language Models in Healthcare: A Comprehensive Benchmark", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/04/25/2024.04.24.24306315.full.pdf", "details": "F Liu, H Zhou, Y Hua, O Rohanian, L Clifton, D Clifton - medRxiv, 2024", "abstract": "The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question- answering task with answer options for evaluation. However, in real clinical settings \u2026"}]
