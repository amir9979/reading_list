[{"title": "A Survey of Attacks on Large Vision-Language Models: Resources, Advances, and Future Trends", "link": "https://arxiv.org/pdf/2407.07403", "details": "D Liu, M Yang, X Qu, P Zhou, W Hu, Y Cheng - arXiv preprint arXiv:2407.07403, 2024", "abstract": "With the significant development of large models in recent years, Large Vision- Language Models (LVLMs) have demonstrated remarkable capabilities across a wide range of multimodal understanding and reasoning tasks. Compared to \u2026"}, {"title": "Preprocessing Pathology Reports for Vision-Language Model Development", "link": "https://openreview.net/pdf%3Fid%3DSUgnMdiJ2q", "details": "R Lucassen, T van de Luijtgaarden, S Moonemans\u2026 - MICCAI Workshop on \u2026, 2024", "abstract": "Pathology reports are increasingly being used for development of vision-language models. Because the reports often include information that cannot directly be derived from paired images, careful selection of information is required to prevent \u2026"}, {"title": "STORYSUMM: Evaluating Faithfulness in Story Summarization", "link": "https://arxiv.org/pdf/2407.06501", "details": "M Subbiah, F Ladhak, A Mishra, G Adams, LB Chilton\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Human evaluation has been the gold standard for checking faithfulness in abstractive summarization. However, with a challenging source domain like narrative, multiple annotators can agree a summary is faithful, while missing details \u2026"}, {"title": "SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training", "link": "https://arxiv.org/pdf/2407.06654", "details": "N He, W Xiong, H Liu, Y Liao, L Ding, K Zhang, G Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The effectiveness of large language models (LLMs) is often hindered by duplicated data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, which risks the loss of valuable information and \u2026"}, {"title": "Aligning Language Models with the Human World", "link": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi%3Farticle%3D1241%26context%3Ddissertations", "details": "R LIU - 2024", "abstract": "Abstract The field of Natural Language Processing (NLP) has undergone a significant transformation with the emergence of large language models (LMs). These models have enabled the development of human-like conversational \u2026"}, {"title": "Towards inclusive biodesign and innovation: lowering barriers to entry in medical device development through large language model tools", "link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11268064/", "details": "JT Moon, NJ Lima, E Froula, H Li, J Newsome\u2026 - BMJ Health & Care \u2026, 2024", "abstract": "In the following narrative review, we discuss the potential role of large language models (LLMs) in medical device innovation, specifically examples using generative pretrained transformer-4. Throughout the biodesign process, LLMs can offer prompt \u2026"}, {"title": "A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding", "link": "https://arxiv.org/pdf/2407.01976", "details": "J Lu, H Yu, Y Wang, Y Ye, J Tang, Z Yang, B Wu, Q Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, many studies have demonstrated that exclusively incorporating OCR- derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate \u2026"}, {"title": "ALFREDO: Active Learning with FeatuRe disEntangelement and DOmain adaptation for medical image classification", "link": "https://www.sciencedirect.com/science/article/pii/S1361841524001865", "details": "D Mahapatra, R Tennakoon, Y George, S Roy\u2026 - Medical image analysis, 2024", "abstract": "State-of-the-art deep learning models often fail to generalize in the presence of distribution shifts between training (source) data and test (target) data. Domain adaptation methods are designed to address this issue using labeled samples \u2026"}]
