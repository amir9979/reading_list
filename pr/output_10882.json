[{"title": "Ensembling classical machine learning and deep learning approaches for morbidity identification from clinical notes", "link": "https://api.taylorfrancis.com/content/chapters/oa-edit/download%3FidentifierName%3Ddoi%26identifierValue%3D10.1201/9781003559085-116%26type%3Dchapterpdf", "details": "M Deepadharani, S Deetchanya, M Hemathbhusani\u2026 - Challenges in Information \u2026, 2025", "abstract": "In this study, we offer a unique technique for detecting co-morbidity in patient clinical data, employing Latent Dirichlet Allocation (LDA) for topic modelling. The LDA method is used to extract latent themes from the clinical notes. By using these \u2026"}, {"title": "PETapter: Leveraging PET-style classification heads for modular few-shot parameter-efficient fine-tuning", "link": "https://arxiv.org/pdf/2412.04975%3F", "details": "J Rieger, M Ruckdeschel, G Wiedemann - arXiv preprint arXiv:2412.04975, 2024", "abstract": "Few-shot learning and parameter-efficient fine-tuning (PEFT) are crucial to overcome the challenges of data scarcity and ever growing language model sizes. This applies in particular to specialized scientific domains, where researchers might lack \u2026"}, {"title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases", "link": "https://arxiv.org/pdf/2412.04862", "details": "LG Research, S An, K Bae, E Choi, K Choi, SJ Choi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8 B, and 2.4 B. These models feature several \u2026"}, {"title": "Language hooks: a modular framework for augmenting LLM reasoning that decouples tool usage from the model and its prompt", "link": "https://arxiv.org/pdf/2412.05967", "details": "D de Mijolla, W Yang, P Duckett, C Frye, M Worrall - arXiv preprint arXiv:2412.05967, 2024", "abstract": "Prompting and fine-tuning have emerged as two competing paradigms for augmenting language models with new capabilities, such as the use of tools. Prompting approaches are quick to set up but rely on providing explicit \u2026"}, {"title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents", "link": "https://arxiv.org/pdf/2412.00821", "details": "R Jaiswal, D Jain, HP Popat, A Anand\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in various reasoning tasks. However, they encounter significant challenges when it comes to scientific reasoning, particularly in physics, which requires not only mathematical \u2026"}, {"title": "AutoReason: Automatic Few-Shot Reasoning Decomposition", "link": "https://arxiv.org/pdf/2412.06975", "details": "A Sevinc, A Gumus - arXiv preprint arXiv:2412.06975, 2024", "abstract": "Chain of Thought (CoT) was introduced in recent research as a method for improving step-by-step reasoning in Large Language Models. However, CoT has limited applications such as its need for hand-crafted few-shot exemplar prompts and no \u2026"}, {"title": "Neural abstractive summarization: improvements at the sequence-level", "link": "https://dr.ntu.edu.sg/bitstream/10356/181414/2/Mathieu_Ravaut_s_PhD_Thesis_Edited_Final.pdf", "details": "M Ravaut - 2024", "abstract": "Automatic text summarization has made a fantastic leap forward in the last five to ten years, fueled by the rise of deep learning systems. Summarization at large consists in compressing an input text or series of texts (such as a scientific paper, news articles \u2026"}]
