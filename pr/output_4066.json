[{"title": "Pre-training with Fractional Denoising to Enhance Molecular Property Prediction", "link": "https://arxiv.org/pdf/2407.11086", "details": "Y Ni, S Feng, X Hong, Y Sun, WY Ma, ZM Ma, Q Ye\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Deep learning methods have been considered promising for accelerating molecular screening in drug discovery and material design. Due to the limited availability of labelled data, various self-supervised molecular pre-training methods have been \u2026"}, {"title": "DiNO-Diffusion. Scaling Medical Diffusion via Self-Supervised Pre-Training", "link": "https://arxiv.org/pdf/2407.11594", "details": "G Jimenez-Perez, P Osorio, J Cersovsky\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Diffusion models (DMs) have emerged as powerful foundation models for a variety of tasks, with a large focus in synthetic image generation. However, their requirement of large annotated datasets for training limits their applicability in medical imaging \u2026"}, {"title": "CXR-Agent: Vision-language models for chest X-ray interpretation with uncertainty aware radiology reporting", "link": "https://arxiv.org/pdf/2407.08811", "details": "N Sharma - arXiv preprint arXiv:2407.08811, 2024", "abstract": "Recently large vision-language models have shown potential when interpreting complex images and generating natural language descriptions using advanced reasoning. Medicine's inherently multimodal nature incorporating scans and text \u2026"}, {"title": "Eye Gaze Guided Cross-Modal Alignment Network for Radiology Report Generation", "link": "https://ieeexplore.ieee.org/abstract/document/10596697/", "details": "P Peng, W Fan, Y Shen, W Liu, X Yang, Q Zhang\u2026 - IEEE Journal of Biomedical \u2026, 2024", "abstract": "The potential benefits of automatic radiology report generation, such as reducing misdiagnosis rates and enhancing clinical diagnosis efficiency, are significant. However, existing data-driven methods lack essential medical prior knowledge \u2026"}, {"title": "AutoTutor meets Large Language Models: A Language Model Tutor with Rich Pedagogy and Guardrails", "link": "https://dl.acm.org/doi/abs/10.1145/3657604.3662041", "details": "S Pal Chowdhury, V Zouhar, M Sachan - \u2026 of the Eleventh ACM Conference on \u2026, 2024", "abstract": "Large Language Models (LLMs) have found several use cases in education, ranging from automatic question generation to essay evaluation. In this paper, we explore the potential of using LLMs to author Intelligent Tutoring Systems. A common pitfall of \u2026"}, {"title": "A joint entity Relation Extraction method for document level Traditional Chinese Medicine texts", "link": "https://www.sciencedirect.com/science/article/pii/S093336572400157X", "details": "W Xu, L Wang, M Zhang, J Zhu, J Yan, Q Wu - Artificial Intelligence in Medicine, 2024", "abstract": "Chinese medicine is a unique and complex medical system with complete and rich scientific theories. The textual data of Traditional Chinese Medicine (TCM) contains a large amount of relevant knowledge in the field of TCM, which can serve as guidance \u2026"}, {"title": "ViANLI: Adversarial Natural Language Inference for Vietnamese", "link": "https://arxiv.org/pdf/2406.17716", "details": "T Van Huynh, K Van Nguyen, NLT Nguyen - arXiv preprint arXiv:2406.17716, 2024", "abstract": "The development of Natural Language Processing (NLI) datasets and models has been inspired by innovations in annotation design. With the rapid development of machine learning models today, the performance of existing machine learning \u2026"}, {"title": "CLUE: A Clinical Language Understanding Evaluation for LLMs", "link": "https://www.researchgate.net/profile/Amin-Dada/publication/381887881_CLUE_A_Clinical_Language_Understanding_Evaluation_for_LLMs/links/6683e82d2aa57f3b826698bc/CLUE-A-Clinical-Language-Understanding-Evaluation-for-LLMs.pdf", "details": "ADOA Koras, MBAB Contreras, KESCM Seibold\u2026", "abstract": "Abstract Large Language Models (LLMs) are expected to significantly contribute to patient care, diagnostics, and administrative processes. Emerging biomedical LLMs aim to address healthcare-specific challenges, including privacy demands and \u2026"}, {"title": "ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts", "link": "https://arxiv.org/pdf/2407.09447", "details": "AF Hardy, H Liu, B Lange, MJ Kochenderfer - arXiv preprint arXiv:2407.09447, 2024", "abstract": "Typical schemes for automated red-teaming large language models (LLMs) focus on discovering prompts that trigger a frozen language model (the defender) to generate toxic text. This often results in the prompting model (the adversary) producing text \u2026"}]
