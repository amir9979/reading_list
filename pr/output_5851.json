[{"title": "On Robustness-Accuracy Characterization of Language Models using Synthetic Datasets", "link": "https://openreview.net/pdf%3Fid%3DC0j44uRPcl", "details": "CY Ko, PY Chen, P Das, YS Chuang, L Daniel - First Conference on Language Modeling", "abstract": "In recent years, language models (LMs) that were pretrained at scale on diverse data have proven to be a successful approach for solving different downstream tasks. However, new concerns about proper performance evaluation have been raised \u2026"}, {"title": "Symbolic Working Memory Enhances Language Models for Complex Rule Application", "link": "https://arxiv.org/pdf/2408.13654", "details": "S Wang, Z Wei, Y Choi, X Ren - arXiv preprint arXiv:2408.13654, 2024", "abstract": "Large Language Models (LLMs) have shown remarkable reasoning performance but struggle with multi-step deductive reasoning involving a series of rule application steps, especially when rules are presented non-sequentially. Our preliminary \u2026"}, {"title": "Math-shepherd: Verify and reinforce llms step-by-step without human annotations", "link": "https://aclanthology.org/2024.acl-long.510.pdf", "details": "P Wang, L Li, Z Shao, R Xu, D Dai, Y Li, D Chen, Y Wu\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "In this paper, we present an innovative process-oriented math process reward model called Math-shepherd, which assigns a reward score to each step of math problem solutions. The training of Math-shepherd is achieved using automatically constructed \u2026"}, {"title": "Assessing Contamination in Large Language Models: Introducing the LogProber method", "link": "https://arxiv.org/pdf/2408.14352", "details": "N Yax, PY Oudeyer, S Palminteri - arXiv preprint arXiv:2408.14352, 2024", "abstract": "In machine learning, contamination refers to situations where testing data leak into the training set. The issue is particularly relevant for the evaluation of the performance of Large Language Models (LLMs), which are generally trained on \u2026"}, {"title": "Controllable citation sentence generation with language models", "link": "https://aclanthology.org/2024.sdp-1.4.pdf", "details": "N Gu, R Hahnloser - Proceedings of the Fourth Workshop on Scholarly \u2026, 2024", "abstract": "Citation generation aims to generate a citation sentence that refers to a chosen paper in the context of a manuscript. However, a rigid citation generation process is at odds with an author's desire to control specific attributes, such as 1) the citation \u2026"}, {"title": "InstructEval: Instruction-Tuned Text Evaluator from Human Preference", "link": "https://aclanthology.org/2024.findings-acl.799.pdf", "details": "W Wu, W Li, X Xiao, J Liu, S Li - Findings of the Association for Computational \u2026, 2024", "abstract": "This paper explores to construct a general text evaluator based on open-source Large Language Models (LLMs), a domain predominantly occupied by commercial counterparts such as GPT-4. Recognizing the limitations of open-source models like \u2026"}, {"title": "Persona is a Double-edged Sword: Enhancing the Zero-shot Reasoning by Ensembling the Role-playing and Neutral Prompts", "link": "https://arxiv.org/pdf/2408.08631", "details": "J Kim, N Yang, K Jung - arXiv preprint arXiv:2408.08631, 2024", "abstract": "Recent studies demonstrate that prompting an appropriate role-playing persona to an LLM improves its reasoning capability. However, assigning a proper persona is difficult since an LLM's performance is extremely sensitive to assigned prompts; \u2026"}, {"title": "Factual and Tailored Recommendation Endorsements using Language Models and Reinforcement Learning", "link": "https://openreview.net/pdf%3Fid%3DxI8C7sfN1H", "details": "J Jeong, Y Chow, G Tennenholtz, CW Hsu\u2026 - First Conference on Language \u2026", "abstract": "Recommender systems (RSs) play a central role in matching candidate items to users based on their preferences. While traditional RSs rely on user feed-back signals, conversational RSs interact with users in natural language. In this work, we \u2026"}, {"title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models", "link": "https://arxiv.org/pdf/2408.12798", "details": "Y Li, H Huang, Y Zhao, X Ma, J Sun - arXiv preprint arXiv:2408.12798, 2024", "abstract": "Generative Large Language Models (LLMs) have made significant strides across various tasks, but they remain vulnerable to backdoor attacks, where specific triggers in the prompt cause the LLM to generate adversary-desired responses. While most \u2026"}]
