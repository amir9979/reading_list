[{"title": "Learning with Enriched Inductive Biases for Vision-Language Models", "link": "https://ruyuanzhang.github.io/files/2501_indctbiasVisLangModel_IJCV.pdf", "details": "L Yang, RY Zhang, Q Chen, X Xie - International Journal of Computer Vision, 2025", "abstract": "Abstract Vision-Language Models, pre-trained on large-scale image-text pairs, serve as strong foundation models for transfer learning across a variety of downstream tasks. For few-shot generalization tasks, ie., when the model is trained on few-shot \u2026"}, {"title": "DuCo-Net: Dual-Contrastive Learning Network for Medical Report Retrieval Leveraging Enhanced Encoders and Augmentations", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10870249.pdf", "details": "ZU Rahman, JH Lee, DT Vu, I Murtza, JY Kim - IEEE Access, 2025", "abstract": "The conventional process of generating medical radiology reports is labor-intensive and time-consuming, requiring radiologists to describe findings meticulously from imaging studies. This manual approach often causes undesirable delays in patient \u2026"}, {"title": "Self-supervised analogical learning using language models", "link": "https://arxiv.org/pdf/2502.00996", "details": "B Zhou, S Jain, Y Zhang, Q Ning, S Wang, Y Benajiba\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can \u2026"}, {"title": "Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences", "link": "https://arxiv.org/pdf/2502.01126", "details": "V Shrivastava, A Kumar, P Liang - arXiv preprint arXiv:2502.01126, 2025", "abstract": "Language models (LMs) should provide reliable confidence estimates to help users detect mistakes in their outputs and defer to human experts when necessary. Asking a language model to assess its confidence (\" Score your confidence from 0-1.\") is a \u2026"}, {"title": "Denoising Multi-Level Cross-Attention and Contrastive Learning for Chest Radiology Report Generation", "link": "https://link.springer.com/article/10.1007/s10278-025-01422-9", "details": "D Zhu, L Liu, X Yang, L Liu, W Peng - Journal of Imaging Informatics in Medicine, 2025", "abstract": "Chest radiology report generation plays a vital role in supporting diagnosis, alleviating physician workload, and reducing the risk of misdiagnosis. However, significant challenges persist:(1) Data bias and background noise in chest images \u2026"}, {"title": "Large-scale benchmarking and boosting transfer learning for medical image analysis", "link": "https://www.sciencedirect.com/science/article/pii/S1361841525000350", "details": "MRH Taher, F Haghighi, MB Gotway, J Liang - Medical Image Analysis, 2025", "abstract": "Transfer learning, particularly fine-tuning models pretrained on photographic images to medical images, has proven indispensable for medical image analysis. There are numerous models with distinct architectures pretrained on various datasets using \u2026"}, {"title": "Advancing Vision-Language Models with Generative AI", "link": "https://www.preprints.org/frontend/manuscript/10b5ed95bd23954c58eef830d9d74bfa/download_pub", "details": "A Vats, R Raja - 2025", "abstract": "Generative AI within large vision-language models (LVLMs) has revolutionized multimodal learning, enabling machines to understand and generate visual content from textual descriptions with unprecedented accuracy. This paper explores state-of \u2026"}, {"title": "Self-Supervised Learning Using Nonlinear Dependence", "link": "https://arxiv.org/pdf/2501.18875", "details": "MH Sepanj, B Ghojogh, P Fieguth - arXiv preprint arXiv:2501.18875, 2025", "abstract": "Self-supervised learning has gained significant attention in contemporary applications, particularly due to the scarcity of labeled data. While existing SSL methodologies primarily address feature variance and linear correlations, they often \u2026"}, {"title": "COSDA: Covariance regularized semantic data augmentation for self-supervised visual representation learning", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125001273", "details": "H Chen, Y Ma, J Jiang, N Zheng - Knowledge-Based Systems, 2025", "abstract": "Recent contrastive learning-based self-supervised learning has seen significant improvements through employing an extensive data augmentation strategy, particularly focusing on the generation of positive pairs. However, the current \u2026"}]
