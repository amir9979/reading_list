[{"title": "Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs", "link": "https://arxiv.org/pdf/2406.11514", "details": "Y Fang, M Li, W Wang, H Lin, F Feng - arXiv preprint arXiv:2406.11514, 2024", "abstract": "Large Language Models (LLMs) excel in various natural language processing tasks but struggle with hallucination issues. Existing solutions have considered utilizing LLMs' inherent reasoning abilities to alleviate hallucination, such as self-correction \u2026"}, {"title": "Entropy-Based Decoding for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2406.17519", "details": "Z Qiu, Z Ou, B Wu, J Li, A Liu, I King - arXiv preprint arXiv:2406.17519, 2024", "abstract": "Augmenting Large Language Models (LLMs) with retrieved external knowledge has proven effective for improving the factual accuracy of generated responses. Despite their success, retrieval-augmented LLMs still face the distractibility issue, where the \u2026"}, {"title": "Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for Information Retrieval", "link": "https://arxiv.org/pdf/2406.12169", "details": "Z Li, H Zhang, J Zhang - arXiv preprint arXiv:2406.12169, 2024", "abstract": "Recent research has explored distilling knowledge from large language models (LLMs) to optimize retriever models, especially within the retrieval-augmented generation (RAG) framework. However, most existing training methods rely on \u2026"}, {"title": "Revealing the True Cost of Locally Differentially Private Protocols: An Auditing Perspective", "link": "https://inria.hal.science/hal-04644975/file/paper120_2024_4_source.pdf", "details": "HH Arcolezi, S Gambs - Proceedings on Privacy Enhancing Technologies, 2024", "abstract": "While the existing literature on Differential Privacy (DP) auditing predominantly focuses on the centralized model (eg, in auditing the DP-SGD algorithm), we advocate for extending this approach to audit Local DP (LDP). To achieve this, we \u2026"}, {"title": "Towards Trustworthy LLMs: Understanding the Security and Privacy Risks of Large Language Models", "link": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi%3Farticle%3D1291%26context%3Ddissertations", "details": "K Gu - 2024", "abstract": "Recent years have witnessed remarkable breakthroughs achieved by large language models (LLMs) in natural language processing. As a result, many industries (eg, tech, finance) have quickly embraced LLMs, which are poised to \u2026"}, {"title": "How Do Large Language Models Acquire Factual Knowledge During Pretraining?", "link": "https://arxiv.org/pdf/2406.11813", "details": "H Chang, J Park, S Ye, S Yang, Y Seo, DS Chang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the recent observation that large language models (LLMs) can store substantial factual knowledge, there is a limited understanding of the mechanisms of how they acquire factual knowledge through pretraining. This work addresses this \u2026"}, {"title": "Suri: Multi-constraint Instruction Following for Long-form Text Generation", "link": "https://arxiv.org/pdf/2406.19371", "details": "CM Pham, S Sun, M Iyyer - arXiv preprint arXiv:2406.19371, 2024", "abstract": "Existing research on instruction following largely focuses on tasks with simple instructions and short responses. In this work, we explore multi-constraint instruction following for generating long-form text. We create Suri, a dataset with 20K human \u2026"}]
