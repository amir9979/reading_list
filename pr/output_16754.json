[{"title": "Debiasing Guidance for Discrete Diffusion with Sequential Monte Carlo", "link": "https://openreview.net/pdf%3Fid%3Dvg0dOu3w9g", "details": "LC Kit, P Jeha, J Frellsen, P Lio, MS Albergo, F Vargas - Frontiers in Probabilistic Inference \u2026", "abstract": "Discrete diffusion models are a class of generative models that produce samples from an approximated data distribution within a discrete state space. Often, there is a need to target specific regions of the data distribution. Current guidance methods aim \u2026"}, {"title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL", "link": "https://arxiv.org/pdf/2505.15791", "details": "F Dai, Z Zhuang, Y Huang, S Huang, B Liao, D Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution \u2026", "entry_id": "http://arxiv.org/abs/2505.15791v1", "updated": "2025-05-21 17:44:37", "published": "2025-05-21 17:44:37", "authors": "Fengyuan Dai;Zifeng Zhuang;Yufei Huang;Siteng Huang;Bangyan Liao;Donglin Wang;Fajie Yuan", "summary": "Diffusion models have emerged as powerful generative tools across various\ndomains, yet tailoring pre-trained models to exhibit specific desirable\nproperties remains challenging. While reinforcement learning (RL) offers a\npromising solution,current methods struggle to simultaneously achieve stable,\nefficient fine-tuning and support non-differentiable rewards. Furthermore,\ntheir reliance on sparse rewards provides inadequate supervision during\nintermediate steps, often resulting in suboptimal generation quality. To\naddress these limitations, dense and differentiable signals are required\nthroughout the diffusion process. Hence, we propose VAlue-based Reinforced\nDiffusion (VARD): a novel approach that first learns a value function\npredicting expection of rewards from intermediate states, and subsequently uses\nthis value function with KL regularization to provide dense supervision\nthroughout the generation process. Our method maintains proximity to the\npretrained model while enabling effective and stable training via\nbackpropagation. Experimental results demonstrate that our approach facilitates\nbetter trajectory guidance, improves training efficiency and extends the\napplicability of RL to diffusion models optimized for complex,\nnon-differentiable reward functions.", "comment": "Under review", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.LG", "links": "http://arxiv.org/abs/2505.15791v1;http://arxiv.org/pdf/2505.15791v1", "pdf_url": "http://arxiv.org/pdf/2505.15791v1"}]
