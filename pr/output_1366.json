'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [More Room for Language: Investigating the Effect of Re'
[{"title": "Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?", "link": "https://arxiv.org/pdf/2404.12728", "details": "C Qin, W Xia, T Wang, F Jiao, Y Hu, B Ding, R Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Analogical reasoning is a unique ability of humans to address unfamiliar challenges by transferring strategies from relevant past experiences. One key finding in psychology is that compared with irrelevant past experiences, recalling relevant ones \u2026"}, {"title": "Unveiling Imitation Learning: Exploring the Impact of Data Falsity to Large Language Model", "link": "https://arxiv.org/pdf/2404.09717", "details": "H Cho - arXiv preprint arXiv:2404.09717, 2024", "abstract": "Many recent studies endeavor to improve open-source language models through imitation learning, and re-training on the synthetic instruction data from state-of-the- art proprietary models like ChatGPT and GPT-4. However, the innate nature of \u2026"}, {"title": "Foundational challenges in assuring alignment and safety of large language models", "link": "https://arxiv.org/pdf/2404.09932", "details": "U Anwar, A Saparov, J Rando, D Paleka, M Turpin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment \u2026"}, {"title": "Fewer Truncations Improve Language Modeling", "link": "https://arxiv.org/pdf/2404.10830", "details": "H Ding, Z Wang, G Paolini, V Kumar, A Deoras, D Roth\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In large language model training, input documents are typically concatenated together and then split into sequences of equal length to avoid padding tokens. Despite its efficiency, the concatenation approach compromises data integrity--it \u2026"}, {"title": "Characterizing LLM Abstention Behavior in Science QA with Context Perturbations", "link": "https://arxiv.org/pdf/2404.12452", "details": "B Wen, B Howe, LL Wang - arXiv preprint arXiv:2404.12452, 2024", "abstract": "The correct model response in the face of uncertainty is to abstain from answering a question so as not to mislead the user. In this work, we study the ability of LLMs to abstain from answering context-dependent science questions when provided \u2026"}, {"title": "Random Masking Finds Winning Tickets for Parameter Efficient Fine-tuning", "link": "https://arxiv.org/pdf/2405.02596", "details": "J Xu, J Zhang - arXiv preprint arXiv:2405.02596, 2024", "abstract": "Fine-tuning large language models (LLM) can be costly. Parameter-efficient fine- tuning (PEFT) addresses the problems by training a fraction of the parameters, whose success reveals the expressiveness and flexibility of pretrained models. This \u2026"}, {"title": "Construction of Domain-specified Japanese Large Language Model for Finance through Continual Pre-training", "link": "https://arxiv.org/pdf/2404.10555", "details": "M Hirano, K Imajo - arXiv preprint arXiv:2404.10555, 2024", "abstract": "Large language models (LLMs) are now widely used in various fields, including finance. However, Japanese financial-specific LLMs have not been proposed yet. Hence, this study aims to construct a Japanese financial-specific LLM through \u2026"}, {"title": "Understanding Inverse Scaling and Emergence in Multitask Representation Learning", "link": "https://proceedings.mlr.press/v238/e-ildiz24a/e-ildiz24a.pdf", "details": "ME Ildiz, Z Zhao, S Oymak - International Conference on Artificial Intelligence and \u2026, 2024", "abstract": "Large language models exhibit strong multitasking capabilities, however, their learning dynamics as a function of task characteristics, sample size, and model complexity remain mysterious. For instance, it is known that, as the model size grows \u2026"}, {"title": "RevOnt: Reverse Engineering of Competency Questions from Knowledge Graphs via Language Models", "link": "https://kclpure.kcl.ac.uk/portal/files/257691951/RevOnt_Revisions_Jongmo_3_.pdf", "details": "F Ciroku, J de Berardinis, J Kim, AM Penuela\u2026 - Journal of Web Semantics, 2024", "abstract": "RevOnt: Reverse Engineering of Competency Questions from Knowledge Graphs via Language Models Page 1 King\u2019s Research Portal Link to publication record in King's Research Portal Citation for published version (APA): Ciroku, F., de Berardinis, J., Kim \u2026"}]
