[{"title": "Ask Me in English Instead: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries", "link": "https://openreview.net/pdf%3Fid%3DGpGSlMoiBn", "details": "Y Jin, M Chandra, G Verma, Y Hu, M De Choudhury\u2026 - The Web Conference 2024", "abstract": "Large language models (LLMs) are transforming the ways the general public accesses and consumes information. Their influence is particularly pronounced in pivotal sectors like healthcare, where lay individuals are increasingly appropriating \u2026"}, {"title": "FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2405.18218", "details": "Y Zhang, Y Li, X Wang, Q Shen, B Plank, B Bischl\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Overparametrized transformer networks are the state-of-the-art architecture for Large Language Models (LLMs). However, such models contain billions of parameters making large compute a necessity, while raising environmental concerns. To \u2026"}, {"title": "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks", "link": "https://dl.acm.org/doi/pdf/10.1145/3589334.3645363", "details": "S Xu, L Pang, H Shen, X Cheng, TS Chua - Proceedings of the ACM on Web \u2026, 2024", "abstract": "Making the contents generated by Large Language Model (LLM), accurate, credible and traceable is crucial, especially in complex knowledge-intensive tasks that require multi-step reasoning and each step needs knowledge to solve. Retrieval \u2026"}, {"title": "Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models", "link": "https://arxiv.org/pdf/2405.17044", "details": "X Gu, M Krenn - arXiv preprint arXiv:2405.17044, 2024", "abstract": "Advanced artificial intelligence (AI) systems with access to millions of research papers could inspire new research ideas that may not be conceived by humans alone. However, how interesting are these AI-generated ideas, and how can we \u2026"}, {"title": "IT5: Text-to-text Pretraining for Italian Language Understanding and Generation", "link": "https://aclanthology.org/2024.lrec-main.823.pdf", "details": "G Sarti, M Nissim - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "We introduce IT5, the first family of encoder-decoder transformer models pretrained specifically on Italian. We document and perform a thorough cleaning procedure for a large Italian corpus and use it to pretrain four IT5 model sizes. We then introduce \u2026"}, {"title": "Enhancing Relation Extraction from Biomedical Texts by Large Language Models", "link": "https://link.springer.com/chapter/10.1007/978-3-031-60615-1_1", "details": "M Asada, K Fukuda - International Conference on Human-Computer \u2026, 2024", "abstract": "In this study, we propose a novel relation extraction method enhanced by large language models (LLMs). We incorporated three relation extraction models that leverage LLMs:(1) relation extraction via in-context few-shot learning with LLMs,(2) \u2026"}]
