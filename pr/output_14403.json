[{"title": "ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models", "link": "https://arxiv.org/pdf/2503.19355", "details": "D Ko, S Kim, Y Suh, M Yoon, M Chandraker, HJ Kim - arXiv preprint arXiv \u2026, 2025", "abstract": "Spatio-temporal reasoning is essential in understanding real-world environments in various fields, eg, autonomous driving and sports analytics. Recent advances have improved the spatial reasoning ability of Vision-Language Models (VLMs) by \u2026"}, {"title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?", "link": "https://arxiv.org/pdf/2503.02199", "details": "A Deng, T Cao, Z Chen, B Hooi - arXiv preprint arXiv:2503.02199, 2025", "abstract": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual \u2026"}, {"title": "Process-based self-rewarding language models", "link": "https://arxiv.org/pdf/2503.03746", "details": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' \u2026"}, {"title": "Multi-Cue Adaptive Visual Token Pruning for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.08019", "details": "B Luan, W Zhou, H Feng, Z Wang, X Li, H Li - arXiv preprint arXiv:2503.08019, 2025", "abstract": "As the computational needs of Large Vision-Language Models (LVLMs) increase, visual token pruning has proven effective in improving inference speed and memory efficiency. Traditional pruning methods in LVLMs predominantly focus on attention \u2026"}, {"title": "Inference retrieval-augmented multi-modal chain-of-thoughts reasoning for language models", "link": "https://ieeexplore.ieee.org/abstract/document/10888701/", "details": "Q He, S Qian, J Zhang, C Wang - ICASSP 2025-2025 IEEE International Conference \u2026, 2025", "abstract": "Recent advancements in Large Language Models (LLMs) have catalyzed the exploration of Chain of Thought (CoT) approaches, particularly in extending their application to multimodal tasks to enhance reasoning capabilities. However, current \u2026"}, {"title": "Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages", "link": "https://arxiv.org/pdf/2503.11609", "details": "M Farina, M Mancini, G Iacca, E Ricci - arXiv preprint arXiv:2503.11609, 2025", "abstract": "An old-school recipe for training a classifier is to (i) learn a good feature extractor and (ii) optimize a linear layer atop. When only a handful of samples are available per category, as in Few-Shot Adaptation (FSA), data are insufficient to fit a large number \u2026"}, {"title": "Balcony: A Lightweight Approach to Dynamic Inference of Generative Language Models", "link": "https://arxiv.org/pdf/2503.05005", "details": "B Jamialahmadi, P Kavehzadeh, M Rezagholizadeh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Deploying large language models (LLMs) in real-world applications is often hindered by strict computational and latency constraints. While dynamic inference offers the flexibility to adjust model behavior based on varying resource budgets, existing \u2026"}, {"title": "Optimizing Language Models for Inference Time Objectives using Reinforcement Learning", "link": "https://arxiv.org/pdf/2503.19595", "details": "Y Tang, K Zheng, G Synnaeve, R Munos - arXiv preprint arXiv:2503.19595, 2025", "abstract": "In this work, we investigate the merits of explicitly optimizing for inference time algorithmic performance during model training. We show how optimizing for inference time performance can improve overall model efficacy. We consider generic \u2026"}, {"title": "Fine-grained knowledge fusion for retrieval-augmented medical visual question answering", "link": "https://www.sciencedirect.com/science/article/pii/S1566253525001320", "details": "X Liang, D Wang, B Jing, Z Jiao, R Li, R Liu, Q Miao\u2026 - Information Fusion, 2025", "abstract": "Given that medical image analysis often requires experts to recall typical symptoms from diagnostic archives or their own experience, implementing retrieval augmentation in multi-modal tasks like Medical Visual Question Answering \u2026"}]
