[{"title": "Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models", "link": "https://arxiv.org/pdf/2412.02980", "details": "A Havrilla, A Dai, L O'Mahony, K Oostermeijer, V Zisler\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it \u2026"}, {"title": "Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework", "link": "https://arxiv.org/pdf/2412.15504", "details": "Z Xu, W Chen, Y Tang, X Li, C Hu, Z Chu, K Ren\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Natural language processing (NLP) has seen remarkable advancements with the development of large language models (LLMs). Despite these advancements, LLMs often produce socially biased outputs. Recent studies have mainly addressed this \u2026"}, {"title": "Frequency Is What You Need: Word-frequency Masking Benefits Vision-Language Model Pre-training", "link": "https://arxiv.org/pdf/2412.16148", "details": "M Liang, M Larson - arXiv preprint arXiv:2412.16148, 2024", "abstract": "Vision Language Models (VLMs) can be trained more efficiently if training sets can be reduced in size. Recent work has shown the benefits of masking text during VLM training using a variety of approaches: truncation, random masking, block masking \u2026"}, {"title": "Eliciting Causal Abilities in Large Language Models for Reasoning Tasks", "link": "https://arxiv.org/pdf/2412.15314", "details": "Y Wang, Z Luo, J Wang, Z Zhou, Y Chen, B Han - arXiv preprint arXiv:2412.15314, 2024", "abstract": "Prompt optimization automatically refines prompting expressions, unlocking the full potential of LLMs in downstream tasks. However, current prompt optimization methods are costly to train and lack sufficient interpretability. This paper proposes \u2026"}]
