[{"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers", "link": "https://openreview.net/pdf%3Fid%3D67tRrjgzsh", "details": "X Lu, Y Zhao, B Qin, L Huo, Q Yang, D Xu - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "LL\\\" aMmlein: Compact and Competitive German-Only Language Models from Scratch", "link": "https://arxiv.org/pdf/2411.11171", "details": "J Pfister, J Wunderle, A Hotho - arXiv preprint arXiv:2411.11171, 2024", "abstract": "We create two German-only decoder models, LL\\\" aMmlein 120M and 1B, transparently from scratch and publish them, along with the training data, for the German NLP research community to use. The model training involved several key \u2026"}, {"title": "Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding", "link": "https://arxiv.org/pdf/2411.04282%3F", "details": "H Chen, Y Feng, Z Liu, W Yao, A Prabhakar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time \u2026"}, {"title": "Factuality of Large Language Models: A Survey", "link": "https://aclanthology.org/2024.emnlp-main.1088.pdf", "details": "Y Wang, M Wang, MA Manzoor, F Liu, G Georgiev\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a \u2026"}, {"title": "RedPajama: an Open Dataset for Training Large Language Models", "link": "https://arxiv.org/pdf/2411.12372%3F", "details": "M Weber, D Fu, Q Anthony, Y Oren, S Adams\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top \u2026"}, {"title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step", "link": "https://arxiv.org/pdf/2411.10440%3F", "details": "G Xu, P Jin, L Hao, Y Song, L Sun, L Yuan - arXiv preprint arXiv:2411.10440, 2024", "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to \u2026"}, {"title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models", "link": "https://arxiv.org/pdf/2411.09595", "details": "Z Wang, J Lorraine, Y Wang, H Su, J Zhu, S Fidler\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This work explores expanding the capabilities of large language models (LLMs) pretrained on text to generate 3D meshes within a unified model. This offers key advantages of (1) leveraging spatial knowledge already embedded in LLMs, derived \u2026"}, {"title": "Fox-1 Technical Report", "link": "https://arxiv.org/pdf/2411.05281", "details": "Z Hu, J Zhang, R Pan, Z Xu, S Avestimehr, C He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Fox-1, a series of small language models (SLMs) consisting of Fox-1-1.6 B and Fox-1-1.6 B-Instruct-v0. 1. These models are pre-trained on 3 trillion tokens of web-scraped document data and fine-tuned with 5 billion tokens of instruction \u2026"}, {"title": "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "link": "https://aclanthology.org/2024.findings-emnlp.693.pdf", "details": "M Li, W Wang, F Feng, F Zhu, Q Wang, TS Chua - Findings of the Association for \u2026, 2024", "abstract": "Abstract Self-detection for Large Language Models (LLMs) seeks to evaluate the trustworthiness of the LLM's output by leveraging its own capabilities, thereby alleviating the issue of output hallucination. However, existing self-detection \u2026"}]
