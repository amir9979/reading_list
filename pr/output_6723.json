[{"title": "Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language Models", "link": "https://arxiv.org/pdf/2409.01584", "details": "S Ozaki, K Hayashi, Y Sakai, H Kamigaito, K Hayashi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As the performance of Large-scale Vision Language Models (LVLMs) improves, they are increasingly capable of responding in multiple languages, and there is an expectation that the demand for explanations generated by LVLMs will grow \u2026"}, {"title": "Effective prompt extraction from language models", "link": "https://openreview.net/pdf%3Fid%3D0o95CVdNuz", "details": "Y Zhang, N Carlini, D Ippolito - First Conference on Language Modeling, 2024", "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden \u2026"}, {"title": "Making Large Vision Language Models to be Good Few-shot Learners", "link": "https://arxiv.org/pdf/2408.11297", "details": "F Liu, W Cai, J Huo, C Zhang, D Chen, J Zhou - arXiv preprint arXiv:2408.11297, 2024", "abstract": "Few-shot classification (FSC) is a fundamental yet challenging task in computer vision that involves recognizing novel classes from limited data. While previous methods have focused on enhancing visual features or incorporating additional \u2026"}, {"title": "Towards Analyzing and Mitigating Sycophancy in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2408.11261", "details": "Y Zhao, R Zhang, J Xiao, C Ke, R Hou, Y Hao, Q Guo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) have shown significant capability in vision- language understanding. However, one critical issue that persists in these models is sycophancy, which means models are unduly influenced by leading or deceptive \u2026"}, {"title": "SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for Large-scale Vision-Language Models", "link": "https://arxiv.org/pdf/2408.12114", "details": "Y Yu, S Chung, BK Lee, YM Ro - arXiv preprint arXiv:2408.12114, 2024", "abstract": "Large-scale Vision-Language Models (LVLMs) have significantly advanced with text- aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate \u2026"}, {"title": "Evaluating Attribute Comprehension in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2408.13898", "details": "H Zhang, Z Yang, Y Liu, X Wang, Z He, K Liang, Z Ma - arXiv preprint arXiv \u2026, 2024", "abstract": "Currently, large vision-language models have gained promising progress on many downstream tasks. However, they still suffer many challenges in fine-grained visual understanding tasks, such as object attribute comprehension. Besides, there have \u2026"}, {"title": "Semformer: Transformer Language Models with Semantic Planning", "link": "https://arxiv.org/pdf/2409.11143", "details": "Y Yin, J Ding, K Song, Y Zhang - arXiv preprint arXiv:2409.11143, 2024", "abstract": "Next-token prediction serves as the dominant component in current neural language models. During the training phase, the model employs teacher forcing, which predicts tokens based on all preceding ground truth tokens. However, this approach \u2026"}, {"title": "Understanding Defects in Generated Codes by Language Models", "link": "https://arxiv.org/pdf/2408.13372", "details": "AM Esfahani, N Kahani, SA Ajila - arXiv preprint arXiv:2408.13372, 2024", "abstract": "This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the \u2026"}, {"title": "Internal and External Knowledge Interactive Refinement Framework for Knowledge-Intensive Question Answering", "link": "https://arxiv.org/pdf/2408.12979", "details": "H Du, D Zhao - arXiv preprint arXiv:2408.12979, 2024", "abstract": "Recent works have attempted to integrate external knowledge into LLMs to address the limitations and potential factual errors in LLM-generated content. However, how to retrieve the correct knowledge from the large amount of external knowledge \u2026"}]
