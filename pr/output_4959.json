[{"title": "A Training Data Recipe to Accelerate A* Search with Language Models", "link": "https://arxiv.org/pdf/2407.09985", "details": "D Gupta, B Li - arXiv preprint arXiv:2407.09985, 2024", "abstract": "Recent works in AI planning have proposed to combine LLMs with iterative tree- search algorithms like A* and MCTS, where LLMs are typically used to calculate the heuristic, guiding the planner towards the goal. However, combining these \u2026"}, {"title": "Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization", "link": "https://arxiv.org/pdf/2407.07880", "details": "J Wu, Y Xie, Z Yang, J Wu, J Chen, J Gao, B Ding\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study addresses the challenge of noise in training datasets for Direct Preference Optimization (DPO), a method for aligning Large Language Models (LLMs) with human preferences. We categorize noise into pointwise noise, which includes low \u2026"}, {"title": "Steering Language Models with Game-Theoretic Solvers", "link": "https://openreview.net/pdf%3Fid%3D5QLtIodDmu", "details": "I Gemp, R Patel, Y Bachrach, M Lanctot, V Dasagi\u2026 - Agentic Markets Workshop at ICML \u2026", "abstract": "Mathematical models of strategic interactions among rational agents have long been studied in game theory. However the interactions studied are often over a small set of discrete actions which is very different from how humans communicate in natural \u2026"}, {"title": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models", "link": "https://arxiv.org/pdf/2408.01933", "details": "B Wang, J Chang, Y Qian, G Chen, J Chen, Z Jiang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have recently showcased remarkable capabilities, spanning a wide range of tasks and applications, including those in the medical domain. Models like GPT-4 excel in medical question answering but may face \u2026"}, {"title": "Enhancing Clinical Relevance of Pretrained Language Models Through Integration of External Knowledge: Case Study on Cardiovascular Diagnosis From Electronic \u2026", "link": "https://ai.jmir.org/2024/1/e56932/", "details": "Q Lu, A Wen, T Nguyen, H Liu - JMIR AI, 2024", "abstract": "Background: Despite their growing use in health care, pretrained language models (PLMs) often lack clinical relevance due to insufficient domain expertise and poor interpretability. A key strategy to overcome these challenges is integrating external \u2026"}, {"title": "Interpretable Differential Diagnosis with Dual-Inference Large Language Models", "link": "https://arxiv.org/pdf/2407.07330", "details": "S Zhou, S Ding, J Wang, M Lin, GB Melton, R Zhang - arXiv preprint arXiv:2407.07330, 2024", "abstract": "Methodological advancements to automate the generation of differential diagnosis (DDx) to predict a list of potential diseases as differentials given patients' symptom descriptions are critical to clinical reasoning and applications such as decision \u2026"}, {"title": "Generative Retrieval with Few-shot Indexing", "link": "https://arxiv.org/pdf/2408.02152", "details": "A Askari, C Meng, M Aliannejadi, Z Ren, E Kanoulas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing generative retrieval (GR) approaches rely on training-based indexing, ie, fine-tuning a model to memorise the associations between a query and the document identifier (docid) of a relevant document. Training-based indexing has \u2026"}, {"title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Trustworthy Response Generation in Chinese", "link": "https://dl.acm.org/doi/pdf/10.1145/3686807", "details": "H Wang, S Zhao, Z Qiang, Z Li, C Liu, N Xi, Y Du, B Qin\u2026 - ACM Transactions on \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to \u2026"}, {"title": "GRAD-SUM: Leveraging Gradient Summarization for Optimal Prompt Engineering", "link": "https://arxiv.org/pdf/2407.12865", "details": "D Austin, E Chartock - arXiv preprint arXiv:2407.12865, 2024", "abstract": "Prompt engineering for large language models (LLMs) is often a manual time- intensive process that involves generating, evaluating, and refining prompts iteratively to ensure high-quality outputs. While there has been work on automating \u2026"}]
