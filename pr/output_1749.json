[{"title": "Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models", "link": "https://arxiv.org/pdf/2405.12522", "details": "C O'Neill, T Bui - arXiv preprint arXiv:2405.12522, 2024", "abstract": "This paper introduces an efficient and robust method for discovering interpretable circuits in large language models using discrete sparse autoencoders. Our approach addresses key limitations of existing techniques, namely computational complexity \u2026"}, {"title": "DocuMint: Docstring Generation for Python using Small Language Models", "link": "https://arxiv.org/pdf/2405.10243", "details": "B Poudel, A Cook, S Traore, S Ameli - arXiv preprint arXiv:2405.10243, 2024", "abstract": "Effective communication, specifically through documentation, is the beating heart of collaboration among contributors in software development. Recent advancements in language models (LMs) have enabled the introduction of a new type of actor in that \u2026"}, {"title": "A Dataset for Evaluating Contextualized Representation of Biomedical Concepts in Language Models", "link": "https://www.nature.com/articles/s41597-024-03317-w", "details": "H Rouhizadeh, I Nikishina, A Yazdani, A Bornet\u2026 - Scientific Data, 2024", "abstract": "Due to the complexity of the biomedical domain, the ability to capture semantically meaningful representations of terms in context is a long-standing challenge. Despite important progress in the past years, no evaluation benchmark has been developed \u2026"}, {"title": "Backdoor Removal for Generative Large Language Models", "link": "https://arxiv.org/pdf/2405.07667", "details": "H Li, Y Chen, Z Zheng, Q Hu, C Chan, H Liu, Y Song - arXiv preprint arXiv \u2026, 2024", "abstract": "With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased \u2026"}, {"title": "Adversarial Robustness for Visual Grounding of Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2405.09981", "details": "K Gao, Y Bai, J Bai, Y Yang, ST Xia - arXiv preprint arXiv:2405.09981, 2024", "abstract": "Multi-modal Large Language Models (MLLMs) have recently achieved enhanced performance across various vision-language tasks including visual grounding capabilities. However, the adversarial robustness of visual grounding remains \u2026"}, {"title": "Lonas: Elastic low-rank adapters for efficient large language models", "link": "https://aclanthology.org/2024.lrec-main.940.pdf", "details": "JP Munoz, J Yuan, Y Zheng, N Jain - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) continue to grow, reaching hundreds of billions of parameters and making it challenging for Deep Learning practitioners with resource-constrained systems to use them, eg, fine-tuning these models for a \u2026"}, {"title": "Correcting Language Model Bias for Text Classification in True Zero-Shot Learning", "link": "https://aclanthology.org/2024.lrec-main.359.pdf", "details": "F Zhao, W Xianlin, C Yan, CK Loo - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Combining pre-trained language models (PLMs) and manual templates is a common practice for text classification in zero-shot scenarios. However, the effect of this approach is highly volatile, ranging from random guesses to near state-of-the-art \u2026"}, {"title": "Enhancing Large Language Models through Transforming Reasoning Problems into Classification Tasks", "link": "https://aclanthology.org/2024.lrec-main.532.pdf", "details": "T Raheja, R Sinha, A Deepak, W Healy, J Srinivasa\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "In this paper, we introduce a novel approach for enhancing the reasoning capabilities of large language models (LLMs) for constraint satisfaction problems (CSPs), by converting reasoning problems into classification tasks. Our method \u2026"}, {"title": "LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language", "link": "https://arxiv.org/pdf/2405.07745", "details": "C Toraman - arXiv preprint arXiv:2405.07745, 2024", "abstract": "Despite advancements in English-dominant generative large language models, further development is needed for low-resource languages to enhance global accessibility. The primary methods for representing these languages are \u2026"}]
