[{"title": "Elements of World Knowledge (EWOK): A cognition-inspired framework for evaluating basic world knowledge in language models", "link": "https://arxiv.org/pdf/2405.09605", "details": "AA Ivanova, A Sathe, B Lipkin, U Kumar, S Radkani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The ability to build and leverage world models is essential for a general-purpose AI agent. Testing such capabilities is hard, in part because the building blocks of world models are ill-defined. We present Elements of World Knowledge (EWOK), a \u2026"}, {"title": "ECR-Chain: Advancing Generative Language Models to Better Emotion-Cause Reasoners through Reasoning Chains", "link": "https://arxiv.org/pdf/2405.10860", "details": "Z Huang, J Zhao, Q Jin - arXiv preprint arXiv:2405.10860, 2024", "abstract": "Understanding the process of emotion generation is crucial for analyzing the causes behind emotions. Causal Emotion Entailment (CEE), an emotion-understanding task, aims to identify the causal utterances in a conversation that stimulate the emotions \u2026"}, {"title": "GRAMMAR: Grounded and Modular Evaluation of Domain-Specific Retrieval-Augmented Language Models", "link": "https://arxiv.org/pdf/2404.19232", "details": "X Li, M Liu, S Gao - arXiv preprint arXiv:2404.19232, 2024", "abstract": "Retrieval-augmented Generation (RAG) systems have been actively studied and deployed across various industries to query on domain-specific knowledge base. However, evaluating these systems presents unique challenges due to the scarcity of \u2026"}, {"title": "An in-depth evaluation of federated learning on biomedical natural language processing for information extraction", "link": "https://www.nature.com/articles/s41746-024-01126-4", "details": "L Peng, G Luo, S Zhou, J Chen, Z Xu, J Sun, R Zhang - NPJ Digital Medicine, 2024", "abstract": "Abstract Language models (LMs) such as BERT and GPT have revolutionized natural language processing (NLP). However, the medical field faces challenges in training LMs due to limited data access and privacy constraints imposed by \u2026"}, {"title": "NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition", "link": "https://arxiv.org/pdf/2405.07609", "details": "E Merdjanovska, A Aynetdinov, A Akbik - arXiv preprint arXiv:2405.07609, 2024", "abstract": "Available training data for named entity recognition (NER) often contains a significant percentage of incorrect labels for entity types and entity boundaries. Such label noise poses challenges for supervised learning and may significantly deteriorate model \u2026"}, {"title": "Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "link": "https://arxiv.org/pdf/2405.10431", "details": "S Furniturewala, S Jandial, A Java, P Banerjee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing debiasing techniques are typically training-based or require access to the model's internals and output distributions, so they are inaccessible to end-users looking to adapt LLM outputs for their particular needs. In this study, we examine \u2026"}, {"title": "Characterizing the Accuracy-Efficiency Trade-off of Low-rank Decomposition in Language Models", "link": "https://arxiv.org/pdf/2405.06626", "details": "C Moar, M Pellauer, H Kwon - arXiv preprint arXiv:2405.06626, 2024", "abstract": "Large language models (LLMs) have emerged and presented their general problem- solving capabilities with one model. However, the model size has increased dramatically with billions of parameters to enable such broad problem-solving \u2026"}, {"title": "Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks", "link": "https://arxiv.org/pdf/2405.10548", "details": "A Chatterjee, E Tanwar, S Dutta, T Chakraborty - arXiv preprint arXiv:2405.10548, 2024", "abstract": "Large Language Models (LLMs) have transformed NLP with their remarkable In- context Learning (ICL) capabilities. Automated assistants based on LLMs are gaining popularity; however, adapting them to novel tasks is still challenging. While colossal \u2026"}, {"title": "What Makes Good Few-shot Examples for Vision-Language Models?", "link": "https://arxiv.org/pdf/2405.13532", "details": "Z Guo, J Lu, X Liu, R Zhao, ZX Qian, F Tan - arXiv preprint arXiv:2405.13532, 2024", "abstract": "Despite the notable advancements achieved by leveraging pre-trained vision- language (VL) models through few-shot tuning for downstream tasks, our detailed empirical study highlights a significant dependence of few-shot learning outcomes \u2026"}]
