[{"title": "Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster", "link": "https://arxiv.org/pdf/2505.18642", "details": "X Chen, S Zhou, K Liang, X Sun, X Liu - arXiv preprint arXiv:2505.18642, 2025", "abstract": "Chain-of-thought (CoT) distillation allows a large language model (LLM) to guide a small language model (SLM) in reasoning tasks. Existing methods train the SLM to learn the long rationale in one iteration, resulting in two issues: 1) Long rationales \u2026", "entry_id": "http://arxiv.org/abs/2505.18642v1", "updated": "2025-05-24 11:04:52", "published": "2025-05-24 11:04:52", "authors": "Xiao Chen;Sihang Zhou;Ke Liang;Xiaoyu Sun;Xinwang Liu", "summary": "Chain-of-thought (CoT) distillation allows a large language model (LLM) to\nguide a small language model (SLM) in reasoning tasks. Existing methods train\nthe SLM to learn the long rationale in one iteration, resulting in two issues:\n1) Long rationales lead to a large token-level batch size during training,\nmaking gradients of core reasoning tokens (i.e., the token will directly affect\nthe correctness of subsequent reasoning) over-smoothed as they contribute a\ntiny fraction of the rationale. As a result, the SLM converges to sharp minima\nwhere it fails to grasp the reasoning logic. 2) The response is slow, as the\nSLM must generate a long rationale before reaching the answer. Therefore, we\npropose chunk-wise training (CWT), which uses a heuristic search to divide the\nrationale into internal semantically coherent chunks and focuses SLM on\nlearning from only one chunk per iteration. In this way, CWT naturally isolates\nnon-reasoning chunks that do not involve the core reasoning token (e.g.,\nsummary and transitional chunks) from the SLM learning for reasoning chunks,\nmaking the fraction of the core reasoning token increase in the corresponding\niteration. Based on CWT, skip-thinking training (STT) is proposed. STT makes\nthe SLM automatically skip non-reasoning medium chunks to reach the answer,\nimproving reasoning speed while maintaining accuracy. We validate our approach\non a variety of SLMs and multiple reasoning tasks.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.18642v1;http://arxiv.org/pdf/2505.18642v1", "pdf_url": "http://arxiv.org/pdf/2505.18642v1"}, {"title": "Pretraining Language Models to Ponder in Continuous Space", "link": "https://arxiv.org/pdf/2505.20674", "details": "B Zeng, S Song, S Huang, Y Wang, H Li, Z He, X Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Humans ponder before articulating complex sentence elements, enabling deeper cognitive processing through focused effort. In this work, we introduce this pondering process into language models by repeatedly invoking the forward process within a \u2026", "entry_id": "http://arxiv.org/abs/2505.20674v1", "updated": "2025-05-27 03:47:33", "published": "2025-05-27 03:47:33", "authors": "Boyi Zeng;Shixiang Song;Siyuan Huang;Yixuan Wang;He Li;Ziwei He;Xinbing Wang;Zhiyu Li;Zhouhan Lin", "summary": "Humans ponder before articulating complex sentence elements, enabling deeper\ncognitive processing through focused effort. In this work, we introduce this\npondering process into language models by repeatedly invoking the forward\nprocess within a single token generation step. During pondering, instead of\ngenerating an actual token sampled from the prediction distribution, the model\nponders by yielding a weighted sum of all token embeddings according to the\npredicted token distribution. The generated embedding is then fed back as input\nfor another forward pass. We show that the model can learn to ponder in this\nway through self-supervised learning, without any human annotations. Our method\nis straightforward and can be seamlessly integrated with various existing\nlanguage models. Experiments across three widely used open-source\narchitectures-GPT-2, Pythia, and LLaMA-and extensive downstream task\nevaluations demonstrate the effectiveness and generality of our method. For\nlanguage modeling tasks, pondering language models achieve performance\ncomparable to vanilla models with twice the number of parameters. On 9\ndownstream benchmarks, our pondering-enhanced Pythia models significantly\noutperform the official Pythia models. Notably, pondering-enhanced Pythia-1B is\ncomparable to TinyLlama-1.1B, which is trained on 10 times more data. The code\nis available at https://github.com/LUMIA-Group/PonderingLM.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.20674v1;http://arxiv.org/pdf/2505.20674v1", "pdf_url": "http://arxiv.org/pdf/2505.20674v1"}, {"title": "Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning", "link": "https://arxiv.org/pdf/2505.18503", "details": "A Chang, L Huang, AJ Boyd, P Bhatia, T Kass-Hout\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal attention distribution on visual inputs, leading to hallucinated or inaccurate outputs. Existing mitigation methods primarily rely on inference-time interventions, which are \u2026", "entry_id": "http://arxiv.org/abs/2505.18503v1", "updated": "2025-05-24 04:45:45", "published": "2025-05-24 04:45:45", "authors": "Aofei Chang;Le Huang;Alex James Boyd;Parminder Bhatia;Taha Kass-Hout;Cao Xiao;Fenglong Ma", "summary": "Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal\nattention distribution on visual inputs, leading to hallucinated or inaccurate\noutputs. Existing mitigation methods primarily rely on inference-time\ninterventions, which are limited in attention adaptation or require additional\nsupervision. To address this, we propose A$^3$Tune, a novel fine-tuning\nframework for Automatic Attention Alignment Tuning. A$^3$Tune leverages\nzero-shot weak labels from SAM, refines them into prompt-aware labels using\nBioMedCLIP, and then selectively modifies visually-critical attention heads to\nimprove alignment while minimizing interference. Additionally, we introduce a\nA$^3$MoE module, enabling adaptive parameter selection for attention tuning\nacross diverse prompts and images. Extensive experiments on medical VQA and\nreport generation benchmarks show that A$^3$Tune outperforms state-of-the-art\nbaselines, achieving enhanced attention distributions and performance in\nMed-LVLMs.", "comment": "Accepted to ACL2025 (main)", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.18503v1;http://arxiv.org/pdf/2505.18503v1", "pdf_url": "http://arxiv.org/pdf/2505.18503v1"}, {"title": "Characterizing the Expressivity of Transformer Language Models", "link": "https://arxiv.org/pdf/2505.23623", "details": "J Li, R Cotterell - arXiv preprint arXiv:2505.23623, 2025", "abstract": "Transformer-based language models (LMs) have achieved widespread empirical success, but their theoretical expressive power remains only partially understood. Prior work often relies on idealized models with assumptions--such as arbitrary \u2026", "entry_id": "http://arxiv.org/abs/2505.23623v1", "updated": "2025-05-29 16:30:30", "published": "2025-05-29 16:30:30", "authors": "Jiaoda Li;Ryan Cotterell", "summary": "Transformer-based language models (LMs) have achieved widespread empirical\nsuccess, but their theoretical expressive power remains only partially\nunderstood. Prior work often relies on idealized models with assumptions --\nsuch as arbitrary numerical precision and hard attention -- that diverge from\nreal-world transformers. In this work, we provide an exact characterization of\nfixed-precision transformers with strict future masking and soft attention, an\nidealization that more closely mirrors practical implementations. We show that\nthese models are precisely as expressive as a specific fragment of linear\ntemporal logic that includes only a single temporal operator: the past\noperator. We further relate this logic to established classes in formal\nlanguage theory, automata theory, and algebra, yielding a rich and unified\ntheoretical framework for understanding transformer expressivity. Finally, we\npresent empirical results that align closely with our theory: transformers\ntrained on languages within their theoretical capacity generalize perfectly\nover lengths, while they consistently fail to generalize on languages beyond\nit.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.23623v1;http://arxiv.org/pdf/2505.23623v1", "pdf_url": "http://arxiv.org/pdf/2505.23623v1"}, {"title": "FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2505.20225", "details": "H Kang, Z Yu, C Xiong - arXiv preprint arXiv:2505.20225, 2025", "abstract": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong efficiency-performance trade-offs by activating only a fraction of the model per token \u2026", "entry_id": "http://arxiv.org/abs/2505.20225v1", "updated": "2025-05-26 17:06:25", "published": "2025-05-26 17:06:25", "authors": "Hao Kang;Zichun Yu;Chenyan Xiong", "summary": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4\nincreasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong\nefficiency-performance trade-offs by activating only a fraction of the model\nper token. Yet academic researchers still lack a fully open, end-to-end MoE\nplatform for investigating scaling, routing, and expert behavior. We release\nFLAME-MoE, a completely open-source research suite composed of seven\ndecoder-only models, ranging from 38M to 1.7B active parameters, whose\narchitecture--64 experts with top-8 gating and 2 shared experts--closely\nreflects modern production LLMs. All training data pipelines, scripts, logs,\nand checkpoints are publicly available to enable reproducible experimentation.\nAcross six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4\npoints over dense baselines trained with identical FLOPs. Leveraging full\ntraining trace transparency, we present initial analyses showing that (i)\nexperts increasingly specialize on distinct token subsets, (ii) co-activation\nmatrices remain sparse, reflecting diverse expert usage, and (iii) routing\nbehavior stabilizes early in training. All code, training logs, and model\ncheckpoints are available at https://github.com/cmu-flame/FLAME-MoE.", "comment": "All code, training logs, and model checkpoints are available at\n  https://github.com/cmu-flame/FLAME-MoE", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.20225v1;http://arxiv.org/pdf/2505.20225v1", "pdf_url": "http://arxiv.org/pdf/2505.20225v1"}, {"title": "Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2505.16416", "details": "C Wang, J Guo, H Li, Y Tian, Y Nie, C Xu, K Han - arXiv preprint arXiv:2505.16416, 2025", "abstract": "Rotary Position Embedding (RoPE) is a widely adopted technique for encoding relative positional information in large language models (LLMs). However, when extended to large vision-language models (LVLMs), its variants introduce \u2026", "entry_id": "http://arxiv.org/abs/2505.16416v1", "updated": "2025-05-22 09:05:01", "published": "2025-05-22 09:05:01", "authors": "Chengcheng Wang;Jianyuan Guo;Hongguang Li;Yuchuan Tian;Ying Nie;Chang Xu;Kai Han", "summary": "Rotary Position Embedding (RoPE) is a widely adopted technique for encoding\nrelative positional information in large language models (LLMs). However, when\nextended to large vision-language models (LVLMs), its variants introduce\nunintended cross-modal positional biases. Specifically, they enforce relative\npositional dependencies between text token indices and image tokens, causing\nspurious alignments. This issue arises because image tokens representing the\nsame content but located at different spatial positions are assigned distinct\npositional biases, leading to inconsistent cross-modal associations. To address\nthis, we propose Per-Token Distance (PTD) - a simple yet effective metric for\nquantifying the independence of positional encodings across modalities.\nInformed by this analysis, we introduce Circle-RoPE, a novel encoding scheme\nthat maps image token indices onto a circular trajectory orthogonal to the\nlinear path of text token indices, forming a cone-like structure. This\nconfiguration ensures that each text token maintains an equal distance to all\nimage tokens, reducing artificial cross-modal biases while preserving\nintra-image spatial information. To further enhance performance, we propose a\nstaggered layer strategy that applies different RoPE variants across layers.\nThis design leverages the complementary strengths of each RoPE variant, thereby\nenhancing the model's overall performance. Our experimental results demonstrate\nthat our method effectively preserves spatial information from images while\nreducing relative positional bias, offering a more robust and flexible\npositional encoding framework for LVLMs. The code is available at\n[https://github.com/lose4578/CircleRoPE](https://github.com/lose4578/CircleRoPE).", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.16416v1;http://arxiv.org/pdf/2505.16416v1", "pdf_url": "http://arxiv.org/pdf/2505.16416v1"}, {"title": "Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models", "link": "https://arxiv.org/pdf/2505.11326", "details": "KP Yu, J Chai - arXiv preprint arXiv:2505.11326, 2025", "abstract": "Vision-language models (VLMs) have shown remarkable progress in offline tasks such as image captioning and video question answering. However, real-time interactive environments impose new demands on VLMs, requiring them to generate \u2026", "entry_id": "http://arxiv.org/abs/2505.11326v1", "updated": "2025-05-16 14:48:30", "published": "2025-05-16 14:48:30", "authors": "Keunwoo Peter Yu;Joyce Chai", "summary": "Vision-language models (VLMs) have shown remarkable progress in offline tasks\nsuch as image captioning and video question answering. However, real-time\ninteractive environments impose new demands on VLMs, requiring them to generate\nutterances that are not only semantically accurate but also precisely timed. We\nidentify two core capabilities necessary for such settings --\n$\\textit{perceptual updating}$ and $\\textit{contingency awareness}$ -- and\npropose a new benchmark task, $\\textbf{Temporally-Grounded Language Generation\n(TGLG)}$, to evaluate them. TGLG requires models to generate utterances in\nresponse to streaming video such that both content and timing align with\ndynamic visual input. To support this benchmark, we curate evaluation datasets\nfrom sports broadcasting and egocentric human interaction domains, and\nintroduce a new metric, $\\textbf{TRACE}$, to evaluate TGLG by jointly measuring\nsemantic similarity and temporal alignment. Finally, we present\n$\\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$,\na model that interleaves visual and linguistic tokens in a time-synchronized\nmanner, enabling real-time language generation without relying on turn-based\nassumptions. Experimental results show that VLM-TSI significantly outperforms a\nstrong baseline, yet overall performance remains modest -- highlighting the\ndifficulty of TGLG and motivating further research in real-time VLMs. Code and\ndata available $\\href{https://github.com/yukw777/tglg}{here}$.", "comment": "18 pages", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.11326v1;http://arxiv.org/pdf/2505.11326v1", "pdf_url": "http://arxiv.org/pdf/2505.11326v1"}, {"title": "Efficient Long CoT Reasoning in Small Language Models", "link": "https://arxiv.org/pdf/2505.18440", "details": "Z Wang, J Jiang, T Qiu, H Liu, X Tang, H Yao - arXiv preprint arXiv:2505.18440, 2025", "abstract": "Recent large reasoning models such as DeepSeek-R1 exhibit strong complex problems solving abilities by generating long chain-of-thought (CoT) reasoning steps. It is challenging to directly train small language models (SLMs) to emerge long \u2026", "entry_id": "http://arxiv.org/abs/2505.18440v1", "updated": "2025-05-24 00:22:52", "published": "2025-05-24 00:22:52", "authors": "Zhaoyang Wang;Jinqi Jiang;Tian Qiu;Hui Liu;Xianfeng Tang;Huaxiu Yao", "summary": "Recent large reasoning models such as DeepSeek-R1 exhibit strong complex\nproblems solving abilities by generating long chain-of-thought (CoT) reasoning\nsteps. It is challenging to directly train small language models (SLMs) to\nemerge long CoT. Thus, distillation becomes a practical method to enable SLMs\nfor such reasoning ability. However, the long CoT often contains a lot of\nredundant contents (e.g., overthinking steps) which may make SLMs hard to learn\nconsidering their relatively poor capacity and generalization. To address this\nissue, we propose a simple-yet-effective method to prune unnecessary steps in\nlong CoT, and then employ an on-policy method for the SLM itself to curate\nvalid and useful long CoT training data. In this way, SLMs can effectively\nlearn efficient long CoT reasoning and preserve competitive performance at the\nsame time. Experimental results across a series of mathematical reasoning\nbenchmarks demonstrate the effectiveness of the proposed method in distilling\nlong CoT reasoning ability into SLMs which maintains the competitive\nperformance but significantly reduces generating redundant reasoning steps.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.18440v1;http://arxiv.org/pdf/2505.18440v1", "pdf_url": "http://arxiv.org/pdf/2505.18440v1"}, {"title": "Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models", "link": "https://arxiv.org/pdf/2505.23667", "details": "L Cao, J Xu, H Liu, J Wang, M Zhou, H Dong, S Han\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they \u2026", "entry_id": "http://arxiv.org/abs/2505.23667v1", "updated": "2025-05-29 17:13:40", "published": "2025-05-29 17:13:40", "authors": "Lang Cao;Jingxian Xu;Hanbing Liu;Jinyu Wang;Mengyu Zhou;Haoyu Dong;Shi Han;Dongmei Zhang", "summary": "Tables are a fundamental structure for organizing and analyzing data, making\neffective table understanding a critical capability for intelligent systems.\nWhile large language models (LMs) demonstrate strong general reasoning\nabilities, they continue to struggle with accurate numerical or symbolic\nreasoning over tabular data, especially in complex scenarios. Spreadsheet\nformulas provide a powerful and expressive medium for representing executable\nsymbolic operations, encoding rich reasoning patterns that remain largely\nunderutilized. In this paper, we propose Formula Tuning (Fortune), a\nreinforcement learning (RL) framework that trains LMs to generate executable\nspreadsheet formulas for question answering over general tabular data. Formula\nTuning reduces the reliance on supervised formula annotations by using binary\nanswer correctness as a reward signal, guiding the model to learn formula\nderivation through reasoning. We provide a theoretical analysis of its\nadvantages and demonstrate its effectiveness through extensive experiments on\nseven table reasoning benchmarks. Formula Tuning substantially enhances LM\nperformance, particularly on multi-step numerical and symbolic reasoning tasks,\nenabling a 7B model to outperform O1 on table understanding. This highlights\nthe potential of formula-driven RL to advance symbolic table reasoning in LMs.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.23667v1;http://arxiv.org/pdf/2505.23667v1", "pdf_url": "http://arxiv.org/pdf/2505.23667v1"}]
