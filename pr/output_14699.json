[{"title": "RESPONSE: Benchmarking the Ability of Language Models to Undertake Commonsense Reasoning in Crisis Situation", "link": "https://arxiv.org/pdf/2503.11348", "details": "A Diallo, A Bikakis, L Dickens, A Hunter, R Miller - arXiv preprint arXiv:2503.11348, 2025", "abstract": "An interesting class of commonsense reasoning problems arises when people are faced with natural disasters. To investigate this topic, we present\\textsf {RESPONSE}, a human-curated dataset containing 1789 annotated instances featuring 6037 sets of \u2026"}, {"title": "Language Models May Verbatim Complete TextThey Were Not Explicitly Trained On", "link": "https://arxiv.org/pdf/2503.17514%3F", "details": "KZ Liu, CA Choquette-Choo, M Jagielski, P Kairouz\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "An important question today is whether a given text was used to train a large language model (LLM). A\\emph {completion} test is often employed: check if the LLM completes a sufficiently complex text. This, however, requires a ground-truth \u2026"}, {"title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing", "link": "https://arxiv.org/pdf/2503.08588", "details": "X Xu, W Xu, N Zhang, J McAuley - arXiv preprint arXiv:2503.08588, 2025", "abstract": "Previous studies have established that language models manifest stereotyped biases. Existing debiasing strategies, such as retraining a model with counterfactual data, representation projection, and prompting often fail to efficiently eliminate bias or \u2026"}, {"title": "Audio-reasoner: Improving reasoning capability in large audio language models", "link": "https://arxiv.org/pdf/2503.02318", "details": "Z Xie, M Lin, Z Liu, P Wu, S Yan, C Miao - arXiv preprint arXiv:2503.02318, 2025", "abstract": "Recent advancements in multimodal reasoning have largely overlooked the audio modality. We introduce Audio-Reasoner, a large-scale audio language model for deep reasoning in audio tasks. We meticulously curated a large-scale and diverse \u2026"}, {"title": "Auditing language models for hidden objectives", "link": "https://arxiv.org/pdf/2503.10965%3F", "details": "S Marks, J Treutlein, T Bricken, J Lindsey, J Marcus\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We study the feasibility of conducting alignment audits: investigations into whether models have undesired objectives. As a testbed, we train a language model with a hidden objective. Our training pipeline first teaches the model about exploitable \u2026"}, {"title": "Process-based self-rewarding language models", "link": "https://arxiv.org/pdf/2503.03746", "details": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' \u2026"}, {"title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models", "link": "https://arxiv.org/pdf/2503.09573%3F", "details": "M Arriola, A Gokaslan, JT Chiu, Z Yang, Z Qi, J Han\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a \u2026"}, {"title": "Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models", "link": "https://arxiv.org/pdf/2503.15888", "details": "B Bi, S Liu, Y Wang, Y Xu, J Fang, L Mei, X Cheng - arXiv preprint arXiv:2503.15888, 2025", "abstract": "Retrieval-Augmented Generation (RAG) mitigates hallucinations in Large Language Models (LLMs) by integrating external knowledge. However, conflicts between parametric knowledge and retrieved context pose challenges, particularly when \u2026"}, {"title": "Integrating Large Language Models with Human Expertise for Disease Detection in Electronic Health Records", "link": "https://arxiv.org/pdf/2504.00053", "details": "J Pan, S Lee, C Cheligeer, EA Martin, K Riazi, H Quan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Objective: Electronic health records (EHR) are widely available to complement administrative data-based disease surveillance and healthcare performance evaluation. Defining conditions from EHR is labour-intensive and requires extensive \u2026"}]
