[{"title": "Enhancing the Reasoning Capabilities of Small Language Models via Solution Guidance Fine-Tuning", "link": "https://arxiv.org/pdf/2412.09906", "details": "J Bi, Y Wu, W Xing, Z Wei - arXiv preprint arXiv:2412.09906, 2024", "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks. Advances in prompt engineering and fine-tuning techniques have further enhanced their ability to address complex reasoning challenges \u2026"}, {"title": "Large language models: game-changers in the healthcare industry", "link": "https://pubmed.ncbi.nlm.nih.gov/39674769/", "details": "B Dong, L Zhang, J Yuan, Y Chen, Q Li, L Shen - Science bulletin, 2024", "abstract": "Large language models: game-changers in the healthcare industry Large language models: game-changers in the healthcare industry Sci Bull (Beijing). 2024 Nov 26:S2095-9273(24)00847-8. doi: 10.1016/j.scib.2024.11.031. Online ahead of print. Authors Bin Dong 1 , Li Zhang \u2026"}, {"title": "Towards Efficient Low-order Hybrid Optimizer for Language Model Fine-tuning", "link": "https://zeyiwen.github.io/papers/aaai25-hybrid_optimizer.pdf", "details": "M Chen, YL Huang, Z Wen - 2025", "abstract": "As the size of language models notably grows, fine-tuning the models becomes more challenging: fine-tuning with firstorder optimizers (eg, SGD and Adam) requires high memory consumption, while fine-tuning with a memory-efficient zeroth-order \u2026"}]
