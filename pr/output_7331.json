[{"title": "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "link": "https://arxiv.org/pdf/2409.14083", "details": "J Sun, J Zhang, Y Zhou, Z Su, X Qu, Y Cheng - arXiv preprint arXiv:2409.14083, 2024", "abstract": "Large Vision-Language Models (LVLMs) have become pivotal at the intersection of computer vision and natural language processing. However, the full potential of LVLMs Retrieval-Augmented Generation (RAG) capabilities remains underutilized \u2026"}, {"title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "link": "https://arxiv.org/pdf/2409.13853", "details": "Z Wang, R Bao, Y Wu, J Taylor, C Xiao, F Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pretrained large language models (LLMs) have revolutionized natural language processing (NLP) tasks such as summarization, question answering, and translation. However, LLMs pose significant security risks due to their tendency to memorize \u2026"}]
