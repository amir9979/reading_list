[{"title": "Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization", "link": "https://arxiv.org/pdf/2503.22577%3F", "details": "I Pikabea, I Lacunza, O Pareras, C Escolano\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Rapid advancements in Visual Language Models (VLMs) have transformed multimodal understanding but are often constrained by generating English responses regardless of the input language. This phenomenon has been termed as \u2026"}, {"title": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection", "link": "https://arxiv.org/pdf/2504.00695", "details": "X Zhu, Z Gu, S Zheng, T Wang, T Li, H Feng, Y Xiao - arXiv preprint arXiv:2504.00695, 2025", "abstract": "Pre-training large language models (LLMs) necessitates enormous diverse textual corpora, making effective data selection a key challenge for balancing computational resources and model performance. Current methodologies primarily emphasize data \u2026"}, {"title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?", "link": "https://arxiv.org/pdf/2504.00509", "details": "K Yan, Y Xu, Z Du, X Yao, Z Wang, X Guo, J Chen - arXiv preprint arXiv:2504.00509, 2025", "abstract": "The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' \u2026"}, {"title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.23064", "details": "Y Ren, K Tertikas, S Maiti, J Han, T Zhang, S S\u00fcsstrunk\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision-Language Models (LVLMs) struggle with puzzles, which require precise perception, rule comprehension, and logical reasoning. Assessing and enhancing their performance in this domain is crucial, as it reflects their ability to \u2026"}, {"title": "Beyond Standard MoE: Mixture of Latent Experts for Resource-Efficient Language Models", "link": "https://arxiv.org/pdf/2503.23100", "details": "Z Liu, H Wu, R She, X Fu, X Han, T Zhong, M Yuan - arXiv preprint arXiv:2503.23100, 2025", "abstract": "Mixture of Experts (MoE) has emerged as a pivotal architectural paradigm for efficient scaling of Large Language Models (LLMs), operating through selective activation of parameter subsets for each input token. Nevertheless, conventional MoE \u2026"}, {"title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning", "link": "https://arxiv.org/pdf/2504.08837", "details": "H Wang, C Qu, Z Huang, W Chu, F Lin, W Chen - arXiv preprint arXiv:2504.08837, 2025", "abstract": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various \u2026"}, {"title": "Synthetic Data Enhances Mathematical Reasoning of Language Models Based on Artificial Intelligence", "link": "https://www.itc.ktu.lt/index.php/ITC/article/view/39713/16892", "details": "Z Han, W Jiang - Information Technology and Control, 2025", "abstract": "Current large language models (LLMs) training involves extensive training data and computing resources to handle multiple natural language processing (NLP) tasks. This paper endeavors to assist individuals to compose feasible mathematical \u2026"}, {"title": "Extracting Patient History from Clinical Text: A Comparative Study of Clinical Large Language Models", "link": "https://arxiv.org/pdf/2503.23281%3F", "details": "H Nghiem, TD Le, S Chen, T Thieu, A Gin, EP Nguyen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Extracting medical history entities (MHEs) related to a patient's chief complaint (CC), history of present illness (HPI), and past, family, and social history (PFSH) helps structure free-text clinical notes into standardized EHRs, streamlining downstream \u2026"}, {"title": "Explainable feature embeddings from histopathology foundation models: a case study for end stage kidney disease risk analysis in diabetic nephropathy patients", "link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13413/134130M/Explainable-feature-embeddings-from-histopathology-foundation-models--a-case/10.1117/12.3047908.short", "details": "HR Kasireddy, N Lucarelli, D Yun, KC Moon\u2026 - Medical Imaging 2025 \u2026, 2025", "abstract": "Foundational models (FMs) based on advanced neural network architectures have demonstrated improved performance in pathology image analysis across various organs due to their increased generalizability. However, their clinical adoption \u2026"}]
