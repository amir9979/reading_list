[{"title": "Algorithmic Fairness Generalization under Covariate and Dependence Shifts Simultaneously", "link": "https://dl.acm.org/doi/abs/10.1145/3637528.3671909", "details": "C Zhao, K Jiang, X Wu, H Wang, L Khan, C Grant\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "The endeavor to preserve the generalization of a fair and invariant classifier across domains, especially in the presence of distribution shifts, becomes a significant and intricate challenge in machine learning. In response to this challenge, numerous \u2026"}, {"title": "Neural Collapse Inspired Debiased Representation Learning for Min-max Fairness", "link": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671902", "details": "S Lu, J Chai, X Wang - Proceedings of the 30th ACM SIGKDD Conference on \u2026, 2024", "abstract": "Although machine learning algorithms demonstrate impressive performance, their trustworthiness remains a critical issue, particularly concerning fairness when implemented in real-world applications. Many notions of group fairness aim to \u2026"}, {"title": "Explainable Artificial Intelligence on Biosignals for Clinical Decision Support", "link": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671459", "details": "MC Maurer, JM Metsch, P Hempel, T Bender\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "Deep learning has proven effective in several areas, including computer vision, natural language processing, and disease prediction, which can support clinicians in making decisions along the clinical pathway. However, in order to successfully \u2026"}, {"title": "Advancing entity alignment with dangling cases: a structure-aware approach through optimal transport learning and contrastive learning", "link": "https://link.springer.com/article/10.1007/s00521-024-10276-1", "details": "J Xu, Y Li, X Xie, N Hu, Y Li, HT Zheng, Y Jiang - Neural Computing and Applications, 2024", "abstract": "Entity alignment (EA) aims to discover the equivalent entities in different knowledge graphs (KGs), which plays an important role in knowledge engineering. Recently, EA with dangling entities has been proposed as a more realistic setting, which assumes \u2026"}, {"title": "SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models", "link": "https://arxiv.org/pdf/2408.02632", "details": "M Diao, R Li, S Liu, G Liao, J Wang, X Cai, W Xu - arXiv preprint arXiv:2408.02632, 2024", "abstract": "As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to \u2026"}, {"title": "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "link": "https://arxiv.org/pdf/2408.02103", "details": "P Wang, X Wang, C Lou, S Mao, P Xie, Y Jiang - arXiv preprint arXiv:2408.02103, 2024", "abstract": "In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language \u2026"}, {"title": "Factual and Tailored Recommendation Endorsements using Language Models and Reinforcement Learning", "link": "https://openreview.net/forum%3Fid%3DxI8C7sfN1H", "details": "J Jeong, Y Chow, G Tennenholtz, CW Hsu\u2026 - First Conference on Language \u2026", "abstract": "Recommender systems (RSs) play a central role in matching candidate items to users based on their preferences. While traditional RSs rely on user feed-back signals, conversational RSs interact with users in natural language. In this work, we \u2026"}, {"title": "Model-Agnostic Random Weighting for Out-of-Distribution Generalization", "link": "https://dl.acm.org/doi/abs/10.1145/3637528.3671762", "details": "Y He, P Tian, R Xu, X Shen, X Zhang, P Cui - Proceedings of the 30th ACM SIGKDD \u2026, 2024", "abstract": "Despite the encouraging successes in numerous applications, machine learning methods grounded on the iid assumption often experience performance deterioration when confronted with the distribution shift between training and test data. This \u2026"}, {"title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Trustworthy Response Generation in Chinese", "link": "https://dl.acm.org/doi/pdf/10.1145/3686807", "details": "H Wang, S Zhao, Z Qiang, Z Li, C Liu, N Xi, Y Du, B Qin\u2026 - ACM Transactions on \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to \u2026"}]
