[{"title": "Combining Incomplete Observational and Randomized Data for Heterogeneous Treatment Effects", "link": "https://dl.acm.org/doi/pdf/10.1145/3627673.3679593", "details": "D Yao, C Tang, Q Cui, L Li - Proceedings of the 33rd ACM International Conference \u2026, 2024", "abstract": "Data from observational studies (OSs) is widely available and readily obtainable yet frequently contains confounding biases. On the other hand, data derived from randomized controlled trials (RCTs) helps to reduce these biases; however, it is \u2026"}, {"title": "How to Train Long-Context Language Models (Effectively)", "link": "https://arxiv.org/pdf/2410.02660%3F", "details": "T Gao, A Wettig, H Yen, D Chen - arXiv preprint arXiv:2410.02660, 2024", "abstract": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development--Instead of perplexity or simple \u2026"}, {"title": "Vision Language Model is NOT All You Need: Augmentation Strategies for Molecule Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3627673.3679607", "details": "N Lee, S Laghuvarapu, C Park, J Sun - Proceedings of the 33rd ACM International \u2026, 2024", "abstract": "Recently, there has been a growing interest among researchers in understanding molecules and their textual descriptions through molecule language models (MoLM). However, despite some early promising developments, the advancement of MoLM \u2026"}, {"title": "Tuning Language Models by Mixture-of-Depths Ensemble", "link": "https://arxiv.org/pdf/2410.13077", "details": "H Luo, L Specia - arXiv preprint arXiv:2410.13077, 2024", "abstract": "Transformer-based Large Language Models (LLMs) traditionally rely on final-layer loss for training and final-layer representations for predictions, potentially overlooking the predictive power embedded in intermediate layers. Surprisingly, we \u2026"}, {"title": "Causal Data Fusion for Panel Data without Pre-Intervention Period", "link": "https://arxiv.org/pdf/2410.16391", "details": "Z Yang, SH Lee, JR K\u00f6hler, AE Ghassami - arXiv preprint arXiv:2410.16391, 2024", "abstract": "Traditional panel data causal inference frameworks, such as difference-in- differences and synthetic control methods, rely on pre-intervention data to estimate counterfactuals, which may not be available in real-world settings when interventions \u2026"}, {"title": "Manual Verbalizer Enrichment for Few-Shot Text Classification", "link": "https://arxiv.org/pdf/2410.06173", "details": "QA Nguyen, N Tomeh, M Lebbah, T Charnois, H Azzag\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the continuous development of pre-trained language models, prompt-based training becomes a well-adopted paradigm that drastically improves the exploitation of models for many natural language processing tasks. Prompting also shows great \u2026"}, {"title": "Two-stage Learning-to-Defer for Multi-Task Learning", "link": "https://arxiv.org/pdf/2410.15729", "details": "M Yannis, YS Heng, C Axel, NL Xing, OW Tsang - arXiv preprint arXiv:2410.15729, 2024", "abstract": "The Learning-to-Defer approach has been explored for classification and, more recently, regression tasks separately. Many contemporary learning tasks, however, involves both classification and regression components. In this paper, we introduce a \u2026"}, {"title": "Large language models enabled multiagent ensemble method for efficient EHR data labeling", "link": "https://arxiv.org/pdf/2410.16543", "details": "J Huang, K Nezafati, I Villanueva-Miranda, Z Gu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study introduces a novel multiagent ensemble method powered by LLMs to address a key challenge in ML-data labeling, particularly in large-scale EHR datasets. Manual labeling of such datasets requires domain expertise and is labor \u2026"}, {"title": "The Role of Deductive and Inductive Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2410.02892", "details": "C Cai, X Zhao, H Liu, Z Jiang, T Zhang, Z Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have achieved substantial progress in artificial intelligence, particularly in reasoning tasks. However, their reliance on static prompt structures, coupled with limited dynamic reasoning capabilities, often constrains their \u2026"}]
