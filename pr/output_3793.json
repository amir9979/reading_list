[{"title": "Understanding the Role of User Profile in the Personalization of Large Language Models", "link": "https://arxiv.org/pdf/2406.17803", "details": "B Wu, Z Shi, HA Rahmani, V Ramineni, E Yilmaz - arXiv preprint arXiv:2406.17803, 2024", "abstract": "Utilizing user profiles to personalize Large Language Models (LLMs) has been shown to enhance the performance on a wide range of tasks. However, the precise role of user profiles and their effect mechanism on LLMs remains unclear. This study \u2026"}, {"title": "GCON: Differentially Private Graph Convolutional Network via Objective Perturbation", "link": "https://arxiv.org/pdf/2407.05034", "details": "J Wei, Y Zhu, X Xiao, E Bao, Y Yang, K Cai, BC Ooi - arXiv preprint arXiv:2407.05034, 2024", "abstract": "Graph Convolutional Networks (GCNs) are a popular machine learning model with a wide range of applications in graph analytics, including healthcare, transportation, and finance. Similar to other neural networks, a GCN may memorize parts of the \u2026"}, {"title": "Hybrid Explanatory Interactive Machine Learning for Medical Diagnosis", "link": "https://link.springer.com/chapter/10.1007/978-3-031-63211-2_9", "details": "E Slany, S Scheele, U Schmid - IFIP International Conference on Artificial Intelligence \u2026, 2024", "abstract": "Abstract Machine learning (ML) models can be an effective assistance in medical diagnosis if they allow physicians to project their knowledge into model's internal mechanism. Using model-agnostic explanatory interactive ML (XIML), physicians \u2026"}, {"title": "Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?", "link": "https://arxiv.org/pdf/2406.16316", "details": "Y Jinnai - arXiv preprint arXiv:2406.16316, 2024", "abstract": "Alignment of the language model with human preferences is a common approach to making a language model useful to end users. However, most alignment work is done in English, and human preference datasets are dominated by English \u2026"}, {"title": "Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language Models", "link": "https://arxiv.org/pdf/2407.07263", "details": "J Parmar, S Satheesh, M Patwary, M Shoeybi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As language models have scaled both their number of parameters and pretraining dataset sizes, the computational cost for pretraining has become intractable except for the most well-resourced teams. This increasing cost makes it ever more important \u2026"}, {"title": "Scaling Laws for Linear Complexity Language Models", "link": "https://arxiv.org/pdf/2406.16690", "details": "X Shen, D Li, R Leng, Z Qin, W Sun, Y Zhong - arXiv preprint arXiv:2406.16690, 2024", "abstract": "The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their \u2026"}, {"title": "Protecting Privacy Through Approximating Optimal Parameters for Sequence Unlearning in Language Models", "link": "https://arxiv.org/pdf/2406.14091", "details": "D Lee, D Rim, M Choi, J Choo - arXiv preprint arXiv:2406.14091, 2024", "abstract": "Although language models (LMs) demonstrate exceptional capabilities on various tasks, they are potentially vulnerable to extraction attacks, which represent a significant privacy risk. To mitigate the privacy concerns of LMs, machine unlearning \u2026"}, {"title": "Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization", "link": "https://arxiv.org/pdf/2406.16743", "details": "Z Zhao, X Zhang, K Xu, X Hu, R Zhang, Z Du, Q Guo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the widespread application of Large Language Models (LLMs), it has become a significant concern to ensure their safety and prevent harmful responses. While current safe-alignment methods based on instruction fine-tuning and Reinforcement \u2026"}, {"title": "Revealing the True Cost of Locally Differentially Private Protocols: An Auditing Perspective", "link": "https://petsymposium.org/popets/2024/popets-2024-0110.pdf", "details": "HH Arcolezi, S Gambs - Proceedings on Privacy Enhancing Technologies, 2024", "abstract": "While the existing literature on Differential Privacy (DP) auditing predominantly focuses on the centralized model (eg, in auditing the DP-SGD algorithm), we advocate for extending this approach to audit Local DP (LDP). To achieve this, we \u2026"}]
