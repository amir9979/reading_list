[{"title": "Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models", "link": "https://arxiv.org/pdf/2505.10446", "details": "Z Huang, Z Chen, Z Wang, T Li, GJ Qi - arXiv preprint arXiv:2505.10446, 2025", "abstract": "We introduce the\\emph {Diffusion Chain of Lateral Thought (DCoLT)}, a reasoning framework for diffusion language models. DCoLT treats each intermediate step in the reverse diffusion process as a latent\" thinking\" action and optimizes the entire \u2026", "entry_id": "http://arxiv.org/abs/2505.10446v2", "updated": "2025-05-21 01:44:47", "published": "2025-05-15 16:06:32", "authors": "Zemin Huang;Zhiyang Chen;Zijun Wang;Tiancheng Li;Guo-Jun Qi", "summary": "We introduce the Diffusion Chain of Lateral Thought (DCoLT), a reasoning\nframework for diffusion language models. DCoLT treats each intermediate step in\nthe reverse diffusion process as a latent \"thinking\" action and optimizes the\nentire reasoning trajectory to maximize the reward on the correctness of the\nfinal answer with outcome-based Reinforcement Learning (RL). Unlike traditional\nChain-of-Thought (CoT) methods that follow a causal, linear thinking process,\nDCoLT allows bidirectional, non-linear reasoning with no strict rule on\ngrammatical correctness amid its intermediate steps of thought. We implement\nDCoLT on two representative Diffusion Language Models (DLMs). First, we choose\nSEDD as a representative continuous-time discrete diffusion model, where its\nconcrete score derives a probabilistic policy to maximize the RL reward over\nthe entire sequence of intermediate diffusion steps. We further consider the\ndiscrete-time masked diffusion language model -- LLaDA, and find that the order\nto predict and unmask tokens plays an essential role to optimize its RL action\nresulting from the ranking-based Unmasking Policy Module (UPM) defined by the\nPlackett-Luce model. Experiments on both math and code generation tasks show\nthat using only public data and 16 H800 GPUs, DCoLT-reinforced DLMs outperform\nother DLMs trained by SFT or RL or even both. Notably, DCoLT-reinforced LLaDA\nboosts its reasoning accuracy by +9.8%, +5.7%, +11.4%, +19.5% on GSM8K, MATH,\nMBPP, and HumanEval.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.10446v2;http://arxiv.org/pdf/2505.10446v2", "pdf_url": "http://arxiv.org/pdf/2505.10446v2"}, {"title": "Pre-training Large Memory Language Models with Internal and External Knowledge", "link": "https://arxiv.org/pdf/2505.15962", "details": "L Zhao, S Zalouk, CK Belardi, J Lovelace, JP Zhou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Neural language models are black-boxes--both linguistic patterns and factual knowledge are distributed across billions of opaque parameters. This entangled encoding makes it difficult to reliably inspect, verify, or update specific facts. We \u2026", "entry_id": "http://arxiv.org/abs/2505.15962v1", "updated": "2025-05-21 19:26:03", "published": "2025-05-21 19:26:03", "authors": "Linxi Zhao;Sofian Zalouk;Christian K. Belardi;Justin Lovelace;Jin Peng Zhou;Kilian Q. Weinberger;Yoav Artzi;Jennifer J. Sun", "summary": "Neural language models are black-boxes -- both linguistic patterns and\nfactual knowledge are distributed across billions of opaque parameters. This\nentangled encoding makes it difficult to reliably inspect, verify, or update\nspecific facts. We propose a new class of language models, Large Memory\nLanguage Models (LMLM) with a pre-training recipe that stores factual knowledge\nin both internal weights and an external database. Our approach strategically\nmasks externally retrieved factual values from the training loss, thereby\nteaching the model to perform targeted lookups rather than relying on\nmemorization in model weights. Our experiments demonstrate that LMLMs achieve\ncompetitive performance compared to significantly larger, knowledge-dense LLMs\non standard benchmarks, while offering the advantages of explicit, editable,\nand verifiable knowledge bases. This work represents a fundamental shift in how\nlanguage models interact with and manage factual knowledge.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.15962v1;http://arxiv.org/pdf/2505.15962v1", "pdf_url": "http://arxiv.org/pdf/2505.15962v1"}, {"title": "Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation", "link": "https://arxiv.org/pdf/2505.16763", "details": "H Yang, Y Zhou, W Han, J Shen - arXiv preprint arXiv:2505.16763, 2025", "abstract": "Text-to-image models are powerful for producing high-quality images based on given text prompts, but crafting these prompts often requires specialized vocabulary. To address this, existing methods train rewriting models with supervision from large \u2026", "entry_id": "http://arxiv.org/abs/2505.16763v1", "updated": "2025-05-22 15:05:07", "published": "2025-05-22 15:05:07", "authors": "Hongji Yang;Yucheng Zhou;Wencheng Han;Jianbing Shen", "summary": "Text-to-image models are powerful for producing high-quality images based on\ngiven text prompts, but crafting these prompts often requires specialized\nvocabulary. To address this, existing methods train rewriting models with\nsupervision from large amounts of manually annotated data and trained aesthetic\nassessment models. To alleviate the dependence on data scale for model training\nand the biases introduced by trained models, we propose a novel prompt\noptimization framework, designed to rephrase a simple user prompt into a\nsophisticated prompt to a text-to-image model. Specifically, we employ the\nlarge vision language models (LVLMs) as the solver to rewrite the user prompt,\nand concurrently, employ LVLMs as a reward model to score the aesthetics and\nalignment of the images generated by the optimized prompt. Instead of laborious\nhuman feedback, we exploit the prior knowledge of the LVLM to provide rewards,\ni.e., AI feedback. Simultaneously, the solver and the reward model are unified\ninto one model and iterated in reinforcement learning to achieve\nself-improvement by giving a solution and judging itself. Results on two\npopular datasets demonstrate that our method outperforms other strong\ncompetitors.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.16763v1;http://arxiv.org/pdf/2505.16763v1", "pdf_url": "http://arxiv.org/pdf/2505.16763v1"}, {"title": "COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3717867.3717923", "details": "P Govil, H Jain, V Bonagiri, A Chadha, P Kumaraguru\u2026 - Proceedings of the 17th \u2026, 2025", "abstract": "Large Language Models (LLMs) often inherit biases from the web data they are trained on, which contains stereotypes and prejudices. Current methods for evaluating and mitigating these biases rely on bias-benchmark datasets. These \u2026"}, {"title": "Dense Communication between Language Models", "link": "https://arxiv.org/pdf/2505.12741", "details": "S Wu, Y Wang, Q Yao - arXiv preprint arXiv:2505.12741, 2025", "abstract": "As higher-level intelligence emerges from the combination of modular components with lower-level intelligence, many works combines Large Language Models (LLMs) for collective intelligence. Such combination is achieved by building communications \u2026", "entry_id": "http://arxiv.org/abs/2505.12741v1", "updated": "2025-05-19 05:56:06", "published": "2025-05-19 05:56:06", "authors": "Shiguang Wu;Yaqing Wang;Quanming Yao", "summary": "As higher-level intelligence emerges from the combination of modular\ncomponents with lower-level intelligence, many works combines Large Language\nModels (LLMs) for collective intelligence. Such combination is achieved by\nbuilding communications among LLMs. While current systems primarily facilitate\nsuch communication through natural language, this paper proposes a novel\nparadigm of direct dense vector communication between LLMs. Our approach\neliminates the unnecessary embedding and de-embedding steps when LLM interact\nwith another, enabling more efficient information transfer, fully\ndifferentiable optimization pathways, and exploration of capabilities beyond\nhuman heuristics. We use such stripped LLMs as vertexes and optimizable seq2seq\nmodules as edges to construct LMNet, with similar structure as MLPs. By\nutilizing smaller pre-trained LLMs as vertexes, we train a LMNet that achieves\ncomparable performance with LLMs in similar size with only less than 0.1%\ntraining cost. This offers a new perspective on scaling for general\nintelligence rather than training a monolithic LLM from scratch. Besides, the\nproposed method can be used for other applications, like customizing LLM with\nlimited data, showing its versatility.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.12741v1;http://arxiv.org/pdf/2505.12741v1", "pdf_url": "http://arxiv.org/pdf/2505.12741v1"}, {"title": "Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2505.16416", "details": "C Wang, J Guo, H Li, Y Tian, Y Nie, C Xu, K Han - arXiv preprint arXiv:2505.16416, 2025", "abstract": "Rotary Position Embedding (RoPE) is a widely adopted technique for encoding relative positional information in large language models (LLMs). However, when extended to large vision-language models (LVLMs), its variants introduce \u2026", "entry_id": "http://arxiv.org/abs/2505.16416v1", "updated": "2025-05-22 09:05:01", "published": "2025-05-22 09:05:01", "authors": "Chengcheng Wang;Jianyuan Guo;Hongguang Li;Yuchuan Tian;Ying Nie;Chang Xu;Kai Han", "summary": "Rotary Position Embedding (RoPE) is a widely adopted technique for encoding\nrelative positional information in large language models (LLMs). However, when\nextended to large vision-language models (LVLMs), its variants introduce\nunintended cross-modal positional biases. Specifically, they enforce relative\npositional dependencies between text token indices and image tokens, causing\nspurious alignments. This issue arises because image tokens representing the\nsame content but located at different spatial positions are assigned distinct\npositional biases, leading to inconsistent cross-modal associations. To address\nthis, we propose Per-Token Distance (PTD) - a simple yet effective metric for\nquantifying the independence of positional encodings across modalities.\nInformed by this analysis, we introduce Circle-RoPE, a novel encoding scheme\nthat maps image token indices onto a circular trajectory orthogonal to the\nlinear path of text token indices, forming a cone-like structure. This\nconfiguration ensures that each text token maintains an equal distance to all\nimage tokens, reducing artificial cross-modal biases while preserving\nintra-image spatial information. To further enhance performance, we propose a\nstaggered layer strategy that applies different RoPE variants across layers.\nThis design leverages the complementary strengths of each RoPE variant, thereby\nenhancing the model's overall performance. Our experimental results demonstrate\nthat our method effectively preserves spatial information from images while\nreducing relative positional bias, offering a more robust and flexible\npositional encoding framework for LVLMs. The code is available at\n[https://github.com/lose4578/CircleRoPE](https://github.com/lose4578/CircleRoPE).", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.16416v1;http://arxiv.org/pdf/2505.16416v1", "pdf_url": "http://arxiv.org/pdf/2505.16416v1"}, {"title": "AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation", "link": "https://arxiv.org/pdf/2505.11887", "details": "X Zhang, Z Ouyang, L Wang, G de Melo, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the proliferation of large language models (LLMs) in the medical domain, there is increasing demand for improved evaluation techniques to assess their capabilities. However, traditional metrics like F1 and ROUGE, which rely on token overlaps to \u2026", "entry_id": "http://arxiv.org/abs/2505.11887v1", "updated": "2025-05-17 07:44:54", "published": "2025-05-17 07:44:54", "authors": "Xiechi Zhang;Zetian Ouyang;Linlin Wang;Gerard de Melo;Zhu Cao;Xiaoling Wang;Ya Zhang;Yanfeng Wang;Liang He", "summary": "With the proliferation of large language models (LLMs) in the medical domain,\nthere is increasing demand for improved evaluation techniques to assess their\ncapabilities. However, traditional metrics like F1 and ROUGE, which rely on\ntoken overlaps to measure quality, significantly overlook the importance of\nmedical terminology. While human evaluation tends to be more reliable, it can\nbe very costly and may as well suffer from inaccuracies due to limits in human\nexpertise and motivation. Although there are some evaluation methods based on\nLLMs, their usability in the medical field is limited due to their proprietary\nnature or lack of expertise. To tackle these challenges, we present\nAutoMedEval, an open-sourced automatic evaluation model with 13B parameters\nspecifically engineered to measure the question-answering proficiency of\nmedical LLMs. The overarching objective of AutoMedEval is to assess the quality\nof responses produced by diverse models, aspiring to significantly reduce the\ndependence on human evaluation. Specifically, we propose a hierarchical\ntraining method involving curriculum instruction tuning and an iterative\nknowledge introspection mechanism, enabling AutoMedEval to acquire professional\nmedical assessment capabilities with limited instructional data. Human\nevaluations indicate that AutoMedEval surpasses other baselines in terms of\ncorrelation with human judgments.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.11887v1;http://arxiv.org/pdf/2505.11887v1", "pdf_url": "http://arxiv.org/pdf/2505.11887v1"}, {"title": "NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning", "link": "https://arxiv.org/pdf/2505.16022", "details": "W Liu, S Qi, X Wang, C Qian, Y Du, Y He - arXiv preprint arXiv:2505.16022, 2025", "abstract": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of incentive training, a reinforcement learning paradigm that computes rewards solely based on the final answer part of a language model's output, thereby encouraging the \u2026", "entry_id": "http://arxiv.org/abs/2505.16022v1", "updated": "2025-05-21 21:12:35", "published": "2025-05-21 21:12:35", "authors": "Wei Liu;Siya Qi;Xinyu Wang;Chen Qian;Yali Du;Yulan He", "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training.", "comment": "20 pages, 5 tables, 12 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.16022v1;http://arxiv.org/pdf/2505.16022v1", "pdf_url": "http://arxiv.org/pdf/2505.16022v1"}, {"title": "IQBench: How \"Smart'' Are Vision-Language Models? A Study with Human IQ Tests", "link": "https://arxiv.org/pdf/2505.12000", "details": "TH Pham, PV Nguyen, DT Hung, BT Duong, VN Thanh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Although large Vision-Language Models (VLMs) have demonstrated remarkable performance in a wide range of multimodal tasks, their true reasoning capabilities on human IQ tests remain underexplored. To advance research on the fluid intelligence \u2026", "entry_id": "http://arxiv.org/abs/2505.12000v1", "updated": "2025-05-17 13:24:08", "published": "2025-05-17 13:24:08", "authors": "Tan-Hanh Pham;Phu-Vinh Nguyen;Dang The Hung;Bui Trong Duong;Vu Nguyen Thanh;Chris Ngo;Tri Quang Truong;Truong-Son Hy", "summary": "Although large Vision-Language Models (VLMs) have demonstrated remarkable\nperformance in a wide range of multimodal tasks, their true reasoning\ncapabilities on human IQ tests remain underexplored. To advance research on the\nfluid intelligence of VLMs, we introduce **IQBench**, a new benchmark designed\nto evaluate VLMs on standardized visual IQ tests. We focus on evaluating the\nreasoning capabilities of VLMs, which we argue are more important than the\naccuracy of the final prediction. **Our benchmark is visually centric,\nminimizing the dependence on unnecessary textual content**, thus encouraging\nmodels to derive answers primarily from image-based information rather than\nlearned textual knowledge. To this end, we manually collected and annotated 500\nvisual IQ questions to **prevent unintentional data leakage during training**.\nUnlike prior work that focuses primarily on the accuracy of the final answer,\nwe evaluate the reasoning ability of the models by assessing their explanations\nand the patterns used to solve each problem, along with the accuracy of the\nfinal prediction and human evaluation. Our experiments show that there are\nsubstantial performance disparities between tasks, with models such as\n`o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet` achieving the highest\naverage accuracies of 0.615, 0.578, and 0.548, respectively. However, all\nmodels struggle with 3D spatial and anagram reasoning tasks, highlighting\nsignificant limitations in current VLMs' general reasoning abilities. In terms\nof reasoning scores, `o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet`\nachieved top averages of 0.696, 0.586, and 0.516, respectively. These results\nhighlight inconsistencies between the reasoning processes of the models and\ntheir final answers, emphasizing the importance of evaluating the accuracy of\nthe reasoning in addition to the final predictions.", "comment": "IQ Test for Multimodal Models", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.12000v1;http://arxiv.org/pdf/2505.12000v1", "pdf_url": "http://arxiv.org/pdf/2505.12000v1"}]
