[{"title": "Is On-Device AI Broken and Exploitable? Assessing the Trust and Ethics in Small Language Models", "link": "https://arxiv.org/pdf/2406.05364", "details": "K Nakka, J Dani, N Saxena - arXiv preprint arXiv:2406.05364, 2024", "abstract": "In this paper, we present a very first study to investigate trust and ethical implications of on-device artificial intelligence (AI), focusing on''small''language models (SLMs) amenable for personal devices like smartphones. While on-device SLMs promise \u2026"}, {"title": "Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe", "link": "https://arxiv.org/pdf/2406.04165", "details": "A Ziarko, AQ Jiang, B Piotrowski, W Li, M Jamnik\u2026 - arXiv e-prints, 2024", "abstract": "Text embeddings are essential for many tasks, such as document retrieval, clustering, and semantic similarity assessment. In this paper, we study how to contrastively train text embedding models in a compute-optimal fashion, given a suite \u2026"}, {"title": "BLSP-Emo: Towards Empathetic Large Speech-Language Models", "link": "https://arxiv.org/pdf/2406.03872", "details": "C Wang, M Liao, Z Huang, J Wu, C Zong, J Zhang - arXiv preprint arXiv:2406.03872, 2024", "abstract": "The recent release of GPT-4o showcased the potential of end-to-end multimodal models, not just in terms of low latency but also in their ability to understand and generate expressive speech with rich emotions. While the details are unknown to the \u2026"}, {"title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling", "link": "https://arxiv.org/pdf/2406.07522", "details": "L Ren, Y Liu, Y Lu, Y Shen, C Liang, W Chen - arXiv preprint arXiv:2406.07522, 2024", "abstract": "Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba \u2026"}, {"title": "UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback", "link": "https://arxiv.org/pdf/2406.07739", "details": "J Wu, E Schoop, A Leung, T Barik, JP Bigham\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) struggle to consistently generate UI code that compiles and produces visually relevant designs. Existing approaches to improve generation rely on expensive human feedback or distilling a proprietary model. In \u2026"}, {"title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models", "link": "https://openreview.net/pdf%3Fid%3D1tRLxQzdep", "details": "P Dong, L Li, Z Tang, X Liu, X Pan, Q Wang, X Chu - Forty-first International \u2026, 2024", "abstract": "Despite the remarkable capabilities, Large Language Models (LLMs) face deployment challenges due to their extensive size. Pruning methods drop a subset of weights to accelerate, but many of them require retraining, which is prohibitively \u2026"}, {"title": "AICoderEval: Improving AI Domain Code Generation of Large Language Models", "link": "https://arxiv.org/pdf/2406.04712", "details": "Y Xia, Y Chen, T Shi, J Wang, J Yang - arXiv preprint arXiv:2406.04712, 2024", "abstract": "Automated code generation is a pivotal capability of large language models (LLMs). However, assessing this capability in real-world scenarios remains challenging. Previous methods focus more on low-level code generation, such as model loading \u2026"}, {"title": "A3VLM: Actionable Articulation-Aware Vision Language Model", "link": "https://arxiv.org/pdf/2406.07549", "details": "S Huang, H Chang, Y Liu, Y Zhu, H Dong, P Gao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision Language Models (VLMs) have received significant attention in recent years in the robotics community. VLMs are shown to be able to perform complex visual reasoning and scene understanding tasks, which makes them regarded as a \u2026"}, {"title": "Chart Question Answering based on Modality Conversion and Large Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3643479.3662057", "details": "YC Liu, WT Chu - Proceedings of the 1st ACM Workshop on AI-Powered \u2026, 2024", "abstract": "A two-stage chart question answering system is proposed in this paper. Chart/plot images are first converted into structured text-based data by a transformer-based conversion model. Based on the structured text data, a large language model (LLM) \u2026"}]
