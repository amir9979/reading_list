[{"title": "Probing Fundamental Visual Comprehend Capabilities on Vision Language Models via Visual Phrases from Structural Data", "link": "https://link.springer.com/article/10.1007/s12559-024-10351-8", "details": "P Xie, B Liu - Cognitive Computation, 2024", "abstract": "Does the model demonstrate exceptional proficiency in \u201citem counting,\u201d\u201ccolor recognition,\u201d or other Fundamental Visual Comprehension Capability (FVCC)? There have been remarkable advancements in the field of multimodal, the pretrained \u2026"}, {"title": "Attention Prompting on Image for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.17143", "details": "R Yu, W Yu, X Wang - arXiv preprint arXiv:2409.17143, 2024", "abstract": "Compared with Large Language Models (LLMs), Large Vision-Language Models (LVLMs) can also accept images as input, thus showcasing more interesting emergent capabilities and demonstrating impressive performance on various vision \u2026"}, {"title": "A Unified Hallucination Mitigation Framework for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.16494", "details": "Y Chang, L Jing, X Zhang, Y Zhang - arXiv preprint arXiv:2409.16494, 2024", "abstract": "Hallucination is a common problem for Large Vision-Language Models (LVLMs) with long generations which is difficult to eradicate. The generation with hallucinations is partially inconsistent with the image content. To mitigate hallucination, current \u2026"}, {"title": "CAST: Cross-modal Alignment Similarity Test for Vision Language Models", "link": "https://arxiv.org/pdf/2409.11007", "details": "G Dagan, O Loginova, A Batra - arXiv preprint arXiv:2409.11007, 2024", "abstract": "Vision Language Models (VLMs) are typically evaluated with Visual Question Answering (VQA) tasks which assess a model's understanding of scenes. Good VQA performance is taken as evidence that the model will perform well on a broader \u2026"}, {"title": "Exploiting Pre-trained Language Models for Black-box Attack against Knowledge Graph Embeddings", "link": "https://dl.acm.org/doi/pdf/10.1145/3688850", "details": "G Yang, L Zhang, Y Liu, H Xie, Z Mao - ACM Transactions on Knowledge Discovery \u2026, 2024", "abstract": "Despite the emerging research on adversarial attacks against Knowledge Graph Embedding (KGE) models, most of them focus on white-box attack settings. However, white-box attacks are difficult to apply in practice compared to black-box attacks \u2026"}, {"title": "Self-simulation and Meta-Model Aggregation Based Heterogeneous Graph Coupled Federated Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10693348/", "details": "C Yan, X Lu, P Lio, P Hui, D He - IEEE Internet of Things Journal, 2024", "abstract": "A heterogeneous information network (heterogeneous graph) federated learning plays a crucial role in enabling multi-party collaboration in the IoT system. However, due to differences in business and data, the local models of each participant are \u2026"}, {"title": "Which is better? Taxonomy induction with learning the optimal structure via contrastive learning", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124010396", "details": "Y Meng, S Zhai, Z Chai, Y Zhang, T Wu, G Qi, W Song - Knowledge-Based Systems, 2024", "abstract": "A taxonomy represents a hierarchically structured knowledge graph that forms the infrastructure for various downstream applications, including recommender systems, web search, and question answering. The exploration of automated induction from \u2026"}, {"title": "A Survey of the Self Supervised Learning Mechanisms for Vision Transformers", "link": "https://arxiv.org/pdf/2408.17059", "details": "A Khan, A Sohail, M Fiaz, M Hassan, TH Afridi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Deep supervised learning models require high volume of labeled data to attain sufficiently good results. Although, the practice of gathering and annotating such big data is costly and laborious. Recently, the application of self supervised learning \u2026"}, {"title": "Exploring the Potential of Neural Machine Translation for Cross-Language Clinical Natural Language Processing (NLP) Resource Generation through Annotation \u2026", "link": "https://www.mdpi.com/2078-2489/15/10/585", "details": "J Rodr\u00edguez-Miret, E Farr\u00e9-Maduell, S Lima-L\u00f3pez\u2026 - Information, 2024", "abstract": "Recent advancements in neural machine translation (NMT) offer promising potential for generating cross-language clinical natural language processing (NLP) resources. There is a pressing need to be able to foster the development of clinical NLP tools \u2026"}]
