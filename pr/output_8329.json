[{"title": "Vision Language Model is NOT All You Need: Augmentation Strategies for Molecule Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3627673.3679607", "details": "N Lee, S Laghuvarapu, C Park, J Sun - Proceedings of the 33rd ACM International \u2026, 2024", "abstract": "Recently, there has been a growing interest among researchers in understanding molecules and their textual descriptions through molecule language models (MoLM). However, despite some early promising developments, the advancement of MoLM \u2026"}, {"title": "Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities", "link": "https://arxiv.org/pdf/2410.17385", "details": "Z Zhang, F Hu, J Lee, F Shi, P Kordjamshidi, J Chai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Spatial expressions in situated communication can be ambiguous, as their meanings vary depending on the frames of reference (FoR) adopted by speakers and listeners. While spatial language understanding and reasoning by vision-language models \u2026"}, {"title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks", "link": "https://arxiv.org/pdf/2410.05160%3F", "details": "Z Jiang, R Meng, X Yang, S Yavuz, Y Zhou, W Chen - arXiv preprint arXiv:2410.05160, 2024", "abstract": "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize \u2026"}, {"title": "Comparing Commercial and Open-Source Large Language Models for Labeling Chest Radiograph Reports", "link": "https://pubs.rsna.org/doi/abs/10.1148/radiol.241139", "details": "FJ Dorfner, L J\u00fcrgensen, L Donle, F Al Mohamad\u2026 - Radiology, 2024", "abstract": "Background Rapid advances in large language models (LLMs) have led to the development of numerous commercial and open-source models. While recent publications have explored OpenAI's GPT-4 to extract information of interest from \u2026"}, {"title": "Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop Question Answering", "link": "https://dl.acm.org/doi/pdf/10.1145/3627673.3679722", "details": "Y Shi, Q Tan, X Wu, S Zhong, K Zhou, N Liu - Proceedings of the 33rd ACM \u2026, 2024", "abstract": "Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging \u2026"}, {"title": "Improve Vision Language Model Chain-of-thought Reasoning", "link": "https://arxiv.org/pdf/2410.16198%3F", "details": "R Zhang, B Zhang, Y Li, H Zhang, Z Sun, Z Gan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with \u2026"}, {"title": "Self-Supervised Contrastive Learning for Consistent Few-Shot Image Representations", "link": "https://link.springer.com/chapter/10.1007/978-3-031-74561-4_15", "details": "S Karimijafarbigloo, R Azad, D Merhof - International Workshop on PRedictive \u2026, 2024", "abstract": "The central challenge in few-shot learning involves (1) acquiring object proposals through the support representation,(2) ensuring consistent representations for images in both support and query sets, and (3) achieving effective metric learning for \u2026"}]
