[{"title": "Prompt Learning with Extended Kalman Filter for Pre-trained Language Models", "link": "https://www.ijcai.org/proceedings/2024/0492.pdf", "details": "Q Li, X Xie, C Wang, SK Zhou", "abstract": "Prompt learning has gained popularity as a means to leverage the knowledge embedded in pre-trained language models (PLMs) for NLP tasks while using a limited number of trainable parameters. While it has shown promise in tasks like \u2026"}, {"title": "Selective Prefix Tuning for Pre-trained Language Models", "link": "https://aclanthology.org/2024.findings-acl.164.pdf", "details": "H Zhang, Z Li, P Wang, H Zhao - Findings of the Association for Computational \u2026, 2024", "abstract": "The prevalent approach for optimizing pre-trained language models in downstream tasks is fine-tuning. However, it is both time-consuming and memory-inefficient. In response, a more efficient method called Prefix Tuning, which insert learnable \u2026"}, {"title": "Extracting lung cancer staging descriptors from pathology reports: A generative language model approach", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001382", "details": "H Cho, S Yoo, B Kim, S Jang, L Sunwoo, S Kim, D Lee\u2026 - Journal of Biomedical \u2026, 2024", "abstract": "Background In oncology, electronic health records contain textual key information for the diagnosis, staging, and treatment planning of patients with cancer. However, text data processing requires a lot of time and effort, which limits the utilization of these \u2026"}, {"title": "EPFL-MAKE at \u201cDischarge Me!\u201d: An LLM System for Automatically Generating Discharge Summaries of Clinical Electronic Health Record", "link": "https://aclanthology.org/2024.bionlp-1.61.pdf", "details": "H Wu, P Boulenger, A Faure, B C\u00e9spedes, F Boukil\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "This paper presents our contribution to the Streamlining Discharge Documentation shared task organized as part of the ACL'24 workshop. We propose MEDISCHARGE (Meditron-7B Based Medical Summary Generation System for Discharge Me), an \u2026"}, {"title": "Protocol for Designing a Model to Predict the Likelihood of Psychosis From Electronic Health Records Using Natural Language Processing and Machine Learning", "link": "https://www.thepermanentejournal.org/doi/abs/10.7812/TPP/23.139", "details": "I Stavers-Sosa, DJ Cronkite, LD Gerstley, A Kelley\u2026 - The Permanente Journal, 2024", "abstract": "Introduction Rapid identification of individuals developing a psychotic spectrum disorder (PSD) is crucial because untreated psychosis is associated with poor outcomes and decreased treatment response. Lack of recognition of early psychotic \u2026"}, {"title": "XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models", "link": "https://aclanthology.org/2024.bionlp-1.35.pdf", "details": "OC Thawakar, AM Shaker, SS Mullappilly, H Cholakkal\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "The latest breakthroughs in large language models (LLMs) and vision-language models (VLMs) have showcased promising capabilities toward performing a wide range of tasks. Such models are typically trained on massive datasets comprising \u2026"}, {"title": "Editable Fairness: Fine-Grained Bias Mitigation in Language Models", "link": "https://arxiv.org/pdf/2408.11843", "details": "R Chen, Y Li, J Yang, JT Zhou, Z Liu - arXiv preprint arXiv:2408.11843, 2024", "abstract": "Generating fair and accurate predictions plays a pivotal role in deploying large language models (LLMs) in the real world. However, existing debiasing methods inevitably generate unfair or incorrect predictions as they are designed and \u2026"}, {"title": "Teaching Small Language Models to Reason for Knowledge-Intensive Multi-Hop Question Answering", "link": "https://aclanthology.org/2024.findings-acl.464.pdf", "details": "X Li, S He, F Lei, JY JunYang, T Su, K Liu, J Zhao - Findings of the Association for \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) can teach small language models (SLMs) to solve complex reasoning tasks (eg, mathematical question answering) by Chain-of- thought Distillation (CoTD). Specifically, CoTD fine-tunes SLMs by utilizing rationales \u2026"}, {"title": "A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models", "link": "https://aclanthology.org/2024.gebnlp-1.1.pdf", "details": "Y Wang, V Demberg - Proceedings of the 5th Workshop on Gender Bias in \u2026, 2024", "abstract": "Pre-trained language models have shown impressive abilities of understanding and generating natural languages. However, they typically inherit undesired human-like bias and stereotypes from training data, which raises concerns about putting these \u2026"}]
