'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Autonomous Data Selection with Language Models for Mat'
[{"title": "Refining Pre-trained Language Models for Domain Adaptation with Entity-Aware Discriminative and Contrastive Learning", "link": "https://epubs.siam.org/doi/pdf/10.1137/1.9781611978032.48", "details": "J Yang, X Hu, Y Shen, G xiao - Proceedings of the 2024 SIAM International \u2026, 2024", "abstract": "With the rapid advancement of pre-trained language models (PLMs), the adaptation of these models to specialized domains has emerged as an essential area of research. However, PLMs encounter substantial challenges when deployed in highly \u2026"}, {"title": "Revisiting the Adversarial Robustness of Vision Language Models: a Multimodal Perspective", "link": "https://arxiv.org/pdf/2404.19287", "details": "W Zhou, S Bai, Q Zhao, B Chen - arXiv preprint arXiv:2404.19287, 2024", "abstract": "Pretrained vision-language models (VLMs) like CLIP have shown impressive generalization performance across various downstream tasks, yet they remain vulnerable to adversarial attacks. While prior research has primarily concentrated on \u2026"}, {"title": "Enhancing Data Quality in Federated Fine-Tuning of Large Language Models", "link": "https://openreview.net/pdf%3Fid%3DaHD3WJ7gQ5", "details": "W Zhao, Y Du, ND Lane, S Chen, Y Wang - ICLR 2024 Workshop on Navigating and \u2026", "abstract": "In the current landscape of large language model training, there is a significant reliance on public domain data, which is nearing exhaustion according to recent research. To further scale up, it is crucial to incorporate collaboration among multiple \u2026"}, {"title": "Optimization of Prompt Learning via Multi-Knowledge Representation for Vision-Language Models", "link": "https://arxiv.org/pdf/2404.10357", "details": "E Zhang, Y Chen, Q Miao, M Tang, J Wang - arXiv preprint arXiv:2404.10357, 2024", "abstract": "Vision-Language Models (VLMs), such as CLIP, play a foundational role in various cross-modal applications. To fully leverage VLMs' potential in adapting to downstream tasks, context optimization methods like Prompt Tuning are essential \u2026"}, {"title": "ATG: Benchmarking Automated Theorem Generation for Generative Language Models", "link": "https://eleanor-h.github.io/publication/confnaacl-2024-atg/confnaacl-2024-atg.pdf", "details": "X Lin, Q Cao, Y Huang, Z Yang, Z Liu, Z Li, X Liang15", "abstract": "Humans can develop new theorems to explore broader and more complex mathematical results. While current generative language models (LMs) have achieved significant improvement in automatically proving theorems, their ability to \u2026"}, {"title": "Prompting Large Language Models for Zero-shot Essay Scoring via Multi-trait Specialization", "link": "https://arxiv.org/pdf/2404.04941", "details": "S Lee, Y Cai, D Meng, Z Wang, Y Wu - arXiv preprint arXiv:2404.04941, 2024", "abstract": "Advances in automated essay scoring (AES) have traditionally relied on labeled essays, requiring tremendous cost and expertise for their acquisition. Recently, large language models (LLMs) have achieved great success in various tasks, but their \u2026"}, {"title": "MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems", "link": "https://arxiv.org/pdf/2404.09486", "details": "K Li, Y Tian, Q Hu, Z Luo, J Ma - arXiv preprint arXiv:2404.09486, 2024", "abstract": "Programming often involves converting detailed and complex specifications into code, a process during which developers typically utilize visual aids to more effectively convey concepts. While recent developments in Large Multimodal Models \u2026"}, {"title": "Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case Study", "link": "https://arxiv.org/pdf/2404.08259", "details": "WH Her, U Kruschwitz - arXiv preprint arXiv:2404.08259, 2024", "abstract": "Machine Translation has made impressive progress in recent years offering close to human-level performance on many languages, but studies have primarily focused on high-resource languages with broad online presence and resources. With the help of \u2026"}, {"title": "Pre-training Concept Frequency is predictive of CLIP Zero-shot Performance", "link": "https://openreview.net/pdf%3Fid%3D55iCzZ1TtD", "details": "V Udandarao, A Prabhu, P Torr, A Bibi, S Albanie\u2026 - ICLR 2024 Workshop on \u2026", "abstract": "Web-crawled pre-training datasets are speculated to be key drivers of zero-shot generalization abilities of Vision-Language Models (VLMs) like CLIP, across a range of downstream classification and retrieval tasks, spanning diverse visual concepts \u2026"}]
