[{"title": "FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2505.20225", "details": "H Kang, Z Yu, C Xiong - arXiv preprint arXiv:2505.20225, 2025", "abstract": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong efficiency-performance trade-offs by activating only a fraction of the model per token \u2026", "entry_id": "http://arxiv.org/abs/2505.20225v1", "updated": "2025-05-26 17:06:25", "published": "2025-05-26 17:06:25", "authors": "Hao Kang;Zichun Yu;Chenyan Xiong", "summary": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4\nincreasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong\nefficiency-performance trade-offs by activating only a fraction of the model\nper token. Yet academic researchers still lack a fully open, end-to-end MoE\nplatform for investigating scaling, routing, and expert behavior. We release\nFLAME-MoE, a completely open-source research suite composed of seven\ndecoder-only models, ranging from 38M to 1.7B active parameters, whose\narchitecture--64 experts with top-8 gating and 2 shared experts--closely\nreflects modern production LLMs. All training data pipelines, scripts, logs,\nand checkpoints are publicly available to enable reproducible experimentation.\nAcross six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4\npoints over dense baselines trained with identical FLOPs. Leveraging full\ntraining trace transparency, we present initial analyses showing that (i)\nexperts increasingly specialize on distinct token subsets, (ii) co-activation\nmatrices remain sparse, reflecting diverse expert usage, and (iii) routing\nbehavior stabilizes early in training. All code, training logs, and model\ncheckpoints are available at https://github.com/cmu-flame/FLAME-MoE.", "comment": "All code, training logs, and model checkpoints are available at\n  https://github.com/cmu-flame/FLAME-MoE", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.20225v1;http://arxiv.org/pdf/2505.20225v1", "pdf_url": "http://arxiv.org/pdf/2505.20225v1"}, {"title": "Winter Soldier: Backdooring Language Models at Pre-Training with Indirect Data Poisoning", "link": "https://arxiv.org/pdf/2506.14913", "details": "W Bouaziz, M Videau, N Usunier, EM El-Mhamdi - arXiv preprint arXiv:2506.14913, 2025", "abstract": "The pre-training of large language models (LLMs) relies on massive text datasets sourced from diverse and difficult-to-curate origins. Although membership inference attacks and hidden canaries have been explored to trace data usage, such methods \u2026", "entry_id": "http://arxiv.org/abs/2506.14913v1", "updated": "2025-06-17 18:46:45", "published": "2025-06-17 18:46:45", "authors": "Wassim Bouaziz;Mathurin Videau;Nicolas Usunier;El-Mahdi El-Mhamdi", "summary": "The pre-training of large language models (LLMs) relies on massive text\ndatasets sourced from diverse and difficult-to-curate origins. Although\nmembership inference attacks and hidden canaries have been explored to trace\ndata usage, such methods rely on memorization of training data, which LM\nproviders try to limit. In this work, we demonstrate that indirect data\npoisoning (where the targeted behavior is absent from training data) is not\nonly feasible but also allow to effectively protect a dataset and trace its\nuse. Using gradient-based optimization prompt-tuning, we make a model learn\narbitrary secret sequences: secret responses to secret prompts that are absent\nfrom the training corpus. We validate our approach on language models\npre-trained from scratch and show that less than 0.005% of poisoned tokens are\nsufficient to covertly make a LM learn a secret and detect it with extremely\nhigh confidence ($p < 10^{-55}$) with a theoretically certifiable scheme.\nCrucially, this occurs without performance degradation (on LM benchmarks) and\ndespite secrets never appearing in the training set.", "comment": "18 pages, 12 figures", "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR;cs.LG;stat.ML", "links": "http://arxiv.org/abs/2506.14913v1;http://arxiv.org/pdf/2506.14913v1", "pdf_url": "http://arxiv.org/pdf/2506.14913v1"}, {"title": "Measuring Sycophancy of Language Models in Multi-turn Dialogues", "link": "https://arxiv.org/pdf/2505.23840", "details": "J Hong, G Byun, S Kim, K Shu - arXiv preprint arXiv:2505.23840, 2025", "abstract": "Large Language Models (LLMs) are expected to provide helpful and harmless responses, yet they often exhibit sycophancy--conforming to user beliefs regardless of factual accuracy or ethical soundness. Prior research on sycophancy has primarily \u2026", "entry_id": "http://arxiv.org/abs/2505.23840v1", "updated": "2025-05-28 14:05:46", "published": "2025-05-28 14:05:46", "authors": "Jiseung Hong;Grace Byun;Seungone Kim;Kai Shu", "summary": "Large Language Models (LLMs) are expected to provide helpful and harmless\nresponses, yet they often exhibit sycophancy--conforming to user beliefs\nregardless of factual accuracy or ethical soundness. Prior research on\nsycophancy has primarily focused on single-turn factual correctness,\noverlooking the dynamics of real-world interactions. In this work, we introduce\nSYCON Bench, a novel benchmark for evaluating sycophantic behavior in\nmulti-turn, free-form conversational settings. Our benchmark measures how\nquickly a model conforms to the user (Turn of Flip) and how frequently it\nshifts its stance under sustained user pressure (Number of Flip). Applying\nSYCON Bench to 17 LLMs across three real-world scenarios, we find that\nsycophancy remains a prevalent failure mode. Our analysis shows that alignment\ntuning amplifies sycophantic behavior, whereas model scaling and reasoning\noptimization strengthen the model's ability to resist undesirable user views.\nReasoning models generally outperform instruction-tuned models but often fail\nwhen they over-index on logical exposition instead of directly addressing the\nuser's underlying beliefs. Finally, we evaluate four additional prompting\nstrategies and demonstrate that adopting a third-person perspective reduces\nsycophancy by up to 63.8% in debate scenario. We release our code and data at\nhttps://github.com/JiseungHong/SYCON-Bench.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.23840v1;http://arxiv.org/pdf/2505.23840v1", "pdf_url": "http://arxiv.org/pdf/2505.23840v1"}, {"title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2506.14824", "details": "Y Zhang, H Gao, H Chen, W Li, Y Ma, V Tresp - arXiv preprint arXiv:2506.14824, 2025", "abstract": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal reasoning and cross-modal retrieval but face deployment challenges in real-world scenarios due to distributed multimodal data and strict privacy requirements \u2026", "entry_id": "http://arxiv.org/abs/2506.14824v1", "updated": "2025-06-12 17:50:50", "published": "2025-06-12 17:50:50", "authors": "Yao Zhang;Hewei Gao;Haokun Chen;Weiguo Li;Yunpu Ma;Volker Tresp", "summary": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal\nreasoning and cross-modal retrieval but face deployment challenges in\nreal-world scenarios due to distributed multimodal data and strict privacy\nrequirements. Federated Learning (FL) offers a solution by enabling\ncollaborative model training without centralizing data. However, realizing FL\nfor MLLMs presents significant challenges, including high computational\ndemands, limited client capacity, substantial communication costs, and\nheterogeneous client data. Existing FL methods assume client-side deployment of\nfull models, an assumption that breaks down for large-scale MLLMs due to their\nmassive size and communication demands. To address these limitations, we\npropose FedNano, the first FL framework that centralizes the LLM on the server\nwhile introducing NanoEdge, a lightweight module for client-specific\nadaptation. NanoEdge employs modality-specific encoders, connectors, and\ntrainable NanoAdapters with low-rank adaptation. This design eliminates the\nneed to deploy LLM on clients, reducing client-side storage by 95%, and\nlimiting communication overhead to only 0.01% of the model parameters. By\ntransmitting only compact NanoAdapter updates, FedNano handles heterogeneous\nclient data and resource constraints while preserving privacy. Experiments\ndemonstrate that FedNano outperforms prior FL baselines, bridging the gap\nbetween MLLM scale and FL feasibility, and enabling scalable, decentralized\nmultimodal AI systems.", "comment": "12 pages, 3 figures", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.MM", "links": "http://arxiv.org/abs/2506.14824v1;http://arxiv.org/pdf/2506.14824v1", "pdf_url": "http://arxiv.org/pdf/2506.14824v1"}, {"title": "Evaluating the Intelligence of large language models: A comparative study using verbal and visual IQ tests", "link": "https://www.sciencedirect.com/science/article/pii/S2949882125000544", "details": "S Abdelkarim, D Lu, DL Flores, S Jaeggi, P Baldi - Computers in Human Behavior \u2026, 2025", "abstract": "Large language models (LLMs) excel on many specialised benchmarks, yet their general-reasoning ability remains opaque. We therefore test 18 models\u2014including GPT-4, Claude 3 and Gemini Pro\u2014on a 14-section IQ suite spanning verbal \u2026"}]
