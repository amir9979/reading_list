[{"title": "CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks", "link": "https://arxiv.org/pdf/2409.03381", "details": "Y Deng, X Qiu, X Tan, C Qu, J Pan, Y Cheng, Y Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Cognitive psychology investigates perception, attention, memory, language, problem- solving, decision-making, and reasoning. Kahneman's dual-system theory elucidates the human decision-making process, distinguishing between the rapid, intuitive \u2026"}, {"title": "Selective Self-Rehearsal: A Fine-Tuning Approach to Improve Generalization in Large Language Models", "link": "https://arxiv.org/pdf/2409.04787", "details": "S Gupta, Y Nandwani, A Yehudai, M Mishra, G Pandey\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task \u2026"}, {"title": "Language Models as Reasoners for Out-of-Distribution Detection", "link": "https://link.springer.com/chapter/10.1007/978-3-031-68738-9_30", "details": "K Kirchheim, F Ortmeier - \u2026 Conference on Computer Safety, Reliability, and \u2026, 2024", "abstract": "Deep neural networks (DNNs) are prone to making wrong predictions with high confidence for data that does not stem from their training distribution. Consequentially, out-of-distribution (OOD) detection is important in safety-critical \u2026"}, {"title": "Revolutionizing Database Q&A with Large Language Models: Comprehensive Benchmark and Evaluation", "link": "https://arxiv.org/pdf/2409.04475", "details": "Y Zheng, B Li, Z Lin, Y Luo, X Zhou, C Lin, J Su, G Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The development of Large Language Models (LLMs) has revolutionized Q&A across various industries, including the database domain. However, there is still a lack of a comprehensive benchmark to evaluate the capabilities of different LLMs and their \u2026"}, {"title": "Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications", "link": "https://arxiv.org/pdf/2409.05314", "details": "A Maatouk, KC Ampudia, R Ying, L Tassiulas - arXiv preprint arXiv:2409.05314, 2024", "abstract": "The emergence of large language models (LLMs) has significantly impacted various fields, from natural language processing to sectors like medicine and finance. However, despite their rapid proliferation, the applications of LLMs in \u2026"}, {"title": "PiTe: Pixel-Temporal Alignment for Large Video-Language Model", "link": "https://arxiv.org/pdf/2409.07239", "details": "Y Liu, P Ding, S Huang, M Zhang, H Zhao, D Wang - arXiv preprint arXiv:2409.07239, 2024", "abstract": "Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models (LVLMs) have emerged as a pivotal advancement, bridging the gap between image and text. However, video making it challenging for LVLMs to perform \u2026"}, {"title": "Targeted training for numerical reasoning with large language models", "link": "https://link.springer.com/article/10.1007/s10115-024-02216-1", "details": "X Li, S Liu, Y Zhu, G Cheng - Knowledge and Information Systems, 2024", "abstract": "After recent gains achieved by large language models (LLMs) on numerical reasoning tasks, it has become of interest to have LLMs teach small models to improve on numerical reasoning. Instructing LLMs to generate Chains of Thought to \u2026"}, {"title": "ELMS: Elasticized Large Language Models On Mobile Devices", "link": "https://arxiv.org/pdf/2409.09071", "details": "W Yin, R Yi, D Xu, G Huang, M Xu, X Liu - arXiv preprint arXiv:2409.09071, 2024", "abstract": "On-device Large Language Models (LLMs) are revolutionizing mobile AI, enabling applications such as UI automation while addressing privacy concerns. Currently, the standard approach involves deploying a single, robust LLM as a universal solution \u2026"}, {"title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges", "link": "https://arxiv.org/pdf/2409.09927", "details": "V Samuel, Y Zhou, HP Zou - arXiv preprint arXiv:2409.09927, 2024", "abstract": "As large language models achieve increasingly impressive results, questions arise about whether such performance is from generalizability or mere data memorization. Thus, numerous data contamination detection methods have been proposed \u2026"}]
