[{"title": "Personalizing Low-Rank Bayesian Neural Networks Via Federated Learning", "link": "https://arxiv.org/pdf/2410.14390", "details": "B Zhang, D Liu, O Simeone, G Wang, D Pezaros, G Zhu - arXiv preprint arXiv \u2026, 2024", "abstract": "To support real-world decision-making, it is crucial for models to be well-calibrated, ie, to assign reliable confidence estimates to their predictions. Uncertainty quantification is particularly important in personalized federated learning (PFL), as \u2026"}, {"title": "Zero-shot extraction of seizure outcomes from clinical notes using generative pretrained transformers", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/11/04/2024.11.01.24316573.full.pdf", "details": "WKS Ojemann, K Xie, K Liu, E Chang, D Roth, B Litt\u2026 - medRxiv, 2024", "abstract": "Purpose Pre\u2013trained encoder transformer models have extracted information from unstructured clinic note text but require manual annotation for supervised fine\u2013 tuning. Large, Generative Pre\u2013trained Transformers (GPTs) may streamline this \u2026"}, {"title": "Self-Supervised Learning of Disentangled Representations for Multivariate Time-Series", "link": "https://arxiv.org/pdf/2410.12606", "details": "C Chang, CT Chan, WY Wang, WC Peng, TF Chen - arXiv preprint arXiv:2410.12606, 2024", "abstract": "Multivariate time-series data in fields like healthcare and industry are informative but challenging due to high dimensionality and lack of labels. Recent self-supervised learning methods excel in learning rich representations without labels but struggle \u2026"}, {"title": "DyGraphformer: Transformer combining dynamic spatio-temporal graph network for multivariate time series forecasting", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007007", "details": "S Han, Y Xun, J Cai, H Yang, Y Li - Neural Networks, 2024", "abstract": "Transformer-based models demonstrate tremendous potential for Multivariate Time Series (MTS) forecasting due to their ability to capture long-term temporal dependencies by using the self-attention mechanism. However, effectively modeling \u2026"}, {"title": "Dynamic deep graph convolution with enhanced transformer networks for time series anomaly detection in IoT", "link": "https://link.springer.com/article/10.1007/s10586-024-04707-w", "details": "R Gao, Z Chen, X Wu, Y Yu, L Zhang - Cluster Computing, 2025", "abstract": "Anomaly detection of multi-time series data during the working process of Internet of Things systems that utilize sensors is one of the key aspects to prevent accidents in industrial information systems. The key challenge is to discover generalized normal \u2026"}, {"title": "Efficient Symmetry-Aware Materials Generation via Hierarchical Generative Flow Networks", "link": "https://arxiv.org/pdf/2411.04323", "details": "TM Nguyen, SA Tawfik, T Tran, S Gupta, S Rana\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Discovering new solid-state materials requires rapidly exploring the vast space of crystal structures and locating stable regions. Generating stable materials with desired properties and compositions is extremely difficult as we search for very small \u2026"}, {"title": "Improving Out-of-Distribution Detection with Disentangled Foreground and Background Features", "link": "https://dl.acm.org/doi/pdf/10.1145/3664647.3681614", "details": "C Ding, G Pang - Proceedings of the 32nd ACM International \u2026, 2024", "abstract": "Detecting out-of-distribution (OOD) inputs is a principal task for ensuring the safety of deploying deep-neural-network classifiers in open-set scenarios. OOD samples can be drawn from arbitrary distributions and exhibit deviations from in-distribution (ID) \u2026"}, {"title": "Dynamic Contrastive Learning for Time Series Representation", "link": "https://arxiv.org/pdf/2410.15416", "details": "AK Shamba, K Bach, G Taylor - arXiv preprint arXiv:2410.15416, 2024", "abstract": "Understanding events in time series is an important task in a variety of contexts. However, human analysis and labeling are expensive and time-consuming. Therefore, it is advantageous to learn embeddings for moments in time series in an \u2026"}, {"title": "FedGKD: personalized federated learning through grouping and distillation", "link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13404/134040E/FedGKD-personalized-federated-learning-through-grouping-and-distillation/10.1117/12.3050605.short", "details": "T Li, S Lin, P Zhao, Z Li, J Wang - Fifth International Conference on Control, Robotics \u2026, 2024", "abstract": "As is well known, the objective of traditional Federated Learning (FL) is to train a global model collaboratively across multiple clients without directly accessing client data. However, traditional federated learning is frequently impeded by the \u2026"}]
