[{"title": "A Particle Swarm Optimization\u2010Based Approach Coupled With Large Language Models for Prompt Optimization", "link": "https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.70049", "details": "PC Hsieh, WP Lee - Expert Systems, 2025", "abstract": "Large language models (LLMs) have been developing rapidly to attract significant attention these days. These models have exhibited remarkable abilities in achieving various natural language processing (NLP) tasks, but the performance depends \u2026"}, {"title": "Parameter-efficient fine-tuning in large language models: a survey of methodologies", "link": "https://link.springer.com/article/10.1007/s10462-025-11236-4", "details": "L Wang, S Chen, L Jiang, S Pan, R Cai, S Yang\u2026 - Artificial Intelligence Review, 2025", "abstract": "The large language models, as predicted by scaling law forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the \u2026"}, {"title": "Unveiling Hidden Collaboration within Mixture-of-Experts in Large Language Models", "link": "https://arxiv.org/pdf/2504.12359", "details": "Y Tang, Y Tang, N Zhang, M Chen, Y Li - arXiv preprint arXiv:2504.12359, 2025", "abstract": "Mixture-of-Experts based large language models (MoE LLMs) have shown significant promise in multitask adaptability by dynamically routing inputs to specialized experts. Despite their success, the collaborative mechanisms among \u2026"}]
