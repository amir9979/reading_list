[{"title": "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL", "link": "https://arxiv.org/pdf/2410.11371", "details": "Q Zhong, K Chen, L Ding, J Liu, B Du, D Tao - arXiv preprint arXiv:2410.11371, 2024", "abstract": "Large Language Models (LLMs) have shown promising performance in text-to-SQL, which involves translating natural language questions into SQL queries. However, current text-to-SQL LLMs are computationally expensive and challenging to deploy \u2026"}, {"title": "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering", "link": "https://aclanthology.org/2024.findings-emnlp.95.pdf", "details": "Y Wang, X Ma, W Chen - Findings of the Association for Computational \u2026, 2024", "abstract": "Large-scale language models (LLMs) like ChatGPT have demonstrated impressive abilities in generating responses based on human instructions. However, their use in the medical field can be challenging due to their lack of specific, in-depth knowledge \u2026"}, {"title": "Global Learning with Triplet Relations in Abstractive Summarization", "link": "https://aclanthology.org/2024.conll-1.15.pdf", "details": "F Lu, J Duan, J Liu - Proceedings of the 28th Conference on Computational \u2026, 2024", "abstract": "Abstractive summarization models learned with token-level maximum likelihood estimation suffer from exposure bias, that the condition for predicting the next token is discrepant during training and inference. Existing solutions bridge this gap by \u2026"}, {"title": "Self-Training Large Language and Vision Assistant for Medical Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.1119.pdf", "details": "G Sun, C Qin, H Fu, L Wang, Z Tao - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "Abstract Large Vision-Language Models (LVLMs) have shown significant potential in assisting medical diagnosis by leveraging extensive biomedical datasets. However, the advancement of medical image understanding and reasoning critically depends \u2026"}, {"title": "Out-of-distribution detection in digital pathology: Do foundation models bring the end to reconstruction-based approaches?", "link": "https://www.sciencedirect.com/science/article/pii/S0010482524014124", "details": "M Pocevi\u010di\u016bt\u0117, Y Ding, R Brom\u00e9e, G Eilertsen - Computers in Biology and Medicine, 2024", "abstract": "Artificial intelligence (AI) has shown promising results for computational pathology tasks. However, one of the limitations in clinical practice is that these algorithms are optimised for the distribution represented by the training data. For out-of-distribution \u2026"}, {"title": "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "link": "https://aclanthology.org/2024.findings-emnlp.61.pdf", "details": "X Wei, H Chen, H Yu, H Fei, Q Liu - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have achieved notable success in commonsense reasoning tasks, benefiting from their extensive world knowledge acquired through extensive pretraining. While approaches like Chain-of-Thought \u2026"}]
