[{"title": "An Analysis Method for the Impact of GenAI Code Suggestions on Software Engineers' Thought Processes", "link": "https://ojs.aaai.org/index.php/AAAI-SS/article/download/31257/33417", "details": "T Yonekawa, H Yamano, I Sakata - Proceedings of the AAAI Symposium Series, 2024", "abstract": "Interactive generative AI can be used in software programming to generate sufficient quality of code. Software developers can utilize the output code of generative AI as well as website resources from search engine results. In this research, we present a \u2026"}, {"title": "NExT: Teaching Large Language Models to Reason about Code Execution", "link": "https://arxiv.org/pdf/2404.14662", "details": "A Ni, M Allamanis, A Cohan, Y Deng, K Shi, C Sutton\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "A fundamental skill among human developers is the ability to understand and reason about program execution. As an example, a programmer can mentally simulate code execution in natural language to debug and repair code (aka. rubber duck \u2026"}, {"title": "More RLHF, More Trust? On The Impact of Human Preference Alignment On Language Model Trustworthiness", "link": "https://arxiv.org/pdf/2404.18870", "details": "AJ Li, S Krishna, H Lakkaraju - arXiv preprint arXiv:2404.18870, 2024", "abstract": "The surge in Large Language Models (LLMs) development has led to improved performance on cognitive tasks as well as an urgent need to align these models with human values in order to safely exploit their power. Despite the effectiveness of \u2026"}, {"title": "Navigating the Modern Evaluation Landscape: Considerations in Benchmarks and Frameworks for Large Language Models (LLMs)", "link": "https://aclanthology.org/2024.lrec-tutorials.4.pdf", "details": "L Choshen, A Gera, Y Perlitz, M Shmueli-Scheuer\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract General-Purpose Language Models have changed the world of Natural Language Processing, if not the world itself. The evaluation of such versatile models, while supposedly similar to evaluation of generation models before them, in fact \u2026"}, {"title": "AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts", "link": "https://arxiv.org/pdf/2405.00361", "details": "Z Liu, J Luo - arXiv preprint arXiv:2405.00361, 2024", "abstract": "We introduce AdaMoLE, a novel method for fine-tuning large language models (LLMs) through an Adaptive Mixture of Low-Rank Adaptation (LoRA) Experts. Moving beyond conventional methods that employ a static top-k strategy for activating \u2026"}]
