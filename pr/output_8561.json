[{"title": "IP-MOT: Instance Prompt Learning for Cross-Domain Multi-Object Tracking", "link": "https://arxiv.org/pdf/2410.23907", "details": "R Luo, Z Song, L Chen, Y Li, M Yang, W Yang - arXiv preprint arXiv:2410.23907, 2024", "abstract": "Multi-Object Tracking (MOT) aims to associate multiple objects across video frames and is a challenging vision task due to inherent complexities in the tracking environment. Most existing approaches train and track within a single domain \u2026"}, {"title": "Personalizing Low-Rank Bayesian Neural Networks Via Federated Learning", "link": "https://arxiv.org/pdf/2410.14390", "details": "B Zhang, D Liu, O Simeone, G Wang, D Pezaros, G Zhu - arXiv preprint arXiv \u2026, 2024", "abstract": "To support real-world decision-making, it is crucial for models to be well-calibrated, ie, to assign reliable confidence estimates to their predictions. Uncertainty quantification is particularly important in personalized federated learning (PFL), as \u2026"}, {"title": "Improved Depth Estimation of Bayesian Neural Networks", "link": "https://arxiv.org/pdf/2410.10395", "details": "B van Erp, B de Vries - arXiv preprint arXiv:2410.10395, 2024", "abstract": "This paper proposes improvements over earlier work by Nazareth and Blei (2022) for estimating the depth of Bayesian neural networks. Here, we propose a discrete truncated normal distribution over the network depth to independently learn its mean \u2026"}, {"title": "Efficient Model Compression for Bayesian Neural Networks", "link": "https://arxiv.org/pdf/2411.00273", "details": "D Saha, Z Liu, F Liang - arXiv preprint arXiv:2411.00273, 2024", "abstract": "Model Compression has drawn much attention within the deep learning community recently. Compressing a dense neural network offers many advantages including lower computation cost, deployability to devices of limited storage and memories \u2026"}, {"title": "Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models", "link": "https://arxiv.org/pdf/2410.09385", "details": "SK Bhethanabhotla, O Swelam, J Siems, D Salinas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces Mamba4Cast, a zero-shot foundation model for time series forecasting. Based on the Mamba architecture and inspired by Prior-data Fitted Networks (PFNs), Mamba4Cast generalizes robustly across diverse time series tasks \u2026"}, {"title": "Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced Time Series Classification", "link": "https://arxiv.org/pdf/2410.18686", "details": "X Tao, T Pan, M Cheng, Y Luo - arXiv preprint arXiv:2410.18686, 2024", "abstract": "Leveraging large language models (LLMs) has garnered increasing attention and introduced novel perspectives in time series classification. However, existing approaches often overlook the crucial dynamic temporal information inherent in time \u2026"}, {"title": "Enhancing Diversity in Bayesian Deep Learning via Hyperspherical Energy Minimization of CKA", "link": "https://arxiv.org/pdf/2411.00259", "details": "D Smerkous, Q Bai, F Li - arXiv preprint arXiv:2411.00259, 2024", "abstract": "Particle-based Bayesian deep learning often requires a similarity metric to compare two networks. However, naive similarity metrics lack permutation invariance and are inappropriate for comparing networks. Centered Kernel Alignment (CKA) on feature \u2026"}, {"title": "Self-Supervised Learning of Disentangled Representations for Multivariate Time-Series", "link": "https://arxiv.org/pdf/2410.12606", "details": "C Chang, CT Chan, WY Wang, WC Peng, TF Chen - arXiv preprint arXiv:2410.12606, 2024", "abstract": "Multivariate time-series data in fields like healthcare and industry are informative but challenging due to high dimensionality and lack of labels. Recent self-supervised learning methods excel in learning rich representations without labels but struggle \u2026"}, {"title": "DyGraphformer: Transformer combining dynamic spatio-temporal graph network for multivariate time series forecasting", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007007", "details": "S Han, Y Xun, J Cai, H Yang, Y Li - Neural Networks, 2024", "abstract": "Transformer-based models demonstrate tremendous potential for Multivariate Time Series (MTS) forecasting due to their ability to capture long-term temporal dependencies by using the self-attention mechanism. However, effectively modeling \u2026"}]
