[{"title": "Measuring Retrieval Complexity in Question Answering Systems", "link": "https://arxiv.org/pdf/2406.03592", "details": "M Gabburo, NP Jedema, S Garg, LFR Ribeiro\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we investigate which questions are challenging for retrieval-based Question Answering (QA). We (i) propose retrieval complexity (RC), a novel metric conditioned on the completeness of retrieved documents, which measures the \u2026"}, {"title": "FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2405.18218", "details": "Y Zhang, Y Li, X Wang, Q Shen, B Plank, B Bischl\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Overparametrized transformer networks are the state-of-the-art architecture for Large Language Models (LLMs). However, such models contain billions of parameters making large compute a necessity, while raising environmental concerns. To \u2026"}, {"title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via Adaptive Heads Fusion", "link": "https://arxiv.org/pdf/2406.06567", "details": "Y Chen, L Zhang, J Shang, Z Zhang, T Liu, S Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) with billions of parameters demonstrate impressive performance. However, the widely used Multi-Head Attention (MHA) in LLMs incurs substantial computational and memory costs during inference. While some efforts \u2026"}, {"title": "Unlocking the Power of Spatial and Temporal Information in Medical Multimodal Pre-training", "link": "https://arxiv.org/pdf/2405.19654", "details": "J Yang, B Su, WX Zhao, JR Wen - arXiv preprint arXiv:2405.19654, 2024", "abstract": "Medical vision-language pre-training methods mainly leverage the correspondence between paired medical images and radiological reports. Although multi-view spatial images and temporal sequences of image-report pairs are available in off-the-shelf \u2026"}, {"title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models", "link": "https://aclanthology.org/2024.naacl-long.53.pdf", "details": "F Jiao, Z Teng, B Ding, Z Liu, N Chen, S Joty - Proceedings of the 2024 Conference of \u2026, 2024", "abstract": "Traditional attempts to enhance the logical reasoning abilities of language models often rely on supervised fine-tuning, limiting their generalization to new tasks or domains. Large Language Models (LLMs), with their capacity to condense vast \u2026"}, {"title": "Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning", "link": "https://openreview.net/pdf%3Fid%3DXwnABAdH5y", "details": "Z Zhang, Y Shi, J Zhu, W Zhou, X Qi, H Li - Forty-first International Conference on Machine \u2026", "abstract": "Trustworthiness is an essential prerequisite for the real-world application of large language models. In this paper, we focus on the trustworthiness of language models with respect to retrieval augmentation. Despite being supported with external \u2026"}, {"title": "Revisiting Pre-training of Embedding Layers in Transformer-based Neural Machine Translation", "link": "https://www.jstage.jst.go.jp/article/jnlp/31/2/31_534/_pdf", "details": "M Neishi, N Yoshinaga - Journal of Natural Language Processing, 2024", "abstract": "Recent trends in the pre-training and fine-tuning paradigm have made significant advances in several natural language processing tasks, including machine translation (MT), particularly for low-resource situations. However, it is reported that \u2026"}, {"title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback", "link": "https://openreview.net/pdf%3Fid%3DBOorDpKHiJ", "details": "G Cui, L Yuan, N Ding, G Yao, B He, W Zhu, Y Ni, G Xie\u2026 - Forty-first International \u2026, 2024", "abstract": "Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability \u2026"}, {"title": "Likelihood-based fine-tuning of protein language models for few-shot fitness prediction and design", "link": "https://www.biorxiv.org/content/biorxiv/early/2024/06/02/2024.05.28.596156.full.pdf", "details": "A Hawkins-Hooker, J Kmec, O Bent, P Duckworth - bioRxiv, 2024", "abstract": "In order to correctly predict amino acid identities within natural proteins, protein language models (PLMs) must implicitly learn distributional constraints on protein sequences upheld over the course of evolution. As a consequence, the sequence \u2026"}]
