[{"title": "Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific Expert Pruning", "link": "https://arxiv.org/pdf/2409.01483", "details": "S Sarkar, L Lausen, V Cevher, S Zha, T Brox, G Karypis - arXiv preprint arXiv \u2026, 2024", "abstract": "Sparse Mixture of Expert (SMoE) models have emerged as a scalable alternative to dense models in language modeling. These models use conditionally activated feedforward subnetworks in transformer blocks, allowing for a separation between \u2026"}, {"title": "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "link": "https://arxiv.org/pdf/2409.17455", "details": "Y Zhou, R Tang, Z Yao, Z Zhu - arXiv preprint arXiv:2409.17455, 2024", "abstract": "Language models (LMs), despite their advances, often depend on spurious correlations, undermining their accuracy and generalizability. This study addresses the overlooked impact of subtler, more complex shortcuts that compromise model \u2026"}, {"title": "Mutual Prompt Leaning for Vision Language Models", "link": "https://link.springer.com/article/10.1007/s11263-024-02243-z", "details": "S Long, Z Zhao, J Yuan, Z Tan, J Liu, J Feng, S Wang\u2026 - International Journal of \u2026, 2024", "abstract": "Large pre-trained vision language models (VLMs) have demonstrated impressive representation learning capabilities, but their transferability across various downstream tasks heavily relies on prompt learning. Since VLMs consist of text and \u2026"}, {"title": "Interpreting and Controlling Linguistic Features in Multilingual Language Models", "link": "https://dspace.cuni.cz/bitstream/handle/20.500.11956/192821/140123221.pdf%3Fsequence%3D1", "details": "T Limisiewicz - 2024", "abstract": "Language models based on neural networks have become the foundation for solving diverse tasks, yet their inner workings remain opaque. This dissertation investigates which components of language models are crucial for representing and processing \u2026"}, {"title": "ScriptSmith: A Unified LLM Framework for Enhancing IT Operations via Automated Bash Script Generation, Assessment, and Refinement", "link": "https://arxiv.org/pdf/2409.17166", "details": "O Chatterjee, P Aggarwal, S Samanta, T Dai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the rapidly evolving landscape of site reliability engineering (SRE), the demand for efficient and effective solutions to manage and resolve issues in site and cloud applications is paramount. This paper presents an innovative approach to action \u2026"}, {"title": "Graph Reasoning with Large Language Models via Pseudo-code Prompting", "link": "https://arxiv.org/pdf/2409.17906", "details": "K Skianis, G Nikolentzos, M Vazirgiannis - arXiv preprint arXiv:2409.17906, 2024", "abstract": "Large language models (LLMs) have recently achieved remarkable success in various reasoning tasks in the field of natural language processing. This success of LLMs has also motivated their use in graph-related tasks. Among others, recent work \u2026"}, {"title": "StressPrompt: Does Stress Impact Large Language Models and Human Performance Similarly?", "link": "https://arxiv.org/pdf/2409.17167", "details": "G Shen, D Zhao, A Bao, X He, Y Dong, Y Zeng - arXiv preprint arXiv:2409.17167, 2024", "abstract": "Human beings often experience stress, which can significantly influence their performance. This study explores whether Large Language Models (LLMs) exhibit stress responses similar to those of humans and whether their performance \u2026"}, {"title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2409.17539", "details": "T Liu, W Xu, W Huang, X Wang, J Wang, H Yang, J Li - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can \u2026"}, {"title": "Decoding by Factual Prompts and Hallucination Prompts Improves Factuality in Large Language Models", "link": "https://www.preprints.org/manuscript/202409.2037/download/final_file", "details": "B Lv, A Feng, C Xie - 2024", "abstract": "Although large language models demonstrate impressive capabilities, they sometimes generate irrelevant or nonsensical text, or produce outputs that deviate from the provided source input\u2014an occurrence commonly referred to as \u2026"}]
