[{"title": "GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models", "link": "https://aclanthology.org/2025.naacl-long.420.pdf", "details": "H Kohli, S Kumar, H Sun - Proceedings of the 2025 Conference of the Nations of \u2026, 2025", "abstract": "The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their reasoning to \u2026"}, {"title": "LLM Ethics Benchmark: A Three-Dimensional Assessment System for Evaluating Moral Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2505.00853", "details": "J Jiao, S Afroogh, A Murali, K Chen, D Atkinson\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This study establishes a novel framework for systematically evaluating the moral reasoning capabilities of large language models (LLMs) as they increasingly integrate into critical societal domains. Current assessment methodologies lack the \u2026"}, {"title": "Atoxia: Red-teaming Large Language Models with Target Toxic Answers", "link": "https://aclanthology.org/2025.findings-naacl.179.pdf", "details": "Y Du, Z Li, P Cheng, X Wan, A Gao - Findings of the Association for Computational \u2026, 2025", "abstract": "Despite the substantial advancements in artificial intelligence, large language models (LLMs) remain being challenged by generation safety. With adversarial jailbreaking prompts, one can effortlessly induce LLMs to output harmful content \u2026"}, {"title": "COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts", "link": "https://arxiv.org/pdf/2504.10158%3F", "details": "J Li, X Zhang, H Zou, Y Guo, R Xu, Y Liu, C Zhu, Y He\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Current object detectors often suffer significant perfor-mance degradation in real- world applications when encountering distributional shifts. Consequently, the out-of- distribution (OOD) generalization capability of object detectors has garnered \u2026"}, {"title": "Future Sight: Fine-Tuning Language Models for Dynamic Story Generation", "link": "https://link.springer.com/chapter/10.1007/978-3-031-90167-6_16", "details": "B Zimmerman, G Sahu, O Vechtomova - \u2026 Intelligence in Music, Sound, Art and Design \u2026, 2025", "abstract": "The recent surge in the development of attention mechanisms has made it possible for language models (LMs) to produce text on par with humans. Unfortunately, the autoregressive nature of attention-based LM decoders inhibits them from attending to \u2026"}, {"title": "RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning", "link": "https://arxiv.org/pdf/2505.03238", "details": "L Boyle, N Baumann, P Sivasothilingam, M Magno\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Future robotic systems operating in real-world environments will require on-board embodied intelligence without continuous cloud connection, balancing capabilities with constraints on computational power and memory. This work presents an \u2026"}, {"title": "Biases in Opinion Dynamics in Multi-Agent Systems of Large Language Models: A Case Study on Funding Allocation", "link": "https://aclanthology.org/2025.findings-naacl.101.pdf", "details": "P Cisneros-Velarde - Findings of the Association for Computational \u2026, 2025", "abstract": "We study the evolution of opinions inside a population of interacting large language models (LLMs). Every LLM needs to decide how much funding to allocate to an item with three initial possibilities: full, partial, or no funding. We identify biases that drive \u2026"}, {"title": "Analyzing and Improving Coherence of Large Language Models in Question Answering", "link": "https://aclanthology.org/2025.naacl-long.588.pdf", "details": "I Lauriola, S Campese, A Moschitti - Proceedings of the 2025 Conference of the \u2026, 2025", "abstract": "Large language models (LLMs) have recently revolutionized natural language processing. These models, however, often suffer from instability or lack of coherence, that is the ability of the models to generate semantically equivalent outputs when \u2026"}, {"title": "Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2504.21277", "details": "G Zhou, P Qiu, C Chen, J Wang, Z Yang, J Xu, M Qiu - arXiv preprint arXiv \u2026, 2025", "abstract": "The integration of reinforcement learning (RL) into the reasoning capabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as a transformative research direction. While MLLMs significantly extend Large Language \u2026"}]
