[{"title": "Mitigating Disparate Impact of Differential Privacy in Federated Learning through Robust Clustering", "link": "https://arxiv.org/pdf/2405.19272", "details": "S Malekmohammadi, A Taik, G Farnadi - arXiv preprint arXiv:2405.19272, 2024", "abstract": "Federated Learning (FL) is a decentralized machine learning (ML) approach that keeps data localized and often incorporates Differential Privacy (DP) to enhance privacy guarantees. Similar to previous work on DP in ML, we observed that \u2026"}, {"title": "CBDMoE: Consistent-but-Diverse Mixture of Experts for Domain Generalization", "link": "https://ieeexplore.ieee.org/abstract/document/10528872/", "details": "F Xu, D Chen, T Jia, S Deng, H Wang - IEEE Transactions on Multimedia, 2024", "abstract": "Machine learning models often suffer from severe performance degradation due to distributional shifts between testing and training data. To address this issue, researchers have focused on domain generalization (DG), which aims to generalize \u2026"}, {"title": "Modeling Dynamic Topics in Chain-Free Fashion by Evolution-Tracking Contrastive Learning and Unassociated Word Exclusion", "link": "https://arxiv.org/pdf/2405.17957", "details": "X Wu, X Dong, L Pan, T Nguyen, AT Luu - arXiv preprint arXiv:2405.17957, 2024", "abstract": "Dynamic topic models track the evolution of topics in sequential documents, which have derived various applications like trend analysis and opinion mining. However, existing models suffer from repetitive topic and unassociated topic issues, failing to \u2026"}, {"title": "Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models", "link": "https://arxiv.org/pdf/2405.16282", "details": "A Kumar, R Morabito, S Umbet, J Kabbara, A Emami - arXiv preprint arXiv \u2026, 2024", "abstract": "As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models \u2026"}]
