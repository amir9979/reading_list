[{"title": "Evaluating BERT-based language models for detecting misinformation", "link": "https://link.springer.com/article/10.1007/s00521-025-11101-z", "details": "R Anggrainingsih, GM Hassan, A Datta - Neural Computing and Applications, 2025", "abstract": "Online misinformation poses a significant challenge due to its rapid spread and limited supervision. To address this issue, automated rumour detection techniques are essential for countering the negative impact of false information. Previous \u2026"}, {"title": "When Debate Fails: Bias Reinforcement in Large Language Models", "link": "https://arxiv.org/pdf/2503.16814", "details": "J Oh, M Jeong, J Ko, SY Yun - arXiv preprint arXiv:2503.16814, 2025", "abstract": "Large Language Models $($ LLMs $) $ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self \u2026"}, {"title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models", "link": "https://arxiv.org/pdf/2503.01763%3F", "details": "Z Shi, Y Wang, L Yan, P Ren, S Wang, D Yin, Z Ren - arXiv preprint arXiv:2503.01763, 2025", "abstract": "Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful \u2026"}, {"title": "OUTLIER-AWARE PREFERENCE OPTIMIZATION FOR LARGE LANGUAGE MODELS", "link": "https://openreview.net/pdf%3Fid%3DYevRFGa9I7", "details": "P Srivastava, SS Nalli, A Deshpande, A Sharma - \u2026 in Foundation Models: The Next Frontier in \u2026", "abstract": "Aligning large language models (LLMs) to user preferences often relies on learning a reward model as a proxy from feedback. However, such reward models can fail on out-of-distribution examples and, if kept static, may reinforce incorrect preferences \u2026"}]
