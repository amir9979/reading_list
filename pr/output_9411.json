[{"title": "Metaaligner: Towards generalizable multi-objective alignment of language models", "link": "https://openreview.net/pdf%3Fid%3DdIVb5C0QFf", "details": "K Yang, Z Liu, Q Xie, J Huang, T Zhang, S Ananiadou - The Thirty-eighth Annual \u2026, 2024", "abstract": "Recent advancements in large language models (LLMs) focus on aligning to heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are dependent on the policy model \u2026"}, {"title": "Multifaceted Natural Language Processing Task\u2013Based Evaluation of Bidirectional Encoder Representations From Transformers Models for Bilingual (Korean and \u2026", "link": "https://medinform.jmir.org/2024/1/e52897/", "details": "K Kim, S Park, J Min, S Park, JY Kim, J Eun, K Jung\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background: The bidirectional encoder representations from transformers (BERT) model has attracted considerable attention in clinical applications, such as patient classification and disease prediction. However, current studies have typically \u2026"}, {"title": "Evaluating the Limitations of Large Language Models in Therapeutic Decision-making for patients with Aortic Stenosis", "link": "https://www.medrxiv.org/content/10.1101/2024.11.20.24313385.full.pdf", "details": "T Roeschl, M Hoffmann, D Hashemi, F Rarreck\u2026 - medRxiv, 2024", "abstract": "Aims: Large language models (LLMs) have shown promise in therapeutic decision- making comparable to medical experts, but these studies have used specially prepared patient data. The aim of this study was to determine whether LLMs can \u2026"}, {"title": "Reducing Distraction in Long-Context Language Models by Focused Learning", "link": "https://arxiv.org/pdf/2411.05928", "details": "Z Wu, B Liu, R Yan, L Chen, T Delteil - arXiv preprint arXiv:2411.05928, 2024", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced their capacity to process long contexts. However, effectively utilizing this long context remains a challenge due to the issue of distraction, where irrelevant \u2026"}, {"title": "EMR-LIP: A lightweight framework for standardizing the preprocessing of longitudinal irregular data in electronic medical records", "link": "https://www.sciencedirect.com/science/article/pii/S0169260724005145", "details": "J Luo, S Huang, L Lan, S Yang, T Cao, J Yin, J Qiu\u2026 - Computer Methods and \u2026, 2024", "abstract": "Abstract Objective Longitudinal data from Electronic Medical Records (EMRs) are increasingly utilized to construct predictive models for various clinical tasks, offering enhanced insights into patient health. However, significant discrepancies exist in \u2026"}, {"title": "Weakly Supervised Language Models for Automated Extraction of Critical Findings from Radiology Reports", "link": "https://www.researchsquare.com/article/rs-5060695/latest.pdf", "details": "A Das, I Talati, JMZ Chaves, D Rubin, I Banerjee - 2024", "abstract": "Critical findings in radiology reports are life threatening conditions that need to be communicated promptly to physicians (\u201ccritical findings\u201d) for timely man-agement of patients. Flagging radiology reports of such incidents could facilitate opportune \u2026"}, {"title": "Ascle: A Python Natural Language Processing Toolkit for Medical Text Generation", "link": "https://s3.ca-central-1.amazonaws.com/assets.jmir.org/assets/preprints/preprint-60601-accepted.pdf", "details": "BR Hsieh, J Goldwasser, A Dave, T Keenan, YH Ke\u2026", "abstract": "Background: Medical texts present significant domain-specific challenges, and manually curating these texts is a timeconsuming and labor-intensive process. Therefore, natural language processing (NLP) algorithms have been developed to \u2026"}, {"title": "Language-Emphasized Cross-Lingual In-Context Learning for Multilingual LLM", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9437-9_26", "details": "J Li, X Wei, X Wang, N Zhuang, L Wang, J Dang - CCF International Conference on \u2026, 2024", "abstract": "With the recent rise of large language models (LLMs), in-context learning (ICL) has shown remarkable performance, eliminating the need for fine-tuning parameters and reducing the reliance on extensive labeled data. However, the intricacies of cross \u2026"}, {"title": "metaTextGrad: Learning to learn with language models as optimizers", "link": "https://openreview.net/pdf%3Fid%3DyzieYIT9hu", "details": "G Xu, M Yuksekgonul, C Guestrin, J Zou - Adaptive Foundation Models: Evolving AI for \u2026", "abstract": "Large language models (LLMs) are increasingly used in learning algorithms, evaluations, and optimization tasks. Recent studies have shown that incorporating self-criticism into LLMs can significantly enhance model performance, with \u2026"}]
