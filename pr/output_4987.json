[{"title": "KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models", "link": "https://arxiv.org/pdf/2408.03297", "details": "R Zhang, Y Xu, Y Xiao, R Zhu, X Jiang, X Chu, J Zhao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks \u2026"}, {"title": "StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation", "link": "https://arxiv.org/pdf/2408.03281", "details": "B Cao, M Ren, H Lin, X Han, F Zhang, J Zhan, L Sun - arXiv preprint arXiv \u2026, 2024", "abstract": "Evaluation is the baton for the development of large language models. Current evaluations typically employ a single-item assessment paradigm for each atomic test objective, which struggles to discern whether a model genuinely possesses the \u2026"}, {"title": "Domain-Specific Pretraining of Language Models: A Comparative Study in the Medical Field", "link": "https://arxiv.org/pdf/2407.14076", "details": "T Kerner - arXiv preprint arXiv:2407.14076, 2024", "abstract": "There are many cases where LLMs are used for specific tasks in a single domain. These usually require less general, but more domain-specific knowledge. Highly capable, general-purpose state-of-the-art language models like GPT-4 or Claude-3 \u2026"}, {"title": "Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement", "link": "https://arxiv.org/pdf/2408.03092", "details": "L Yu, B Yu, H Yu, F Huang, Y Li - arXiv preprint arXiv:2408.03092, 2024", "abstract": "Merging Large Language Models (LLMs) aims to amalgamate multiple homologous LLMs into one with all the capabilities. Ideally, any LLMs sharing the same backbone should be mergeable, irrespective of whether they are Fine-Tuned (FT) with minor \u2026"}, {"title": "Fine-tuning language models for joint rewriting and completion of code with potential bugs", "link": "https://www.amazon.science/publications/fine-tuning-language-models-for-joint-rewriting-and-completion-of-code-with-potential-bugs", "details": "D Wang, J Zhao, H Pei, S Tan, S Zha - 2024", "abstract": "Handling drafty partial code remains a notable challenge in real-time code suggestion applications. Previous work has demonstrated shortcomings of large language models of code (CodeLLMs) in completing partial code with potential bugs \u2026"}, {"title": "Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models", "link": "https://arxiv.org/pdf/2408.02416", "details": "Z Liang, H Hu, Q Ye, Y Xiao, H Li - arXiv preprint arXiv:2408.02416, 2024", "abstract": "The drastic increase of large language models'(LLMs) parameters has led to a new research direction of fine-tuning-free downstream customization by prompts, ie, task descriptions. While these prompt-based services (eg OpenAI's GPTs) play an \u2026"}, {"title": "Using Large Language Models for the Interpretation of Building Regulations", "link": "https://arxiv.org/pdf/2407.21060", "details": "S Fuchs, M Witbrock, J Dimyadi, R Amor - arXiv preprint arXiv:2407.21060, 2024", "abstract": "Compliance checking is an essential part of a construction project. The recent rapid uptake of building information models (BIM) in the construction industry has created more opportunities for automated compliance checking (ACC). BIM enables sharing \u2026"}, {"title": "Cool-Fusion: Fuse Large Language Models without Training", "link": "https://arxiv.org/pdf/2407.19807", "details": "C Liu, X Quan, Y Pan, L Lin, W Wu, X Chen - arXiv preprint arXiv:2407.19807, 2024", "abstract": "We focus on the problem of fusing two or more heterogeneous large language models (LLMs) to facilitate their complementary strengths. One of the challenges on model fusion is high computational load, ie to fine-tune or to align vocabularies via \u2026"}, {"title": "SNFinLLM: Systematic and Nuanced Financial Domain Adaptation of Chinese Large Language Models", "link": "https://arxiv.org/pdf/2408.02302", "details": "S Zhao, L Qiao, K Luo, QW Zhang, J Lu, D Yin - arXiv preprint arXiv:2408.02302, 2024", "abstract": "Large language models (LLMs) have become powerful tools for advancing natural language processing applications in the financial industry. However, existing financial LLMs often face challenges such as hallucinations or superficial parameter \u2026"}]
