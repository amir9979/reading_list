[{"title": "ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models", "link": "https://arxiv.org/pdf/2412.00447", "details": "X Ye, Y Gan, Y Ge, XP Zhang, Y Tang - arXiv preprint arXiv:2412.00447, 2024", "abstract": "Large Vision Language Models (LVLMs) have achieved significant success across multi-modal tasks. However, the computational cost of processing long visual tokens can be prohibitively expensive on resource-limited devices. Previous methods have \u2026"}, {"title": "HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.08378", "details": "S Zhu, W Dong, J Song, Y Guo, B Zheng - arXiv preprint arXiv:2412.08378, 2024", "abstract": "Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into \u2026"}, {"title": "Text-Guided Zero-Shot 3D Style Transfer of Neural Radiance Fields", "link": "https://link.springer.com/chapter/10.1007/978-3-031-78186-5_9", "details": "W Li, WS Zheng - International Conference on Pattern Recognition, 2024", "abstract": "Abstract 3D style transfer aims to generate novel, stylized views while maintaining multi-view consistency. However, current approaches primarily focus on uniformly stylizing entire 3D scenes, limiting the versatility of 3D style transfer. To address this \u2026"}, {"title": "Unleash the Power of Vision-Language Models by Visual Attention Prompt and Multi-modal Interaction", "link": "https://ieeexplore.ieee.org/abstract/document/10814093/", "details": "W Zhang, L Wu, Z Zhang, T Yu, C Ma, X Jin, X Yang\u2026 - IEEE Transactions on \u2026, 2024", "abstract": "Pre-trained vision-language models (VLMs) like CLIP [1], equipped with parameter- efficient tuning (PET) methods like prompting [2], have shown impressive knowledge transferability on new downstream tasks, but they are still prone to be limited by \u2026"}, {"title": "Espresso: High Compression For Rich Extraction From Videos for Your Vision-Language Model", "link": "https://arxiv.org/pdf/2412.04729", "details": "KP Yu, A Dave, R Ambrus, J Mercat - arXiv preprint arXiv:2412.04729, 2024", "abstract": "Most of the current vision-language models (VLMs) for videos struggle to understand videos longer than a few seconds. This is primarily due to the fact that they do not scale to utilizing a large number of frames. In order to address this limitation, we \u2026"}, {"title": "Compositional Image Retrieval via Instruction-Aware Contrastive Learning", "link": "https://arxiv.org/pdf/2412.05756", "details": "W Zhong, W An, F Jiang, H Ma, Y Guo, J Huang - arXiv preprint arXiv:2412.05756, 2024", "abstract": "Composed Image Retrieval (CIR) involves retrieving a target image based on a composed query of an image paired with text that specifies modifications or changes to the visual reference. CIR is inherently an instruction-following task, as the model \u2026"}, {"title": "ConMix: Contrastive Learning with Mixup Augmentation for Dialogue Summarization", "link": "https://link.springer.com/chapter/10.1007/978-981-96-0847-8_18", "details": "Z Chen, J Xiao - International Conference on Advanced Data Mining \u2026, 2024", "abstract": "Seq2seq models have achieved remarkable performance on dialogue summarization, but the exposure bias problem still remains. Contrastive learning has been widely adopted to address this issue. However, previous contrastive learning \u2026"}, {"title": "Do Language Models Understand Time?", "link": "https://arxiv.org/pdf/2412.13845", "details": "X Ding, L Wang - arXiv preprint arXiv:2412.13845, 2024", "abstract": "Large language models (LLMs) have revolutionized video-based computer vision applications, including action recognition, anomaly detection, and video summarization. Videos inherently pose unique challenges, combining spatial \u2026"}, {"title": "SAT: Spatial Aptitude Training for Multimodal Language Models", "link": "https://arxiv.org/pdf/2412.07755", "details": "A Ray, J Duan, R Tan, D Bashkirova, R Hendrix\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Spatial perception is a fundamental component of intelligence. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only test for static spatial reasoning, such as categorizing the relative \u2026"}]
