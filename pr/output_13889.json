[{"title": "Interactive Medical Image Analysis with Concept-based Similarity Reasoning", "link": "https://arxiv.org/pdf/2503.06873", "details": "TD Huy, SK Tran, P Nguyen, NH Tran, TB Sam\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The ability to interpret and intervene model decisions is important for the adoption of computer-aided diagnosis methods in clinical workflows. Recent concept-based methods link the model predictions with interpretable concepts and modify their \u2026"}, {"title": "Synthesizing global and local perspectives in contrastive learning for graph anomaly detection", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125003363", "details": "Q Yang, H Yu, Z Liu, P Li, X Chen, X Luo - Knowledge-Based Systems, 2025", "abstract": "Graph data has shown explosive growth, with application scenarios covering social networks, e-commerce networks, financial transaction networks, etc. In this context, graph anomaly detection is particularly important, aiming to prevent various \u2026"}, {"title": "USP: Unified Self-Supervised Pretraining for Image Generation and Understanding", "link": "https://arxiv.org/pdf/2503.06132", "details": "X Chu, R Li, Y Wang - arXiv preprint arXiv:2503.06132, 2025", "abstract": "Recent studies have highlighted the interplay between diffusion models and representation learning. Intermediate representations from diffusion models can be leveraged for downstream visual tasks, while self-supervised vision models can \u2026"}, {"title": "IDEA Prune: An Integrated Enlarge-and-Prune Pipeline in Generative Language Model Pretraining", "link": "https://arxiv.org/pdf/2503.05920", "details": "Y Li, X Du, A Jaiswal, T Lei, T Zhao, C Wang, J Wang - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in large language models have intensified the need for efficient and deployable models within limited inference budgets. Structured pruning pipelines have shown promise in token efficiency compared to training target-size \u2026"}, {"title": "Twofold Debiasing Enhances Fine-Grained Learning with Coarse Labels", "link": "https://arxiv.org/pdf/2502.19816", "details": "X Zhao, J Jin, Y Li, Y Yao - arXiv preprint arXiv:2502.19816, 2025", "abstract": "The Coarse-to-Fine Few-Shot (C2FS) task is designed to train models using only coarse labels, then leverages a limited number of subclass samples to achieve fine- grained recognition capabilities. This task presents two main challenges: coarse \u2026"}, {"title": "Differentiable Information Enhanced Model-Based Reinforcement Learning", "link": "https://arxiv.org/pdf/2503.01178", "details": "X Zhang, X Cai, B Liu, W Huang, SC Zhu, S Qi, Y Yang - arXiv preprint arXiv \u2026, 2025", "abstract": "Differentiable environments have heralded new possibilities for learning control policies by offering rich differentiable information that facilitates gradient-based methods. In comparison to prevailing model-free reinforcement learning approaches \u2026"}, {"title": "Fine-Grained Alignment and Noise Refinement for Compositional Text-to-Image Generation", "link": "https://arxiv.org/pdf/2503.06506", "details": "AM Izadi, SMH Hosseini, SV Tabar, A Abdollahi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Text-to-image generative models have made significant advancements in recent years; however, accurately capturing intricate details in textual prompts, such as entity missing, attribute binding errors, and incorrect relationships remains a \u2026"}, {"title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs", "link": "https://arxiv.org/pdf/2503.01743%3F", "details": "A Abouelenin, A Ashfaq, A Atkinson, H Awadalla\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent \u2026"}]
