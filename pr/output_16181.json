[{"title": "The Framework and Implementation of Using Large Language Models to Answer Questions about Building Codes and Standards", "link": "https://ascelibrary.org/doi/abs/10.1061/JCCEE5.CPENG-6037", "details": "I Joffe, G Felobes, Y Elgouhari, M Talebi Kalaleh, Q Mei\u2026 - Journal of Computing in \u2026, 2025", "abstract": "Civil and structural engineering design projects are subject to strict regulations of relevant codes and standards to guarantee that certain standards of safety, reliability, and efficiency are met. However, ensuring that all engineering designs comply with \u2026"}, {"title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching", "link": "https://arxiv.org/pdf/2505.04588", "details": "H Sun, Z Qiao, J Guo, X Fan, Y Hou, Y Jiang, P Xie\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by \u2026"}, {"title": "Benchmarking Vision Language Models on German Factual Data", "link": "https://arxiv.org/pdf/2504.11108%3F", "details": "R Peinl, V Tischler - arXiv preprint arXiv:2504.11108, 2025", "abstract": "Similar to LLMs, the development of vision language models is mainly driven by English datasets and models trained in English and Chinese language, whereas support for other languages, even those considered high-resource languages such \u2026"}, {"title": "Pre-Trained Language Models for Mental Health: An Empirical Study on Arabic Q&A Classification", "link": "https://www.mdpi.com/2227-9032/13/9/985", "details": "H Alhuzali, A Alasmari - Healthcare, 2025", "abstract": "Background: Pre-Trained Language Models hold significant promise for revolutionizing mental health care by delivering accessible and culturally sensitive resources. Despite this potential, their efficacy in mental health applications \u2026"}, {"title": "On the Applicability of Code Language Models to Scientific Computing Programs", "link": "https://ieeexplore.ieee.org/abstract/document/10977820/", "details": "Q Zhao, F Liu, X Long, C Wu, L Zhang - IEEE Transactions on Software Engineering, 2025", "abstract": "Scientific Computing Programming Languages (SCPLs), like MATLAB and R, are popular and widely used for computational mathematics. In recent years, pre-trained code language models (CLMs) have automated many code-related tasks, covering \u2026"}, {"title": "Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System", "link": "https://arxiv.org/pdf/2505.01315", "details": "SS Muhaimin, S Mastorakis - arXiv preprint arXiv:2505.01315, 2025", "abstract": "The recent growth in the use of Large Language Models has made them vulnerable to sophisticated adversarial assaults, manipulative prompts, and encoded malicious inputs. Existing countermeasures frequently necessitate retraining models, which is \u2026"}, {"title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment", "link": "https://arxiv.org/pdf/2504.12663", "details": "X Zhang, R Chen, Y Feng, Z Liu - arXiv preprint arXiv:2504.12663, 2025", "abstract": "Aligning language models with human preferences presents significant challenges, particularly in achieving personalization without incurring excessive computational costs. Existing methods rely on reward signals and additional annotated data \u2026"}]
