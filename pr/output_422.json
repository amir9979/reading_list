'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Small Language Models Learn Enhanced Reasoning Skills '
[{"title": "Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering", "link": "https://arxiv.org/html/2403.04890v1", "details": "O Gramopadhye, SS Nachane, P Chanda\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language models (LLMs) have demonstrated significant potential in transforming healthcare by automating tasks such as clinical documentation, information retrieval, and decision support. In this aspect, carefully engineered \u2026"}, {"title": "Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models", "link": "https://arxiv.org/pdf/2403.12966", "details": "Z Liu, Y Dong, Y Rao, J Zhou, J Lu - arXiv preprint arXiv:2403.12966, 2024", "abstract": "In the realm of vision-language understanding, the proficiency of models in interpreting and reasoning over visual content has become a cornerstone for numerous applications. However, it is challenging for the visual encoder in Large \u2026"}, {"title": "Language models scale reliably with over-training and on downstream tasks", "link": "https://arxiv.org/pdf/2403.08540", "details": "SY Gadre, G Smyrnis, V Shankar, S Gururangan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute \u2026"}, {"title": "Debiasing Large Visual Language Models", "link": "https://arxiv.org/pdf/2403.05262", "details": "YF Zhang, W Yu, Q Wen, X Wang, Z Zhang, L Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the realms of computer vision and natural language processing, Large Vision- Language Models (LVLMs) have become indispensable tools, proficient in generating textual descriptions based on visual inputs. Despite their advancements \u2026"}, {"title": "Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving", "link": "https://arxiv.org/html/2403.19838v1", "details": "A Gopalkrishnan, R Greer, M Trivedi - arXiv preprint arXiv:2403.19838, 2024", "abstract": "Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) have become prominent in autonomous driving research, as these models can provide interpretable textual reasoning and responses for end-to-end autonomous driving \u2026"}, {"title": "Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity", "link": "https://arxiv.org/html/2403.09428v1", "details": "Z Zhi, Z Liu, M Elbadawi, A Daneshmend, M Orlu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal machine learning with missing modalities is an increasingly relevant challenge arising in various applications such as healthcare. This paper extends the current research into missing modalities to the low-data regime, ie, a downstream \u2026"}, {"title": "Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning", "link": "https://arxiv.org/pdf/2404.00909", "details": "R Li, Y Wu, X He - arXiv preprint arXiv:2404.00909, 2024", "abstract": "Generative vision-language models (VLMs) have shown impressive performance in zero-shot vision-language tasks like image captioning and visual question answering. However, improving their zero-shot reasoning typically requires second \u2026"}, {"title": "DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.08857", "details": "M Huang, Y Long, X Deng, R Chu, J Xiong, X Liang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Text-to-image (T2I) generation models have significantly advanced in recent years. However, effective interaction with these models is challenging for average users due to the need for specialized prompt engineering knowledge and the inability to \u2026"}, {"title": "GiT: Towards Generalist Vision Transformer through Universal Language Interface", "link": "https://arxiv.org/pdf/2403.09394", "details": "H Wang, H Tang, L Jiang, S Shi, MF Naeem, H Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper proposes a simple, yet effective framework, called GiT, simultaneously applicable for various vision tasks only with a vanilla ViT. Motivated by the universality of the Multi-layer Transformer architecture (eg, GPT) widely used in large \u2026"}]
