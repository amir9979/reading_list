[{"title": "Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning", "link": "https://arxiv.org/pdf/2412.08614", "details": "F Lu, W Wu, K Zheng, S Ma, B Gong, J Liu, W Zhai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating detailed captions comprehending text-rich visual content in images has received growing attention for Large Vision-Language Models (LVLMs). However, few studies have developed benchmarks specifically tailored for detailed captions to \u2026"}, {"title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding", "link": "https://arxiv.org/pdf/2412.16158", "details": "C Tao, S Su, X Zhu, C Zhang, Z Chen, J Liu, W Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advance of Large Language Models (LLMs) has catalyzed the development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid modality-specific encoders, offer a promising alternative to the compositional ones \u2026"}, {"title": "ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models", "link": "https://arxiv.org/pdf/2412.00447", "details": "X Ye, Y Gan, Y Ge, XP Zhang, Y Tang - arXiv preprint arXiv:2412.00447, 2024", "abstract": "Large Vision Language Models (LVLMs) have achieved significant success across multi-modal tasks. However, the computational cost of processing long visual tokens can be prohibitively expensive on resource-limited devices. Previous methods have \u2026"}, {"title": "Development of a Large-scale Dataset of Chest Computed Tomography Reports in Japanese and a High-performance Finding Classification Model", "link": "https://arxiv.org/pdf/2412.15907", "details": "Y Yamagishi, Y Nakamura, T Kikuchi, Y Sonoda\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Background: Recent advances in large language models highlight the need for high- quality multilingual medical datasets. While Japan leads globally in CT scanner deployment and utilization, the lack of large-scale Japanese radiology datasets has \u2026"}, {"title": "Compositional Image Retrieval via Instruction-Aware Contrastive Learning", "link": "https://arxiv.org/pdf/2412.05756", "details": "W Zhong, W An, F Jiang, H Ma, Y Guo, J Huang - arXiv preprint arXiv:2412.05756, 2024", "abstract": "Composed Image Retrieval (CIR) involves retrieving a target image based on a composed query of an image paired with text that specifies modifications or changes to the visual reference. CIR is inherently an instruction-following task, as the model \u2026"}, {"title": "Text-Guided Zero-Shot 3D Style Transfer of Neural Radiance Fields", "link": "https://link.springer.com/chapter/10.1007/978-3-031-78186-5_9", "details": "W Li, WS Zheng - International Conference on Pattern Recognition, 2024", "abstract": "Abstract 3D style transfer aims to generate novel, stylized views while maintaining multi-view consistency. However, current approaches primarily focus on uniformly stylizing entire 3D scenes, limiting the versatility of 3D style transfer. To address this \u2026"}, {"title": "MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer Classification and Detection", "link": "https://arxiv.org/pdf/2412.15925", "details": "A Moglia, EC Nastasio, L Mainardi, P Cerveri - arXiv preprint arXiv:2412.15925, 2024", "abstract": "Problem: Pancreas radiological imaging is challenging due to the small size, blurred boundaries, and variability of shape and position of the organ among patients. Goal: In this work we present MiniGPT-Pancreas, a Multimodal Large Language Model \u2026"}, {"title": "Frequency Is What You Need: Word-frequency Masking Benefits Vision-Language Model Pre-training", "link": "https://arxiv.org/pdf/2412.16148", "details": "M Liang, M Larson - arXiv preprint arXiv:2412.16148, 2024", "abstract": "Vision Language Models (VLMs) can be trained more efficiently if training sets can be reduced in size. Recent work has shown the benefits of masking text during VLM training using a variety of approaches: truncation, random masking, block masking \u2026"}, {"title": "Espresso: High Compression For Rich Extraction From Videos for Your Vision-Language Model", "link": "https://arxiv.org/pdf/2412.04729", "details": "KP Yu, A Dave, R Ambrus, J Mercat - arXiv preprint arXiv:2412.04729, 2024", "abstract": "Most of the current vision-language models (VLMs) for videos struggle to understand videos longer than a few seconds. This is primarily due to the fact that they do not scale to utilizing a large number of frames. In order to address this limitation, we \u2026"}]
