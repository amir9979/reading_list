[{"title": "Generative Example-Based Explanations: Bridging the Gap between Generative Modeling and Explainability", "link": "https://arxiv.org/pdf/2410.20890", "details": "P Vaeth, AM Fruehwald, B Paassen, M Gregorova - arXiv preprint arXiv:2410.20890, 2024", "abstract": "Recently, several methods have leveraged deep generative modeling to produce example-based explanations of decision algorithms for high-dimensional input data. Despite promising results, a disconnect exists between these methods and the \u2026"}, {"title": "Test-Time Adaptation of 3D Point Clouds via Denoising Diffusion Models", "link": "https://arxiv.org/pdf/2411.14495", "details": "H Dastmalchi, A An, A Cheraghian, S Rahman\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Test-time adaptation (TTA) of 3D point clouds is crucial for mitigating discrepancies between training and testing samples in real-world scenarios, particularly when handling corrupted point clouds. LiDAR data, for instance, can be affected by sensor \u2026"}, {"title": "CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense", "link": "https://arxiv.org/pdf/2410.23091", "details": "M Zhang, K Bi, W Chen, Q Chen, J Guo, X Cheng - arXiv preprint arXiv:2410.23091, 2024", "abstract": "Despite ongoing efforts to defend neural classifiers from adversarial attacks, they remain vulnerable, especially to unseen attacks. In contrast, humans are difficult to be cheated by subtle manipulations, since we make judgments only based on \u2026"}]
