[{"title": "Probing Fundamental Visual Comprehend Capabilities on Vision Language Models via Visual Phrases from Structural Data", "link": "https://link.springer.com/article/10.1007/s12559-024-10351-8", "details": "P Xie, B Liu - Cognitive Computation, 2024", "abstract": "Does the model demonstrate exceptional proficiency in \u201citem counting,\u201d\u201ccolor recognition,\u201d or other Fundamental Visual Comprehension Capability (FVCC)? There have been remarkable advancements in the field of multimodal, the pretrained \u2026"}, {"title": "FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs", "link": "https://arxiv.org/pdf/2409.13612", "details": "B Yan, Z Zhang, L Jing, E Hossain, X Du - arXiv preprint arXiv:2409.13612, 2024", "abstract": "The rapid development of Large Vision-Language Models (LVLMs) often comes with widespread hallucination issues, making cost-effective and comprehensive assessments increasingly vital. Current approaches mainly rely on costly annotations \u2026"}, {"title": "A Unified Hallucination Mitigation Framework for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.16494", "details": "Y Chang, L Jing, X Zhang, Y Zhang - arXiv preprint arXiv:2409.16494, 2024", "abstract": "Hallucination is a common problem for Large Vision-Language Models (LVLMs) with long generations which is difficult to eradicate. The generation with hallucinations is partially inconsistent with the image content. To mitigate hallucination, current \u2026"}, {"title": "Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models", "link": "https://arxiv.org/pdf/2409.12512", "details": "J Rao, X Liu, Z Lin, L Ding, J Li, D Tao - arXiv preprint arXiv:2409.12512, 2024", "abstract": "Knowledge distillation (KD) is a technique that compresses large teacher models by training smaller student models to mimic them. The success of KD in auto-regressive language models mainly relies on Reverse KL for mode-seeking and student \u2026"}, {"title": "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models", "link": "https://arxiv.org/pdf/2409.13592", "details": "A Nandy, Y Agarwal, A Patwa, MM Das, A Bansal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Understanding satire and humor is a challenging task for even current Vision- Language models. In this paper, we propose the challenging tasks of Satirical Image Detection (detecting whether an image is satirical), Understanding (generating the \u2026"}, {"title": "Training Language Models to Self-Correct via Reinforcement Learning", "link": "https://arxiv.org/pdf/2409.12917", "details": "A Kumar, V Zhuang, R Agarwal, Y Su, JD Co-Reyes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Existing approaches for training self-correction either require multiple models or rely on a \u2026"}, {"title": "pFedGPA: Diffusion-based Generative Parameter Aggregation for Personalized Federated Learning", "link": "https://arxiv.org/pdf/2409.05701", "details": "J Lai, J Li, J Xu, Y Wu, B Tang, S Chen, Y Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Federated Learning (FL) offers a decentralized approach to model training, where data remains local and only model parameters are shared between the clients and the central server. Traditional methods, such as Federated Averaging (FedAvg) \u2026"}, {"title": "JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images", "link": "https://arxiv.org/pdf/2409.12953", "details": "Z Wang, J Liu, CW Tang, H Alomari, A Sivakumar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing vision-language understanding benchmarks largely consist of images of objects in their usual contexts. As a consequence, recent multimodal large language models can perform well with only a shallow visual understanding by relying on \u2026"}, {"title": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Foundation Models", "link": "https://arxiv.org/pdf/2409.11919", "details": "A Cardiel, E Zablocki, O Sim\u00e9oni, E Ramzi, M Cord - arXiv preprint arXiv:2409.11919, 2024", "abstract": "Vision Language Models (VLMs) have shown impressive performances on numerous tasks but their zero-shot capabilities can be limited compared to dedicated or fine-tuned models. Yet, fine-tuning VLMs comes with limitations as it requireswhite \u2026"}]
