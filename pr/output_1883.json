[{"title": "THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2405.05256", "details": "P Kaul, Z Li, H Yang, Y Dukler, A Swaminathan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term\" Type I hallucinations\". Instead, they focus on \u2026"}, {"title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision", "link": "https://arxiv.org/pdf/2405.04086", "details": "Y Tong, S Wang, D Li, Y Wang, S Han, Z Lin, C Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations \u2026"}, {"title": "MDCNet: Long-term time series forecasting with mode decomposition and 2D convolution", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124006208", "details": "J Su, D Xie, Y Duan, Y Zhou, X Hu, S Duan - Knowledge-Based Systems, 2024", "abstract": "Long-term time series forecasting is widely used in various real-world applications, such as weather, traffic, energy, healthcare, etc. Recently, time series decomposition techniques have been adopted in many mainstream forecasting models, such as the \u2026"}, {"title": "Argumentative Large Language Models for Explainable and Contestable Decision-Making", "link": "https://arxiv.org/pdf/2405.02079", "details": "G Freedman, A Dejl, D Gorur, X Yin, A Rago, F Toni - arXiv preprint arXiv:2405.02079, 2024", "abstract": "The diversity of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them a promising candidate for use in decision-making. However, they are currently limited \u2026"}, {"title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models", "link": "https://arxiv.org/pdf/2405.09818", "details": "C Team - arXiv preprint arXiv:2405.09818, 2024", "abstract": "We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an \u2026"}, {"title": "Advanced Natural-based interaction for the ITAlian language: LLaMAntino-3-ANITA", "link": "https://arxiv.org/pdf/2405.07101", "details": "M Polignano, P Basile, G Semeraro - arXiv preprint arXiv:2405.07101, 2024", "abstract": "In the pursuit of advancing natural language processing for the Italian language, we introduce a state-of-the-art Large Language Model (LLM) based on the novel Meta LLaMA-3 model: LLaMAntino-3-ANITA-8B-Inst-DPO-ITA. We fine-tuned the original \u2026"}, {"title": "Interpretable Multi-task Learning with Shared Variable Embeddings", "link": "https://arxiv.org/pdf/2405.06330", "details": "M \u017belaszczyk, J Ma\u0144dziuk - arXiv preprint arXiv:2405.06330, 2024", "abstract": "This paper proposes a general interpretable predictive system with shared information. The system is able to perform predictions in a multi-task setting where distinct tasks are not bound to have the same input/output structure. Embeddings of \u2026"}, {"title": "Backdoor Removal for Generative Large Language Models", "link": "https://arxiv.org/pdf/2405.07667", "details": "H Li, Y Chen, Z Zheng, Q Hu, C Chan, H Liu, Y Song - arXiv preprint arXiv \u2026, 2024", "abstract": "With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased \u2026"}, {"title": "Improving Language Models Trained with Translated Data via Continual Pre-Training and Dictionary Learning Analysis", "link": "https://arxiv.org/pdf/2405.14277", "details": "S Boughorbel, MD Parvez, M Hawasly - arXiv preprint arXiv:2405.14277, 2024", "abstract": "Training LLMs in low resources languages usually utilizes data augmentation with machine translation (MT) from English language. However, translation brings a number of challenges: there are large costs attached to translating and curating huge \u2026"}]
