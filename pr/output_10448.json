[{"title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models", "link": "https://bmva-archive.org.uk/bmvc/2024/papers/Paper_66/paper.pdf", "details": "E Ali, MH Khan - 2024", "abstract": "Recent advances in large-scale vision-language models have achieved impressive performance in various zero-shot image classification tasks. While prior studies have demonstrated significant improvements by introducing few-shot labelled target \u2026"}, {"title": "iPrOp: Interactive Prompt Optimization for Large Language Models with a Human in the Loop", "link": "https://arxiv.org/pdf/2412.12644", "details": "J Li, R Klinger - arXiv preprint arXiv:2412.12644, 2024", "abstract": "Prompt engineering has made significant contributions to the era of large language models, yet its effectiveness depends on the skills of a prompt author. Automatic prompt optimization can support the prompt development process, but requires \u2026"}, {"title": "Assessing the Limitations of Large Language Models in Clinical Fact Decomposition", "link": "https://arxiv.org/pdf/2412.12422", "details": "M Munnangi, A Swaminathan, JA Fries, J Jindal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Verifying factual claims is critical for using large language models (LLMs) in healthcare. Recent work has proposed fact decomposition, which uses LLMs to rewrite source text into concise sentences conveying a single piece of information, as \u2026"}, {"title": "Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models", "link": "https://arxiv.org/pdf/2412.02980", "details": "A Havrilla, A Dai, L O'Mahony, K Oostermeijer, V Zisler\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it \u2026"}, {"title": "Cross-modal Information Flow in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2411.18620", "details": "Z Zhang, S Yadav, F Han, E Shutova - arXiv preprint arXiv:2411.18620, 2024", "abstract": "The recent advancements in auto-regressive multimodal large language models (MLLMs) have demonstrated promising progress for vision-language tasks. While there exists a variety of studies investigating the processing of linguistic information \u2026"}, {"title": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over Aligned Large Language Models", "link": "https://arxiv.org/pdf/2412.12865", "details": "Y Fan, Y Hong, Q Wang, J Bao, H Jiang, Y Song - arXiv preprint arXiv:2412.12865, 2024", "abstract": "Alignment, endowing a pre-trained Large language model (LLM) with the ability to follow instructions, is crucial for its real-world applications. Conventional supervised fine-tuning (SFT) methods formalize it as causal language modeling typically with a \u2026"}, {"title": "Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large Language Models", "link": "https://arxiv.org/pdf/2412.12687", "details": "S Oh, J Kim, J Park, SW Ko, TQS Quek, SL Kim - arXiv preprint arXiv:2412.12687, 2024", "abstract": "This paper studies a hybrid language model (HLM) architecture that integrates a small language model (SLM) operating on a mobile device with a large language model (LLM) hosted at the base station (BS) of a wireless network. The HLM token \u2026"}]
