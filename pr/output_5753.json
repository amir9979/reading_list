[{"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "link": "https://arxiv.org/pdf/2408.12337", "details": "KS Phogat, SA Puranam, S Dasaratha, C Harsha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain \u2026"}, {"title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism", "link": "https://arxiv.org/pdf/2408.10473", "details": "G Li, X Zhao, L Liu, Z Li, D Li, L Tian, J He, A Sirasao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational \u2026"}, {"title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.06663", "details": "K Sun, M Dredze - arXiv preprint arXiv:2408.06663, 2024", "abstract": "The development of large language models leads to the formation of a pre-train-then- align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream \u2026"}, {"title": "Importance Weighting Can Help Large Language Models Self-Improve", "link": "https://arxiv.org/pdf/2408.09849", "details": "C Jiang, C Chan, W Xue, Q Liu, Y Guo - arXiv preprint arXiv:2408.09849, 2024", "abstract": "Large language models (LLMs) have shown remarkable capability in numerous tasks and applications. However, fine-tuning LLMs using high-quality datasets under external supervision remains prohibitively expensive. In response, LLM self \u2026"}, {"title": "Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models", "link": "https://arxiv.org/pdf/2408.10682", "details": "H Yuan, Z Jin, P Cao, Y Chen, K Liu, J Zhao - arXiv preprint arXiv:2408.10682, 2024", "abstract": "LLM have achieved success in many fields but still troubled by problematic content in the training corpora. LLM unlearning aims at reducing their influence and avoid undesirable behaviours. However, existing unlearning methods remain vulnerable to \u2026"}, {"title": "FactorLLM: Factorizing Knowledge via Mixture of Experts for Large Language Models", "link": "https://arxiv.org/pdf/2408.11855", "details": "Z Zhao, M Dong, R Zhang, W Zheng, Y Zhang, H Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has demonstrated that Feed-Forward Networks (FFNs) in Large Language Models (LLMs) play a pivotal role in storing diverse linguistic and factual knowledge. Conventional methods frequently face challenges due to knowledge \u2026"}, {"title": "Jailbreak Open-Sourced Large Language Models via Enforced Decoding", "link": "https://aclanthology.org/2024.acl-long.299.pdf", "details": "H Zhang, Z Guo, H Zhu, B Cao, L Lin, J Jia, J Chen\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In \u2026"}, {"title": "Benchmarking Large Language Models for Math Reasoning Tasks", "link": "https://arxiv.org/pdf/2408.10839", "details": "K Se\u00dfler, Y Rong, E G\u00f6zl\u00fckl\u00fc, E Kasneci - arXiv preprint arXiv:2408.10839, 2024", "abstract": "The use of Large Language Models (LLMs) in mathematical reasoning has become a cornerstone of related research, demonstrating the intelligence of these models and enabling potential practical applications through their advanced performance \u2026"}, {"title": "Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators", "link": "https://arxiv.org/pdf/2408.12325", "details": "D Yang, D Xiao, J Wei, M Li, Z Chen, K Li, L Zhang - arXiv preprint arXiv:2408.12325, 2024", "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone to generate responses that contradict verifiable facts, ie, unfaithful hallucination content. Existing efforts generally focus on optimizing model parameters or editing semantic \u2026"}]
