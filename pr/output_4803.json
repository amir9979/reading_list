[{"title": "IGU-Aug: Information-guided unsupervised augmentation and pixel-wise contrastive learning for medical image analysis", "link": "https://ieeexplore.ieee.org/abstract/document/10620395/", "details": "Q Quan, Q Yao, H Zhu, SK Zhou - IEEE Transactions on Medical Imaging, 2024", "abstract": "Contrastive learning (CL) is a form of self-supervised learning and has been widely used for various tasks. Different from widely studied instance-level contrastive learning, pixel-wise contrastive learning mainly helps with pixel-wise dense \u2026"}, {"title": "LIONs: An Empirically Optimized Approach to Align Language Models", "link": "https://arxiv.org/pdf/2407.06542", "details": "X Yu, Q Wu, Y Li, Z Yu - arXiv preprint arXiv:2407.06542, 2024", "abstract": "Alignment is a crucial step to enhance the instruction-following and conversational abilities of language models. Despite many recent work proposing new algorithms, datasets, and training pipelines, there is a lack of comprehensive studies measuring \u2026"}, {"title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "link": "https://arxiv.org/pdf/2407.21417", "details": "Z Wu, Y Zhang, P Qi, Y Xu, R Han, Y Zhang, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (ie, follow open-ended instructions) and \u2026"}, {"title": "Large-Scale Study on AI's Impact on Identifying Chest Radiographs with No Actionable Disease in Outpatient Imaging", "link": "https://www.sciencedirect.com/science/article/pii/S1076633224003908", "details": "A Mansoor, I Schmuecking, FC Ghesu, B Georgescu\u2026 - Academic Radiology, 2024", "abstract": "Rationale and Objectives Given the high volume of chest radiographs, radiologists frequently encounter heavy workloads. In outpatient imaging, a substantial portion of chest radiographs show no actionable findings. Automatically identifying these cases \u2026"}, {"title": "ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts", "link": "https://arxiv.org/pdf/2407.09447", "details": "AF Hardy, H Liu, B Lange, MJ Kochenderfer - arXiv preprint arXiv:2407.09447, 2024", "abstract": "Typical schemes for automated red-teaming large language models (LLMs) focus on discovering prompts that trigger a frozen language model (the defender) to generate toxic text. This often results in the prompting model (the adversary) producing text \u2026"}, {"title": "Role Exchange-Based Self-Training Semi-Supervision Framework for Complex Medical Image Segmentation", "link": "https://ieeexplore.ieee.org/abstract/document/10620642/", "details": "Y Wu, G Wu, J Lin, Y Wang, J Yu - IEEE Transactions on Neural Networks and \u2026, 2024", "abstract": "Segmentation of complex medical images such as vascular network and pulmonary tracheal network requires segmentation of many tiny targets on each tomographic section of the 3-D medical image volume. Although semantic segmentation of \u2026"}, {"title": "Interpretable Differential Diagnosis with Dual-Inference Large Language Models", "link": "https://arxiv.org/pdf/2407.07330", "details": "S Zhou, S Ding, J Wang, M Lin, GB Melton, R Zhang - arXiv preprint arXiv:2407.07330, 2024", "abstract": "Methodological advancements to automate the generation of differential diagnosis (DDx) to predict a list of potential diseases as differentials given patients' symptom descriptions are critical to clinical reasoning and applications such as decision \u2026"}, {"title": "Contrasting the performance of mainstream Large Language Models in Radiology Board Examinations", "link": "https://www.researchsquare.com/article/rs-4573702/latest.pdf", "details": "B Wei, X Zhang, Y Shao, X Sun, L Chen - 2024", "abstract": "Objective This study evaluates the performance of mainstream Large Language Models, including GPT-4, Claude, Bard, Tongyi Qianwen, and Gemini Pro, in radiology board exams. Methods A comparative analysis of 150 multiple-choice \u2026"}, {"title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT", "link": "https://arxiv.org/pdf/2407.21058", "details": "M Ali, S Panda, Q Shen, M Wick, A Kobren - arXiv preprint arXiv:2407.21058, 2024", "abstract": "In the current landscape of language model research, larger models, larger datasets and more compute seems to be the only way to advance towards intelligence. While there have been extensive studies of scaling laws and models' scaling behaviors, the \u2026"}]
