[{"title": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering", "link": "https://arxiv.org/pdf/2405.12701", "details": "M Jeong, H Hwang, C Yoon, T Lee, J Kang - arXiv preprint arXiv:2405.12701, 2024", "abstract": "In the medical domain, numerous scenarios necessitate the long-form generation ability of large language models (LLMs). Specifically, when addressing patients' questions, it is essential that the model's response conveys factual claims \u2026"}, {"title": "THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2405.05256", "details": "P Kaul, Z Li, H Yang, Y Dukler, A Swaminathan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term\" Type I hallucinations\". Instead, they focus on \u2026"}, {"title": "The Dawn of Natural Language to SQL: Are We Fully Ready?", "link": "https://arxiv.org/pdf/2406.01265", "details": "B Li, Y Luo, C Chai, G Li, N Tang - arXiv preprint arXiv:2406.01265, 2024", "abstract": "Translating users' natural language questions into SQL queries (ie, NL2SQL) significantly lowers the barriers to accessing relational databases. The emergence of Large Language Models has introduced a novel paradigm in NL2SQL tasks \u2026"}, {"title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision", "link": "https://arxiv.org/pdf/2405.04086", "details": "Y Tong, S Wang, D Li, Y Wang, S Han, Z Lin, C Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations \u2026"}, {"title": "MCNet: Multivariate Long-Term Time Series Forecasting with Local and Global Context Modeling", "link": "https://www.sciencedirect.com/science/article/pii/S0020025524007783", "details": "J Sun, J Zhai - Information Sciences, 2024", "abstract": "Time series data typically exhibit various intra-sequence and inter-sequence correlations, resulting in intricate, intertwined dependencies, which pose challenges for accurately predicting future long-term trends. Previous studies have not fully \u2026"}, {"title": "Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning? An Extensive Investigation into the Capabilities and Limitations of \u2026", "link": "https://arxiv.org/pdf/2406.00257", "details": "MS Islam, R Rahman, A Masry, MTR Laskar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Natural language is a powerful complementary modality of communication for data visualizations, such as bar and line charts. To facilitate chart-based reasoning using natural language, various downstream tasks have been introduced recently such as \u2026"}, {"title": "FedHPL: Efficient Heterogeneous Federated Learning with Prompt Tuning and Logit Distillation", "link": "https://arxiv.org/pdf/2405.17267", "details": "Y Ma, L Cheng, Y Wang, Z Zhong, X Xu, M Wang - arXiv preprint arXiv:2405.17267, 2024", "abstract": "Federated learning (FL) is a popular privacy-preserving paradigm that enables distributed clients to collaboratively train models with a central server while keeping raw data locally. In practice, distinct model architectures, varying data distributions \u2026"}, {"title": "TAeKD: Teacher Assistant Enhanced Knowledge Distillation for Closed-Source Multilingual Neural Machine Translation", "link": "https://aclanthology.org/2024.lrec-main.1350.pdf", "details": "B Lv, X Liu, K Wei, P Luo, Y Yu - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Knowledge Distillation (KD) serves as an efficient method for transferring language knowledge from open-source large language models (LLMs) to more computationally efficient models. However, challenges arise when attempting to \u2026"}, {"title": "Interpretable Multi-task Learning with Shared Variable Embeddings", "link": "https://arxiv.org/pdf/2405.06330", "details": "M \u017belaszczyk, J Ma\u0144dziuk - arXiv preprint arXiv:2405.06330, 2024", "abstract": "This paper proposes a general interpretable predictive system with shared information. The system is able to perform predictions in a multi-task setting where distinct tasks are not bound to have the same input/output structure. Embeddings of \u2026"}]
