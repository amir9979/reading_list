[{"title": "Gradient Extrapolation for Debiased Representation Learning", "link": "https://arxiv.org/pdf/2503.13236", "details": "I Asaad, M Shadaydeh, J Denzler - arXiv preprint arXiv:2503.13236, 2025", "abstract": "Machine learning classification models trained with empirical risk minimization (ERM) often inadvertently rely on spurious correlations. When absent in the test data, these unintended associations between non-target attributes and target labels lead \u2026"}, {"title": "SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion", "link": "https://arxiv.org/pdf/2503.11576", "details": "A Nassar, A Marafioti, M Omenetti, M Lysak\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce SmolDocling, an ultra-compact vision-language model targeting end-to- end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements \u2026"}, {"title": "IPAD: Inverse Prompt for AI Detection--A Robust and Explainable LLM-Generated Text Detector", "link": "https://arxiv.org/pdf/2502.15902", "details": "Z Chen, Y Feng, C He, Y Deng, H Pu, B Li - arXiv preprint arXiv:2502.15902, 2025", "abstract": "Large Language Models (LLMs) have attained human-level fluency in text generation, which complicates the distinguishing between human-written and LLM- generated texts. This increases the risk of misuse and highlights the need for reliable \u2026"}, {"title": "IDEA Prune: An Integrated Enlarge-and-Prune Pipeline in Generative Language Model Pretraining", "link": "https://arxiv.org/pdf/2503.05920", "details": "Y Li, X Du, A Jaiswal, T Lei, T Zhao, C Wang, J Wang - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in large language models have intensified the need for efficient and deployable models within limited inference budgets. Structured pruning pipelines have shown promise in token efficiency compared to training target-size \u2026"}, {"title": "EU-Nets: Enhanced, Explainable and Parsimonious U-Nets", "link": "https://arxiv.org/pdf/2502.18122", "details": "B Sun, P Li\u00f2 - arXiv preprint arXiv:2502.18122, 2025", "abstract": "In this study, we propose MHEX+, a framework adaptable to any U-Net architecture. Built upon MHEX+, we introduce novel U-Net variants, EU-Nets, which enhance explainability and uncertainty estimation, addressing the limitations of traditional U \u2026"}, {"title": "Hierarchical Vision\u2013Language Pre-Training with Freezing Strategy for Multi-Level Semantic Alignment", "link": "https://www.mdpi.com/2079-9292/14/4/816", "details": "H Xie, Y Qin, S Ding - Electronics, 2025", "abstract": "Vision\u2013language pre-training (VLP) faces challenges in aligning hierarchical textual semantics (words/phrases/sentences) with multi-scale visual features (objects/relations/global context). We propose a hierarchical VLP model (HieVLP) \u2026"}, {"title": "Iterative Counterfactual Data Augmentation", "link": "https://arxiv.org/pdf/2502.18249%3F", "details": "M Plyler, M Chi - arXiv preprint arXiv:2502.18249, 2025", "abstract": "Counterfactual data augmentation (CDA) is a method for controlling information or biases in training datasets by generating a complementary dataset with typically opposing biases. Prior work often either relies on hand-crafted rules or algorithmic \u2026"}, {"title": "TabGLM: Tabular Graph Language Model for Learning Transferable Representations Through Multi-Modal Consistency Minimization", "link": "https://arxiv.org/pdf/2502.18847", "details": "A Majee, M Xenochristou, WP Chen - arXiv preprint arXiv:2502.18847, 2025", "abstract": "Handling heterogeneous data in tabular datasets poses a significant challenge for deep learning models. While attention-based architectures and self-supervised learning have achieved notable success, their application to tabular data remains \u2026"}, {"title": "Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models", "link": "https://arxiv.org/pdf/2503.12072", "details": "A Ravichander, J Fisher, T Sorensen, X Lu, Y Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-quality training data has proven crucial for developing performant large language models (LLMs). However, commercial LLM providers disclose few, if any, details about the data used for training. This lack of transparency creates multiple \u2026"}]
