[{"title": "High-Fidelity Transfer of Functional Priors for Wide Bayesian Neural Networks by Learning Activations", "link": "https://arxiv.org/pdf/2410.15777", "details": "M Sendera, A Sorkhei, T Ku\u015bmierczyk - arXiv preprint arXiv:2410.15777, 2024", "abstract": "Function-space priors in Bayesian Neural Networks provide a more intuitive approach to embedding beliefs directly into the model's output, thereby enhancing regularization, uncertainty quantification, and risk-aware decision-making. However \u2026"}, {"title": "Personalizing Low-Rank Bayesian Neural Networks Via Federated Learning", "link": "https://arxiv.org/pdf/2410.14390", "details": "B Zhang, D Liu, O Simeone, G Wang, D Pezaros, G Zhu - arXiv preprint arXiv \u2026, 2024", "abstract": "To support real-world decision-making, it is crucial for models to be well-calibrated, ie, to assign reliable confidence estimates to their predictions. Uncertainty quantification is particularly important in personalized federated learning (PFL), as \u2026"}, {"title": "Time Series Classification with Large Language Models via Linguistic Scaffolding", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10706904.pdf", "details": "H Jang, JY Yang, J Hwang, E Yang - IEEE Access, 2024", "abstract": "Time series classification requires specialized models that can effectively capture temporal structures. Consequently, Large Language Models (LLMs) have emerged as promising candidates due to their proficiency in sequence modeling and semantic \u2026"}, {"title": "TS-TCD: Triplet-Level Cross-Modal Distillation for Time-Series Forecasting Using Large Language Models", "link": "https://arxiv.org/pdf/2409.14978", "details": "P Wang, H Zheng, S Dai, W Yue, W Zhu, X Wang - arXiv preprint arXiv:2409.14978, 2024", "abstract": "In recent years, large language models (LLMs) have shown great potential in time- series analysis by capturing complex dependencies and improving predictive performance. However, existing approaches often struggle with modality alignment \u2026"}, {"title": "Fine-Tuning Personalization in Federated Learning to Mitigate Adversarial Clients", "link": "https://arxiv.org/pdf/2409.20329", "details": "Y Allouah, AE Mrini, R Guerraoui, N Gupta, R Pinot - arXiv preprint arXiv:2409.20329, 2024", "abstract": "Federated learning (FL) is an appealing paradigm that allows a group of machines (aka clients) to learn collectively while keeping their data local. However, due to the heterogeneity between the clients' data distributions, the model obtained through the \u2026"}, {"title": "Dynamic Contrastive Learning for Time Series Representation", "link": "https://arxiv.org/pdf/2410.15416", "details": "AK Shamba, K Bach, G Taylor - arXiv preprint arXiv:2410.15416, 2024", "abstract": "Understanding events in time series is an important task in a variety of contexts. However, human analysis and labeling are expensive and time-consuming. Therefore, it is advantageous to learn embeddings for moments in time series in an \u2026"}, {"title": "Online Zero-Shot Classification with CLIP Supplementary", "link": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/09976-supp.pdf", "details": "Q Qian, J Hu", "abstract": "Online Zero-Shot Classification with CLIP Supplementary Page 1 Online Zero-Shot Classification with CLIP Supplementary Qi Qian1 \u22c6 and Juhua Hu2 1 Alibaba Group, Bellevue, WA 98004, USA 2 School of Engineering and Technology, University of \u2026"}]
