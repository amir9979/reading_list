'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [How Important is Domain Specificity in Language Models'
[{"title": "Smaller Language Models are Better Zero-shot Machine-Generated Text Detectors", "link": "https://aclanthology.org/2024.eacl-short.25.pdf", "details": "N Mireshghallah, J Mattern, S Gao, R Shokri\u2026 - Proceedings of the 18th \u2026, 2024", "abstract": "As large language models are becoming more embedded in different user-facing services, it is important to be able to distinguish between human-written and machine- generated text to verify the authenticity of news articles, product reviews, etc. Thus, in \u2026"}, {"title": "\" My Answer is C\": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models", "link": "https://arxiv.org/pdf/2402.14499", "details": "X Wang, B Ma, C Hu, L Weber-Genzel, P R\u00f6ttger\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The \u2026"}, {"title": "RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models", "link": "https://arxiv.org/html/2403.02271v1", "details": "S Najafi, A Fyshe - arXiv preprint arXiv:2403.02271, 2024", "abstract": "Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter- efficient fine-tuning methods that optimize input prompts or adjust a small number of \u2026"}, {"title": "Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models", "link": "https://arxiv.org/pdf/2402.13492", "details": "S Maekawa, H Iso, S Gurajada, N Bhutani - arXiv preprint arXiv:2402.13492, 2024", "abstract": "While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant \u2026"}, {"title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.07860", "details": "S Zhao, S Hao, B Zi, H Xu, KYK Wong - arXiv preprint arXiv:2403.07860, 2024", "abstract": "Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding \u2026"}, {"title": "STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning", "link": "https://arxiv.org/html/2402.13468v1", "details": "N Beck, A Iyer, R Iyer - arXiv preprint arXiv:2402.13468, 2024", "abstract": "As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models. Active learning, which attempts to mine \u2026"}, {"title": "Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach", "link": "https://arxiv.org/pdf/2403.12297", "details": "M Mahbub, GM Dams, S Srinivasan, C Rizy, I Danciu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Substance use disorder (SUD) poses a major concern due to its detrimental effects on health and society. SUD identification and treatment depend on a variety of factors such as severity, co-determinants (eg, withdrawal symptoms), and social \u2026"}, {"title": "Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity", "link": "https://arxiv.org/pdf/2403.05257", "details": "S Hoshino, A Kato, S Murakami, P Zhang - arXiv preprint arXiv:2403.05257, 2024", "abstract": "Learning better sentence embeddings leads to improved performance for natural language understanding tasks including semantic textual similarity (STS) and natural language inference (NLI). As prior studies leverage large-scale labeled NLI datasets \u2026"}, {"title": "Learn to Compare: Localize Actions under Weak Supervision", "link": "https://web.pkusz.edu.cn/adsp/files/2024/02/10-CVPR2021-Learn-to-Compare.pdf", "details": "C Zhang, M Cao, D Yang, J Chen, Y Zou", "abstract": "Weakly-supervised temporal action localization (WSTAL) aims to localize actions in untrimmed videos with only video-level labels. Most existing models follow the \u201clocalization by classification\u201d procedure: locate temporal regions contributing most \u2026"}]
