[{"title": "Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge", "link": "https://arxiv.org/pdf/2503.04036", "details": "X Cui, JTZ Wei, S Swayamdipta, R Jia - arXiv preprint arXiv:2503.04036, 2025", "abstract": "Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques \u2026"}, {"title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing", "link": "https://arxiv.org/pdf/2503.15815%3F", "details": "VA Dasu, V Gupta, S Tizpaz-Niari, G Tan - arXiv preprint arXiv:2503.15815, 2025", "abstract": "This paper explores pruning attention heads as a post-processing bias mitigation method for large language models (LLMs). Modern AI systems such as LLMs are expanding into sensitive social contexts where fairness concerns become especially \u2026"}, {"title": "Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment", "link": "https://arxiv.org/pdf/2503.04647", "details": "W Yang, J Wu, C Wang, C Zong, J Zhang - arXiv preprint arXiv:2503.04647, 2025", "abstract": "Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences. While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is \u2026"}, {"title": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2504.01857", "details": "Z Yu, T Li, C Wang, H Chen, L Zhou - arXiv preprint arXiv:2504.01857, 2025", "abstract": "Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing reasoning capabilities in large language models (LLMs), with self-consistency demonstrating notable promise in boosting performance. However, inherent \u2026"}, {"title": "Language model personalization via reward factorization", "link": "https://arxiv.org/pdf/2503.06358", "details": "I Shenfeld, F Faltings, P Agrawal, A Pacchiano - arXiv preprint arXiv:2503.06358, 2025", "abstract": "Modern large language models (LLMs) are optimized for human-aligned responses using Reinforcement Learning from Human Feedback (RLHF). However, existing RLHF approaches assume a universal preference model and fail to account for \u2026"}]
