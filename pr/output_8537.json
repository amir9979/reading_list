[{"title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models", "link": "https://arxiv.org/pdf/2410.18785%3F", "details": "Q Li, X Liu, Z Tang, P Dong, Z Li, X Pan, X Chu - arXiv preprint arXiv:2410.18785, 2024", "abstract": "Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality, with many methods excelling across these criteria. Some \u2026"}, {"title": "Reducing the Scope of Language Models with Circuit Breakers", "link": "https://arxiv.org/pdf/2410.21597", "details": "D Yunis, S Huo, C Gunasekara, D Contractor - arXiv preprint arXiv:2410.21597, 2024", "abstract": "Language models are now deployed in a wide variety of user-facing applications, often for specific purposes like answering questions about documentation or acting as coding assistants. As these models are intended for particular purposes, they \u2026"}, {"title": "Analysing the Residual Stream of Language Models Under Knowledge Conflicts", "link": "https://arxiv.org/pdf/2410.16090", "details": "Y Zhao, X Du, G Hong, AP Gema, A Devoto, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model \u2026"}, {"title": "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes", "link": "https://arxiv.org/pdf/2410.16930", "details": "BR Christ, Z Gottesman, J Kropko, T Hartvigsen - arXiv preprint arXiv:2410.16930, 2024", "abstract": "Math reasoning is a highly active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be \u2026"}, {"title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models", "link": "https://arxiv.org/pdf/2410.18252", "details": "M Noukhovitch, S Huang, S Xhonneux, A Hosseini\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this \u2026"}, {"title": "Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Models Alignment", "link": "https://arxiv.org/pdf/2410.16714", "details": "M Wang, C Ma, Q Chen, L Meng, Y Han, J Xiao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-play methods have demonstrated remarkable success in enhancing model capabilities across various domains. In the context of Reinforcement Learning from Human Feedback (RLHF), self-play not only boosts Large Language Model (LLM) \u2026"}, {"title": "Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in Language Models", "link": "https://openreview.net/pdf%3Fid%3DHOLs697aIx", "details": "A Dutta, YC Hsiao - NeurIPS 2024 Workshop on Open-World Agents", "abstract": "We propose a novel in-context learning algorithm for building autonomous decision- making language agents. The language agent continuously attempts to solve the same task by reasoning, acting, observing and then self-correcting each time the task \u2026"}, {"title": "SafeBench: A Safety Evaluation Framework for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2410.18927", "details": "Z Ying, A Liu, S Liang, L Huang, J Guo, W Zhou, X Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal Large Language Models (MLLMs) are showing strong safety concerns (eg, generating harmful outputs for users), which motivates the development of safety evaluation benchmarks. However, we observe that existing safety benchmarks for \u2026"}, {"title": "How Many Are in This Image A Safety Evaluation Benchmark for Vision LLMs", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-72983-6_3.pdf", "details": "J Han, W Zhou, H Yao, C Xie", "abstract": "This work focuses on benchmarking the capabilities of vision large language models (VLLMs) in visual reasoning. Different from prior studies, we shift our focus from evaluating standard performance to introducing a comprehensive safety evaluation \u2026"}]
