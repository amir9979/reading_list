[{"title": "Teaching Language Models to Critique via Reinforcement Learning", "link": "https://arxiv.org/pdf/2502.03492", "details": "Z Xie, L Chen, W Mao, J Xu, L Kong - arXiv preprint arXiv:2502.03492, 2025", "abstract": "Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide accurate judgments and actionable suggestions. In this work, we \u2026"}, {"title": "Improving Foundation Model for Endoscopy Video Analysis via Representation Learning on Long Sequences", "link": "https://ieeexplore.ieee.org/abstract/document/10885043/", "details": "Z Wang, C Liu, L Zhu, T Wang, S Zhang, Q Dou - IEEE Journal of Biomedical and \u2026, 2025", "abstract": "Recent advancements in endoscopy video analysis have relied on the utilization of relatively short video clips extracted from longer videos or millions of individual frames. However, these approaches tend to neglect the domain-specific \u2026"}, {"title": "Estimating the Observability of an Outcome from an Electronic Health Records Dataset Using External Data", "link": "https://academic.oup.com/aje/advance-article-abstract/doi/10.1093/aje/kwaf013/7994438", "details": "M Yan, H Hong, J Wilson, BA Goldstein - American Journal of Epidemiology, 2025", "abstract": "One of the key limitations of electronic health records (EHR) data is that not all health care encounters are observed. The degree to which patient information is captured is referred to as observability. Poor observability, and in particular differential \u2026"}, {"title": "Evaluating Entity Retrieval in Electronic Health Records: a Semantic Gap Perspective", "link": "https://arxiv.org/pdf/2502.06252", "details": "Z Zhao, H Yuan, J Liu, H Chen, H Ying, S Zhou, S Yu - arXiv preprint arXiv \u2026, 2025", "abstract": "Entity retrieval plays a crucial role in the utilization of Electronic Health Records (EHRs) and is applied across a wide range of clinical practices. However, a comprehensive evaluation of this task is lacking due to the absence of a public \u2026"}, {"title": "Beyond In-Distribution Success: Scaling Curves of CoT Granularity for Language Model Generalization", "link": "https://arxiv.org/pdf/2502.18273", "details": "R Wang, W Huang, S Song, H Zhang, Y Iwasawa\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Generalization to novel compound tasks under distribution shift is important for deploying transformer-based language models (LMs). This work investigates Chain- of-Thought (CoT) reasoning as a means to enhance OOD generalization. Through \u2026"}, {"title": "Towards Better Understanding of Program-of-Thought Reasoning in Cross-Lingual and Multilingual Environments", "link": "https://arxiv.org/pdf/2502.17956", "details": "P Payoungkhamdee, P Tuchinda, J Baek\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multi-step reasoning is essential for large language models (LLMs), yet multilingual performance remains challenging. While Chain-of-Thought (CoT) prompting improves reasoning, it struggles with non-English languages due to the \u2026"}, {"title": "CryptoX: Compositional Reasoning Evaluation of Large Language Models", "link": "https://arxiv.org/pdf/2502.07813", "details": "J Shi, C Wei, L Yang, ZM Wang, C Yang, G Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The compositional reasoning capacity has long been regarded as critical to the generalization and intelligence emergence of large language models LLMs. However, despite numerous reasoning-related benchmarks, the compositional \u2026"}, {"title": "Comprehensive analysis of transparency and accessibility of chatgpt, deepseek, and other sota large language models", "link": "https://www.preprints.org/frontend/manuscript/1ed0ef5c816d69833a6b6a32ca2dd3bb/download_pub", "details": "R Sapkota, S Raza, M Karkee - Preprints. org DOI, 2025", "abstract": "Despite increasing discussions on open-source Artificial Intelligence (AI), existing research lacks a discussion on the transparency and accessibility of state-of-the-art (SoTA) Large Language Models (LLMs). The Open Source Initiative (OSI) has \u2026"}]
