[{"title": "Process-based Self-Rewarding Language Models", "link": "https://arxiv.org/pdf/2503.03746", "details": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' \u2026"}, {"title": "Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models", "link": "https://arxiv.org/pdf/2503.02318", "details": "Z Xie, M Lin, Z Liu, P Wu, S Yan, C Miao - arXiv preprint arXiv:2503.02318, 2025", "abstract": "Recent advancements in multimodal reasoning have largely overlooked the audio modality. We introduce Audio-Reasoner, a large-scale audio language model for deep reasoning in audio tasks. We meticulously curated a large-scale and diverse \u2026"}, {"title": "Self-Steering Language Models", "link": "https://openreview.net/pdf%3Fid%3Dx7E2Qt7n0V", "details": "G Grand, JB Tenenbaum, V Mansinghka, AK Lew\u2026 - \u2026 : VerifAI: AI Verification in the Wild", "abstract": "For many reasoning tasks, augmenting language models with test-time compute can significantly boost performance. However, scaling inference is costly for complex problems that require extensive search or sampling. Nevertheless, even when LMs \u2026"}, {"title": "Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks", "link": "https://arxiv.org/pdf/2503.04833", "details": "L Lu, S Pang, S Liang, H Zhu, X Zeng, A Liu, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal large language models (MLLMs) have made remarkable strides in cross- modal comprehension and generation tasks. However, they remain vulnerable to jailbreak attacks, where crafted perturbations bypass security guardrails and elicit \u2026"}, {"title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems", "link": "https://arxiv.org/pdf/2503.03686", "details": "R Ye, S Tang, R Ge, Y Du, Z Yin, S Chen, J Shao - arXiv preprint arXiv:2503.03686, 2025", "abstract": "LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability \u2026"}, {"title": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs", "link": "https://arxiv.org/pdf/2503.05139", "details": "L Team, B Zeng, C Huang, C Zhang, C Tian, C Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this technical report, we tackle the challenges of training large-scale Mixture of Experts (MoE) models, focusing on overcoming cost inefficiency and resource limitations prevalent in such systems. To address these issues, we present two \u2026"}, {"title": "CROWDSELECT: Synthetic Instruction Data Selection with Multi-LLM Wisdom", "link": "https://arxiv.org/pdf/2503.01836%3F", "details": "Y Li, L Yang, W Shen, P Zhou, Y Wan, W Lin, D Chen - arXiv preprint arXiv \u2026, 2025", "abstract": "Distilling advanced Large Language Models' instruction-following capabilities into smaller models using a selected subset has become a mainstream approach in model training. While existing synthetic instruction data selection strategies rely \u2026"}, {"title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation", "link": "https://arxiv.org/pdf/2503.02832", "details": "S Zhang, X Zhang, T Zhang, B Hu, Y Chen, J Xu - arXiv preprint arXiv:2503.02832, 2025", "abstract": "In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most \u2026"}, {"title": "Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety", "link": "https://arxiv.org/pdf/2503.05021", "details": "Y Zhang, M Li, W Han, Y Yao, Z Cen, D Zhao - arXiv preprint arXiv:2503.05021, 2025", "abstract": "Large Language Models (LLMs) are vulnerable to jailbreak attacks that exploit weaknesses in traditional safety alignment, which often relies on rigid refusal heuristics or representation engineering to block harmful outputs. While they are \u2026"}]
