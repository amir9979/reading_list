[{"title": "Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination", "link": "https://arxiv.org/pdf/2409.12746", "details": "ES Salido, R Morante, J Gonzalo, G Marco\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this article we present UNED-ACCESS 2024, a bilingual dataset that consists of 1003 multiple-choice questions of university entrance level exams in Spanish and English. Questions are originally formulated in Spanish and translated manually into \u2026"}, {"title": "Egalitarian Language Representation in Language Models: It All Begins with Tokenizers", "link": "https://arxiv.org/pdf/2409.11501", "details": "M Velayuthan, K Sarveswaran - arXiv preprint arXiv:2409.11501, 2024", "abstract": "Tokenizers act as a bridge between human language and the latent space of language models, influencing how language is represented in these models. Due to the immense popularity of English-Centric Large Language Models (LLMs), efforts \u2026"}, {"title": "Qlarify: Recursively Expandable Abstracts for Dynamic Information Retrieval over Scientific Papers", "link": "https://dl.acm.org/doi/abs/10.1145/3654777.3676397", "details": "R Fok, JC Chang, T August, AX Zhang, DS Weld - Proceedings of the 37th Annual \u2026, 2024", "abstract": "Navigating the vast scientific literature often starts with browsing a paper's abstract. However, when a reader seeks additional information, not present in the abstract, they face a costly cognitive chasm during their dive into the full text. To bridge this \u2026"}, {"title": "Generative Chain-of-Thought for Zero-Shot Cognitive Reasoning", "link": "https://link.springer.com/chapter/10.1007/978-3-031-72344-5_22", "details": "L Liu, D Zhang, S Zhu, S Li - International Conference on Artificial Neural Networks, 2024", "abstract": "Cognitive reasoning holds a significant place within the field of Natural Language Processing (NLP). Yet, the exploration of zero-shot scenarios, which align more closely with real-life situations than supervised scenarios, has been relatively limited \u2026"}, {"title": "Investigating Layer Importance in Large Language Models", "link": "https://arxiv.org/pdf/2409.14381", "details": "Y Zhang, Y Dong, K Kawaguchi - arXiv preprint arXiv:2409.14381, 2024", "abstract": "Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in \u2026"}, {"title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "link": "https://arxiv.org/pdf/2409.13853", "details": "Z Wang, R Bao, Y Wu, J Taylor, C Xiao, F Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pretrained large language models (LLMs) have revolutionized natural language processing (NLP) tasks such as summarization, question answering, and translation. However, LLMs pose significant security risks due to their tendency to memorize \u2026"}, {"title": "From Linguistic Giants to Sensory Maestros: A Survey on Cross-Modal Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2409.18996", "details": "S Qian, Z Zhou, D Xue, B Wang, C Xu - arXiv preprint arXiv:2409.18996, 2024", "abstract": "Cross-modal reasoning (CMR), the intricate process of synthesizing and drawing inferences across divergent sensory modalities, is increasingly recognized as a crucial capability in the progression toward more sophisticated and anthropomorphic \u2026"}, {"title": "Search for Efficient Large Language Models", "link": "https://arxiv.org/pdf/2409.17372", "details": "X Shen, P Zhao, Y Gong, Z Kong, Z Zhan, Y Wu, M Lin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have long held sway in the realms of artificial intelligence research. Numerous efficient techniques, including weight pruning, quantization, and distillation, have been embraced to compress LLMs, targeting \u2026"}, {"title": "Counterfactual Causal Inference in Natural Language with Large Language Models", "link": "https://arxiv.org/pdf/2410.06392", "details": "G Gendron, JM Ro\u017eanec, M Witbrock, G Dobbie - arXiv preprint arXiv:2410.06392, 2024", "abstract": "Causal structure discovery methods are commonly applied to structured data where the causal variables are known and where statistical testing can be used to assess the causal relationships. By contrast, recovering a causal structure from unstructured \u2026"}]
