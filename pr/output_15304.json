[{"title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge", "link": "https://arxiv.org/pdf/2504.07887", "details": "R Cantini, A Orsino, M Ruggiero, D Talia - arXiv preprint arXiv:2504.07887, 2025", "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised \u2026"}, {"title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis", "link": "https://arxiv.org/pdf/2504.10352", "details": "Y Yang, S Liu, J Li, Y Hu, H Wu, H Wang, J Yu, L Meng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and \u2026"}, {"title": "Weight Ensembling Improves Reasoning in Language Models", "link": "https://arxiv.org/pdf/2504.10478", "details": "X Dang, C Baek, K Wen, Z Kolter, A Raghunathan - arXiv preprint arXiv:2504.10478, 2025", "abstract": "We investigate a failure mode that arises during the training of reasoning models, where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, the Pass@ 1 rate reliably improves during supervised finetuning \u2026"}, {"title": "Decoupling Contrastive Decoding: Robust Hallucination Mitigation in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2504.08809", "details": "W Chen, X Yan, B Wen, F Yang, T Gao, D Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Although multimodal large language models (MLLMs) exhibit remarkable reasoning capabilities on complex multimodal understanding tasks, they still suffer from the notorious hallucination issue: generating outputs misaligned with obvious visual or \u2026"}, {"title": "Summarizing Online Patient Conversations Using Generative Language Models: Experimental and Comparative Study", "link": "https://medinform.jmir.org/2025/1/e62909/", "details": "RAS Nair, M Hartung, P Heinisch, J Jaskolski\u2026 - JMIR Medical Informatics, 2025", "abstract": "Background: Social media is acknowledged by regulatory bodies (eg, the Food and Drug Administration) as an important source of patient experience data to learn about patients' unmet needs, priorities, and preferences. However, current methods \u2026"}, {"title": "HLMEA: Unsupervised Entity Alignment Based on Hybrid Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/33294/35449", "details": "X Jin, Z Wang, J Chen, L Yang, B Oh, S Hwang, J Li - Proceedings of the AAAI \u2026, 2025", "abstract": "Entity alignment (EA) is crucial for integrating knowledge graphs (KGs) constructed from diverse sources. Conventional unsupervised EA approaches attempt to eliminate human intervention but often suffer from accuracy limitations. With the rise \u2026"}, {"title": "V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2504.06148", "details": "X Zheng, L Li, Z Yang, P Yu, AJ Wang, R Yan, Y Yao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current \u2026"}, {"title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?", "link": "https://arxiv.org/pdf/2504.10000", "details": "Y Wang, J Guan, J Liang, R He - arXiv preprint arXiv:2504.10000, 2025", "abstract": "Multi-modal large language models (MLLMs) have made significant progress, yet their safety alignment remains limited. Typically, current open-source MLLMs rely on the alignment inherited from their language module to avoid harmful generations \u2026"}, {"title": "From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models", "link": "https://arxiv.org/pdf/2504.06214", "details": "C Xu, W Ping, P Xu, Z Liu, B Wang, M Shoeybi, B Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Long-context capabilities are essential for a wide range of applications, including document and video understanding, in-context learning, and inference-time scaling, all of which require models to process and reason over long sequences of text and \u2026"}]
