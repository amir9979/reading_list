[{"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}, {"title": "Self-supervised analogical learning using language models", "link": "https://arxiv.org/pdf/2502.00996", "details": "B Zhou, S Jain, Y Zhang, Q Ning, S Wang, Y Benajiba\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can \u2026"}, {"title": "EHealth: A Chinese Biomedical Language Model Built via Multi-Level Text Discrimination", "link": "https://ieeexplore.ieee.org/abstract/document/10857372/", "details": "Q Wang, S Dai, B Xu, Y Lyu, H Wu, H Wang - IEEE Transactions on Audio, Speech \u2026, 2025", "abstract": "Pre-trained language models (PLMs) have recently revolutionized the field of natural language processing, impacting not only the general domain but also the biomedical domain. Most previous studies on constructing biomedical PLMs relied simply on \u2026"}, {"title": "How Much Do Code Language Models Remember? An Investigation on Data Extraction Attacks before and after Fine-tuning", "link": "https://arxiv.org/pdf/2501.17501", "details": "F Salerno, A Al-Kaswan, M Izadi - arXiv preprint arXiv:2501.17501, 2025", "abstract": "Code language models, while widely popular, are often trained on unsanitized source code gathered from across the Internet. Previous work revealed that pre- trained models can remember the content of their training data and regurgitate them \u2026"}, {"title": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models", "link": "https://arxiv.org/pdf/2501.17420", "details": "Y Li, H Shirado, S Das - arXiv preprint arXiv:2501.17420, 2025", "abstract": "While advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating \u2026"}, {"title": "Engaging Preference Optimization Alignment in Large Language Model for Continual Radiology Report Generation: A Hybrid Approach", "link": "https://link.springer.com/article/10.1007/s12559-025-10404-6", "details": "A Izhar, N Idris, N Japar - Cognitive Computation, 2025", "abstract": "Large language models (LLMs) remain relatively underutilized in medical imaging, particularly in radiology, which is essential for disease diagnosis and management. Nonetheless, radiology report generation (RRG) is a time-consuming task that can \u2026"}, {"title": "MCG-Net: Medical Chief Complaint-guided Multi-modal Masked Content Pre-training for chest image classification", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425002829", "details": "L Zou, J Li, H Chen, M Liang, J Ke, Y Zhong, J Chen - Expert Systems with \u2026, 2025", "abstract": "Medical image classification plays a crucial role in disease diagnosis, personalized treatment, and clinical decision-making, with significant progress driven by deep learning (DL). However, DL's performance heavily relies on large-scale, accurately \u2026"}, {"title": "Denoising Multi-Level Cross-Attention and Contrastive Learning for Chest Radiology Report Generation", "link": "https://link.springer.com/article/10.1007/s10278-025-01422-9", "details": "D Zhu, L Liu, X Yang, L Liu, W Peng - Journal of Imaging Informatics in Medicine, 2025", "abstract": "Chest radiology report generation plays a vital role in supporting diagnosis, alleviating physician workload, and reducing the risk of misdiagnosis. However, significant challenges persist:(1) Data bias and background noise in chest images \u2026"}, {"title": "Adapter-Enhanced Hierarchical Cross-Modal Pre-training for Lightweight Medical Report Generation", "link": "https://ieeexplore.ieee.org/abstract/document/10856362/", "details": "T Yu, W Lu, Y Yang, W Han, Q Huang, J Yu, K Zhang - IEEE Journal of Biomedical \u2026, 2025", "abstract": "Automatic medical report generation is an emerging field that aims to transform medical images into descriptive, clinically relevant narratives, potentially reducing the workload for radiologists significantly. Despite substantial progress, the \u2026"}]
