[{"title": "Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation", "link": "https://arxiv.org/pdf/2502.11306", "details": "H Nguyen, Z He, SA Gandre, U Pasupulety\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) often suffer from hallucination, generating factually incorrect or ungrounded content, which limits their reliability in high-stakes applications. A key factor contributing to hallucination is the use of hard labels during \u2026"}, {"title": "Evaluation of Best-of-N Sampling Strategies for Language Model Alignment", "link": "https://arxiv.org/pdf/2502.12668", "details": "Y Ichihara, Y Jinnai, T Morimura, K Ariu, K Abe\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) with human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward \u2026"}, {"title": "Towards Efficient Automatic Self-Pruning of Large Language Models", "link": "https://arxiv.org/pdf/2502.14413", "details": "W Huang, Y Zhang, X Zheng, F Chao, R Ji - arXiv preprint arXiv:2502.14413, 2025", "abstract": "Despite exceptional capabilities, Large Language Models (LLMs) still face deployment challenges due to their enormous size. Post-training structured pruning is a promising solution that prunes LLMs without the need for retraining, reducing \u2026"}, {"title": "Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking", "link": "https://arxiv.org/pdf/2502.12970", "details": "J Zhu, L Yan, S Wang, D Yin, L Sha - arXiv preprint arXiv:2502.12970, 2025", "abstract": "The reasoning abilities of Large Language Models (LLMs) have demonstrated remarkable advancement and exceptional performance across diverse domains. However, leveraging these reasoning capabilities to enhance LLM safety against \u2026"}, {"title": "Comprehensive Analysis of Transparency and Accessibility of ChatGPT, DeepSeek, And other SoTA Large Language Models", "link": "https://www.preprints.org/frontend/manuscript/1ed0ef5c816d69833a6b6a32ca2dd3bb/download_pub", "details": "R Sapkota, S Raza, M Karkee - Preprints. org DOI, 2025", "abstract": "Despite increasing discussions on open-source Artificial Intelligence (AI), existing research lacks a discussion on the transparency and accessibility of state-of-the-art (SoTA) Large Language Models (LLMs). The Open Source Initiative (OSI) has \u2026"}, {"title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models", "link": "https://arxiv.org/pdf/2502.12464", "details": "S Lee, DB Lee, D Wagner, M Kang, H Seong, T Bocklet\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To \u2026"}, {"title": "Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation", "link": "https://proceedings.neurips.cc/paper_files/paper/2024/file/e142fd2b70f10db2543c64bca1417de8-Paper-Conference.pdf", "details": "W JIAWEI, R Jiang, C Yang, Z Wu, R Shibasaki\u2026 - Advances in Neural \u2026, 2025", "abstract": "This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively \u2026"}, {"title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region", "link": "https://arxiv.org/pdf/2502.13946", "details": "CT Leong, Q Yin, J Wang, W Li - arXiv preprint arXiv:2502.13946, 2025", "abstract": "The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a \u2026"}, {"title": "Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2502.13260", "details": "Y Cui, P He, J Zeng, H Liu, X Tang, Z Dai, Y Han, C Luo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Chain-of-Thought (CoT) reasoning, which breaks down complex tasks into intermediate reasoning steps, has significantly enhanced the performance of large language models (LLMs) on challenging tasks. However, the detailed reasoning \u2026"}]
