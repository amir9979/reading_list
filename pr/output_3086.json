[{"title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions", "link": "https://arxiv.org/pdf/2405.19744", "details": "C Li, W Yang, J Zhang, J Lu, S Wang, C Zong - arXiv preprint arXiv:2405.19744, 2024", "abstract": "Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples \u2026"}, {"title": "Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?", "link": "https://arxiv.org/pdf/2406.14492", "details": "G Geigle, R Timofte, G Glava\u0161 - arXiv preprint arXiv:2406.14492, 2024", "abstract": "Large vision-language models (LVLMs) have recently dramatically pushed the state of the art in image captioning and many image understanding tasks (eg, visual question answering). LVLMs, however, often\\textit {hallucinate} and produce \u2026"}, {"title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models", "link": "https://arxiv.org/pdf/2406.14852", "details": "J Wang, Y Ming, Z Shi, V Vineet, X Wang, N Joshi - arXiv preprint arXiv:2406.14852, 2024", "abstract": "Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoning--a fundamental \u2026"}, {"title": "Visual-Text Cross Alignment: Refining the Similarity Score in Vision-Language Models", "link": "https://arxiv.org/pdf/2406.02915", "details": "J Li, H Li, S Erfani, L Feng, J Bailey, F Liu - arXiv preprint arXiv:2406.02915, 2024", "abstract": "It has recently been discovered that using a pre-trained vision-language model (VLM), eg, CLIP, to align a whole query image with several finer text descriptions generated by a large language model can significantly enhance zero-shot \u2026"}, {"title": "Query-based Semantic Gaussian Field for Scene Representation in Reinforcement Learning", "link": "https://arxiv.org/pdf/2406.02370", "details": "J Wang, Z Zhang, Q Zhang, J Li, J Sun, M Sun, J He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Latent scene representation plays a significant role in training reinforcement learning (RL) agents. To obtain good latent vectors describing the scenes, recent works incorporate the 3D-aware latent-conditioned NeRF pipeline into scene \u2026"}, {"title": "Refusal in Language Models Is Mediated by a Single Direction", "link": "https://arxiv.org/pdf/2406.11717", "details": "A Arditi, O Obeso, A Syed, D Paleka, N Rimsky\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying \u2026"}, {"title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback", "link": "https://openreview.net/pdf%3Fid%3DBOorDpKHiJ", "details": "G Cui, L Yuan, N Ding, G Yao, B He, W Zhu, Y Ni, G Xie\u2026 - Forty-first International \u2026, 2024", "abstract": "Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability \u2026"}, {"title": "Confidence Regulation Neurons in Language Models", "link": "https://arxiv.org/pdf/2406.16254", "details": "A Stolfo, B Wu, W Gurnee, Y Belinkov, X Song\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their widespread use, the mechanisms by which large language models (LLMs) represent and regulate uncertainty in next-token predictions remain largely unexplored. This study investigates two critical components believed to influence this \u2026"}, {"title": "AgentGym: Evolving Large Language Model-based Agents across Diverse Environments", "link": "https://arxiv.org/pdf/2406.04151", "details": "Z Xi, Y Ding, W Chen, B Hong, H Guo, J Wang, D Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community. Large language models (LLMs) are considered a promising foundation to build such \u2026"}]
