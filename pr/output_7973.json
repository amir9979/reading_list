[{"title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks", "link": "https://arxiv.org/pdf/2410.05160%3F", "details": "Z Jiang, R Meng, X Yang, S Yavuz, Y Zhou, W Chen - arXiv preprint arXiv:2410.05160, 2024", "abstract": "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize \u2026"}, {"title": "Imaging foundation model for universal enhancement of non-ideal measurement CT", "link": "https://arxiv.org/pdf/2410.01591%3F", "details": "Y Liu, R Ge, Y He, Z Wu, C You, S Li, Y Chen - arXiv preprint arXiv:2410.01591, 2024", "abstract": "Non-ideal measurement computed tomography (NICT), which sacrifices optimal imaging standards for new advantages in CT imaging, is expanding the clinical application scope of CT images. However, with the reduction of imaging standards \u2026"}, {"title": "Ask, Pose, Unite: Scaling Data Acquisition for Close Interactions with Vision Language Models", "link": "https://arxiv.org/abs/2410.00309", "details": "L Bravo-S\u00e1nchez, J Heo, Z Weng, KC Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Social dynamics in close human interactions pose significant challenges for Human Mesh Estimation (HME), particularly due to the complexity of physical contacts and the scarcity of training data. Addressing these challenges, we introduce a novel data \u2026"}, {"title": "No Need to Talk: Asynchronous Mixture of Language Models", "link": "https://arxiv.org/pdf/2410.03529%3F", "details": "A Filippova, A Katharopoulos, D Grangier, R Collobert - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce SmallTalk LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each model of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth \u2026"}, {"title": "MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router", "link": "https://arxiv.org/pdf/2410.12013", "details": "Y Xie, Z Zhang, D Zhou, C Xie, Z Song, X Liu, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) architectures face challenges such as high memory consumption and redundancy in experts. Pruning MoE can reduce network weights while maintaining model performance. Motivated by the recent observation of \u2026"}, {"title": "CXPMRG-Bench: Pre-training and Benchmarking for X-ray Medical Report Generation on CheXpert Plus Dataset", "link": "https://arxiv.org/pdf/2410.00379", "details": "X Wang, F Wang, Y Li, Q Ma, S Wang, B Jiang, C Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "X-ray image-based medical report generation (MRG) is a pivotal area in artificial intelligence which can significantly reduce diagnostic burdens and patient wait times. Despite significant progress, we believe that the task has reached a bottleneck due \u2026"}]
