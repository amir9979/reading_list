'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Posterior concentrations of fully-connected Bayesian n'
[{"title": "A Framework for Variational Inference of Lightweight Bayesian Neural Networks with Heteroscedastic Uncertainties", "link": "https://arxiv.org/html/2402.14532v1", "details": "DJ Schodt, R Brown, M Merritt, S Park, D Menolascino\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Obtaining heteroscedastic predictive uncertainties from a Bayesian Neural Network (BNN) is vital to many applications. Often, heteroscedastic aleatoric uncertainties are learned as outputs of the BNN in addition to the predictive means, however doing so \u2026"}, {"title": "ConvTimeNet: A Deep Hierarchical Fully Convolutional Model for Multivariate Time Series Analysis", "link": "https://arxiv.org/pdf/2403.01493", "details": "M Cheng, J Yang, T Pan, Q Liu, Z Li - arXiv preprint arXiv:2403.01493, 2024", "abstract": "This paper introduces ConvTimeNet, a novel deep hierarchical fully convolutional network designed to serve as a general-purpose model for time series analysis. The key design of this network is twofold, designed to overcome the limitations of \u2026"}, {"title": "Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection", "link": "https://arxiv.org/html/2403.01485v1", "details": "S Dauncey, C Holmes, C Williams, F Falck - arXiv preprint arXiv:2403.01485, 2024", "abstract": "Likelihood-based deep generative models such as score-based diffusion models and variational autoencoders are state-of-the-art machine learning models approximating high-dimensional distributions of data such as images, text, or audio \u2026"}, {"title": "Variational Learning is Effective for Large Deep Networks", "link": "https://arxiv.org/html/2402.17641v1", "details": "Y Shen, N Daheim, B Cong, P Nickl, GM Marconi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms \u2026"}]
