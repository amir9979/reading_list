[{"title": "How to Train Long-Context Language Models (Effectively)", "link": "https://arxiv.org/pdf/2410.02660%3F", "details": "T Gao, A Wettig, H Yen, D Chen - arXiv preprint arXiv:2410.02660, 2024", "abstract": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development--Instead of perplexity or simple \u2026"}, {"title": "No Need to Talk: Asynchronous Mixture of Language Models", "link": "https://arxiv.org/pdf/2410.03529%3F", "details": "A Filippova, A Katharopoulos, D Grangier, R Collobert - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce SmallTalk LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each model of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth \u2026"}, {"title": "MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders", "link": "https://arxiv.org/pdf/2410.06845", "details": "C Li, M Fung, Q Wang, C Han, M Li, J Wang, H Ji - arXiv preprint arXiv:2410.06845, 2024", "abstract": "Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health \u2026"}, {"title": "On Unsupervised Prompt Learning for Classification with Black-box Language Models", "link": "https://arxiv.org/pdf/2410.03124", "details": "ZY Zhang, J Zhang, H Yao, G Niu, M Sugiyama - arXiv preprint arXiv:2410.03124, 2024", "abstract": "Large language models (LLMs) have achieved impressive success in text-formatted learning problems, and most popular LLMs have been deployed in a black-box fashion. Meanwhile, fine-tuning is usually necessary for a specific downstream task \u2026"}, {"title": "CriteriaMapper: establishing the automatic identification of clinical trial cohorts from electronic health records by matching normalized eligibility criteria and patient \u2026", "link": "https://www.nature.com/articles/s41598-024-77447-x", "details": "K Lee, Y Mai, Z Liu, K Raja, T Jun, M Ma, T Wang, L Ai\u2026 - Scientific Reports, 2024", "abstract": "The use of electronic health records (EHRs) holds the potential to enhance clinical trial activities. However, the identification of eligible patients within EHRs presents considerable challenges. We aimed to develop a CriteriaMapper system for \u2026"}, {"title": "Developing a Research Center for Artificial Intelligence in Medicine", "link": "https://www.mcpdigitalhealth.org/article/S2949-7612\\(24\\)00106-8/fulltext", "details": "CP Langlotz, J Kim, N Shah, MP Lungren, DB Larson\u2026 - Mayo Clinic Proceedings: Digital \u2026", "abstract": "Artificial intelligence (AI) and machine learning (ML) are driving innovation in the biosciences and are already affecting key elements of medical scholarship and clinical care. Many Schools of Medicine are capitalizing on the promise of these new \u2026"}, {"title": "ColaCare: Enhancing Electronic Health Record Modeling through Large Language Model-Driven Multi-Agent Collaboration", "link": "https://arxiv.org/pdf/2410.02551%3F", "details": "Z Wang, Y Zhu, H Zhao, X Zheng, T Wang, W Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce ColaCare, a framework that enhances Electronic Health Record (EHR) modeling through multi-agent collaboration driven by Large Language Models (LLMs). Our approach seamlessly integrates domain-specific expert models with \u2026"}, {"title": "Negative-Prompt-driven Alignment for Generative Language Model", "link": "https://arxiv.org/pdf/2410.12194", "details": "S Qiao, N Xv, B Liu, X Geng - arXiv preprint arXiv:2410.12194, 2024", "abstract": "Large language models have achieved remarkable capabilities, but aligning their outputs with human values and preferences remains a significant challenge. Existing alignment methods primarily focus on positive examples while overlooking the \u2026"}, {"title": "Scaling Parameter-Constrained Language Models with Quality Data", "link": "https://arxiv.org/pdf/2410.03083", "details": "E Chang, M Paltenghi, Y Li, PJ Lin, C Zhao, P Huber\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws in language modeling traditionally quantify training loss as a function of dataset size and model parameters, providing compute-optimal estimates but often neglecting the impact of data quality on model generalization. In this paper, we \u2026"}]
