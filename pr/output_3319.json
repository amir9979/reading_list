[{"title": "How structured are the representations in transformer-based vision encoders? An analysis of multi-object representations in vision-language models", "link": "https://arxiv.org/pdf/2406.09067", "details": "T Khajuria, BO Dias, J Aru - arXiv preprint arXiv:2406.09067, 2024", "abstract": "Forming and using symbol-like structured representations for reasoning has been considered essential for generalising over novel inputs. The primary tool that allows generalisation outside training data distribution is the ability to abstract away \u2026"}, {"title": "SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations", "link": "https://arxiv.org/pdf/2406.11171", "details": "SH Dumpala, A Jaiswal, C Sastry, E Milios, S Oore\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their remarkable successes, state-of-the-art large language models (LLMs), including vision-and-language models (VLMs) and unimodal language models (ULMs), fail to understand precise semantics. For example, semantically equivalent \u2026"}]
