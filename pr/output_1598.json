'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Enhancing Text-to-SQL Capabilities of Large Language M'
[{"title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "link": "https://arxiv.org/pdf/2405.00402", "details": "L Ranaldi, A Freitas - arXiv preprint arXiv:2405.00402, 2024", "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches \u2026"}, {"title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning", "link": "https://arxiv.org/pdf/2404.17140", "details": "Y Zhang, M Khalifa, L Logeswaran, J Kim, M Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors. This work explores whether \u2026"}, {"title": "Causal Evaluation of Language Models", "link": "https://arxiv.org/pdf/2405.00622", "details": "S Chen, B Peng, M Chen, R Wang, M Xu, X Zeng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for \u2026"}, {"title": "Path-Aware Cross-Attention Network for Question Answering", "link": "https://link.springer.com/chapter/10.1007/978-981-97-2253-2_9", "details": "Z Luo, Y Xiong, B Tang - Pacific-Asia Conference on Knowledge Discovery and \u2026, 2024", "abstract": "Abstract Reasoning is an essential ability in QA systems, and the integration of this ability into QA systems has been the subject of considerable research. A prevalent strategy involves incorporating domain knowledge graphs using Graph Neural \u2026"}, {"title": "Vision Language Models in Autonomous Driving: A Survey and Outlook", "link": "https://ieeexplore.ieee.org/iel7/7274857/7448921/10531702.pdf", "details": "X Zhou, M Liu, E Yurtsever, BL Zagar, W Zimmer\u2026 - IEEE Transactions on \u2026, 2024", "abstract": "The applications of Vision-Language Models (VLMs) in the field of Autonomous Driving (AD) have attracted widespread attention due to their outstanding performance and the ability to leverage Large Language Models (LLMs). By \u2026"}, {"title": "Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation", "link": "https://arxiv.org/pdf/2404.17489", "details": "W Cui, R Hosseinzadeh, J Ma, T Wu, Y Sui, K Golestan - arXiv preprint arXiv \u2026, 2024", "abstract": "Contrastive learning is a model pre-training technique by first creating similar views of the original data, and then encouraging the data and its corresponding views to be close in the embedding space. Contrastive learning has witnessed success in image \u2026"}, {"title": "Efficient and Accurate Contextual Re-Ranking for Knowledge Graph Question Answering", "link": "https://aclanthology.org/2024.lrec-main.496.pdf", "details": "K Sun, NP Jedema, K Sharma, R Janssen, J Pujara\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "The efficacy of neural \u201cretrieve and generate\u201d systems is well established for question answering (QA) over unstructured text. Recent efforts seek to extend this approach to knowledge graph (KG) QA by converting structured triples to \u2026"}, {"title": "JEMHopQA: Dataset for Japanese Explainable Multi-Hop Question Answering", "link": "https://aclanthology.org/2024.lrec-main.831.pdf", "details": "A Ishii, N Inoue, H Suzuki, S Sekine - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "We present JEMHopQA, a multi-hop QA dataset for the development of explainable QA systems. The dataset consists not only of question-answer pairs, but also of supporting evidence in the form of derivation triples, which contributes to making the \u2026"}, {"title": "Causal Diffusion Autoencoders: Toward Counterfactual Generation via Diffusion Probabilistic Models", "link": "https://arxiv.org/pdf/2404.17735", "details": "A Komanduri, C Zhao, F Chen, X Wu - arXiv preprint arXiv:2404.17735, 2024", "abstract": "Diffusion probabilistic models (DPMs) have become the state-of-the-art in high- quality image generation. However, DPMs have an arbitrary noisy latent space with no interpretable or controllable semantics. Although there has been significant \u2026"}]
