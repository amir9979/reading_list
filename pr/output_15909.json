[{"title": "Multimodal Data Fusion for Tabular and Textual Data: Zero-Shot, Few-Shot, and Fine-Tuning of Generative Pre-Trained Transformer Models", "link": "https://www.mdpi.com/2673-2688/6/4/72", "details": "S Jaradat, M Elhenawy, R Nayak, A Paz, HI Ashqar\u2026 - AI, 2025", "abstract": "In traffic safety analysis, previous research has often focused on tabular data or textual crash narratives in isolation, neglecting the potential benefits of a hybrid multimodal approach. This study introduces the Multimodal Data Fusion (MDF) \u2026"}, {"title": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items", "link": "https://arxiv.org/pdf/2504.11186", "details": "M Zou, S Srinivasan, TWS Lo, K Zou, GD Yang, X Ai\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in reasoning-focused large language models (LLMs) mark a shift from general LLMs toward models designed for complex decision-making, a crucial aspect in medicine. However, their performance in specialized domains like \u2026"}, {"title": "Benchmark for Measuring Intersectional Bias in Race and Gender of Large Language Models Using Synthetic Data", "link": "https://koreascience.kr/article/JAKO202511236003787.pdf", "details": "H Bae - The Transactions of the Korea Information Processing \u2026, 2025", "abstract": "Abstract Large Language Models (LLMs) are generative AI systems trained on vast datasets, capable of producing human-like text and widely used across industries. However, LLMs risk generating biased outputs by internalizing stereotypes related to \u2026"}]
