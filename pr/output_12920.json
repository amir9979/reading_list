[{"title": "MedS $^ 3$: Towards Medical Small Language Models with Self-Evolved Slow Thinking", "link": "https://arxiv.org/pdf/2501.12051%3F", "details": "S Jiang, Y Liao, Z Chen, Y Zhang, Y Wang, Y Wang - arXiv preprint arXiv:2501.12051, 2025", "abstract": "Medical language models (MLMs) have become pivotal in advancing medical natural language processing. However, prior models that rely on pre-training or supervised fine-tuning often exhibit low data efficiency and limited practicality in real \u2026"}, {"title": "An Experimental Study on Exploring Strong Lightweight Vision Transformers via Masked Image Modeling Pre-training", "link": "https://link.springer.com/article/10.1007/s11263-024-02327-w", "details": "J Gao, S Lin, S Wang, Y Kou, Z Li, L Li, C Zhang\u2026 - International Journal of \u2026, 2025", "abstract": "Masked image modeling (MIM) pre-training for large-scale vision transformers (ViTs) has enabled promising downstream performance on top of the learned self- supervised ViT features. In this paper, we question if the extremely simple lightweight \u2026"}]
