[{"title": "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "link": "https://aclanthology.org/2024.findings-emnlp.61.pdf", "details": "X Wei, H Chen, H Yu, H Fei, Q Liu - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have achieved notable success in commonsense reasoning tasks, benefiting from their extensive world knowledge acquired through extensive pretraining. While approaches like Chain-of-Thought \u2026"}, {"title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step", "link": "https://arxiv.org/pdf/2411.10440%3F", "details": "G Xu, P Jin, L Hao, Y Song, L Sun, L Yuan - arXiv preprint arXiv:2411.10440, 2024", "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to \u2026"}, {"title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models", "link": "https://openreview.net/pdf%3Fid%3DLC1QAqhePv", "details": "D Zhang, Z Hu, S Zhoubian, Z Du, K Yang, Z Wang\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving \u2026"}, {"title": "LM2: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.920.pdf", "details": "G Juneja, S Dutta, T Chakraborty - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}, {"title": "Frozen Large-scale Pretrained Vision-Language Models are the Effective Foundational Backbone for Multimodal Breast Cancer Prediction", "link": "https://ieeexplore.ieee.org/iel8/6221020/6363502/10769012.pdf", "details": "HQ Vo, L Wang, KK Wong, CF Ezeana, X Yu, W Yang\u2026 - IEEE Journal of Biomedical \u2026, 2024", "abstract": "Breast cancer is a pervasive global health concern among women. Leveraging multimodal data from enterprise patient databases-including Picture Archiving and Communication Systems (PACS) and Electronic Health Records (EHRs)-holds \u2026"}, {"title": "2-Factor Retrieval for Improved Human-AI Decision Making in Radiology", "link": "https://arxiv.org/pdf/2412.00372", "details": "J Solomon, L Jalilian, A Vilesov, M Mathew, T Grogan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Human-machine teaming in medical AI requires us to understand to what degree a trained clinician should weigh AI predictions. While previous work has shown the potential of AI assistance at improving clinical predictions, existing clinical decision \u2026"}, {"title": "Multi-Hop Interpretable Meta Learning for Few-Shot Temporal Knowledge Graph Completion", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024009109", "details": "L Bai, S Han, L Zhu - Neural Networks, 2024", "abstract": "Multi-hop path completion is a key part of temporal knowledge graph completion, which aims to infer complex relationships and obtain interpretable completion results. However, the traditional multi-hop path completion models mainly focus on \u2026"}, {"title": "Measuring Non-Adversarial Reproduction of Training Data in Large Language Models", "link": "https://arxiv.org/pdf/2411.10242%3F", "details": "M Aerni, J Rando, E Debenedetti, N Carlini, D Ippolito\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models memorize parts of their training data. Memorizing short snippets and facts is required to answer questions about the world and to be fluent in any language. But models have also been shown to reproduce long verbatim \u2026"}, {"title": "ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large Language Models for Code Generation", "link": "https://arxiv.org/pdf/2411.07112", "details": "X Jiang, Y Dong, Y Tao, H Liu, Z Jin, W Jiao, G Li - arXiv preprint arXiv:2411.07112, 2024", "abstract": "Large language models (LLMs) have achieved impressive performance in code generation recently, offering programmers revolutionary assistance in software development. However, due to the auto-regressive nature of LLMs, they are \u2026"}]
