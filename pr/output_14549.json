[{"title": "Boosting the Generalization and Reasoning of Vision Language Models with Curriculum Reinforcement Learning", "link": "https://arxiv.org/pdf/2503.07065", "details": "H Deng, D Zou, R Ma, H Luo, Y Cao, Y Kang - arXiv preprint arXiv:2503.07065, 2025", "abstract": "While state-of-the-art vision-language models (VLMs) have demonstrated remarkable capabilities in complex visual-text tasks, their success heavily relies on massive model scaling, limiting their practical deployment. Small-scale VLMs offer a \u2026"}, {"title": "From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.06260", "details": "M Dai, J Sun, Z Zhao, S Liu, R Li, J Gao, X Li - arXiv preprint arXiv:2503.06260, 2025", "abstract": "Aligning large vision-language models (LVLMs) with human preferences is challenging due to the scarcity of fine-grained, high-quality, and multimodal preference data without human annotations. Existing methods relying on direct \u2026"}, {"title": "scRCA: A Siamese network-based pipeline for annotating cell types using noisy single-cell RNA-seq reference data", "link": "https://www.sciencedirect.com/science/article/pii/S0010482525004196", "details": "Y Liu, C Li, LC Shen, H Yan, G Wei, RB Gasser, X Hu\u2026 - Computers in Biology and \u2026, 2025", "abstract": "Accurate cell type annotation is fundamentally critical for single-cell sequencing (scRNA-seq) data analysis to provide insightful knowledge of tissue-specific cell heterogeneity and cell state transition tracking. Cell type annotation is usually \u2026"}, {"title": "Combinatorial Optimization via LLM-driven Iterated Fine-tuning", "link": "https://arxiv.org/pdf/2503.06917", "details": "P Awasthi, S Gollapudi, R Kumar, K Munagala - arXiv preprint arXiv:2503.06917, 2025", "abstract": "We present a novel way to integrate flexible, context-dependent constraints into combinatorial optimization by leveraging Large Language Models (LLMs) alongside traditional algorithms. Although LLMs excel at interpreting nuanced, locally specified \u2026"}, {"title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding", "link": "https://arxiv.org/pdf/2503.05061", "details": "M Krumdick, C Lovering, V Reddy, S Ebner, C Tanner - arXiv preprint arXiv \u2026, 2025", "abstract": "LLM-as-a-Judge is a framework that uses an LLM (large language model) to evaluate the quality of natural language text-typically text that is also generated by an LLM. This framework holds great promise due to its relative low-cost, ease of use \u2026"}, {"title": "A Weighted Cross-entropy Loss for Mitigating LLM Hallucinations in Cross-lingual Continual Pretraining", "link": "https://ieeexplore.ieee.org/abstract/document/10888877/", "details": "Y Fan, R Li, G Zhang, C Shi, X Wang - \u2026 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "Recently, due to the explosive advances of large language models (LLMs) on English, cross-lingual continual pretraining has been widely applied in obtaining Chinese LLMs. However, previous studies showed that these LLMs have suffered \u2026"}, {"title": "Memorize or Generalize? Evaluating LLM Code Generation with Evolved Questions", "link": "https://arxiv.org/pdf/2503.02296%3F", "details": "W Chen, L Zhang, L Zhong, L Peng, Z Wang, J Shang - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) are known to exhibit a memorization phenomenon in code generation: instead of truly understanding the underlying principles of a programming problem, they tend to memorize the original prompt and its solution \u2026"}, {"title": "Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching", "link": "https://arxiv.org/pdf/2503.05179%3F", "details": "SA Aytes, J Baek, SJ Hwang - arXiv preprint arXiv:2503.05179, 2025", "abstract": "Recent advances in large language models have demonstrated remarkable reasoning capabilities through Chain of Thought (CoT) prompting, but often at the cost of excessive verbosity in their intermediate outputs, which increases \u2026"}, {"title": "A survey on post-training of large language models", "link": "https://arxiv.org/pdf/2503.06072", "details": "G Tie, Z Zhao, D Song, F Wei, R Zhou, Y Dai, W Yin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained \u2026"}]
