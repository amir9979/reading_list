[{"title": "e-Health CSIRO at RRG24: Entropy-Augmented Self-Critical Sequence Training for Radiology Report Generation", "link": "https://arxiv.org/pdf/2408.03500", "details": "A Nicolson, J Liu, J Dowling, A Nguyen, B Koopman - arXiv preprint arXiv:2408.03500, 2024", "abstract": "The Shared Task on Large-Scale Radiology Report Generation (RRG24) aims to expedite the development of assistive systems for interpreting and reporting on chest X-ray (CXR) images. This task challenges participants to develop models that \u2026"}, {"title": "Skip\\n: A simple method to reduce hallucination in large vision-language models", "link": "https://oar.a-star.edu.sg/storage/d/d66g61dkp7/nn-bias-in-visual-language-models-camera-ready.pdf", "details": "Z Han, Z Bai, H Mei, Q Xu, C Zhang, MZ Shou - arXiv preprint arXiv:2402.01345, 2024", "abstract": "Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination \u2026"}, {"title": "Open (Clinical) LLMs are Sensitive to Instruction Phrasings", "link": "https://aclanthology.org/2024.bionlp-1.5.pdf", "details": "AM Ceballos-Arroyo, M Munnangi, J Sun, K Zhang\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "Abstract Instruction-tuned Large Language Models (LLMs) can perform a wide range of tasks given natural language instructions to do so, but they are sensitive to how such instructions are phrased. This issue is especially concerning in healthcare, as \u2026"}, {"title": "iHealth-Chile-3&2 at RRG24: Template Based Report Generation", "link": "https://aclanthology.org/2024.bionlp-1.53.pdf", "details": "O Loch, P Messina, R Elberg, D Campanini, \u00c1 Soto\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "This paper presents the approaches of the iHealth-Chile-3 and iHealth-Chile-2 teams for the shared task of Large-Scale Radiology Report Generation at the BioNLP workshop. Inspired by prior work on template-based report generation, both teams \u2026"}, {"title": "iHealth-Chile-1 at RRG24: In-context Learning and Finetuning of a Large Multimodal Model for Radiology Report Generation", "link": "https://aclanthology.org/2024.bionlp-1.52.pdf", "details": "D Campanini, O Loch, P Messina, R Elberg, D Parra - Proceedings of the 23rd \u2026, 2024", "abstract": "This paper presents the approach of the iHealth-Chile-1 team for the shared task of Large-Scale Radiology Report Generation at the BioNLP workshop, inspired by progress in large multimodal models for processing images and text. In this work, we \u2026"}, {"title": "EPFL-MAKE at \u201cDischarge Me!\u201d: An LLM System for Automatically Generating Discharge Summaries of Clinical Electronic Health Record", "link": "https://aclanthology.org/2024.bionlp-1.61.pdf", "details": "H Wu, P Boulenger, A Faure, B C\u00e9spedes, F Boukil\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "This paper presents our contribution to the Streamlining Discharge Documentation shared task organized as part of the ACL'24 workshop. We propose MEDISCHARGE (Meditron-7B Based Medical Summary Generation System for Discharge Me), an \u2026"}, {"title": "Addressing Model and Data Heterogeneity in Multimodal Large Language Model Training", "link": "https://arxiv.org/pdf/2408.04275", "details": "Z Zhang, Y Zhong, R Ming, H Hu, J Sun, Z Ge, Y Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal large language models (LLMs) have demonstrated significant potential in a wide range of AI applications. Yet, training multimodal LLMs suffers from low efficiency and scalability, due to the inherent model heterogeneity and data \u2026"}, {"title": "Do Clinicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation", "link": "https://aclanthology.org/2024.bionlp-1.15.pdf", "details": "Z Yao, A Jaafar, B Wang, Z Yang, H Yu - Proceedings of the 23rd Workshop on \u2026, 2024", "abstract": "This study examines the effect of prompt engineering on the performance of Large Language Models (LLMs) in clinical note generation. We introduce an Automatic Prompt Optimization (APO) framework to refine initial prompts and compare the \u2026"}, {"title": "Understanding and Modeling Job Marketplace with Pretrained Language Models", "link": "https://arxiv.org/pdf/2408.04381", "details": "Y Zhu, L Wu, B Zhang, S Wang, Q Guo, L Hong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Job marketplace is a heterogeneous graph composed of interactions among members (job-seekers), companies, and jobs. Understanding and modeling job marketplace can benefit both job seekers and employers, ultimately contributing to \u2026"}]
