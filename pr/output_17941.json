[{"title": "Flexible Realignment of Language Models", "link": "https://arxiv.org/pdf/2506.12704", "details": "W Zhu, R Xie, W Zhang, R Wang - arXiv preprint arXiv:2506.12704, 2025", "abstract": "Realignment becomes necessary when a language model (LM) fails to meet expected performance. We propose a flexible realignment framework that supports quantitative control of alignment degree during training and inference. This \u2026", "entry_id": "http://arxiv.org/abs/2506.12704v1", "updated": "2025-06-15 03:26:59", "published": "2025-06-15 03:26:59", "authors": "Wenhong Zhu;Ruobing Xie;Weinan Zhang;Rui Wang", "summary": "Realignment becomes necessary when a language model (LM) fails to meet\nexpected performance. We propose a flexible realignment framework that supports\nquantitative control of alignment degree during training and inference. This\nframework incorporates Training-time Realignment (TrRa), which efficiently\nrealigns the reference model by leveraging the controllable fusion of logits\nfrom both the reference and already aligned models. For example, TrRa reduces\ntoken usage by 54.63% on DeepSeek-R1-Distill-Qwen-1.5B without any performance\ndegradation, outperforming DeepScaleR-1.5B's 33.86%. To complement TrRa during\ninference, we introduce a layer adapter that enables smooth Inference-time\nRealignment (InRa). This adapter is initialized to perform an identity\ntransformation at the bottom layer and is inserted preceding the original\nlayers. During inference, input embeddings are simultaneously processed by the\nadapter and the original layer, followed by the remaining layers, and then\ncontrollably interpolated at the logit level. We upgraded\nDeepSeek-R1-Distill-Qwen-7B from a slow-thinking model to one that supports\nboth fast and slow thinking, allowing flexible alignment control even during\ninference. By encouraging deeper reasoning, it even surpassed its original\nperformance.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.12704v1;http://arxiv.org/pdf/2506.12704v1", "pdf_url": "http://arxiv.org/pdf/2506.12704v1"}, {"title": "Improving Diversity in Language Models: When Temperature Fails, Change the Loss", "link": "https://openreview.net/pdf%3Fid%3DRsyMfsqzeG", "details": "A Verine, F Le Bronnec, K Zheng, A Allauzen\u2026 - Forty-second International \u2026", "abstract": "Increasing diversity in language models is a challenging yet essential objective. A common approach is to raise the decoding temperature. In this work, we investigate this approach through a simplistic yet common case to provide insights into why \u2026"}, {"title": "Assessing the Role of Data Quality in Training Bilingual Language Models", "link": "https://arxiv.org/pdf/2506.12966", "details": "S Seto, M ter Hoeve, M de Seyssel, D Grangier - arXiv preprint arXiv:2506.12966, 2025", "abstract": "Bilingual and multilingual language models offer a promising path toward scaling NLP systems across diverse languages and users. However, their performance often varies wildly between languages as prior works show that adding more languages \u2026", "entry_id": "http://arxiv.org/abs/2506.12966v1", "updated": "2025-06-15 21:08:51", "published": "2025-06-15 21:08:51", "authors": "Skyler Seto;Maartje ter Hoeve;Maureen de Seyssel;David Grangier", "summary": "Bilingual and multilingual language models offer a promising path toward\nscaling NLP systems across diverse languages and users. However, their\nperformance often varies wildly between languages as prior works show that\nadding more languages can degrade performance for some languages (such as\nEnglish), while improving others (typically more data constrained languages).\nIn this work, we investigate causes of these inconsistencies by comparing\nbilingual and monolingual language models. Our analysis reveals that unequal\ndata quality, not just data quantity, is a major driver of performance\ndegradation in bilingual settings. We propose a simple yet effective data\nfiltering strategy to select higher-quality bilingual training data with only\nhigh quality English data. Applied to French, German, and Chinese, our approach\nimproves monolingual performance by 2-4% and reduces bilingual model\nperformance gaps to 1%. These results highlight the overlooked importance of\ndata quality in multilingual pretraining and offer a practical recipe for\nbalancing performance.", "comment": "26 pages, 18 figures, 25 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.12966v1;http://arxiv.org/pdf/2506.12966v1", "pdf_url": "http://arxiv.org/pdf/2506.12966v1"}, {"title": "Mercury: Ultra-Fast Language Models Based on Diffusion", "link": "https://arxiv.org/pdf/2506.17298", "details": "I Labs, S Khanna, S Kharbanda, S Li, H Varma\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present Mercury, a new generation of commercial-scale large language models (LLMs) based on diffusion. These models are parameterized via the Transformer architecture and trained to predict multiple tokens in parallel. In this report, we detail \u2026", "entry_id": "http://arxiv.org/abs/2506.17298v1", "updated": "2025-06-17 17:06:18", "published": "2025-06-17 17:06:18", "authors": "Inception Labs;Samar Khanna;Siddhant Kharbanda;Shufan Li;Harshit Varma;Eric Wang;Sawyer Birnbaum;Ziyang Luo;Yanis Miraoui;Akash Palrecha;Stefano Ermon;Aditya Grover;Volodymyr Kuleshov", "summary": "We present Mercury, a new generation of commercial-scale large language\nmodels (LLMs) based on diffusion. These models are parameterized via the\nTransformer architecture and trained to predict multiple tokens in parallel. In\nthis report, we detail Mercury Coder, our first set of diffusion LLMs designed\nfor coding applications. Currently, Mercury Coder comes in two sizes: Mini and\nSmall. These models set a new state-of-the-art on the speed-quality frontier.\nBased on independent evaluations conducted by Artificial Analysis, Mercury\nCoder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109\ntokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform\nspeed-optimized frontier models by up to 10x on average while maintaining\ncomparable quality. We discuss additional results on a variety of code\nbenchmarks spanning multiple languages and use-cases as well as real-world\nvalidation by developers on Copilot Arena, where the model currently ranks\nsecond on quality and is the fastest model overall. We also release a public\nAPI at https://platform.inceptionlabs.ai/ and free playground at\nhttps://chat.inceptionlabs.ai", "comment": "15 pages; equal core, cross-function, senior authors listed\n  alphabetically", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2506.17298v1;http://arxiv.org/pdf/2506.17298v1", "pdf_url": "http://arxiv.org/pdf/2506.17298v1"}, {"title": "Improving Rationality in the Reasoning Process of Language Models through Self-playing Game", "link": "https://openreview.net/pdf%3Fid%3DPPsiS5nSlv", "details": "P Wang, J Li, Z Tang, H Gui - Forty-second International Conference on Machine \u2026", "abstract": "Large language models (LLMs) have demonstrated considerable reasoning abilities in various tasks such as mathematics and coding. However, recent studies indicate that even the best models lack true comprehension of their reasoning processes. In \u2026"}, {"title": "SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models", "link": "https://arxiv.org/pdf/2506.12935", "details": "X Diao, C Zhang, K Kong, W Wu, C Ma, Z Ouyang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While large language models have shown reasoning capabilities, their application to the audio modality, particularly in large audio-language models (ALMs), remains significantly underdeveloped. Addressing this gap requires a systematic approach \u2026", "entry_id": "http://arxiv.org/abs/2506.12935v1", "updated": "2025-06-15 18:26:08", "published": "2025-06-15 18:26:08", "authors": "Xingjian Diao;Chunhui Zhang;Keyi Kong;Weiyi Wu;Chiyu Ma;Zhongyu Ouyang;Peijun Qing;Soroush Vosoughi;Jiang Gui", "summary": "While large language models have shown reasoning capabilities, their\napplication to the audio modality, particularly in large audio-language models\n(ALMs), remains significantly underdeveloped. Addressing this gap requires a\nsystematic approach, involving a capable base model, high-quality\nreasoning-oriented audio data, and effective training algorithms. In this\nstudy, we present a comprehensive solution: we introduce the Audio Logical\nReasoning (ALR) dataset, consisting of 6,446 text-audio annotated samples\nspecifically designed for complex reasoning tasks. Building on this resource,\nwe propose SoundMind, a rule-based reinforcement learning (RL) algorithm\ntailored to endow ALMs with deep bimodal reasoning abilities. By training\nQwen2.5-Omni-7B on the ALR dataset using SoundMind, our approach achieves\nstate-of-the-art performance in audio logical reasoning. This work highlights\nthe impact of combining high-quality, reasoning-focused datasets with\nspecialized RL techniques, advancing the frontier of auditory intelligence in\nlanguage models. Our code and the proposed dataset are available at\nhttps://github.com/xid32/SoundMind.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.MM;cs.SD;eess.AS", "links": "http://arxiv.org/abs/2506.12935v1;http://arxiv.org/pdf/2506.12935v1", "pdf_url": "http://arxiv.org/pdf/2506.12935v1"}, {"title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation", "link": "https://arxiv.org/pdf/2506.19816", "details": "H Li, S Yang, Y Chen, Y Tian, X Yang, X Chen, H Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent vision-language-action (VLA) models built on pretrained vision-language models (VLMs) have demonstrated strong generalization across manipulation tasks. However, they remain constrained by a single-frame observation paradigm and \u2026", "entry_id": "http://arxiv.org/abs/2506.19816v1", "updated": "2025-06-24 17:30:27", "published": "2025-06-24 17:30:27", "authors": "Hao Li;Shuai Yang;Yilun Chen;Yang Tian;Xiaoda Yang;Xinyi Chen;Hanqing Wang;Tai Wang;Feng Zhao;Dahua Lin;Jiangmiao Pang", "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.", "comment": "36 pages, 21 figures", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.CV", "links": "http://arxiv.org/abs/2506.19816v1;http://arxiv.org/pdf/2506.19816v1", "pdf_url": "http://arxiv.org/pdf/2506.19816v1"}, {"title": "How much do language models memorize?", "link": "https://arxiv.org/pdf/2505.24832", "details": "JX Morris, C Sitawarin, C Guo, N Kokhlikyan, GE Suh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We propose a new method for estimating how much a model``knows''about a datapoint and use it to measure the capacity of modern language models. Prior studies of language model memorization have struggled to disentangle \u2026", "entry_id": "http://arxiv.org/abs/2505.24832v3", "updated": "2025-06-18 15:27:03", "published": "2025-05-30 17:34:03", "authors": "John X. Morris;Chawin Sitawarin;Chuan Guo;Narine Kokhlikyan;G. Edward Suh;Alexander M. Rush;Kamalika Chaudhuri;Saeed Mahloujifar", "summary": "We propose a new method for estimating how much a model knows about a\ndatapoint and use it to measure the capacity of modern language models. Prior\nstudies of language model memorization have struggled to disentangle\nmemorization from generalization. We formally separate memorization into two\ncomponents: unintended memorization, the information a model contains about a\nspecific dataset, and generalization, the information a model contains about\nthe true data-generation process. When we completely eliminate generalization,\nwe can compute the total memorization, which provides an estimate of model\ncapacity: our measurements estimate that GPT-style models have a capacity of\napproximately 3.6 bits per parameter. We train language models on datasets of\nincreasing size and observe that models memorize until their capacity fills, at\nwhich point \"grokking\" begins, and unintended memorization decreases as models\nbegin to generalize. We train hundreds of transformer language models ranging\nfrom $500K$ to $1.5B$ parameters and produce a series of scaling laws relating\nmodel capacity and data size to membership inference.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.24832v3;http://arxiv.org/pdf/2505.24832v3", "pdf_url": "http://arxiv.org/pdf/2505.24832v3"}, {"title": "Self-Adapting Language Models", "link": "https://arxiv.org/pdf/2506.10943", "details": "A Zweiger, J Pari, H Guo, E Aky\u00fcrek, Y Kim, P Agrawal - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by \u2026", "entry_id": "http://arxiv.org/abs/2506.10943v1", "updated": "2025-06-12 17:48:13", "published": "2025-06-12 17:48:13", "authors": "Adam Zweiger;Jyothish Pari;Han Guo;Ekin Aky\u00fcrek;Yoon Kim;Pulkit Agrawal", "summary": "Large language models (LLMs) are powerful but static; they lack mechanisms to\nadapt their weights in response to new tasks, knowledge, or examples. We\nintroduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to\nself-adapt by generating their own finetuning data and update directives. Given\na new input, the model produces a self-edit-a generation that may restructure\nthe information in different ways, specify optimization hyperparameters, or\ninvoke tools for data augmentation and gradient-based updates. Through\nsupervised finetuning (SFT), these self-edits result in persistent weight\nupdates, enabling lasting adaptation. To train the model to produce effective\nself-edits, we use a reinforcement learning loop with the downstream\nperformance of the updated model as the reward signal. Unlike prior approaches\nthat rely on separate adaptation modules or auxiliary networks, SEAL directly\nuses the model's own generation to control its adaptation process. Experiments\non knowledge incorporation and few-shot generalization show that SEAL is a\npromising step toward language models capable of self-directed adaptation. Our\nwebsite and code is available at https://jyopari.github.io/posts/seal.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2506.10943v1;http://arxiv.org/pdf/2506.10943v1", "pdf_url": "http://arxiv.org/pdf/2506.10943v1"}]
