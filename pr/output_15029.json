[{"title": "Poisson-Process Topic Model for Integrating Knowledge from Pre-trained Language Models", "link": "https://arxiv.org/pdf/2503.17809%3F", "details": "M Austern, Y Guo, ZT Ke, T Liu - arXiv preprint arXiv:2503.17809, 2025", "abstract": "Topic modeling is traditionally applied to word counts without accounting for the context in which words appear. Recent advancements in large language models (LLMs) offer contextualized word embeddings, which capture deeper meaning and \u2026"}, {"title": "Optimizing translation for low-resource languages: Efficient fine-tuning with custom prompt engineering in large language models", "link": "https://www.sciencedirect.com/science/article/pii/S2666827025000325", "details": "PW Khoboko, V Marivate, J Sefara - Machine Learning with Applications, 2025", "abstract": "Training large language models (LLMs) can be prohibitively expensive. However, the emergence of new Parameter-Efficient Fine-Tuning (PEFT) strategies provides a cost-effective approach to unlocking the potential of LLMs across a variety of natural \u2026"}, {"title": "Window Token Concatenation for Efficient Visual Large Language Models", "link": "https://arxiv.org/pdf/2504.04024", "details": "Y Li, W Bao, B Ye, Z Tan, T Chen, H Liu, Y Kong - arXiv preprint arXiv:2504.04024, 2025", "abstract": "To effectively reduce the visual tokens in Visual Large Language Models (VLLMs), we propose a novel approach called Window Token Concatenation (WiCo). Specifically, we employ a sliding window to concatenate spatially adjacent visual \u2026"}]
