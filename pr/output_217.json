'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Foresightâ€”a generative pretrained transformer for mod'
[{"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models", "link": "https://arxiv.org/pdf/2403.02436", "details": "X Lu, Y Zhao, B Qin - arXiv preprint arXiv:2403.02436, 2024", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Language models scale reliably with over-training and on downstream tasks", "link": "https://arxiv.org/pdf/2403.08540", "details": "SY Gadre, G Smyrnis, V Shankar, S Gururangan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute \u2026"}, {"title": "Debiasing Large Visual Language Models", "link": "https://arxiv.org/pdf/2403.05262", "details": "YF Zhang, W Yu, Q Wen, X Wang, Z Zhang, L Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the realms of computer vision and natural language processing, Large Vision- Language Models (LVLMs) have become indispensable tools, proficient in generating textual descriptions based on visual inputs. Despite their advancements \u2026"}, {"title": "Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models", "link": "https://arxiv.org/html/2402.19014v1", "details": "X Li, Y Wu, X Jiang, Z Guo, M Gong, H Cao, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, the advent of Large Visual-Language Models (LVLMs) has received increasing attention across various domains, particularly in the field of visual document understanding (VDU). Different from conventional vision-language tasks \u2026"}, {"title": "Uncertainty-Aware Evaluation for Vision-Language Models", "link": "https://arxiv.org/pdf/2402.14418", "details": "V Kostumov, B Nutfullin, O Pilipenko, E Ilyushin - arXiv preprint arXiv:2402.14418, 2024", "abstract": "Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty \u2026"}, {"title": "Merino: Entropy-driven Design for Generative Language Models on IoT Devices", "link": "https://arxiv.org/html/2403.07921v1", "details": "Y Zhao, M Lin, H Tang, Q Wu, J Wang - arXiv preprint arXiv:2403.07921, 2024", "abstract": "Generative Large Language Models (LLMs) stand as a revolutionary advancement in the modern era of artificial intelligence (AI). However, directly deploying LLMs in resource-constrained hardware, such as Internet-of-Things (IoT) devices, is difficult \u2026"}, {"title": "Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning", "link": "https://arxiv.org/pdf/2403.03864", "details": "D Ghosal, VTY Han, CY Ken, S Poria - arXiv preprint arXiv:2403.03864, 2024", "abstract": "This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models \u2026"}, {"title": "Enhancing Data Quality in Federated Fine-Tuning of Foundation Models", "link": "https://arxiv.org/pdf/2403.04529", "details": "W Zhao, Y Du, ND Lane, S Chen, Y Wang - arXiv preprint arXiv:2403.04529, 2024", "abstract": "In the current landscape of foundation model training, there is a significant reliance on public domain data, which is nearing exhaustion according to recent research. To further scale up, it is crucial to incorporate collaboration among multiple specialized \u2026"}, {"title": "Cross-Dimension Attentive Feature Fusion Network for Unsupervised Time-Series Anomaly Detection.", "link": "https://cdn.techscience.cn/files/CMES/2024/TSP_CMES-139-3/TSP_CMES_47065/TSP_CMES_47065.pdf", "details": "R Wang, Y Zhou, G Luo, P Chen, D Peng - CMES-Computer Modeling in Engineering \u2026, 2024", "abstract": "Time series anomaly detection is crucial in various industrial applications to identify unusual behaviors within the time series data. Due to the challenges associated with annotating anomaly events, time series reconstruction has become a prevalent \u2026"}]
