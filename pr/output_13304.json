[{"title": "Unsupervised Topic Models are Data Mixers for Pre-training Language Models", "link": "https://arxiv.org/pdf/2502.16802", "details": "J Peng, X Zhuang, Q Jiantao, R Ma, J Yu, T Bai, C He - arXiv preprint arXiv \u2026, 2025", "abstract": "The performance of large language models (LLMs) is significantly affected by the quality and composition of their pre-training data, which is inherently diverse, spanning various domains, sources, and topics. Effectively integrating these \u2026"}, {"title": "EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models", "link": "https://arxiv.org/pdf/2502.06663", "details": "X Xing, Z Liu, S Xiao, B Gao, Y Liang, W Zhang, H Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Modern large language models (LLMs) driven by scaling laws, achieve intelligence emergency in large model sizes. Recently, the increasing concerns about cloud costs, latency, and privacy make it an urgent requirement to develop compact edge \u2026"}, {"title": "How Much Do Code Language Models Remember? An Investigation on Data Extraction Attacks before and after Fine-tuning", "link": "https://arxiv.org/pdf/2501.17501", "details": "F Salerno, A Al-Kaswan, M Izadi - arXiv preprint arXiv:2501.17501, 2025", "abstract": "Code language models, while widely popular, are often trained on unsanitized source code gathered from across the Internet. Previous work revealed that pre- trained models can remember the content of their training data and regurgitate them \u2026"}, {"title": "Large language models for scientific discovery in molecular property prediction", "link": "https://www.nature.com/articles/s42256-025-00994-z", "details": "Y Zheng, HY Koh, J Ju, ATN Nguyen, LT May, GI Webb\u2026 - Nature Machine Intelligence, 2025", "abstract": "Large language models (LLMs) are a form of artificial intelligence system encapsulating vast knowledge in the form of natural language. These systems are adept at numerous complex tasks including creative writing, storytelling, translation \u2026"}, {"title": "Anatomical grounding pre-training for medical phrase grounding", "link": "https://arxiv.org/pdf/2502.16585", "details": "W Zhang, S Chandra, A Nicolson - arXiv preprint arXiv:2502.16585, 2025", "abstract": "Medical Phrase Grounding (MPG) maps radiological findings described in medical reports to specific regions in medical images. The primary obstacle hindering progress in MPG is the scarcity of annotated data available for training and \u2026"}, {"title": "Forecasting Rare Language Model Behaviors", "link": "https://arxiv.org/pdf/2502.16797", "details": "E Jones, M Tong, J Mu, M Mahfoud, J Leike, R Grosse\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Standard language model evaluations can fail to capture risks that emerge only at deployment scale. For example, a model may produce safe responses during a small- scale beta test, yet reveal dangerous information when processing billions of \u2026"}, {"title": "Large Language Models are Powerful EHR Encoders", "link": "https://arxiv.org/pdf/2502.17403", "details": "S Hegselmann, G von Arnim, T Rheude, N Kronenberg\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific EHR foundation models trained on \u2026"}, {"title": "PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation", "link": "https://arxiv.org/pdf/2502.15857", "details": "T Fan, G Ma, Y Song, L Fan, K Chen, Q Yang - arXiv preprint arXiv:2502.15857, 2025", "abstract": "Compressing Large Language Models (LLMs) into task-specific Small Language Models (SLMs) encounters two significant challenges: safeguarding domain-specific knowledge privacy and managing limited resources. To tackle these challenges, we \u2026"}]
