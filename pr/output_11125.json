[{"title": "HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.08378", "details": "S Zhu, W Dong, J Song, Y Guo, B Zheng - arXiv preprint arXiv:2412.08378, 2024", "abstract": "Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into \u2026"}, {"title": "CPLLM: Clinical prediction with large language models", "link": "https://journals.plos.org/digitalhealth/article%3Fid%3D10.1371/journal.pdig.0000680", "details": "O Ben Shoham, N Rappoport - PLOS Digital Health, 2024", "abstract": "We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for predicting clinical disease and readmission. We utilized quantization and fine-tuned the LLM using \u2026"}, {"title": "Copyright-Protected Language Generation via Adaptive Model Fusion", "link": "https://arxiv.org/pdf/2412.06619%3F", "details": "J Abad, K Donhauser, F Pinto, F Yang - arXiv preprint arXiv:2412.06619, 2024", "abstract": "The risk of language models reproducing copyrighted material from their training data has led to the development of various protective measures. Among these, inference-time strategies that impose constraints via post-processing have shown \u2026"}, {"title": "Hybrid-LLM-GNN: integrating large language models and graph neural networks for enhanced materials property prediction", "link": "https://pubs.rsc.org/en/content/articlepdf/2024/dd/d4dd00199k", "details": "Y Li, V Gupta, MNT Kilic, K Choudhary, D Wines\u2026 - Digital Discovery, 2024", "abstract": "Graph-centric learning has attracted significant interest in materials informatics. Accordingly, a family of graph-based machine learning models, primarily utilizing Graph Neural Networks (GNN), has been developed to provide accurate prediction \u2026"}, {"title": "Training large language models to reason in a continuous latent space", "link": "https://arxiv.org/pdf/2412.06769%3F", "details": "S Hao, S Sukhbaatar, DJ Su, X Li, Z Hu, J Weston\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) are restricted to reason in the\" language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may \u2026"}, {"title": "Efficient and Context-Aware Label Propagation for Zero-/Few-Shot Training-Free Adaptation of Vision-Language Model", "link": "https://arxiv.org/pdf/2412.18303", "details": "Y Li, Y Su, A Goodge, K Jia, X Xu - arXiv preprint arXiv:2412.18303, 2024", "abstract": "Vision-language models (VLMs) have revolutionized machine learning by leveraging large pre-trained models to tackle various downstream tasks. Despite improvements in label, training, and data efficiency, many state-of-the-art VLMs still require task \u2026"}, {"title": "Microbial general model: Leveraging large language model for contextualized microbiome analysis", "link": "https://www.biorxiv.org/content/10.1101/2024.12.30.630825.full.pdf", "details": "H Zhang, Z Kang, Y Zhang, L Song, K Ning, R Yang - bioRxiv, 2025", "abstract": "Microbial communities significantly impact medicine, biotechnology, and agriculture. Advanced sequencing technologies have generated extensive microbiome data, enabling the discovery of substantial evolutionary and ecological patterns. However \u2026"}, {"title": "Exploring Multi-Grained Concept Annotations for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2412.05939", "details": "X Xu, T Niu, Y Xie, L Qin, W Che, MY Kan - arXiv preprint arXiv:2412.05939, 2024", "abstract": "Multimodal Large Language Models (MLLMs) excel in vision--language tasks by pre- training solely on coarse-grained concept annotations (eg, image captions). We hypothesize that integrating fine-grained concept annotations (eg, object labels and \u2026"}, {"title": "QAPyramid: Fine-grained Evaluation of Content Selection for Text Summarization", "link": "https://arxiv.org/pdf/2412.07096", "details": "S Zhang, D Wan, A Cattan, A Klein, I Dagan, M Bansal - arXiv preprint arXiv \u2026, 2024", "abstract": "How to properly conduct human evaluations for text summarization is a longstanding challenge. The Pyramid human evaluation protocol, which assesses content selection by breaking the reference summary into sub-units and verifying their \u2026"}]
