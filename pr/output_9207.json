[{"title": "Can Language Models Learn to Skip Steps?", "link": "https://arxiv.org/pdf/2411.01855%3F", "details": "T Liu, Q Guo, X Hu, C Jiayang, Y Zhang, X Qiu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Trained on vast corpora of human language, language models demonstrate emergent human-like reasoning abilities. Yet they are still far from true intelligence, which opens up intriguing opportunities to explore the parallels of humans and \u2026"}, {"title": "LM2: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.920.pdf", "details": "G Juneja, S Dutta, T Chakraborty - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}, {"title": "Zero-Shot Open-Vocabulary OOD Object Detection and Grounding using Vision Language Models", "link": "https://openreview.net/pdf%3Fid%3DQ2wVVeOpz8", "details": "P Sinhamahapatra, S Bose, K Roscher, S G\u00fcnnemann - Northern Lights Deep Learning \u2026", "abstract": "Automated driving involves complex perception tasks that require a precise understanding of diverse traffic scenarios and confident navigation. Traditional data- driven algorithms trained on closed-set data often fail to generalize upon out-of \u2026"}, {"title": "Coverage-Constrained Human-AI Cooperation with Multiple Experts", "link": "https://arxiv.org/pdf/2411.11976", "details": "Z Zhang, C Nguyen, K Wells, TT Do, G Carneiro - arXiv preprint arXiv:2411.11976, 2024", "abstract": "Human-AI cooperative classification (HAI-CC) approaches aim to develop hybrid intelligent systems that enhance decision-making in various high-stakes real-world scenarios by leveraging both human expertise and AI capabilities. Current HAI-CC \u2026"}, {"title": "Retrieval In Decoder benefits generative models for explainable complex question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007573", "details": "J Feng, Q Wang, H Qiu, L Liu - Neural Networks, 2024", "abstract": "Abstract Large-scale Language Models (LLMs) utilizing the Chain-of-Thought prompting demonstrate exceptional performance in a variety of tasks. However, the persistence of factual hallucinations remains a significant challenge in practical \u2026"}, {"title": "LongReward: Improving Long-context Large Language Models with AI Feedback", "link": "https://arxiv.org/pdf/2410.21252", "details": "J Zhang, Z Hou, X Lv, S Cao, Z Hou, Y Niu, L Hou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT \u2026"}, {"title": "Flaming-hot Initiation with Regular Execution Sampling for Large Language Models", "link": "https://arxiv.org/pdf/2410.21236", "details": "W Chen, Z Zhang, G Liu, R Zheng, W Shi, C Dun, Z Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Since the release of ChatGPT, large language models (LLMs) have demonstrated remarkable capabilities across various domains. A key challenge in developing these general capabilities is efficiently sourcing diverse, high-quality data. This \u2026"}, {"title": "RedPajama: an Open Dataset for Training Large Language Models", "link": "https://arxiv.org/pdf/2411.12372", "details": "M Weber, D Fu, Q Anthony, Y Oren, S Adams\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top \u2026"}, {"title": "Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2410.21728", "details": "K Luo, Z Ding, Z Weng, L Qiao, M Zhao, X Li, D Yin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Chain of Thought (CoT) prompting approaches have significantly consolidated the reasoning capabilities of large language models (LLMs), they still face limitations that require extensive human effort or have performance needs to be improved \u2026"}]
