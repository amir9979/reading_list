[{"title": "HCoTT: Hierarchical Chain-of-Thought Distillation", "link": "https://ieeexplore.ieee.org/abstract/document/10890771/", "details": "Z Wang, X Zhuang, Z Zhu, Y Zou - ICASSP 2025-2025 IEEE International Conference \u2026, 2025", "abstract": "Chains of Thought (CoT) have shown potential in augmenting the reasoning capabilities of language models, yet their effectiveness is predominantly observed in large language models (LLMs). Recently, several attempts have been made to inject \u2026"}, {"title": "A Weighted Cross-entropy Loss for Mitigating LLM Hallucinations in Cross-lingual Continual Pretraining", "link": "https://ieeexplore.ieee.org/abstract/document/10888877/", "details": "Y Fan, R Li, G Zhang, C Shi, X Wang - \u2026 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "Recently, due to the explosive advances of large language models (LLMs) on English, cross-lingual continual pretraining has been widely applied in obtaining Chinese LLMs. However, previous studies showed that these LLMs have suffered \u2026"}, {"title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models", "link": "https://arxiv.org/pdf/2502.10250%3F", "details": "GK Kumar, I Chaabane, K Wu - arXiv preprint arXiv:2502.10250, 2025", "abstract": "Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved \u2026"}, {"title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG", "link": "https://arxiv.org/pdf/2502.08356", "details": "K Bhushan, Y Nandwani, D Khandelwal, S Gupta\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the \u2026"}, {"title": "Franken-Adapter: Cross-Lingual Adaptation of LLMs by Embedding Surgery", "link": "https://arxiv.org/pdf/2502.08037", "details": "F Jiang, H Yu, G Chung, T Cohn - arXiv preprint arXiv:2502.08037, 2025", "abstract": "The capabilities of Large Language Models (LLMs) in low-resource languages lag far behind those in English, making their universal accessibility a significant challenge. To alleviate this, we present $\\textit {Franken-Adapter} $, a modular \u2026"}, {"title": "SelfElicit: Your Language Model Secretly Knows Where is the Relevant Evidence", "link": "https://arxiv.org/pdf/2502.08767", "details": "Z Liu, RA Amjad, R Adkathimar, T Wei, H Tong - arXiv preprint arXiv:2502.08767, 2025", "abstract": "Providing Language Models (LMs) with relevant evidence in the context (either via retrieval or user-provided) can significantly improve their ability to provide factually correct grounded responses. However, recent studies have found that LMs often \u2026"}, {"title": "AquaPipe: A Quality-Aware Pipeline for Knowledge Retrieval and Large Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3709661", "details": "R Yu, W Huang, S Bai, J Zhou, F Wu - Proceedings of the ACM on Management of \u2026, 2025", "abstract": "The knowledge retrieval methods such as Approximate Nearest Neighbor Search (ANNS) significantly enhance the generation quality of Large Language Models (LLMs) by introducing external knowledge, and this method is called Retrieval \u2026"}, {"title": "SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks", "link": "https://arxiv.org/pdf/2502.11090", "details": "H Cao, Y Wang, S Jing, Z Peng, Z Bai, Z Cao, M Fang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to \u2026"}, {"title": "LLM4EFFI: Leveraging Large Language Models to Enhance Code Efficiency and Correctness", "link": "https://arxiv.org/pdf/2502.18489", "details": "T Ye, W Huang, X Zhang, T Ma, P Liu, J Yin, W Wang - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs), particularly Code LLMs, have demonstrated impressive performance in code generation. Current research primarily focuses on the correctness of generated code, while efficiency remains less explored. Recent \u2026"}]
