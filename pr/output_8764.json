[{"title": "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "link": "https://aclanthology.org/2024.emnlp-main.759.pdf", "details": "F Liu, Z Li, H Zhou, Q Yin, J Yang, X Tang, C Luo\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question- answering (QA) task with answer options for evaluation. However, many clinical \u2026"}, {"title": "Mixture of neural fields for heterogeneous reconstruction in cryo-EM", "link": "https://openreview.net/pdf%3Fid%3DTuspoNzIdB", "details": "A Levy, R Raghu, D Shustin, ARY Peng, H Li\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Cryo-electron microscopy (cryo-EM) is an experimental technique for protein structure determination that images an ensemble of macromolecules in near- physiological contexts. While recent advances enable the reconstruction of dynamic \u2026"}, {"title": "AQLoRA: An Adaptive Quantization-Based Efficient Fine-Tuning Method for LLMs", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9434-8_21", "details": "X Huang, Y Huo, DF Wong, Y Wang, L Cai, Y Jiang - CCF International Conference on \u2026, 2024", "abstract": "Large language models (LLMs) have shown exceptional performance in the domain of composite artificial intelligence tasks, offering a preliminary insight into the potential of general artificial intelligence. The fine-tuning process for LLMs \u2026"}, {"title": "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.1249.pdf", "details": "X Lin, M Li, R Zemel, H Ji, SF Chang - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "Recently, enabling pretrained language models (PLMs) to perform zero-shot crossmodal tasks such as video question answering has been extensively studied. A popular approach is to learn a projection network that projects visual features into the \u2026"}, {"title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "link": "https://arxiv.org/pdf/2410.11829%3F", "details": "Y Cao, Y Liu, Z Chen, G Shi, W Wang, D Zhao, T Lu - arXiv preprint arXiv:2410.11829, 2024", "abstract": "Despite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating \u2026"}, {"title": "Domain Adaptation with a Single Vision-Language Embedding", "link": "https://arxiv.org/pdf/2410.21361", "details": "M Fahes, TH Vu, A Bursuc, P P\u00e9rez, R de Charette - arXiv preprint arXiv:2410.21361, 2024", "abstract": "Domain adaptation has been extensively investigated in computer vision but still requires access to target data at the training time, which might be difficult to obtain in some uncommon conditions. In this paper, we present a new framework for domain \u2026"}, {"title": "Tr-Net: Token Relation Inspired table filling network for joint entity and relation extraction", "link": "https://www.sciencedirect.com/science/article/pii/S0885230824001323", "details": "Y Kong, Z Yang, Z Ding, W Liu, S Zhang, J Xu, H Lin - Computer Speech & Language, 2024", "abstract": "Recently, table filling models have achieved promising performance in jointly extracting relation triplets from complex sentences, leveraging their inherent structural advantage of delineating entities and relations as table cells. Nonetheless \u2026"}, {"title": "Unified Representation of Genomic and Biomedical Concepts through Multi-Task, Multi-Source Contrastive Learning", "link": "https://arxiv.org/pdf/2410.10144", "details": "H Yuan, S Liu, K Cho, K Liao, A Pereira, T Cai - arXiv preprint arXiv:2410.10144, 2024", "abstract": "We introduce GENomic Encoding REpresentation with Language Model (GENEREL), a framework designed to bridge genetic and biomedical knowledge bases. What sets GENEREL apart is its ability to fine-tune language models to infuse \u2026"}, {"title": "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.110.pdf", "details": "D Hao, Q Wang, L Guo, J Jiang, J Liu - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "While large pre-trained visual-language models have shown promising results on traditional visual question answering benchmarks, it is still challenging for them to answer complex VQA problems which requires diverse world knowledge. Motivated \u2026"}]
