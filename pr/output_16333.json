[{"title": "Achieving GPT-4o level performance in astronomy with a specialized 8B-parameter large language model", "link": "https://www.nature.com/articles/s41598-025-97131-y", "details": "T de Haan, YS Ting, T Ghosal, TD Nguyen\u2026 - Scientific Reports, 2025", "abstract": "Abstract AstroSage-Llama-3.1-8B is a domain-specialized natural-language AI assistant tailored for research in astronomy, astrophysics, cosmology, and astronomical instrumentation. Trained on the complete collection of astronomy \u2026"}, {"title": "DICE: A Framework for Dimensional and Contextual Evaluation of Language Models", "link": "https://arxiv.org/pdf/2504.10359%3F", "details": "A Shrivastava, PA Aoyagui - arXiv preprint arXiv:2504.10359, 2025", "abstract": "Language models (LMs) are increasingly being integrated into a wide range of applications, yet the modern evaluation paradigm does not sufficiently reflect how they are actually being used. Current evaluations rely on benchmarks that often lack \u2026"}, {"title": "Summarization of Multimodal Presentations with Vision-Language Models: Study of the Effect of Modalities and Structure", "link": "https://arxiv.org/pdf/2504.10049%3F", "details": "T Gigant, C Guinaudeau, F Dufaux - arXiv preprint arXiv:2504.10049, 2025", "abstract": "Vision-Language Models (VLMs) can process visual and textual information in multiple formats: texts, images, interleaved texts and images, or even hour-long videos. In this work, we conduct fine-grained quantitative and qualitative analyses of \u2026"}, {"title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation", "link": "https://arxiv.org/pdf/2504.20859", "details": "G Hadad, H Roitman, Y Eshel, B Shapira, L Rokach - arXiv preprint arXiv:2504.20859, 2025", "abstract": "As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents``X-Cross''--a novel cross-domain sequential-recommendation model \u2026"}, {"title": "SymPlanner: Deliberate Planning in Language Models with Symbolic Representation", "link": "https://arxiv.org/pdf/2505.01479", "details": "S Xiong, J Zhou, Z Liu, Y Su - arXiv preprint arXiv:2505.01479, 2025", "abstract": "Planning remains a core challenge for language models (LMs), particularly in domains that require coherent multi-step action sequences grounded in external constraints. We introduce SymPlanner, a novel framework that equips LMs with \u2026"}, {"title": "Learning to Erase Private Knowledge from Multi-Documents for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2504.09910", "details": "Y Wang, H Zhang, L Pang, Y Tong, B Guo, H Zheng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-Augmented Generation (RAG) is a promising technique for applying LLMs to proprietary domains. However, retrieved documents may contain sensitive knowledge, posing risks of privacy leakage in generative results. Thus, effectively \u2026"}]
