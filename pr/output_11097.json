[{"title": "$ S^ 3$: Synonymous Semantic Space for Improving Zero-Shot Generalization of Vision-Language Models", "link": "https://arxiv.org/pdf/2412.04925", "details": "X Yin, Q Wang, B Cao, Q Hu - arXiv preprint arXiv:2412.04925, 2024", "abstract": "Recently, many studies have been conducted to enhance the zero-shot generalization ability of vision-language models (eg, CLIP) by addressing the semantic misalignment between image and text embeddings in downstream tasks \u2026"}, {"title": "Cross-Modal Mapping: Eliminating the Modality Gap for Few-Shot Image Classification", "link": "https://arxiv.org/pdf/2412.20110", "details": "X Yang, P Peng, W Xie, X Lu, J Wen - arXiv preprint arXiv:2412.20110, 2024", "abstract": "In few-shot image classification tasks, methods based on pretrained vision-language models (such as CLIP) have achieved significant progress. Many existing approaches directly utilize visual or textual features as class prototypes, however \u2026"}, {"title": "Federated Source-free Domain Adaptation for Classification: Weighted Cluster Aggregation for Unlabeled Data", "link": "https://arxiv.org/pdf/2412.13757", "details": "J Mori, K Kihara, T Miyagawa, AF Ebihara, I Teranishi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Federated learning (FL) commonly assumes that the server or some clients have labeled data, which is often impractical due to annotation costs and privacy concerns. Addressing this problem, we focus on a source-free domain adaptation \u2026"}]
