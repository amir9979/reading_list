[{"title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures", "link": "https://arxiv.org/pdf/2410.07698", "details": "Y Chen, Y Zhang, L Cao, K Yuan, Z Wen - arXiv preprint arXiv:2410.07698, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs when adapting large language models (LLMs) for downstream applications. However, traditional first-order (FO) fine-tuning algorithms incur substantial memory overhead \u2026"}, {"title": "VHELM: A Holistic Evaluation of Vision Language Models", "link": "https://arxiv.org/pdf/2410.07112", "details": "T Lee, H Tu, CH Wong, W Zheng, Y Zhou, Y Mai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation \u2026"}, {"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "link": "https://arxiv.org/pdf/2410.05639", "details": "R Zhao, ZL Thai, Y Zhang, S Hu, Y Ba, J Zhou, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the \u2026"}, {"title": "Efficient Fine-Tuning for Low-Resource Tibetan Pre-trained Language Models", "link": "https://link.springer.com/chapter/10.1007/978-3-031-72350-6_28", "details": "M Zhou, Z Daiqing, N Qun, T Nyima - International Conference on Artificial Neural \u2026, 2024", "abstract": "For low-resource languages like Tibetan, the availability of pre-trained language models (PLMs) is severely limited both in quantity and performance. Therefore, it is crucial to explore the optimization of these limited PLMs. In this paper, leveraging the \u2026"}, {"title": "Representation-Enhanced Neural Knowledge Integration with Application to Large-Scale Medical Ontology Learning", "link": "https://arxiv.org/pdf/2410.07454", "details": "S Liu, T Cai, X Li - arXiv preprint arXiv:2410.07454, 2024", "abstract": "A large-scale knowledge graph enhances reproducibility in biomedical data discovery by providing a standardized, integrated framework that ensures consistent interpretation across diverse datasets. It improves generalizability by connecting data \u2026"}, {"title": "Developing an automated algorithm for identification of children and adolescents with diabetes using electronic health records from the OneFlorida+ clinical research \u2026", "link": "https://dom-pubs.onlinelibrary.wiley.com/doi/abs/10.1111/dom.15987", "details": "P Li, E Spector, K Alkhuzam, R Patel, WT Donahoo\u2026 - Diabetes, Obesity and Metabolism", "abstract": "Aim To develop an automated computable phenotype (CP) algorithm for identifying diabetes cases in children and adolescents using electronic health records (EHRs) from the UF Health System. Materials and Methods The CP algorithm was iteratively \u2026"}, {"title": "Language Models\" Grok\" to Copy", "link": "https://arxiv.org/pdf/2409.09281", "details": "A Lv, R Xie, X Sun, Z Kang, R Yan - arXiv preprint arXiv:2409.09281, 2024", "abstract": "We examine the pre-training dynamics of language models, focusing on their ability to copy text from preceding context--a fundamental skill for various LLM applications, including in-context learning (ICL) and retrieval-augmented generation (RAG). We \u2026"}, {"title": "Prompt tuning discriminative language models for hierarchical text classification", "link": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/50E5499348A0E72F0C4F3AFC622133A7/S2977042424000517a.pdf/div-class-title-prompt-tuning-discriminative-language-models-for-hierarchical-text-classification-div.pdf", "details": "J du Toit, M Dunaiski - Natural Language Processing", "abstract": "Hierarchical text classification (HTC) is a natural language processing task which aims to categorise a text document into a set of classes from a hierarchical class structure. Recent approaches to solve HTC tasks focus on leveraging pre-trained \u2026"}, {"title": "Automatic structuring of radiology reports with on-premise open-source large language models", "link": "https://link.springer.com/article/10.1007/s00330-024-11074-y", "details": "P Wo\u017anicki, C Laqua, I Fiku, A Hekalo, D Truhn\u2026 - European Radiology, 2024", "abstract": "Objectives Structured reporting enhances comparability, readability, and content detail. Large language models (LLMs) could convert free text into structured data without disrupting radiologists' reporting workflow. This study evaluated an on \u2026"}]
