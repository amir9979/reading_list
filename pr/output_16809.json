[{"title": "STP: Special token prompt for parameter-efficient tuning of pre-trained language models", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425012874", "details": "Y Yan, H Yu, D Wang, J Ye, F Liu, W Xu - Expert Systems with Applications, 2025", "abstract": "Fine-tuning has become the standard method for using large pre-trained language models to accomplish specific downstream tasks. However, full fine-tuning requires updating all model parameters, which is not only computationally expensive but also \u2026"}, {"title": "Kastor: Fine-tuned Small Language Models for Shape-based Active Relation Extraction", "link": "https://hal.science/hal-05078493/document", "details": "C Ringwald, F Gandon, C Faron, F Michel, H Abi Akl - Extended Semantic Web \u2026, 2025", "abstract": "RDF pattern-based extraction is a compelling approach for fine-tuning small language models (SLMs) by focusing a relation extraction task on a specified SHACL shape. This technique enables the development of efficient models trained on limited \u2026"}, {"title": "Large Language Models in Crisis Informatics for Zero and Few-Shot Classification", "link": "https://dl.acm.org/doi/pdf/10.1145/3736160", "details": "C S\u00e1nchez, A Abeliuk, B Poblete - ACM Transactions on the Web, 2025", "abstract": "This article presents an exploration of the use of pre-trained Large Language Models (LLMs) for crisis classification to address labeled data dependency issues. We present a methodology that enhances open LLMs through fine-tuning, creating zero \u2026"}, {"title": "Medication Extraction and Entity Linking using Stacked and Voted Ensembles on LLMs", "link": "https://aclanthology.org/2025.cl4health-1.26.pdf", "details": "P Romero, L Han, G Nenadic - Proceedings of the Second Workshop on Patient \u2026, 2025", "abstract": "Abstract Medication Extraction and Mining of its related attributes play an important role in healthcare NLP research due to its practical applications in hospital settings, such as their mapping into standard clinical knowledge bases (SNOMED-CT, BNF \u2026"}, {"title": "Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented Generation via Knowledge Graph Walks", "link": "https://arxiv.org/pdf/2505.16849", "details": "M B\u00f6ckling, H Paulheim, A Iana - arXiv preprint arXiv:2505.16849, 2025", "abstract": "Large Language Models (LLMs) have showcased impressive reasoning abilities, but often suffer from hallucinations or outdated knowledge. Knowledge Graph (KG)- based Retrieval-Augmented Generation (RAG) remedies these shortcomings by \u2026", "entry_id": "http://arxiv.org/abs/2505.16849v1", "updated": "2025-05-22 16:11:35", "published": "2025-05-22 16:11:35", "authors": "Martin B\u00f6ckling;Heiko Paulheim;Andreea Iana", "summary": "Large Language Models (LLMs) have showcased impressive reasoning abilities,\nbut often suffer from hallucinations or outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) remedies these shortcomings by\ngrounding LLM responses in structured external information from a knowledge\nbase. However, many KG-based RAG approaches struggle with (i) aligning KG and\ntextual representations, (ii) balancing retrieval accuracy and efficiency, and\n(iii) adapting to dynamically updated KGs. In this work, we introduce\nWalk&Retrieve, a simple yet effective KG-based framework that leverages\nwalk-based graph traversal and knowledge verbalization for corpus generation\nfor zero-shot RAG. Built around efficient KG walks, our method does not require\nfine-tuning on domain-specific data, enabling seamless adaptation to KG\nupdates, reducing computational overhead, and allowing integration with any\noff-the-shelf backbone LLM. Despite its simplicity, Walk&Retrieve performs\ncompetitively, often outperforming existing RAG systems in response accuracy\nand hallucination reduction. Moreover, it demonstrates lower query latency and\nrobust scalability to large KGs, highlighting the potential of lightweight\nretrieval strategies as strong baselines for future RAG research.", "comment": "Accepted at the Information Retrieval's Role in RAG Systems (IR-RAG\n  2025) in conjunction with SIGIR 2025", "journal_ref": null, "primary_category": "cs.IR", "categories": "cs.IR;H.3.3; I.2.7", "links": "http://arxiv.org/abs/2505.16849v1;http://arxiv.org/pdf/2505.16849v1", "pdf_url": "http://arxiv.org/pdf/2505.16849v1"}, {"title": "MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models", "link": "https://arxiv.org/pdf/2505.16700", "details": "X Gao, S Xie, J Zhai, S Ma, C Shen - arXiv preprint arXiv:2505.16700, 2025", "abstract": "As Large Language Models (LLMs) evolve from passive text generators to active reasoning agents capable of tool interaction, the Model Context Protocol (MCP) has emerged as a standardized framework for dynamic tool discovery and orchestration \u2026", "entry_id": "http://arxiv.org/abs/2505.16700v1", "updated": "2025-05-22 14:02:37", "published": "2025-05-22 14:02:37", "authors": "Xuanqi Gao;Siyi Xie;Juan Zhai;Shqing Ma;Chao Shen", "summary": "As Large Language Models (LLMs) evolve from passive text generators to active\nreasoning agents capable of tool interaction, the Model Context Protocol (MCP)\nhas emerged as a standardized framework for dynamic tool discovery and\norchestration. Despite widespread industry adoption, existing evaluation\nmethodologies fail to adequately assess tool utilization capabilities within\nthis new paradigm. This paper introduces MCP-RADAR, the first comprehensive\nbenchmark specifically designed to evaluate LLM performance in the MCP\nframework through a novel five-dimensional approach measuring: answer accuracy,\ntool selection efficiency, computational resource efficiency, parameter\nconstruction accuracy, and execution speed. Unlike conventional benchmarks that\nrely on subjective human evaluations or binary success metrics, MCP-RADAR\nemploys objective, quantifiable measurements across multiple task domains\nincluding software engineering, mathematical reasoning, and general\nproblem-solving. Our evaluations of leading commercial and open-source LLMs\nreveal distinctive capability profiles with significant trade-offs between\naccuracy, efficiency, and speed, challenging traditional single-metric\nperformance rankings. Besides, we provide valuable guidance for developers to\noptimize their tools for maximum model compatibility and effectiveness. While\nfocused on MCP due to its standardized approach, our methodology remains\napplicable across all LLM agent tool integration frameworks, providing valuable\ninsights for both LLM developers and tool creators to optimize the entire\nLLM-tool interaction ecosystem. The implementation, configurations, and\ndatasets used in our evaluation are publicly available at\nhttps://anonymous.4open.science/r/MCPRadar-B143.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.16700v1;http://arxiv.org/pdf/2505.16700v1", "pdf_url": "http://arxiv.org/pdf/2505.16700v1"}]
