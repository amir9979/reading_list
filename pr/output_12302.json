[{"title": "Bayesian Neural Networks for One-to-Many Mapping in Image Enhancement", "link": "https://arxiv.org/pdf/2501.14265", "details": "G Huang, N Anantrasirichai, F Ye, Z Qi, RR Lin, Q Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In image enhancement tasks, such as low-light and underwater image enhancement, a degraded image can correspond to multiple plausible target images due to dynamic photography conditions, such as variations in illumination. This naturally \u2026"}, {"title": "Multivariate Time Series Anomaly Detection by Capturing Coarse-Grained Intra-and Inter-Variate Dependencies", "link": "https://arxiv.org/pdf/2501.16364", "details": "Y Xie, H Zhang, MA Babar - arXiv preprint arXiv:2501.16364, 2025", "abstract": "Multivariate time series anomaly detection is essential for failure management in web application operations, as it directly influences the effectiveness and timeliness of implementing remedial or preventive measures. This task is often framed as a semi \u2026"}, {"title": "FreEformer: Frequency Enhanced Transformer for Multivariate Time Series Forecasting", "link": "https://arxiv.org/pdf/2501.13989", "details": "W Yue, Y Liu, X Ying, B Xing, R Guo, J Shi - arXiv preprint arXiv:2501.13989, 2025", "abstract": "This paper presents\\textbf {FreEformer}, a simple yet effective model that leverages a\\textbf {Fre} quency\\textbf {E} nhanced Trans\\textbf {former} for multivariate time series forecasting. Our work is based on the assumption that the frequency spectrum \u2026"}, {"title": "Hypergraph-based Zero-shot Multi-modal Product Attribute Value Extraction", "link": "https://openreview.net/pdf%3Fid%3DtmQDHqzupm", "details": "J Hu, J Gong, H Shen, H Eldardiry - THE WEB CONFERENCE 2025", "abstract": "It is essential for e-commerce platforms to provide accurate, complete, and timely product attribute values, in order to improve the search and recommendation experience for both customers and sellers. In the real-world scenario, it is difficult for \u2026"}, {"title": "Training-Free Zero-Shot Temporal Action Detection with Vision-Language Models", "link": "https://arxiv.org/pdf/2501.13795%3F", "details": "C Han, H Wang, J Kuang, L Zhang, J Gui - arXiv preprint arXiv:2501.13795, 2025", "abstract": "Existing zero-shot temporal action detection (ZSTAD) methods predominantly use fully supervised or unsupervised strategies to recognize unseen activities. However, these training-based methods are prone to domain shifts and require high \u2026"}]
