[{"title": "Training Language Models on Synthetic Edit Sequences Improves Code Synthesis", "link": "https://arxiv.org/pdf/2410.02749", "details": "U Piterbarg, L Pinto, R Fergus - arXiv preprint arXiv:2410.02749, 2024", "abstract": "Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality \u2026"}, {"title": "How to Train Long-Context Language Models (Effectively)", "link": "https://arxiv.org/pdf/2410.02660", "details": "T Gao, A Wettig, H Yen, D Chen - arXiv preprint arXiv:2410.02660, 2024", "abstract": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development--Instead of perplexity or simple \u2026"}, {"title": "Ascle\u2014A Python Natural Language Processing Toolkit for Medical Text Generation: Development and Evaluation Study", "link": "https://www.jmir.org/2024/1/e60601/", "details": "R Yang, Q Zeng, K You, Y Qiao, L Huang, CC Hsieh\u2026 - Journal of Medical Internet \u2026, 2024", "abstract": "Background Medical texts present significant domain-specific challenges, and manually curating these texts is a time-consuming and labor-intensive process. To address this, natural language processing (NLP) algorithms have been developed to \u2026"}, {"title": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models", "link": "https://arxiv.org/pdf/2410.02355", "details": "J Fang, H Jiang, K Wang, Y Ma, X Wang, X He, T Chua - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) often exhibit hallucinations due to incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating \u2026"}, {"title": "Preserving Generalization of Language models in Few-shot Continual Relation Extraction", "link": "https://arxiv.org/pdf/2410.00334", "details": "Q Tran, NX Thanh, NH Anh, NL Hai, T Le, L Van Ngo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Few-shot Continual Relations Extraction (FCRE) is an emerging and dynamic area of study where models can sequentially integrate knowledge from new relations with limited labeled data while circumventing catastrophic forgetting and preserving prior \u2026"}, {"title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models", "link": "https://arxiv.org/pdf/2409.19375", "details": "Z Han, J Yang, J Li, Q Hu, Q Xu, MZ Shou, C Zhang - arXiv preprint arXiv:2409.19375, 2024", "abstract": "Vision-language foundation models (eg, CLIP) have shown remarkable performance across a wide range of tasks. However, deploying these models may be unreliable when significant distribution gaps exist between the training and test data. The \u2026"}, {"title": "ColaCare: Enhancing Electronic Health Record Modeling through Large Language Model-Driven Multi-Agent Collaboration", "link": "https://arxiv.org/pdf/2410.02551", "details": "Z Wang, Y Zhu, H Zhao, X Zheng, T Wang, W Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce ColaCare, a framework that enhances Electronic Health Record (EHR) modeling through multi-agent collaboration driven by Large Language Models (LLMs). Our approach seamlessly integrates domain-specific expert models with \u2026"}, {"title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models", "link": "https://arxiv.org/pdf/2410.01335", "details": "L Bandarkar, B Muller, P Yuvraj, R Hou, N Singhal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large \u2026"}, {"title": "Advancing Medical Radiograph Representation Learning: A Hybrid Pre-training Paradigm with Multilevel Semantic Granularity", "link": "https://arxiv.org/pdf/2410.00448", "details": "H Jiang, X Hao, Y Huang, C Ma, J Zhang, Y Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces an innovative approach to Medical Vision-Language Pre- training (Med-VLP) area in the specialized context of radiograph representation learning. While conventional methods frequently merge textual annotations into \u2026"}]
