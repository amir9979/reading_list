[{"title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models", "link": "https://aclanthology.org/2024.findings-naacl.18.pdf", "details": "Z Su, Z Lin, B Baixue, H Chen, S Hu, W Zhou, G Ding\u2026 - Findings of the Association \u2026, 2024", "abstract": "Generative language models are usually pre-trained on large text corpus via predicting the next token (ie, sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language \u2026"}, {"title": "Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement", "link": "https://arxiv.org/pdf/2405.15973", "details": "X Wang, J Chen, Z Wang, Y Zhou, Y Zhou, H Yao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision-language models (LVLMs) have achieved impressive results in various visual question-answering and reasoning tasks through vision instruction tuning on specific datasets. However, there is still significant room for improvement in the \u2026"}, {"title": "Adaptive Rank Selections for Low-Rank Approximation of Language Models", "link": "https://aclanthology.org/2024.naacl-long.13.pdf", "details": "S Gao, T Hua, YC Hsu, Y Shen, H Jin - Proceedings of the 2024 Conference of the \u2026, 2024", "abstract": "Abstract Singular Value Decomposition (SVD) or its weighted variants has significantly progressed in compressing language models. Previous works assume the same importance for all operations and assign the same number of ranks for \u2026"}, {"title": "Memory augmented language models through mixture of word experts", "link": "https://aclanthology.org/2024.naacl-long.249.pdf", "details": "C dos Santos, J Lee-Thorp, I Noble, CC Chang\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Scaling up the number of parameters of language models has proven to be an effective approach to improve performance. For dense models, increasing their size proportionally increases their computational footprint. In this work, we seek to \u2026"}, {"title": "PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models", "link": "https://aclanthology.org/2024.naacl-long.336.pdf", "details": "HJ Kim, YJ Kim, JY Bak - Proceedings of the 2024 Conference of the North \u2026, 2024", "abstract": "Pre-trained language models (PLMs) show impressive performance in various downstream NLP tasks. However, pre-training large language models demands substantial memory and training compute. Furthermore, due to the substantial \u2026"}, {"title": "Experiences of Electronic Health Records' and Client Information Systems' Use on a Mobile Device and Factors Associated With Work Time Savings Among Practical \u2026", "link": "https://www.jmir.org/2024/1/e46954/", "details": "S Paatela, M Kyyts\u00f6nen, K Saranto, UM Kinnunen\u2026 - Journal of Medical Internet \u2026, 2024", "abstract": "Background The transmission of clinical information in nursing predominantly occurs through digital solutions, such as computers and mobile devices, in today's era. Various technological systems, including electronic health records (EHRs) and client \u2026"}, {"title": "DataComp-LM: In search of the next generation of training sets for language models", "link": "https://arxiv.org/abs/2406.11794", "details": "J Li, A Fang, G Smyrnis, M Ivgi, M Jordan, S Gadre\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce DataComp for Language Models (DCLM), a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl \u2026"}, {"title": "Extracting Systemic Anticancer Therapy and Response Information From Clinical Notes Following the RECIST Definition", "link": "https://ascopubs.org/doi/abs/10.1200/CCI.23.00166", "details": "X Zuo, A Kumar, S Shen, J Li, G Cong, E Jin, Q Chen\u2026 - JCO Clinical Cancer \u2026, 2024", "abstract": "PURPOSE The RECIST guidelines provide a standardized approach for evaluating the response of cancer to treatment, allowing for consistent comparison of treatment efficacy across different therapies and patients. However, collecting such information \u2026"}, {"title": "Benchmarking Children's ASR with Supervised and Self-supervised Speech Foundation Models", "link": "https://arxiv.org/pdf/2406.10507", "details": "R Fan, NB Shankar, A Alwan - arXiv preprint arXiv:2406.10507, 2024", "abstract": "Speech foundation models (SFMs) have achieved state-of-the-art results for various speech tasks in supervised (eg Whisper) or self-supervised systems (eg WavLM). However, the performance of SFMs for child ASR has not been systematically \u2026"}]
