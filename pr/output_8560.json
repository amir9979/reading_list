[{"title": "Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning", "link": "https://arxiv.org/pdf/2410.10360", "details": "Y Xu, R Zhang, X Jiang, Y Feng, Y Xiao, X Ma, R Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Retrieval-Augmented Generation (RAG) offers an effective solution to the issues faced by Large Language Models (LLMs) in hallucination generation and knowledge obsolescence by incorporating externally retrieved knowledge. However, due to \u2026"}, {"title": "The last iterate advantage: Empirical auditing and principled heuristic analysis of differentially private sgd", "link": "https://arxiv.org/pdf/2410.06186", "details": "T Steinke, M Nasr, A Ganesh, B Balle\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We propose a simple heuristic privacy analysis of noisy clipped stochastic gradient descent (DP-SGD) in the setting where only the last iterate is released and the intermediate iterates remain hidden. Namely, our heuristic assumes a linear structure \u2026"}, {"title": "Federated Heterogeneous Contrastive Distillation for Molecular Representation Learning", "link": "https://dl.acm.org/doi/abs/10.1145/3627673.3679725", "details": "J Feng, Z Wang, Z Wei, Y Li, B Ding, H Xu - \u2026 of the 33rd ACM International Conference \u2026, 2024", "abstract": "With the increasing application of deep learning to solve scientific problems in biochemistry, molecular federated learning has become popular due to its ability to offer distributed privacy-preserving solutions. However, most existing molecular \u2026"}, {"title": "$\\beta $-calibration of Language Model Confidence Scores for Generative QA", "link": "https://arxiv.org/pdf/2410.06615", "details": "P Manggala, A Mastakouri, E Kirschbaum\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim \u2026"}, {"title": "Chest X-ray synthetic data for better testing and evaluation of ML models", "link": "https://cs231n.stanford.edu/2024/papers/chest-x-ray-synthetic-data-for-better-testing-and-evaluation-of-.pdf", "details": "E Bismuth, A Geslin, M Paschali", "abstract": "Abstract The use of Machine Learning (ML) models in radiology has significantly advanced medical imaging diagnostics. However, model performance may not accurately reflect certain underrepresented conditions if such conditions or minorities \u2026"}, {"title": "Evolutionary Contrastive Distillation for Language Model Alignment", "link": "https://arxiv.org/pdf/2410.07513", "details": "J Katz-Samuels, Z Li, H Yun, P Nigam, Y Xu, V Petricek\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The ability of large language models (LLMs) to execute complex instructions is essential for their real-world applications. However, several recent studies indicate that LLMs struggle with challenging instructions. In this paper, we propose \u2026"}, {"title": "Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures", "link": "https://arxiv.org/pdf/2410.06672", "details": "J Wang, X Ge, W Shu, Q Tang, Y Zhou, Z He, X Qiu - arXiv preprint arXiv:2410.06672, 2024", "abstract": "The hypothesis of Universality in interpretability suggests that different neural networks may converge to implement similar algorithms on similar tasks. In this work, we investigate two mainstream architectures for language modeling, namely \u2026"}, {"title": "Drowzee: Metamorphic Testing for Fact-conflicting Hallucination Detection in Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3689776", "details": "N Li, Y Li, Y Liu, L Shi, K Wang, H Wang - Proceedings of the ACM on Programming \u2026, 2024", "abstract": "Large language models (LLMs) have revolutionized language processing, but face critical challenges with security, privacy, and generating hallucinations\u2014coherent but factually inaccurate outputs. A major issue is fact-conflicting hallucination (FCH) \u2026"}, {"title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models", "link": "https://arxiv.org/pdf/2410.06154", "details": "MJ Mirza, M Zhao, Z Mao, S Doveh, W Lin, P Gavrikov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task \u2026"}]
