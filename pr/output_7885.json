[{"title": "Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization", "link": "https://arxiv.org/pdf/2410.09302", "details": "G Liu, K Ji, R Zheng, Z Wu, C Dun, Q Gu, L Yan - arXiv preprint arXiv:2410.09302, 2024", "abstract": "Reinforcement Learning (RL) plays a crucial role in aligning large language models (LLMs) with human preferences and improving their ability to perform complex tasks. However, current approaches either require significant computational resources due \u2026"}, {"title": "Language Model Preference Evaluation with Multiple Weak Evaluators", "link": "https://arxiv.org/pdf/2410.12869", "details": "Z Hu, J Zhang, Z Xiong, A Ratner, H Xiong, R Krishna - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the remarkable success of Large Language Models (LLMs), evaluating their outputs' quality regarding preference remains a critical challenge. Existing works usually leverage a powerful LLM (eg, GPT4) as the judge for comparing LLMs' output \u2026"}, {"title": "Improving Instruction-Following in Language Models through Activation Steering", "link": "https://arxiv.org/pdf/2410.12877", "details": "A Stolfo, V Balachandran, S Yousefi, E Horvitz, B Nushi - arXiv preprint arXiv \u2026, 2024", "abstract": "The ability to follow instructions is crucial for numerous real-world applications of language models. In pursuit of deeper insights and more powerful capabilities, we derive instruction-specific vector representations from language models and use \u2026"}, {"title": "Unearthing Skill-Level Insights for Understanding Trade-Offs of Foundation Models", "link": "https://arxiv.org/pdf/2410.13826", "details": "M Moayeri, V Balachandran, V Chandrasekaran\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With models getting stronger, evaluations have grown more complex, testing multiple skills in one benchmark and even in the same instance at once. However, skill-wise performance is obscured when inspecting aggregate accuracy, under-utilizing the \u2026"}, {"title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "link": "https://arxiv.org/pdf/2410.11829", "details": "Y Cao, Y Liu, Z Chen, G Shi, W Wang, D Zhao, T Lu - arXiv preprint arXiv:2410.11829, 2024", "abstract": "Despite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating \u2026"}, {"title": "Simultaneous Reward Distillation and Preference Learning: Get You a Language Model Who Can Do Both", "link": "https://arxiv.org/pdf/2410.08458", "details": "A Nath, C Jung, E Seefried, N Krishnaswamy - arXiv preprint arXiv:2410.08458, 2024", "abstract": "Reward modeling of human preferences is one of the cornerstones of building usable generative large language models (LLMs). While traditional RLHF-based alignment methods explicitly maximize the expected rewards from a separate reward \u2026"}, {"title": "A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement", "link": "https://arxiv.org/pdf/2410.13828", "details": "H Yuan, Y Zeng, Y Wu, H Wang, M Wang, L Leqi - arXiv preprint arXiv:2410.13828, 2024", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for language model (LM) alignment. At its core, RLHF uses a margin-based loss for preference optimization, specifying ideal LM behavior only by \u2026"}, {"title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining", "link": "https://arxiv.org/pdf/2410.08102", "details": "T Bai, L Yang, ZH Wong, J Peng, X Zhuang, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these \u2026"}, {"title": "Better to Ask in English: Evaluation of Large Language Models on English, Low-resource and Cross-Lingual Settings", "link": "https://arxiv.org/pdf/2410.13153", "details": "K Dey, P Tarannum, MA Hasan, I Razzak, U Naseem - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) are trained on massive amounts of data, enabling their application across diverse domains and tasks. Despite their remarkable performance, most LLMs are developed and evaluated primarily in English \u2026"}]
