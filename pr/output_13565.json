[{"title": "A multimodal multidomain multilingual medical foundation model for zero shot clinical diagnosis", "link": "https://www.nature.com/articles/s41746-024-01339-7", "details": "F Liu, Z Li, Q Yin, J Huang, J Luo, A Thakur, K Branson\u2026 - npj Digital Medicine, 2025", "abstract": "Radiology images are one of the most commonly used in daily clinical diagnosis. Typically, clinical diagnosis using radiology images involves disease reporting and classification, where the former is a multimodal task whereby textual reports are \u2026"}, {"title": "BiasICL: In-Context Learning and Demographic Biases of Vision Language Models", "link": "https://arxiv.org/pdf/2503.02334", "details": "S Xu, J Janizek, Y Jiang, R Daneshjou - arXiv preprint arXiv:2503.02334, 2025", "abstract": "Vision language models (VLMs) show promise in medical diagnosis, but their performance across demographic subgroups when using in-context learning (ICL) remains poorly understood. We examine how the demographic composition of \u2026"}, {"title": "Semantic-aware contrastive learning via multi-prompt alignment", "link": "https://link.springer.com/article/10.1007/s10994-024-06665-1", "details": "Z Zhao, H Qin, M Kong, L Chen, D Xie, J Zhu, Q Zhu - Machine Learning, 2025", "abstract": "The role of the sample generation mechanism in contrastive learning is pivotal. It not only determines the pairings of positive and negative samples but also enriches the diversity of the sample pool, thereby substantially affecting the quality of the learned \u2026"}, {"title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06788", "details": "H Diao, X Li, Y Cui, Y Wang, H Deng, T Pan, W Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient \u2026"}, {"title": "X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography from Chest Radiography via Tri-Modal Contrastive Learning", "link": "https://arxiv.org/pdf/2503.02162", "details": "J You, Y Gao, S Kim, C Mcintosh - arXiv preprint arXiv:2503.02162, 2025", "abstract": "Computed tomography (CT) is a key imaging modality for diagnosis, yet its clinical utility is marred by high radiation exposure and long turnaround times, restricting its use for larger-scale screening. Although chest radiography (CXR) is more accessible \u2026"}, {"title": "Enhancing Generalization in Camera Trap Image Recognition: Fine-Tuning Visual Language Models", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225004989", "details": "Z Yang, Y Tian, L Wang, J Zhang - Neurocomputing, 2025", "abstract": "This study introduces a novel fine-tuning approach for enhancing the generalization capabilities of visual language models in the context of wildlife monitoring, particularly for camera trap image recognition. In this paper, we introduce Ecological \u2026"}, {"title": "Abn-BLIP: Abnormality-aligned Bootstrapping Language-Image Pre-training for Pulmonary Embolism Diagnosis and Report Generation from CTPA", "link": "https://arxiv.org/pdf/2503.02034", "details": "Z Zhong, Y Wang, L Bi, Z Ma, SH Ahn, CJ Mullin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical imaging plays a pivotal role in modern healthcare, with computed tomography pulmonary angiography (CTPA) being a critical tool for diagnosing pulmonary embolism and other thoracic conditions. However, the complexity of \u2026"}, {"title": "Noise is an Efficient Learner for Zero-Shot Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06019", "details": "R Imam, A Hanif, J Zhang, KW Dawoud\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, test-time adaptation has garnered attention as a method for tuning models without labeled data. The conventional modus operandi for adapting pre-trained vision-language models (VLMs) during test-time primarily focuses on tuning \u2026"}, {"title": "Discovering Global False Negatives On the Fly for Self-supervised Contrastive Learning", "link": "https://arxiv.org/pdf/2502.20612", "details": "V Balmaseda, B Wang, CL Lin, T Yang - arXiv preprint arXiv:2502.20612, 2025", "abstract": "In self-supervised contrastive learning, negative pairs are typically constructed using an anchor image and a sample drawn from the entire dataset, excluding the anchor. However, this approach can result in the creation of negative pairs with similar \u2026"}]
