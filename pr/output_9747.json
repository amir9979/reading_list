[{"title": "DHCP: Detecting Hallucinations by Cross-modal Attention Pattern in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.18659", "details": "Y Zhang, R Xie, J Chen, X Sun, Y Wang - arXiv preprint arXiv:2411.18659, 2024", "abstract": "Large vision-language models (LVLMs) have demonstrated exceptional performance on complex multimodal tasks. However, they continue to suffer from significant hallucination issues, including object, attribute, and relational \u2026"}, {"title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step", "link": "https://arxiv.org/pdf/2411.10440%3F", "details": "G Xu, P Jin, L Hao, Y Song, L Sun, L Yuan - arXiv preprint arXiv:2411.10440, 2024", "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to \u2026"}, {"title": "metaTextGrad: Learning to learn with language models as optimizers", "link": "https://openreview.net/pdf%3Fid%3DyzieYIT9hu", "details": "G Xu, M Yuksekgonul, C Guestrin, J Zou - Adaptive Foundation Models: Evolving AI for \u2026", "abstract": "Large language models (LLMs) are increasingly used in learning algorithms, evaluations, and optimization tasks. Recent studies have shown that incorporating self-criticism into LLMs can significantly enhance model performance, with \u2026"}, {"title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models", "link": "https://arxiv.org/pdf/2411.09595", "details": "Z Wang, J Lorraine, Y Wang, H Su, J Zhu, S Fidler\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This work explores expanding the capabilities of large language models (LLMs) pretrained on text to generate 3D meshes within a unified model. This offers key advantages of (1) leveraging spatial knowledge already embedded in LLMs, derived \u2026"}, {"title": "Mixtures of In-Context Learners", "link": "https://arxiv.org/pdf/2411.02830", "details": "G Hong, E van Krieken, E Ponti, N Malkin, P Minervini - arXiv preprint arXiv \u2026, 2024", "abstract": "In-context learning (ICL) adapts LLMs by providing demonstrations without fine- tuning the model parameters; however, it does not differentiate between demonstrations and quadratically increases the complexity of Transformer LLMs \u2026"}, {"title": "Measuring Non-Adversarial Reproduction of Training Data in Large Language Models", "link": "https://arxiv.org/pdf/2411.10242%3F", "details": "M Aerni, J Rando, E Debenedetti, N Carlini, D Ippolito\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models memorize parts of their training data. Memorizing short snippets and facts is required to answer questions about the world and to be fluent in any language. But models have also been shown to reproduce long verbatim \u2026"}, {"title": "MEMO-Bench: A Multiple Benchmark for Text-to-Image and Multimodal Large Language Models on Human Emotion Analysis", "link": "https://arxiv.org/pdf/2411.11235", "details": "Y Zhou, Z Zhang, J Cao, J Jia, Y Jiang, F Wen, X Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Artificial Intelligence (AI) has demonstrated significant capabilities in various fields, and in areas such as human-computer interaction (HCI), embodied intelligence, and the design and animation of virtual digital humans, both practitioners and users are \u2026"}, {"title": "Free $^ 2$ Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.17041", "details": "J Kim, BS Kim, JC Ye - arXiv preprint arXiv:2411.17041, 2024", "abstract": "Diffusion models have achieved impressive results in generative tasks like text-to- image (T2I) and text-to-video (T2V) synthesis. However, achieving accurate text alignment in T2V generation remains challenging due to the complex temporal \u2026"}, {"title": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks", "link": "https://arxiv.org/pdf/2411.19325", "details": "MS Danish, MA Munir, SRA Shah, K Kuckreja, FS Khan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While numerous recent benchmarks focus on evaluating generic Vision-Language Models (VLMs), they fall short in addressing the unique demands of geospatial applications. Generic VLM benchmarks are not designed to handle the complexities \u2026"}]
