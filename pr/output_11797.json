[{"title": "MedVLM: Medical Vision-Language Model for Consumer Devices", "link": "https://ieeexplore.ieee.org/abstract/document/10816095/", "details": "M Ayaz, M Khan, M Saqib, A Khelifi, M Sajjad\u2026 - IEEE Consumer Electronics \u2026, 2024", "abstract": "Generative Artificial Intelligence (GenAI) has enabled significant advancements in healthcare by supporting complex medical tasks through multimodal data processing. However, existing models often lack the adaptability required for diverse \u2026"}, {"title": "Vision-Language Models Do Not Understand Negation", "link": "https://arxiv.org/pdf/2501.09425", "details": "K Alhamoud, S Alshammari, Y Tian, G Li, P Torr, Y Kim\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Many practical vision-language applications require models that understand negation, eg, when using natural language to retrieve images which contain certain objects but not others. Despite advancements in vision-language models (VLMs) \u2026"}, {"title": "Training Medical Large Vision-Language Models with Abnormal-Aware Feedback", "link": "https://arxiv.org/pdf/2501.01377", "details": "Y Zhou, L Song, J Shen - arXiv preprint arXiv:2501.01377, 2025", "abstract": "Existing Medical Large Vision-Language Models (Med-LVLMs), which encapsulate extensive medical knowledge, demonstrate excellent capabilities in understanding medical images and responding to human queries based on these images \u2026"}, {"title": "Efficient Architectures for High Resolution Vision-Language Models", "link": "https://arxiv.org/pdf/2501.02584", "details": "M Carvalho, B Martins - arXiv preprint arXiv:2501.02584, 2025", "abstract": "Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This \u2026"}, {"title": "GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models", "link": "https://arxiv.org/pdf/2501.01428%3F", "details": "Z Qi, Z Zhang, Y Fang, J Wang, H Zhao - arXiv preprint arXiv:2501.01428, 2025", "abstract": "In recent years, 2D Vision-Language Models (VLMs) have made significant strides in image-text understanding tasks. However, their performance in 3D spatial comprehension, which is critical for embodied intelligence, remains limited. Recent \u2026"}, {"title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding", "link": "https://arxiv.org/pdf/2412.16158%3F", "details": "C Tao, S Su, X Zhu, C Zhang, Z Chen, J Liu, W Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advance of Large Language Models (LLMs) has catalyzed the development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid modality-specific encoders, offer a promising alternative to the compositional ones \u2026"}, {"title": "Guiding Medical Vision-Language Models with Explicit Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations", "link": "https://arxiv.org/pdf/2501.02385", "details": "K Zhu, Z Qin, H Yi, Z Jiang, Q Lao, S Zhang, K Li - arXiv preprint arXiv:2501.02385, 2025", "abstract": "With the recent advancements in vision-language models (VLMs) driven by large language models (LLMs), many researchers have focused on models that comprised of an image encoder, an image-to-language projection layer, and a text decoder \u2026"}, {"title": "Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models", "link": "https://arxiv.org/pdf/2412.18609%3F", "details": "J Yi, ST Wasim, Y Luo, M Naseer, J Gall - arXiv preprint arXiv:2412.18609, 2024", "abstract": "We present an efficient encoder-free approach for video-language understanding that achieves competitive performance while significantly reducing computational overhead. Current video-language models typically rely on heavyweight image \u2026"}, {"title": "Bias for Action: Video Implicit Neural Representations with Bias Modulation", "link": "https://arxiv.org/pdf/2501.09277", "details": "A Kayabasi, AK Vadathya, G Balakrishnan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We propose a new continuous video modeling framework based on implicit neural representations (INRs) called ActINR. At the core of our approach is the observation that INRs can be considered as a learnable dictionary, with the shapes of the basis \u2026"}]
