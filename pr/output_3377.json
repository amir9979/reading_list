[{"title": "Zero-Shot Video Editing through Adaptive Sliding Score Distillation", "link": "https://arxiv.org/pdf/2406.04888", "details": "L Zhu, Y Bao, J Huo, J Wu, YK Lai, W Li, Y Gao - arXiv preprint arXiv:2406.04888, 2024", "abstract": "The burgeoning field of text-based video generation (T2V) has reignited significant interest in the research of controllable video editing. Although pre-trained T2V-based editing models have achieved efficient editing capabilities, current works are still \u2026"}, {"title": "Generative Pre-Trained Diffusion Paradigm for Zero-Shot Time Series Forecasting", "link": "https://arxiv.org/pdf/2406.02212", "details": "J Yang, T Dai, N Li, J Wu, P Liu, J Li, J Bao, H Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, generative pre-trained paradigms such as Large Language Models (LLMs) and Large Vision Models (LVMs) have achieved revolutionary advancements and widespread real-world applications. Particularly, the emergence of pre-trained \u2026"}, {"title": "Demonstration Augmentation for Zero-shot In-context Learning", "link": "https://arxiv.org/pdf/2406.01224", "details": "Y Su, Y Tai, Y Ji, J Li, B Yan, M Zhang - arXiv preprint arXiv:2406.01224, 2024", "abstract": "Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates. However, many studies \u2026"}, {"title": "Robust Image Classification in the Presence of Out-of-Distribution and Adversarial Samples Using Attractors in Neural Networks", "link": "https://arxiv.org/pdf/2406.10579", "details": "N Alipour, SA SeyyedSalehi - arXiv preprint arXiv:2406.10579, 2024", "abstract": "The proper handling of out-of-distribution (OOD) samples in deep classifiers is a critical concern for ensuring the suitability of deep neural networks in safety-critical systems. Existing approaches developed for robust OOD detection in the presence of \u2026"}, {"title": "FedPA: Generator-Based Heterogeneous Federated Prototype Adversarial Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10579863/", "details": "L Jiang, X Wang, X Yang, J Shu, H Lin, X Yi - IEEE Transactions on Dependable and \u2026, 2024", "abstract": "Federated Learning is an emerging distributed algorithm that is designed to collaboratively train the global model without accessing clients' private data. However, heterogeneity of data among clients leads to significant degradation in \u2026"}, {"title": "Multivariate Time Series Modeling and Forecasting with Parallelized Convolution and Decomposed Sparse-Transformer", "link": "https://ieeexplore.ieee.org/abstract/document/10552140/", "details": "S Ma, YB Zhao, Y Kang, P Bai - IEEE Transactions on Artificial Intelligence, 2024", "abstract": "Many real-world scenarios require accurate predictions of time series, especially in the case of long sequence time-series forecasting (LSTF), such as predicting traffic flow and electricity consumption. However, existing time series prediction models \u2026"}, {"title": "One Process Spatiotemporal Learning of Transformers via Vcls Token for Multivariate Time Series Forecasting", "link": "https://www.researchgate.net/profile/Jingzehua-Xu/publication/381739943_One_Process_Spatiotemporal_Learning_of_Transformers_via_Vcls_Token_for_Multivariate_Time_Series_Forecasting/links/667d11d5f3b61c4e2c8ebd08/One-Process-Spatiotemporal-Learning-of-Transformers-via-Vcls-Token-for-Multivariate-Time-Series-Forecasting.pdf", "details": "T Cai, H Wu, D Niu, X Xia, J Jiang, J Xu", "abstract": "Previous Transformer-based models for multivariate time series forecasting mainly focus on temporal dependence learning and neglect the association between variables. The recent method of adding Attention on spatial (variate) tokens before or \u2026"}]
