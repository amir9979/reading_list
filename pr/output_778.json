'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Investigating Regularization of Self-Play Language Mod'
[{"title": "MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering", "link": "https://arxiv.org/pdf/2403.19116", "details": "C Guan, M Huang, P Zhang - arXiv preprint arXiv:2403.19116, 2024", "abstract": "In today's fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested \u2026"}, {"title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models", "link": "https://arxiv.org/pdf/2403.18814", "details": "Y Li, Y Zhang, C Wang, Z Zhong, Y Chen, R Chu, S Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared \u2026"}, {"title": "VLRM: Vision-Language Models act as Reward Models for Image Captioning", "link": "https://arxiv.org/pdf/2404.01911", "details": "M Dzabraev, A Kunitsyn, A Ivaniuta - arXiv preprint arXiv:2404.01911, 2024", "abstract": "In this work, we present an unsupervised method for enhancing an image captioning model (in our case, BLIP2) using reinforcement learning and vision-language models like CLIP and BLIP2-ITM as reward models. The RL-tuned model is able to \u2026"}, {"title": "Conceptual and Unbiased Reasoning in Language Models", "link": "https://arxiv.org/pdf/2404.00205", "details": "B Zhou, H Zhang, S Chen, D Yu, H Wang, B Peng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Conceptual reasoning, the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on large language models' capability to perform conceptual reasoning. In this work, we \u2026"}, {"title": "Enhancing Question Answering for Enterprise Knowledge Bases using Large Language Models", "link": "https://arxiv.org/pdf/2404.08695", "details": "F Jiang, C Qin, K Yao, C Fang, F Zhuang, H Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Efficient knowledge management plays a pivotal role in augmenting both the operational efficiency and the innovative capacity of businesses and organizations. By indexing knowledge through vectorization, a variety of knowledge retrieval \u2026"}, {"title": "PraFFL: A Preference-Aware Scheme in Fair Federated Learning", "link": "https://arxiv.org/pdf/2404.08973", "details": "R Ye, M Tang - arXiv preprint arXiv:2404.08973, 2024", "abstract": "Fairness in federated learning has emerged as a critical concern, aiming to develop an unbiased model for any special group (eg, male or female) of sensitive features. However, there is a trade-off between model performance and fairness, ie, improving \u2026"}, {"title": "Source-Aware Training Enables Knowledge Attribution in Language Models", "link": "https://arxiv.org/pdf/2404.01019", "details": "M Khalifa, D Wadden, E Strubell, H Lee, L Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source (s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining \u2026"}, {"title": "Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models", "link": "https://arxiv.org/pdf/2404.02657", "details": "T Wu, C Tao, J Wang, Z Zhao, N Wong - arXiv preprint arXiv:2404.02657, 2024", "abstract": "Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the \u2026"}, {"title": "ASMR: Aggregated Semantic Matching Retrieval Unleashing Commonsense Ability of LLM through Open-Ended Question Answering", "link": "https://proceedings.aaai-make.info/AAAI-MAKE-PREPRINTS-2024/02547-LinP.pdf", "details": "PY Lin, E Chandra, JY Hsu - 2024", "abstract": "Commonsense reasoning refers to the ability to make inferences, draw conclusions, and understand the world based on general knowledge and commonsense. Whether Large Language Models (LLMs) have commonsense reasoning ability remains a \u2026"}]
