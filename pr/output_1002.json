'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HRCL: Hierarchical Relation Contrastive Learning for Low-Res'
[{"title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers", "link": "https://arxiv.org/pdf/2404.18443", "details": "R Xu, W Shi, Y Yu, Y Zhuang, Y Zhu, MD Wang, JC Ho\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Developing effective biomedical retrieval models is important for excelling at knowledge-intensive biomedical tasks but still challenging due to the deficiency of sufficient publicly annotated biomedical data and computational resources. We \u2026"}, {"title": "SERPENT-VLM: Self-Refining Radiology Report Generation Using Vision Language Models", "link": "https://arxiv.org/pdf/2404.17912", "details": "MN Kapadnis, S Patnaik, A Nandy, S Ray, P Goyal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Radiology Report Generation (R2Gen) demonstrates how Multi-modal Large Language Models (MLLMs) can automate the creation of accurate and coherent radiological reports. Existing methods often hallucinate details in text-based reports \u2026"}, {"title": "TSOANet: Time-Sensitive Orthogonal Attention Network for medical event prediction", "link": "https://www.sciencedirect.com/science/article/pii/S0933365724001271", "details": "H Chen, J Zhang, Y Xiang, S Lu, B Tang - Artificial Intelligence in Medicine, 2024", "abstract": "Abstract Medical Event Prediction (MEP) based on Electronic Medical Records (EMR) is an essential and valuable task for healthcare. For a patient, information in the EMR can be organized into a structured sequence, consisting of multiple visits \u2026"}, {"title": "Observation, Analysis, and Solution: Exploring Strong Lightweight Vision Transformers via Masked Image Modeling Pre-Training", "link": "https://arxiv.org/pdf/2404.12210", "details": "J Gao, S Lin, S Wang, Y Kou, Z Li, L Li, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Masked image modeling (MIM) pre-training for large-scale vision transformers (ViTs) in computer vision has enabled promising downstream performance on top of the learned self-supervised ViT features. In this paper, we question if the extremely \u2026"}, {"title": "Empowering Large Language Models for Textual Data Augmentation", "link": "https://arxiv.org/pdf/2404.17642", "details": "Y Li, K Ding, J Wang, K Lee - arXiv preprint arXiv:2404.17642, 2024", "abstract": "With the capabilities of understanding and executing natural language instructions, Large language models (LLMs) can potentially act as a powerful tool for textual data augmentation. However, the quality of augmented data depends heavily on the \u2026"}, {"title": "OBMI: oversampling borderline minority instances by a two-stage Tomek link-finding procedure for class imbalance problem", "link": "https://link.springer.com/article/10.1007/s40747-024-01399-y", "details": "Q Leng, J Guo, J Tao, X Meng, C Wang - Complex & Intelligent Systems, 2024", "abstract": "Mitigating the impact of class imbalance datasets on classifiers poses a challenge to the machine learning community. Conventional classifiers do not perform well as they are habitually biased toward the majority class. Among existing solutions, the \u2026"}, {"title": "NeuroNet: A Novel Hybrid Self-Supervised Learning Framework for Sleep Stage Classification Using Single-Channel EEG", "link": "https://arxiv.org/pdf/2404.17585", "details": "CH Lee, H Kim, H Han, MK Jung, BC Yoon, DJ Kim - arXiv preprint arXiv:2404.17585, 2024", "abstract": "The classification of sleep stages is a pivotal aspect of diagnosing sleep disorders and evaluating sleep quality. However, the conventional manual scoring process, conducted by clinicians, is time-consuming and prone to human bias. Recent \u2026"}, {"title": "CodecLM: Aligning Language Models with Tailored Synthetic Data", "link": "https://arxiv.org/pdf/2404.05875", "details": "Z Wang, CL Li, V Perot, LT Le, J Miao, Z Zhang, CY Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Instruction tuning has emerged as the key in aligning large language models (LLMs) with specific task instructions, thereby mitigating the discrepancy between the next- token prediction objective and users' actual goals. To reduce the labor and time cost \u2026"}, {"title": "Meta In-Context Learning Makes Large Language Models Better Zero and Few-Shot Relation Extractors", "link": "https://arxiv.org/pdf/2404.17807", "details": "G Li, P Wang, J Liu, Y Guo, K Ji, Z Shang, Z Xu - arXiv preprint arXiv:2404.17807, 2024", "abstract": "Relation extraction (RE) is an important task that aims to identify the relationships between entities in texts. While large language models (LLMs) have revealed remarkable in-context learning (ICL) capability for general zero and few-shot \u2026"}]
