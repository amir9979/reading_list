[{"title": "xLAM: A Family of Large Action Models to Empower AI Agent Systems", "link": "https://arxiv.org/pdf/2409.03215", "details": "J Zhang, T Lan, M Zhu, Z Liu, T Hoang, S Kokane\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Autonomous agents powered by large language models (LLMs) have attracted significant research interest. However, the open-source community faces many challenges in developing specialized models for agent tasks, driven by the scarcity of \u2026"}, {"title": "REM: A Ranking-Based Automatic Evaluation Method for LLMs", "link": "https://link.springer.com/chapter/10.1007/978-3-031-72344-5_25", "details": "J Yang, Y Tan, W Hu, Z Yang, X Zhou, Z Luo, W Luo - International Conference on \u2026, 2024", "abstract": "Abstract The emergence of Large Language Models (LLMs) has garnered attention due to their remarkable comprehension and generation capabilities across various language tasks and application scenarios. However, traditional evaluation methods \u2026"}, {"title": "Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications", "link": "https://arxiv.org/pdf/2409.05314", "details": "A Maatouk, KC Ampudia, R Ying, L Tassiulas - arXiv preprint arXiv:2409.05314, 2024", "abstract": "The emergence of large language models (LLMs) has significantly impacted various fields, from natural language processing to sectors like medicine and finance. However, despite their rapid proliferation, the applications of LLMs in \u2026"}, {"title": "PiTe: Pixel-Temporal Alignment for Large Video-Language Model", "link": "https://arxiv.org/pdf/2409.07239", "details": "Y Liu, P Ding, S Huang, M Zhang, H Zhao, D Wang - arXiv preprint arXiv:2409.07239, 2024", "abstract": "Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models (LVLMs) have emerged as a pivotal advancement, bridging the gap between image and text. However, video making it challenging for LVLMs to perform \u2026"}, {"title": "Self-Evolutionary Large Language Models through Uncertainty-Enhanced Preference Optimization", "link": "https://arxiv.org/pdf/2409.11212", "details": "J Wang, Y Zhou, X Zhang, M Bao, P Yan - arXiv preprint arXiv:2409.11212, 2024", "abstract": "Iterative preference optimization has recently become one of the de-facto training paradigms for large language models (LLMs), but the performance is still underwhelming due to too much noisy preference data yielded in the loop. To \u2026"}, {"title": "Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models", "link": "https://arxiv.org/pdf/2409.05486", "details": "C Thorne, C Druckenbrodt, K Szarkowska, D Goyal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The quality and capabilities of large language models cannot be currently fully assessed with automated, benchmark evaluations. Instead, human evaluations that expand on traditional qualitative techniques from natural language generation \u2026"}, {"title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges", "link": "https://arxiv.org/pdf/2409.09927", "details": "V Samuel, Y Zhou, HP Zou - arXiv preprint arXiv:2409.09927, 2024", "abstract": "As large language models achieve increasingly impressive results, questions arise about whether such performance is from generalizability or mere data memorization. Thus, numerous data contamination detection methods have been proposed \u2026"}, {"title": "An Evaluation Framework for Attributed Information Retrieval using Large Language Models", "link": "https://arxiv.org/pdf/2409.08014", "details": "H Djeddal, P Erbacher, R Toukal, L Soulier\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the growing success of Large Language models (LLMs) in information-seeking scenarios, search engines are now adopting generative approaches to provide answers along with in-line citations as attribution. While existing work focuses mainly \u2026"}, {"title": "Towards Building a Robust Knowledge Intensive Question Answering Model with Large Language Models", "link": "https://arxiv.org/pdf/2409.05385", "details": "HX Hong, SY Shao, WZ Wang, DM Duan, J Xiongnan - arXiv preprint arXiv \u2026, 2024", "abstract": "The development of LLMs has greatly enhanced the intelligence and fluency of question answering, while the emergence of retrieval enhancement has enabled models to better utilize external information. However, the presence of noise and \u2026"}]
