[{"title": "Aligning Language Models with the Human World", "link": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi%3Farticle%3D1241%26context%3Ddissertations", "details": "R LIU - 2024", "abstract": "Abstract The field of Natural Language Processing (NLP) has undergone a significant transformation with the emergence of large language models (LMs). These models have enabled the development of human-like conversational \u2026"}, {"title": "Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?", "link": "https://arxiv.org/pdf/2406.16316", "details": "Y Jinnai - arXiv preprint arXiv:2406.16316, 2024", "abstract": "Alignment of the language model with human preferences is a common approach to making a language model useful to end users. However, most alignment work is done in English, and human preference datasets are dominated by English \u2026"}, {"title": "Entropy-Based Decoding for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2406.17519", "details": "Z Qiu, Z Ou, B Wu, J Li, A Liu, I King - arXiv preprint arXiv:2406.17519, 2024", "abstract": "Augmenting Large Language Models (LLMs) with retrieved external knowledge has proven effective for improving the factual accuracy of generated responses. Despite their success, retrieval-augmented LLMs still face the distractibility issue, where the \u2026"}, {"title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models", "link": "https://arxiv.org/pdf/2407.02408", "details": "S Wang, P Wang, T Zhou, Y Dong, Z Tan, J Li - arXiv preprint arXiv:2407.02408, 2024", "abstract": "As Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases \u2026"}, {"title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application", "link": "https://arxiv.org/pdf/2407.01885", "details": "C Yang, W Lu, Y Zhu, Y Wang, Q Chen, C Gao, B Yan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have showcased exceptional capabilities in various domains, attracting significant interest from both academia and industry. Despite their impressive performance, the substantial size and computational demands of LLMs \u2026"}, {"title": "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models", "link": "https://arxiv.org/pdf/2406.19999", "details": "X Chen, B Liao, J Qi, P Eustratiadis, C Monz, A Bisazza\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Following multiple instructions is a crucial ability for large language models (LLMs). Evaluating this ability comes with significant challenges:(i) limited coherence between multiple instructions,(ii) positional bias where the order of instructions \u2026"}, {"title": "Rethinking Entity-level Unlearning for Large Language Models", "link": "https://arxiv.org/pdf/2406.15796", "details": "W Ma, X Feng, W Zhong, L Huang, Y Ye, B Qin - arXiv preprint arXiv:2406.15796, 2024", "abstract": "Large language model unlearning has gained increasing attention due to its potential to mitigate security and privacy concerns. Current research predominantly focuses on Instance-level unlearning, specifically aiming at forgetting predefined \u2026"}, {"title": "Towards Aligning Language Models with Textual Feedback", "link": "https://openreview.net/pdf%3Fid%3DPurTK6zas8", "details": "SA Lloret, S Dhuliawala, K Murugesan, M Sachan - ICML 2024 Workshop on Models of \u2026", "abstract": "We present ALT (ALignment with Textual feedback), an approach that aligns models toward certain user preferences expressed in text. We posit that text allows for an interface for users to provide richer feedback than comparative preferences. In our \u2026"}, {"title": "Paraphrase and Aggregate with Large Language Models for Minimizing Intent Classification Errors", "link": "https://arxiv.org/pdf/2406.17163", "details": "V Yadav, Z Tang, V Srinivasan - arXiv preprint arXiv:2406.17163, 2024", "abstract": "Large language models (LLM) have achieved remarkable success in natural language generation but lesser focus has been given to their applicability in decision making tasks such as classification. We show that LLMs like LLaMa can achieve high \u2026"}]
