[{"title": "Training Language Models on Synthetic Edit Sequences Improves Code Synthesis", "link": "https://arxiv.org/pdf/2410.02749", "details": "U Piterbarg, L Pinto, R Fergus - arXiv preprint arXiv:2410.02749, 2024", "abstract": "Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality \u2026"}, {"title": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models", "link": "https://arxiv.org/pdf/2410.02355", "details": "J Fang, H Jiang, K Wang, Y Ma, X Wang, X He, T Chua - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) often exhibit hallucinations due to incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating \u2026"}, {"title": "3D-CT-GPT: Generating 3D Radiology Reports through Integration of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.19330", "details": "H Chen, W Zhao, Y Li, T Zhong, Y Wang, Y Shang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical image analysis is crucial in modern radiological diagnostics, especially given the exponential growth in medical imaging data. The demand for automated report generation systems has become increasingly urgent. While prior research has \u2026"}, {"title": "Investigating the Impact of Model Complexity in Large Language Models", "link": "https://arxiv.org/pdf/2410.00699", "details": "J Luo, H Wang, W Huang - arXiv preprint arXiv:2410.00699, 2024", "abstract": "Large Language Models (LLMs) based on the pre-trained fine-tuning paradigm have become pivotal in solving natural language processing tasks, consistently achieving state-of-the-art performance. Nevertheless, the theoretical understanding of how \u2026"}, {"title": "PALM: Few-Shot Prompt Learning for Audio Language Models", "link": "https://arxiv.org/pdf/2409.19806", "details": "A Hanif, MT Agro, MA Qazi, H Aldarmaki - arXiv preprint arXiv:2409.19806, 2024", "abstract": "Audio-Language Models (ALMs) have recently achieved remarkable success in zero- shot audio recognition tasks, which match features of audio waveforms with class- specific text prompt features, inspired by advancements in Vision-Language Models \u2026"}, {"title": "INSIGHTBUDDY-AI: Medication Extraction and Entity Linking using Large Language Models and Ensemble Learning", "link": "https://arxiv.org/pdf/2409.19467", "details": "P Romero, L Han, G Nenadic - arXiv preprint arXiv:2409.19467, 2024", "abstract": "Medication Extraction and Mining play an important role in healthcare NLP research due to its practical applications in hospital settings, such as their mapping into standard clinical knowledge bases (SNOMED-CT, BNF, etc.). In this work, we \u2026"}, {"title": "Ascle\u2014A Python Natural Language Processing Toolkit for Medical Text Generation: Development and Evaluation Study", "link": "https://www.jmir.org/2024/1/e60601/", "details": "R Yang, Q Zeng, K You, Y Qiao, L Huang, CC Hsieh\u2026 - Journal of Medical Internet \u2026, 2024", "abstract": "Background Medical texts present significant domain-specific challenges, and manually curating these texts is a time-consuming and labor-intensive process. To address this, natural language processing (NLP) algorithms have been developed to \u2026"}, {"title": "Clinical Notes as Narratives: Implications for Large Language Models in Healthcare", "link": "https://link.springer.com/article/10.1007/s11606-024-09093-y", "details": "TD Brender, LA Celi, JM Cobert - Journal of General Internal Medicine, 2024", "abstract": "OpenAI's ChatGPT sparked tremendous excitement regarding potential healthcare applications of large language models (LLM). LLMs trained on electronic health record (EHR) notes could enrich the feature space for many tasks including risk \u2026"}, {"title": "Pretrained Patient Trajectories for Adverse Drug Event Prediction Using Common Data Model-based Electronic Health Records", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/09/30/2024.09.30.24314595.full.pdf", "details": "J Kim, JS Kim, JH Lee, MG Kim, T Kim, C Cho, RW Park\u2026 - medRxiv, 2024", "abstract": "Pretraining electronic health record (EHR) data through language models by treating patient trajectories as natural language sentences has improved various medical tasks. However, EHR pretraining models have never been utilized in adverse drug \u2026"}]
