'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Investigating Regularization of Self-Play Language Mod'
[{"title": "Emergent Abilities in Reduced-Scale Generative Language Models", "link": "https://arxiv.org/pdf/2404.02204", "details": "S Muckatira, V Deshpande, V Lialin, A Rumshisky - arXiv preprint arXiv:2404.02204, 2024", "abstract": "Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study \u2026"}, {"title": "FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS By admin No Comments", "link": "https://youraisales.com/finetuned-language-models-are-zero-shot-learners-2/", "details": "J Wei, M Bosma, VY Zhao, K Guu, AW Yu, B Lester\u2026", "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning\u2014\ufb01netuning language models on a collection of datasets described via instructions\u2014substantially improves zero-shot \u2026"}, {"title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2404.05567", "details": "B Pan, Y Shen, H Liu, M Mishra, G Zhang, A Oliva\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) language models can reduce computational costs by 2- 4$\\times $ compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios. However, MoE models generally \u2026"}, {"title": "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models", "link": "https://arxiv.org/pdf/2404.02575", "details": "H Chae, Y Kim, S Kim, KT Ong, B Kwak, M Kim, S Kim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution. Such nature of algorithmic reasoning makes it a challenge for large \u2026"}, {"title": "Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning", "link": "https://arxiv.org/pdf/2404.00909", "details": "R Li, Y Wu, X He - arXiv preprint arXiv:2404.00909, 2024", "abstract": "Generative vision-language models (VLMs) have shown impressive performance in zero-shot vision-language tasks like image captioning and visual question answering. However, improving their zero-shot reasoning typically requires second \u2026"}, {"title": "MAP: Model Aggregation and Personalization in Federated Learning with Incomplete Classes", "link": "https://arxiv.org/pdf/2404.09232", "details": "XC Li, S Song, Y Li, B Li, Y Shao, Y Yang, DC Zhan - IEEE Transactions on \u2026, 2024", "abstract": "In some real-world applications, data samples are usually distributed on local devices, where federated learning (FL) techniques are proposed to coordinate decentralized clients without directly sharing users' private data. FL commonly \u2026"}, {"title": "Multi-task Contrastive Learning for Anomaly Detection on Attributed Networks", "link": "https://link.springer.com/chapter/10.1007/978-981-97-2242-6_2", "details": "J Zhang, Y Ding - Pacific-Asia Conference on Knowledge Discovery and \u2026, 2024", "abstract": "Anomaly detection on attributed networks is a vital task in graph data mining and has been widely applied in many real-world scenarios. Despite the promising performance, existing contrastive learning-based anomaly detection models still \u2026"}, {"title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning", "link": "https://arxiv.org/pdf/2404.17140", "details": "Y Zhang, M Khalifa, L Logeswaran, J Kim, M Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors. This work explores whether \u2026"}, {"title": "FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations", "link": "https://arxiv.org/pdf/2404.06619", "details": "J Dwivedi-Yu, R Dwivedi, T Schick - arXiv preprint arXiv:2404.06619, 2024", "abstract": "The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes \u2026"}]
