[{"title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.10139", "details": "P Xia, S Han, S Qiu, Y Zhou, Z Wang, W Zheng, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this \u2026"}, {"title": "Prompt tuning discriminative language models for hierarchical text classification", "link": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/50E5499348A0E72F0C4F3AFC622133A7/S2977042424000517a.pdf/div-class-title-prompt-tuning-discriminative-language-models-for-hierarchical-text-classification-div.pdf", "details": "J du Toit, M Dunaiski - Natural Language Processing", "abstract": "Hierarchical text classification (HTC) is a natural language processing task which aims to categorise a text document into a set of classes from a hierarchical class structure. Recent approaches to solve HTC tasks focus on leveraging pre-trained \u2026"}, {"title": "Automated real-world data integration improves cancer outcome prediction", "link": "https://www.nature.com/articles/s41586-024-08167-5", "details": "J Jee, C Fong, K Pichotta, TN Tran, A Luthra, M Waters\u2026 - Nature, 2024", "abstract": "The digitization of health records and growing availability of tumour DNA sequencing provide an opportunity to study the determinants of cancer outcomes with unprecedented richness. Patient data are often stored in unstructured text and siloed \u2026"}, {"title": "Thank You, Stingray: Multilingual Large Language Models Can Not (Yet) Disambiguate Cross-Lingual Word Sense", "link": "https://arxiv.org/pdf/2410.21573", "details": "S Cahyawijaya, R Zhang, H Lovenia, JCB Cruz\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multilingual large language models (LLMs) have gained prominence, but concerns arise regarding their reliability beyond English. This study addresses the gap in cross- lingual semantic evaluation by introducing a novel benchmark for cross-lingual \u2026"}, {"title": "Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models", "link": "https://arxiv.org/pdf/2410.12790%3F", "details": "C Zhang, S Stepputtis, K Sycara, Y Xie - arXiv preprint arXiv:2410.12790, 2024", "abstract": "Test-time adaptation, which enables models to generalize to diverse data with unlabeled test samples, holds significant value in real-world scenarios. Recently, researchers have applied this setting to advanced pre-trained vision-language \u2026"}, {"title": "Tuning Language Models by Mixture-of-Depths Ensemble", "link": "https://arxiv.org/pdf/2410.13077", "details": "H Luo, L Specia - arXiv preprint arXiv:2410.13077, 2024", "abstract": "Transformer-based Large Language Models (LLMs) traditionally rely on final-layer loss for training and final-layer representations for predictions, potentially overlooking the predictive power embedded in intermediate layers. Surprisingly, we \u2026"}, {"title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition", "link": "https://arxiv.org/pdf/2410.13259", "details": "Q Yang, P Wang, LD Plonsky, FL Oswald, H Chen - arXiv preprint arXiv:2410.13259, 2024", "abstract": "We examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of \u2026"}, {"title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures", "link": "https://arxiv.org/pdf/2410.07698", "details": "Y Chen, Y Zhang, L Cao, K Yuan, Z Wen - arXiv preprint arXiv:2410.07698, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs when adapting large language models (LLMs) for downstream applications. However, traditional first-order (FO) fine-tuning algorithms incur substantial memory overhead \u2026"}, {"title": "CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-73116-7_23.pdf", "details": "G Tzimiropoulos", "abstract": "Despite recent successes, LVLMs or Large Vision Language Models are prone to hallucinating details like objects and their properties or relations, limiting their real- world deployment. To address this and improve their robustness, we present CLIP \u2026"}]
