[{"title": "$ S^ 3$: Synonymous Semantic Space for Improving Zero-Shot Generalization of Vision-Language Models", "link": "https://arxiv.org/pdf/2412.04925", "details": "X Yin, Q Wang, B Cao, Q Hu - arXiv preprint arXiv:2412.04925, 2024", "abstract": "Recently, many studies have been conducted to enhance the zero-shot generalization ability of vision-language models (eg, CLIP) by addressing the semantic misalignment between image and text embeddings in downstream tasks \u2026"}, {"title": "Liquid: Language Models are Scalable Multi-modal Generators", "link": "https://arxiv.org/pdf/2412.04332", "details": "J Wu, Y Jiang, C Ma, Y Liu, H Zhao, Z Yuan, S Bai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Liquid, an auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared \u2026"}, {"title": "Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.06775", "details": "YL Lee, YH Tsai, WC Chiu - arXiv preprint arXiv:2412.06775, 2024", "abstract": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To \u2026"}, {"title": "The Transformative Potential of Large Language Models in Mining Electronic Health Records Data: Content Analysis", "link": "https://medinform.jmir.org/2025/1/e58457", "details": "AJ Wals Zurita, H Miras del Rio\u2026 - JMIR Medical Informatics, 2025", "abstract": "Background In this study, we evaluate the accuracy, efficiency, and cost- effectiveness of large language models in extracting and structuring information from free-text clinical reports, particularly in identifying and classifying patient \u2026"}, {"title": "Contrastive concept-phrase pre-training for generating clinically accurate and interpretable chest X-ray reports", "link": "https://link.springer.com/article/10.1007/s00521-024-10640-1", "details": "A Tubaishat, T Zia, D Windridge, M Nawaz, S Razzaq - Neural Computing and \u2026, 2024", "abstract": "Automated radiology report generation is an emerging field for improving patient care and alleviating radiologist workload. However, existing methods face a range of challenges such as limited data availability, clinical metric performance, and \u2026"}, {"title": "Do Large Language Models have Shared Weaknesses in Medical Question Answering?", "link": "https://openreview.net/pdf%3Fid%3DZjQ04tsRQl", "details": "AM Bean, K Korgul, F Krones, R McCraith, A Mahdi - Advancements In Medical \u2026, 2024", "abstract": "Large language models (LLMs) have made rapid improvement on medical benchmarks, but their unreliability remains a persistent challenge for safe real-world uses. To design for the use LLMs as a category, rather than for specific models \u2026"}, {"title": "An evaluation framework for clinical use of large language models in patient interaction tasks", "link": "https://www.nature.com/articles/s41591-024-03328-5", "details": "S Johri, J Jeong, BA Tran, DI Schlessinger\u2026 - Nature Medicine, 2025", "abstract": "The integration of large language models (LLMs) into clinical diagnostics has the potential to transform doctor\u2013patient interactions. However, the readiness of these models for real-world clinical application remains inadequately tested. This paper \u2026"}, {"title": "Training large language models to reason in a continuous latent space", "link": "https://arxiv.org/pdf/2412.06769%3F", "details": "S Hao, S Sukhbaatar, DJ Su, X Li, Z Hu, J Weston\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) are restricted to reason in the\" language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may \u2026"}, {"title": "BenCzechMark: A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism", "link": "https://arxiv.org/pdf/2412.17933", "details": "M Fajcik, M Docekal, J Dolezal, K Ondrej, K Bene\u0161\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present BenCzechMark (BCM), the first comprehensive Czech language benchmark designed for large language models, offering diverse tasks, multiple task formats, and multiple evaluation metrics. Its scoring system is grounded in statistical \u2026"}]
