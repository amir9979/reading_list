[{"title": "Two-Stage Medical Image-Text Transfer with Supervised Contrastive Learning", "link": "https://link.springer.com/chapter/10.1007/978-3-031-72353-7_32", "details": "X Wang, S Yin, Y Wang, J Li, S Li - International Conference on Artificial Neural \u2026, 2024", "abstract": "Cross-modal translation enables automatic information transformation across different modalities, including images, text, and speech, enabling various applications in the medical domain, such as medical image description and Medical \u2026"}, {"title": "ShapeMamba-EM: Fine-Tuning Foundation Model with Local Shape Descriptors and Mamba Blocks for 3D EM Image Segmentation", "link": "https://arxiv.org/pdf/2408.14114", "details": "R Shi, Q Pang, L Ma, L Duan, T Huang, T Jiang - arXiv preprint arXiv:2408.14114, 2024", "abstract": "Electron microscopy (EM) imaging offers unparalleled resolution for analyzing neural tissues, crucial for uncovering the intricacies of synaptic connections and neural processes fundamental to understanding behavioral mechanisms. Recently, the \u2026"}, {"title": "Robust image representations with counterfactual contrastive learning", "link": "https://arxiv.org/pdf/2409.10365", "details": "M Roschewitz, FDS Ribeiro, T Xia, G Khara, B Glocker - arXiv preprint arXiv \u2026, 2024", "abstract": "Contrastive pretraining can substantially increase model generalisation and downstream performance. However, the quality of the learned representations is highly dependent on the data augmentation strategy applied to generate positive \u2026"}, {"title": "Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic", "link": "https://arxiv.org/pdf/2408.16326", "details": "X Zheng, J Lou, B Cao, X Wen, Y Ji, H Lin, Y Lu, X Han\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-critic has become an important mechanism for enhancing the reasoning performance of LLMs. However, current approaches mainly involve basic prompts without further training, which tend to be over-simplified, leading to limited accuracy \u2026"}, {"title": "The more quality information the better: Hierarchical generation of multi-evidence alignment and fusion model for multimodal entity and relation extraction", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324002346", "details": "X He, S Li, Y Zhang, B Li, S Xu, Y Zhou - Information Processing & Management, 2025", "abstract": "Abstract Multimodal Entity and Relation Extraction (MERE) encompasses tasks, including Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction (MRE), aiming to extract valuable information from environments rich in \u2026"}, {"title": "PL-TTS: A Generalizable Prompt-based Diffusion TTS Augmented by Large Language Model", "link": "https://www.isca-archive.org/interspeech_2024/li24y_interspeech.pdf", "details": "S Li, Q Mao, J Shi", "abstract": "With the increasing demand for style-controlled speech synthesis, traditional TTS methods for controlling acoustic features clearly have significant limitations. Therefore, using text style descriptions to achieve style-controlled TTS has become a \u2026"}, {"title": "ConDense: Consistent 2D/3D Pre-training for Dense and Sparse Features from Multi-View Images", "link": "https://arxiv.org/pdf/2408.17027", "details": "X Zhang, Z Wang, H Zhou, S Ghosh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To advance the state of the art in the creation of 3D foundation models, this paper introduces the ConDense framework for 3D pre-training utilizing existing pre-trained 2D networks and large-scale multi-view datasets. We propose a novel 2D-3D joint \u2026"}, {"title": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models", "link": "https://arxiv.org/pdf/2408.15915", "details": "Y Yang, Y Qin, T Wu, Z Xu, G Li, P Guo, H Shao, Y Shi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The cultivation of expertise for large language models (LLMs) to solve tasks of specific areas often requires special-purpose tuning with calibrated behaviors on the expected stable outputs. To avoid huge cost brought by manual preparation of \u2026"}]
