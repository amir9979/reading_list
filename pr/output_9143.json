[{"title": "Exploration of an intrinsically explainable self-attention based model for prototype generation on single-channel EEG sleep stage classification", "link": "https://www.nature.com/articles/s41598-024-79139-y", "details": "B Adey, A Habib, C Karmakar - Scientific Reports, 2024", "abstract": "Prototype-based methods in deep learning offer interpretable explanations for decisions by comparing inputs to typical representatives in the data. This study explores the adaptation of SESM, a self-attention-based prototype method successful \u2026"}, {"title": "HIST-AID: Leveraging Historical Patient Reports for Enhanced Multi-Modal Automatic Diagnosis", "link": "https://arxiv.org/pdf/2411.10684", "details": "H Huang, CM Deniz, K Cho, S Chopra, D Madaan - arXiv preprint arXiv:2411.10684, 2024", "abstract": "Chest X-ray imaging is a widely accessible and non-invasive diagnostic tool for detecting thoracic abnormalities. While numerous AI models assist radiologists in interpreting these images, most overlook patients' historical data. To bridge this gap \u2026"}, {"title": "Adversarial Attacks on Large Language Models Using Regularized Relaxation", "link": "https://arxiv.org/pdf/2410.19160", "details": "SJ Chacko, S Biswas, CM Islam, FT Liza, X Liu - arXiv preprint arXiv:2410.19160, 2024", "abstract": "As powerful Large Language Models (LLMs) are now widely used for numerous practical applications, their safety is of critical importance. While alignment techniques have significantly improved overall safety, LLMs remain vulnerable to \u2026"}, {"title": "Efficient Federated Unlearning with Adaptive Differential Privacy Preservation", "link": "https://arxiv.org/abs/2411.11044", "details": "Y Jiang, X Tong, Z Liu, H Ye, CW Tan, KY Lam - arXiv preprint arXiv:2411.11044, 2024", "abstract": "Federated unlearning (FU) offers a promising solution to effectively address the need to erase the impact of specific clients' data on the global model in federated learning (FL), thereby granting individuals the``Right to be Forgotten\". The most \u2026"}, {"title": "Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "link": "https://aclanthology.org/2024.findings-emnlp.78.pdf", "details": "H Hwang, D Kim, S Kim, S Ye, M Seo - Findings of the Association for Computational \u2026, 2024", "abstract": "Training on large amounts of rationales (ie, CoT Fine-tuning) has been found effective for improving mathematical reasoning of large language models (LLMs). However, acquiring human-authored solutions or augmenting rationales from \u2026"}, {"title": "Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models", "link": "https://arxiv.org/pdf/2411.00492", "details": "DX Long, DN Yen, AT Luu, K Kawaguchi, MY Kan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple \u2026"}]
