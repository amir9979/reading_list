[{"title": "RAP: Retrieval-Augmented Personalization for Multimodal **Large Language Models**", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Hao_RAP_Retrieval-Augmented_Personalization_for_Multimodal_Large_Language_Models_CVPR_2025_paper.pdf", "details": "H Hao, J Han, C Li, YF Li, X Yue - Proceedings of the Computer Vision and Pattern \u2026, 2025", "abstract": "\u2026 This **evaluation** process is repeated three times with different seeds, resulting in a total of 1,182 images used for **evaluation** , and we report the average results. Qualitative Comparison. In Table 2, we present image captions generated by \u2026"}, {"title": "Bridging gait recognition and **large language models** sequence modeling", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Bridging_Gait_Recognition_and_Large_Language_Models_Sequence_Modeling_CVPR_2025_paper.pdf", "details": "S Yang, J Wang, S Hou, X Liu, C Cao, L Wang\u2026 - Proceedings of the \u2026, 2025", "abstract": "\u2026 and **evaluate** GaitLLM on four diverse public gait recognition datasets: Gait3D [36], GREW [39], CCPG [15], and SUSTech1K [28], covering a wide range of real-world conditions. We adhere to the official **evaluation** \u2026 (CL), making it suitable for \u2026"}, {"title": "**Evaluating** the capacity of **large language models** to interpret emotions in images", "link": "https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0324127", "details": "H Alrasheed, A Alghihab, A Pentland, S Alghowinem - PloS one, 2025", "abstract": "\u2026 The primary goal of this study is to **evaluate** the accuracy of **Large** **Language** **Models** (LLMs), particularly GPT-4, in perceiving emotions in images and their textual descriptions, specifically across the dimensions of valence and arousal. We \u2026"}, {"title": "Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation", "link": "https://arxiv.org/pdf/2506.02648", "details": "Y Yang, MK Chen, Q Liu, M Hu, Q Chen, G Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 In this section, we **evaluate** state-of-the-art **large** **language** **models** and investigate the following research questions through experimental results: i) How do current LLMs perform in abstract reasoning across different cognitive levels? (Section \u2026", "entry_id": "http://arxiv.org/abs/2506.02648v1", "updated": "2025-06-03 09:01:08", "published": "2025-06-03 09:01:08", "authors": "Yue Yang;MingKang Chen;Qihua Liu;Mengkang Hu;Qiguang Chen;Gengrui Zhang;Shuyue Hu;Guangtao Zhai;Yu Qiao;Yu Wang;Wenqi Shao;Ping Luo", "summary": "Recent advances in large language models (LLMs) have demonstrated impressive\nreasoning capacities that mirror human-like thinking. However, whether LLMs\npossess genuine fluid intelligence (i.e., the ability to reason abstractly and\ngeneralize rules in novel situations) remains an open question. Existing\nreasoning benchmarks either focus on domain-specific knowledge (crystallized\nintelligence) or lack interpretability. To address these limitations, we\npropose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a\nhierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning\ntasks organized across four cognitive levels, with each task featuring multiple\ndynamic variants that test the same underlying latent rule. This design enables\nfine-grained, interpretable, and reliable assessments of fluid intelligence. We\nevaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o,\nClaude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1).\nExperimental results reveal that although most LLMs achieve competent and\nrobust performance in low-level cognition, they struggle with high-level\ncognition and exhibit limited generalization as task complexity grows. Our\nfindings highlight the gap between current LLMs and true human-like fluid\nintelligence and offer a new path for systematically tracking reasoning\nprogress in LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2506.02648v1;http://arxiv.org/pdf/2506.02648v1", "pdf_url": "http://arxiv.org/pdf/2506.02648v1"}, {"title": "SocialEval: Evaluating Social Intelligence of Large Language Models", "link": "https://arxiv.org/pdf/2506.00900", "details": "J Zhou, Y Chen, Y Shi, X Zhang, L Lei, Y Feng, Z Xiong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Upon it, we perform outcome-oriented goal achievement **evaluation** and process-oriented interpersonal ability **evaluation** of both LLMs and humans. Results reveal that LLMs fall behind humans in both SI **evaluations** , while both of them exhibit crosslinguistic \u2026", "entry_id": "http://arxiv.org/abs/2506.00900v1", "updated": "2025-06-01 08:36:51", "published": "2025-06-01 08:36:51", "authors": "Jinfeng Zhou;Yuxuan Chen;Yihan Shi;Xuanming Zhang;Leqi Lei;Yi Feng;Zexuan Xiong;Miao Yan;Xunzhi Wang;Yaru Cao;Jianing Yin;Shuai Wang;Quanyu Dai;Zhenhua Dong;Hongning Wang;Minlie Huang", "summary": "LLMs exhibit promising Social Intelligence (SI) in modeling human behavior,\nraising the need to evaluate LLMs' SI and their discrepancy with humans. SI\nequips humans with interpersonal abilities to behave wisely in navigating\nsocial interactions to achieve social goals. This presents an operational\nevaluation paradigm: outcome-oriented goal achievement evaluation and\nprocess-oriented interpersonal ability evaluation, which existing work fails to\naddress. To this end, we propose SocialEval, a script-based bilingual SI\nbenchmark, integrating outcome- and process-oriented evaluation by manually\ncrafting narrative scripts. Each script is structured as a world tree that\ncontains plot lines driven by interpersonal ability, providing a comprehensive\nview of how LLMs navigate social interactions. Experiments show that LLMs fall\nbehind humans on both SI evaluations, exhibit prosociality, and prefer more\npositive social behaviors, even if they lead to goal failure. Analysis of LLMs'\nformed representation space and neuronal activations reveals that LLMs have\ndeveloped ability-specific functional partitions akin to the human brain.", "comment": "ACL 2025, Repository: \\url{https://github.com/thu-coai/SocialEval}", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.00900v1;http://arxiv.org/pdf/2506.00900v1", "pdf_url": "http://arxiv.org/pdf/2506.00900v1"}, {"title": "NavBench: Probing Multimodal Large Language Models for Embodied Navigation", "link": "https://arxiv.org/pdf/2506.01031", "details": "Y Qiao, H Hong, W Lyu, D An, S Zhang, Y Xie, X Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 To ensure a fair and standardized **evaluation** protocol, we **evaluate** MLLMs via viewpoint selection rather than low-level action prediction (eg, turning or moving forward). This abstraction, consistent with prior embodied navigation benchmarks [9 \u2026", "entry_id": "http://arxiv.org/abs/2506.01031v1", "updated": "2025-06-01 14:21:02", "published": "2025-06-01 14:21:02", "authors": "Yanyuan Qiao;Haodong Hong;Wenqi Lyu;Dong An;Siqi Zhang;Yutong Xie;Xinyu Wang;Qi Wu", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated strong\ngeneralization in vision-language tasks, yet their ability to understand and\nact within embodied environments remains underexplored. We present NavBench, a\nbenchmark to evaluate the embodied navigation capabilities of MLLMs under\nzero-shot settings. NavBench consists of two components: (1) navigation\ncomprehension, assessed through three cognitively grounded tasks including\nglobal instruction alignment, temporal progress estimation, and local\nobservation-action reasoning, covering 3,200 question-answer pairs; and (2)\nstep-by-step execution in 432 episodes across 72 indoor scenes, stratified by\nspatial, cognitive, and execution complexity. To support real-world deployment,\nwe introduce a pipeline that converts MLLMs' outputs into robotic actions. We\nevaluate both proprietary and open-source models, finding that GPT-4o performs\nwell across tasks, while lighter open-source models succeed in simpler cases.\nResults also show that models with higher comprehension scores tend to achieve\nbetter execution performance. Providing map-based context improves decision\naccuracy, especially in medium-difficulty scenarios. However, most models\nstruggle with temporal understanding, particularly in estimating progress\nduring navigation, which may pose a key challenge.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.01031v1;http://arxiv.org/pdf/2506.01031v1", "pdf_url": "http://arxiv.org/pdf/2506.01031v1"}, {"title": "Performance **evaluation** of **large language models** in pediatric nephrology clinical decision support: a comprehensive assessment", "link": "https://link.springer.com/article/10.1007/s00467-025-06819-w", "details": "O Niel, D Dookhun, A Caliment - Pediatric Nephrology, 2025", "abstract": "\u2026 the conceptualization of artificial intelligence (AI) in 1950 and the subsequent evolution of machine learning methodologies\u2014particularly deep learning with its enhanced multilayer connectivity capabilities\u2014we are now witnessing the \u2026"}, {"title": "The State of Large Language Models for African Languages: Progress and Challenges", "link": "https://arxiv.org/pdf/2506.02280", "details": "KY Hussen, WT Sewunetie, AA Ayele, SH Imam\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 This study presents a three-stage review to **evaluate** LLMs\u2019 current status, challenges, and prospects for African languages. The first stage investigates both commercial and open-source LLMs models with more than 7 billion parameters \u2026", "entry_id": "http://arxiv.org/abs/2506.02280v1", "updated": "2025-06-02 21:39:40", "published": "2025-06-02 21:39:40", "authors": "Kedir Yassin Hussen;Walelign Tewabe Sewunetie;Abinew Ali Ayele;Sukairaj Hafiz Imam;Shamsuddeen Hassan Muhammad;Seid Muhie Yimam", "summary": "Large Language Models (LLMs) are transforming Natural Language Processing\n(NLP), but their benefits are largely absent for Africa's 2,000 low-resource\nlanguages. This paper comparatively analyzes African language coverage across\nsix LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).\nThe evaluation covers language coverage, training sets, technical limitations,\nscript problems, and language modelling roadmaps. The work identifies 42\nsupported African languages and 23 available public data sets, and it shows a\nbig gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are\nalways treated while there is over 98\\% of unsupported African languages.\nMoreover, the review shows that just Latin, Arabic, and Ge'ez scripts are\nidentified while 20 active scripts are neglected. Some of the primary\nchallenges are lack of data, tokenization biases, computational costs being\nvery high, and evaluation issues. These issues demand language standardization,\ncorpus development by the community, and effective adaptation methods for\nAfrican languages.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2506.02280v1;http://arxiv.org/pdf/2506.02280v1", "pdf_url": "http://arxiv.org/pdf/2506.02280v1"}, {"title": "TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2506.01341", "details": "Y Zhang, M Wang, X Li, K Ren, C Zhu, U Naseem - arXiv preprint arXiv:2506.01341, 2025", "abstract": "\u2026 To comprehensively explore the limitations of current **large** **language** **models** (LLMs) in multi-turn and multi-step reasoning tasks, we selected both commercial and widely-used open-source models for **evaluation** , employing different prompting methods. The \u2026", "entry_id": "http://arxiv.org/abs/2506.01341v1", "updated": "2025-06-02 05:47:50", "published": "2025-06-02 05:47:50", "authors": "Yiran Zhang;Mo Wang;Xiaoyang Li;Kaixuan Ren;Chencheng Zhu;Usman Naseem", "summary": "Despite impressive advances in large language models (LLMs), existing\nbenchmarks often focus on single-turn or single-step tasks, failing to capture\nthe kind of iterative reasoning required in real-world settings. To address\nthis limitation, we introduce TurnBench, a novel benchmark that evaluates\nmulti-turn, multi-step reasoning through an interactive code-breaking task\ninspired by a \"Turing Machine Board Game.\" In each episode, a model must\nuncover hidden logical or arithmetic rules by making sequential guesses,\nreceiving structured feedback, and integrating clues across multiple rounds.\nThis dynamic setup requires models to reason over time, adapt based on past\ninformation, and maintain consistency across steps-capabilities underexplored\nin current benchmarks. TurnBench includes two modes: Classic, which tests\nstandard reasoning, and Nightmare, which introduces increased complexity and\nrequires robust inferential chains. To support fine-grained analysis, we\nprovide ground-truth annotations for intermediate reasoning steps. Our\nevaluation of state-of-the-art LLMs reveals significant gaps: the best model\nachieves 81.5% accuracy in Classic mode, but performance drops to 17.8% in\nNightmare mode. In contrast, human participants achieve 100% in both,\nunderscoring the challenge TurnBench poses to current models. By incorporating\nfeedback loops and hiding task rules, TurnBench reduces contamination risks and\nprovides a rigorous testbed for diagnosing and advancing multi-step, multi-turn\nreasoning in LLMs.", "comment": "Preprint", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.01341v1;http://arxiv.org/pdf/2506.01341v1", "pdf_url": "http://arxiv.org/pdf/2506.01341v1"}]
