[{"title": "AntLM: Bridging Causal and Masked Language Models", "link": "https://arxiv.org/pdf/2412.03275", "details": "X Yu, B Guo, S Luo, J Wang, T Ji, Y Wu - arXiv preprint arXiv:2412.03275, 2024", "abstract": "Causal Language Modeling (CLM) and Masked Language Modeling (MLM) are two mainstream learning paradigms based on Transformer networks, specifically the Decoder-only and Encoder-only architectures. The strengths of each paradigm in \u2026"}, {"title": "Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.06775", "details": "YL Lee, YH Tsai, WC Chiu - arXiv preprint arXiv:2412.06775, 2024", "abstract": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To \u2026"}, {"title": "BAMBA: A Bimodal Adversarial Multi-Round Black-Box Jailbreak Attacker for LVLMs", "link": "https://arxiv.org/pdf/2412.05892", "details": "R Cheng, Y Ding, S Cao, S Yuan, Z Wang, X Jia - arXiv preprint arXiv:2412.05892, 2024", "abstract": "LVLMs are widely used but vulnerable to illegal or unethical responses under jailbreak attacks. To ensure their responsible deployment in real-world applications, it is essential to understand their vulnerabilities. There are four main issues in current \u2026"}, {"title": "Exploring Multi-Grained Concept Annotations for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2412.05939", "details": "X Xu, T Niu, Y Xie, L Qin, W Che, MY Kan - arXiv preprint arXiv:2412.05939, 2024", "abstract": "Multimodal Large Language Models (MLLMs) excel in vision--language tasks by pre- training solely on coarse-grained concept annotations (eg, image captions). We hypothesize that integrating fine-grained concept annotations (eg, object labels and \u2026"}, {"title": "Accelerating Multimodel Large Language Models by Searching Optimal Vision Token Reduction", "link": "https://arxiv.org/pdf/2412.00556", "details": "S Zhao, Z Wang, F Juefei-Xu, X Xia, M Liu, X Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Prevailing Multimodal Large Language Models (MLLMs) encode the input image (s) as vision tokens and feed them into the language backbone, similar to how Large Language Models (LLMs) process the text tokens. However, the number of vision \u2026"}, {"title": "Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective", "link": "https://arxiv.org/pdf/2412.00554", "details": "Y Zhou, B Di Eugenio, L Cheng - arXiv preprint arXiv:2412.00554, 2024", "abstract": "This paper studies the performance of large language models (LLMs), particularly regarding demographic fairness, in solving real-world healthcare tasks. We evaluate state-of-the-art LLMs with three prevalent learning frameworks across six diverse \u2026"}, {"title": "A Practical Examination of AI-Generated Text Detectors for Large Language Models", "link": "https://arxiv.org/pdf/2412.05139", "details": "B Tufts, X Zhao, L Li - arXiv preprint arXiv:2412.05139, 2024", "abstract": "The proliferation of large language models has raised growing concerns about their misuse, particularly in cases where AI-generated text is falsely attributed to human authors. Machine-generated content detectors claim to effectively identify such text \u2026"}, {"title": "CNNSum: Exploring Long-Conext Summarization with Large Language Models in Chinese Novels", "link": "https://arxiv.org/pdf/2412.02819", "details": "L Wei, H Yan, X Lu, J Zhu, J Wang, W Zhang - arXiv preprint arXiv:2412.02819, 2024", "abstract": "Large Language Models (LLMs) have been well-researched in many long-context tasks. However, due to high annotation costs, high-quality long-context summary datasets for training or evaluation are scarce, limiting further research. In this work \u2026"}, {"title": "Training Large Language Models to Reason in a Continuous Latent Space", "link": "https://arxiv.org/pdf/2412.06769", "details": "S Hao, S Sukhbaatar, DJ Su, X Li, Z Hu, J Weston\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) are restricted to reason in the\" language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may \u2026"}]
