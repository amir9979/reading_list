'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Can only LLMs do Reasoning?: Potential of Small Langua'
[{"title": "CodecLM: Aligning Language Models with Tailored Synthetic Data", "link": "https://arxiv.org/pdf/2404.05875", "details": "Z Wang, CL Li, V Perot, LT Le, J Miao, Z Zhang, CY Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Instruction tuning has emerged as the key in aligning large language models (LLMs) with specific task instructions, thereby mitigating the discrepancy between the next- token prediction objective and users' actual goals. To reduce the labor and time cost \u2026"}, {"title": "Causal Evaluation of Language Models", "link": "https://arxiv.org/pdf/2405.00622", "details": "S Chen, B Peng, M Chen, R Wang, M Xu, X Zeng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for \u2026"}, {"title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies", "link": "https://arxiv.org/pdf/2404.06395", "details": "S Hu, Y Tu, X Han, C He, G Cui, X Long, Z Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This \u2026"}, {"title": "Investigating Regularization of Self-Play Language Models", "link": "https://arxiv.org/pdf/2404.04291", "details": "R Alami, A Abubaker, M Achab, MEA Seddik, S Lahlou - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper explores the effects of various forms of regularization in the context of language model alignment via self-play. While both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) require to collect \u2026"}, {"title": "Refining Pre-trained Language Models for Domain Adaptation with Entity-Aware Discriminative and Contrastive Learning", "link": "https://epubs.siam.org/doi/pdf/10.1137/1.9781611978032.48", "details": "J Yang, X Hu, Y Shen, G xiao - Proceedings of the 2024 SIAM International \u2026, 2024", "abstract": "With the rapid advancement of pre-trained language models (PLMs), the adaptation of these models to specialized domains has emerged as an essential area of research. However, PLMs encounter substantial challenges when deployed in highly \u2026"}, {"title": "FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations", "link": "https://arxiv.org/pdf/2404.06619", "details": "J Dwivedi-Yu, R Dwivedi, T Schick - arXiv preprint arXiv:2404.06619, 2024", "abstract": "The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes \u2026"}, {"title": "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models", "link": "https://arxiv.org/pdf/2404.02575", "details": "H Chae, Y Kim, S Kim, KT Ong, B Kwak, M Kim, S Kim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution. Such nature of algorithmic reasoning makes it a challenge for large \u2026"}, {"title": "Intrinsic LoRA: A Generalist Approach for Discovering Knowledge in Generative Models", "link": "https://openreview.net/pdf%3Fid%3DxHKWN3Yi6U", "details": "X Du, N Kolkin, G Shakhnarovich, A Bhattad - Synthetic Data for Computer Vision Workshop \u2026", "abstract": "Generative models have been shown to be capable of creating images that closely mimic real scenes, suggesting they inherently encode scene representations. We introduce Intrinsic LoRA (I-LoRA), a general approach that uses Low-Rank \u2026"}, {"title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "link": "https://arxiv.org/pdf/2405.00402", "details": "L Ranaldi, A Freitas - arXiv preprint arXiv:2405.00402, 2024", "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches \u2026"}]
