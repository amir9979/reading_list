[{"title": "Continually Self-Improving Language Models for Bariatric Surgery Question--Answering", "link": "https://arxiv.org/pdf/2505.16102", "details": "YK Atri, TH Shin, T Hartvigsen - arXiv preprint arXiv:2505.16102, 2025", "abstract": "\u2026 models to assess their performance on our **medical** **question** **answering** benchmark: \u2026 **Medical** **large** **language** **models** are vulnerable to data-poisoning attacks. Nat. Med., 31(2):618\u2013\u2026 Toward expertlevel **medical** **question** **answering** \u2026", "entry_id": "http://arxiv.org/abs/2505.16102v1", "updated": "2025-05-22 01:02:51", "published": "2025-05-22 01:02:51", "authors": "Yash Kumar Atri;Thomas H Shin;Thomas Hartvigsen", "summary": "While bariatric and metabolic surgery (MBS) is considered the gold standard\ntreatment for severe and morbid obesity, its therapeutic efficacy hinges upon\nactive and longitudinal engagement with multidisciplinary providers, including\nsurgeons, dietitians/nutritionists, psychologists, and endocrinologists. This\nengagement spans the entire patient journey, from preoperative preparation to\nlong-term postoperative management. However, this process is often hindered by\nnumerous healthcare disparities, such as logistical and access barriers, which\nimpair easy patient access to timely, evidence-based, clinician-endorsed\ninformation. To address these gaps, we introduce bRAGgen, a novel adaptive\nretrieval-augmented generation (RAG)-based model that autonomously integrates\nreal-time medical evidence when response confidence dips below dynamic\nthresholds. This self-updating architecture ensures that responses remain\ncurrent and accurate, reducing the risk of misinformation. Additionally, we\npresent bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,\nvalidated by an expert bariatric surgeon. bRAGq constitutes the first\nlarge-scale, domain-specific benchmark for comprehensive MBS care. In a\ntwo-phase evaluation, bRAGgen is benchmarked against state-of-the-art models\nusing both large language model (LLM)--based metrics and expert surgeon review.\nAcross all evaluation dimensions, bRAGgen demonstrates substantially superior\nperformance in generating clinically accurate and relevant responses.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.16102v1;http://arxiv.org/pdf/2505.16102v1", "pdf_url": "http://arxiv.org/pdf/2505.16102v1"}, {"title": " **Large Language Models** in Portuguese for Healthcare: A Systematic Review", "link": "https://www.researchsquare.com/article/rs-6673690/latest", "details": "AM Shimaoka, AC da Silva Junior, JM Duarte\u2026 - 2025", "abstract": "\u2026 This study addresses **Large** **Language** **Models** (LLMs) pre-trained in Portuguese for healthcare applications, focusing on contextual embeddings. Research on LLMs for natural language processing (NLP) tasks in Portuguese is limited, especially \u2026"}, {"title": "SzegedAI at ArchEHR-QA 2025: Combining LLMs with traditional methods for grounded **question answering**", "link": "https://www.kispeterzsm.com/files/pdf/SzegedAI-ArchEHR-2025.pdf", "details": "SB Nagy, B Nyerges, ZM Kisp\u00e9ter, G T\u00f3th, AT Szl\u00faka\u2026", "abstract": "\u2026 Our approaches include multiple prompting techniques for **large** **language** **models** (LLMs), sentence similarity methods, and traditional feature engineering. We are aiming to explore both modern and traditional solutions to the task. To combine \u2026"}, {"title": "CT-Agent: A Multimodal-LLM Agent for 3D CT Radiology Question Answering", "link": "https://arxiv.org/pdf/2505.16229", "details": "Y Mao, W Xu, Y Qin, Y Gao - arXiv preprint arXiv:2505.16229, 2025", "abstract": "\u2026 To address the above challenges, we propose CT-Agent, a multimodal **large** **language** **models** (LLMs)-driven agentic framework for CTQA. CT-Agent is anatomy-aware and token-efficient. It tackles the anatomical complexity based on an organ-specific \u2026", "entry_id": "http://arxiv.org/abs/2505.16229v1", "updated": "2025-05-22 04:59:20", "published": "2025-05-22 04:59:20", "authors": "Yuren Mao;Wenyi Xu;Yuyang Qin;Yunjun Gao", "summary": "Computed Tomography (CT) scan, which produces 3D volumetric medical data that\ncan be viewed as hundreds of cross-sectional images (a.k.a. slices), provides\ndetailed anatomical information for diagnosis. For radiologists, creating CT\nradiology reports is time-consuming and error-prone. A visual question\nanswering (VQA) system that can answer radiologists' questions about some\nanatomical regions on the CT scan and even automatically generate a radiology\nreport is urgently needed. However, existing VQA systems cannot adequately\nhandle the CT radiology question answering (CTQA) task for: (1) anatomic\ncomplexity makes CT images difficult to understand; (2) spatial relationship\nacross hundreds slices is difficult to capture. To address these issues, this\npaper proposes CT-Agent, a multimodal agentic framework for CTQA. CT-Agent\nadopts anatomically independent tools to break down the anatomic complexity;\nfurthermore, it efficiently captures the across-slice spatial relationship with\na global-local token compression strategy. Experimental results on two 3D chest\nCT datasets, CT-RATE and RadGenome-ChestCT, verify the superior performance of\nCT-Agent.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.16229v1;http://arxiv.org/pdf/2505.16229v1", "pdf_url": "http://arxiv.org/pdf/2505.16229v1"}, {"title": "Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools", "link": "https://arxiv.org/pdf/2505.16113", "details": "P Lymperopoulos, V Sarathy - arXiv preprint arXiv:2505.16113, 2025", "abstract": "\u2026 As **large** **language** **models** (LLMs) have been increasingly deployed in practical applications, their **problem** -solving capacity has enabled practitioners to combine them with external tools to augment their reasoning abilities and available \u2026", "entry_id": "http://arxiv.org/abs/2505.16113v1", "updated": "2025-05-22 01:34:23", "published": "2025-05-22 01:34:23", "authors": "Panagiotis Lymperopoulos;Vasanth Sarathy", "summary": "Modern Large Language Models (LLMs) often require external tools, such as\nmachine learning classifiers or knowledge retrieval systems, to provide\naccurate answers in domains where their pre-trained knowledge is insufficient.\nThis integration of LLMs with external tools expands their utility but also\nintroduces a critical challenge: determining the trustworthiness of responses\ngenerated by the combined system. In high-stakes applications, such as medical\ndecision-making, it is essential to assess the uncertainty of both the LLM's\ngenerated text and the tool's output to ensure the reliability of the final\nresponse. However, existing uncertainty quantification methods do not account\nfor the tool-calling scenario, where both the LLM and external tool contribute\nto the overall system's uncertainty. In this work, we present a novel framework\nfor modeling tool-calling LLMs that quantifies uncertainty by jointly\nconsidering the predictive uncertainty of the LLM and the external tool. We\nextend previous methods for uncertainty quantification over token sequences to\nthis setting and propose efficient approximations that make uncertainty\ncomputation practical for real-world applications. We evaluate our framework on\ntwo new synthetic QA datasets, derived from well-known machine learning\ndatasets, which require tool-calling for accurate answers. Additionally, we\napply our method to retrieval-augmented generation (RAG) systems and conduct a\nproof-of-concept experiment demonstrating the effectiveness of our uncertainty\nmetrics in scenarios where external information retrieval is needed. Our\nresults show that the framework is effective in enhancing trust in LLM-based\nsystems, especially in cases where the LLM's internal knowledge is insufficient\nand external tools are required.", "comment": "10 pages 3 figures 3 tables", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CL", "links": "http://arxiv.org/abs/2505.16113v1;http://arxiv.org/pdf/2505.16113v1", "pdf_url": "http://arxiv.org/pdf/2505.16113v1"}, {"title": "Can **Large Language Models** address **problem** gambling? Expert insights from gambling treatment professionals", "link": "https://www.researchsquare.com/article/rs-6700963/latest", "details": "K Ghaharian, M Soligo, R Young, L Golab, SW Kraus\u2026 - 2025", "abstract": "\u2026 and **large** **language** **models** (LLMs), has caused a shift in behavior. People are increasingly turning to LLM-based chatbots to find **answers** to **questions** \u2026 On other occasions, single- **question** **answers** centered around self-reflection on topics such \u2026"}, {"title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models", "link": "https://arxiv.org/pdf/2505.16211", "details": "K Li, C Shen, Y Liu, J Han, K Zheng, X Zou, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 To evaluate privacy leakage in ALLMs, we use an audio **question** **answering** (AQA) framework, with detailed reasoning and evaluation prompts provided in Appendix G.3. Experiments optionally include privacy-enhancing prompts (eg, \u201cBank account \u2026", "entry_id": "http://arxiv.org/abs/2505.16211v1", "updated": "2025-05-22 04:27:46", "published": "2025-05-22 04:27:46", "authors": "Kai Li;Can Shen;Yile Liu;Jirui Han;Kelong Zheng;Xuechao Zou;Zhe Wang;Xingjian Du;Shun Zhang;Hanjun Luo;Yingbin Jin;Xinxin Xing;Ziyang Ma;Yue Liu;Xiaojun Jia;Yifan Zhang;Junfeng Fang;Kun Wang;Yibo Yan;Haoyang Li;Yiming Li;Xiaobin Zhuang;Yang Liu;Haibo Hu;Zhuo Chen;Zhizheng Wu;Xiaolin Hu;Eng-Siong Chng;XiaoFeng Wang;Wenyuan Xu;Wei Dong;Xinfeng Li", "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.", "comment": "Technical Report", "journal_ref": null, "primary_category": "cs.SD", "categories": "cs.SD;cs.AI;cs.CL;eess.AS", "links": "http://arxiv.org/abs/2505.16211v1;http://arxiv.org/pdf/2505.16211v1", "pdf_url": "http://arxiv.org/pdf/2505.16211v1"}]
