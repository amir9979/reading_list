[{"title": "MBTSAD: Mitigating Backdoors in Language Models Based on Token Splitting and Attention Distillation", "link": "https://arxiv.org/pdf/2501.02754", "details": "Y Ding, J Niu, P Yi - arXiv preprint arXiv:2501.02754, 2025", "abstract": "In recent years, attention-based models have excelled across various domains but remain vulnerable to backdoor attacks, often from downloading or fine-tuning on poisoned datasets. Many current methods to mitigate backdoors in NLP models rely \u2026"}, {"title": "Grounding Deliberate Reasoning in Multimodal Large Language Models", "link": "https://link.springer.com/chapter/10.1007/978-981-96-2061-6_2", "details": "J Chen, Y Liu, D Li, X An, W Deng, Z Feng, Y Zhao\u2026 - International Conference on \u2026, 2024", "abstract": "Abstract The rise of Multimodal Large Language Models, renowned for their advanced instruction-following and reasoning capabilities, has significantly propelled the field of visual reasoning. However, due to limitations in their image \u2026"}, {"title": "LLM360 K2: Scaling Up 360-Open-Source Large Language Models", "link": "https://arxiv.org/pdf/2501.07124", "details": "Z Liu, B Tan, H Wang, W Neiswanger, T Tao, H Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree OPEN SOURCE approach to the largest and most powerful models under project LLM360. While open-source LLMs continue to advance, the answer to\" How are the \u2026"}, {"title": "LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models", "link": "https://arxiv.org/pdf/2501.00055", "details": "M Yu, J Fang, Y Zhou, X Fan, K Wang, S Pan, Q Wen - arXiv preprint arXiv \u2026, 2024", "abstract": "While safety-aligned large language models (LLMs) are increasingly used as the cornerstone for powerful systems such as multi-agent frameworks to solve complex real-world problems, they still suffer from potential adversarial queries, such as \u2026"}, {"title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models", "link": "https://arxiv.org/pdf/2501.04945", "details": "Q Ren, J Zeng, Q He, J Liang, Y Xiao, W Zhou, Z Sun\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "It is crucial for large language models (LLMs) to follow instructions that involve multiple constraints. However, soft constraints are semantically related and difficult to verify through automated methods. These constraints remain a significant challenge \u2026"}, {"title": "Disentangling Exploration of Large Language Models by Optimal Exploitation", "link": "https://arxiv.org/pdf/2501.08925", "details": "T Grams, P Betz, C Bartelt - arXiv preprint arXiv:2501.08925, 2025", "abstract": "Exploration is a crucial skill for self-improvement and open-ended problem-solving. However, it remains uncertain whether large language models can effectively explore the state-space. Existing evaluations predominantly focus on the trade-off \u2026"}, {"title": "Dynamic Skill Adaptation for Large Language Models", "link": "https://arxiv.org/pdf/2412.19361%3F", "details": "J Chen, D Yang - arXiv preprint arXiv:2412.19361, 2024", "abstract": "We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework to adapt novel and complex skills to Large Language Models (LLMs). Compared with previous work which learns from human-curated and static data in random orders \u2026"}, {"title": "Find the Intention of Instruction: Comprehensive Evaluation of Instruction Understanding for Large Language Models", "link": "https://arxiv.org/pdf/2412.19450%3F", "details": "H Moon, J Seo, S Lee, C Park, H Lim - arXiv preprint arXiv:2412.19450, 2024", "abstract": "One of the key strengths of Large Language Models (LLMs) is their ability to interact with humans by generating appropriate responses to given instructions. This ability, known as instruction-following capability, has established a foundation for the use of \u2026"}, {"title": "Incorporating Molecular Knowledge in Large Language Models via Multimodal Modeling", "link": "https://ieeexplore.ieee.org/abstract/document/10838383/", "details": "Z Yang, K Lv, J Shu, Z Li, P Xiao - IEEE Transactions on Computational Social \u2026, 2025", "abstract": "In recent years, large language models (LLMs) represented by GPT-4 have achieved tremendous success in natural language-centered tasks. Nevertheless, LLMs face inherent challenges in tasks involving both natural language and molecular \u2026"}]
