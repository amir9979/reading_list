[{"title": "Enhancing Remote Sensing Vision-Language Models for Zero-Shot Scene Classification", "link": "https://arxiv.org/pdf/2409.00698", "details": "KE Khoury, M Zanella, B G\u00e9rin, T Godelaine, B Macq\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-Language Models for remote sensing have shown promising uses thanks to their extensive pretraining. However, their conventional usage in zero-shot scene classification methods still involves dividing large images into patches and making \u2026"}, {"title": "LitFM: A Retrieval Augmented Structure-aware Foundation Model For Citation Graphs", "link": "https://arxiv.org/pdf/2409.12177", "details": "J Zhang, J Chen, A Maatouk, N Bui, Q Xie, L Tassiulas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the advent of large language models (LLMs), managing scientific literature via LLMs has become a promising direction of research. However, existing approaches often overlook the rich structural and semantic relevance among scientific literature \u2026"}, {"title": "Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization", "link": "https://arxiv.org/pdf/2409.12903", "details": "M Samragh, I Mirzadeh, KA Vahid, F Faghri, M Cho\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The pre-training phase of language models often begins with randomly initialized parameters. With the current trends in scaling models, training their large number of parameters can be extremely slow and costly. In contrast, small language models are \u2026"}, {"title": "Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization", "link": "https://direct.mit.edu/tacl/article/124459", "details": "G Chrysostomou, Z Zhao, M Williams, N Aletras - Transactions of the Association for \u2026, 2024", "abstract": "Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges: their considerable size and tendency to hallucinate. Hallucinations are concerning because they erode \u2026"}, {"title": "Robust image representations with counterfactual contrastive learning", "link": "https://arxiv.org/pdf/2409.10365", "details": "M Roschewitz, FDS Ribeiro, T Xia, G Khara, B Glocker - arXiv preprint arXiv \u2026, 2024", "abstract": "Contrastive pretraining can substantially increase model generalisation and downstream performance. However, the quality of the learned representations is highly dependent on the data augmentation strategy applied to generate positive \u2026"}, {"title": "Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination", "link": "https://arxiv.org/pdf/2409.12746", "details": "ES Salido, R Morante, J Gonzalo, G Marco\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this article we present UNED-ACCESS 2024, a bilingual dataset that consists of 1003 multiple-choice questions of university entrance level exams in Spanish and English. Questions are originally formulated in Spanish and translated manually into \u2026"}]
