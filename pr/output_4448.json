[{"title": "Evaluating the clinical benefits of LLMs", "link": "https://www.nature.com/articles/s41591-024-03181-6", "details": "S Bedi, SS Jain, NH Shah - Nature Medicine, 2024", "abstract": "Evaluating the clinical benefits of LLMs | Nature Medicine Skip to main content Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser \u2026"}, {"title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models", "link": "https://arxiv.org/pdf/2407.03181", "details": "H Puerto, T Chubakov, X Zhu, HT Madabushi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model \u2026"}, {"title": "Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models: Enhancing Performance and Reducing Inference Costs", "link": "https://arxiv.org/pdf/2407.00945", "details": "E Liu, J Zhu, Z Lin, X Ning, MB Blaschko, S Yan, G Dai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancement of large language models (LLMs) has led to architectures with billions to trillions of parameters, posing significant deployment challenges due to their substantial demands on memory, processing power, and energy \u2026"}, {"title": "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models", "link": "https://arxiv.org/pdf/2407.17467", "details": "J Gu, Z Yang, C Ding, R Zhao, F Tan - arXiv preprint arXiv:2407.17467, 2024", "abstract": "Large Language Models (LLMs) excel in diverse tasks but often underperform in specialized fields due to limited domain-specific or proprietary corpus. Continual pre- training (CPT) enhances LLM capabilities by imbuing new domain-specific or \u2026"}, {"title": "Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language Models", "link": "https://arxiv.org/pdf/2407.07263", "details": "J Parmar, S Satheesh, M Patwary, M Shoeybi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As language models have scaled both their number of parameters and pretraining dataset sizes, the computational cost for pretraining has become intractable except for the most well-resourced teams. This increasing cost makes it ever more important \u2026"}, {"title": "Peer Reviewed: Public Health Surveillance in Electronic Health Records: Lessons From PCORnet", "link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11262136/", "details": "N Ghildayal, K Nagavedu, JL Wiltz, S Back\u2026 - Preventing Chronic Disease, 2024", "abstract": "Methods In 2018, infrastructure enhancements included addition of a table to store patients' residential zip codes and expansion of a modular program to generate population health statistics across conditions. Chronic disease surveillance case \u2026"}, {"title": "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "link": "https://arxiv.org/pdf/2407.02118", "details": "W Zheng, W Pan, X Xu, L Qin, L Yue, M Zhou - arXiv preprint arXiv:2407.02118, 2024", "abstract": "In recent years, Large Language Models (LLMs) have made significant strides towards Artificial General Intelligence. However, training these models from scratch requires substantial computational resources and vast amounts of text data. In this \u2026"}, {"title": "Applying Information Retrieval to the Electronic Health Record for Cohort Discovery", "link": "https://dmice.ohsu.edu/hersh/irehr-24.pdf", "details": "W Hersh", "abstract": "Hersh, W., Pentecost, J., Hickam, D., 1996. A task-oriented approach to information retrieval evaluation. Journal of the American Society for Information Science 47, 50\u2013 56\\. https://doi. org/10.1002/(SICI) 1097-4571 (199601) 47: 1< 50:: AID-ASI5> 3.0 \u2026"}, {"title": "Speculative Speech Recognition by Audio-Prefixed Low-Rank Adaptation of Language Models", "link": "https://arxiv.org/pdf/2407.04641", "details": "B Yusuf, MK Baskar, A Rosenberg, B Ramabhadran - arXiv preprint arXiv:2407.04641, 2024", "abstract": "This paper explores speculative speech recognition (SSR), where we empower conventional automatic speech recognition (ASR) with speculation capabilities, allowing the recognizer to run ahead of audio. We introduce a metric for measuring \u2026"}]
