[{"title": "Continually Tuning a Large Language Model for Multi-domain Radiology Report Generation", "link": "https://papers.miccai.org/miccai-2024/paper/0254_paper.pdf", "details": "Y Sun, HG Khor, Y Wang, Z Wang, H Zhao, Y Zhang\u2026 - International Conference on \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated potential across various tasks, including vision-language applications like chest X-ray (XR) report generation (RG) in healthcare. Recent RG approaches focus on optimizing model performance for a \u2026"}, {"title": "LMOD: A Large Multimodal Ophthalmology Dataset and Benchmark for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.01620", "details": "Z Qin, Y Yin, D Campbell, X Wu, K Zou, YC Tham, N Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Ophthalmology relies heavily on detailed image analysis for diagnosis and treatment planning. While large vision-language models (LVLMs) have shown promise in understanding complex visual information, their performance on ophthalmology \u2026"}, {"title": "UniAff: A Unified Representation of Affordances for Tool Usage and Articulation with Vision-Language Models", "link": "https://arxiv.org/pdf/2409.20551", "details": "Q Yu, S Huang, X Yuan, Z Jiang, C Hao, X Li, H Chang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Previous studies on robotic manipulation are based on a limited understanding of the underlying 3D motion constraints and affordances. To address these challenges, we propose a comprehensive paradigm, termed UniAff, that integrates 3D object-centric \u2026"}, {"title": "SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation", "link": "https://arxiv.org/pdf/2409.18082", "details": "X Li, S Huang, Q Yu, Z Jiang, C Hao, Y Zhu, H Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Automating garment manipulation poses a significant challenge for assistive robotics due to the diverse and deformable nature of garments. Traditional approaches typically require separate models for each garment type, which limits scalability and \u2026"}, {"title": "Helpful DoggyBot: Open-World Object Fetching using Legged Robots and Vision-Language Models", "link": "https://arxiv.org/pdf/2410.00231", "details": "Q Wu, Z Fu, X Cheng, X Wang, C Finn - arXiv preprint arXiv:2410.00231, 2024", "abstract": "Learning-based methods have achieved strong performance for quadrupedal locomotion. However, several challenges prevent quadrupeds from learning helpful indoor skills that require interaction with environments and humans: lack of end \u2026"}, {"title": "Magnet: We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function", "link": "https://arxiv.org/pdf/2409.19967", "details": "C Zhuang, Y Hu, P Gao - arXiv preprint arXiv:2409.19967, 2024", "abstract": "Text-to-image diffusion models particularly Stable Diffusion, have revolutionized the field of computer vision. However, the synthesis quality often deteriorates when asked to generate images that faithfully represent complex prompts involving \u2026"}, {"title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models", "link": "https://arxiv.org/pdf/2409.19375", "details": "Z Han, J Yang, J Li, Q Hu, Q Xu, MZ Shou, C Zhang - arXiv preprint arXiv:2409.19375, 2024", "abstract": "Vision-language foundation models (eg, CLIP) have shown remarkable performance across a wide range of tasks. However, deploying these models may be unreliable when significant distribution gaps exist between the training and test data. The \u2026"}, {"title": "AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation", "link": "https://arxiv.org/pdf/2410.00371", "details": "J Duan, W Pumacay, N Kumar, YR Wang, S Tian\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Robotic manipulation in open-world settings requires not only task execution but also the ability to detect and learn from failures. While recent advances in vision-language models (VLMs) and large language models (LLMs) have improved robots' spatial \u2026"}, {"title": "LoGra-Med: Long Context Multi-Graph Alignment for Medical Vision-Language Model", "link": "https://arxiv.org/pdf/2410.02615", "details": "DMH Nguyen, NT Diep, TQ Nguyen, HB Le, T Nguyen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "State-of-the-art medical multi-modal large language models (med-MLLM), like LLaVA-Med or BioMedGPT, leverage instruction-following data in pre-training. However, those models primarily focus on scaling the model size and data volume to \u2026"}]
