[{"title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge", "link": "https://arxiv.org/pdf/2407.19594", "details": "T Wu, W Yuan, O Golovneva, J Xu, Y Tian, J Jiao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can \u2026"}, {"title": "DOSSIER: Fact checking in electronic health records while preserving patient privacy", "link": "https://www.amazon.science/publications/dossier-fact-checking-in-electronic-health-records-while-preserving-patient-privacy", "details": "H Zhang, S Nagesh, M Shyani, N Mishra - 2024", "abstract": "Given a particular claim about a specific document, the fact checking problem is to determine if the claim is true and, if so, provide corroborating evidence. The problem is motivated by contexts where a document is too lengthy to quickly read and find an \u2026"}, {"title": "Multi-task Heterogeneous Graph Learning on Electronic Health Records", "link": "https://arxiv.org/pdf/2408.07569", "details": "TH Chan, G Yin, K Bae, L Yu - arXiv preprint arXiv:2408.07569, 2024", "abstract": "Learning electronic health records (EHRs) has received emerging attention because of its capability to facilitate accurate medical diagnosis. Since the EHRs contain enriched information specifying complex interactions between entities, modeling \u2026"}, {"title": "Boosting entity recognition by leveraging cross-task domain models for weak supervision", "link": "https://www.amazon.science/publications/boosting-entity-recognition-by-leveraging-cross-task-domain-models-for-weak-supervision", "details": "S Agrawal, S Merugu, V Sembium - 2024", "abstract": "Entity Recognition (ER) is a common natural language processing task encountered in a number of real-world applications. For common domains and named entities such as places and organisations, there exists sufficient high quality annotated data \u2026"}, {"title": "Pre-training data selection for biomedical domain adaptation using journal impact metrics", "link": "https://aclanthology.org/2024.bionlp-1.27.pdf", "details": "M Lai-king, P Paroubek - Proceedings of the 23rd Workshop on Biomedical \u2026, 2024", "abstract": "Abstract Domain adaptation is a widely used method in natural language processing (NLP) to improve the performance of a language model within a specific domain. This method is particularly common in the biomedical domain, which sees regular \u2026"}, {"title": "XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models", "link": "https://aclanthology.org/2024.bionlp-1.35.pdf", "details": "OC Thawakar, AM Shaker, SS Mullappilly, H Cholakkal\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "The latest breakthroughs in large language models (LLMs) and vision-language models (VLMs) have showcased promising capabilities toward performing a wide range of tasks. Such models are typically trained on massive datasets comprising \u2026"}, {"title": "MindLLM: Lightweight large language model pre-training, evaluation and domain application", "link": "https://www.sciencedirect.com/science/article/pii/S2666651024000111", "details": "Y Yang, H Sun, J Li, R Liu, Y Li, Y Liu, Y Gao, H Huang - AI Open, 2024", "abstract": "Abstract Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is \u2026"}, {"title": "Cognitive Assessment of Language Models", "link": "https://openreview.net/pdf%3Fid%3DpxRh1meUvN", "details": "D McDuff, D Munday, X Liu, I Galatzer-Levy - ICML 2024 Workshop on LLMs and Cognition", "abstract": "Large language models (LLMs) are a subclass of generative artificial intelligence that can interpret language inputs to generate novel responses. These capabilities are conceptualized as a significant step forward in artificial intelligence because the \u2026"}, {"title": "Zero-shot Cross-lingual Alignment for Embedding Initialization", "link": "https://aclanthology.org/2024.findings-acl.358.pdf", "details": "X Ai, Z Huang - Findings of the Association for Computational \u2026, 2024", "abstract": "For multilingual training, we present CrossInit, an initialization method that initializes embeddings into similar geometrical structures across languages in an unsupervised manner. CrossInit leverages a common cognitive linguistic mechanism, Zipf's law \u2026"}]
