[{"title": "DrugReX: an explainable drug repurposing system powered by large language models and literature-based knowledge graph", "link": "https://www.researchsquare.com/article/rs-6728958/latest", "details": "LC Huang, H Paek, K Lee, E Calay, D Pillai, N Ofoegbu\u2026 - 2025", "abstract": "Drug repurposing offers a time-efficient and cost-effective approach for therapeutic development by finding new uses for existing drugs. Additionally, achieving explainability in drug repurposing remains a challenge due to the lack of \u2026"}, {"title": "Large language models (LLMs) for antibiotic prescribing\u2014moving the needle from 'parlor trick'to practical tool", "link": "https://www.clinicalmicrobiologyandinfection.com/article/S1198-743X\\(25\\)00245-9/abstract", "details": "KE Goodman, PD Tamma - Clinical Microbiology and Infection, 2025", "abstract": "In the roughly two-and-a-half years since ChatGPT's public release, generative AI- based large language models (LLMs) are already reshaping the healthcare landscape. In the United States, increasing numbers of healthcare systems are using \u2026"}, {"title": "When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment", "link": "https://arxiv.org/pdf/2506.07452", "details": "Y Xiao, S Tonekaboni, W Gerych, V Suriyakumar\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) can be prompted with specific styles (eg, formatting responses as lists), including in jailbreak queries. Although these style patterns are semantically unrelated to the malicious intents behind jailbreak queries, their safety \u2026", "entry_id": "http://arxiv.org/abs/2506.07452v1", "updated": "2025-06-09 05:57:39", "published": "2025-06-09 05:57:39", "authors": "Yuxin Xiao;Sana Tonekaboni;Walter Gerych;Vinith Suriyakumar;Marzyeh Ghassemi", "summary": "Large language models (LLMs) can be prompted with specific styles (e.g.,\nformatting responses as lists), including in jailbreak queries. Although these\nstyle patterns are semantically unrelated to the malicious intents behind\njailbreak queries, their safety impact remains unclear. In this work, we seek\nto understand whether style patterns compromise LLM safety, how superficial\nstyle alignment increases model vulnerability, and how best to mitigate these\nrisks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,\nand find that malicious queries with style patterns inflate the attack success\nrate (ASR) for nearly all models. Notably, ASR inflation correlates with both\nthe length of style patterns and the relative attention an LLM exhibits on\nthem. We then investigate superficial style alignment, and find that\nfine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of\nthose same styles. Finally, we propose SafeStyle, a defense strategy that\nincorporates a small amount of safety training data augmented to match the\ndistribution of style patterns in the fine-tuning data. Across three LLMs and\nfive fine-tuning style settings, SafeStyle consistently outperforms baselines\nin maintaining LLM safety.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CL;cs.CY", "links": "http://arxiv.org/abs/2506.07452v1;http://arxiv.org/pdf/2506.07452v1", "pdf_url": "http://arxiv.org/pdf/2506.07452v1"}]
