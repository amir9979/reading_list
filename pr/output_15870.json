[{"title": "Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2504.13945", "details": "Z Wu, T Song, N Xie, W Zhang, M Zhu, S Wu, S Sun\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The rapid advancement of large vision-language models (LVLMs) has significantly propelled applications in document understanding, particularly in optical character recognition (OCR) and multilingual translation. However, current evaluations of \u2026"}, {"title": "SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data", "link": "https://arxiv.org/pdf/2504.20648", "details": "M Ogezi, F Shi - arXiv preprint arXiv:2504.20648, 2025", "abstract": "Vision-language models (VLMs) work well in tasks ranging from image captioning to visual question answering (VQA), yet they struggle with spatial reasoning, a key skill for understanding our physical world that humans excel at. We find that spatial \u2026"}, {"title": "Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models", "link": "https://arxiv.org/pdf/2504.12315", "details": "X Ji, J Wang, H Zhang, J Zhang, H Zhou, C Sun, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the development of Multimodal Large Language Models (MLLMs), numerous outstanding accomplishments have emerged within the open-source community. Due to the complexity of creating and training multimodal data pairs, it is still a \u2026"}, {"title": "FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models", "link": "https://arxiv.org/pdf/2504.20860", "details": "M Singha, S Roy, S Mehrotra, A Jha, M Abdar\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Textual prompt tuning adapts Vision-Language Models (eg, CLIP) in federated learning by tuning lightweight input tokens (or prompts) on local client data, while keeping network weights frozen. Post training, only the prompts are shared by the \u2026"}, {"title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models", "link": "https://arxiv.org/pdf/2504.14395", "details": "B Jalaian, ND Bastian - arXiv preprint arXiv:2504.14395, 2025", "abstract": "To develop trustworthy Vision-Language Models (VLMs), it is essential to address adversarial robustness and hallucination mitigation, both of which impact factual accuracy in high-stakes applications such as defense and healthcare. Existing \u2026"}, {"title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "link": "https://arxiv.org/pdf/2504.14366", "details": "P Haller, J Golde, A Akbik - arXiv preprint arXiv:2504.14366, 2025", "abstract": "Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures \u2026"}, {"title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "link": "https://arxiv.org/pdf/2504.14194", "details": "X Zhuang, J Peng, R Ma, Y Wang, T Bai, X Wei, J Qiu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural \u2026"}, {"title": "Exploring Multimodal Language Models for Sustainability Disclosure Extraction: A Comparative Study", "link": "https://aclanthology.org/2025.insights-1.13.pdf", "details": "T Gupta, T Goel, I Verma - The Sixth Workshop on Insights from Negative Results \u2026, 2025", "abstract": "Sustainability metrics have increasingly become a crucial non-financial criterion in investment decision-making. Organizations worldwide are recognizing the importance of sustainability and are proactively highlighting their efforts through \u2026"}, {"title": "Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models", "link": "https://arxiv.org/pdf/2504.20469", "details": "E Fane, M Surdeanu, E Blanco, SR Corman - arXiv preprint arXiv:2504.20469, 2025", "abstract": "Understanding how news narratives frame entities is crucial for studying media's impact on societal perceptions of events. In this paper, we evaluate the zero-shot capabilities of large language models (LLMs) in classifying framing roles. Through \u2026"}]
