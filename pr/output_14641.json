[{"title": "From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.06260", "details": "M Dai, J Sun, Z Zhao, S Liu, R Li, J Gao, X Li - arXiv preprint arXiv:2503.06260, 2025", "abstract": "Aligning large vision-language models (LVLMs) with human preferences is challenging due to the scarcity of fine-grained, high-quality, and multimodal preference data without human annotations. Existing methods relying on direct \u2026"}, {"title": "Synthetic Data Enhances Mathematical Reasoning of Language Models Based on Artificial Intelligence", "link": "https://www.itc.ktu.lt/index.php/ITC/article/view/39713/16892", "details": "Z Han, W Jiang - Information Technology and Control, 2025", "abstract": "Current large language models (LLMs) training involves extensive training data and computing resources to handle multiple natural language processing (NLP) tasks. This paper endeavors to assist individuals to compose feasible mathematical \u2026"}, {"title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding", "link": "https://arxiv.org/pdf/2503.05061", "details": "M Krumdick, C Lovering, V Reddy, S Ebner, C Tanner - arXiv preprint arXiv \u2026, 2025", "abstract": "LLM-as-a-Judge is a framework that uses an LLM (large language model) to evaluate the quality of natural language text-typically text that is also generated by an LLM. This framework holds great promise due to its relative low-cost, ease of use \u2026"}, {"title": "A Weighted Cross-entropy Loss for Mitigating LLM Hallucinations in Cross-lingual Continual Pretraining", "link": "https://ieeexplore.ieee.org/abstract/document/10888877/", "details": "Y Fan, R Li, G Zhang, C Shi, X Wang - \u2026 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "Recently, due to the explosive advances of large language models (LLMs) on English, cross-lingual continual pretraining has been widely applied in obtaining Chinese LLMs. However, previous studies showed that these LLMs have suffered \u2026"}, {"title": "Memorize or Generalize? Evaluating LLM Code Generation with Evolved Questions", "link": "https://arxiv.org/pdf/2503.02296%3F", "details": "W Chen, L Zhang, L Zhong, L Peng, Z Wang, J Shang - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) are known to exhibit a memorization phenomenon in code generation: instead of truly understanding the underlying principles of a programming problem, they tend to memorize the original prompt and its solution \u2026"}, {"title": "Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching", "link": "https://arxiv.org/pdf/2503.05179%3F", "details": "SA Aytes, J Baek, SJ Hwang - arXiv preprint arXiv:2503.05179, 2025", "abstract": "Recent advances in large language models have demonstrated remarkable reasoning capabilities through Chain of Thought (CoT) prompting, but often at the cost of excessive verbosity in their intermediate outputs, which increases \u2026"}, {"title": "A survey on post-training of large language models", "link": "https://arxiv.org/pdf/2503.06072", "details": "G Tie, Z Zhao, D Song, F Wei, R Zhou, Y Dai, W Yin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained \u2026"}, {"title": "Benchmarking Reasoning Robustness in Large Language Models", "link": "https://arxiv.org/pdf/2503.04550", "details": "T Yu, Y Jing, X Zhang, W Jiang, W Wu, Y Wang, W Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite the recent success of large language models (LLMs) in reasoning such as DeepSeek, we for the first time identify a key dilemma in reasoning robustness and generalization: significant performance degradation on novel or incomplete data \u2026"}, {"title": "Sample-aware Adaptive Structured Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2503.06184", "details": "J Kong, X Ma, J Wang, X Zhang - arXiv preprint arXiv:2503.06184, 2025", "abstract": "Large language models (LLMs) have achieved outstanding performance in natural language processing, but enormous model sizes and high computational costs limit their practical deployment. Structured pruning can effectively reduce the resource \u2026"}]
