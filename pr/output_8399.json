[{"title": "Grad-CAM Enabled Breast Cancer Classification with a 3D Inception-ResNet V2: Empowering Radiologists with Explainable Insights", "link": "https://www.mdpi.com/2072-6694/16/21/3668", "details": "FM Talaat, SA Gamel, RM El-Balka, M Shehata\u2026 - Cancers, 2024", "abstract": "Breast cancer (BCa) poses a severe threat to women's health worldwide as it is the most frequently diagnosed type of cancer and the primary cause of death for female patients. The biopsy procedure remains the gold standard for accurate and effective \u2026"}, {"title": "The Last Iterate Advantage: Empirical Auditing and Principled Heuristic Analysis of Differentially Private SGD", "link": "https://arxiv.org/pdf/2410.06186", "details": "T Steinke, M Nasr, A Ganesh, B Balle\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We propose a simple heuristic privacy analysis of noisy clipped stochastic gradient descent (DP-SGD) in the setting where only the last iterate is released and the intermediate iterates remain hidden. Namely, our heuristic assumes a linear structure \u2026"}, {"title": "Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings", "link": "https://arxiv.org/pdf/2410.22685", "details": "YS Grewal, EV Bonilla, TD Bui - arXiv preprint arXiv:2410.22685, 2024", "abstract": "Accurately quantifying uncertainty in large language models (LLMs) is crucial for their reliable deployment, especially in high-stakes applications. Current state-of-the- art methods for measuring semantic uncertainty in LLMs rely on strict bidirectional \u2026"}, {"title": "$\\beta $-calibration of Language Model Confidence Scores for Generative QA", "link": "https://arxiv.org/pdf/2410.06615", "details": "P Manggala, A Mastakouri, E Kirschbaum\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim \u2026"}, {"title": "AmpleGCG-Plus: A Strong Generative Model of Adversarial Suffixes to Jailbreak LLMs with Higher Success Rates in Fewer Attempts", "link": "https://arxiv.org/pdf/2410.22143", "details": "V Kumar, Z Liao, J Jones, H Sun - arXiv preprint arXiv:2410.22143, 2024", "abstract": "Although large language models (LLMs) are typically aligned, they remain vulnerable to jailbreaking through either carefully crafted prompts in natural language or, interestingly, gibberish adversarial suffixes. However, gibberish tokens \u2026"}, {"title": "Chest X-ray synthetic data for better testing and evaluation of ML models", "link": "https://cs231n.stanford.edu/2024/papers/chest-x-ray-synthetic-data-for-better-testing-and-evaluation-of-.pdf", "details": "E Bismuth, A Geslin, M Paschali", "abstract": "Abstract The use of Machine Learning (ML) models in radiology has significantly advanced medical imaging diagnostics. However, model performance may not accurately reflect certain underrepresented conditions if such conditions or minorities \u2026"}, {"title": "Evolutionary Contrastive Distillation for Language Model Alignment", "link": "https://arxiv.org/pdf/2410.07513", "details": "J Katz-Samuels, Z Li, H Yun, P Nigam, Y Xu, V Petricek\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The ability of large language models (LLMs) to execute complex instructions is essential for their real-world applications. However, several recent studies indicate that LLMs struggle with challenging instructions. In this paper, we propose \u2026"}, {"title": "Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures", "link": "https://arxiv.org/pdf/2410.06672", "details": "J Wang, X Ge, W Shu, Q Tang, Y Zhou, Z He, X Qiu - arXiv preprint arXiv:2410.06672, 2024", "abstract": "The hypothesis of Universality in interpretability suggests that different neural networks may converge to implement similar algorithms on similar tasks. In this work, we investigate two mainstream architectures for language modeling, namely \u2026"}, {"title": "Falcon Mamba: The First Competitive Attention-free 7B Language Model", "link": "https://arxiv.org/pdf/2410.05355", "details": "J Zuo, M Velikanov, DE Rhaiem, I Chahed, Y Belkada\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this technical report, we present Falcon Mamba 7B, a new base large language model based on the novel Mamba architecture. Falcon Mamba 7B is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model \u2026"}]
