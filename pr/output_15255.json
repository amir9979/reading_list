[{"title": "Pre-trained Language Models and Few-shot Learning for Medical Entity Extraction", "link": "https://arxiv.org/pdf/2504.04385", "details": "X Wang, G Liu, B Zhu, J He, H Zheng, H Zhang - arXiv preprint arXiv:2504.04385, 2025", "abstract": "This study proposes a medical entity extraction method based on Transformer to enhance the information extraction capability of medical literature. Considering the professionalism and complexity of medical texts, we compare the performance of \u2026"}, {"title": "Ontology-based Protein-Protein Interaction Explanation Using Large Language Models", "link": "https://www.biorxiv.org/content/10.1101/2025.04.07.647599.full.pdf", "details": "NB Cam, H Rehana, J Zheng, B Bansal, Y He, J Hur\u2026 - bioRxiv, 2025", "abstract": "Protein-protein interactions (PPIs) play a crucial role in various biological processes, and understanding these interactions is essential for advancing biomedical research. Automated extraction and analysis of PPI information from the rapidly growing \u2026"}, {"title": "Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation", "link": "https://arxiv.org/pdf/2504.02411%3F", "details": "A Misrahi, N Chirkova, M Louis, V Nikoulina - arXiv preprint arXiv:2504.02411, 2025", "abstract": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, but multi-domain applications face challenges like lack of diverse benchmarks and poor out-of-domain generalization. The first contribution of this work is to introduce a diverse benchmark \u2026"}, {"title": "FLUE: Streamlined Uncertainty Estimation for Large Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/33840/35995", "details": "S Gao, T Gong, Z Lin, R Xu, H Zhou, J Li - Proceedings of the AAAI Conference on \u2026, 2025", "abstract": "Uncertainty estimation is essential for practical applications such as decision- making, risk assessment, and human-AI collaboration. However, Uncertainty estimation in open-ended question-answering (QA) tasks presents unique \u2026"}, {"title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey", "link": "https://arxiv.org/pdf/2503.15850", "details": "X Liu, T Chen, L Da, C Chen, Z Lin, H Wei - arXiv preprint arXiv:2503.15850, 2025", "abstract": "Large Language Models (LLMs) excel in text generation, reasoning, and decision- making, enabling their adoption in high-stakes domains such as healthcare, law, and transportation. However, their reliability is a major concern, as they often produce \u2026"}, {"title": "Efficient Tuning of Large Language Models for Knowledge-Grounded Dialogue Generation", "link": "https://arxiv.org/pdf/2504.07754", "details": "B Zhang, H Ma, D Li, J Ding, J Wang, B Xu, HF Lin - arXiv preprint arXiv:2504.07754, 2025", "abstract": "Large language models (LLMs) demonstrate remarkable text comprehension and generation capabilities but often lack the ability to utilize up-to-date or domain- specific knowledge not included in their training data. To address this gap, we \u2026"}, {"title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?", "link": "https://arxiv.org/pdf/2504.02767", "details": "A Algaba, V Holst, F Tori, M Mobini, B Verbeken\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The spread of scientific knowledge depends on how researchers discover and cite previous work. The adoption of large language models (LLMs) in the scientific research process introduces a new layer to these citation practices. However, it \u2026"}, {"title": "Medication information extraction using local large language models", "link": "https://www.medrxiv.org/content/10.1101/2025.03.28.25324847.full.pdf", "details": "P Richter-Pechanski, M Seiferling, C KIRIAKOU\u2026 - medRxiv, 2025", "abstract": "Medication information is crucial for clinical routine and research. However, a vast amount is stored in unstructured text, such as doctoral letters, requiring manual extraction--a resource-intensive, error-prone task. Automating this process comes \u2026"}, {"title": "May the Memory Be With You: Efficient and Infinitely Updatable State for Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3721146.3721951", "details": "E Chukwu, L Bindschaedler - Proceedings of the 5th Workshop on Machine Learning \u2026, 2025", "abstract": "Large language models (LLMs) excel at natural language tasks but lack persistent state management for personalized and adaptive interactions. We propose a framework that endows these models with stateful capabilities by combining retrieval \u2026"}]
