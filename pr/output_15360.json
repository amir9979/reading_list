[{"title": "Can Large Language Models Trade? Testing Financial Theories with LLM Agents in Market Simulations", "link": "https://arxiv.org/pdf/2504.10789", "details": "A Lopez-Lira - arXiv preprint arXiv:2504.10789, 2025", "abstract": "This paper presents a realistic simulated stock market where large language models (LLMs) act as heterogeneous competing trading agents. The open-source framework incorporates a persistent order book with market and limit orders, partial fills \u2026"}, {"title": "Summarization Metrics for Spanish and Basque: Do Automatic Scores and LLM-Judges Correlate with Humans?", "link": "https://arxiv.org/pdf/2503.17039", "details": "J Barnes, N Perez, A Bonet-Jover, B Altuna - arXiv preprint arXiv:2503.17039, 2025", "abstract": "Studies on evaluation metrics and LLM-as-a-Judge models for automatic text summarization have largely been focused on English, limiting our understanding of their effectiveness in other languages. Through our new dataset BASSE (BAsque \u2026"}, {"title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis", "link": "https://arxiv.org/pdf/2503.23145", "details": "A Wei, T Suresh, J Cao, N Kannan, Y Wu, K Yan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by \u2026"}, {"title": "Every Sample Matters: Leveraging Mixture-of-Experts and High-Quality Data for Efficient and Accurate Code LLM", "link": "https://arxiv.org/pdf/2503.17793", "details": "L Team, W Cai, Y Cao, C Chen, C Chen, S Chen, Q Cui\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in code large language models (LLMs) have demonstrated remarkable capabilities in code generation and understanding. It is still challenging to build a code LLM with comprehensive performance yet ultimate efficiency. Many \u2026"}, {"title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce", "link": "https://arxiv.org/pdf/2504.11343", "details": "W Xiong, J Yao, Y Xu, B Pang, L Wang, D Sahoo, J Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1 \u2026"}, {"title": "Unlocking efficient long-to-short llm reasoning with model merging", "link": "https://arxiv.org/pdf/2503.20641%3F", "details": "H Wu, Y Yao, S Liu, Z Liu, X Fu, X Han, X Li, HL Zhen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of \u2026"}, {"title": "A Dual-Space Framework for General Knowledge Distillation of Large Language Models", "link": "https://arxiv.org/pdf/2504.11426", "details": "X Zhang, S Zhang, Y Liang, F Meng, Y Chen, J Xu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Knowledge distillation (KD) is a promising solution to compress large language models (LLMs) by transferring their knowledge to smaller models. During this process, white-box KD methods usually minimize the distance between the output \u2026"}, {"title": "A Survey on Mixture of Experts in Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10937907/", "details": "W Cai, J Jiang, F Wang, J Tang, S Kim, J Huang - IEEE Transactions on Knowledge \u2026, 2025", "abstract": "Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size \u2026"}, {"title": "2-D Transformer: Extending Large Language Models to Long-Context With Few Memory", "link": "https://ieeexplore.ieee.org/abstract/document/10937248/", "details": "X He, J Liu, Y Duan - IEEE Transactions on Neural Networks and Learning \u2026, 2025", "abstract": "The ability of processing long contexts is crucial for large language models (LLMs), but training LLMs with a long-context window requires substantial computational resources. Many sought to mitigate this through the sparse attention mechanism \u2026"}]
