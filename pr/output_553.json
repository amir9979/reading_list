'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Prompt-Enhanced Generation for Multimodal Open Questi'
[{"title": "Aligning Large and Small Language Models via Chain-of-Thought Reasoning", "link": "https://aclanthology.org/2024.eacl-long.109.pdf", "details": "L Ranaldi, A Freitas - Proceedings of the 18th Conference of the European \u2026, 2024", "abstract": "Abstract Chain-of-Thought (CoT) prompting empowersthe reasoning abilities of Large Language Models (LLMs), eliciting them to solve complexreasoning tasks in a step-wise manner. However, these capabilities appear only in models with billions of \u2026"}, {"title": "Fine-Tuning Language Models with Reward Learning on Policy", "link": "https://arxiv.org/pdf/2403.19279", "details": "H Lang, F Huang, Y Li - arXiv preprint arXiv:2403.19279, 2024", "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences. RLHF contains three steps, ie, human preference collecting, reward learning, and policy \u2026"}, {"title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models", "link": "https://arxiv.org/pdf/2403.20262", "details": "T Thonet, J Rozen, L Besacier - arXiv preprint arXiv:2403.20262, 2024", "abstract": "Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities \u2026"}, {"title": "Emergent Abilities in Reduced-Scale Generative Language Models", "link": "https://arxiv.org/pdf/2404.02204", "details": "S Muckatira, V Deshpande, V Lialin, A Rumshisky - arXiv preprint arXiv:2404.02204, 2024", "abstract": "Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study \u2026"}, {"title": "Efficient schema-less text-to-SQL conversion using large language models", "link": "https://www.accscience.com/journal/AIH/articles/online_first/1377", "details": "Y Mellah, V Kocaman, HU Haq, D Talby - Artificial Intelligence in Health, 2024", "abstract": "Large language models (LLMs) are increasingly being applied to several tasks including text-to-SQL (the process of converting natural language to SQL queries). While most studies revolve around training LLMs on large SQL corpora for better \u2026"}, {"title": "BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English", "link": "https://arxiv.org/pdf/2403.10900", "details": "S Shafayat, HM Hasan, MRC Mahim, RA Putri\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this study, we introduce BEnQA, a dataset comprising parallel Bengali and English exam questions for middle and high school levels in Bangladesh. Our dataset consists of approximately 5K questions covering several subjects in science with \u2026"}, {"title": "Conceptual and Unbiased Reasoning in Language Models", "link": "https://arxiv.org/pdf/2404.00205", "details": "B Zhou, H Zhang, S Chen, D Yu, H Wang, B Peng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Conceptual reasoning, the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on large language models' capability to perform conceptual reasoning. In this work, we \u2026"}, {"title": "Generative Language Models for Personalized Information Understanding", "link": "https://scholarworks.umass.edu/cgi/viewcontent.cgi%3Farticle%3D4123%26context%3Ddissertations_2", "details": "P Cai - 2024", "abstract": "A major challenge in information understanding stems from the diverse nature of the audience, where individuals possess varying preferences, experiences, educational and cultural backgrounds. Consequently, adopting a one-size-fits-all approach to \u2026"}, {"title": "LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models", "link": "https://arxiv.org/pdf/2404.07004", "details": "I Tufanov, K Hambardzumyan, J Ferrando, E Voita - arXiv preprint arXiv:2404.07004, 2024", "abstract": "We present the LM Transparency Tool (LM-TT), an open-source interactive toolkit for analyzing the internal workings of Transformer-based language models. Differently from previously existing tools that focus on isolated parts of the decision-making \u2026"}]
