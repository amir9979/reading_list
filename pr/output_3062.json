[{"title": "CancerLLM: A Large Language Model in Cancer Domain", "link": "https://arxiv.org/pdf/2406.10459", "details": "M Li, A Blaes, S Johnson, H Liu, H Xu, R Zhang - arXiv preprint arXiv:2406.10459, 2024", "abstract": "Medical Large Language Models (LLMs) such as ClinicalCamel 70B, Llama3- OpenBioLLM 70B have demonstrated impressive performance on a wide variety of medical NLP task. However, there still lacks a large language model (LLM) \u2026"}, {"title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2406.13233", "details": "Z Zeng, Y Miao, H Gao, H Zhang, Z Deng - arXiv preprint arXiv:2406.13233, 2024", "abstract": "Mixture of experts (MoE) has become the standard for constructing production-level large language models (LLMs) due to its promise to boost model capacity without causing significant overheads. Nevertheless, existing MoE methods usually enforce \u2026"}, {"title": "Towards a Personal Health Large Language Model", "link": "https://arxiv.org/pdf/2406.06474", "details": "J Cosentino, A Belyaeva, X Liu, NA Furlotte, Z Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In health, most large language model (LLM) research has focused on clinical tasks. However, mobile and wearable devices, which are rarely integrated into such tasks, provide rich, longitudinal data for personal health monitoring. Here we present \u2026"}, {"title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models", "link": "https://arxiv.org/pdf/2406.02224", "details": "T Fan, G Ma, Y Kang, H Gu, L Fan, Q Yang - arXiv preprint arXiv:2406.02224, 2024", "abstract": "Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small \u2026"}, {"title": "Abstraction-of-Thought Makes Language Models Better Reasoners", "link": "https://arxiv.org/pdf/2406.12442", "details": "R Hong, H Zhang, X Pan, D Yu, C Zhang - arXiv preprint arXiv:2406.12442, 2024", "abstract": "Abstract reasoning, the ability to reason from the abstract essence of a problem, serves as a key to generalization in human reasoning. However, eliciting language models to perform reasoning with abstraction remains unexplored. This paper seeks \u2026"}, {"title": "MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering", "link": "https://arxiv.org/pdf/2406.06573", "details": "RO Ness, K Matton, H Helm, S Zhang, J Bajwa\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLM) have achieved impressive performance on medical question-answering benchmarks. However, high benchmark accuracy does not imply that the performance generalizes to real-world clinical settings. Medical \u2026"}, {"title": "P3Sum: Preserving Author's Perspective in News Summarization with Diffusion Language Models", "link": "https://aclanthology.org/2024.naacl-long.119.pdf", "details": "Y Liu, S Feng, X Han, V Balachandran, CY Park\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "In this work, we take a first step towards designing summarization systems that are faithful to the author's intent, not only the semantic content of the article. Focusing on a case study of preserving political perspectives in news summarization, we find that \u2026"}, {"title": "Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain", "link": "https://arxiv.org/pdf/2406.06435", "details": "B Hu, B Ray, A Leung, A Summerville, D Joy, C Funk\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In difficult decision-making scenarios, it is common to have conflicting opinions among expert human decision-makers as there may not be a single right answer. Such decisions may be guided by different attributes that can be used to characterize \u2026"}, {"title": "VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks", "link": "https://arxiv.org/pdf/2406.08394", "details": "J Wu, M Zhong, S Xing, Z Lai, Z Liu, W Wang, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present VisionLLM v2, an end-to-end generalist multimodal large model (MLLM) that unifies visual perception, understanding, and generation within a single framework. Unlike traditional MLLMs limited to text output, VisionLLM v2 significantly \u2026"}]
