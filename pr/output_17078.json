[{"title": "Interpreting Chest X-rays Like a Radiologist: A Benchmark with Clinical Reasoning", "link": "https://arxiv.org/pdf/2505.23143", "details": "J Guan, Q Chen, L Liang, Y Liu, VMH Phan, MS To\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Artificial intelligence (AI)-based chest X-ray (CXR) interpretation assistants have demonstrated significant progress and are increasingly being applied in clinical settings. However, contemporary medical AI models often adhere to a simplistic input \u2026", "entry_id": "http://arxiv.org/abs/2505.23143v1", "updated": "2025-05-29 06:30:40", "published": "2025-05-29 06:30:40", "authors": "Jinquan Guan;Qi Chen;Lizhou Liang;Yuhang Liu;Vu Minh Hieu Phan;Minh-Son To;Jian Chen;Yutong Xie", "summary": "Artificial intelligence (AI)-based chest X-ray (CXR) interpretation\nassistants have demonstrated significant progress and are increasingly being\napplied in clinical settings. However, contemporary medical AI models often\nadhere to a simplistic input-to-output paradigm, directly processing an image\nand an instruction to generate a result, where the instructions may be integral\nto the model's architecture. This approach overlooks the modeling of the\ninherent diagnostic reasoning in chest X-ray interpretation. Such reasoning is\ntypically sequential, where each interpretive stage considers the images, the\ncurrent task, and the contextual information from previous stages. This\noversight leads to several shortcomings, including misalignment with clinical\nscenarios, contextless reasoning, and untraceable errors. To fill this gap, we\nconstruct CXRTrek, a new multi-stage visual question answering (VQA) dataset\nfor CXR interpretation. The dataset is designed to explicitly simulate the\ndiagnostic reasoning process employed by radiologists in real-world clinical\nsettings for the first time. CXRTrek covers 8 sequential diagnostic stages,\ncomprising 428,966 samples and over 11 million question-answer (Q&A) pairs,\nwith an average of 26.29 Q&A pairs per sample. Building on the CXRTrek dataset,\nwe propose a new vision-language large model (VLLM), CXRTrekNet, specifically\ndesigned to incorporate the clinical reasoning flow into the VLLM framework.\nCXRTrekNet effectively models the dependencies between diagnostic stages and\ncaptures reasoning patterns within the radiological context. Trained on our\ndataset, the model consistently outperforms existing medical VLLMs on the\nCXRTrek benchmarks and demonstrates superior generalization across multiple\ntasks on five diverse external datasets. The dataset and model can be found in\nour repository (https://github.com/guanjinquan/CXRTrek).", "comment": "10 pages (main text), 18 pages (appendix)", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.23143v1;http://arxiv.org/pdf/2505.23143v1", "pdf_url": "http://arxiv.org/pdf/2505.23143v1"}, {"title": "LLaMA-XR: A Novel Framework for Radiology Report Generation using LLaMA and QLoRA Fine Tuning", "link": "https://www.researchgate.net/profile/Ashad-Kabir/publication/392194093_LLaMA-XR_A_Novel_Framework_for_Radiology_Report_Generation_using_LLaMA_and_QLoRA_Fine_Tuning/links/6838526fdf0e3f544f5bc76d/LLaMA-XR-A-Novel-Framework-for-Radiology-Report-Generation-using-LLaMA-and-QLoRA-Fine-Tuning.pdf", "details": "MZB Jahangira, MA Kabirb, S Akterd, I Jahana\u2026", "abstract": "Automated radiology report generation holds significant potential to reduce radiologists' workload and enhance diagnostic accuracy. However, generating precise and clinically meaningful reports from chest radiographs remains \u2026"}, {"title": "Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation", "link": "https://arxiv.org/pdf/2505.01091", "details": "D Molino, F di Feola, L Shen, P Soda, V Guarrasi - arXiv preprint arXiv:2505.01091, 2025", "abstract": "Generative models have revolutionized Artificial Intelligence (AI), particularly in multimodal applications. However, adapting these models to the medical domain poses unique challenges due to the complexity of medical data and the stringent \u2026", "entry_id": "http://arxiv.org/abs/2505.01091v1", "updated": "2025-05-02 08:07:24", "published": "2025-05-02 08:07:24", "authors": "Daniele Molino;Francesco di Feola;Linlin Shen;Paolo Soda;Valerio Guarrasi", "summary": "Generative models have revolutionized Artificial Intelligence (AI),\nparticularly in multimodal applications. However, adapting these models to the\nmedical domain poses unique challenges due to the complexity of medical data\nand the stringent need for clinical accuracy. In this work, we introduce a\nframework specifically designed for multimodal medical data generation. By\nenabling the generation of multi-view chest X-rays and their associated\nclinical report, it bridges the gap between general-purpose vision-language\nmodels and the specialized requirements of healthcare. Leveraging the MIMIC-CXR\ndataset, the proposed framework shows superior performance in generating\nhigh-fidelity images and semantically coherent reports. Our quantitative\nevaluation reveals significant results in terms of FID and BLEU scores,\nshowcasing the quality of the generated data. Notably, our framework achieves\ncomparable or even superior performance compared to real data on downstream\ndisease classification tasks, underlining its potential as a tool for medical\nresearch and diagnostics. This study highlights the importance of\ndomain-specific adaptations in enhancing the relevance and utility of\ngenerative models for clinical applications, paving the way for future\nadvancements in synthetic multimodal medical data generation.", "comment": "arXiv admin note: substantial text overlap with arXiv:2501.04614", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.01091v1;http://arxiv.org/pdf/2505.01091v1", "pdf_url": "http://arxiv.org/pdf/2505.01091v1"}, {"title": "DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification", "link": "https://arxiv.org/pdf/2505.23595", "details": "Y Mohamed, N Mohamed, K Abouhashad, F Tang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While Multi-Task Learning (MTL) offers inherent advantages in complex domains such as medical imaging by enabling shared representation learning, effectively balancing task contributions remains a significant challenge. This paper addresses \u2026", "entry_id": "http://arxiv.org/abs/2505.23595v1", "updated": "2025-05-29 16:08:26", "published": "2025-05-29 16:08:26", "authors": "Youssef Mohamed;Noran Mohamed;Khaled Abouhashad;Feilong Tang;Sara Atito;Shoaib Jameel;Imran Razzak;Ahmed B. Zaky", "summary": "While Multi-Task Learning (MTL) offers inherent advantages in complex domains\nsuch as medical imaging by enabling shared representation learning, effectively\nbalancing task contributions remains a significant challenge. This paper\naddresses this critical issue by introducing DeepChest, a novel,\ncomputationally efficient and effective dynamic task-weighting framework\nspecifically designed for multi-label chest X-ray (CXR) classification. Unlike\nexisting heuristic or gradient-based methods that often incur substantial\noverhead, DeepChest leverages a performance-driven weighting mechanism based on\neffective analysis of task-specific loss trends. Given a network architecture\n(e.g., ResNet18), our model-agnostic approach adaptively adjusts task\nimportance without requiring gradient access, thereby significantly reducing\nmemory usage and achieving a threefold increase in training speed. It can be\neasily applied to improve various state-of-the-art methods. Extensive\nexperiments on a large-scale CXR dataset demonstrate that DeepChest not only\noutperforms state-of-the-art MTL methods by 7% in overall accuracy but also\nyields substantial reductions in individual task losses, indicating improved\ngeneralization and effective mitigation of negative transfer. The efficiency\nand performance gains of DeepChest pave the way for more practical and robust\ndeployment of deep learning in critical medical diagnostic applications. The\ncode is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.23595v1;http://arxiv.org/pdf/2505.23595v1", "pdf_url": "http://arxiv.org/pdf/2505.23595v1"}]
