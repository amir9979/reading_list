[{"title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism", "link": "https://arxiv.org/pdf/2408.10473", "details": "G Li, X Zhao, L Liu, Z Li, D Li, L Tian, J He, A Sirasao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational \u2026"}, {"title": "Untie the Knots: An Efficient Data Augmentation Strategy for Long-Context Pre-Training in Language Models", "link": "https://arxiv.org/pdf/2409.04774", "details": "J Tian, D Zheng, Y Cheng, R Wang, C Zhang, D Zhang - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLM) have prioritized expanding the context window from which models can incorporate more information. However, training models to handle long contexts presents significant challenges. These include the scarcity of high \u2026"}, {"title": "Interactive Machine Teaching by Labeling Rules and Instances", "link": "https://arxiv.org/pdf/2409.05199", "details": "G Karamanolakis, D Hsu, L Gravano - arXiv preprint arXiv:2409.05199, 2024", "abstract": "Weakly supervised learning aims to reduce the cost of labeling data by using expert- designed labeling rules. However, existing methods require experts to design effective rules in a single shot, which is difficult in the absence of proper guidance \u2026"}, {"title": "An Evidence-Based Framework For Heterogeneous Electronic Health Records: A Case Study In Mortality Prediction", "link": "https://link.springer.com/chapter/10.1007/978-3-031-67977-3_9", "details": "Y Ruan, L Huang, Q Xu, M Feng - International Conference on Belief Functions, 2024", "abstract": "Abstract Electronic Health Records (EHRs), characterized by their centralization of patient comprehensive disease and history information, hold significant promise to improve healthcare quality and efficiency. However, the heterogeneous nature of \u2026"}, {"title": "Application of a Data Quality Framework to Ductal Carcinoma In Situ Using Electronic Health Record Data From the All of Us Research Program", "link": "https://ascopubs.org/doi/abs/10.1200/CCI.24.00052", "details": "L Berman, Y Ostchega, J Giannini, LP Anandan\u2026 - JCO Clinical Cancer \u2026, 2024", "abstract": "PURPOSE The specific aims of this paper are to (1) develop and operationalize an electronic health record (EHR) data quality framework,(2) apply the dimensions of the framework to the phenotype and treatment pathways of ductal carcinoma in situ \u2026"}, {"title": "Evaluating and Enhancing the Fitness-for-Purpose of Electronic Health Record Data: Qualitative Study on Current Practices and Pathway to an Automated Approach \u2026", "link": "https://medinform.jmir.org/2024/1/e57153/", "details": "GK Wabo, P Moorthy, F Siegel, SA Seuchter\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background: Leveraging electronic health record (EHR) data for clinical or research purposes heavily depends on data fitness. However, there is a lack of standardized frameworks to evaluate EHR data suitability, leading to inconsistent quality in data \u2026"}, {"title": "GROOT-1.5: Learning to Follow Multi-Modal Instructions from Weak Supervision", "link": "https://openreview.net/pdf%3Fid%3Dzxdi4Kdfjq", "details": "S Cai, B Zhang, Z Wang, X Ma, A Liu, Y Liang - \u2026 modal Foundation Model meets Embodied AI \u2026", "abstract": "This paper studies the problem of learning an agent policy that can follow various forms of instructions. Specifically, we focus on multi-modal instructions: the policy is expected to accomplish tasks specified in 1) a reference video, aka one-shot \u2026"}, {"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "link": "https://arxiv.org/pdf/2408.12337", "details": "KS Phogat, SA Puranam, S Dasaratha, C Harsha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain \u2026"}, {"title": "On the Relationship between Truth and Political Bias in Language Models", "link": "https://arxiv.org/pdf/2409.05283", "details": "S Fulay, W Brannon, S Mohanty, C Overney\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language model alignment research often attempts to ensure that models are not only helpful and harmless, but also truthful and unbiased. However, optimizing these objectives simultaneously can obscure how improving one aspect might impact the \u2026"}]
