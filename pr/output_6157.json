[{"title": "Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis", "link": "https://arxiv.org/pdf/2409.00106", "details": "A Nagar, S Jaiswal, C Tan - arXiv preprint arXiv:2409.00106, 2024", "abstract": "Vision-language models (VLMs) have shown impressive zero-and few-shot performance on real-world visual question answering (VQA) benchmarks, alluding to their capabilities as visual reasoning engines. However, the benchmarks being used \u2026"}, {"title": "OLMoE: Open Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2409.02060", "details": "N Muennighoff, L Soldaini, D Groeneveld, K Lo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to \u2026"}, {"title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism", "link": "https://arxiv.org/pdf/2408.10473", "details": "G Li, X Zhao, L Liu, Z Li, D Li, L Tian, J He, A Sirasao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational \u2026"}, {"title": "A neural network approach to predict opioid misuse among previously hospitalized patients using electronic health records", "link": "https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0309424", "details": "L Vega, W Conneen, MA Veronin, RP Schumaker - Plos one, 2024", "abstract": "Can Electronic Health Records (EHR) predict opioid misuse in general patient populations? This research trained three backpropagation neural networks to explore EHR predictors using existing patient data. Model 1 used patient diagnosis \u2026"}, {"title": "The overview of the BioRED (Biomedical Relation Extraction Dataset) track at BioCreative VIII", "link": "https://academic.oup.com/database/article/doi/10.1093/database/baae069/7729400", "details": "R Islamaj, PT Lai, CH Wei, L Luo, T Almeida\u2026 - Database, 2024", "abstract": "Abstract The BioRED track at BioCreative VIII calls for a community effort to identify, semantically categorize, and highlight the novelty factor of the relationships between biomedical entities in unstructured text. Relation extraction is crucial for many \u2026"}, {"title": "Report Cards: Qualitative Evaluation of Language Models Using Natural Language Summaries", "link": "https://arxiv.org/pdf/2409.00844", "details": "B Yang, F Cui, K Paster, J Ba, P Vaezipoor, S Pitis\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid development and dynamic nature of large language models (LLMs) make it difficult for conventional quantitative benchmarks to accurately assess their capabilities. We propose report cards, which are human-interpretable, natural \u2026"}, {"title": "Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific Expert Pruning", "link": "https://arxiv.org/pdf/2409.01483", "details": "S Sarkar, L Lausen, V Cevher, S Zha, T Brox, G Karypis - arXiv preprint arXiv \u2026, 2024", "abstract": "Sparse Mixture of Expert (SMoE) models have emerged as a scalable alternative to dense models in language modeling. These models use conditionally activated feedforward subnetworks in transformer blocks, allowing for a separation between \u2026"}, {"title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios", "link": "https://aclanthology.org/2024.findings-acl.230.pdf", "details": "L Lin, J Fu, P Liu, Q Li, Y Gong, J Wan, F Zhang\u2026 - Findings of the Association \u2026, 2024", "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local \u2026"}, {"title": "In Defense of RAG in the Era of Long-Context Language Models", "link": "https://arxiv.org/pdf/2409.01666", "details": "T Yu, A Xu, R Akkiraju - arXiv preprint arXiv:2409.01666, 2024", "abstract": "Overcoming the limited context limitations in early-generation LLMs, retrieval- augmented generation (RAG) has been a reliable solution for context-based answer generation in the past. Recently, the emergence of long-context LLMs allows the \u2026"}]
