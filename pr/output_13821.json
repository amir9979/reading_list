[{"title": "Is Self-Supervised Pre-training on Satellite Imagery Better than ImageNet? A Systematic Study with Sentinel-2", "link": "https://arxiv.org/pdf/2502.10669", "details": "S Lahrichi, Z Sheng, S Xia, K Bradbury, J Malof - arXiv preprint arXiv:2502.10669, 2025", "abstract": "Self-supervised learning (SSL) has demonstrated significant potential in pre-training robust models with limited labeled data, making it particularly valuable for remote sensing (RS) tasks. A common assumption is that pre-training on domain-aligned \u2026"}, {"title": "Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding", "link": "https://arxiv.org/pdf/2502.09906", "details": "TD Truong, HQ Nguyen, XB Nguyen, A Dowling, X Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual \u2026"}, {"title": "Utilizing GPT-4 to interpret oral mucosal disease photographs for structured report generation", "link": "https://www.nature.com/articles/s41598-025-89328-y", "details": "ZZ Zhan, YT Xiong, CY Wang, BT Zhang, WJ Lian\u2026 - Scientific Reports, 2025", "abstract": "The aim of this study is to evaluate GPT-4's reasoning ability to interpret oral mucosal disease photos and generate structured reports from free-text inputs, while exploring the role of prompt engineering in enhancing its performance. Prompt received by \u2026"}, {"title": "Quantifying Racial Bias in SpO2 Measurements Using a Machine Learning Approach", "link": "https://www.scitepress.org/Papers/2025/131170/131170.pdf", "details": "HB Karli, E Hilborn, BD Unluturk", "abstract": "This paper investigates the racial biases in pulse oximetry, focusing on the importance of noninvasive peripheral oxygen saturation (SpO2) measurements in classifying patient race and ethnicity. Using the publicly available BOLD dataset, our \u2026"}, {"title": "Language Models Can Predict Their Own Behavior", "link": "https://arxiv.org/pdf/2502.13329", "details": "D Ashok, J May - arXiv preprint arXiv:2502.13329, 2025", "abstract": "Autoregressive Language Models output text by sequentially predicting the next token to generate, with modern methods like Chain-of-Thought (CoT) prompting achieving state-of-the-art reasoning capabilities by scaling the number of generated \u2026"}, {"title": "MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning", "link": "https://arxiv.org/pdf/2502.19634", "details": "J Pan, C Liu, J Wu, F Liu, J Zhu, HB Li, C Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show \u2026"}, {"title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models", "link": "https://arxiv.org/pdf/2502.10250%3F", "details": "GK Kumar, I Chaabane, K Wu - arXiv preprint arXiv:2502.10250, 2025", "abstract": "Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved \u2026"}, {"title": "EventLens: Enhancing Visual Commonsense Reasoning by Leveraging Event-Aware Pretraining and Cross-modal Linking", "link": "https://ieeexplore.ieee.org/abstract/document/10887841/", "details": "M Ma, Z Yu, Y Ma, G Li, Z Yang - ICASSP 2025-2025 IEEE International Conference \u2026, 2025", "abstract": "Visual Commonsense Reasoning (VCR) is a cognitive task, challenging models to answer visual questions, and to explain the rationale behind their answers. While Large Language Models (LLMs) offer potential for this task, VCR's complex scenes \u2026"}, {"title": "Pretraining GPT-style models in Hungarian", "link": "https://www.infocommunications.hu/documents/169298/4797540/InfocomJournal_2025_1_EA_1_vj.pdf", "details": "K Szentmih\u00e1lyi, DM Nemeskey, AM Szekeres\u2026", "abstract": "In this paper, we introduce two bilingual large lan-guage models, named OTP-1.5 B and OTP-13B, designed with a focus on both English and Hungarian languages. Both models utilize an 8k token context window and are trained on a dataset of 640 \u2026"}]
