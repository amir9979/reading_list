[{"title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios", "link": "https://aclanthology.org/2024.findings-acl.230.pdf", "details": "L Lin, J Fu, P Liu, Q Li, Y Gong, J Wan, F Zhang\u2026 - Findings of the Association \u2026, 2024", "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local \u2026"}, {"title": "Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models", "link": "https://aclanthology.org/2024.acl-long.276.pdf", "details": "J Duan, H Cheng, S Wang, A Zavalny, C Wang, R Xu\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) show promising results in language generation and instruction following but frequently \u201challucinate\u201d, making their outputs less reliable. Despite Uncertainty Quantification's (UQ) potential solutions \u2026"}, {"title": "MedSyn: LLM-based Synthetic Medical Text Generation Framework", "link": "https://arxiv.org/pdf/2408.02056", "details": "G Kumichev, P Blinov, Y Kuzkina, V Goncharov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating synthetic text addresses the challenge of data availability in privacy- sensitive domains such as healthcare. This study explores the applicability of synthetic data in real-world medical settings. We introduce MedSyn, a novel medical \u2026"}, {"title": "CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning", "link": "https://arxiv.org/pdf/2407.21011", "details": "Y Du, B Chang, NC Dvornek - arXiv preprint arXiv:2407.21011, 2024", "abstract": "Recent advancements in Contrastive Language-Image Pre-training (CLIP) have demonstrated notable success in self-supervised representation learning across various tasks. However, the existing CLIP-like approaches often demand extensive \u2026"}, {"title": "Entity Retrieval for Answering Entity-Centric Questions", "link": "https://arxiv.org/pdf/2408.02795", "details": "HS Shavarani, A Sarkar - arXiv preprint arXiv:2408.02795, 2024", "abstract": "The similarity between the question and indexed documents is a crucial factor in document retrieval for retrieval-augmented question answering. Although this is typically the only method for obtaining the relevant documents, it is not the sole \u2026"}, {"title": "VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks", "link": "https://arxiv.org/pdf/2407.19795", "details": "J Choi, J Kwon, JM Yun, S Yu, YB Kim - arXiv preprint arXiv:2407.19795, 2024", "abstract": "Domain generalizability is a crucial aspect of a deep learning model since it determines the capability of the model to perform well on data from unseen domains. However, research on the domain generalizability of deep learning models for vision \u2026"}, {"title": "Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction", "link": "https://aclanthology.org/2024.acl-long.19.pdf", "details": "Y Jian, T Liu, Y Tao, C Zhang, S Vosoughi, H Yang - \u2026 of the 62nd Annual Meeting of \u2026, 2024", "abstract": "We introduce EVL Gen, a streamlined framework designed for the pre-training of visually conditioned language generation models with high computational demands, utilizing frozen pre-trained large language models (LLMs). The conventional \u2026"}, {"title": "Optimizing Federated Learning Using Remote Embeddings for Graph Neural Networks", "link": "https://link.springer.com/chapter/10.1007/978-3-031-69766-1_32", "details": "P Naman, Y Simmhan - European Conference on Parallel Processing, 2024", "abstract": "Abstract Graph Neural Networks (GNNs) have experienced rapid advancements in recent years due to their ability to learn meaningful representations from graph data structures. Federated Learning (FL) has emerged as a viable machine learning \u2026"}, {"title": "Making Long-Context Language Models Better Multi-Hop Reasoners", "link": "https://arxiv.org/pdf/2408.03246", "details": "Y Li, S Liang, MR Lyu, L Wang - arXiv preprint arXiv:2408.03246, 2024", "abstract": "Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased \u2026"}]
