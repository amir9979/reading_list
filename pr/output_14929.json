[{"title": "TULIP: Towards Unified Language-Image Pretraining", "link": "https://arxiv.org/pdf/2503.15485", "details": "Z Tang, L Lian, S Eisape, XD Wang, R Herzig, A Yala\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and fine-grained object \u2026"}, {"title": "Advancing Medical Representation Learning Through High-Quality Data", "link": "https://arxiv.org/pdf/2503.14377%3F", "details": "N Baghbanzadeh, A Fallahpour, Y Parhizkar, F Ogidi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite the growing scale of medical Vision-Language datasets, the impact of dataset quality on model performance remains under-explored. We introduce Open- PMC, a high-quality medical dataset from PubMed Central, containing 2.2 million \u2026"}, {"title": "Behaviour Discovery and Attribution for Explainable Reinforcement Learning", "link": "https://arxiv.org/pdf/2503.14973%3F", "details": "R Rishav, S Nath, V Michalski, SE Kahou - arXiv preprint arXiv:2503.14973, 2025", "abstract": "Explaining the decisions made by reinforcement learning (RL) agents is critical for building trust and ensuring reliability in real-world applications. Traditional approaches to explainability often rely on saliency analysis, which can be limited in \u2026"}, {"title": "UniViTAR: Unified Vision Transformer with Native Resolution", "link": "https://arxiv.org/pdf/2504.01792", "details": "L Qiao, Y Gan, B Wang, J Qin, S Xu, S Yang, L Ma - arXiv preprint arXiv:2504.01792, 2025", "abstract": "Conventional Vision Transformer simplifies visual modeling by standardizing input resolutions, often disregarding the variability of natural visual data and compromising spatial-contextual fidelity. While preliminary explorations have superficially \u2026"}, {"title": "Cognitive Memory in Large Language Models", "link": "https://arxiv.org/pdf/2504.02441", "details": "L Shan, S Luo, Z Zhu, Y Yuan, Y Wu - arXiv preprint arXiv:2504.02441, 2025", "abstract": "This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long \u2026"}, {"title": "Negation Scope Conversion for Unifying Negation-Annotated Datasets", "link": "https://www.jstage.jst.go.jp/article/jnlp/32/1/32_300/_pdf", "details": "A Yoshida, Y Kato, S Matsubara - Journal of Natural Language Processing, 2025", "abstract": "Negation scope resolution is a technique that identifies the part of a sentence affected by the negation cue. The three major corpora used for it, the BioScope corpus, the SFU review corpus, and the Sherlock dataset, have different annotation \u2026"}]
