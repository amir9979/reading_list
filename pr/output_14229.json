[{"title": "Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning", "link": "https://arxiv.org/pdf/2502.19634", "details": "J Pan, C Liu, J Wu, F Liu, J Zhu, HB Li, C Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show \u2026"}, {"title": "Towards Statistical Factuality Guarantee for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2502.20560", "details": "Z Li, C Yan, NJ Jackson, W Cui, B Li, J Zhang, BA Malin - arXiv preprint arXiv \u2026, 2025", "abstract": "Advancements in Large Vision-Language Models (LVLMs) have demonstrated promising performance in a variety of vision-language tasks involving image- conditioned free-form text generation. However, growing concerns about \u2026"}, {"title": "Abn-BLIP: Abnormality-aligned Bootstrapping Language-Image Pre-training for Pulmonary Embolism Diagnosis and Report Generation from CTPA", "link": "https://arxiv.org/pdf/2503.02034", "details": "Z Zhong, Y Wang, L Bi, Z Ma, SH Ahn, CJ Mullin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical imaging plays a pivotal role in modern healthcare, with computed tomography pulmonary angiography (CTPA) being a critical tool for diagnosing pulmonary embolism and other thoracic conditions. However, the complexity of \u2026"}, {"title": "MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data with Vision Generation Task using Discrete Visual Representations", "link": "https://arxiv.org/pdf/2503.01019", "details": "Z Zhang, Y Yu, Y Chen, X Yang, SY Yeo - arXiv preprint arXiv:2503.01019, 2025", "abstract": "Despite significant progress in Vision-Language Pre-training (VLP), current approaches predominantly emphasize feature extraction and cross-modal comprehension, with limited attention to generating or transforming visual content \u2026"}, {"title": "CoCa-CXR: Contrastive Captioners Learn Strong Temporal Structures for Chest X-Ray Vision-Language Understanding", "link": "https://arxiv.org/pdf/2502.20509", "details": "Y Chen, S Xu, A Sellergren, Y Matias, A Hassidim\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models have proven to be of great benefit for medical image analysis since they learn rich semantics from both images and reports. Prior efforts have focused on better alignment of image and text representations to enhance \u2026"}, {"title": "Does Cross-Domain Pre-Training Truly Help Time-Series Foundation Models?", "link": "https://openreview.net/pdf%3Fid%3DPbhGeGBN7X", "details": "Z Zhang, J Zhang, S Zheng, Y Gu, J Bian - ICLR 2025 Workshop on Foundation Models in \u2026", "abstract": "Inspired by the success of pre-training large language models, recent efforts have explored cross-domain pre-training for time-series foundation models (TSFMs). However, the distinct data generation dynamics and contextual limitations of time \u2026"}, {"title": "Transformer-based self-supervised learning of pixel-and frequency-domain features for DMI grading on OCTA images", "link": "https://opg.optica.org/viewmedia.cfm%3Furi%3Dao-64-7-1668%26seq%3D0%26html%3Dtrue", "details": "Z Chen, W Zhong, F Yu, H Shen, Y Meng, W Xiong\u2026 - Applied Optics, 2025", "abstract": "Diabetic macular ischemia (DMI) is a critical vision-threatening complication of diabetic retinopathy. While optical coherence tomography angiography (OCTA) enables non-invasive DMI progression diagnosis, acquiring labeled datasets \u2026"}, {"title": "MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning via Modality Alignment and Retention", "link": "https://arxiv.org/pdf/2503.00374", "details": "T Wang, J Fan, D Zhang, D Liu, Y Xia, H Huang, W Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Histopathology and transcriptomics are fundamental modalities in oncology, encapsulating the morphological and molecular aspects of the disease. Multi-modal self-supervised learning has demonstrated remarkable potential in learning \u2026"}, {"title": "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions", "link": "https://arxiv.org/pdf/2503.03278", "details": "J Li, C Liu, W Bai, R Arcucci, CI Bercea, JA Schnabel - arXiv preprint arXiv \u2026, 2025", "abstract": "Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains \u2026"}]
