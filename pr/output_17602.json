[{"title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model", "link": "https://arxiv.org/pdf/2506.09659", "details": "E Ahmed, U Berdica, M Elliott, D Horak, JN Foerster - arXiv preprint arXiv:2506.09659, 2025", "abstract": "Obtaining multiple meaningfully diverse, high quality samples from Large Language Models for a fixed prompt remains an open challenge. Current methods for increasing diversity often only operate at the token-level, paraphrasing the same \u2026", "entry_id": "http://arxiv.org/abs/2506.09659v1", "updated": "2025-06-12 00:45:11", "published": "2025-06-11 12:26:45", "authors": "Eltayeb Ahmed;Uljad Berdica;Martha Elliott;Danijela Horak;Jakob N. Foerster", "summary": "Obtaining multiple meaningfully diverse, high quality samples from Large Language Models for a fixed prompt remains an open challenge. Current methods for increasing diversity often only operate at the token-level, paraphrasing the same response. This is problematic because it leads to poor exploration on reasoning problems and to unengaging, repetitive conversational agents. To address this we propose Intent Factored Generation (IFG), factorising the sampling process into two stages. First, we sample a semantically dense intent, e.g., a summary or keywords. Second, we sample the final response conditioning on both the original prompt and the intent from the first stage. This allows us to use a higher temperature during the intent step to promote conceptual diversity, and a lower temperature during the final generation to ensure the outputs are coherent and self-consistent. Additionally, we find that prompting the model to explicitly state its intent for each step of the chain-of-thought before generating the step is beneficial for reasoning tasks. We demonstrate our method's effectiveness across a diverse set of tasks. We show this method improves both pass@k and Reinforcement Learning from Verifier Feedback on maths and code tasks. For instruction-tuning, we combine IFG with Direct Preference Optimisation to increase conversational diversity without sacrificing reward. Finally, we achieve higher diversity while maintaining the quality of generations on a general language modelling task, using a new dataset of reader comments and news articles that we collect and open-source. In summary, we present a simple method of increasing the sample diversity of LLMs while maintaining performance. This method can be implemented by changing the prompt and varying the temperature during generation, making it easy to integrate into many algorithms for gains across various applications.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL;cs.LG", "links": "https://arxiv.org/abs/2506.09659v1;https://arxiv.org/pdf/2506.09659v1", "pdf_url": null}, {"title": "Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?", "link": "https://arxiv.org/pdf/2506.10415", "details": "Y Song, Y Du, D Paperno, A Gatt - arXiv preprint arXiv:2506.10415, 2025", "abstract": "This paper introduces the TempVS benchmark, which focuses on temporal grounding and reasoning capabilities of Multimodal Large Language Models (MLLMs) in image sequences. TempVS consists of three main tests (ie, event relation \u2026", "entry_id": "http://arxiv.org/abs/2506.10415v1", "updated": "2025-06-13 00:26:50", "published": "2025-06-12 07:12:31", "authors": "Yingjin Song;Yupei Du;Denis Paperno;Albert Gatt", "summary": "This paper introduces the TempVS benchmark, which focuses on temporal grounding and reasoning capabilities of Multimodal Large Language Models (MLLMs) in image sequences. TempVS consists of three main tests (i.e., event relation inference, sentence ordering and image ordering), each accompanied with a basic grounding test. TempVS requires MLLMs to rely on both visual and linguistic modalities to understand the temporal order of events. We evaluate 38 state-of-the-art MLLMs, demonstrating that models struggle to solve TempVS, with a substantial performance gap compared to human capabilities. We also provide fine-grained insights that suggest promising directions for future research. Our TempVS benchmark data and code are available at https://github.com/yjsong22/TempVS.", "comment": "27 pages, 14 figures. Accepted to ACL 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.CV", "links": "https://arxiv.org/abs/2506.10415v1;https://arxiv.org/pdf/2506.10415v1", "pdf_url": null}, {"title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting", "link": "https://arxiv.org/pdf/2506.09428", "details": "F Ding, B Wang - arXiv preprint arXiv:2506.09428, 2025", "abstract": "Supervised Fine-Tuning (SFT), while enhancing large language models (LLMs)'instruction-following capabilities and domain-specific task adaptability, often diminishes their general capabilities. Moreover, due to the inaccessibility of original \u2026", "entry_id": "http://arxiv.org/abs/2506.09428v1", "updated": "2025-06-12 00:27:35", "published": "2025-06-11 06:23:50", "authors": "Fei Ding;Baiqiao Wang", "summary": "Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)' instruction-following capabilities and domain-specific task adaptability, often diminishes their general capabilities. Moreover, due to the inaccessibility of original pre-training data, catastrophic forgetting tends to be exacerbated when third-party practitioners implement SFT on open-sourced models. To address this challenge, we propose a novel, more cost-effective SFT method which could effectively reduce the risk of catastrophic forgetting without access to original SFT data. Our approach begins by reconstructing the likely SFT instruction distribution of the base model, followed by a multi-model screening process to select optimal data, which is then mixed with new data for SFT. Experimental results demonstrate that our method preserves generalization capabilities in general domains while improving task-specific performance.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "https://arxiv.org/abs/2506.09428v1;https://arxiv.org/pdf/2506.09428v1", "pdf_url": null}, {"title": "Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward", "link": "https://arxiv.org/pdf/2506.07218", "details": "T Xiao, X Xu, Z Huang, H Gao, Q Liu, Q Liu, E Chen - arXiv preprint arXiv:2506.07218, 2025", "abstract": "Enhancing the multimodal reasoning capabilities of Multimodal Large Language Models (MLLMs) is a challenging task that has attracted increasing attention in the community. Recently, several studies have applied Reinforcement Learning with \u2026", "entry_id": "http://arxiv.org/abs/2506.07218v1", "updated": "2025-06-10 01:01:39", "published": "2025-06-08 16:48:42", "authors": "Tong Xiao;Xin Xu;Zhenya Huang;Hongyu Gao;Quan Liu;Qi Liu;Enhong Chen", "summary": "Enhancing the multimodal reasoning capabilities of Multimodal Large Language Models (MLLMs) is a challenging task that has attracted increasing attention in the community. Recently, several studies have applied Reinforcement Learning with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the reasoning abilities of MLLMs. However, these works largely overlook the enhancement of multimodal perception capabilities in MLLMs, which serve as a core prerequisite and foundational component of complex multimodal reasoning. Through McNemar's test, we find that existing RLVR method fails to effectively enhance the multimodal perception capabilities of MLLMs, thereby limiting their further improvement in multimodal reasoning. To address this limitation, we propose Perception-R1, which introduces a novel visual perception reward that explicitly encourages MLLMs to perceive the visual content accurately, thereby can effectively incentivizing both their multimodal perception and reasoning capabilities. Specifically, we first collect textual visual annotations from the CoT trajectories of multimodal problems, which will serve as visual references for reward assignment. During RLVR training, we employ a judging LLM to assess the consistency between the visual annotations and the responses generated by MLLM, and assign the visual perception reward based on these consistency judgments. Extensive experiments on several multimodal reasoning benchmarks demonstrate the effectiveness of our Perception-R1, which achieves state-of-the-art performance on most benchmarks using only 1,442 training data.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CV", "links": "https://arxiv.org/abs/2506.07218v1;https://arxiv.org/pdf/2506.07218v1", "pdf_url": null}, {"title": "SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence", "link": "https://arxiv.org/pdf/2506.07966", "details": "Z Gong, W Li, O Ma, S Li, J Ji, X Yang, G Luo, J Yan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in various multimodal tasks. To pursue higher intelligence in space, MLLMs require integrating multiple atomic spatial capabilities to handle complex and dynamic tasks \u2026", "entry_id": "http://arxiv.org/abs/2506.07966v1", "updated": "2025-06-10 01:47:33", "published": "2025-06-09 17:41:36", "authors": "Ziyang Gong;Wenhao Li;Oliver Ma;Songyuan Li;Jiayi Ji;Xue Yang;Gen Luo;Junchi Yan;Rongrong Ji", "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in various multimodal tasks. To pursue higher intelligence in space, MLLMs require integrating multiple atomic spatial capabilities to handle complex and dynamic tasks. However, existing benchmarks struggle to comprehensively evaluate the spatial intelligence of common MLLMs from the atomic level to the compositional level. To fill this gap, we present SpaCE-10, a comprehensive benchmark for compositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial capabilities, which are combined to form 8 compositional capabilities. Based on these definitions, we propose a novel hierarchical annotation pipeline to generate high-quality and diverse question-answer (QA) pairs. With over 150+ hours of human expert effort, we obtain over 5k QA pairs for 811 real indoor scenes in SpaCE-10, which covers various evaluation settings like point cloud input and multi-choice QA. We conduct an extensive evaluation of common MLLMs on SpaCE-10 and find that even the most advanced MLLM still lags behind humans by large margins. Through our careful study, we also draw several significant findings that benefit the MLLM community. For example, we reveal that the shortcoming of counting capability greatly limits the compositional spatial capabilities of existing MLLMs. The evaluation code and benchmark datasets are available at https://github.com/Cuzyoung/SpaCE-10.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "https://arxiv.org/abs/2506.07966v1;https://arxiv.org/pdf/2506.07966v1", "pdf_url": null}, {"title": "SafeLawBench: Towards Safe Alignment of Large Language Models", "link": "https://arxiv.org/pdf/2506.06636", "details": "C Cao, H Zhu, J Ji, Q Sun, Z Zhu, Y Wu, J Dai, Y Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the growing prevalence of large language models (LLMs), the safety of LLMs has raised significant concerns. However, there is still a lack of definitive standards for evaluating their safety due to the subjective nature of current safety benchmarks \u2026", "entry_id": "http://arxiv.org/abs/2506.06636v1", "updated": "2025-06-10 00:19:19", "published": "2025-06-07 03:09:59", "authors": "Chuxue Cao;Han Zhu;Jiaming Ji;Qichao Sun;Zhenghao Zhu;Yinyu Wu;Juntao Dai;Yaodong Yang;Sirui Han;Yike Guo", "summary": "With the growing prevalence of large language models (LLMs), the safety of LLMs has raised significant concerns. However, there is still a lack of definitive standards for evaluating their safety due to the subjective nature of current safety benchmarks. To address this gap, we conducted the first exploration of LLMs' safety evaluation from a legal perspective by proposing the SafeLawBench benchmark. SafeLawBench categorizes safety risks into three levels based on legal standards, providing a systematic and comprehensive framework for evaluation. It comprises 24,860 multi-choice questions and 1,106 open-domain question-answering (QA) tasks. Our evaluation included 2 closed-source LLMs and 18 open-source LLMs using zero-shot and few-shot prompting, highlighting the safety features of each model. We also evaluated the LLMs' safety-related reasoning stability and refusal behavior. Additionally, we found that a majority voting mechanism can enhance model performance. Notably, even leading SOTA models like Claude-3.5-Sonnet and GPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench, while the average accuracy of 20 LLMs remains at 68.8\\%. We urge the community to prioritize research on the safety of LLMs.", "comment": "Accepted to ACL2025 Findings", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "https://arxiv.org/abs/2506.06636v1;https://arxiv.org/pdf/2506.06636v1", "pdf_url": null}, {"title": "Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models", "link": "https://arxiv.org/pdf/2506.07121", "details": "RJ Wang, K Xue, Z Qin, Z Li, S Tang, HT Li, S Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Ensuring safety of large language models (LLMs) is important. Red teaming--a systematic approach to identifying adversarial prompts that elicit harmful responses from target LLMs--has emerged as a crucial safety evaluation method. Within this \u2026", "entry_id": "http://arxiv.org/abs/2506.07121v1", "updated": "2025-06-10 00:56:20", "published": "2025-06-08 13:07:41", "authors": "Ren-Jian Wang;Ke Xue;Zeyu Qin;Ziniu Li;Sheng Tang;Hao-Tian Li;Shengcai Liu;Chao Qian", "summary": "Ensuring safety of large language models (LLMs) is important. Red teaming--a systematic approach to identifying adversarial prompts that elicit harmful responses from target LLMs--has emerged as a crucial safety evaluation method. Within this framework, the diversity of adversarial prompts is essential for comprehensive safety assessments. We find that previous approaches to red-teaming may suffer from two key limitations. First, they often pursue diversity through simplistic metrics like word frequency or sentence embedding similarity, which may not capture meaningful variation in attack strategies. Second, the common practice of training a single attacker model restricts coverage across potential attack styles and risk categories. This paper introduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to address these limitations. QDRT achieves goal-driven diversity through behavior-conditioned training and implements a behavioral replay buffer in an open-ended manner. Additionally, it trains multiple specialized attackers capable of generating high-quality attacks across diverse styles and risk categories. Our empirical evaluation demonstrates that QDRT generates attacks that are both more diverse and more effective against a wide range of target LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the field of LLM safety by providing a systematic and effective approach to automated red-teaming, ultimately supporting the responsible deployment of LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.NE", "links": "https://arxiv.org/abs/2506.07121v1;https://arxiv.org/pdf/2506.07121v1", "pdf_url": null}, {"title": "AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length", "link": "https://arxiv.org/pdf/2506.10525", "details": "J Cheng, F Liu, C Wu, L Zhang - arXiv preprint arXiv:2506.10525, 2025", "abstract": "While Large Language Models (LLMs) have significantly advanced code generation efficiency, they face inherent challenges in balancing performance and inference costs across diverse programming tasks. Dynamically selecting the optimal LLM \u2026", "entry_id": "http://arxiv.org/abs/2506.10525v1", "updated": "2025-06-13 00:33:28", "published": "2025-06-12 09:43:48", "authors": "Junhang Cheng;Fang Liu;Chengru Wu;Li Zhang", "summary": "While Large Language Models (LLMs) have significantly advanced code generation efficiency, they face inherent challenges in balancing performance and inference costs across diverse programming tasks. Dynamically selecting the optimal LLM based on task difficulty and resource constraints offers a promising approach to achieve an optimal balance between efficiency and performance. However, existing model selection methods are resource-intensive and often neglect cost efficiency. Moreover, these approaches rely on human-annotated difficulty labels that are frequently inaccessible in real-world settings and may not align with the LLM's own assessment of task difficulty. In this paper, we introduce AdaptiveLLM, a framework that dynamically selects optimal LLMs for a given coding task by automatically assessing task difficulty. Our framework first estimates task difficulty using Chain-of-Thought lengths generated by reasoning model, clusters these into three difficulty levels via k-means, and fine-tunes CodeBERT to embed difficulty-aware features. A trained XGBoost classifier then selects the best model for each problem, optimizing the performance-cost trade-off. Experimental results show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score while reducing resource consumption by 88.9% compared to baseline method ComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an approximately 15% accuracy improvement, while maintaining the same level of cost consumption. Apart from that, the difficulty assessment using CoT provides more reliable selection criteria than human evaluation. Our replication package is available at https://github.com/cjhCoder7/AdaptiveLLM.", "comment": "Accepted by Internetware 2025", "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE", "links": "https://arxiv.org/abs/2506.10525v1;https://arxiv.org/pdf/2506.10525v1", "pdf_url": null}, {"title": "PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier", "link": "https://arxiv.org/pdf/2506.10406", "details": "Y Jiang, Y Xiong, Y Yuan, C Xin, W Xu, Y Yue, Q Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks, yet they still struggle to reliably verify the correctness of their own outputs. Existing solutions to this verification challenge often depend on \u2026", "entry_id": "http://arxiv.org/abs/2506.10406v1", "updated": "2025-06-13 00:26:23", "published": "2025-06-12 06:59:35", "authors": "Yuhua Jiang;Yuwen Xiong;Yufeng Yuan;Chao Xin;Wenyuan Xu;Yu Yue;Qianchuan Zhao;Lin Yan", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks, yet they still struggle to reliably verify the correctness of their own outputs. Existing solutions to this verification challenge often depend on separate verifier models or require multi-stage self-correction training pipelines, which limit scalability. In this paper, we propose Policy as Generative Verifier (PAG), a simple and effective framework that empowers LLMs to self-correct by alternating between policy and verifier roles within a unified multi-turn reinforcement learning (RL) paradigm. Distinct from prior approaches that always generate a second attempt regardless of model confidence, PAG introduces a selective revision mechanism: the model revises its answer only when its own generative verification step detects an error. This verify-then-revise workflow not only alleviates model collapse but also jointly enhances both reasoning and verification abilities. Extensive experiments across diverse reasoning benchmarks highlight PAG's dual advancements: as a policy, it enhances direct generation and self-correction accuracy; as a verifier, its self-verification outperforms self-consistency.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "https://arxiv.org/abs/2506.10406v1;https://arxiv.org/pdf/2506.10406v1", "pdf_url": null}]
