[{"title": "Mixture of Experts Using Tensor Products", "link": "https://arxiv.org/pdf/2405.16671", "details": "Z Su, F Mo, P Tiwari, B Wang, JY Nie, JG Simonsen - arXiv preprint arXiv:2405.16671, 2024", "abstract": "In multi-task learning, the conventional approach involves training a model on multiple tasks simultaneously. However, the training signals from different tasks can interfere with one another, potentially leading to\\textit {negative transfer}. To mitigate \u2026"}, {"title": "Convergence Behavior of an Adversarial Weak Supervision Method", "link": "https://arxiv.org/pdf/2405.16013", "details": "S An, S Dasgupta - arXiv preprint arXiv:2405.16013, 2024", "abstract": "Labeling data via rules-of-thumb and minimal label supervision is central to Weak Supervision, a paradigm subsuming subareas of machine learning such as crowdsourced learning and semi-supervised ensemble learning. By using this \u2026"}, {"title": "Comparative Analysis of Open-Source Language Models in Summarizing Medical Text Data", "link": "https://arxiv.org/pdf/2405.16295", "details": "Y Chen, Z Wang, B Wen, F Zulkernine - arXiv preprint arXiv:2405.16295, 2024", "abstract": "Unstructured text in medical notes and dialogues contains rich information. Recent advancements in Large Language Models (LLMs) have demonstrated superior performance in question answering and summarization tasks on unstructured text \u2026"}, {"title": "AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models", "link": "https://arxiv.org/pdf/2405.07626", "details": "S Liu, D Yao, L Fang, Z Li, W Li, K Feng, XW Ji, J Bi - arXiv preprint arXiv:2405.07626, 2024", "abstract": "Detecting anomaly edges for dynamic graphs aims to identify edges significantly deviating from the normal pattern and can be applied in various domains, such as cybersecurity, financial transactions and AIOps. With the evolving of time, the types of \u2026"}, {"title": "BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation", "link": "https://arxiv.org/pdf/2405.17039", "details": "C Jia, P Wang, Z Li, YC Li, Z Zhang, N Tang, Y Yu - arXiv preprint arXiv:2405.17039, 2024", "abstract": "Large language models (LLMs) have catalyzed a paradigm shift in natural language processing, yet their limited controllability poses a significant challenge for downstream applications. We aim to address this by drawing inspiration from the \u2026"}, {"title": "IAD: In-Context Learning Ability Decoupler of Large Language Models in Meta-Training", "link": "https://aclanthology.org/2024.lrec-main.749.pdf", "details": "Y Liu, X Chen, G Xing, J Zhang, R Yan - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) exhibit remarkable In-Context Learning (ICL) ability, where the model learns tasks from prompts consisting of input-output examples. However, the pre-training objectives of LLMs often misalign with ICL \u2026"}, {"title": "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks", "link": "https://openreview.net/pdf%3Fid%3Dtr0TcqitMH", "details": "S Xu, L Pang, H Shen, X Cheng, TS Chua - The Web Conference 2024, 2024", "abstract": "Making the contents generated by Large Language Model (LLM) such as ChatGPT, accurate, credible and traceable is crucial, especially in complex knowledge- intensive tasks that require multi-step reasoning and each step needs knowledge to \u2026"}, {"title": "Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.708.pdf", "details": "O Ignat, Z Jin, A Abzaliev, L Biester, S Castro, N Deng\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Recent progress in large language models (LLMs) has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that \u201cit's all been solved.\u201d Not surprisingly, this has, in turn, made \u2026"}]
