[{"title": "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models", "link": "https://arxiv.org/pdf/2410.17389", "details": "M Lin, S Shi, Y Guo, B Chalaki, V Tadiparthi, EM Pari\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The correct specification of reward models is a well-known challenge in reinforcement learning. Hand-crafted reward functions often lead to inefficient or suboptimal policies and may not be aligned with user values. Reinforcement \u2026"}, {"title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?", "link": "https://arxiv.org/pdf/2410.23856", "details": "Z Zhou, R Tao, J Zhu, Y Luo, Z Wang, B Han - arXiv preprint arXiv:2410.23856, 2024", "abstract": "This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We \u2026"}, {"title": "Metaaligner: Towards generalizable multi-objective alignment of language models", "link": "https://openreview.net/pdf%3Fid%3DdIVb5C0QFf", "details": "K Yang, Z Liu, Q Xie, J Huang, T Zhang, S Ananiadou - The Thirty-eighth Annual \u2026, 2024", "abstract": "Recent advancements in large language models (LLMs) focus on aligning to heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are dependent on the policy model \u2026"}, {"title": "LM2: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.920.pdf", "details": "G Juneja, S Dutta, T Chakraborty - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}, {"title": "Externally Valid Policy Evaluation from Randomized Trials Using Additional Observational Data", "link": "https://openreview.net/pdf%3Fid%3D2pgc5xDJ1b", "details": "S Ek, D Zachariah - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Randomized trials are widely considered as the gold standard for evaluating the effects of decision policies. Trial data is, however, drawn from a population which may differ from the intended target population and this raises a problem of external \u2026"}, {"title": "Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings", "link": "https://arxiv.org/pdf/2410.22685", "details": "YS Grewal, EV Bonilla, TD Bui - arXiv preprint arXiv:2410.22685, 2024", "abstract": "Accurately quantifying uncertainty in large language models (LLMs) is crucial for their reliable deployment, especially in high-stakes applications. Current state-of-the- art methods for measuring semantic uncertainty in LLMs rely on strict bidirectional \u2026"}, {"title": "S $^{2} $ FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity", "link": "https://openreview.net/pdf%3Fid%3DlEUle8S4xQ", "details": "X Yang, J Leng, G Guo, J Zhao, R Nakada, L Zhang\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in \u2026"}, {"title": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning", "link": "https://arxiv.org/pdf/2410.19290", "details": "Y Liu, S Chang, T Jaakkola, Y Zhang - arXiv preprint arXiv:2410.19290, 2024", "abstract": "Recent studies have identified one aggravating factor of LLM hallucinations as the knowledge inconsistency between pre-training and fine-tuning, where unfamiliar fine- tuning data mislead the LLM to fabricate plausible but wrong outputs. In this paper \u2026"}, {"title": "Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "link": "https://aclanthology.org/2024.findings-emnlp.78.pdf", "details": "H Hwang, D Kim, S Kim, S Ye, M Seo - Findings of the Association for Computational \u2026, 2024", "abstract": "Training on large amounts of rationales (ie, CoT Fine-tuning) has been found effective for improving mathematical reasoning of large language models (LLMs). However, acquiring human-authored solutions or augmenting rationales from \u2026"}]
