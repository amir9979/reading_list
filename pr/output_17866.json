[{"title": "Effective **Medical** Visual **Question Answering** Using Dynamic Prompting and Decoding Knowledge Editing", "link": "https://link.springer.com/article/10.1007/s41019-025-00291-0", "details": "ZJ Zhou, YF Huang, XL Wang, X Hong - Data Science and Engineering, 2025", "abstract": "\u2026 Furthermore, realizing the strong generalization capability of **large** **language** **models** , some studies have explored training or fine-tuning \u2026 the **medical** visual **question** **answering** task in this paper as an example, given a **medical** image \\\\(\\textbf \u2026"}, {"title": "Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives", "link": "https://arxiv.org/pdf/2506.18116", "details": "B Haider, A Gorti, A Chadha, M Gaur - arXiv preprint arXiv:2506.18116, 2025", "abstract": "\u2026 This research examines bias in **Large** **Language** **Models** within mental health contexts using publicly available, anonymized Reddit posts from the IMHI dataset. We acknowledge several key ethical considerations inherent to this work, guided by \u2026", "entry_id": "http://arxiv.org/abs/2506.18116v1", "updated": "2025-06-22 18:00:16", "published": "2025-06-22 18:00:16", "authors": "Batool Haider;Atmika Gorti;Aman Chadha;Manas Gaur", "summary": "Large Language Models (LLMs) in mental healthcare risk propagating biases\nthat reinforce stigma and harm marginalized groups. While previous research\nidentified concerning trends, systematic methods for detecting intersectional\nbiases remain limited. This work introduces a multi-hop question answering\n(MHQA) framework to explore LLM response biases in mental health discourse. We\nanalyze content from the Interpretable Mental Health Instruction (IMHI) dataset\nacross symptom presentation, coping mechanisms, and treatment approaches. Using\nsystematic tagging across age, race, gender, and socioeconomic status, we\ninvestigate bias patterns at demographic intersections. We evaluate four LLMs:\nClaude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic\ndisparities across sentiment, demographics, and mental health conditions. Our\nMHQA approach demonstrates superior detection compared to conventional methods,\nidentifying amplification points where biases magnify through sequential\nreasoning. We implement two debiasing techniques: Roleplay Simulation and\nExplicit Bias Reduction, achieving 66-94% bias reductions through few-shot\nprompting with BBQ dataset examples. These findings highlight critical areas\nwhere LLMs reproduce mental healthcare biases, providing actionable insights\nfor equitable AI development.", "comment": "19 Pages, 7 Figures, 4 Tables (Note: Under Review)", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.CY", "links": "http://arxiv.org/abs/2506.18116v1;http://arxiv.org/pdf/2506.18116v1", "pdf_url": "http://arxiv.org/pdf/2506.18116v1"}, {"title": "Taming Vision-Language Models for Medical Image Analysis: A Comprehensive Review", "link": "https://arxiv.org/pdf/2506.18378", "details": "H Lin, C Xu, J Qin - arXiv preprint arXiv:2506.18378, 2025", "abstract": "\u2026 In genetic biomarker prediction [90], **large** **language** **models** (LLMs) are employed to generate **medical** \u2026 of **Medical** Visual **Question** **Answer** ing (VQA) is to develop AI systems capable of interpreting **medical** images and **answering** **clinically** relevant \u2026", "entry_id": "http://arxiv.org/abs/2506.18378v1", "updated": "2025-06-23 08:11:24", "published": "2025-06-23 08:11:24", "authors": "Haoneng Lin;Cheng Xu;Jing Qin", "summary": "Modern Vision-Language Models (VLMs) exhibit unprecedented capabilities in\ncross-modal semantic understanding between visual and textual modalities. Given\nthe intrinsic need for multi-modal integration in clinical applications, VLMs\nhave emerged as a promising solution for a wide range of medical image analysis\ntasks. However, adapting general-purpose VLMs to medical domain poses numerous\nchallenges, such as large domain gaps, complicated pathological variations, and\ndiversity and uniqueness of different tasks. The central purpose of this review\nis to systematically summarize recent advances in adapting VLMs for medical\nimage analysis, analyzing current challenges, and recommending promising yet\nurgent directions for further investigations. We begin by introducing core\nlearning strategies for medical VLMs, including pretraining, fine-tuning, and\nprompt learning. We then categorize five major VLM adaptation strategies for\nmedical image analysis. These strategies are further analyzed across eleven\nmedical imaging tasks to illustrate their current practical implementations.\nFurthermore, we analyze key challenges that impede the effective adaptation of\nVLMs to clinical applications and discuss potential directions for future\nresearch. We also provide an open-access repository of related literature to\nfacilitate further research, available at\nhttps://github.com/haonenglin/Awesome-VLM-for-MIA. It is anticipated that this\narticle can help researchers who are interested in harnessing VLMs in medical\nimage analysis tasks have a better understanding on their capabilities and\nlimitations, as well as current technical barriers, to promote their\ninnovative, robust, and safe application in clinical practice.", "comment": "34 pages", "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.CV", "links": "http://arxiv.org/abs/2506.18378v1;http://arxiv.org/pdf/2506.18378v1", "pdf_url": "http://arxiv.org/pdf/2506.18378v1"}, {"title": "Towards Global-level Mechanistic Interpretability: A Perspective of Modular Circuits of **Large Language Models**", "link": "https://openreview.net/pdf%3Fid%3Ddo5vVfKEXZ", "details": "Y He, W Zheng, Y Dong, Y Zhu, C Chen, J Li - Forty-second International Conference on \u2026", "abstract": "\u2026 2020) for multi-class **medical** abstract classification, \u201c **Medical** Abstracts\u201d (Schopf et al.\u2026 2022) for **medical** multiple-choice **question** **answering** , and \u201cSymptom to Diagnosis\u201d (Gretel.ai\u2026 in discovering modular circuits within **large** **language** **models** \u2026"}, {"title": "Can Common VLMs Rival Medical VLMs? Evaluation and Strategic Insights", "link": "https://arxiv.org/pdf/2506.17337", "details": "Y Zhong, R Jin, X Li, Q Dou - arXiv preprint arXiv:2506.17337, 2025", "abstract": "\u2026 key **question** : Can efficient fine-tuned common VLMs rival generalist **medical** VLMs for solving specific medical imaging tasks? This study systematically evaluates common and **medical** VLMs across disease diagnosis and visual **question** \u2026", "entry_id": "http://arxiv.org/abs/2506.17337v1", "updated": "2025-06-19 07:59:00", "published": "2025-06-19 07:59:00", "authors": "Yuan Zhong;Ruinan Jin;Xiaoxiao Li;Qi Dou", "summary": "Medical vision-language models (VLMs) leverage large-scale pretraining for\ndiverse imaging tasks but require substantial computational and data resources.\nMeanwhile, common or general-purpose VLMs (e.g., CLIP, LLaVA), though not\ntrained for medical use, show promise with fine-tuning. This raises a key\nquestion: Can efficient fine-tuned common VLMs rival generalist medical VLMs\nfor solving specific medical imaging tasks? This study systematically evaluates\ncommon and medical VLMs across disease diagnosis and visual question answering\n(VQA). Using CLIP-based and LLaVA-based models, we examine (1) off-the-shelf\nperformance gaps in in-domain (ID) settings, (2) whether fine-tuning bridges\nthese gaps, and (3) generalization to out-of-domain (OOD) tasks on unseen\nmedical modalities. While medical-specific pretraining provides advantages in\nID settings, common VLMs match or surpass medical-specific models after\nlightweight fine-tuning, with LoRA-based adaptation proving highly effective\namong different tasks. In OOD tasks, common VLMs demonstrate strong\nadaptability in some tasks, challenging the assumption that medical-specific\npre-training is essential. These findings suggest that leveraging common VLMs\nwith fine-tuning offers a scalable and cost-effective alternative to developing\nlarge-scale medical VLMs, providing crucial insights for future research in the\nmedical imaging field.", "comment": null, "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2506.17337v1;http://arxiv.org/pdf/2506.17337v1", "pdf_url": "http://arxiv.org/pdf/2506.17337v1"}, {"title": "Position: **Medical** Large Language Model Benchmarks Should Prioritize Construct Validity", "link": "https://openreview.net/pdf%3Fid%3DYuMEUNNpeb", "details": "A Alaa, T Hartvigsen, N Golchini, S Dutta, F Dean\u2026 - Forty-second International \u2026", "abstract": "\u2026 We conducted a systematic PubMed search to identify open-access articles evaluating **large** **language** **models** (LLMs) in **clinical** contexts, focusing on publications from the last five years. Using the PubMed query function, we retrieved \u2026"}, {"title": "Pareto-Optimized Open-Source LLMs for Healthcare via Context Retrieval", "link": "https://link.springer.com/chapter/10.1007/978-3-031-96235-6_27", "details": "J Bayarri-Planas, AK Gururajan, D Garcia-Gasulla - IFIP International Conference on \u2026, 2025", "abstract": "\u2026 This study leverages optimized context retrieval to enhance open-source **Large** **Language** **Models** (LLMs) for cost-effective, high performance healthcare AI. We demonstrate that this approach achieves state-of-the-art accuracy on **medical** \u2026"}, {"title": "**Medical** LLMs: Fine-Tuning vs. Retrieval-Augmented Generation", "link": "https://www.mdpi.com/2306-5354/12/7/687", "details": "B Pingua, A Sahoo, M Kandpal, D Murmu, J Rautaray\u2026 - Bioengineering, 2025", "abstract": "\u2026 **medical** **question** **answering** tasks. The efficiency of the model was gauged from its ability to accurately interpret and **answer** **medical** **questions** \u2026 study provides insightful information about tailoring **large** **language** **models** (LLMs) for **answering** \u2026"}, {"title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs", "link": "https://arxiv.org/pdf/2506.16962", "details": "H Sun, Y Jiang, W Lou, Y Zhang, W Li, L Wang, M Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **large** **language** **models** (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the **medical** \u2026 of **medical** visual **question** **answering** and reasoning benchmarks. Code will be available at https://github.com/manglu097/Chiron-o1 \u2026", "entry_id": "http://arxiv.org/abs/2506.16962v1", "updated": "2025-06-20 12:51:19", "published": "2025-06-20 12:51:19", "authors": "Haoran Sun;Yankai Jiang;Wenjie Lou;Yujie Zhang;Wenjie Li;Lilong Wang;Mianxin Liu;Lei Liu;Xiaosong Wang", "summary": "Multimodal large language models (MLLMs) have begun to demonstrate robust\nreasoning capabilities on general tasks, yet their application in the medical\ndomain remains in its early stages. Constructing chain-of-thought (CoT)\ntraining data is essential for bolstering the reasoning abilities of medical\nMLLMs. However, existing approaches exhibit a deficiency in offering a\ncomprehensive framework for searching and evaluating effective reasoning paths\ntowards critical diagnosis. To address this challenge, we propose Mentor-Intern\nCollaborative Search (MICS), a novel reasoning-path searching scheme to\ngenerate rigorous and effective medical CoT data. MICS first leverages mentor\nmodels to initialize the reasoning, one step at a time, then prompts each\nintern model to continue the thinking along those initiated paths, and finally\nselects the optimal reasoning path according to the overall reasoning\nperformance of multiple intern models. The reasoning performance is determined\nby an MICS-Score, which assesses the quality of generated reasoning paths.\nEventually, we construct MMRP, a multi-task medical reasoning dataset with\nranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum\nlearning strategy, with robust visual question-answering and generalizable\nreasoning capabilities. Extensive experiments demonstrate that Chiron-o1,\ntrained on our CoT dataset constructed using MICS, achieves state-of-the-art\nperformance across a list of medical visual question answering and reasoning\nbenchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing\nStep-by-Step and Verifiable Medical Reasoning in MLLMs", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI;cs.CL", "links": "http://arxiv.org/abs/2506.16962v1;http://arxiv.org/pdf/2506.16962v1", "pdf_url": "http://arxiv.org/pdf/2506.16962v1"}]
