[{"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "link": "https://arxiv.org/pdf/2408.00724", "details": "Y Wu, Z Sun, S Li, S Welleck, Y Yang - arXiv preprint arXiv:2408.00724, 2024", "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth \u2026"}, {"title": "Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models", "link": "https://aclanthology.org/2024.acl-long.276.pdf", "details": "J Duan, H Cheng, S Wang, A Zavalny, C Wang, R Xu\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) show promising results in language generation and instruction following but frequently \u201challucinate\u201d, making their outputs less reliable. Despite Uncertainty Quantification's (UQ) potential solutions \u2026"}, {"title": "CARL: Unsupervised Code-Based Adversarial Attacks for Programming Language Models via Reinforcement Learning", "link": "https://dl.acm.org/doi/abs/10.1145/3688839", "details": "K Yao, H Wang, C Qin, H Zhu, Y Wu, L Zhang - ACM Transactions on Software Engineering \u2026", "abstract": "Code based adversarial attacks play a crucial role in revealing vulnerabilities of software system. Recently, pre-trained programming language models (PLMs) have demonstrated remarkable success in various significant software engineering tasks \u2026"}, {"title": "EUDA: An Efficient Unsupervised Domain Adaptation via Self-Supervised Vision Transformer", "link": "https://arxiv.org/pdf/2407.21311", "details": "A Abedi, QM Wu, N Zhang, F Pourpanah - arXiv preprint arXiv:2407.21311, 2024", "abstract": "Unsupervised domain adaptation (UDA) aims to mitigate the domain shift issue, where the distribution of training (source) data differs from that of testing (target) data. Many models have been developed to tackle this problem, and recently vision \u2026"}, {"title": "DDK: Distilling Domain Knowledge for Efficient Large Language Models", "link": "https://arxiv.org/pdf/2407.16154", "details": "J Liu, C Zhang, J Guo, Y Zhang, H Que, K Deng, Z Bai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the advanced intelligence abilities of large language models (LLMs) in various applications, they still face significant computational and storage demands. Knowledge Distillation (KD) has emerged as an effective strategy to improve the \u2026"}, {"title": "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "link": "https://arxiv.org/pdf/2408.02103", "details": "P Wang, X Wang, C Lou, S Mao, P Xie, Y Jiang - arXiv preprint arXiv:2408.02103, 2024", "abstract": "In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language \u2026"}, {"title": "Fine-tuning Language Models for Joint Rewriting and Completion of Code with Potential Bugs", "link": "https://aclanthology.org/2024.findings-acl.938.pdf", "details": "D Wang, J Zhao, H Pei, S Tan, S Zha - Findings of the Association for Computational \u2026, 2024", "abstract": "Handling drafty partial code remains a notable challenge in real-time code suggestion applications. Previous work has demonstrated shortcomings of large language models of code (CodeLLMs) in completing partial code with potential bugs \u2026"}, {"title": "Cognitive Assessment of Language Models", "link": "https://openreview.net/pdf%3Fid%3DpxRh1meUvN", "details": "D McDuff, D Munday, X Liu, I Galatzer-Levy - ICML 2024 Workshop on LLMs and Cognition", "abstract": "Large language models (LLMs) are a subclass of generative artificial intelligence that can interpret language inputs to generate novel responses. These capabilities are conceptualized as a significant step forward in artificial intelligence because the \u2026"}, {"title": "Chain of Condition: Construct, Verify and Solve Conditions for Conditional Question Answering", "link": "https://arxiv.org/pdf/2408.05442", "details": "J Lin, Y Lai, Y Feng - arXiv preprint arXiv:2408.05442, 2024", "abstract": "Conditional question answering (CQA) is an important task that aims to find probable answers and identify conditions that need to be satisfied to support the answer. Existing approaches struggle with CQA due to two main challenges:(1) precisely \u2026"}]
