'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Text Quality-Based Pruning for Efficient Training of L'
[{"title": "Gaze-infused BERT: Do human gaze signals help pre-trained language models?", "link": "https://link.springer.com/article/10.1007/s00521-024-09725-8", "details": "B Wang, B Liang, L Zhou, R Xu - Neural Computing and Applications, 2024", "abstract": "This research delves into the intricate connection between self-attention mechanisms in large-scale pre-trained language models, like BERT, and human gaze patterns, with the aim of harnessing gaze information to enhance the performance of natural \u2026"}, {"title": "Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback", "link": "https://arxiv.org/pdf/2404.14233", "details": "W Xiao, Z Huang, L Gan, W He, H Li, Z Yu, H Jiang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapidly developing Large Vision Language Models (LVLMs) have shown notable capabilities on a range of multi-modal tasks, but still face the hallucination phenomena where the generated texts do not align with the given contexts \u2026"}, {"title": "GRAMMAR: Grounded and Modular Evaluation of Domain-Specific Retrieval-Augmented Language Models", "link": "https://arxiv.org/pdf/2404.19232", "details": "X Li, M Liu, S Gao - arXiv preprint arXiv:2404.19232, 2024", "abstract": "Retrieval-augmented Generation (RAG) systems have been actively studied and deployed across various industries to query on domain-specific knowledge base. However, evaluating these systems presents unique challenges due to the scarcity of \u2026"}, {"title": "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models", "link": "https://arxiv.org/pdf/2404.15846", "details": "Q He, J Zeng, Q He, J Liang, Y Xiao - arXiv preprint arXiv:2404.15846, 2024", "abstract": "It is imperative for Large language models (LLMs) to follow instructions with elaborate requirements (ie Complex Instructions Following). Yet, it remains under- explored how to enhance the ability of LLMs to follow complex instructions with \u2026"}, {"title": "ExcluIR: Exclusionary Neural Information Retrieval", "link": "https://arxiv.org/pdf/2404.17288", "details": "W Zhang, M Zhang, S Wu, J Pei, Z Ren, M de Rijke\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Exclusion is an important and universal linguistic skill that humans use to express what they do not want. However, in information retrieval community, there is little research on exclusionary retrieval, where users express what they do not want in \u2026"}, {"title": "An Improved Machine Learning Model for Pulmonary Embolism Detection and Segmentation", "link": "https://www.preprints.org/manuscript/202404.1810/download/final_file", "details": "K Do\u011fan, T SEL\u00c7UK, A ALKAN - 2024", "abstract": "Pulmonary Embolism (PE) is the obstruction of blood arteries in the lungs by a blood clot. The mortality risk for PE is approximately 30%. Detecting pulmonary embolism in the segmental arteries of the lung is more challenging than in the main arteries \u2026"}, {"title": "Language model-based labeling of German thoracic radiology reports", "link": "https://www.thieme-connect.com/products/ejournals/html/10.1055/a-2287-5054", "details": "A Wollek, P Haitzer, T Sedlmeyr, S Hyska, J Rueckel\u2026 - R\u00f6Fo-Fortschritte auf dem \u2026, 2024", "abstract": "The aim of this study was to explore the potential of weak supervision in a deep learning-based label prediction model. The goal was to use this model to extract labels from German free-text thoracic radiology reports on chest X-ray images and for \u2026"}, {"title": "Characterizing the Accuracy-Efficiency Trade-off of Low-rank Decomposition in Language Models", "link": "https://arxiv.org/pdf/2405.06626", "details": "M PELLAUER - arXiv preprint arXiv:2405.06626, 2024", "abstract": "Large language models (LLMs) such as GPT-4 [1] have opened a new era of artificial intelligence (AI) technologies, based on broad problem-solving capabilities and even encompassing generative tasks [2] interfaced with natural languages. Natural \u2026"}, {"title": "Meta In-Context Learning Makes Large Language Models Better Zero and Few-Shot Relation Extractors", "link": "https://arxiv.org/pdf/2404.17807", "details": "G Li, P Wang, J Liu, Y Guo, K Ji, Z Shang, Z Xu - arXiv preprint arXiv:2404.17807, 2024", "abstract": "Relation extraction (RE) is an important task that aims to identify the relationships between entities in texts. While large language models (LLMs) have revealed remarkable in-context learning (ICL) capability for general zero and few-shot \u2026"}]
