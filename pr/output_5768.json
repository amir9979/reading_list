[{"title": "XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models", "link": "https://aclanthology.org/2024.bionlp-1.35.pdf", "details": "OC Thawakar, AM Shaker, SS Mullappilly, H Cholakkal\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "The latest breakthroughs in large language models (LLMs) and vision-language models (VLMs) have showcased promising capabilities toward performing a wide range of tasks. Such models are typically trained on massive datasets comprising \u2026"}, {"title": "Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering", "link": "https://arxiv.org/pdf/2407.21368", "details": "D Guo, D Terzopoulos - arXiv preprint arXiv:2407.21368, 2024", "abstract": "Large Vision-Language Models (LVLMs) have achieved significant success in recent years, and they have been extended to the medical domain. Although demonstrating satisfactory performance on medical Visual Question Answering (VQA) tasks \u2026"}, {"title": "A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models", "link": "https://aclanthology.org/2024.gebnlp-1.1.pdf", "details": "Y Wang, V Demberg - Proceedings of the 5th Workshop on Gender Bias in \u2026, 2024", "abstract": "Pre-trained language models have shown impressive abilities of understanding and generating natural languages. However, they typically inherit undesired human-like bias and stereotypes from training data, which raises concerns about putting these \u2026"}, {"title": "EPFL-MAKE at \u201cDischarge Me!\u201d: An LLM System for Automatically Generating Discharge Summaries of Clinical Electronic Health Record", "link": "https://aclanthology.org/2024.bionlp-1.61.pdf", "details": "H Wu, P Boulenger, A Faure, B C\u00e9spedes, F Boukil\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "This paper presents our contribution to the Streamlining Discharge Documentation shared task organized as part of the ACL'24 workshop. We propose MEDISCHARGE (Meditron-7B Based Medical Summary Generation System for Discharge Me), an \u2026"}, {"title": "Improving Self-training with Prototypical Learning for Source-Free Domain Adaptation on Clinical Text", "link": "https://aclanthology.org/2024.bionlp-1.1.pdf", "details": "S Shimizu, S Yada, L Raithel, E Aramaki - Proceedings of the 23rd Workshop on \u2026, 2024", "abstract": "Abstract Domain adaptation is crucial in the clinical domain since the performance of a model trained on one domain (source) degrades seriously when applied to another domain (target). However, conventional domain adaptation methods often cannot be \u2026"}, {"title": "Explainable AI Reloaded: Challenging the XAI Status Quo in the Era of Large Language Models", "link": "https://arxiv.org/pdf/2408.05345", "details": "U Ehsan, MO Riedl - arXiv preprint arXiv:2408.05345, 2024", "abstract": "When the initial vision of Explainable (XAI) was articulated, the most popular framing was to open the (proverbial)\" black-box\" of AI so that we could understand the inner workings. With the advent of Large Language Models (LLMs), the very ability to open \u2026"}, {"title": "Large Language Models Can Learn Representation in Natural Language", "link": "https://aclanthology.org/2024.findings-acl.542.pdf", "details": "Y Guo, Y Liang, D Zhao, N Duan - Findings of the Association for Computational \u2026, 2024", "abstract": "One major challenge for Large Language Models (LLMs) is completing complex tasks involving multiple entities, such as tool APIs. To tackle this, one approach is to retrieve relevant entities to enhance LLMs in task completion. A crucial issue here is \u2026"}, {"title": "How Proficient Are Large Language Models in Formal Languages? An In-Depth Insight for Knowledge Base Question Answering", "link": "https://aclanthology.org/2024.findings-acl.45.pdf", "details": "J Liu, S Cao, J Shi, T Zhang, L Nie, L Hu, L Hou, J Li - Findings of the Association for \u2026, 2024", "abstract": "Abstract Knowledge Base Question Answering (KBQA) aims to answer natural language questions based on facts in knowledge bases. A typical approach to KBQA is semantic parsing, which translates a question into an executable logical form in a \u2026"}, {"title": "Yale at \u201cDischarge Me!\u201d: Evaluating Constrained Generation of Discharge Summaries with Unstructured and Structured Information", "link": "https://aclanthology.org/2024.bionlp-1.64.pdf", "details": "V Socrates, T Huang, X Ai, S Fereydooni, Q Chen\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "In this work, we propose our top-ranking (2nd place) pipeline for the generation of discharge summary subsections as a part of the BioNLP 2024 Shared Task 2:\u201cDischarge Me!\u201d. We evaluate both encoder-decoder and state-of-the-art decoder \u2026"}]
