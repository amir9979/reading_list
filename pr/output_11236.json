[{"title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining", "link": "https://arxiv.org/pdf/2501.00958", "details": "W Zhang, H Zhang, X Li, J Sun, Y Shen, W Lu, D Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge \u2026"}, {"title": "Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models", "link": "https://arxiv.org/pdf/2412.18609%3F", "details": "J Yi, ST Wasim, Y Luo, M Naseer, J Gall - arXiv preprint arXiv:2412.18609, 2024", "abstract": "We present an efficient encoder-free approach for video-language understanding that achieves competitive performance while significantly reducing computational overhead. Current video-language models typically rely on heavyweight image \u2026"}, {"title": "SAT: Spatial Aptitude Training for Multimodal Language Models", "link": "https://arxiv.org/pdf/2412.07755", "details": "A Ray, J Duan, R Tan, D Bashkirova, R Hendrix\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Spatial perception is a fundamental component of intelligence. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only test for static spatial reasoning, such as categorizing the relative \u2026"}, {"title": "MMFactory: A Universal Solution Search Engine for Vision-Language Tasks", "link": "https://arxiv.org/pdf/2412.18072", "details": "WC Fan, T Rahman, L Sigal - arXiv preprint arXiv:2412.18072, 2024", "abstract": "With advances in foundational and vision-language models, and effective fine-tuning techniques, a large number of both general and special-purpose models have been developed for a variety of visual tasks. Despite the flexibility and accessibility of these \u2026"}, {"title": "MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization", "link": "https://arxiv.org/pdf/2412.06141", "details": "K Zhu, P Xia, Y Li, H Zhu, S Wang, H Yao - arXiv preprint arXiv:2412.06141, 2024", "abstract": "The advancement of Large Vision-Language Models (LVLMs) has propelled their application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter factuality challenges due to modality misalignment, where the models prioritize \u2026"}, {"title": "ICONS: Influence Consensus for Vision-Language Data Selection", "link": "https://arxiv.org/pdf/2501.00654", "details": "X Wu, M Xia, R Shao, Z Deng, PW Koh, O Russakovsky - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual Instruction Tuning typically requires a large amount of vision-language training data. This data often containing redundant information that increases computational costs without proportional performance gains. In this work, we \u2026"}, {"title": "Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment", "link": "https://arxiv.org/pdf/2412.19326", "details": "Z Yan, Z Li, Y He, C Wang, K Li, X Li, X Zeng, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Current multimodal large language models (MLLMs) struggle with fine-grained or precise understanding of visuals though they give comprehensive perception and reasoning in a spectrum of vision applications. Recent studies either develop tool \u2026"}, {"title": "EchoApex: A General-Purpose Vision Foundation Model for Echocardiography", "link": "https://www.researchsquare.com/article/rs-5332987/latest.pdf", "details": "Y Zhang, A Amadou, S Piat, P Klein, I Schmuecking\u2026 - 2024", "abstract": "Quantitative evaluation of echocardiography is essential for precise assessment of cardiac condition and guiding treatment decisions. The diverse nature of echo images, including variations in probe types, manufacturers, and pathologies, poses \u2026"}, {"title": "URFM: a general Ultrasound Representation Foundation Model for advancing ultrasound image diagnosis", "link": "https://www.researchsquare.com/article/rs-5662163/latest.pdf", "details": "Q Kang, Q Lao, J Gao, W Bao, Q Lu, K Li - 2024", "abstract": "Ultrasound imaging is pivotal in clinical diagnostics, providing critical insights into a wide range of diseases and organs. However, advancing artificial intelligence (AI) in this field is hindered by challenges such as the reliance on large labeled datasets \u2026"}]
