[{"title": "Soft Prompts Go Hard: Steering Visual Language Models with Hidden Meta-Instructions", "link": "https://arxiv.org/pdf/2407.08970", "details": "T Zhang, C Zhang, JX Morris, E Bagdasaryan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce a new type of indirect injection vulnerabilities in language models that operate on images: hidden\" meta-instructions\" that influence how the model interprets the image and steer the model's outputs to express an adversary-chosen \u2026"}, {"title": "Algorithmic Language Models with Neurally Compiled Libraries", "link": "https://arxiv.org/pdf/2407.04899", "details": "L Saldyt, S Kambhampati - arXiv preprint arXiv:2407.04899, 2024", "abstract": "Important tasks such as reasoning and planning are fundamentally algorithmic, meaning that solving them robustly requires acquiring true reasoning or planning algorithms, rather than shortcuts. Large Language Models lack true algorithmic \u2026"}, {"title": "Multi-grained Correspondence Learning of Audio-language Models for Few-shot Audio Recognition", "link": "https://openreview.net/pdf%3Fid%3D3BFo7AYjoa", "details": "S Zhao, X Linhai, Y Liu, S Du - ACM Multimedia 2024", "abstract": "Large-scale pre-trained audio-language models excel in general multi-modal representation, facilitating their adaptation to downstream audio recognition tasks in a data-efficient manner. However, existing few-shot audio recognition methods \u2026"}, {"title": "A Training Data Recipe to Accelerate A* Search with Language Models", "link": "https://arxiv.org/pdf/2407.09985", "details": "D Gupta, B Li - arXiv preprint arXiv:2407.09985, 2024", "abstract": "Recent works in AI planning have proposed to combine LLMs with iterative tree- search algorithms like A* and MCTS, where LLMs are typically used to calculate the heuristic, guiding the planner towards the goal. However, combining these \u2026"}, {"title": "Generalization vs Memorization: Tracing Language Models' Capabilities Back to Pretraining Data", "link": "https://arxiv.org/pdf/2407.14985", "details": "A Antoniades, X Wang, Y Elazar, A Amayuelas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the proven utility of large language models (LLMs) in real-world applications, there remains a lack of understanding regarding how they leverage their large-scale pretraining text corpora to achieve such capabilities. In this work, we investigate the \u2026"}, {"title": "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation", "link": "https://arxiv.org/pdf/2407.02056", "details": "X Wang, Y Li, S Feng, P Yuan, B Pan, H Wang, Y Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-consistency (SC), leveraging multiple samples from LLMs, shows significant gains on various reasoning tasks but struggles with free-form generation due to the difficulty of aggregating answers. Its variants, UCS and USC, rely on sample \u2026"}, {"title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application", "link": "https://arxiv.org/pdf/2407.01885", "details": "C Yang, W Lu, Y Zhu, Y Wang, Q Chen, C Gao, B Yan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have showcased exceptional capabilities in various domains, attracting significant interest from both academia and industry. Despite their impressive performance, the substantial size and computational demands of LLMs \u2026"}, {"title": "A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data", "link": "https://arxiv.org/pdf/2407.02112", "details": "A Tschalzev, S Marton, S L\u00fcdtke, C Bartelt\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Tabular data is prevalent in real-world machine learning applications, and new models for supervised learning of tabular data are frequently proposed. Comparative studies assessing the performance of models typically consist of model-centric \u2026"}, {"title": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents", "link": "https://arxiv.org/pdf/2407.01887", "details": "F Xia, H Liu, Y Yue, T Li - arXiv preprint arXiv:2407.01887, 2024", "abstract": "In-context decision-making is an important capability of artificial general intelligence, which Large Language Models (LLMs) have effectively demonstrated in various scenarios. However, LLMs often face challenges when dealing with numerical \u2026"}]
