[{"title": "Enhancing Machine-Generated Text Detection: Adversarial Fine-Tuning of Pre-Trained Language Models", "link": "https://ieeexplore.ieee.org/iel7/6287639/6514899/10520300.pdf", "details": "DH Lee, B Jang - IEEE Access, 2024", "abstract": "Advances in large language models (LLMs) have revolutionized the natural language processing field. However, the texts generated raise several social issues. In addition, detecting machine-generated text is becoming increasingly difficult \u2026"}, {"title": "Towards Real World Debiasing: A Fine-grained Analysis On Spurious Correlation", "link": "https://arxiv.org/pdf/2405.15240", "details": "Z Wang, P Kuang, Z Chu, J Wang, K Ren - arXiv preprint arXiv:2405.15240, 2024", "abstract": "Spurious correlations in training data significantly hinder the generalization capability of machine learning models when faced with distribution shifts in real- world scenarios. To tackle the problem, numerous debias approaches have been \u2026"}, {"title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models", "link": "https://arxiv.org/pdf/2405.01535", "details": "S Kim, J Suk, S Longpre, BY Lin, J Shin, S Welleck\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source \u2026"}, {"title": "HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models", "link": "https://arxiv.org/pdf/2405.10299", "details": "RS Sukthanker, A Zela, B Staffler, JKH Franke, F Hutter - arXiv preprint arXiv \u2026, 2024", "abstract": "The expanding size of language models has created the necessity for a comprehensive examination across various dimensions that reflect the desiderata with respect to the tradeoffs between various hardware metrics, such as latency \u2026"}, {"title": "Worldwide Federated Training of Language Models", "link": "https://arxiv.org/pdf/2405.14446", "details": "A Iacob, L Sani, B Marino, P Aleksandrov, ND Lane - arXiv preprint arXiv:2405.14446, 2024", "abstract": "The reliance of language model training on massive amounts of computation and vast datasets scraped from potentially low-quality, copyrighted, or sensitive data has come into question practically, legally, and ethically. Federated learning provides a \u2026"}, {"title": "Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models", "link": "https://arxiv.org/pdf/2405.12522", "details": "C O'Neill, T Bui - arXiv preprint arXiv:2405.12522, 2024", "abstract": "This paper introduces an efficient and robust method for discovering interpretable circuits in large language models using discrete sparse autoencoders. Our approach addresses key limitations of existing techniques, namely computational complexity \u2026"}, {"title": "A primer on the inner workings of transformer-based language models", "link": "https://arxiv.org/pdf/2405.00208", "details": "J Ferrando, G Sarti, A Bisazza, MR Costa-juss\u00e0 - arXiv preprint arXiv:2405.00208, 2024", "abstract": "The rapid progress of research aimed at interpreting the inner workings of advanced language models has highlighted a need for contextualizing the insights gained from years of work in this area. This primer provides a concise technical introduction to the \u2026"}, {"title": "Autonomous Data Selection with Language Models for Mathematical Texts", "link": "https://openreview.net/pdf%3Fid%3DbBF077z8LF", "details": "Y Zhang, Y Luo, Y Yuan, AC Yao - ICLR 2024 Workshop on Navigating and \u2026, 2024", "abstract": "To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or \u2026"}, {"title": "Structural Pruning of Pre-trained Language Models via Neural Architecture Search", "link": "https://arxiv.org/pdf/2405.02267", "details": "A Klein, J Golebiowski, X Ma, V Perrone\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models (PLM), for example BERT or RoBERTa, mark the state- of-the-art for natural language understanding task when fine-tuned on labeled data. However, their large size poses challenges in deploying them for inference in real \u2026"}]
