'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PMC-LLaMA: toward building open-source language models for m'
[{"title": "Understanding emergent abilities of language models from the loss perspective", "link": "https://arxiv.org/pdf/2403.15796", "details": "Z Du, A Zeng, Y Dong, J Tang - arXiv preprint arXiv:2403.15796, 2024", "abstract": "Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) \u2026"}, {"title": "Personalized Federated Graph Learning on Non-IID Electronic Health Records", "link": "https://scholarsmine.mst.edu/cgi/viewcontent.cgi%3Farticle%3D2465%26context%3Dcomsci_facwork", "details": "T Tang, Z Han, Z Cai, S Yu, X Zhou, T Oseni, SK Das - IEEE Transactions on Neural \u2026, 2024", "abstract": "Understanding the latent disease patterns embedded in electronic health records (EHRs) is crucial for making precise and proactive healthcare decisions. Federated graph learning-based methods are commonly employed to extract complex disease \u2026"}, {"title": "DINGO: Towards Diverse and Fine-Grained Instruction-Following Evaluation", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29768/31322", "details": "Z Gu, X Sun, F Lian, Z Kang, C Xu, J Fan - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Instruction-following is particularly crucial for large language models (LLMs) to support diverse user requests. While existing work has made progress in aligning LLMs with human preferences, evaluating their capabilities on instruction-following \u2026"}, {"title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models", "link": "https://arxiv.org/pdf/2403.19647", "details": "S Marks, C Rager, EJ Michaud, Y Belinkov, D Bau\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic \u2026"}, {"title": "Learning To Guide Human Decision Makers With Vision-Language Models", "link": "https://arxiv.org/pdf/2403.16501", "details": "D Banerjee, S Teso, BS Grunel, A Passerini - arXiv preprint arXiv:2403.16501, 2024", "abstract": "There is increasing interest in developing AIs for assisting human decision making in\\textit {high-stakes} tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain.% Mainstream approaches team up an \u2026"}, {"title": "MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models", "link": "https://arxiv.org/pdf/2403.17141", "details": "K Yang, Z Liu, Q Xie, T Zhang, N Song, J Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in large language models (LLMs) aim to tackle heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are parameter-adherent to the policy model \u2026"}, {"title": "Generative Language Models for Personalized Information Understanding", "link": "https://scholarworks.umass.edu/cgi/viewcontent.cgi%3Farticle%3D4123%26context%3Ddissertations_2", "details": "P Cai - 2024", "abstract": "A major challenge in information understanding stems from the diverse nature of the audience, where individuals possess varying preferences, experiences, educational and cultural backgrounds. Consequently, adopting a one-size-fits-all approach to \u2026"}, {"title": "Information Retrieval Techniques for Question Answering based on Pre-Trained Language Models", "link": "https://rcs.cic.ipn.mx/2023_152_12/Information%2520Retrieval%2520Techniques%2520for%2520Question%2520Answering%2520based%2520on%2520Pre-Trained.pdf", "details": "A Cadena, FF L\u00f3pez-Ponce, G Sierra, J L\u00e1zaro\u2026", "abstract": "This paper presents a comparative study between two prominent pre-trained language models, RoBERTa and GPT-3, focused on their performance in Question Answering (QA). Broker exams serve as a rigorous evaluation guide, with which we \u2026"}, {"title": "Predicting involuntary admission following inpatient psychiatric treatment using machine learning trained on electronic health record data", "link": "https://www.medrxiv.org/content/10.1101/2024.04.11.24305658.full.pdf", "details": "E Perfalk, JG Damgaard, M Bernstorff, L Hansen\u2026 - medRxiv, 2024", "abstract": "Background Involuntary admissions to psychiatric hospitals are on the rise. If patients at elevated risk of involuntary admission could be identified, prevention may be possible. Objectives To develop and validate a prediction model for involuntary \u2026"}]
