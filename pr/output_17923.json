[{"title": "EvoLM: In Search of Lost Language Model Training Dynamics", "link": "https://arxiv.org/pdf/2506.16029", "details": "Z Qi, F Nie, A Alahi, J Zou, H Lakkaraju, Y Du, E Xing\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and \u2026", "entry_id": "http://arxiv.org/abs/2506.16029v1", "updated": "2025-06-19 04:58:47", "published": "2025-06-19 04:58:47", "authors": "Zhenting Qi;Fan Nie;Alexandre Alahi;James Zou;Himabindu Lakkaraju;Yilun Du;Eric Xing;Sham Kakade;Hanlin Zhang", "summary": "Modern language model (LM) training has been divided into multiple stages,\nmaking it difficult for downstream developers to evaluate the impact of design\nchoices made at each stage. We present EvoLM, a model suite that enables\nsystematic and transparent analysis of LMs' training dynamics across\npre-training, continued pre-training, supervised fine-tuning, and reinforcement\nlearning. By training over 100 LMs with 1B and 4B parameters from scratch, we\nrigorously evaluate both upstream (language modeling) and downstream\n(problem-solving) reasoning capabilities, including considerations of both\nin-domain and out-of-domain generalization. Key insights highlight the\ndiminishing returns from excessive pre-training and post-training, the\nimportance and practices of mitigating forgetting during domain-specific\ncontinued pre-training, the crucial role of continued pre-training in bridging\npre-training and post-training phases, and various intricate trade-offs when\nconfiguring supervised fine-tuning and reinforcement learning. To facilitate\nopen research and reproducibility, we release all pre-trained and post-trained\nmodels, training datasets for all stages, and our entire training and\nevaluation pipeline.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2506.16029v1;http://arxiv.org/pdf/2506.16029v1", "pdf_url": "http://arxiv.org/pdf/2506.16029v1"}, {"title": "RadioRAG: Online Retrieval-augmented Generation for Radiology Question Answering", "link": "https://pubs.rsna.org/doi/abs/10.1148/ryai.240476", "details": "S Tayebi Arasteh, M Lotfinia, K Bressem, R Siepmann\u2026 - Radiology: Artificial \u2026, 2025", "abstract": "\u201cJust Accepted\u201d papers have undergone full peer review and have been accepted for publication in Radiology: Artificial Intelligence. This article will undergo copyediting, layout, and proof review before it is published in its final version. Please note that \u2026"}, {"title": "Search Engines in the AI Era: A Qualitative Understanding to the False Promise of Factual and Verifiable Source-Cited Responses in LLM-based Search", "link": "https://dl.acm.org/doi/abs/10.1145/3715275.3732089", "details": "P Narayanan Venkit, P Laban, Y Zhou, Y Mao, CS Wu - Proceedings of the 2025 ACM \u2026, 2025", "abstract": "As Large Language Model (LLM) applications transition from research to widespread sociotechnical products, they are fundamentally changing how people access and process information. Answer engines-LLM-powered search tools that generate \u2026"}, {"title": "Inference-Time Reward Hacking in Large Language Models", "link": "https://arxiv.org/pdf/2506.19248", "details": "H Khalaf, CM Verdun, A Oesterling, H Lakkaraju\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to LLM outputs indicating, for example, which response would likely be preferred by a user \u2026", "entry_id": "http://arxiv.org/abs/2506.19248v1", "updated": "2025-06-24 02:05:25", "published": "2025-06-24 02:05:25", "authors": "Hadi Khalaf;Claudio Mayrink Verdun;Alex Oesterling;Himabindu Lakkaraju;Flavio du Pin Calmon", "summary": "A common paradigm to improve the performance of large language models is\noptimizing for a reward model. Reward models assign a numerical score to LLM\noutputs indicating, for example, which response would likely be preferred by a\nuser or is most aligned with safety goals. However, reward models are never\nperfect. They inevitably function as proxies for complex desiderata such as\ncorrectness, helpfulness, and safety. By overoptimizing for a misspecified\nreward, we can subvert intended alignment goals and reduce overall performance\n-- a phenomenon commonly referred to as reward hacking. In this work, we\ncharacterize reward hacking in inference-time alignment and demonstrate when\nand how we can mitigate it by hedging on the proxy reward. We study this\nphenomenon under Best-of-$n$ (BoN) and Soft-Best-of-$n$ (SBoN), and we\nintroduce Best-of-Poisson (BoP) that provides an efficient, near-exact\napproximation of the optimal reward-KL divergence policy at inference time. We\nshow that the characteristic pattern of hacking as observed in practice (where\nthe true reward first increases before declining) is an inevitable property of\na broad class of inference-time mechanisms, including BoN and BoP. To counter\nthis effect, hedging offers a tactical choice to avoid placing undue confidence\nin high but potentially misleading proxy reward signals. We introduce\nHedgeTune, an efficient algorithm to find the optimal inference-time parameter\nand avoid reward hacking. We demonstrate through experiments that hedging\nmitigates reward hacking and achieves superior distortion-reward tradeoffs with\nminimal computational overhead.", "comment": "Accepted to ICML 2025 Workshop on Models of Human Feedback for AI\n  Alignment", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2506.19248v1;http://arxiv.org/pdf/2506.19248v1", "pdf_url": "http://arxiv.org/pdf/2506.19248v1"}, {"title": "Towards Large Language Models with Self-Consistent Natural Language Explanations", "link": "https://arxiv.org/pdf/2506.07523", "details": "S Admoni, O Amir, A Hallak, Y Ziser - arXiv preprint arXiv:2506.07523, 2025", "abstract": "Large language models (LLMs) seem to offer an easy path to interpretability: just ask them to explain their decisions. Yet, studies show that these post-hoc explanations often misrepresent the true decision process, as revealed by mismatches in feature \u2026", "entry_id": "http://arxiv.org/abs/2506.07523v2", "updated": "2025-06-12 08:54:59", "published": "2025-06-09 08:06:33", "authors": "Sahar Admoni;Ofra Amir;Assaf Hallak;Yftah Ziser", "summary": "Large language models (LLMs) seem to offer an easy path to interpretability:\njust ask them to explain their decisions. Yet, studies show that these post-hoc\nexplanations often misrepresent the true decision process, as revealed by\nmismatches in feature importance. Despite growing evidence of this\ninconsistency, no systematic solutions have emerged, partly due to the high\ncost of estimating feature importance, which limits evaluations to small\ndatasets. To address this, we introduce the Post-hoc Self-Consistency Bank\n(PSCB) - a large-scale benchmark of decisions spanning diverse tasks and\nmodels, each paired with LLM-generated explanations and corresponding feature\nimportance scores. Analysis of PSCB reveals that self-consistency scores barely\ndiffer between correct and incorrect predictions. We also show that the\nstandard metric fails to meaningfully distinguish between explanations. To\novercome this limitation, we propose an alternative metric that more\neffectively captures variation in explanation quality. We use it to fine-tune\nLLMs via Direct Preference Optimization (DPO), leading to significantly better\nalignment between explanations and decision-relevant features, even under\ndomain shift. Our findings point to a scalable path toward more trustworthy,\nself-consistent LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.07523v2;http://arxiv.org/pdf/2506.07523v2", "pdf_url": "http://arxiv.org/pdf/2506.07523v2"}, {"title": "Hallucination Detection in Large Language Models Using Diversion Decoding", "link": "https://link.springer.com/chapter/10.1007/978-3-031-96590-6_7", "details": "B Abdeen, SM Siddiqui, MT Ahmed, A Singhal, L Khan\u2026 - IFIP Annual Conference on \u2026, 2025", "abstract": "Large language models (LLMs) have emerged as a powerful tool for retrieving knowledge through seamless, human-like interactions. Despite their advanced text generation capabilities, LLMs exhibit hallucination tendencies, where they generate \u2026"}]
