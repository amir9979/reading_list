[{"title": "A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis", "link": "https://arxiv.org/pdf/2505.23601", "details": "S Liu, B Zheng, W Chen, Z Peng, Z Yin, J Shao, J Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 We introduce EndoBench, the most comprehensive benchmark to date for **evaluating** multi-modal **large** **language** **models** in endoscopic image analysis. Our results show that while proprietary and domain-adapted MLLMs outperform open-source \u2026", "entry_id": "http://arxiv.org/abs/2505.23601v1", "updated": "2025-05-29 16:14:34", "published": "2025-05-29 16:14:34", "authors": "Shengyuan Liu;Boyun Zheng;Wenting Chen;Zhihao Peng;Zhenfei Yin;Jing Shao;Jiancong Hu;Yixuan Yuan", "summary": "Endoscopic procedures are essential for diagnosing and treating internal\ndiseases, and multi-modal large language models (MLLMs) are increasingly\napplied to assist in endoscopy analysis. However, current benchmarks are\nlimited, as they typically cover specific endoscopic scenarios and a small set\nof clinical tasks, failing to capture the real-world diversity of endoscopic\nscenarios and the full range of skills needed in clinical workflows. To address\nthese issues, we introduce EndoBench, the first comprehensive benchmark\nspecifically designed to assess MLLMs across the full spectrum of endoscopic\npractice with multi-dimensional capacities. EndoBench encompasses 4 distinct\nendoscopic scenarios, 12 specialized clinical tasks with 12 secondary subtasks,\nand 5 levels of visual prompting granularities, resulting in 6,832 rigorously\nvalidated VQA pairs from 21 diverse datasets. Our multi-dimensional evaluation\nframework mirrors the clinical workflow--spanning anatomical recognition,\nlesion analysis, spatial localization, and surgical operations--to holistically\ngauge the perceptual and diagnostic abilities of MLLMs in realistic scenarios.\nWe benchmark 23 state-of-the-art models, including general-purpose,\nmedical-specialized, and proprietary MLLMs, and establish human clinician\nperformance as a reference standard. Our extensive experiments reveal: (1)\nproprietary MLLMs outperform open-source and medical-specialized models\noverall, but still trail human experts; (2) medical-domain supervised\nfine-tuning substantially boosts task-specific accuracy; and (3) model\nperformance remains sensitive to prompt format and clinical task complexity.\nEndoBench establishes a new standard for evaluating and advancing MLLMs in\nendoscopy, highlighting both progress and persistent gaps between current\nmodels and expert clinical reasoning. We publicly release our benchmark and\ncode.", "comment": "36 pages, 18 figures", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.23601v1;http://arxiv.org/pdf/2505.23601v1", "pdf_url": "http://arxiv.org/pdf/2505.23601v1"}, {"title": "Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models", "link": "https://arxiv.org/pdf/2505.23715", "details": "J Li, G Li, Y Chang, Y Wu - arXiv preprint arXiv:2505.23715, 2025", "abstract": "\u2026 In conclusion, we present PCBench, a benchmark designed to **evaluate** **large** **language** **models** \u2019 (LLMs) ability to critique flawed premises by \u2026 D Details on **evaluation** metrics To rigorously **evaluate** the premise critique abilities of **large** \u2026", "entry_id": "http://arxiv.org/abs/2505.23715v1", "updated": "2025-05-29 17:49:44", "published": "2025-05-29 17:49:44", "authors": "Jinzhe Li;Gengxu Li;Yi Chang;Yuan Wu", "summary": "Large language models (LLMs) have witnessed rapid advancements, demonstrating\nremarkable capabilities. However, a notable vulnerability persists: LLMs often\nuncritically accept flawed or contradictory premises, leading to inefficient\nreasoning and unreliable outputs. This emphasizes the significance of\npossessing the \\textbf{Premise Critique Ability} for LLMs, defined as the\ncapacity to proactively identify and articulate errors in input premises. Most\nexisting studies assess LLMs' reasoning ability in ideal settings, largely\nignoring their vulnerabilities when faced with flawed premises. Thus, we\nintroduce the \\textbf{Premise Critique Bench (PCBench)}, designed by\nincorporating four error types across three difficulty levels, paired with\nmulti-faceted evaluation metrics. We conducted systematic evaluations of 15\nrepresentative LLMs. Our findings reveal: (1) Most models rely heavily on\nexplicit prompts to detect errors, with limited autonomous critique; (2)\nPremise critique ability depends on question difficulty and error type, with\ndirect contradictions being easier to detect than complex or procedural errors;\n(3) Reasoning ability does not consistently correlate with the premise critique\nability; (4) Flawed premises trigger overthinking in reasoning models, markedly\nlengthening responses due to repeated attempts at resolving conflicts. These\ninsights underscore the urgent need to enhance LLMs' proactive evaluation of\ninput validity, positioning premise critique as a foundational capability for\ndeveloping reliable, human-centric systems. The code is available at\nhttps://github.com/MLGroupJLU/Premise_Critique.", "comment": "31 pages,13 figures,15 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.23715v1;http://arxiv.org/pdf/2505.23715v1", "pdf_url": "http://arxiv.org/pdf/2505.23715v1"}, {"title": "Domain Specific Benchmarks for **Evaluating** Multimodal **Large Language Models**", "link": "https://www.preprints.org/frontend/manuscript/d56e264011a4a69f9edd5f4fdcfbb212/download_pub", "details": "K Anjum, MA Arshad, K Hayawi, E Polyzos, A Tariq\u2026 - 2025", "abstract": "\u2026 Manufacturing & Design Recent years have seen the emergence of several specialized benchmarks and studies aimed at rigorously **evaluating** the capabilities of **large** **language** **models** (LLMs) and multimodal LLMs (MLLMs) in the context of \u2026"}, {"title": "Talent or Luck? Evaluating Attribution Bias in Large Language Models", "link": "https://arxiv.org/pdf/2505.22910", "details": "C Raj, M Banerjee, A Caliskan, A Anastasopoulos\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "When a student fails an exam, do we tend to blame their effort or the test's difficulty? Attribution, defined as how reasons are assigned to event outcomes, shapes perceptions, reinforces stereotypes, and influences decisions. Attribution Theory in \u2026", "entry_id": "http://arxiv.org/abs/2505.22910v1", "updated": "2025-05-28 22:18:46", "published": "2025-05-28 22:18:46", "authors": "Chahat Raj;Mahika Banerjee;Aylin Caliskan;Antonios Anastasopoulos;Ziwei Zhu", "summary": "When a student fails an exam, do we tend to blame their effort or the test's\ndifficulty? Attribution, defined as how reasons are assigned to event outcomes,\nshapes perceptions, reinforces stereotypes, and influences decisions.\nAttribution Theory in social psychology explains how humans assign\nresponsibility for events using implicit cognition, attributing causes to\ninternal (e.g., effort, ability) or external (e.g., task difficulty, luck)\nfactors. LLMs' attribution of event outcomes based on demographics carries\nimportant fairness implications. Most works exploring social biases in LLMs\nfocus on surface-level associations or isolated stereotypes. This work proposes\na cognitively grounded bias evaluation framework to identify how models'\nreasoning disparities channelize biases toward demographic groups.", "comment": "18 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.22910v1;http://arxiv.org/pdf/2505.22910v1", "pdf_url": "http://arxiv.org/pdf/2505.22910v1"}, {"title": "Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons", "link": "https://arxiv.org/pdf/2505.23477", "details": "K Vishwanath, A Alyakin, M Ghosh, JV Lee, DA Alber\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Recently, these questions have also served as benchmarks for **evaluating** **large** **language** **models** \u2019 (LLMs) \u2026 **evaluate** their robustness to the inclusion of distractor statements. METHODS: A comprehensive **evaluation** was conducted using 28 state-of-the-art \u2026", "entry_id": "http://arxiv.org/abs/2505.23477v1", "updated": "2025-05-29 14:27:14", "published": "2025-05-29 14:27:14", "authors": "Krithik Vishwanath;Anton Alyakin;Mrigayu Ghosh;Jin Vivian Lee;Daniel Alexander Alber;Karl L. Sangwon;Douglas Kondziolka;Eric Karl Oermann", "summary": "The Congress of Neurological Surgeons Self-Assessment for Neurological\nSurgeons (CNS-SANS) questions are widely used by neurosurgical residents to\nprepare for written board examinations. Recently, these questions have also\nserved as benchmarks for evaluating large language models' (LLMs) neurosurgical\nknowledge. This study aims to assess the performance of state-of-the-art LLMs\non neurosurgery board-like questions and to evaluate their robustness to the\ninclusion of distractor statements. A comprehensive evaluation was conducted\nusing 28 large language models. These models were tested on 2,904 neurosurgery\nboard examination questions derived from the CNS-SANS. Additionally, the study\nintroduced a distraction framework to assess the fragility of these models. The\nframework incorporated simple, irrelevant distractor statements containing\npolysemous words with clinical meanings used in non-clinical contexts to\ndetermine the extent to which such distractions degrade model performance on\nstandard medical benchmarks. 6 of the 28 tested LLMs achieved board-passing\noutcomes, with the top-performing models scoring over 15.7% above the passing\nthreshold. When exposed to distractions, accuracy across various model\narchitectures was significantly reduced-by as much as 20.4%-with one model\nfailing that had previously passed. Both general-purpose and medical\nopen-source models experienced greater performance declines compared to\nproprietary variants when subjected to the added distractors. While current\nLLMs demonstrate an impressive ability to answer neurosurgery board-like exam\nquestions, their performance is markedly vulnerable to extraneous, distracting\ninformation. These findings underscore the critical need for developing novel\nmitigation strategies aimed at bolstering LLM resilience against in-text\ndistractions, particularly for safe and effective clinical deployment.", "comment": "22 pages, 3 main figures, 3 supplemental figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.23477v1;http://arxiv.org/pdf/2505.23477v1", "pdf_url": "http://arxiv.org/pdf/2505.23477v1"}, {"title": "Permissioned LLMs: Enforcing Access Control in Large Language Models", "link": "https://arxiv.org/pdf/2505.22860", "details": "B Jayaraman, VJ Marathe, H Mozaffari, WF Shen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 We also introduce a novel metric, called access advantage, to empirically **evaluate** the \u2026 **evaluation**. We demonstrate the efficacy of our PermLLM mechanisms through extensive experiments on four public datasets (GPQA, RCV1 \u2026", "entry_id": "http://arxiv.org/abs/2505.22860v1", "updated": "2025-05-28 20:47:02", "published": "2025-05-28 20:47:02", "authors": "Bargav Jayaraman;Virendra J. Marathe;Hamid Mozaffari;William F. Shen;Krishnaram Kenthapadi", "summary": "In enterprise settings, organizational data is segregated, siloed and\ncarefully protected by elaborate access control frameworks. These access\ncontrol structures can completely break down if an LLM fine-tuned on the siloed\ndata serves requests, for downstream tasks, from individuals with disparate\naccess privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs\nthat superimpose the organizational data access control structures on query\nresponses they generate. We formalize abstractions underpinning the means to\ndetermine whether access control enforcement happens correctly over LLM query\nresponses. Our formalism introduces the notion of a relevant response that can\nbe used to prove whether a PermLLM mechanism has been implemented correctly. We\nalso introduce a novel metric, called access advantage, to empirically evaluate\nthe efficacy of a PermLLM mechanism. We introduce three novel PermLLM\nmechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired\naccess control. We furthermore present two instantiations of access\nadvantage--(i) Domain Distinguishability Index (DDI) based on Membership\nInference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility\nevaluation. We demonstrate the efficacy of our PermLLM mechanisms through\nextensive experiments on four public datasets (GPQA, RCV1, SimpleQA, and WMDP),\nin addition to evaluating the validity of DDI and UGI metrics themselves for\nquantifying access control in LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.22860v1;http://arxiv.org/pdf/2505.22860v1", "pdf_url": "http://arxiv.org/pdf/2505.22860v1"}, {"title": "SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services", "link": "https://arxiv.org/pdf/2505.23065", "details": "H Guo, Z Xie, S Cao, B Wang, W Liu, A Le, L Li, Z Li - arXiv preprint arXiv:2505.23065, 2025", "abstract": "\u2026 In summary, on one hand, current multimodal benchmarks fail to comprehensively **evaluate** realworld SNS. On the other hand, this deficiency in objective and thorough assessment hinders the practical application of MLLMs in \u2026", "entry_id": "http://arxiv.org/abs/2505.23065v1", "updated": "2025-05-29 04:16:24", "published": "2025-05-29 04:16:24", "authors": "Hongcheng Guo;Zheyong Xie;Shaosheng Cao;Boyang Wang;Weiting Liu;Anjie Le;Lei Li;Zhoujun Li", "summary": "With the increasing integration of visual and textual content in Social\nNetworking Services (SNS), evaluating the multimodal capabilities of Large\nLanguage Models (LLMs) is crucial for enhancing user experience, content\nunderstanding, and platform intelligence. Existing benchmarks primarily focus\non text-centric tasks, lacking coverage of the multimodal contexts prevalent in\nmodern SNS ecosystems. In this paper, we introduce SNS-Bench-VL, a\ncomprehensive multimodal benchmark designed to assess the performance of\nVision-Language LLMs in real-world social media scenarios. SNS-Bench-VL\nincorporates images and text across 8 multimodal tasks, including note\ncomprehension, user engagement analysis, information retrieval, and\npersonalized recommendation. It comprises 4,001 carefully curated multimodal\nquestion-answer pairs, covering single-choice, multiple-choice, and open-ended\ntasks. We evaluate over 25 state-of-the-art multimodal LLMs, analyzing their\nperformance across tasks. Our findings highlight persistent challenges in\nmultimodal social context comprehension. We hope SNS-Bench-VL will inspire\nfuture research towards robust, context-aware, and human-aligned multimodal\nintelligence for next-generation social networking services.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.23065v1;http://arxiv.org/pdf/2505.23065v1", "pdf_url": "http://arxiv.org/pdf/2505.23065v1"}, {"title": "Assessing output reliability and similarity of **large language models** in software development: A comparative case study approach", "link": "https://www.sciencedirect.com/science/article/pii/S0950584925001260", "details": "DK Kim, H Ming - Information and Software Technology, 2025", "abstract": "Context: Generative **large** **language** **models** (LLMs) are increasingly used across various activities in software development, offering significant potential to enhance productivity. However, there is a lack of systematic study examining the reliability \u2026"}, {"title": "Leveraging **large language models** for preoperative prevention of cardiopulmonary bypass-associated acute kidney injury", "link": "https://www.tandfonline.com/doi/pdf/10.1080/0886022X.2025.2509786", "details": "K Wang, L Lin, R Zheng, S Nan, X Lu, H Duan - Renal Failure, 2025", "abstract": "\u2026 We propose a multimodal fusion model based on LLM to **evaluate** AKI risk using preoperative clinical characteristics. Then, a structural equation was constructed to optimize intraoperative interventions. The risk of preoperative assessment was a \u2026"}]
