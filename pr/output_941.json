'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Refining Pre-trained Language Models for Domain Adapta'
[{"title": "VLRM: Vision-Language Models act as Reward Models for Image Captioning", "link": "https://arxiv.org/pdf/2404.01911", "details": "M Dzabraev, A Kunitsyn, A Ivaniuta - arXiv preprint arXiv:2404.01911, 2024", "abstract": "In this work, we present an unsupervised method for enhancing an image captioning model (in our case, BLIP2) using reinforcement learning and vision-language models like CLIP and BLIP2-ITM as reward models. The RL-tuned model is able to \u2026"}, {"title": "Distributed Rumor Source Detection Via Boosted Federated Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10504633/", "details": "R Wang, Y Zhang, W Wan, M Chen, M Guizani - IEEE Transactions on Knowledge \u2026, 2024", "abstract": "How to localize the rumor source is a common interest of all sectors of the society. Many researchers have tried to use deep-learning-based graph models to detect rumor sources, but they have neglected how to train their deep-learning-based graph \u2026"}, {"title": "Few-Shot Adaptation of Vision-Language Foundation Models via Dual-Path Inference", "link": "https://openreview.net/pdf%3Fid%3DV1y84mlYHd", "details": "C Zhang, S Stepputtis, KP Sycara, Y Xie - ICLR 2024 Workshop on Mathematical and \u2026", "abstract": "Leveraging vast datasets on the Internet, large-scale Vision-Language Models (VLMs) demonstrates great potential in learning open-world visual concepts, and exhibit remarkable performance across a wide range of downstream tasks through \u2026"}, {"title": "Generative AI-Based Text Generation Methods Using Pre-Trained GPT-2 Model", "link": "https://arxiv.org/pdf/2404.01786", "details": "R Pandey, H Waghela, S Rakshit, A Rangari, A Singh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This work delved into the realm of automatic text generation, exploring a variety of techniques ranging from traditional deterministic approaches to more modern stochastic methods. Through analysis of greedy search, beam search, top-k \u2026"}, {"title": "ContraSim\u2013Analyzing Neural Representations Based on Contrastive Learning", "link": "https://belinkov.com/assets/pdf/naacl2024-contrasim.pdf", "details": "A Rahamim, Y Belinkov", "abstract": "Recent work has compared neural network representations via similarity-based analyses to improve model interpretation. The quality of a similarity measure is typically evaluated by its success in assigning a high score to representations that \u2026"}, {"title": "F-MALLOC: Feed-forward Memory Allocation for Continual Learning in Neural Machine Translation", "link": "https://arxiv.org/pdf/2404.04846", "details": "J Wu, Y Liu, C Zong - arXiv preprint arXiv:2404.04846, 2024", "abstract": "In the evolving landscape of Neural Machine Translation (NMT), the pretrain-then- finetune paradigm has yielded impressive results. However, the persistent challenge of Catastrophic Forgetting (CF) remains a hurdle. While previous work has \u2026"}, {"title": "Source-Aware Training Enables Knowledge Attribution in Language Models", "link": "https://arxiv.org/pdf/2404.01019", "details": "M Khalifa, D Wadden, E Strubell, H Lee, L Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source (s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining \u2026"}, {"title": "Context versus Prior Knowledge in Language Models", "link": "https://arxiv.org/pdf/2404.04633", "details": "K Du, V Sn\u00e6bjarnarson, N Stoehr, JC White, A Schein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To answer a question, language models often need to integrate prior knowledge learned during pretraining and new information presented in context. We hypothesize that models perform this integration in a predictable way across different \u2026"}, {"title": "$\\texttt {LM}^\\texttt {2} $: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://arxiv.org/pdf/2404.02255", "details": "G Juneja, S Dutta, T Chakraborty - arXiv preprint arXiv:2404.02255, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}]
