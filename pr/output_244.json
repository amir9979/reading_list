'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [EVALUATION OF MEDIUM-SIZED LANGUAGE MODELS IN GERMAN A'
[{"title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.07860", "details": "S Zhao, S Hao, B Zi, H Xu, KYK Wong - arXiv preprint arXiv:2403.07860, 2024", "abstract": "Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding \u2026"}, {"title": "A Comprehensive Overhaul of Multimodal Assistant with Small Language Models", "link": "https://arxiv.org/pdf/2403.06199", "details": "M Zhu, Y Zhu, X Liu, N Liu, Z Xu, C Shen, Y Peng, Z Ou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal Large Language Models (MLLMs) have showcased impressive skills in tasks related to visual understanding and reasoning. Yet, their widespread application faces obstacles due to the high computational demands during both the \u2026"}, {"title": "$\\mathbf {(N, K)} $-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model", "link": "https://arxiv.org/html/2403.07191v1", "details": "Y Zhang, L Chen, B Liu, Y Yang, Q Cui, Y Tao, H Yang - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these \u2026"}, {"title": "Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous Driving", "link": "https://arxiv.org/html/2403.05907v1", "details": "J Cao, Z Li, N Wang, C Ma - arXiv preprint arXiv:2403.05907, 2024", "abstract": "Recent studies have highlighted the promising application of NeRF in autonomous driving contexts. However, the complexity of outdoor environments, combined with the restricted viewpoints in driving scenarios, complicates the task of precisely \u2026"}, {"title": "LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation", "link": "https://arxiv.org/html/2403.12019v1", "details": "Y Lan, F Hong, S Yang, S Zhou, X Meng, B Dai, X Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The field of neural rendering has witnessed significant progress with advancements in generative models and differentiable rendering techniques. Though 2D diffusion has achieved success, a unified 3D diffusion pipeline remains unsettled. This paper \u2026"}, {"title": "Diffusion Models are Geometry Critics: Single Image 3D Editing Using Pre-Trained Diffusion Priors", "link": "https://arxiv.org/pdf/2403.11503", "details": "R Wang, J Xiang, J Yang, X Tong - arXiv preprint arXiv:2403.11503, 2024", "abstract": "We propose a novel image editing technique that enables 3D manipulations on single images, such as object rotation and translation. Existing 3D-aware image editing approaches typically rely on synthetic multi-view datasets for training \u2026"}, {"title": "RAFT: Adapting Language Model to Domain Specific RAG", "link": "https://arxiv.org/pdf/2403.10131", "details": "T Zhang, SG Patil, N Jain, S Shen, M Zaharia, I Stoica\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (eg, time-critical news, or private \u2026"}, {"title": "Algorithmic progress in language models", "link": "https://arxiv.org/html/2403.05812v1", "details": "A Ho, T Besiroglu, E Erdil, D Owen, R Rahman, ZC Guo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We investigate the rate at which algorithms for pre-training language models have improved since the advent of deep learning. Using a dataset of over 200 language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we find that \u2026"}, {"title": "Common 7B Language Models Already Possess Strong Math Capabilities", "link": "https://arxiv.org/html/2403.04706v1", "details": "C Li, W Wang, J Hu, Y Wei, N Zheng, H Hu, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits \u2026"}]
