[{"title": "DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling Language Models", "link": "https://openreview.net/forum%3Fid%3Dpvg1OdUtDQ", "details": "S Lu, W Zhao, C Tao, A Gupta, S Wu, T Chung, N Peng - Forty-first International Conference \u2026", "abstract": "NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable generation with large language models. It is designed to avoid catastrophic forgetting while achieving guaranteed convergence to an entropy-maximized closed-form \u2026"}, {"title": "White-box Multimodal Jailbreaks Against Large Vision-Language Models", "link": "https://arxiv.org/pdf/2405.17894", "details": "R Wang, X Ma, H Zhou, C Ji, G Ye, YG Jiang - arXiv preprint arXiv:2405.17894, 2024", "abstract": "Recent advancements in Large Vision-Language Models (VLMs) have underscored their superiority in various multimodal tasks. However, the adversarial robustness of VLMs has not been fully explored. Existing methods mainly assess robustness \u2026"}, {"title": "Super Tiny Language Models", "link": "https://arxiv.org/pdf/2405.14159", "details": "D Hillier, L Guertler, C Tan, P Agrawal, C Ruirui\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancement of large language models (LLMs) has led to significant improvements in natural language processing but also poses challenges due to their high computational and energy demands. This paper introduces a series of research \u2026"}, {"title": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records", "link": "https://aclanthology.org/2024.cl4health-1.23.pdf", "details": "J Lov\u00f3n-Melgarejo, T Ben-Haddi, J Di Scala\u2026 - Proceedings of the First \u2026, 2024", "abstract": "The lack of standardized evaluation benchmarks in the medical domain for text inputs can be a barrier to widely adopting and leveraging the potential of natural language models for health-related downstream tasks. This paper revisited an \u2026"}, {"title": "MAPLM: A Real-World Large-Scale Vision-Language Benchmark for Map and Traffic Scene Understanding", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Cao_MAPLM_A_Real-World_Large-Scale_Vision-Language_Benchmark_for_Map_and_Traffic_CVPR_2024_paper.pdf", "details": "X Cao, T Zhou, Y Ma, W Ye, C Cui, K Tang, Z Cao\u2026 - Proceedings of the IEEE \u2026, 2024", "abstract": "Vision-language generative AI has demonstrated remarkable promise for empowering cross-modal scene understanding of autonomous driving and high- definition (HD) map systems. However current benchmark datasets lack multi-modal \u2026"}, {"title": "ReFiNe: Recursive Field Networks for Cross-Modal Multi-Scene Representation", "link": "https://arxiv.org/pdf/2406.04309", "details": "K LIU, A RARES - 2024", "abstract": "Neural fields that encode scene properties at arbitrary resolutions using neural networks have reached unprecedented levels of detail. Typically using fully- connected multi-layer perceptrons (MLPs) to predict continuous field values, they \u2026"}, {"title": "Representing Animatable Avatar via Factorized Neural Fields", "link": "https://arxiv.org/pdf/2406.00637", "details": "C Song, Z Wu, B Wandt, L Sigal, H Rhodin - arXiv preprint arXiv:2406.00637, 2024", "abstract": "For reconstructing high-fidelity human 3D models from monocular videos, it is crucial to maintain consistent large-scale body shapes along with finely matched subtle wrinkles. This paper explores the observation that the per-frame rendering results \u2026"}, {"title": "Unified Editing of Panorama, 3D Scenes, and Videos Through Disentangled Self-Attention Injection", "link": "https://arxiv.org/pdf/2405.16823", "details": "G Kwon, J Park, JC Ye - arXiv preprint arXiv:2405.16823, 2024", "abstract": "While text-to-image models have achieved impressive capabilities in image generation and editing, their application across various modalities often necessitates training separate models. Inspired by existing method of single image editing with \u2026"}, {"title": "Situational Awareness Matters in 3D Vision Language Reasoning", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Man_Situational_Awareness_Matters_in_3D_Vision_Language_Reasoning_CVPR_2024_paper.pdf", "details": "Y Man, LY Gui, YX Wang - Proceedings of the IEEE/CVF Conference on Computer \u2026, 2024", "abstract": "Being able to carry out complicated vision language reasoning tasks in 3D space represents a significant milestone in developing household robots and human- centered embodied AI. In this work we demonstrate that a critical and distinct \u2026"}]
