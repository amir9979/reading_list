[{"title": "Self-Supervised Representation Distribution Learning for Reliable Data Augmentation in Histopathology WSI Classification", "link": "https://ieeexplore.ieee.org/abstract/document/10643565/", "details": "K Tang, Z Jiang, K Wu, J Shi, F Xie, W Wang, H Wu\u2026 - IEEE Transactions on \u2026, 2024", "abstract": "Multiple instance learning (MIL) based whole slide image (WSI) classification is often carried out on the representations of patches extracted from WSI with a pre-trained patch encoder. The performance of classification relies on both patch-level \u2026"}, {"title": "End-to-end Learning of Logical Rules for Enhancing Document-level Relation Extraction", "link": "https://aclanthology.org/2024.acl-long.391.pdf", "details": "K Qi, J Du, H Wan - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Document-level relation extraction (DocRE) aims to extract relations between entities in a whole document. One of the pivotal challenges of DocRE is to capture the intricate interdependencies between relations of entity pairs. Previous methods have \u2026"}, {"title": "Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable Cyclic Image-Report Generation", "link": "https://aclanthology.org/2024.acl-long.514.pdf", "details": "W Chen, L Shen, J Lin, J Luo, X Li, Y Yuan - Proceedings of the 62nd Annual Meeting \u2026, 2024", "abstract": "Fine-grained vision-language models (VLM) have been widely used for inter- modality local alignment between the predefined fixed patches and textual words. However, in medical analysis, lesions exhibit varying sizes and positions, and using \u2026"}, {"title": "SICAR at RRG2024: GPU Poor's Guide to Radiology Report Generation", "link": "https://aclanthology.org/2024.bionlp-1.55.pdf", "details": "K Udomlapsakul, P Pengpun, T Saengja\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "Radiology report generation (RRG) aims to create free-text radiology reports from clinical imaging. Our solution employs a lightweight multimodal language model (MLLM) enhanced with a two-stage post-processing strategy, utilizing a Large \u2026"}, {"title": "Contrastive Multitask Transformer for Hospital Mortality and Length-of-Stay Prediction", "link": "https://link.springer.com/chapter/10.1007/978-3-031-67278-1_11", "details": "F Pick, X Xie, LY Wu - International Conference on AI in Healthcare, 2024", "abstract": "Motivated by the performance on clinical prediction tasks of a transformer-based model (STraTS), we propose a multitask training scheme to exploit information in multiple labels with the goal of improving generalisation, alongside a novel \u2026"}]
