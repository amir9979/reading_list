[{"title": "Are Expert-Level Language Models Expert-Level Annotators?", "link": "https://arxiv.org/pdf/2410.03254", "details": "YM Tseng, WL Chen, CC Chen, HH Chen - arXiv preprint arXiv:2410.03254, 2024", "abstract": "Data annotation refers to the labeling or tagging of textual data with relevant information. A large body of works have reported positive results on leveraging LLMs as an alternative to human annotators. However, existing studies focus on classic \u2026"}, {"title": "Gauging, enriching and applying geography knowledge in Pre-trained Language Models", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324002516", "details": "N Ramrakhiyani, V Varma, GK Palshikar, S Pawar - Information Processing & \u2026, 2025", "abstract": "Abstract To employ Pre-trained Language Models (PLMs) as knowledge containers in niche domains it is important to gauge the knowledge of these PLMs about facts in these domains. It is also an important pre-requisite to know how much enrichment \u2026"}, {"title": "Do Vision and Language Models Share Concepts? A Vector Space Alignment Study", "link": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00698/124631", "details": "J Li, Y Kementchedjhieva, C Fierro, A S\u00f8gaard - Transactions of the Association for \u2026, 2024", "abstract": "Large-scale pretrained language models (LMs) are said to \u201clack the ability to connect utterances to the world\u201d(Bender and Koller,), because they do not have \u201cmental models of the world\u201d(Mitchell and Krakauer,). If so, one would expect LM \u2026"}, {"title": "The Disparate Benefits of Deep Ensembles", "link": "https://arxiv.org/pdf/2410.13831", "details": "K Schweighofer, A Arnaiz-Rodriguez, S Hochreiter\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Ensembles of Deep Neural Networks, Deep Ensembles, are widely used as a simple way to boost predictive performance. However, their impact on algorithmic fairness is not well understood yet. Algorithmic fairness investigates how a model's performance \u2026"}, {"title": "Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models", "link": "https://arxiv.org/pdf/2410.00131", "details": "J Liu, J Ren, R Jin, Z Zhang, Y Zhou, P Valduriez\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As a promising paradigm to collaboratively train models with decentralized data, Federated Learning (FL) can be exploited to fine-tune Large Language Models (LLMs). While LLMs correspond to huge size, the scale of the training data \u2026"}, {"title": "Disentangling Likes and Dislikes in Personalized Generative Explainable Recommendation", "link": "https://arxiv.org/pdf/2410.13248", "details": "R Shimizu, T Wada, Y Wang, J Kruse, S O'Brien\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research on explainable recommendation generally frames the task as a standard text generation problem, and evaluates models simply based on the textual similarity between the predicted and ground-truth explanations. However, this \u2026"}, {"title": "AI Fairness in Medical Imaging: Controlling for Disease Severity", "link": "https://link.springer.com/chapter/10.1007/978-3-031-72787-0_3", "details": "P Mukherjee, RM Summers - MICCAI Workshop on Fairness of AI in Medical Imaging, 2024", "abstract": "A new criterion for assessing fairness of AI models in medical imaging is proposed. The key idea is to control for disease severity, which as a mediator, affects the presentation of disease in medical images, and hence the performance of AI \u2026"}, {"title": "LoGra-Med: Long context multi-graph alignment for medical vision-language model", "link": "https://arxiv.org/pdf/2410.02615%3F", "details": "DMH Nguyen, NT Diep, TQ Nguyen, HB Le, T Nguyen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "State-of-the-art medical multi-modal large language models (med-MLLM), like LLaVA-Med or BioMedGPT, leverage instruction-following data in pre-training. However, those models primarily focus on scaling the model size and data volume to \u2026"}, {"title": "Advancing Medical Radiograph Representation Learning: A Hybrid Pre-training Paradigm with Multilevel Semantic Granularity", "link": "https://arxiv.org/pdf/2410.00448", "details": "H Jiang, X Hao, Y Huang, C Ma, J Zhang, Y Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces an innovative approach to Medical Vision-Language Pre- training (Med-VLP) area in the specialized context of radiograph representation learning. While conventional methods frequently merge textual annotations into \u2026"}]
