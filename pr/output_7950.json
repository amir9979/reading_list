[{"title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures", "link": "https://arxiv.org/pdf/2410.07698", "details": "Y Chen, Y Zhang, L Cao, K Yuan, Z Wen - arXiv preprint arXiv:2410.07698, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs when adapting large language models (LLMs) for downstream applications. However, traditional first-order (FO) fine-tuning algorithms incur substantial memory overhead \u2026"}, {"title": "CREAM: Consistency Regularized Self-Rewarding Language Models", "link": "https://arxiv.org/pdf/2410.12735", "details": "Z Wang, W He, Z Liang, X Zhang, C Bansal, Y Wei\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent self-rewarding large language models (LLM) have successfully applied LLM- as-a-Judge to iteratively improve the alignment performance without the need of human annotations for preference data. These methods commonly utilize the same \u2026"}, {"title": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement", "link": "https://arxiv.org/pdf/2410.02108", "details": "X Peng, C Xia, X Yang, C Xiong, CS Wu, C Xing - arXiv preprint arXiv:2410.02108, 2024", "abstract": "Post-training Large Language Models (LLMs) with explicit reasoning trajectories can enhance their reasoning abilities. However, acquiring such high-quality trajectory data typically demands meticulous supervision from humans or superior models \u2026"}, {"title": "POSIX: A Prompt Sensitivity Index For Large Language Models", "link": "https://arxiv.org/pdf/2410.02185", "details": "A Chatterjee, HK Renduchintala, S Bhatia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are found to be surprisingly sensitive to minor variations in prompts, often generating significantly divergent outputs in response to minor variations in the prompts, such as spelling \u2026"}, {"title": "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "link": "https://arxiv.org/pdf/2410.06554", "details": "Y Chen, D Zhu, Y Sun, X Chen, W Zhang, X Shen - arXiv preprint arXiv:2410.06554, 2024", "abstract": "Reinforcement Learning from Human Feedback significantly enhances Natural Language Processing by aligning language models with human expectations. A critical factor in this alignment is the strength of reward models used during training \u2026"}, {"title": "Understanding Reasoning in Chain-of-Thought from the Hopfieldian View", "link": "https://arxiv.org/pdf/2410.03595", "details": "L Hu, L Liu, S Yang, X Chen, Z Tan, MA Ali, M Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models have demonstrated remarkable abilities across various tasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to enhance reasoning capabilities. However, existing research primarily focuses on \u2026"}, {"title": "Quantifying Generalization Complexity for Large Language Models", "link": "https://arxiv.org/pdf/2410.01769%3F", "details": "Z Qi, H Luo, X Huang, Z Zhao, Y Jiang, X Fan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating \u2026"}, {"title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.10139", "details": "P Xia, S Han, S Qiu, Y Zhou, Z Wang, W Zheng, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this \u2026"}, {"title": "How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?", "link": "https://arxiv.org/pdf/2410.07571", "details": "S Lee, G Kim, J Kim, H Lee, H Chang, SH Park, M Seo - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-Language adaptation (VL adaptation) transforms Large Language Models (LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this process often compromises the inherent safety capabilities embedded in the original \u2026"}]
