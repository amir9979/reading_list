[{"title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines", "link": "https://arxiv.org/pdf/2410.21220", "details": "Z Zhang, Y Zhang, X Ding, X Yue - arXiv preprint arXiv:2410.21220, 2024", "abstract": "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This \u2026"}, {"title": "LanFL: Differentially Private Federated Learning with Large Language Models using Synthetic Samples", "link": "https://arxiv.org/pdf/2410.19114", "details": "H Wu, D Klabjan - arXiv preprint arXiv:2410.19114, 2024", "abstract": "Federated Learning (FL) is a collaborative, privacy-preserving machine learning framework that enables multiple participants to train a single global model. However, the recent advent of powerful Large Language Models (LLMs) with tens to hundreds \u2026"}, {"title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ", "link": "https://arxiv.org/pdf/2410.09613", "details": "A Poulis, E Tsalapati, M Koubarakis - arXiv preprint arXiv:2410.09613, 2024", "abstract": "Recent advancements in transformer-based language models have sparked research into their logical reasoning capabilities. Most of the benchmarks used to evaluate these models are simple: generated from short (fragments of) first-order \u2026"}, {"title": "Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings", "link": "https://arxiv.org/pdf/2410.22685", "details": "YS Grewal, EV Bonilla, TD Bui - arXiv preprint arXiv:2410.22685, 2024", "abstract": "Accurately quantifying uncertainty in large language models (LLMs) is crucial for their reliable deployment, especially in high-stakes applications. Current state-of-the- art methods for measuring semantic uncertainty in LLMs rely on strict bidirectional \u2026"}, {"title": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation", "link": "https://arxiv.org/pdf/2410.08895", "details": "K Ding, Q Yu, H Zhang, G Meng, S Xiang - arXiv preprint arXiv:2410.08895, 2024", "abstract": "Cache-based approaches stand out as both effective and efficient for adapting vision- language models (VLMs). Nonetheless, the existing cache model overlooks three crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text similarity \u2026"}, {"title": "Context-Enhanced Multi-View Trajectory Representation Learning: Bridging the Gap through Self-Supervised Models", "link": "https://arxiv.org/pdf/2410.13196", "details": "T Qian, J Li, Y Chen, G Cong, T Sun, F Wang, Y Xu - arXiv preprint arXiv:2410.13196, 2024", "abstract": "Modeling trajectory data with generic-purpose dense representations has become a prevalent paradigm for various downstream applications, such as trajectory classification, travel time estimation and similarity computation. However, existing \u2026"}, {"title": "DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2410.11988", "details": "S Gao, CH Lin, T Hua, T Zheng, Y Shen, H Jin, YC Hsu - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have achieved remarkable success in various natural language processing tasks, including language modeling, understanding, and generation. However, the increased memory and computational costs \u2026"}, {"title": "MMAD: The First-Ever Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection", "link": "https://arxiv.org/pdf/2410.09453", "details": "X Jiang, J Li, H Deng, Y Liu, BB Gao, Y Zhou, J Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the field of industrial inspection, Multimodal Large Language Models (MLLMs) have a high potential to renew the paradigms in practical applications due to their robust language capabilities and generalization abilities. However, despite their \u2026"}, {"title": "Reassessing Non-Autoregressive Neural Machine Translation with a Fine-Grained Error Taxonomy", "link": "https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA240946", "details": "Y Liu, L Wang, Z Tu, D Xiong - ECAI 2024, 2024", "abstract": "Non-autoregressive neural machine translation (NAT) has made remarkable progress since it is proposed. The performance of NAT in terms of BLEU has approached or even matched that of autoregressive neural machine translation (AT) \u2026"}]
