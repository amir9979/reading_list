[{"title": "Interactive dual-stream contrastive learning for radiology report generation", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001369", "details": "Z Zhang, A Jiang - Journal of Biomedical Informatics, 2024", "abstract": "Radiology report generation automates diagnostic narrative synthesis from medical imaging data. Current report generation methods primarily employ knowledge graphs for image enhancement, neglecting the interpretability and guiding function of \u2026"}, {"title": "LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs", "link": "https://arxiv.org/pdf/2408.13467", "details": "C Park, J Jiang, F Wang, S Paul, J Tang, S Kim - arXiv preprint arXiv:2408.13467, 2024", "abstract": "The widespread adoption of cloud-based proprietary large language models (LLMs) has introduced significant challenges, including operational dependencies, privacy concerns, and the necessity of continuous internet connectivity. In this work, we \u2026"}, {"title": "Effective prompt extraction from language models", "link": "https://openreview.net/pdf%3Fid%3D0o95CVdNuz", "details": "Y Zhang, N Carlini, D Ippolito - First Conference on Language Modeling, 2024", "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden \u2026"}, {"title": "Gemma 2: Improving open language models at a practical size", "link": "https://arxiv.org/pdf/2408.00118", "details": "G Team, M Riviere, S Pathak, PG Sessa, C Hardin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to \u2026"}, {"title": "Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models", "link": "https://aclanthology.org/2024.acl-long.815.pdf", "details": "T Madusanka, I Pratt-Hartmann, RT Batista-Navarro - \u2026 of the 62nd Annual Meeting of \u2026, 2024", "abstract": "Efforts to apply transformer-based language models (TLMs) to the problem of reasoning in natural language have enjoyed ever-increasing success in recent years. The most fundamental task in this area to which nearly all others can be \u2026"}, {"title": "Enhancing Clinical Relevance of Pretrained Language Models Through Integration of External Knowledge: Case Study on Cardiovascular Diagnosis From Electronic \u2026", "link": "https://ai.jmir.org/2024/1/e56932/", "details": "Q Lu, A Wen, T Nguyen, H Liu - JMIR AI, 2024", "abstract": "Background: Despite their growing use in health care, pretrained language models (PLMs) often lack clinical relevance due to insufficient domain expertise and poor interpretability. A key strategy to overcome these challenges is integrating external \u2026"}, {"title": "Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs", "link": "https://arxiv.org/pdf/2408.14397", "details": "X Zhang, JN Acosta, HY Zhou, P Rajpurkar - arXiv preprint arXiv:2408.14397, 2024", "abstract": "Recent advancements in artificial intelligence have significantly improved the automatic generation of radiology reports. However, existing evaluation methods fail to reveal the models' understanding of radiological images and their capacity to \u2026"}]
