[{"title": "On Speeding Up Language Model Evaluation", "link": "https://arxiv.org/pdf/2407.06172", "details": "JP Zhou, CK Belardi, R Wu, T Zhang, CP Gomes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) currently dominate the field of natural language processing (NLP), representing the state-of-the-art across a diverse array of tasks. Developing a model of this nature, from training to inference, requires making \u2026"}, {"title": "Strong Copyright Protection for Language Models via Adaptive Model Fusion", "link": "https://arxiv.org/pdf/2407.20105", "details": "J Abad, K Donhauser, F Pinto, F Yang - arXiv preprint arXiv:2407.20105, 2024", "abstract": "The risk of language models unintentionally reproducing copyrighted material from their training data has led to the development of various protective measures. In this paper, we propose model fusion as an effective solution to safeguard against \u2026"}, {"title": "How Chinese are Chinese Language Models? The Puzzling Lack of Language Policy in China's LLMs", "link": "https://arxiv.org/pdf/2407.09652", "details": "AW Wen-Yi, UES Jo, LJ Lin, D Mimno - arXiv preprint arXiv:2407.09652, 2024", "abstract": "Contemporary language models are increasingly multilingual, but Chinese LLM developers must navigate complex political and business considerations of language diversity. Language policy in China aims at influencing the public \u2026"}, {"title": "Generalization vs Memorization: Tracing Language Models' Capabilities Back to Pretraining Data", "link": "https://arxiv.org/pdf/2407.14985", "details": "A Antoniades, X Wang, Y Elazar, A Amayuelas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the proven utility of large language models (LLMs) in real-world applications, there remains a lack of understanding regarding how they leverage their large-scale pretraining text corpora to achieve such capabilities. In this work, we investigate the \u2026"}, {"title": "SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training", "link": "https://arxiv.org/pdf/2407.06654", "details": "N He, W Xiong, H Liu, Y Liao, L Ding, K Zhang, G Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The effectiveness of large language models (LLMs) is often hindered by duplicated data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, which risks the loss of valuable information and \u2026"}, {"title": "Pruning Large Language Models to Intra-module Low-rank Architecture with Transitional Activations", "link": "https://arxiv.org/pdf/2407.05690", "details": "B Shen, Z Lin, D Zha, W Liu, J Luan, B Wang, W Wang - arXiv preprint arXiv \u2026, 2024", "abstract": "Structured pruning fundamentally reduces computational and memory overheads of large language models (LLMs) and offers a feasible solution for end-side LLM deployment. Structurally pruned models remain dense and high-precision, highly \u2026"}, {"title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "link": "https://arxiv.org/pdf/2407.07071", "details": "YS Chuang, L Qiu, CY Hsieh, R Krishna, Y Kim, J Glass - arXiv preprint arXiv \u2026, 2024", "abstract": "When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a \u2026"}, {"title": "LLMBox: A Comprehensive Library for Large Language Models", "link": "https://arxiv.org/pdf/2407.05563", "details": "T Tang, Y Hu, B Li, W Luo, Z Qin, H Sun, J Wang, S Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To facilitate the research on large language models (LLMs), this paper presents a comprehensive and unified library, LLMBox, to ease the development, use, and evaluation of LLMs. This library is featured with three main merits:(1) a unified data \u2026"}, {"title": "Demystifying Verbatim Memorization in Large Language Models", "link": "https://arxiv.org/pdf/2407.17817", "details": "J Huang, D Yang, C Potts - arXiv preprint arXiv:2407.17817, 2024", "abstract": "Large Language Models (LLMs) frequently memorize long sequences verbatim, often with serious legal and privacy implications. Much prior work has studied such verbatim memorization using observational data. To complement such work, we \u2026"}]
