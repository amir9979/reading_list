[{"title": "SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through Combat", "link": "https://arxiv.org/pdf/2506.04721", "details": "Y Jiang, W Ding, S Feng, G Durrett, Y Tsvetkov - arXiv preprint arXiv:2506.04721, 2025", "abstract": "We propose SPARTA ALIGNMENT, an algorithm to collectively align multiple LLMs through competition and combat. To complement a single model's lack of diversity in generation and biases in evaluation, multiple LLMs form a\" sparta tribe\" to compete \u2026", "entry_id": "http://arxiv.org/abs/2506.04721v1", "updated": "2025-06-05 07:51:23", "published": "2025-06-05 07:51:23", "authors": "Yuru Jiang;Wenxuan Ding;Shangbin Feng;Greg Durrett;Yulia Tsvetkov", "summary": "We propose SPARTA ALIGNMENT, an algorithm to collectively align multiple LLMs\nthrough competition and combat. To complement a single model's lack of\ndiversity in generation and biases in evaluation, multiple LLMs form a \"sparta\ntribe\" to compete against each other in fulfilling instructions while serving\nas judges for the competition of others. For each iteration, one instruction\nand two models are selected for a duel, the other models evaluate the two\nresponses, and their evaluation scores are aggregated through a adapted\nelo-ranking based reputation system, where winners/losers of combat gain/lose\nweight in evaluating others. The peer-evaluated combat results then become\npreference pairs where the winning response is preferred over the losing one,\nand all models learn from these preferences at the end of each iteration.\nSPARTA ALIGNMENT enables the self-evolution of multiple LLMs in an iterative\nand collective competition process. Extensive experiments demonstrate that\nSPARTA ALIGNMENT outperforms initial models and 4 self-alignment baselines\nacross 10 out of 12 tasks and datasets with 7.0% average improvement. Further\nanalysis reveals that SPARTA ALIGNMENT generalizes more effectively to unseen\ntasks and leverages the expertise diversity of participating models to produce\nmore logical, direct and informative outputs.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.04721v1;http://arxiv.org/pdf/2506.04721v1", "pdf_url": "http://arxiv.org/pdf/2506.04721v1"}, {"title": "Information Locality as an Inductive Bias for Neural Language Models", "link": "https://arxiv.org/pdf/2506.05136", "details": "T Someya, A Svete, B DuSell, TJ O'Donnell\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Inductive biases are inherent in every machine learning system, shaping how models generalize from finite data. In the case of neural language models (LMs), debates persist as to whether these biases align with or diverge from human \u2026", "entry_id": "http://arxiv.org/abs/2506.05136v1", "updated": "2025-06-05 15:21:05", "published": "2025-06-05 15:21:05", "authors": "Taiga Someya;Anej Svete;Brian DuSell;Timothy J. O'Donnell;Mario Giulianelli;Ryan Cotterell", "summary": "Inductive biases are inherent in every machine learning system, shaping how\nmodels generalize from finite data. In the case of neural language models\n(LMs), debates persist as to whether these biases align with or diverge from\nhuman processing constraints. To address this issue, we propose a quantitative\nframework that allows for controlled investigations into the nature of these\nbiases. Within our framework, we introduce $m$-local entropy$\\unicode{x2013}$an\ninformation-theoretic measure derived from average lossy-context\nsurprisal$\\unicode{x2013}$that captures the local uncertainty of a language by\nquantifying how effectively the $m-1$ preceding symbols disambiguate the next\nsymbol. In experiments on both perturbed natural language corpora and languages\ndefined by probabilistic finite-state automata (PFSAs), we show that languages\nwith higher $m$-local entropy are more difficult for Transformer and LSTM LMs\nto learn. These results suggest that neural LMs, much like humans, are highly\nsensitive to the local statistical structure of a language.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.05136v1;http://arxiv.org/pdf/2506.05136v1", "pdf_url": "http://arxiv.org/pdf/2506.05136v1"}, {"title": "On Entity Identification in Language Models", "link": "https://arxiv.org/pdf/2506.02701", "details": "M Sakata, S Yokoi, B Heinzerling, T Ito, K Inui - arXiv preprint arXiv:2506.02701, 2025", "abstract": "We analyze the extent to which internal representations of language models (LMs) identify and distinguish mentions of named entities, focusing on the many-to-many correspondence between entities and their mentions. We first formulate two \u2026", "entry_id": "http://arxiv.org/abs/2506.02701v3", "updated": "2025-06-05 01:17:55", "published": "2025-06-03 09:55:21", "authors": "Masaki Sakata;Benjamin Heinzerling;Sho Yokoi;Takumi Ito;Kentaro Inui", "summary": "We analyze the extent to which internal representations of language models\n(LMs) identify and distinguish mentions of named entities, focusing on the\nmany-to-many correspondence between entities and their mentions. We first\nformulate two problems of entity mentions -- ambiguity and variability -- and\npropose a framework analogous to clustering quality metrics. Specifically, we\nquantify through cluster analysis of LM internal representations the extent to\nwhich mentions of the same entity cluster together and mentions of different\nentities remain separated. Our experiments examine five Transformer-based\nautoregressive models, showing that they effectively identify and distinguish\nentities with metrics analogous to precision and recall ranging from 0.66 to\n0.9. Further analysis reveals that entity-related information is compactly\nrepresented in a low-dimensional linear subspace at early LM layers.\nAdditionally, we clarify how the characteristics of entity representations\ninfluence word prediction performance. These findings are interpreted through\nthe lens of isomorphism between LM representations and entity-centric knowledge\nstructures in the real world, providing insights into how LMs internally\norganize and use entity information.", "comment": "ACL 2025 Findings; 26 pages, 13 figures, 9 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.02701v3;http://arxiv.org/pdf/2506.02701v3", "pdf_url": "http://arxiv.org/pdf/2506.02701v3"}]
