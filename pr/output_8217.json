[{"title": "No Need to Talk: Asynchronous Mixture of Language Models", "link": "https://arxiv.org/pdf/2410.03529%3F", "details": "A Filippova, A Katharopoulos, D Grangier, R Collobert - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce SmallTalk LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each model of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth \u2026"}, {"title": "When Attention Sink Emerges in Language Models: An Empirical View", "link": "https://arxiv.org/pdf/2410.10781", "details": "X Gu, T Pang, C Du, Q Liu, F Zhang, C Du, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as attention sink. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV \u2026"}, {"title": "Scaling Parameter-Constrained Language Models with Quality Data", "link": "https://arxiv.org/pdf/2410.03083", "details": "E Chang, M Paltenghi, Y Li, PJ Lin, C Zhao, P Huber\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws in language modeling traditionally quantify training loss as a function of dataset size and model parameters, providing compute-optimal estimates but often neglecting the impact of data quality on model generalization. In this paper, we \u2026"}, {"title": "MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models", "link": "https://arxiv.org/pdf/2410.09733", "details": "H Hua, Y Tang, Z Zeng, L Cao, Z Yang, H He, C Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video \u2026"}, {"title": "Ascle\u2014A Python Natural Language Processing Toolkit for Medical Text Generation: Development and Evaluation Study", "link": "https://www.jmir.org/2024/1/e60601/", "details": "R Yang, Q Zeng, K You, Y Qiao, L Huang, CC Hsieh\u2026 - Journal of Medical Internet \u2026, 2024", "abstract": "Background Medical texts present significant domain-specific challenges, and manually curating these texts is a time-consuming and labor-intensive process. To address this, natural language processing (NLP) algorithms have been developed to \u2026"}, {"title": "Reflections on interactive visualization of electronic health records: past, present, future", "link": "https://academic.oup.com/jamia/article-abstract/31/11/2423/7824391", "details": "A Arleo, AT Chen, D Gotz, S Kandaswamy, J Bernard - Journal of the American \u2026, 2024", "abstract": "In the early 2000s, the transition to paperless documentation of patients' health data begun at large scale, with the introduction of Electronic Health and Medical Records (EHR and EMR, respectively). This constituted a paradigm shift in how patient data \u2026"}, {"title": "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "link": "https://arxiv.org/pdf/2410.03122", "details": "Z Zhao, Y Yang, Y Li, Y Cao - arXiv preprint arXiv:2410.03122, 2024", "abstract": "The ripple effect poses a significant challenge in knowledge editing for large language models. Namely, when a single fact is edited, the model struggles to accurately update the related facts in a sequence, which is evaluated by multi-hop \u2026"}, {"title": "IPO: Interpretable Prompt Optimization for Vision-Language Models", "link": "https://arxiv.org/pdf/2410.15397", "details": "Y Du, W Sun, CGM Snoek - arXiv preprint arXiv:2410.15397, 2024", "abstract": "Pre-trained vision-language models like CLIP have remarkably adapted to various downstream tasks. Nonetheless, their performance heavily depends on the specificity of the input text prompts, which requires skillful prompt template \u2026"}, {"title": "Prompt tuning discriminative language models for hierarchical text classification", "link": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/50E5499348A0E72F0C4F3AFC622133A7/S2977042424000517a.pdf/div-class-title-prompt-tuning-discriminative-language-models-for-hierarchical-text-classification-div.pdf", "details": "J du Toit, M Dunaiski - Natural Language Processing", "abstract": "Hierarchical text classification (HTC) is a natural language processing task which aims to categorise a text document into a set of classes from a hierarchical class structure. Recent approaches to solve HTC tasks focus on leveraging pre-trained \u2026"}]
