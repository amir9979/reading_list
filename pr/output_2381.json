[{"title": "Language Models Do Hard Arithmetic Tasks Easily and Hardly Do Easy Arithmetic Tasks", "link": "https://arxiv.org/pdf/2406.02356", "details": "A Gambardella, Y Iwasawa, Y Matsuo - arXiv preprint arXiv:2406.02356, 2024", "abstract": "The ability (and inability) of large language models (LLMs) to perform arithmetic tasks has been the subject of much theoretical and practical debate. We show that LLMs are frequently able to correctly and confidently predict the first digit of n-digit by \u2026"}, {"title": "Calibrating Reasoning in Language Models with Internal Consistency", "link": "https://arxiv.org/pdf/2405.18711", "details": "Z Xie, J Guo, T Yu, S Li - arXiv preprint arXiv:2405.18711, 2024", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought (CoT) prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious \u2026"}, {"title": "Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature", "link": "https://arxiv.org/pdf/2406.01946", "details": "T Zhou, X Zhao, X Xu, S Ren - arXiv preprint arXiv:2406.01946, 2024", "abstract": "Text watermarks for large language models (LLMs) have been commonly used to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content. While existing watermarking \u2026"}, {"title": "TAIA: Large Language Models are Out-of-Distribution Data Learners", "link": "https://arxiv.org/pdf/2405.20192", "details": "S Jiang, Y Liao, Y Zhang, Y Wang, Y Wang - arXiv preprint arXiv:2405.20192, 2024", "abstract": "Fine-tuning on task-specific question-answer pairs is a predominant method for enhancing the performance of instruction-tuned large language models (LLMs) on downstream tasks. However, in certain specialized domains, such as healthcare or \u2026"}, {"title": "Hyperbolic Pre-Trained Language Model", "link": "https://ieeexplore.ieee.org/abstract/document/10542420/", "details": "W Chen, X Han, Y Lin, K He, R Xie, J Zhou, Z Liu\u2026 - IEEE/ACM Transactions on \u2026, 2024", "abstract": "In recent years, we have witnessed significant improvements in pre-trained language models (PLM) brought about by the scaling of parameter sizes and data amounts. However, this also brings high computational and storage costs. In this paper, we \u2026"}, {"title": "Zyda: A 1.3 T Dataset for Open Language Modeling", "link": "https://arxiv.org/pdf/2406.01981", "details": "Y Tokpanov, B Millidge, P Glorioso, J Pilault, A Ibrahim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State- of-the-art language models, even at relatively smaller sizes, typically require training \u2026"}, {"title": "Kestrel: Point Grounding Multimodal LLM for Part-Aware 3D Vision-Language Understanding", "link": "https://arxiv.org/pdf/2405.18937", "details": "J Fei, M Ahmed, J Ding, EM Bakr, M Elhoseiny - arXiv preprint arXiv:2405.18937, 2024", "abstract": "While 3D MLLMs have achieved significant progress, they are restricted to object and scene understanding and struggle to understand 3D spatial structures at the part level. In this paper, we introduce Kestrel, representing a novel approach that \u2026"}, {"title": "LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback", "link": "https://arxiv.org/pdf/2406.01771", "details": "W Lai, M Mesgar, A Fraser - arXiv preprint arXiv:2406.01771, 2024", "abstract": "To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones. While recent multilingual LLMs \u2026"}, {"title": "A Survey on Large Language Models for Code Generation", "link": "https://arxiv.org/pdf/2406.00515", "details": "J Jiang, F Wang, J Shen, S Kim, S Kim - arXiv preprint arXiv:2406.00515, 2024", "abstract": "Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This \u2026"}]
