[{"title": "MolFusion: Multimodal Fusion Learning for Molecular Representations via Multi-granularity Views", "link": "https://arxiv.org/pdf/2406.18020", "details": "M Cai, S Zhao, H Wang, Y Du, Z Qiang, B Qin, T Liu - arXiv preprint arXiv:2406.18020, 2024", "abstract": "Artificial Intelligence predicts drug properties by encoding drug molecules, aiding in the rapid screening of candidates. Different molecular representations, such as SMILES and molecule graphs, contain complementary information for molecular \u2026"}, {"title": "Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination", "link": "https://ieeecai.org/2024/wp-content/pdfs/540900a486/540900a486.pdf", "details": "X Zhong, K Batmanghelich, L Sun", "abstract": "Vision-language models pre-trained on large scale of unlabeled biomedical images and associated reports learn generalizable semantic representations. These multi- modal representations can benefit various downstream tasks in the biomedical \u2026"}, {"title": "Unveiling Encoder-Free Vision-Language Models", "link": "https://arxiv.org/pdf/2406.11832", "details": "H Diao, Y Cui, X Li, Y Wang, H Lu, X Wang - arXiv preprint arXiv:2406.11832, 2024", "abstract": "Existing vision-language models (VLMs) mostly rely on vision encoders to extract visual features followed by large language models (LLMs) for visual-language tasks. However, the vision encoders set a strong inductive bias in abstracting visual \u2026"}]
