[{"title": "Cost-of-Pass: An Economic Framework for Evaluating Language Models", "link": "https://arxiv.org/pdf/2504.13359", "details": "MH Erol, B El, M Suzgun, M Yuksekgonul, J Zou - arXiv preprint arXiv:2504.13359, 2025", "abstract": "The widespread adoption of AI systems in the economy hinges on their ability to generate economic value that outweighs their inference costs. Evaluating this tradeoff requires metrics that account for both performance and costs. We propose a \u2026"}, {"title": "Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results", "link": "https://arxiv.org/pdf/2504.13677", "details": "A Santilli, A Golinski, M Kirchhof, F Danieli, A Blaas\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for improving their safety and reliability. Evaluations often use performance metrics like AUROC to assess how well UQ methods (eg, negative sequence probabilities) correlate with \u2026"}, {"title": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning", "link": "https://arxiv.org/pdf/2504.13500", "details": "J Wang, J Jiang, Y Liu, M Zhang, X Cai - arXiv preprint arXiv:2504.13500, 2025", "abstract": "In this paper, we introduce a new\\emph {process prejudge} strategy in LLM reasoning to demonstrate that bootstrapping with process prejudge allows the LLM to adaptively anticipate the errors encountered when advancing the subsequent \u2026"}, {"title": "A Dual-Space Framework for General Knowledge Distillation of Large Language Models", "link": "https://arxiv.org/pdf/2504.11426", "details": "X Zhang, S Zhang, Y Liang, F Meng, Y Chen, J Xu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Knowledge distillation (KD) is a promising solution to compress large language models (LLMs) by transferring their knowledge to smaller models. During this process, white-box KD methods usually minimize the distance between the output \u2026"}, {"title": "FLUE: Streamlined Uncertainty Estimation for Large Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/33840/35995", "details": "S Gao, T Gong, Z Lin, R Xu, H Zhou, J Li - Proceedings of the AAAI Conference on \u2026, 2025", "abstract": "Uncertainty estimation is essential for practical applications such as decision- making, risk assessment, and human-AI collaboration. However, Uncertainty estimation in open-ended question-answering (QA) tasks presents unique \u2026"}, {"title": "Exploring Conversational Adaptability: Assessing the Proficiency of Large Language Models in Dynamic Alignment with Updated User Intent", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/34534/36689", "details": "YC Chen, HH Huang - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "This paper presents a practical problem in dialogue systems: the capability to adapt to changing user intentions and resolve inconsistencies in conversation histories. It is crucial in scenarios like train ticket booking, where travel plans often change \u2026"}, {"title": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning", "link": "https://arxiv.org/pdf/2504.13032", "details": "Z Wang, SX Teo, JJ Chew, W Shi - arXiv preprint arXiv:2504.13032, 2025", "abstract": "Recent advancements in large language models (LLMs) have enabled their use as agents for planning complex tasks. Existing methods typically rely on a thought- action-observation (TAO) process to enhance LLM performance, but these \u2026"}, {"title": "Teaching Large Language Models to Reason through Learning and Forgetting", "link": "https://arxiv.org/pdf/2504.11364", "details": "T Ni, A Nie, S Chaudhary, Y Liu, H Rangwala, R Fakoor - arXiv preprint arXiv \u2026, 2025", "abstract": "Leveraging inference-time search in large language models has proven effective in further enhancing a trained model's capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational \u2026"}, {"title": "Explore What LLM Does Not Know in Complex Question Answering", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/34638/36793", "details": "X Lin, Z Huang, Z Zhang, J Zhou, E Chen - Proceedings of the AAAI Conference on \u2026, 2025", "abstract": "Complex question answering (QA) is a challenging task in artificial intelligence research which requires reasoning based on related knowledge. The retrieval- augmented generation (RAG) based on large language models (LLMs) have \u2026"}]
