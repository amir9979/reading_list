[{"title": "Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "link": "https://arxiv.org/pdf/2405.10431", "details": "S Furniturewala, S Jandial, A Java, P Banerjee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing debiasing techniques are typically training-based or require access to the model's internals and output distributions, so they are inaccessible to end-users looking to adapt LLM outputs for their particular needs. In this study, we examine \u2026"}, {"title": "Large Language Models Can Self-Correct with Minimal Effort", "link": "https://arxiv.org/pdf/2405.14092", "details": "Z Wu, Q Zeng, Z Zhang, Z Tan, C Shen, M Jiang - arXiv preprint arXiv:2405.14092, 2024", "abstract": "Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple \u2026"}, {"title": "Scaling Laws for Discriminative Classification in Large Language Models", "link": "https://arxiv.org/pdf/2405.15765", "details": "D Wyatte, F Tahmasbi, M Li, T Markovich - arXiv preprint arXiv:2405.15765, 2024", "abstract": "Modern large language models (LLMs) represent a paradigm shift in what can plausibly be expected of machine learning models. The fact that LLMs can effectively generate sensible answers to a diverse range of queries suggests that they would be \u2026"}, {"title": "A Systematic Analysis on the Temporal Generalization of Language Models in Social Media", "link": "https://arxiv.org/pdf/2405.13017", "details": "A Ushio, J Camacho-Collados - arXiv preprint arXiv:2405.13017, 2024", "abstract": "In machine learning, temporal shifts occur when there are differences between training and test splits in terms of time. For streaming data such as news or social media, models are commonly trained on a fixed corpus from a certain period of time \u2026"}, {"title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation", "link": "https://arxiv.org/pdf/2405.13769", "details": "C Chhun, FM Suchanek, C Clavel - arXiv preprint arXiv:2405.13769, 2024", "abstract": "Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high \u2026"}, {"title": "Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs", "link": "https://arxiv.org/pdf/2405.15485", "details": "S Guo, A Didolkar, NR Ke, A Goyal, F Husz\u00e1r\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We are beginning to see progress in language model assisted scientific discovery. Motivated by the use of LLMs as a general scientific assistant, this paper assesses the domain knowledge of LLMs through its understanding of different mathematical \u2026"}, {"title": "Effective In-Context Example Selection through Data Compression", "link": "https://arxiv.org/pdf/2405.11465", "details": "Z Sun, K Zhang, H Wang, X Zhang, J Xu - arXiv preprint arXiv:2405.11465, 2024", "abstract": "In-context learning has been extensively validated in large language models. However, the mechanism and selection strategy for in-context example selection, which is a crucial ingredient in this approach, lacks systematic and in-depth \u2026"}, {"title": "GECKO: Generative Language Model for English, Code and Korean", "link": "https://arxiv.org/pdf/2405.15640", "details": "S Oh, D Kim - arXiv preprint arXiv:2405.15640, 2024", "abstract": "We introduce GECKO, a bilingual large language model (LLM) optimized for Korean and English, along with programming languages. GECKO is pretrained on the balanced, high-quality corpus of Korean and English employing LLaMA architecture \u2026"}, {"title": "DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction", "link": "https://arxiv.org/pdf/2405.12100", "details": "H Chen, B Zeng, X Lin, L He, A Zhou - arXiv preprint arXiv:2405.12100, 2024", "abstract": "Math world problems correction (MWPC) is a novel task dedicated to rectifying reasoning errors in the process of solving mathematical problems. In this paper, leveraging the advancements in large language models (LLMs), we address two key \u2026"}]
