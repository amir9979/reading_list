[{"title": "Language Models as Measurement Tools: Using Instruction-Based Models to Increase Validity, Robustness and Data Efficiency", "link": "https://research.vu.nl/files/355675396/dissertationlaurerfinal%2520-%252066c885c7e9d0b.pdf", "details": "M Laurer - 2024", "abstract": "From millions of social media posts, to decades of legal text-more and more relevant information is hidden in digital text corpora that are too large for manual analyses. The key promise of machine learning is to automate parts of the manual analysis \u2026"}, {"title": "Cross-lingual Natural Language Processing on Limited Annotated Case/Radiology Reports in English and Japanese: Insights from the Real-MedNLP Workshop", "link": "https://www.thieme-connect.com/products/ejournals/pdf/10.1055/a-2405-2489.pdf", "details": "S Yada, Y Nakamura, S Wakamiya, E Aramaki - Methods of Information in Medicine, 2024", "abstract": "Background: Textual datasets (corpora) are crucial for the application of natural language processing (NLP) models. However, corpus creation in the medical field is challenging, primarily because of privacy issues with raw clinical data such as health \u2026"}, {"title": "Leveraging large language models through natural language processing to provide interpretable machine learning predictions of mental deterioration in real time", "link": "https://link.springer.com/article/10.1007/s13369-024-09508-2", "details": "F de Arriba-P\u00e9rez, S Garc\u00eda-M\u00e9ndez - Arabian Journal for Science and Engineering, 2024", "abstract": "Based on official estimates, 50 million people worldwide are affected by dementia, and this number increases by 10 million new patients every year. Without a cure, clinical prognostication and early intervention represent the most effective ways to \u2026"}, {"title": "With Good MT There is No Need For End-to-End: A Case for Translate-then-Summarize Cross-lingual Summarization", "link": "https://arxiv.org/pdf/2409.00414", "details": "D Varab, C Hardmeier - arXiv preprint arXiv:2409.00414, 2024", "abstract": "Recent work has suggested that end-to-end system designs for cross-lingual summarization are competitive solutions that perform on par or even better than traditional pipelined designs. A closer look at the evidence reveals that this intuition \u2026"}, {"title": "Large Language Models Know What Makes Exemplary Contexts", "link": "https://arxiv.org/pdf/2408.07505", "details": "Q Long, J Chen - arXiv preprint arXiv:2408.07505, 2024", "abstract": "In-context learning (ICL) has proven to be a significant capability with the advancement of Large Language models (LLMs). By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without \u2026"}, {"title": "Causal-Guided Active Learning for Debiasing Large Language Models", "link": "https://arxiv.org/pdf/2408.12942", "details": "Z Sun, L Du, X Ding, Y Ma, K Qiu, T Liu, B Qin - arXiv preprint arXiv:2408.12942, 2024", "abstract": "Although achieving promising performance, recent analyses show that current generative large language models (LLMs) may still capture dataset biases and utilize them for generation, leading to poor generalizability and harmfulness of LLMs \u2026"}, {"title": "Importance Weighting Can Help Large Language Models Self-Improve", "link": "https://arxiv.org/pdf/2408.09849", "details": "C Jiang, C Chan, W Xue, Q Liu, Y Guo - arXiv preprint arXiv:2408.09849, 2024", "abstract": "Large language models (LLMs) have shown remarkable capability in numerous tasks and applications. However, fine-tuning LLMs using high-quality datasets under external supervision remains prohibitively expensive. In response, LLM self \u2026"}, {"title": "Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression", "link": "https://arxiv.org/pdf/2408.15491", "details": "H Hou, F Ma, B Bai, X Zhu, F Yu - arXiv preprint arXiv:2408.15491, 2024", "abstract": "Large Language Models (LLMs) have garnered widespread attention due to their remarkable performance across various tasks. However, to mitigate the issue of hallucinations, LLMs often incorporate retrieval-augmented pipeline to provide them \u2026"}, {"title": "Reasoning and Planning with Large Language Models in Code Development", "link": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671452", "details": "H Ding, Z Fan, I Guehring, G Gupta, W Ha, J Huan\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code \u2026"}]
