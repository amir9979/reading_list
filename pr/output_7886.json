[{"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "link": "https://arxiv.org/pdf/2410.05639", "details": "R Zhao, ZL Thai, Y Zhang, S Hu, Y Ba, J Zhou, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the \u2026"}, {"title": "Attention Prompting on Image for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.17143%3F", "details": "R Yu, W Yu, X Wang - arXiv preprint arXiv:2409.17143, 2024", "abstract": "Compared with Large Language Models (LLMs), Large Vision-Language Models (LVLMs) can also accept images as input, thus showcasing more interesting emergent capabilities and demonstrating impressive performance on various vision \u2026"}, {"title": "Do Vision-Language Models Really Understand Visual Language?", "link": "https://arxiv.org/pdf/2410.00193", "details": "B Giledereli, Y Hou, Y Tu, M Sachan - arXiv preprint arXiv:2410.00193, 2024", "abstract": "Visual language is a system of communication that conveys information through symbols, shapes, and spatial arrangements. Diagrams are a typical example of a visual language depicting complex concepts and their relationships in the form of an \u2026"}, {"title": "A Unified Hallucination Mitigation Framework for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.16494", "details": "Y Chang, L Jing, X Zhang, Y Zhang - arXiv preprint arXiv:2409.16494, 2024", "abstract": "Hallucination is a common problem for Large Vision-Language Models (LVLMs) with long generations which is difficult to eradicate. The generation with hallucinations is partially inconsistent with the image content. To mitigate hallucination, current \u2026"}, {"title": "ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context Information in Multi-Turn Multimodal Medical Dialogue", "link": "https://arxiv.org/pdf/2409.17610", "details": "Z Li, C Zou, S Ma, Z Yang, C Du, Y Tang, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rocketing prosperity of large language models (LLMs) in recent years has boosted the prevalence of vision-language models (VLMs) in the medical sector. In our online medical consultation scenario, a doctor responds to the texts and images \u2026"}, {"title": "Zero-Shot Multi-Hop Question Answering via Monte-Carlo Tree Search with Large Language Models", "link": "https://arxiv.org/pdf/2409.19382", "details": "S Lee, J Shin, Y Ahn, S Seo, O Kwon, KE Kim - arXiv preprint arXiv:2409.19382, 2024", "abstract": "Recent advances in large language models (LLMs) have significantly impacted the domain of multi-hop question answering (MHQA), where systems are required to aggregate information and infer answers from disparate pieces of text. However, the \u2026"}, {"title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "link": "https://arxiv.org/pdf/2410.11829", "details": "Y Cao, Y Liu, Z Chen, G Shi, W Wang, D Zhao, T Lu - arXiv preprint arXiv:2410.11829, 2024", "abstract": "Despite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating \u2026"}, {"title": "Language-based reasoning graph neural network for commonsense question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007408", "details": "M Yang, Y Wang, Y Gu - Neural Networks, 2024", "abstract": "Abstract Language model (LM) has played an increasingly important role in the common-sense understanding and reasoning in the CSQA task (Common Sense Question Answering). However, due to the amount of model parameters, increasing \u2026"}, {"title": "Context-Enhanced Multi-View Trajectory Representation Learning: Bridging the Gap through Self-Supervised Models", "link": "https://arxiv.org/pdf/2410.13196", "details": "T Qian, J Li, Y Chen, G Cong, T Sun, F Wang, Y Xu - arXiv preprint arXiv:2410.13196, 2024", "abstract": "Modeling trajectory data with generic-purpose dense representations has become a prevalent paradigm for various downstream applications, such as trajectory classification, travel time estimation and similarity computation. However, existing \u2026"}]
