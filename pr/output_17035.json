[{"title": "PathCare: Integrating Clinical Pathway Information to Enable Healthcare Prediction at the Neuron Level", "link": "https://www.mdpi.com/2306-5354/12/6/578", "details": "D Sui, L Gu, C Zhang, K Yang, X Li, L Ma, L Wang\u2026 - Bioengineering, 2025", "abstract": "Electronic Health Records (EHRs) offer valuable insights for healthcare prediction. Existing methods approach EHR analysis through direct imputation techniques in data space or representation learning in feature space. However, these approaches \u2026"}, {"title": "Leveraging long context in retrieval augmented language models for medical question answering", "link": "https://www.nature.com/articles/s41746-025-01651-w", "details": "G Zhang, Z Xu, Q Jin, F Chen, Y Fang, Y Liu\u2026 - npj Digital Medicine, 2025", "abstract": "While holding great promise for improving and facilitating healthcare through applications of medical literature summarization, large language models (LLMs) struggle to produce up-to-date responses on evolving topics due to outdated \u2026"}, {"title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models", "link": "https://arxiv.org/pdf/2505.22232", "details": "M Ali, M Brack, M L\u00fcbbering, E Wendt, AG Khan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic \u2026", "entry_id": "http://arxiv.org/abs/2505.22232v1", "updated": "2025-05-28 11:06:54", "published": "2025-05-28 11:06:54", "authors": "Mehdi Ali;Manuel Brack;Max L\u00fcbbering;Elias Wendt;Abbas Goher Khan;Richard Rutmann;Alex Jude;Maurice Kraus;Alexander Arno Weber;Felix Stollenwerk;David Kacz\u00e9r;Florian Mai;Lucie Flek;Rafet Sifa;Nicolas Flores-Herr;Joachim K\u00f6hler;Patrick Schramowski;Michael Fromm;Kristian Kersting", "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development.", "comment": "Project page available at https://huggingface.co/spaces/Jackal-AI/JQL", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.22232v1;http://arxiv.org/pdf/2505.22232v1", "pdf_url": "http://arxiv.org/pdf/2505.22232v1"}, {"title": "Pre-training on high-resolution X-ray images: an experimental study", "link": "https://link.springer.com/article/10.1007/s44267-025-00080-3", "details": "X Wang, Y Li, W Wu, J Jin, Y Rong, B Jiang, C Li\u2026 - Visual Intelligence, 2025", "abstract": "Existing X-ray image based pre-trained vision models are typically trained on a relatively small-scale dataset (less than 500,000 samples) with limited resolution (eg,\\\\(224\\times 224\\\\)). However, the key to the success of self-supervised pre \u2026"}, {"title": "Comparing Text-Based Clinical Risk Prediction in Critical Care: A Note-Specific Hierarchical Network and Large Language Models", "link": "https://ieeexplore.ieee.org/iel8/6221020/6363502/11016178.pdf", "details": "J Liu, A Nguyen, D Capurro, K Verspoor - IEEE Journal of Biomedical and Health \u2026, 2025", "abstract": "Clinical predictive analysis is a crucial task with numerous applications and has been extensively studied using machine learning approaches. Clinical notes, a vital data source, have been employed to develop natural language processing (NLP) \u2026"}, {"title": "multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data", "link": "https://arxiv.org/pdf/2505.21680", "details": "AJ Loza, JY Kim, S Song, Y Liu, JJY Sung, RA Taylor\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Real-world processes often generate data that are a mix of categorical and numeric values that are recorded at irregular and informative intervals. Discrete token-based approaches are limited in numeric representation capacity while methods like neural \u2026", "entry_id": "http://arxiv.org/abs/2505.21680v1", "updated": "2025-05-27 18:58:37", "published": "2025-05-27 18:58:37", "authors": "Andrew J. Loza;Jun Yup Kim;Shangzheng Song;Yihang Liu;Joseph J. Y. Sung;R Andrew Taylor;Dennis L. Shung", "summary": "Real-world processes often generate data that are a mix of categorical and\nnumeric values that are recorded at irregular and informative intervals.\nDiscrete token-based approaches are limited in numeric representation capacity\nwhile methods like neural ordinary differential equations are not well suited\nfor categorical data or informative sampling and require augmentation to handle\ncertain classes of trajectories. Here, we present multivariateGPT, a single\narchitecture for modeling sequences of mixed categorical (including tokenized\ntext) and numeric data. This is accomplished with an autoregressive sequence\ndecomposition, embedding scheme, and loss function that extend the next token\nprediction task to likelihood estimation of the joint distribution of next\ntoken class and value. We demonstrate how this approach can efficiently learn\nto generalize patterns in simple physical systems and model complex time series\nincluding electrocardiograms and multivariate electronic health record data.\nThis work extends the utility of transformer based models to additional classes\nof data.", "comment": "15 pates, 5 figures", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;68T07;I.2.6; I.5.1", "links": "http://arxiv.org/abs/2505.21680v1;http://arxiv.org/pdf/2505.21680v1", "pdf_url": "http://arxiv.org/pdf/2505.21680v1"}, {"title": "Mitigating medical dataset bias by learning adaptive agreement from a biased council", "link": "https://www.sciencedirect.com/science/article/pii/S1361841525001768", "details": "L Luo, X Huang, M Wang, Z Wan, W Ma, H Chen - Medical Image Analysis, 2025", "abstract": "Dataset bias in images is an important yet less explored topic in medical images. Deep learning could be prone to learning spurious correlation raised by dataset bias, resulting in inaccurate, unreliable, and unfair models, which impedes its adoption in \u2026"}, {"title": "Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2", "link": "https://arxiv.org/pdf/2505.21715", "details": "MZ Hossain, M Ahmed, M Samu, MR Islam - arXiv preprint arXiv:2505.21715, 2025", "abstract": "The automated generation of radiology reports from chest X-ray images holds significant promise in enhancing diagnostic workflows while preserving patient privacy. Traditional centralized approaches often require sensitive data transfer \u2026", "entry_id": "http://arxiv.org/abs/2505.21715v1", "updated": "2025-05-27 20:01:12", "published": "2025-05-27 20:01:12", "authors": "Md. Zahid Hossain;Mustofa Ahmed;Most. Sharmin Sultana Samu;Md. Rakibul Islam", "summary": "The automated generation of radiology reports from chest X-ray images holds\nsignificant promise in enhancing diagnostic workflows while preserving patient\nprivacy. Traditional centralized approaches often require sensitive data\ntransfer, posing privacy concerns. To address this, the study proposes a\nMultimodal Federated Learning framework for chest X-ray report generation using\nthe IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the\nencoder and GPT-2 as the report generator, enabling decentralized training\nwithout sharing raw data. Three Federated Learning (FL) aggregation strategies:\nFedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)\nwere evaluated. Among these, Krum Aggregation demonstrated superior performance\nacross lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore\nand RaTEScore. The results show that FL can match or surpass centralized models\nin generating clinically relevant and semantically rich radiology reports. This\nlightweight and privacy-preserving framework paves the way for collaborative\nmedical AI development without compromising data confidentiality.", "comment": "Preprint, manuscript under-review", "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2505.21715v1;http://arxiv.org/pdf/2505.21715v1", "pdf_url": "http://arxiv.org/pdf/2505.21715v1"}, {"title": "Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation", "link": "https://arxiv.org/pdf/2505.22222", "details": "Y Kim, J Wu, SH Kim, P Vasudev, J Shen, H Wu - arXiv preprint arXiv:2505.22222, 2025", "abstract": "Recent advancements in multimodal Large Language Models (LLMs) have significantly enhanced the automation of medical image analysis, particularly in generating radiology reports from chest X-rays (CXR). However, these models still \u2026", "entry_id": "http://arxiv.org/abs/2505.22222v1", "updated": "2025-05-28 10:54:40", "published": "2025-05-28 10:54:40", "authors": "Yunsoo Kim;Jinge Wu;Su-Hwan Kim;Pardeep Vasudev;Jiashu Shen;Honghan Wu", "summary": "Recent advancements in multimodal Large Language Models (LLMs) have\nsignificantly enhanced the automation of medical image analysis, particularly\nin generating radiology reports from chest X-rays (CXR). However, these models\nstill suffer from hallucinations and clinically significant errors, limiting\ntheir reliability in real-world applications. In this study, we propose Look &\nMark (L&M), a novel grounding fixation strategy that integrates radiologist eye\nfixations (Look) and bounding box annotations (Mark) into the LLM prompting\nframework. Unlike conventional fine-tuning, L&M leverages in-context learning\nto achieve substantial performance gains without retraining. When evaluated\nacross multiple domain-specific and general-purpose models, L&M demonstrates\nsignificant gains, including a 1.2% improvement in overall metrics (A.AVG) for\nCXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for\nLLaVA-Med. General-purpose models also benefit from L&M combined with\nin-context learning, with LLaVA-OV achieving an 87.3% clinical average\nperformance (C.AVG)-the highest among all models, even surpassing those\nexplicitly trained for CXR report generation. Expert evaluations further\nconfirm that L&M reduces clinically significant errors (by 0.43 average errors\nper report), such as false predictions and omissions, enhancing both accuracy\nand reliability. These findings highlight L&M's potential as a scalable and\nefficient solution for AI-assisted radiology, paving the way for improved\ndiagnostic workflows in low-resource clinical settings.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.CL", "links": "http://arxiv.org/abs/2505.22222v1;http://arxiv.org/pdf/2505.22222v1", "pdf_url": "http://arxiv.org/pdf/2505.22222v1"}]
