[{"title": "Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality", "link": "https://arxiv.org/pdf/2411.11531", "details": "V Chekalina, A Razzigaev, E Goncharova, A Kuznetsov - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper we present an approach to reduce hallucinations in Large Language Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional modality. Our method involves transforming input text into a set of KG embeddings and using \u2026"}, {"title": "Gradient Localization Improves Lifelong Pretraining of Language Models", "link": "https://arxiv.org/pdf/2411.04448", "details": "J Fernandez, Y Bisk, E Strubell - arXiv preprint arXiv:2411.04448, 2024", "abstract": "Large Language Models (LLMs) trained on web-scale text corpora have been shown to capture world knowledge in their parameters. However, the mechanism by which language models store different types of knowledge is poorly understood. In this \u2026"}, {"title": "Working memory identifies reasoning limits in language models", "link": "https://aclanthology.org/2024.emnlp-main.938.pdf", "details": "C Zhang, Y Jian, Z Ouyang, S Vosoughi - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "This study explores the inherent limitations of large language models (LLMs) from a scaling perspective, focusing on the upper bounds of their cognitive capabilities. We integrate insights from cognitive science to quantitatively examine how LLMs perform \u2026"}, {"title": "MPLite: Multi-Aspect Pretraining for Mining Clinical Health Records", "link": "https://arxiv.org/pdf/2411.11161", "details": "E Yang, P Hu, X Han, Y Ning - arXiv preprint arXiv:2411.11161, 2024", "abstract": "The adoption of digital systems in healthcare has resulted in the accumulation of vast electronic health records (EHRs), offering valuable data for machine learning methods to predict patient health outcomes. However, single-visit records of patients \u2026"}, {"title": "TP-UNet: Temporal Prompt Guided UNet for Medical Image Segmentation", "link": "https://arxiv.org/pdf/2411.11305", "details": "R Wang, L Zhuang, H Chen, B Xu, R Cai - arXiv preprint arXiv:2411.11305, 2024", "abstract": "The advancement of medical image segmentation techniques has been propelled by the adoption of deep learning techniques, particularly UNet-based approaches, which exploit semantic information to improve the accuracy of segmentations \u2026"}, {"title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding", "link": "https://aclanthology.org/2024.emnlp-main.870.pdf", "details": "L Tu, S Yavuz, J Qu, J Xu, R Meng, C Xiong, Y Zhou - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired \u2026"}]
