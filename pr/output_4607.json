[{"title": "The Art of Saying No: Contextual Noncompliance in Language Models", "link": "https://arxiv.org/pdf/2407.12043", "details": "F Brahman, S Kumar, V Balachandran, P Dasigi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Chat-based language models are designed to be helpful, yet they should not comply with every user request. While most existing work primarily focuses on refusal of\" unsafe\" queries, we posit that the scope of noncompliance should be broadened. We \u2026"}, {"title": "Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models", "link": "https://arxiv.org/pdf/2407.19474", "details": "N Bitton-Guetta, A Slobodkin, A Maimon, E Habba\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Imagine observing someone scratching their arm; to understand why, additional context would be necessary. However, spotting a mosquito nearby would immediately offer a likely explanation for the person's discomfort, thereby alleviating \u2026"}, {"title": "Do Language Models Have a Critical Period for Language Acquisition?", "link": "https://arxiv.org/pdf/2407.19325", "details": "I Constantinescu, T Pimentel, R Cotterell, A Warstadt - arXiv preprint arXiv \u2026, 2024", "abstract": "Humans appear to have a critical period (CP) for language acquisition: Second language (L2) acquisition becomes harder after early childhood, and ceasing exposure to a first language (L1) after this period (but not before) typically does not \u2026"}, {"title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models", "link": "https://arxiv.org/pdf/2407.03181", "details": "H Puerto, T Chubakov, X Zhu, HT Madabushi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model \u2026"}, {"title": "Efficient Training of Language Models with Compact and Consistent Next Token Distributions", "link": "https://arxiv.org/pdf/2407.02819", "details": "A Sathe, S Sarawagi - arXiv preprint arXiv:2407.02819, 2024", "abstract": "Maximizing the likelihood of the next token is an established, statistically sound objective for pre-training language models. In this paper we show that we can train better models faster by pre-aggregating the corpus with a collapsed $ n $-gram \u2026"}, {"title": "Strong Copyright Protection for Language Models via Adaptive Model Fusion", "link": "https://arxiv.org/pdf/2407.20105", "details": "J Abad, K Donhauser, F Pinto, F Yang - arXiv preprint arXiv:2407.20105, 2024", "abstract": "The risk of language models unintentionally reproducing copyrighted material from their training data has led to the development of various protective measures. In this paper, we propose model fusion as an effective solution to safeguard against \u2026"}, {"title": "Solving Robotics Problems in Zero-Shot with Vision-Language Models", "link": "https://arxiv.org/pdf/2407.19094", "details": "Z Wang, R Shen, B Stadie - arXiv preprint arXiv:2407.19094, 2024", "abstract": "We introduce Wonderful Team, a multi-agent visual LLM (VLLM) framework for solving robotics problems in the zero-shot regime. By zero-shot we mean that, for a novel environment, we feed a VLLM an image of the robot's environment and a \u2026"}, {"title": "Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models", "link": "https://arxiv.org/pdf/2407.15504", "details": "A Girish, A Nagle, M Bondaschi, M Gastpar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We formalize the problem of prompt compression for large language models (LLMs) and present a framework to unify token-level prompt compression methods which create hard prompts for black-box models. We derive the distortion-rate function for \u2026"}, {"title": "Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation", "link": "https://arxiv.org/pdf/2407.19619", "details": "M Bhattarai, JE Santos, S Jones, A Biswas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The advent of large language models (LLMs) has significantly advanced the field of code translation, enabling automated translation between programming languages. However, these models often struggle with complex translation tasks due to \u2026"}]
