[{"title": "HERGen: Elevating Radiology Report Generation with Longitudinal Data", "link": "https://arxiv.org/pdf/2407.15158", "details": "F Wang, S Du, L Yu - arXiv preprint arXiv:2407.15158, 2024", "abstract": "Radiology reports provide detailed descriptions of medical imaging integrated with patients' medical histories, while report writing is traditionally labor-intensive, increasing radiologists' workload and the risk of diagnostic errors. Recent efforts in \u2026"}, {"title": "When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?", "link": "https://arxiv.org/pdf/2407.15211", "details": "R Schaeffer, D Valentine, L Bailey, J Chua\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The integration of new modalities into frontier AI systems offers exciting capabilities, but also increases the possibility such systems can be adversarially manipulated in undesirable ways. In this work, we focus on a popular class of vision-language \u2026"}, {"title": "Assessing Brittleness of Image-Text Retrieval Benchmarks from Vision-Language Models Perspective", "link": "https://arxiv.org/pdf/2407.15239", "details": "M Hendriksen, S Zhang, R Reinanda, M Yahya, E Meij\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Image-text retrieval (ITR), an important task in information retrieval (IR), is driven by pretrained vision-language models (VLMs) that consistently achieve state-of-the-art performance. However, a significant challenge lies in the brittleness of existing ITR \u2026"}, {"title": "Evaluating language models as risk scores", "link": "https://arxiv.org/pdf/2407.14614", "details": "AF Cruz, M Hardt, C Mendler-D\u00fcnner - arXiv preprint arXiv:2407.14614, 2024", "abstract": "Current question-answering benchmarks predominantly focus on accuracy in realizable prediction tasks. Conditioned on a question and answer-key, does the most likely token match the ground truth? Such benchmarks necessarily fail to \u2026"}, {"title": "In-Context Learning Improves Compositional Understanding of Vision-Language Models", "link": "https://arxiv.org/pdf/2407.15487", "details": "M Nulli, A Ibrahimi, A Pal, H Lee, I Najdenkoska - arXiv preprint arXiv:2407.15487, 2024", "abstract": "Vision-Language Models (VLMs) have shown remarkable capabilities in a large number of downstream tasks. Nonetheless, compositional image understanding remains a rather difficult task due to the object bias present in training data. In this \u2026"}]
