'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Addax: Memory-Efficient Fine-Tuning of Language Models'
[{"title": "Gaze-infused BERT: Do human gaze signals help pre-trained language models?", "link": "https://link.springer.com/article/10.1007/s00521-024-09725-8", "details": "B Wang, B Liang, L Zhou, R Xu - Neural Computing and Applications, 2024", "abstract": "This research delves into the intricate connection between self-attention mechanisms in large-scale pre-trained language models, like BERT, and human gaze patterns, with the aim of harnessing gaze information to enhance the performance of natural \u2026"}, {"title": "Is Less More? Quality, Quantity and Context in Idiom Processing with Natural Language Models", "link": "https://arxiv.org/abs/2405.08497", "details": "A Knietaite, A Allsebrook, A Minkov, A Tomaszewski\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Compositionality in language models presents a problem when processing idiomatic expressions, as their meaning often cannot be directly derived from their individual parts. Although fine-tuning and other optimization strategies can be used to improve \u2026"}, {"title": "Self-Play Preference Optimization for Language Model Alignment", "link": "https://arxiv.org/pdf/2405.00675", "details": "Y Wu, Z Sun, H Yuan, K Ji, Y Yang, Q Gu - arXiv preprint arXiv:2405.00675, 2024", "abstract": "Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest \u2026"}, {"title": "Bridging the Bosphorus: Advancing Turkish Large Language Models through Strategies for Low-Resource Language Adaptation and Benchmarking", "link": "https://arxiv.org/pdf/2405.04685", "details": "EC Acikgoz, M Erdogan, D Yuret - arXiv preprint arXiv:2405.04685, 2024", "abstract": "Large Language Models (LLMs) are becoming crucial across various fields, emphasizing the urgency for high-quality models in underrepresented languages. This study explores the unique challenges faced by low-resource languages, such \u2026"}]
