[{"title": "CARE: Aligning Language Models for Regional Cultural Awareness", "link": "https://arxiv.org/pdf/2504.05154%3F", "details": "G Guo, T Naous, H Wakaki, Y Nishimura, Y Mitsufuji\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing language models (LMs) often exhibit a Western-centric bias and struggle to represent diverse cultural knowledge. Previous attempts to address this rely on synthetic data and express cultural knowledge only in English. In this work, we study \u2026"}, {"title": "Vision-language foundation model for generalizable nasal disease diagnosis using unlabeled endoscopic records", "link": "https://www.sciencedirect.com/science/article/pii/S0031320325003061", "details": "X Liu, W Gong, X Chen, Z Li, Y Liu, L Wang, Q Liu\u2026 - Pattern Recognition, 2025", "abstract": "Medical artificial intelligence (AI) holds significant potential in identifying signs of health conditions in nasal endoscopic images, thereby accelerating the diagnosis of diseases and systemic disorders. However, the performance of AI models heavily \u2026"}, {"title": "Understanding contraceptive switching rationales from real world clinical notes using large language models", "link": "https://www.nature.com/articles/s41746-025-01615-0", "details": "BY Miao, CYK Williams, E Chinedu-Eneh, T Zack\u2026 - npj Digital Medicine, 2025", "abstract": "Understanding reasons for treatment switching is of significant medical interest, but these factors are often only found in unstructured clinical notes and can be difficult to extract. We evaluated the zero-shot abilities of GPT-4 and eight other open-source \u2026"}, {"title": "Challenges in the Postmarket Surveillance of Clinical Prediction Models", "link": "https://ai.nejm.org/doi/abs/10.1056/AIp2401116", "details": "S Ansari, B Baur, K Singh, AJ Admon - NEJM AI, 2025", "abstract": "Predictive artificial intelligence (AI) models enhance clinical workflows with applications such as prognostication and decision support, yet suffer from postdeployment performance challenges due to dataset shifts. Regulatory guidelines \u2026"}, {"title": "ELOQUENT CLEF Shared Tasks for Evaluation of Generative Language Model Quality, 2025 Edition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-88720-8_56", "details": "J Karlgren, E Artemova, O Bojar, V Mikhailov\u2026 - European Conference on \u2026, 2025", "abstract": "The ELOQUENT lab for evaluation of generative language model quality and usefulness addresses high-level quality criteria through a set of open-ended shared tasks implemented, where possible, to leverage the ability of systems built on \u2026"}, {"title": "Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning", "link": "https://arxiv.org/pdf/2504.05632", "details": "S Kabra, A Jha, C Reddy - arXiv preprint arXiv:2504.05632, 2025", "abstract": "Recent advances in large-scale generative language models have shown that reasoning capabilities can significantly improve model performance across a variety of tasks. However, the impact of reasoning on a model's ability to mitigate \u2026"}, {"title": "Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models", "link": "https://arxiv.org/pdf/2504.12315", "details": "X Ji, J Wang, H Zhang, J Zhang, H Zhou, C Sun, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the development of Multimodal Large Language Models (MLLMs), numerous outstanding accomplishments have emerged within the open-source community. Due to the complexity of creating and training multimodal data pairs, it is still a \u2026"}, {"title": "Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic Evaluation of Language Models", "link": "https://arxiv.org/pdf/2504.01001%3F", "details": "J Pombal, NM Guerreiro, R Rei, AFT Martins - arXiv preprint arXiv:2504.01001, 2025", "abstract": "As language models improve and become capable of performing more complex tasks across modalities, evaluating them automatically becomes increasingly challenging. Developing strong and robust task-specific automatic metrics gets \u2026"}, {"title": "A Reality Check of Vision-Language Pre-training in Radiology: Have We Progressed Using Text?", "link": "https://arxiv.org/pdf/2504.05227", "details": "J Silva-Rodr\u00edguez, J Dolz, IB Ayed - arXiv preprint arXiv:2504.05227, 2025", "abstract": "Vision-language pre-training has recently gained popularity as it allows learning rich feature representations using large-scale data sources. This paradigm has quickly made its way into the medical image analysis community. In particular, there is an \u2026"}]
