[{"title": "LoRA $^ 2$: Multi-Scale Low-Rank Approximations for Fine-Tuning Large Language Models", "link": "https://arxiv.org/pdf/2408.06854", "details": "JC Zhang, YJ Xiong, HX Qiu, DH Zhu, CM Xia - arXiv preprint arXiv:2408.06854, 2024", "abstract": "Fine-tuning large language models (LLMs) with high parameter efficiency for downstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA) significantly reduces the number of trainable parameters for fine-tuning. Although it \u2026"}, {"title": "Decoupling Breaks Data Barriers: A Decoupled Pre-training Framework for Multi-Intent Spoken Language Understanding", "link": "https://www.ijcai.org/proceedings/2024/0715.pdf", "details": "L Qin, Q Chen, J Zhou, Q Li, C Lu, W Che", "abstract": "Abstract Multi-intent Spoken Language Understanding (Multi-intent SLU) can extract multiple intents in a single utterance, gaining increasing attention. Nevertheless, current multi-intent SLU approaches still heavily rely on large amounts of annotated \u2026"}, {"title": "SICAR at RRG2024: GPU Poor's Guide to Radiology Report Generation", "link": "https://aclanthology.org/2024.bionlp-1.55.pdf", "details": "K Udomlapsakul, P Pengpun, T Saengja\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "Radiology report generation (RRG) aims to create free-text radiology reports from clinical imaging. Our solution employs a lightweight multimodal language model (MLLM) enhanced with a two-stage post-processing strategy, utilizing a Large \u2026"}]
