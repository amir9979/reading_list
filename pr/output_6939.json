[{"title": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language Models on a Single GPU", "link": "https://arxiv.org/pdf/2409.09086", "details": "Z Ning, J Zhao, Q Jin, W Ding, M Guo - arXiv preprint arXiv:2409.09086, 2024", "abstract": "Multimodal Large Language Models (MLLMs) are distinguished by their multimodal comprehensive ability and widely used in many real-world applications including GPT-4o, autonomous driving and robotics. Despite their impressive performance, the \u2026"}, {"title": "Generating Synthetic Datasets for Few-shot Prompt Tuning", "link": "https://openreview.net/pdf%3Fid%3DVd0KvChLXr", "details": "X Guo, Z Du, B Li, C Miao - First Conference on Language Modeling", "abstract": "A major limitation of prompt tuning is its dependence on large labeled training datasets. Under few-shot learning settings, prompt tuning lags far behind full-model fine-tuning, limiting its scope of application. In this paper, we leverage the powerful \u2026"}]
