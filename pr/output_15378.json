[{"title": "Synthetic Data Enhances Mathematical Reasoning of Language Models Based on Artificial Intelligence", "link": "https://www.itc.ktu.lt/index.php/ITC/article/view/39713/16892", "details": "Z Han, W Jiang - Information Technology and Control, 2025", "abstract": "Current large language models (LLMs) training involves extensive training data and computing resources to handle multiple natural language processing (NLP) tasks. This paper endeavors to assist individuals to compose feasible mathematical \u2026"}, {"title": "Ethereum Price Prediction Employing Large Language Models for Short-term and Few-shot Forecasting", "link": "https://arxiv.org/pdf/2503.23190", "details": "E Makri, G Palaiokrassas, S Bouraga\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Cryptocurrencies have transformed financial markets with their innovative blockchain technology and volatile price movements, presenting both challenges and opportunities for predictive analytics. Ethereum, being one of the leading \u2026"}, {"title": "Reimagining Urban Science: Scaling Causal Inference with Large Language Models", "link": "https://arxiv.org/pdf/2504.12345", "details": "Y Xia, A Qu, Y Zheng, Y Tang, D Zhuang, Y Liang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Urban causal research is essential for understanding the complex dynamics of cities and informing evidence-based policies. However, it is challenged by the inefficiency and bias of hypothesis generation, barriers to multimodal data complexity, and the \u2026"}, {"title": "Advancing Feature Extraction in Healthcare through the Integration of Knowledge Graphs and Large Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/35224/37379", "details": "FL Piya, R Beheshti - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "The exponential growth of unstructured medical data presents a unique opportunity and challenge for advancing healthcare. Traditional methods struggle to extract meaningful insights from this complex data due to its inherent noise, ambiguity, and \u2026"}, {"title": "Data-efficient LLM Fine-tuning for Code Generation", "link": "https://arxiv.org/pdf/2504.12687", "details": "W Lv, X Xia, SJ Huang - arXiv preprint arXiv:2504.12687, 2025", "abstract": "Large language models (LLMs) have demonstrated significant potential in code generation tasks. However, there remains a performance gap between open-source and closed-source models. To address this gap, existing approaches typically \u2026"}, {"title": "LLM+ MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language", "link": "https://arxiv.org/pdf/2503.17309", "details": "K Chu, X Zhao, C Weber, S Wermter - arXiv preprint arXiv:2503.17309, 2025", "abstract": "Bimanual robotic manipulation provides significant versatility, but also presents an inherent challenge due to the complexity involved in the spatial and temporal coordination between two hands. Existing works predominantly focus on attaining \u2026"}, {"title": "The Role of Code Readability in Large Language Model Code Summarization", "link": "https://www.researchgate.net/profile/Sergey-Yurish/publication/390829629_Advances_in_Signal_Processing_and_Artificial_Intelligence_Proceedings_of_the_7th_International_Conference_on_Advances_in_Signal_Processing_and_Artificial_Intelligence_8-10_April_2025_Innsbruck_Austria/links/67ff7640bfbe974b23aabbd6/Advances-in-Signal-Processing-and-Artificial-Intelligence-Proceedings-of-the-7th-International-Conference-on-Advances-in-Signal-Processing-and-Artificial-Intelligence-8-10-April-2025-Innsbruck-Austri.pdf%23page%3D149", "details": "B Szalontai, G Szalay, T M\u00e1rton, A Sike, P M\u00e1tray\u2026 - Advances in Signal \u2026, 2025", "abstract": "Large Language Models (LLMs) have demonstrated good performance in various software engineering tasks, including code summarization-explaining what a code snippet does. In this paper, we argue that the readability of the input code is crucial to \u2026"}, {"title": "Enhancing Radiology Clinical Histories Through Transformer-Based Automated Clinical Note Summarization", "link": "https://link.springer.com/article/10.1007/s10278-025-01477-8", "details": "N Eghbali, C Klochko, Z Mahdi, L Alhiari, J Lee\u2026 - Journal of Imaging \u2026, 2025", "abstract": "Insufficient clinical information provided in radiology requests, coupled with the cumbersome nature of electronic health records (EHRs), poses significant challenges for radiologists in extracting pertinent clinical data and compiling detailed \u2026"}, {"title": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates", "link": "https://arxiv.org/pdf/2503.16334", "details": "Y Shen, L Huang - arXiv preprint arXiv:2503.16334, 2025", "abstract": "Recent findings reveal that much of the knowledge in a Transformer-based Large Language Model (LLM) is encoded in its feed-forward (FFN) layers, where each FNN layer can be interpreted as the summation of sub-updates, each corresponding to a \u2026"}]
