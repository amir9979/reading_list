[{"title": "Pairwise Proximal Policy Optimization: Language Model Alignment with Comparative RL", "link": "https://openreview.net/pdf%3Fid%3D7iaAlIlV2H", "details": "T Wu, B Zhu, R Zhang, Z Wen, K Ramchandran, J Jiao - First Conference on Language \u2026", "abstract": "LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior is Reinforcement Learning with Human Feedback (RLHF). This involves training a reward model with \u2026"}, {"title": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language Models", "link": "https://arxiv.org/pdf/2408.15778", "details": "J Gui, Y Liu, J Cheng, X Gu, X Liu, H Wang, Y Dong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated notable capabilities across various tasks, showcasing complex problem-solving abilities. Understanding and executing complex rules, along with multi-step planning, are fundamental to logical \u2026"}, {"title": "Assessing Contamination in Large Language Models: Introducing the LogProber method", "link": "https://arxiv.org/pdf/2408.14352", "details": "N Yax, PY Oudeyer, S Palminteri - arXiv preprint arXiv:2408.14352, 2024", "abstract": "In machine learning, contamination refers to situations where testing data leak into the training set. The issue is particularly relevant for the evaluation of the performance of Large Language Models (LLMs), which are generally trained on \u2026"}, {"title": "Generating Synthetic Datasets for Few-shot Prompt Tuning", "link": "https://openreview.net/pdf%3Fid%3DVd0KvChLXr", "details": "X Guo, Z Du, B Li, C Miao - First Conference on Language Modeling", "abstract": "A major limitation of prompt tuning is its dependence on large labeled training datasets. Under few-shot learning settings, prompt tuning lags far behind full-model fine-tuning, limiting its scope of application. In this paper, we leverage the powerful \u2026"}, {"title": "Towards Improving Large Language Models' Planning Capabilities on WoT Thing Descriptions by Generating Python Objects as Intermediary Representations", "link": "https://ceur-ws.org/Vol-3749/akr3-06.pdf", "details": "L Kinder, T K\u00e4fer - 2024", "abstract": "This paper presents a novel method for plan generation with Large Language Model (LLM). We propose utilizing Web-of-Thing Thing Descriptions (WoT TD) to inform the LLM about available devices for interaction. We investigate a novel pipeline in \u2026"}, {"title": "Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage", "link": "https://arxiv.org/pdf/2408.17354", "details": "MRU Rashid, J Liu, T Koike-Akino, S Mehnaz, Y Wang - arXiv preprint arXiv \u2026, 2024", "abstract": "Fine-tuning large language models on private data for downstream applications poses significant privacy risks in potentially exposing sensitive information. Several popular community platforms now offer convenient distribution of a large variety of \u2026"}]
