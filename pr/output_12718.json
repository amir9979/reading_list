[{"title": "CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification", "link": "https://arxiv.org/pdf/2501.12266", "details": "C Patr\u00edcio, I Rio-Torto, JS Cardoso, LF Teixeira\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter \u2026"}, {"title": "Rethinking Homogeneity of Vision and Text Tokens in Large Vision-and-Language Models", "link": "https://arxiv.org/pdf/2502.01906", "details": "CW Kuo, S Zhu, F Chen, X Shen, L Wen - arXiv preprint arXiv:2502.01906, 2025", "abstract": "Large vision-and-language models (LVLMs) typically treat visual and textual embeddings as homogeneous inputs to a large language model (LLM). However, these inputs are inherently different: visual inputs are multi-dimensional and \u2026"}, {"title": "Are Traditional Deep Learning Model Approaches as Effective as a Retinal-Specific Foundation Model for Ocular and Systemic Disease Detection?", "link": "https://arxiv.org/pdf/2501.12016", "details": "SME Yew, X Lei, JHL Goh, Y Chen, S Srinivasan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Background: RETFound, a self-supervised, retina-specific foundation model (FM), showed potential in downstream applications. However, its comparative performance with traditional deep learning (DL) models remains incompletely understood. This \u2026"}, {"title": "Focus Your Attention: Multiple Instance Learning with Attention Modification for Whole Slide Pathological Image Classification", "link": "https://ieeexplore.ieee.org/iel8/76/4358651/10838539.pdf", "details": "H Cheng, S Huang, L Cai, Y Xu, R Wang, Y Zhang - IEEE Transactions on Circuits \u2026, 2025", "abstract": "Computer-aided pathology diagnosis based on whole slide images, which is often formulated as a weakly supervised multiple instance learning (MIL) paradigm. Current approaches generally employ attention mechanisms to aggregate instance \u2026"}, {"title": "Towards Zero-Shot & Explainable Video Description by Reasoning over Graphs of Events in Space and Time", "link": "https://arxiv.org/pdf/2501.08460", "details": "M Masala, M Leordeanu - arXiv preprint arXiv:2501.08460, 2025", "abstract": "In the current era of Machine Learning, Transformers have become the de facto approach across a variety of domains, such as computer vision and natural language processing. Transformer-based solutions are the backbone of current state-of-the-art \u2026"}, {"title": "Fine-grained medical image out-of-distribution detection through multi-view feature uncertainty and adversarial sample generation", "link": "https://www.sciencedirect.com/science/article/pii/S0031320325000615", "details": "J Wei, G Wang, S Zhang - Pattern Recognition, 2025", "abstract": "Abstract Out-Of-Distribution (OOD) detection is important for reliable deployment of medical image classifiers but challenging considering the fine-grained distinction between in-distribution and OOD samples. Existing medical image OOD detection \u2026"}, {"title": "Enhancing few-shot KB-VQA with panoramic image captions guided by Large Language Models", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225000451", "details": "P Qiang, H Tan, X Li, D Wang, R Li, X Sun, H Zhang\u2026 - Neurocomputing, 2025", "abstract": "Current state-of-the-art (SOTA) KB-VQA techniques involve transforming images into image captions as prompts to harness the potent reasoning capabilities of large language models (LLMs) for generating answers. However, generic image captions \u2026"}]
