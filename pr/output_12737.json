[{"title": "When Evolution Strategy Meets Language Models Tuning", "link": "https://aclanthology.org/2025.coling-main.357.pdf", "details": "B Huang, Y Jiang, M Chen, Y Wang, H Chen, W Wang - Proceedings of the 31st \u2026, 2025", "abstract": "Supervised Fine-tuning has been pivotal in training autoregressive language models, yet it introduces exposure bias. To mitigate this, Post Fine-tuning, including on-policy and off-policy methods, has emerged as a solution to enhance models \u2026"}, {"title": "Vision-language models do not understand negation", "link": "https://arxiv.org/pdf/2501.09425", "details": "K Alhamoud, S Alshammari, Y Tian, G Li, P Torr, Y Kim\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Many practical vision-language applications require models that understand negation, eg, when using natural language to retrieve images which contain certain objects but not others. Despite advancements in vision-language models (VLMs) \u2026"}, {"title": "Risk-Aware Distributional Intervention Policies for Language Models", "link": "https://arxiv.org/pdf/2501.15758", "details": "B Nguyen, B Nguyen, D Nguyen, VA Nguyen - arXiv preprint arXiv:2501.15758, 2025", "abstract": "Language models are prone to occasionally undesirable generations, such as harmful or toxic content, despite their impressive capability to produce texts that appear accurate and coherent. This paper presents a new two-stage approach to \u2026"}, {"title": "When Large Vision Language Models Meet Multimodal Sequential Recommendation: An Empirical Study", "link": "https://openreview.net/pdf%3Fid%3DE8bjWloEvU", "details": "P Zhou, C Liu, J Ren, X Zhou, XIE Yueqi, M Cao, Z Rao\u2026 - THE WEB CONFERENCE 2025", "abstract": "As multimedia content continues to grow on the Web, the integration of visual and textual data has become a crucial challenge for Web applications, particularly in recommendation systems. Large Vision Language Models (LVLMs) have \u2026"}, {"title": "Dynamic link prediction: Using language models and graph structures for temporal knowledge graph completion with emerging entities and relations", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425002702", "details": "R Ong, J Sun, YK Guo, O Serban - Expert Systems with Applications, 2025", "abstract": "Abstract Knowledge graphs (KGs) represent real-world facts through entities and relations. However, static KGs fail to capture continuously emerging entities and relations over time. Temporal knowledge graphs address this by incorporating time \u2026"}, {"title": "An Empirical Study on Challenging Math Problem Solving with LLM-based Conversational Agents", "link": "https://etda.libraries.psu.edu/files/final_submissions/31835", "details": "Y Wu - 2025", "abstract": "The application of Large Language Models (LLMs) in solving mathematical problems expressed in natural language is a promising area of research, especially given their potential to serve as foundational models across various domains. This paper \u2026"}, {"title": "Benchmarking Large Language Models via Random Variables", "link": "https://arxiv.org/pdf/2501.11790", "details": "Z Hong, H Wu, S Dong, J Dong, Y Xiao, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the continuous advancement of large language models (LLMs) in mathematical reasoning, evaluating their performance in this domain has become a prominent research focus. Recent studies have raised concerns about the reliability of current \u2026"}, {"title": "Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective", "link": "https://aclanthology.org/2025.coling-main.212.pdf", "details": "T Li, Z Wang, W Liu, M Wu, S Dou, C Lv, X Wang\u2026 - Proceedings of the 31st \u2026, 2025", "abstract": "The recent surge in jailbreaking attacks has revealed significant vulnerabilities in Large Language Models (LLMs) when exposed to malicious inputs. While various defense strategies have been proposed to mitigate these threats, there has been \u2026"}, {"title": "Chain-of-Specificity: Enhancing Task-Specific Constraint Adherence in Large Language Models", "link": "https://aclanthology.org/2025.coling-main.164.pdf", "details": "K Wei, J Zhong, H Zhang, F Zhang, D Zhang, L Jin\u2026 - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract Large Language Models (LLMs) exhibit remarkable generative capabilities, enabling the generation of valuable information. Despite these advancements, previous research found that LLMs sometimes struggle with adhering to specific \u2026"}]
