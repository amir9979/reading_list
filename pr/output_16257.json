[{"title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "link": "https://arxiv.org/pdf/2504.14194", "details": "X Zhuang, J Peng, R Ma, Y Wang, T Bai, X Wei, J Qiu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural \u2026"}, {"title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "link": "https://arxiv.org/pdf/2504.14366", "details": "P Haller, J Golde, A Akbik - arXiv preprint arXiv:2504.14366, 2025", "abstract": "Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures \u2026"}, {"title": "Platonic Grounding for Efficient Multimodal Language Models", "link": "https://arxiv.org/pdf/2504.19327", "details": "M Choraria, X Wu, A Bhimaraju, N Sekhar, Y Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The hyperscaling of data and parameter count in Transformer-based models is yielding diminishing performance improvement, especially when weighed against training costs. Such plateauing indicates the importance of methods for more efficient \u2026"}, {"title": "ScriptSmith: A Unified LLM Framework for Enhancing IT Operations via Automated Bash Script Generation, Assessment, and Refinement", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/35147/37302", "details": "P Aggarwal, O Chatterjee, T Dai, S Samanta\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "In the rapidly evolving landscape of site reliability engineering (SRE), the demand for efficient and effective solutions to manage and resolve issues in site and cloud applications is paramount. This paper presents an innovative approach to action \u2026"}, {"title": "Can Long-Context Language Models Solve Repository-Level Code Generation?", "link": "https://openreview.net/pdf%3Fid%3DpmcWo9DtDw", "details": "Y PENG, ZZ Wang, D Fried - LTI Student Research Symposium 2025", "abstract": "With the advance of real-world tasks that necessitate increasingly long contexts, recent language models (LMs) have begun to support longer context windows. One particularly complex task is repository-level code generation, where retrieval \u2026"}, {"title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data", "link": "https://arxiv.org/pdf/2505.05427", "details": "Y Wang, Z Fu, J Cai, P Tang, H Lyu, Y Fang, Z Zheng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Data quality has become a key factor in enhancing model performance with the rapid development of large language models (LLMs). Model-driven data filtering has increasingly become a primary approach for acquiring high-quality data. However, it \u2026"}, {"title": "Efficient Tuning of Large Language Models for Knowledge-Grounded Dialogue Generation", "link": "https://arxiv.org/pdf/2504.07754%3F", "details": "B Zhang, H Ma, D Li, J Ding, J Wang, B Xu, HF Lin - arXiv preprint arXiv:2504.07754, 2025", "abstract": "Large language models (LLMs) demonstrate remarkable text comprehension and generation capabilities but often lack the ability to utilize up-to-date or domain- specific knowledge not included in their training data. To address this gap, we \u2026"}, {"title": "Development and analysis of new nonparametric techniques for causal inference in observational studies", "link": "https://belgelik.isikun.edu.tr/xmlui/bitstream/handle/iubelgelik/6762/vol.15.no.5_19.pdf%3Fsequence%3D1%26isAllowed%3Dy", "details": "SR Thanoonr - TWMS Journal of Applied and Engineering \u2026, 2025", "abstract": "The lack of randomization methods in observational studies blocks researchers from reaching valid conclusions based on their data. The lack of randomisation techniques results in multiple experimental factors which appear in the research \u2026"}, {"title": "Foundation models for electronic health records: representation dynamics and transferability", "link": "https://arxiv.org/pdf/2504.10422", "details": "MC Burkhart, B Ramadan, Z Liao, K Chhikara\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Foundation models (FMs) trained on electronic health records (EHRs) have shown strong performance on a range of clinical prediction tasks. However, adapting these models to local health systems remains challenging due to limited data availability \u2026"}]
