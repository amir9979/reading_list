[{"title": "Boosting the Generalization and Reasoning of Vision Language Models with Curriculum Reinforcement Learning", "link": "https://arxiv.org/pdf/2503.07065%3F", "details": "H Deng, D Zou, R Ma, H Luo, Y Cao, Y Kang - arXiv preprint arXiv:2503.07065, 2025", "abstract": "While state-of-the-art vision-language models (VLMs) have demonstrated remarkable capabilities in complex visual-text tasks, their success heavily relies on massive model scaling, limiting their practical deployment. Small-scale VLMs offer a \u2026"}, {"title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.23064", "details": "Y Ren, K Tertikas, S Maiti, J Han, T Zhang, S S\u00fcsstrunk\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision-Language Models (LVLMs) struggle with puzzles, which require precise perception, rule comprehension, and logical reasoning. Assessing and enhancing their performance in this domain is crucial, as it reflects their ability to \u2026"}, {"title": "Balcony: A Lightweight Approach to Dynamic Inference of Generative Language Models", "link": "https://arxiv.org/pdf/2503.05005", "details": "B Jamialahmadi, P Kavehzadeh, M Rezagholizadeh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Deploying large language models (LLMs) in real-world applications is often hindered by strict computational and latency constraints. While dynamic inference offers the flexibility to adjust model behavior based on varying resource budgets, existing \u2026"}, {"title": "Optimizing Language Models for Inference Time Objectives using Reinforcement Learning", "link": "https://arxiv.org/pdf/2503.19595", "details": "Y Tang, K Zheng, G Synnaeve, R Munos - arXiv preprint arXiv:2503.19595, 2025", "abstract": "In this work, we investigate the merits of explicitly optimizing for inference time algorithmic performance during model training. We show how optimizing for inference time performance can improve overall model efficacy. We consider generic \u2026"}, {"title": "Multidimensional Consistency Improves Reasoning in Language Models", "link": "https://arxiv.org/pdf/2503.02670", "details": "H Lai, X Zhang, M Nissim - arXiv preprint arXiv:2503.02670, 2025", "abstract": "While Large language models (LLMs) have proved able to address some complex reasoning tasks, we also know that they are highly sensitive to input variation, which can lead to different solution paths and final answers. Answer consistency across \u2026"}, {"title": "Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic Evaluation of Language Models", "link": "https://arxiv.org/pdf/2504.01001", "details": "J Pombal, NM Guerreiro, R Rei, AFT Martins - arXiv preprint arXiv:2504.01001, 2025", "abstract": "As language models improve and become capable of performing more complex tasks across modalities, evaluating them automatically becomes increasingly challenging. Developing strong and robust task-specific automatic metrics gets \u2026"}, {"title": "Enhanced Continual Learning of Vision-Language Models with Model Fusion", "link": "https://arxiv.org/pdf/2503.10705", "details": "H Gao, Z Zhang, Y Wei, L Zhao, G Li, Y Li, L Kong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-Language Models (VLMs) represent a breakthrough in artificial intelligence by integrating visual and textual modalities to achieve impressive zero-shot capabilities. However, VLMs are susceptible to catastrophic forgetting when sequentially fine \u2026"}, {"title": "Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models via Watermarking", "link": "https://arxiv.org/pdf/2503.04636", "details": "Y Xu, A Liu, X Hu, L Wen, H Xiong - arXiv preprint arXiv:2503.04636, 2025", "abstract": "As open-source large language models (LLMs) like Llama3 become more capable, it is crucial to develop watermarking techniques to detect their potential misuse. Existing watermarking methods either add watermarks during LLM inference, which \u2026"}, {"title": "Combinatorial Optimization via LLM-driven Iterated Fine-tuning", "link": "https://arxiv.org/pdf/2503.06917", "details": "P Awasthi, S Gollapudi, R Kumar, K Munagala - arXiv preprint arXiv:2503.06917, 2025", "abstract": "We present a novel way to integrate flexible, context-dependent constraints into combinatorial optimization by leveraging Large Language Models (LLMs) alongside traditional algorithms. Although LLMs excel at interpreting nuanced, locally specified \u2026"}]
