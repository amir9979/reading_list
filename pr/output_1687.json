'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Recall Them All: Retrieval-Augmented Language Models f'
[{"title": "Adaptive Reinforcement Tuning Language Models as Hard Data Generators for Sentence Representation", "link": "https://aclanthology.org/2024.lrec-main.33.pdf", "details": "B Xu, Y Wu, S Wei, M Du, H Wang - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Sentence representation learning is a fundamental task in NLP. Existing methods use contrastive learning (CL) to learn effective sentence representations, which benefit from high-quality contrastive data but require extensive human annotation \u2026"}, {"title": "Redefining Information Retrieval of Structured Database via Large Language Models", "link": "https://arxiv.org/pdf/2405.05508", "details": "M Wang, Y Zhang, Q Zhao, J Yang, H Zhang - arXiv preprint arXiv:2405.05508, 2024", "abstract": "Retrieval augmentation is critical when Language Models (LMs) exploit non- parametric knowledge related to the query through external knowledge bases before reasoning. The retrieved information is incorporated into LMs as context alongside \u2026"}, {"title": "Backdoor Removal for Generative Large Language Models", "link": "https://arxiv.org/pdf/2405.07667", "details": "H Li, Y Chen, Z Zheng, Q Hu, C Chan, H Liu, Y Song - arXiv preprint arXiv \u2026, 2024", "abstract": "With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased \u2026"}, {"title": "BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models", "link": "https://arxiv.org/pdf/2405.04756", "details": "CF Luo, A Ghawanmeh, X Zhu, FK Khattak - arXiv preprint arXiv:2405.04756, 2024", "abstract": "Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly. The language model can also \u2026"}, {"title": "LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play", "link": "https://arxiv.org/pdf/2405.06373", "details": "LC Lu, SJ Chen, TM Pai, CH Yu, H Lee, SH Sun - arXiv preprint arXiv:2405.06373, 2024", "abstract": "Large language models (LLMs) have shown exceptional proficiency in natural language processing but often fall short of generating creative and original responses to open-ended questions. To enhance LLM creativity, our key insight is to \u2026"}, {"title": "Linearizing Large Language Models", "link": "https://arxiv.org/pdf/2405.06640", "details": "J Mercat, I Vasiljevic, S Keh, K Arora, A Dave, A Gaidon\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor \u2026"}, {"title": "Elements of World Knowledge (EWOK): A cognition-inspired framework for evaluating basic world knowledge in language models", "link": "https://ui.adsabs.harvard.edu/abs/2024arXiv240509605I/abstract", "details": "AA Ivanova, A Sathe, B Lipkin, U Kumar, S Radkani\u2026 - arXiv e-prints, 2024", "abstract": "The ability to build and leverage world models is essential for a general-purpose AI agent. Testing such capabilities is hard, in part because the building blocks of world models are ill-defined. We present Elements of World Knowledge (EWOK), a \u2026"}, {"title": "Lonas: Elastic low-rank adapters for efficient large language models", "link": "https://aclanthology.org/2024.lrec-main.940.pdf", "details": "JP Munoz, J Yuan, Y Zheng, N Jain - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) continue to grow, reaching hundreds of billions of parameters and making it challenging for Deep Learning practitioners with resource-constrained systems to use them, eg, fine-tuning these models for a \u2026"}, {"title": "Correcting Language Model Bias for Text Classification in True Zero-Shot Learning", "link": "https://aclanthology.org/2024.lrec-main.359.pdf", "details": "F Zhao, W Xianlin, C Yan, CK Loo - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Combining pre-trained language models (PLMs) and manual templates is a common practice for text classification in zero-shot scenarios. However, the effect of this approach is highly volatile, ranging from random guesses to near state-of-the-art \u2026"}]
