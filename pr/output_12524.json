[{"title": "Self-supervised analogical learning using language models", "link": "https://arxiv.org/pdf/2502.00996", "details": "B Zhou, S Jain, Y Zhang, Q Ning, S Wang, Y Benajiba\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can \u2026"}, {"title": "Scaling Laws for Differentially Private Language Models", "link": "https://arxiv.org/pdf/2501.18914", "details": "R McKenna, Y Huang, A Sinha, B Balle, Z Charles\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Scaling laws have emerged as important components of large language model (LLM) training as they can predict performance gains through scale, and provide guidance on important hyper-parameter choices that would otherwise be expensive \u2026"}, {"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}, {"title": "Calling a Spade a Heart: Gaslighting Multimodal Large Language Models via Negation", "link": "https://arxiv.org/pdf/2501.19017", "details": "B Zhu, Y Gui, J Chen, CW Ngo, EP Lim - arXiv preprint arXiv:2501.19017, 2025", "abstract": "Multimodal Large Language Models (MLLMs) have exhibited remarkable advancements in integrating different modalities, excelling in complex understanding and generation tasks. Despite their success, MLLMs remain vulnerable to \u2026"}, {"title": "Scaling Embedding Layers in Language Models", "link": "https://arxiv.org/pdf/2502.01637", "details": "D Yu, E Cohen, B Ghazi, Y Huang, P Kamath, R Kumar\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We propose SCONE ($\\textbf {S} $ calable, $\\textbf {C} $ ontextualized, $\\textbf {O} $ ffloaded, $\\textbf {N} $-gram $\\textbf {E} $ mbedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To \u2026"}, {"title": "Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective", "link": "https://arxiv.org/pdf/2502.01524", "details": "X Ma, H Xie, SJ Qin - arXiv preprint arXiv:2502.01524, 2025", "abstract": "The integration of vision-language modalities has been a significant focus in multimodal learning, traditionally relying on Vision-Language Pretrained Models. However, with the advent of Large Language Models (LLMs), there has been a \u2026"}, {"title": "FinMoE: A MoE-based Large Chinese Financial Language Model", "link": "https://aclanthology.org/2025.finnlp-1.4.pdf", "details": "X Zhang, Q Yang - Proceedings of the Joint Workshop of the 9th Financial \u2026, 2025", "abstract": "Large-scale language models have demonstrated remarkable success, achieving strong performance across a variety of general tasks. However, when applied to domain-specific fields, such as finance, these models face challenges due to the \u2026"}, {"title": "LLM-BS: Enhancing Large Language Models for Recommendation through Exogenous Behavior-Semantics Integration", "link": "https://openreview.net/pdf%3Fid%3Drm07DoACiF", "details": "M Hong, Y Xia, Z Wang, J Zhu, Y Wang, S Cai, X Yang\u2026 - THE WEB CONFERENCE 2025", "abstract": "Large language models (LLMs) are increasingly leveraged as foundational backbones in the development of advanced recommender systems, offering enhanced capabilities through their extensive knowledge and reasoning. Existing \u2026"}, {"title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models", "link": "https://arxiv.org/pdf/2501.17905", "details": "M Feng, J Wu, S Zhang, P Shao, R Jin, Z Wen, J Tao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have achieved significant progress across various domains, but their increasing scale results in high computational and memory costs. Recent studies have revealed that LLMs exhibit sparsity, providing the potential to \u2026"}]
