[{"title": "Respond in my Language: Mitigating Language Inconsistency in Response Generation based on Large Language Models", "link": "https://aclanthology.org/2024.acl-long.229.pdf", "details": "L Zhang, Q Jin, H Huang, D Zhang, F Wei - Proceedings of the 62nd Annual Meeting \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) show strong instruction understanding ability across multiple languages. However, they are easily biased towards English in instruction tuning, and generate English responses even given non-English \u2026"}, {"title": "Multiple Representation Contrastive Self-Supervised Learning for Pulmonary Nodule Detection", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124009419", "details": "A Torki, P Adibi, HB Kashani - Knowledge-Based Systems, 2024", "abstract": "Self-supervised learning aims to create semantic-enriched representation from unannotated data. A prevalent strategy in this field involves training a unified representation space that is invariant to various transformation combinations \u2026"}, {"title": "Masked Channel Modeling for Bootstrapping Visual Pre-training", "link": "https://link.springer.com/article/10.1007/s11263-024-02204-6", "details": "Y Liu, X Wang, M Zhu, Y Cao, T Huang, C Shen - International Journal of Computer \u2026, 2024", "abstract": "Large vision models have achieved great success in computer vision recently, eg, CLIP for large-scale image-text contrastive learning. They have prominent potential in representation learning and show strong transfer ability in various downstream \u2026"}, {"title": "Scaling up Multimodal Pre-training for Sign Language Understanding", "link": "https://arxiv.org/pdf/2408.08544", "details": "W Zhou, W Zhao, H Hu, Z Li, H Li - arXiv preprint arXiv:2408.08544, 2024", "abstract": "Sign language serves as the primary meaning of communication for the deaf-mute community. Different from spoken language, it commonly conveys information by the collaboration of manual features, ie, hand gestures and body movements, and non \u2026"}, {"title": "Scaling Backwards: Minimal Synthetic Pre-training?", "link": "https://arxiv.org/pdf/2408.00677", "details": "R Nakamura, R Tadokoro, R Yamada, YM Asano\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-training and transfer learning are an important building block of current computer vision systems. While pre-training is usually performed on large real-world image datasets, in this paper we ask whether this is truly necessary. To this end, we search \u2026"}, {"title": "Pretraining of 3D image segmentation models for retinal OCT using denoising-based self-supervised learning", "link": "https://opg.optica.org/viewmedia.cfm%3Furi%3Dboe-15-9-5025%26seq%3D0%26html%3Dtrue", "details": "A Rivail, T Ara\u00fajo, U Schmidt-Erfurth, H Bogunovi\u0107 - Biomedical Optics Express, 2024", "abstract": "Deep learning algorithms have allowed the automation of segmentation for many biomarkers in retinal OCTs, enabling comprehensive clinical research and precise patient monitoring. These segmentation algorithms predominantly rely on supervised \u2026"}, {"title": "VIOLA: Conditional Language Models for Speech Recognition, Synthesis, and Translation", "link": "https://ieeexplore.ieee.org/abstract/document/10613503/", "details": "T Wang, L Zhou, Z Zhang, Y Wu, S Liu, Y Gaur, Z Chen\u2026 - IEEE/ACM Transactions on \u2026, 2024", "abstract": "Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VIOLA, a single auto-regressive Transformer decoder-only network that \u2026"}, {"title": "Unsupervised Representation Learning by Balanced Self Attention Matching", "link": "https://arxiv.org/pdf/2408.02014", "details": "D Shalam, S Korman - arXiv preprint arXiv:2408.02014, 2024", "abstract": "Many leading self-supervised methods for unsupervised representation learning, in particular those for embedding image features, are built on variants of the instance discrimination task, whose optimization is known to be prone to instabilities that can \u2026"}, {"title": "HySparK: Hybrid Sparse Masking for Large Scale Medical Image Pre-Training", "link": "https://arxiv.org/pdf/2408.05815", "details": "F Tang, R Xu, Q Yao, X Fu, Q Quan, H Zhu, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The generative self-supervised learning strategy exhibits remarkable learning representational capabilities. However, there is limited attention to end-to-end pre- training methods based on a hybrid architecture of CNN and Transformer, which can \u2026"}]
