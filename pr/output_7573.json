[{"title": "VHELM: A Holistic Evaluation of Vision Language Models", "link": "https://arxiv.org/pdf/2410.07112", "details": "T Lee, H Tu, CH Wong, W Zheng, Y Zhou, Y Mai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation \u2026"}, {"title": "EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.07133", "details": "R Zhao, H Yuan, Y Wei, S Zhang, Y Gu, L Ran, X Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only \u2026"}, {"title": "Mutual Prompt Leaning for Vision Language Models", "link": "https://link.springer.com/article/10.1007/s11263-024-02243-z", "details": "S Long, Z Zhao, J Yuan, Z Tan, J Liu, J Feng, S Wang\u2026 - International Journal of \u2026, 2024", "abstract": "Large pre-trained vision language models (VLMs) have demonstrated impressive representation learning capabilities, but their transferability across various downstream tasks heavily relies on prompt learning. Since VLMs consist of text and \u2026"}, {"title": "Attention Prompting on Image for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.17143", "details": "R Yu, W Yu, X Wang - arXiv preprint arXiv:2409.17143, 2024", "abstract": "Compared with Large Language Models (LLMs), Large Vision-Language Models (LVLMs) can also accept images as input, thus showcasing more interesting emergent capabilities and demonstrating impressive performance on various vision \u2026"}, {"title": "AnyAttack: Towards Large-scale Self-supervised Generation of Targeted Adversarial Examples for Vision-Language Models", "link": "https://arxiv.org/pdf/2410.05346", "details": "J Zhang, J Ye, X Ma, Y Li, Y Yang, J Sang, DY Yeung - arXiv preprint arXiv \u2026, 2024", "abstract": "Due to their multimodal capabilities, Vision-Language Models (VLMs) have found numerous impactful applications in real-world scenarios. However, recent studies have revealed that VLMs are vulnerable to image-based adversarial attacks \u2026"}, {"title": "A Unified Hallucination Mitigation Framework for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.16494", "details": "Y Chang, L Jing, X Zhang, Y Zhang - arXiv preprint arXiv:2409.16494, 2024", "abstract": "Hallucination is a common problem for Large Vision-Language Models (LVLMs) with long generations which is difficult to eradicate. The generation with hallucinations is partially inconsistent with the image content. To mitigate hallucination, current \u2026"}, {"title": "ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context Information in Multi-Turn Multimodal Medical Dialogue", "link": "https://arxiv.org/pdf/2409.17610", "details": "Z Li, C Zou, S Ma, Z Yang, C Du, Y Tang, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rocketing prosperity of large language models (LLMs) in recent years has boosted the prevalence of vision-language models (VLMs) in the medical sector. In our online medical consultation scenario, a doctor responds to the texts and images \u2026"}, {"title": "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "link": "https://arxiv.org/pdf/2410.05077", "details": "FM Molfese, S Conia, R Orlando, R Navigli - arXiv preprint arXiv:2410.05077, 2024", "abstract": "Current Large Language Models (LLMs) have shown strong reasoning capabilities in commonsense question answering benchmarks, but the process underlying their success remains largely opaque. As a consequence, recent approaches have \u2026"}, {"title": "Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering", "link": "https://arxiv.org/pdf/2410.08085", "details": "Y Sui, B Hooi - arXiv preprint arXiv:2410.08085, 2024", "abstract": "Recent works integrating Knowledge Graphs (KGs) have led to promising improvements in enhancing reasoning accuracy of Large Language Models (LLMs). However, current benchmarks mainly focus on closed tasks, leaving a gap in the \u2026"}]
