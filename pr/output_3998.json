[{"title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models", "link": "https://arxiv.org/pdf/2406.19146", "details": "T Porian, M Wortsman, J Jitsev, L Schmidt, Y Carmon - arXiv preprint arXiv \u2026, 2024", "abstract": "Kaplan et al. and Hoffmann et al. developed influential scaling laws for the optimal model size as a function of the compute budget, but these laws yield substantially different predictions. We explain the discrepancy by reproducing the Kaplan scaling \u2026"}, {"title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2406.13233", "details": "Z Zeng, Y Miao, H Gao, H Zhang, Z Deng - arXiv preprint arXiv:2406.13233, 2024", "abstract": "Mixture of experts (MoE) has become the standard for constructing production-level large language models (LLMs) due to its promise to boost model capacity without causing significant overheads. Nevertheless, existing MoE methods usually enforce \u2026"}, {"title": "Semi-supervised Double Deep Learning Temporal Risk Prediction (SeDDLeR) with Electronic Health Records", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001035", "details": "IE Nogues, J Wen, Y Zhao, CL Bonzel, VM Castro\u2026 - Journal of Biomedical \u2026, 2024", "abstract": "Background: Risk prediction plays a crucial role in planning for prevention, monitoring, and treatment. Electronic Health Records (EHRs) offer an expansive repository of temporal medical data encompassing both risk factors and outcome \u2026"}, {"title": "BadCLM: Backdoor Attack in Clinical Language Models for Electronic Health Records", "link": "https://arxiv.org/pdf/2407.05213", "details": "W Lyu, Z Bi, F Wang, C Chen - arXiv preprint arXiv:2407.05213, 2024", "abstract": "The advent of clinical language models integrated into electronic health records (EHR) for clinical decision support has marked a significant advancement, leveraging the depth of clinical notes for improved decision-making. Despite their \u2026"}, {"title": "WeakAL: Combining Active Learning and Weak Supervision", "link": "https://www.academia.edu/download/79241724/paper.pdf", "details": "W Lehner", "abstract": "Supervised Learning requires a huge amount of labeled data, making efficient labeling one of the most critical components for the success of Machine Learning (ML). One well-known method to gain labeled data efficiently is Active Learning (AL) \u2026"}, {"title": "Protecting Privacy Through Approximating Optimal Parameters for Sequence Unlearning in Language Models", "link": "https://arxiv.org/pdf/2406.14091", "details": "D Lee, D Rim, M Choi, J Choo - arXiv preprint arXiv:2406.14091, 2024", "abstract": "Although language models (LMs) demonstrate exceptional capabilities on various tasks, they are potentially vulnerable to extraction attacks, which represent a significant privacy risk. To mitigate the privacy concerns of LMs, machine unlearning \u2026"}, {"title": "Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?", "link": "https://arxiv.org/pdf/2406.13121", "details": "J Lee, A Chen, Z Dai, D Dua, DS Sachan, M Boratko\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. Leveraging LCLMs' ability to natively ingest and process entire corpora of \u2026"}, {"title": "MAGNET: Improving the Multilingual Fairness of Language Models with Adaptive Gradient-Based Tokenization", "link": "https://arxiv.org/pdf/2407.08818", "details": "O Ahia, S Kumar, H Gonen, V Hoffman, T Limisiewicz\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In multilingual settings, non-Latin scripts and low-resource languages are usually disadvantaged in terms of language models' utility, efficiency, and cost. Specifically, previous studies have reported multiple modeling biases that the current tokenization \u2026"}]
