[{"title": "CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training", "link": "https://arxiv.org/pdf/2406.10670", "details": "D Brandfonbrener, H Zhang, A Kirsch, JR Schwarz\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Selecting high-quality data for pre-training is crucial in shaping the downstream task performance of language models. A major challenge lies in identifying this optimal subset, a problem generally considered intractable, thus necessitating scalable and \u2026"}, {"title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models", "link": "https://arxiv.org/pdf/2407.06460", "details": "W Shi, J Lee, Y Huang, S Malladi, J Zhao, A Holtzman\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly \u2026"}, {"title": "Evaluating Copyright Takedown Methods for Language Models", "link": "https://arxiv.org/pdf/2406.18664", "details": "B Wei, W Shi, Y Huang, NA Smith, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) derive their capabilities from extensive training on diverse data, including potentially copyrighted material. These models can memorize and generate content similar to their training data, posing potential concerns. Therefore \u2026"}, {"title": "PhraseAug: An Augmented Medical Report Generation Model with Phrasebook", "link": "https://ieeexplore.ieee.org/abstract/document/10560051/", "details": "X Mei, L Yang, D Gao, X Cai, J Han, T Liu - IEEE Transactions on Medical Imaging, 2024", "abstract": "Medical report generation is a valuable and challenging task, which automatically generates accurate and fluent diagnostic reports for medical images, reducing workload of radiologists and improving efficiency of disease diagnosis. Fine-grained \u2026"}, {"title": "A comparative study of large language model-based zero-shot inference and task-specific supervised classification of breast cancer pathology reports", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae146/7696538", "details": "M Sushil, T Zack, D Mandair, Z Zheng, A Wali, YN Yu\u2026 - Journal of the American \u2026, 2024", "abstract": "Objective Although supervised machine learning is popular for information extraction from clinical notes, creating large annotated datasets requires extensive domain expertise and is time-consuming. Meanwhile, large language models (LLMs) have \u2026"}]
