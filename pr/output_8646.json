[{"title": "MSc-SQL: Multi-Sample Critiquing Small Language Models For Text-To-SQL Translation", "link": "https://arxiv.org/pdf/2410.12916", "details": "SK Gorti, I Gofman, Z Liu, J Wu, N Vouitsis, G Yu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Text-to-SQL generation enables non-experts to interact with databases via natural language. Recent advances rely on large closed-source models like GPT-4 that present challenges in accessibility, privacy, and latency. To address these issues, we \u2026"}, {"title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines", "link": "https://arxiv.org/pdf/2410.21220", "details": "Z Zhang, Y Zhang, X Ding, X Yue - arXiv preprint arXiv:2410.21220, 2024", "abstract": "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This \u2026"}, {"title": "MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.17637", "details": "Z Liu, Y Zang, X Dong, P Zhang, Y Cao, H Duan, C He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs. This is typically achieved by using labeled datasets of chosen/rejected pairs and employing \u2026"}, {"title": "Dynamic Strategy Planning for Efficient Question Answering with Large Language Models", "link": "https://arxiv.org/pdf/2410.23511", "details": "T Parekh, P Prakash, A Radovic, A Shekher\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Research has shown the effectiveness of reasoning (eg, Chain-of-Thought), planning (eg, SelfAsk), and retrieval augmented generation strategies to improve the performance of Large Language Models (LLMs) on various tasks, such as question \u2026"}, {"title": "Q-VLM: Post-training Quantization for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.08119", "details": "C Wang, Z Wang, X Xu, Y Tang, J Zhou, J Lu - arXiv preprint arXiv:2410.08119, 2024", "abstract": "In this paper, we propose a post-training quantization framework of large vision- language models (LVLMs) for efficient multi-modal inference. Conventional quantization methods sequentially search the layer-wise rounding functions by \u2026"}, {"title": "LanFL: Differentially Private Federated Learning with Large Language Models using Synthetic Samples", "link": "https://arxiv.org/pdf/2410.19114", "details": "H Wu, D Klabjan - arXiv preprint arXiv:2410.19114, 2024", "abstract": "Federated Learning (FL) is a collaborative, privacy-preserving machine learning framework that enables multiple participants to train a single global model. However, the recent advent of powerful Large Language Models (LLMs) with tens to hundreds \u2026"}, {"title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ", "link": "https://arxiv.org/pdf/2410.09613", "details": "A Poulis, E Tsalapati, M Koubarakis - arXiv preprint arXiv:2410.09613, 2024", "abstract": "Recent advancements in transformer-based language models have sparked research into their logical reasoning capabilities. Most of the benchmarks used to evaluate these models are simple: generated from short (fragments of) first-order \u2026"}, {"title": "Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings", "link": "https://arxiv.org/pdf/2410.22685", "details": "YS Grewal, EV Bonilla, TD Bui - arXiv preprint arXiv:2410.22685, 2024", "abstract": "Accurately quantifying uncertainty in large language models (LLMs) is crucial for their reliable deployment, especially in high-stakes applications. Current state-of-the- art methods for measuring semantic uncertainty in LLMs rely on strict bidirectional \u2026"}, {"title": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation", "link": "https://arxiv.org/pdf/2410.08895", "details": "K Ding, Q Yu, H Zhang, G Meng, S Xiang - arXiv preprint arXiv:2410.08895, 2024", "abstract": "Cache-based approaches stand out as both effective and efficient for adapting vision- language models (VLMs). Nonetheless, the existing cache model overlooks three crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text similarity \u2026"}]
