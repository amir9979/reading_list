[{"title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "link": "https://arxiv.org/pdf/2407.21417", "details": "Z Wu, Y Zhang, P Qi, Y Xu, R Han, Y Zhang, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (ie, follow open-ended instructions) and \u2026"}, {"title": "Making Long-Context Language Models Better Multi-Hop Reasoners", "link": "https://arxiv.org/pdf/2408.03246", "details": "Y Li, S Liang, MR Lyu, L Wang - arXiv preprint arXiv:2408.03246, 2024", "abstract": "Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased \u2026"}, {"title": "Effective prompt extraction from language models", "link": "https://openreview.net/forum%3Fid%3D0o95CVdNuz", "details": "Y Zhang, N Carlini, D Ippolito - First Conference on Language Modeling, 2024", "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden \u2026"}, {"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "link": "https://arxiv.org/pdf/2408.00724", "details": "Y Wu, Z Sun, S Li, S Welleck, Y Yang - arXiv preprint arXiv:2408.00724, 2024", "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth \u2026"}, {"title": "Causal estimators for incorporating external controls in randomized trials with longitudinal outcomes", "link": "https://academic.oup.com/jrsssa/advance-article/doi/10.1093/jrsssa/qnae075/7742118", "details": "X Zhou, J Zhu, C Drake, H Pang - Journal of the Royal Statistical Society Series A \u2026, 2024", "abstract": "Incorporating external data, such as external controls, holds the promise of improving the efficiency of traditional randomized controlled trials especially when treating rare diseases or diseases with unmet needs. To this end, we propose novel weighting \u2026"}, {"title": "Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement", "link": "https://arxiv.org/pdf/2408.03092", "details": "L Yu, B Yu, H Yu, F Huang, Y Li - arXiv preprint arXiv:2408.03092, 2024", "abstract": "Merging Large Language Models (LLMs) aims to amalgamate multiple homologous LLMs into one with all the capabilities. Ideally, any LLMs sharing the same backbone should be mergeable, irrespective of whether they are Fine-Tuned (FT) with minor \u2026"}, {"title": "Reasoning Beyond Bias: A Study on Counterfactual Prompting and Chain of Thought Reasoning", "link": "https://arxiv.org/pdf/2408.08651", "details": "K Moore, J Roberts, T Pham, D Fisher - arXiv preprint arXiv:2408.08651, 2024", "abstract": "Language models are known to absorb biases from their training data, leading to predictions driven by statistical regularities rather than semantic relevance. We investigate the impact of these biases on answer choice preferences in the Massive \u2026"}, {"title": "Reasoning and Planning with Large Language Models in Code Development", "link": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671452", "details": "H Ding, Z Fan, I Guehring, G Gupta, W Ha, J Huan\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code \u2026"}, {"title": "LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin", "link": "https://aclanthology.org/2024.acl-long.106.pdf", "details": "S Dou, E Zhou, Y Liu, S Gao, W Shen, L Xiong, Y Zhou\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Substantially increasing instruction data is a direct solution to \u2026"}]
