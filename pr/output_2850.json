[{"title": "An exploratory study of self-supervised pre-training on partially supervised multi-label classification on chest X-ray images", "link": "https://www.sciencedirect.com/science/article/pii/S156849462400629X", "details": "N Dong, M Kampffmeyer, H Su, E Xing - Applied Soft Computing, 2024", "abstract": "This paper serves as the first empirical study on self-supervised pre-training on partially supervised learning, an emerging yet unexplored learning paradigm with missing annotations. This is particularly important in the medical imaging domain \u2026"}, {"title": "Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models", "link": "https://arxiv.org/pdf/2406.04274", "details": "X Ji, S Kulkarni, M Wang, T Xie - arXiv preprint arXiv:2406.04274, 2024", "abstract": "This work studies the challenge of aligning large language models (LLMs) with offline preference data. We focus on alignment by Reinforcement Learning from Human Feedback (RLHF) in particular. While popular preference optimization \u2026"}, {"title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment", "link": "https://arxiv.org/pdf/2405.19332", "details": "S Zhang, D Yu, H Sharma, Z Yang, S Wang, H Hassan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed \u2026"}, {"title": "Amend to Alignment: Decoupled Prompt Tuning for Mitigating Spurious Correlation in Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3Df8G2KSCSdp", "details": "J Zhang, X Ma, S Guo, P Li, W Xu, X Tang, Z Hong - Forty-first International Conference on \u2026", "abstract": "Fine-tuning the learnable prompt for a pre-trained vision-language model (VLM), such as CLIP, has demonstrated exceptional efficiency in adapting to a broad range of downstream tasks. Existing prompt tuning methods for VLMs do not distinguish \u2026"}, {"title": "DiffPoGAN: Diffusion Policies with Generative Adversarial Networks for Offline Reinforcement Learning", "link": "https://arxiv.org/pdf/2406.09089", "details": "X Hu, S Li, Y Xu, B Tang, L Chen - arXiv preprint arXiv:2406.09089, 2024", "abstract": "Offline reinforcement learning (RL) can learn optimal policies from pre-collected offline datasets without interacting with the environment, but the sampled actions of the agent cannot often cover the action distribution under a given state, resulting in \u2026"}, {"title": "Few-Shot Learning for Medical Image Segmentation Using 3D U-Net and Model-Agnostic Meta-Learning (MAML)", "link": "https://www.mdpi.com/2075-4418/14/12/1213", "details": "AM Alsaleh, E Albalawi, A Algosaibi, SS Albakheet\u2026 - Diagnostics, 2024", "abstract": "Deep learning has attained state-of-the-art results in general image segmentation problems; however, it requires a substantial number of annotated images to achieve the desired outcomes. In the medical field, the availability of annotated images is \u2026"}, {"title": "Joint Spatial-Temporal Modeling and Contrastive Learning for Self-supervised Heart Rate Measurement", "link": "https://arxiv.org/pdf/2406.04942", "details": "W Qian, Q Li, K Li, X Wang, X Sun, M Wang, D Guo - arXiv preprint arXiv:2406.04942, 2024", "abstract": "This paper briefly introduces the solutions developed by our team, HFUT-VUT, for Track 1 of self-supervised heart rate measurement in the 3rd Vision-based Remote Physiological Signal Sensing (RePSS) Challenge hosted at IJCAI 2024. The goal is \u2026"}, {"title": "Exploring Adversarial Robustness of Deep State Space Models", "link": "https://arxiv.org/pdf/2406.05532", "details": "B Qi, Y Luo, J Gao, P Li, K Tian, Z Ma, B Zhou - arXiv preprint arXiv:2406.05532, 2024", "abstract": "Deep State Space Models (SSMs) have proven effective in numerous task scenarios but face significant security challenges due to Adversarial Perturbations (APs) in real- world deployments. Adversarial Training (AT) is a mainstream approach to \u2026"}, {"title": "Probabilistic Perspectives on Error Minimization in Adversarial Reinforcement Learning", "link": "https://arxiv.org/pdf/2406.04724", "details": "R Belaire, A Sinha, P Varakantham - arXiv preprint arXiv:2406.04724, 2024", "abstract": "Deep Reinforcement Learning (DRL) policies are critically vulnerable to adversarial noise in observations, posing severe risks in safety-critical scenarios. For example, a self-driving car receiving manipulated sensory inputs about traffic signs could lead to \u2026"}]
