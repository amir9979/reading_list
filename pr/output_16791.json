[{"title": "Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools", "link": "https://arxiv.org/pdf/2505.16113", "details": "P Lymperopoulos, V Sarathy - arXiv preprint arXiv:2505.16113, 2025", "abstract": "\u2026 As **large** **language** **models** (LLMs) have been increasingly deployed in practical applications, their problem-solving capacity has enabled \u2026 we evaluate our framework in quantifying the uncertainty of QA tool-calling LLM systems using the \u2026", "entry_id": "http://arxiv.org/abs/2505.16113v1", "updated": "2025-05-22 01:34:23", "published": "2025-05-22 01:34:23", "authors": "Panagiotis Lymperopoulos;Vasanth Sarathy", "summary": "Modern Large Language Models (LLMs) often require external tools, such as\nmachine learning classifiers or knowledge retrieval systems, to provide\naccurate answers in domains where their pre-trained knowledge is insufficient.\nThis integration of LLMs with external tools expands their utility but also\nintroduces a critical challenge: determining the trustworthiness of responses\ngenerated by the combined system. In high-stakes applications, such as medical\ndecision-making, it is essential to assess the uncertainty of both the LLM's\ngenerated text and the tool's output to ensure the reliability of the final\nresponse. However, existing uncertainty quantification methods do not account\nfor the tool-calling scenario, where both the LLM and external tool contribute\nto the overall system's uncertainty. In this work, we present a novel framework\nfor modeling tool-calling LLMs that quantifies uncertainty by jointly\nconsidering the predictive uncertainty of the LLM and the external tool. We\nextend previous methods for uncertainty quantification over token sequences to\nthis setting and propose efficient approximations that make uncertainty\ncomputation practical for real-world applications. We evaluate our framework on\ntwo new synthetic QA datasets, derived from well-known machine learning\ndatasets, which require tool-calling for accurate answers. Additionally, we\napply our method to retrieval-augmented generation (RAG) systems and conduct a\nproof-of-concept experiment demonstrating the effectiveness of our uncertainty\nmetrics in scenarios where external information retrieval is needed. Our\nresults show that the framework is effective in enhancing trust in LLM-based\nsystems, especially in cases where the LLM's internal knowledge is insufficient\nand external tools are required.", "comment": "10 pages 3 figures 3 tables", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CL", "links": "http://arxiv.org/abs/2505.16113v1;http://arxiv.org/pdf/2505.16113v1", "pdf_url": "http://arxiv.org/pdf/2505.16113v1"}]
