[{"title": "Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models", "link": "https://arxiv.org/pdf/2505.17496", "details": "CY Hsiao, KH Lu, KW Chang, CK Yang, WC Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "End-to-end training of Spoken Language Models (SLMs) commonly involves adapting pre-trained text-based Large Language Models (LLMs) to the speech modality through multi-stage training on diverse tasks such as ASR, TTS and spoken \u2026", "entry_id": "http://arxiv.org/abs/2505.17496v1", "updated": "2025-05-23 05:50:14", "published": "2025-05-23 05:50:14", "authors": "Chi-Yuan Hsiao;Ke-Han Lu;Kai-Wei Chang;Chih-Kai Yang;Wei-Chih Chen;Hung-yi Lee", "summary": "End-to-end training of Spoken Language Models (SLMs) commonly involves\nadapting pre-trained text-based Large Language Models (LLMs) to the speech\nmodality through multi-stage training on diverse tasks such as ASR, TTS and\nspoken question answering (SQA). Although this multi-stage continual learning\nequips LLMs with both speech understanding and generation capabilities, the\nsubstantial differences in task and data distributions across stages can lead\nto catastrophic forgetting, where previously acquired knowledge is lost. This\npaper investigates catastrophic forgetting and evaluates three mitigation\nstrategies-model merging, discounting the LoRA scaling factor, and experience\nreplay to balance knowledge retention with new learning. Results show that\nexperience replay is the most effective, with further gains achieved by\ncombining it with other methods. These findings provide insights for developing\nmore robust and efficient SLM training pipelines.", "comment": "Accepted to Interspeech 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG;cs.SD;eess.AS", "links": "http://arxiv.org/abs/2505.17496v1;http://arxiv.org/pdf/2505.17496v1", "pdf_url": "http://arxiv.org/pdf/2505.17496v1"}, {"title": "Learning to Focus: Context Extraction for Efficient Code Vulnerability Detection with Language Models", "link": "https://arxiv.org/pdf/2505.17460", "details": "X Zheng, X Qian, H Zhou, S Yang, Y He, S Jana\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Language models (LMs) show promise for vulnerability detection but struggle with long, real-world code due to sparse and uncertain vulnerability locations. These issues, exacerbated by token limits, often cause models to miss vulnerability-related \u2026", "entry_id": "http://arxiv.org/abs/2505.17460v1", "updated": "2025-05-23 04:41:54", "published": "2025-05-23 04:41:54", "authors": "Xinran Zheng;Xingzhi Qian;Huichi Zhou;Shuo Yang;Yiling He;Suman Jana;Lorenzo Cavallaro", "summary": "Language models (LMs) show promise for vulnerability detection but struggle\nwith long, real-world code due to sparse and uncertain vulnerability locations.\nThese issues, exacerbated by token limits, often cause models to miss\nvulnerability-related signals, thereby impairing effective learning. A key\nintuition is to enhance LMs with concise, information-rich context.\nCommit-based annotations offer precise, CWE-agnostic supervision, but are\nunavailable during inference, as they depend on historical code changes.\nMoreover, their extreme sparsity, often covering only a few lines, makes it\ndifficult for LMs to process directly. In this paper, we propose FocusVul, a\nmodel-agnostic framework that improves LM-based vulnerability detection by\nlearning to select sensitive context. FocusVul learns commit-based annotation\npatterns through hierarchical semantic modeling and generalizes them to\nidentify line-level vulnerability-relevant regions during inference. It then\nextracts LM-oriented context via both dependency and execution flows\nsurrounding selected regions, yielding semantically rich inputs for effective\nvulnerability detection. Experiments on real-world benchmarks show that\nFocusVul consistently outperforms heuristic-based and full-function fine-tuning\napproaches, improving classification performance by 164.04% and reducing FLOPs\nby 19.12% on average.", "comment": null, "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE", "links": "http://arxiv.org/abs/2505.17460v1;http://arxiv.org/pdf/2505.17460v1", "pdf_url": "http://arxiv.org/pdf/2505.17460v1"}, {"title": "Compression Hacking: A Supplementary Perspective on Informatics Metric of Language Models from Geometric Distortion", "link": "https://arxiv.org/pdf/2505.17793", "details": "J Zang, M Ning, Y Wei, S Dou, J Zhang, N Mo, B Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, the concept of``compression as intelligence''has provided a novel informatics metric perspective for language models (LMs), emphasizing that highly structured representations signify the intelligence level of LMs. However, from a \u2026", "entry_id": "http://arxiv.org/abs/2505.17793v1", "updated": "2025-05-23 12:11:03", "published": "2025-05-23 12:11:03", "authors": "Jianxiang Zang;Meiling Ning;Yongda Wei;Shihan Dou;Jiazheng Zhang;Nijia Mo;Binhong Li;Tao Gui;Qi Zhang;Xuanjing Huang", "summary": "Recently, the concept of ``compression as intelligence'' has provided a novel\ninformatics metric perspective for language models (LMs), emphasizing that\nhighly structured representations signify the intelligence level of LMs.\nHowever, from a geometric standpoint, the word representation space of highly\ncompressed LMs tends to degenerate into a highly anisotropic state, which\nhinders the LM's ability to comprehend instructions and directly impacts its\nperformance. We found this compression-anisotropy synchronicity is essentially\nthe ``Compression Hacking'' in LM representations, where noise-dominated\ndirections tend to create the illusion of high compression rates by sacrificing\nspatial uniformity. Based on this, we propose three refined compression metrics\nby incorporating geometric distortion analysis and integrate them into a\nself-evaluation pipeline. The refined metrics exhibit strong alignment with the\nLM's comprehensive capabilities, achieving Spearman correlation coefficients\nabove 0.9, significantly outperforming both the original compression and other\ninternal structure-based metrics. This confirms that compression hacking\nsubstantially enhances the informatics interpretation of LMs by incorporating\ngeometric distortion of representations.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.17793v1;http://arxiv.org/pdf/2505.17793v1", "pdf_url": "http://arxiv.org/pdf/2505.17793v1"}, {"title": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models", "link": "https://arxiv.org/pdf/2505.15634", "details": "Z Li, X Wang, Y Yang, Z Yao, H Xiong, M Du - arXiv preprint arXiv:2505.15634, 2025", "abstract": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT length, as seen in models such as DeepSeek-R1, significantly enhances this \u2026", "entry_id": "http://arxiv.org/abs/2505.15634v2", "updated": "2025-05-24 15:20:30", "published": "2025-05-21 15:17:59", "authors": "Zihao Li;Xu Wang;Yuzhe Yang;Ziyu Yao;Haoyi Xiong;Mengnan Du", "summary": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and\nmathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT\nlength, as seen in models such as DeepSeek-R1, significantly enhances this\nreasoning for complex problems, but requires costly and high-quality long CoT\ndata and fine-tuning. This work, inspired by the deep thinking paradigm of\nDeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of\nan LLM without external datasets. Our method first employs Sparse Autoencoders\n(SAEs) to extract interpretable features from vanilla CoT. These features are\nthen used to steer the LLM's internal states during generation. Recognizing\nthat many LLMs do not have corresponding pre-trained SAEs, we further introduce\na novel SAE-free steering algorithm, which directly computes steering\ndirections from the residual activations of an LLM, obviating the need for an\nexplicit SAE. Experimental results demonstrate that both our SAE-based and\nsubsequent SAE-free steering algorithms significantly enhance the reasoning\ncapabilities of LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.15634v2;http://arxiv.org/pdf/2505.15634v2", "pdf_url": "http://arxiv.org/pdf/2505.15634v2"}, {"title": "Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization", "link": "https://arxiv.org/pdf/2505.15660", "details": "J Zhou, K Ye, J Liu, T Ma, Z Wang, R Qiu, KY Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The generalization capabilities of vision-language-action (VLA) models to unseen tasks are crucial to achieving general-purpose robotic manipulation in open-world settings. However, the cross-task generalization capabilities of existing VLA models \u2026", "entry_id": "http://arxiv.org/abs/2505.15660v2", "updated": "2025-05-24 15:33:43", "published": "2025-05-21 15:35:57", "authors": "Jiaming Zhou;Ke Ye;Jiayi Liu;Teli Ma;Zifan Wang;Ronghe Qiu;Kun-Yu Lin;Zhilin Zhao;Junwei Liang", "summary": "The generalization capabilities of vision-language-action (VLA) models to\nunseen tasks are crucial to achieving general-purpose robotic manipulation in\nopen-world settings. However, the cross-task generalization capabilities of\nexisting VLA models remain significantly underexplored. To address this gap, we\nintroduce AGNOSTOS, a novel simulation benchmark designed to rigorously\nevaluate cross-task zero-shot generalization in manipulation. AGNOSTOS\ncomprises 23 unseen manipulation tasks for testing, distinct from common\ntraining task distributions, and incorporates two levels of generalization\ndifficulty to assess robustness. Our systematic evaluation reveals that current\nVLA models, despite being trained on diverse datasets, struggle to generalize\neffectively to these unseen tasks. To overcome this limitation, we propose\nCross-Task In-Context Manipulation (X-ICM), a method that conditions large\nlanguage models (LLMs) on in-context demonstrations from seen tasks to predict\naction sequences for unseen tasks. Additionally, we introduce a dynamics-guided\nsample selection strategy that identifies relevant demonstrations by capturing\ncross-task dynamics. On AGNOSTOS, X-ICM significantly improves cross-task\nzero-shot generalization performance over leading VLAs. We believe AGNOSTOS and\nX-ICM will serve as valuable tools for advancing general-purpose robotic\nmanipulation.", "comment": "Project Page: https://jiaming-zhou.github.io/AGNOSTOS", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.CV", "links": "http://arxiv.org/abs/2505.15660v2;http://arxiv.org/pdf/2505.15660v2", "pdf_url": "http://arxiv.org/pdf/2505.15660v2"}, {"title": "Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack", "link": "https://arxiv.org/pdf/2505.15323", "details": "S Cappelletti, T Poppi, S Poppi, ZX Yong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) are increasingly evaluated on multiple-choice question answering (MCQA) tasks using* first-token probability*(FTP), which selects the answer option whose initial token has the highest likelihood. While efficient, FTP \u2026", "entry_id": "http://arxiv.org/abs/2505.15323v1", "updated": "2025-05-21 09:58:38", "published": "2025-05-21 09:58:38", "authors": "Silvia Cappelletti;Tobia Poppi;Samuele Poppi;Zheng-Xin Yong;Diego Garcia-Olano;Marcella Cornia;Lorenzo Baraldi;Rita Cucchiara", "summary": "Large Language Models (LLMs) are increasingly evaluated on multiple-choice\nquestion answering (MCQA) tasks using *first-token probability* (FTP), which\nselects the answer option whose initial token has the highest likelihood. While\nefficient, FTP can be fragile: models may assign high probability to unrelated\ntokens (*misalignment*) or use a valid token merely as part of a generic\npreamble rather than as a clear answer choice (*misinterpretation*),\nundermining the reliability of symbolic evaluation. We propose a simple\nsolution: the *prefilling attack*, a structured natural-language prefix (e.g.,\n\"*The correct option is:*\") prepended to the model output. Originally explored\nin AI safety, we repurpose prefilling to steer the model to respond with a\nclean, valid option, without modifying its parameters. Empirically, the FTP\nwith prefilling strategy substantially improves accuracy, calibration, and\noutput consistency across a broad set of LLMs and MCQA benchmarks. It\noutperforms standard FTP and often matches the performance of open-ended\ngeneration approaches that require full decoding and external classifiers,\nwhile being significantly more efficient. Our findings suggest that prefilling\nis a simple, robust, and low-cost method to enhance the reliability of\nFTP-based evaluation in multiple-choice settings.", "comment": "13 pages, 5 figures, 7 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15323v1;http://arxiv.org/pdf/2505.15323v1", "pdf_url": "http://arxiv.org/pdf/2505.15323v1"}, {"title": "Self-Training Large Language Models with Confident Reasoning", "link": "https://arxiv.org/pdf/2505.17454", "details": "H Jang, Y Jang, S Lee, J Ok, S Ahn - arXiv preprint arXiv:2505.17454, 2025", "abstract": "Large language models (LLMs) have shown impressive performance by generating reasoning paths before final answers, but learning such a reasoning path requires costly human supervision. To address this issue, recent studies have explored self \u2026", "entry_id": "http://arxiv.org/abs/2505.17454v1", "updated": "2025-05-23 04:25:10", "published": "2025-05-23 04:25:10", "authors": "Hyosoon Jang;Yunhui Jang;Sungjae Lee;Jungseul Ok;Sungsoo Ahn", "summary": "Large language models (LLMs) have shown impressive performance by generating\nreasoning paths before final answers, but learning such a reasoning path\nrequires costly human supervision. To address this issue, recent studies have\nexplored self-training methods that improve reasoning capabilities using\npseudo-labels generated by the LLMs themselves. Among these, confidence-based\nself-training fine-tunes LLMs to prefer reasoning paths with high-confidence\nanswers, where confidence is estimated via majority voting. However, such\nmethods exclusively focus on the quality of the final answer and may ignore the\nquality of the reasoning paths, as even an incorrect reasoning path leads to a\ncorrect answer by chance. Instead, we advocate the use of reasoning-level\nconfidence to identify high-quality reasoning paths for self-training,\nsupported by our empirical observations. We then propose a new self-training\nmethod, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths\nthrough Policy Optimization. Our experiments show that CORE-PO improves the\naccuracy of outputs on four in-distribution and two out-of-distribution\nbenchmarks, compared to existing self-training methods.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CL", "links": "http://arxiv.org/abs/2505.17454v1;http://arxiv.org/pdf/2505.17454v1", "pdf_url": "http://arxiv.org/pdf/2505.17454v1"}, {"title": "First Finish Search: Efficient Test-Time Scaling in Large Language Models", "link": "https://arxiv.org/pdf/2505.18149", "details": "A Agarwal, A Sengupta, T Chakraborty - arXiv preprint arXiv:2505.18149, 2025", "abstract": "Test-time scaling (TTS), which involves dynamic allocation of compute during inference, offers a promising way to improve reasoning in large language models. While existing TTS methods work well, they often rely on long decoding paths or \u2026", "entry_id": "http://arxiv.org/abs/2505.18149v1", "updated": "2025-05-23 17:57:43", "published": "2025-05-23 17:57:43", "authors": "Aradhye Agarwal;Ayan Sengupta;Tanmoy Chakraborty", "summary": "Test-time scaling (TTS), which involves dynamic allocation of compute during\ninference, offers a promising way to improve reasoning in large language\nmodels. While existing TTS methods work well, they often rely on long decoding\npaths or require a large number of samples to be generated, increasing the\ntoken usage and inference latency. We observe the surprising fact that for\nreasoning tasks, shorter traces are much more likely to be correct than longer\nones. Motivated by this, we introduce First Finish Search (FFS), a\ntraining-free parallel decoding strategy that launches $n$ independent samples\nand returns as soon as any one completes. We evaluate FFS alongside simple\ndecoding, beam search, majority voting, and budget forcing on four reasoning\nmodels (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and\nacross four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With\nDeepSeek-R1, FFS achieves $82.23\\%$ accuracy on the AIME datasets, a $15\\%$\nimprovement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's\no4-mini performance. Our theoretical analysis explains why stopping at the\nshortest trace is likely to yield a correct answer and identifies the\nconditions under which early stopping may be suboptimal. The elegance and\nsimplicity of FFS demonstrate that straightforward TTS strategies can perform\nremarkably well, revealing the untapped potential of simple approaches at\ninference time.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.18149v1;http://arxiv.org/pdf/2505.18149v1", "pdf_url": "http://arxiv.org/pdf/2505.18149v1"}, {"title": "Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought", "link": "https://arxiv.org/pdf/2505.15431", "details": "A Liu, B Zhou, C Xu, C Zhou, CC Zhang, C Xu, C Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS, a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It synergistically combines Mamba's long-sequence processing efficiency with \u2026", "entry_id": "http://arxiv.org/abs/2505.15431v2", "updated": "2025-05-22 06:44:25", "published": "2025-05-21 12:11:53", "authors": "Tencent Hunyuan Team;Ao Liu;Botong Zhou;Can Xu;Chayse Zhou;ChenChen Zhang;Chengcheng Xu;Chenhao Wang;Decheng Wu;Dengpeng Wu;Dian Jiao;Dong Du;Dong Wang;Feng Zhang;Fengzong Lian;Guanghui Xu;Guanwei Zhang;Hai Wang;Haipeng Luo;Han Hu;Huilin Xu;Jiajia Wu;Jianchen Zhu;Jianfeng Yan;Jiaqi Zhu;Jihong Zhang;Jinbao Xue;Jun Xia;Junqiang Zheng;Kai Liu;Kai Zhang;Kai Zheng;Kejiao Li;Keyao Wang;Lan Jiang;Lixin Liu;Lulu Wu;Mengyuan Huang;Peijie Yu;Peiqi Wang;Qian Wang;Qianbiao Xiang;Qibin Liu;Qingfeng Sun;Richard Guo;Ruobing Xie;Saiyong Yang;Shaohua Chen;Shihui Hu;Shuai Li;Shuaipeng Li;Shuang Chen;Suncong Zheng;Tao Yang;Tian Zhang;Tinghao Yu;Weidong Han;Weijie Liu;Weijin Zhou;Weikang Wang;Wesleye Chen;Xiao Feng;Xiaoqin Ren;Xingwu Sun;Xiong Kuang;Xuemeng Huang;Xun Cao;Yanfeng Chen;Yang Du;Yang Zhen;Yangyu Tao;Yaping Deng;Yi Shen;Yigeng Hong;Yiqi Chen;Yiqing Huang;Yuchi Deng;Yue Mao;Yulong Wang;Yuyuan Zeng;Zenan Xu;Zhanhui Kang;Zhe Zhao;ZhenXiang Yan;Zheng Fang;Zhichao Hu;Zhongzhi Chen;Zhuoyu Li;Zongwei Li;Alex Yan;Ande Liang;Baitong Liu;Beiping Pan;Bin Xing;Binghong Wu;Bingxin Qu;Bolin Ni;Boyu Wu;Chen Li;Cheng Jiang;Cheng Zhang;Chengjun Liu;Chengxu Yang;Chengzhong Xu;Chiyu Wang;Chong Zha;Daisy Yi;Di Wang;Fanyang Lu;Fei Chen;Feifei Liu;Feng Zheng;Guanghua Yu;Guiyang Li;Guohua Wang;Haisheng Lin;Han Liu;Han Wang;Hao Fei;Hao Lu;Haoqing Jiang;Haoran Sun;Haotian Zhu;Huangjin Dai;Huankui Chen;Huawen Feng;Huihui Cai;Huxin Peng;Jackson Lv;Jiacheng Shi;Jiahao Bu;Jianbo Li;Jianglu Hu;Jiangtao Guan;Jianing Xu;Jianwei Cai;Jiarong Zhang;Jiawei Song;Jie Jiang;Jie Liu;Jieneng Yang;Jihong Zhang;Jin lv;Jing Zhao;Jinjian Li;Jinxing Liu;Jun Zhao;Juntao Guo;Kai Wang;Kan Wu;Lei Fu;Lei He;Lei Wang;Li Liu;Liang Dong;Liya Zhan;Long Cheng;Long Xu;Mao Zheng;Meng Liu;Mengkang Hu;Nanli Chen;Peirui Chen;Peng He;Pengju Pan;Pengzhi Wei;Qi Yang;Qi Yi;Roberts Wang;Rongpeng Chen;Rui Sun;Rui Yang;Ruibin Chen;Ruixu Zhou;Shaofeng Zhang;Sheng Zhang;Shihao Xu;Shuaishuai Chang;Shulin Liu;SiQi Wang;Songjia Feng;Songling Yuan;Tao Zhang;Tianjiao Lang;Tongkai Li;Wei Deng;Wei Li;Weichao Wang;Weigang Zhang;Weixuan Sun;Wen Ouyang;Wenxiang Jiao;Wenzhi Sun;Wenzhuo Jia;Xiang Zhang;Xiangyu He;Xianshun Ren;XiaoYing Zhu;Xiaolong Guo;Xiaoxue Li;Xiaoyu Ma;Xican Lu;Xinhua Feng;Xinting Huang;Xinyu Guan;Xirui Li;Xu Zhang;Xudong Gao;Xun Luo;Xuxiang Qi;Yangkun Chen;Yangyu Tao;Yanling Xiao;Yantao Mai;Yanze Chen;Yao Ding;Yeting Yang;YiFan Song;Yifan Yang;Yijiao Zhu;Yinhe Wu;Yixian Liu;Yong Yang;Yuanjun Cai;Yuanlin Tu;Yue Zhang;Yufei Huang;Yuhang Zhou;Yuhao Jiang;Yuhong Liu;Yuhui Hu;Yujin Lin;Yun Yang;Yunhao Wang;Yusong Zhang;Zekun Wu;Zelong Zhang;Zhan Yu;Zhaoliang Yang;Zhe Zhao;Zheng Li;Zhenyu Huang;Zhiguang Liu;Zhijiang Xu;Zhiqing Kui;Zhiyin Zeng;Zhiyuan Xiong;Zhuo Han;Zifan Wu;Zigang Geng;Zilong Zhao;Ziyan Tang;Ziyuan Zhu;Zonglei Zhu;Zhijiang Xu", "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15431v2;http://arxiv.org/pdf/2505.15431v2", "pdf_url": "http://arxiv.org/pdf/2505.15431v2"}]
