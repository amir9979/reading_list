[{"title": "Unsupervised Topic Models are Data Mixers for Pre-training Language Models", "link": "https://arxiv.org/pdf/2502.16802", "details": "J Peng, X Zhuang, Q Jiantao, R Ma, J Yu, T Bai, C He - arXiv preprint arXiv \u2026, 2025", "abstract": "The performance of large language models (LLMs) is significantly affected by the quality and composition of their pre-training data, which is inherently diverse, spanning various domains, sources, and topics. Effectively integrating these \u2026"}, {"title": "Learning with Enriched Inductive Biases for Vision-Language Models", "link": "https://ruyuanzhang.github.io/files/2501_indctbiasVisLangModel_IJCV.pdf", "details": "L Yang, RY Zhang, Q Chen, X Xie - International Journal of Computer Vision, 2025", "abstract": "Abstract Vision-Language Models, pre-trained on large-scale image-text pairs, serve as strong foundation models for transfer learning across a variety of downstream tasks. For few-shot generalization tasks, ie., when the model is trained on few-shot \u2026"}, {"title": "Large Language Models are Powerful EHR Encoders", "link": "https://arxiv.org/pdf/2502.17403", "details": "S Hegselmann, G von Arnim, T Rheude, N Kronenberg\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific EHR foundation models trained on \u2026"}, {"title": "MCG-Net: Medical Chief Complaint-guided Multi-modal Masked Content Pre-training for chest image classification", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425002829", "details": "L Zou, J Li, H Chen, M Liang, J Ke, Y Zhong, J Chen - Expert Systems with \u2026, 2025", "abstract": "Medical image classification plays a crucial role in disease diagnosis, personalized treatment, and clinical decision-making, with significant progress driven by deep learning (DL). However, DL's performance heavily relies on large-scale, accurately \u2026"}, {"title": "Enhancing Generalization in Camera Trap Image Recognition: Fine-Tuning Visual Language Models", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225004989", "details": "Z Yang, Y Tian, L Wang, J Zhang - Neurocomputing, 2025", "abstract": "This study introduces a novel fine-tuning approach for enhancing the generalization capabilities of visual language models in the context of wildlife monitoring, particularly for camera trap image recognition. In this paper, we introduce Ecological \u2026"}, {"title": "KGGen: Extracting Knowledge Graphs from Plain Text with Language Models", "link": "https://arxiv.org/pdf/2502.09956", "details": "B Mo, K Yu, J Kazdan, P Mpala, L Yu, C Cundy\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent interest in building foundation models for KGs has highlighted a fundamental challenge: knowledge-graph data is relatively scarce. The best-known KGs are primarily human-labeled, created by pattern-matching, or extracted using early NLP \u2026"}, {"title": "Few-shot Continual Relation Extraction via Open Information Extraction", "link": "https://arxiv.org/pdf/2502.16648", "details": "T Nguyen, A Nguyen, Q Tran, T Vu, D Nguyen, L Ngo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Typically, Few-shot Continual Relation Extraction (FCRE) models must balance retaining prior knowledge while adapting to new tasks with extremely limited data. However, real-world scenarios may also involve unseen or undetermined relations \u2026"}, {"title": "Cross-modal Augmented Transformer for Automated Medical Report Generation", "link": "https://ieeexplore.ieee.org/iel8/6221039/6563131/10857391.pdf", "details": "Y Tang, Y Yuan, F Tao, M Tang - IEEE Journal of Translational Engineering in Health \u2026, 2025", "abstract": "In clinical practice, interpreting medical images and composing diagnostic reports typically involve significant manual workload. Therefore, an automated report generation framework that mimics a doctor's diagnosis better meets the requirements \u2026"}, {"title": "InsightVision: A Comprehensive, Multi-Level Chinese-based Benchmark for Evaluating Implicit Visual Semantics in Large Vision Language Models", "link": "https://arxiv.org/pdf/2502.15812", "details": "X Yin, Y Hong, Y Guo, Y Tu, W Wang, G Liu - arXiv preprint arXiv:2502.15812, 2025", "abstract": "In the evolving landscape of multimodal language models, understanding the nuanced meanings conveyed through visual cues-such as satire, insult, or critique- remains a significant challenge. Existing evaluation benchmarks primarily focus on \u2026"}]
