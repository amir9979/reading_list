[{"title": "Replicability in High Dimensional Statistics", "link": "https://arxiv.org/pdf/2406.02628", "details": "M Hopkins, R Impagliazzo, D Kane, S Liu, C Ye - arXiv preprint arXiv:2406.02628, 2024", "abstract": "The replicability crisis is a major issue across nearly all areas of empirical science, calling for the formal study of replicability in statistics. Motivated in this context,[Impagliazzo, Lei, Pitassi, and Sorrell STOC 2022] introduced the notion of \u2026"}, {"title": "Frequency Enhanced Pre-training for Cross-city Few-shot Traffic Forecasting", "link": "https://arxiv.org/pdf/2406.02614", "details": "Z Liu, J Ding, G Zheng - arXiv preprint arXiv:2406.02614, 2024", "abstract": "The field of Intelligent Transportation Systems (ITS) relies on accurate traffic forecasting to enable various downstream applications. However, developing cities often face challenges in collecting sufficient training traffic data due to limited \u2026"}, {"title": "Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding", "link": "https://arxiv.org/pdf/2406.02472", "details": "Z Zhang, Y Cao, C Ye, Y Ma, L Liao, TS Chua - arXiv preprint arXiv:2406.02472, 2024", "abstract": "The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events. We refer to the complex events composed of many news articles over an extended \u2026"}, {"title": "EFTNAS: Searching for Efficient Language Models in First-Order Weight-Reordered Super-Networks", "link": "https://aclanthology.org/2024.lrec-main.497.pdf", "details": "JP Munoz, Y Zheng, N Jain - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Transformer-based models have demonstrated outstanding performance in natural language processing (NLP) tasks and many other domains, eg, computer vision. Depending on the size of these models, which have grown exponentially in the past \u2026"}, {"title": "TAIA: Large Language Models are Out-of-Distribution Data Learners", "link": "https://arxiv.org/pdf/2405.20192", "details": "S Jiang, Y Liao, Y Zhang, Y Wang, Y Wang - arXiv preprint arXiv:2405.20192, 2024", "abstract": "Fine-tuning on task-specific question-answer pairs is a predominant method for enhancing the performance of instruction-tuned large language models (LLMs) on downstream tasks. However, in certain specialized domains, such as healthcare or \u2026"}, {"title": "Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models", "link": "https://arxiv.org/pdf/2405.17044", "details": "X Gu, M Krenn - arXiv preprint arXiv:2405.17044, 2024", "abstract": "Advanced artificial intelligence (AI) systems with access to millions of research papers could inspire new research ideas that may not be conceived by humans alone. However, how interesting are these AI-generated ideas, and how can we \u2026"}, {"title": "Few-shot biomedical relation extraction using data augmentation and domain information", "link": "https://www.sciencedirect.com/science/article/pii/S0925231224006520", "details": "B Guo, D Zhao, X Dong, J Meng, H Lin - Neurocomputing, 2024", "abstract": "Relation extraction (RE) plays a pivotal role in biomedical information extraction. However, traditional approaches are often limited by high data annotation costs and extensive time investments. To address this challenge, this study proposes an \u2026"}, {"title": "Demonstration Augmentation for Zero-shot In-context Learning", "link": "https://arxiv.org/pdf/2406.01224", "details": "Y Su, Y Tai, Y Ji, J Li, B Yan, M Zhang - arXiv preprint arXiv:2406.01224, 2024", "abstract": "Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates. However, many studies \u2026"}]
