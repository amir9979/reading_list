[{"title": "Symptom-BERT: Enhancing Cancer Symptom Detection in EHR Clinical Notes", "link": "https://www.sciencedirect.com/science/article/pii/S088539242400784X", "details": "N Zeinali, A Albashayreh, W Fan, SG White - Journal of Pain and Symptom \u2026, 2024", "abstract": "Context Extracting cancer symptom documentation allows clinicians to develop highly individualized symptom prediction algorithms to deliver symptom management care. Leveraging advanced language models to detect symptom data \u2026"}, {"title": "On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?", "link": "https://arxiv.org/pdf/2405.02266", "details": "M Zanella, IB Ayed - arXiv preprint arXiv:2405.02266, 2024", "abstract": "The development of large vision-language models, notably CLIP, has catalyzed research into effective adaptation techniques, with a particular focus on soft prompt tuning. Conjointly, test-time augmentation, which utilizes multiple augmented views \u2026"}, {"title": "Meta In-Context Learning Makes Large Language Models Better Zero and Few-Shot Relation Extractors", "link": "https://arxiv.org/pdf/2404.17807", "details": "G Li, P Wang, J Liu, Y Guo, K Ji, Z Shang, Z Xu - arXiv preprint arXiv:2404.17807, 2024", "abstract": "Relation extraction (RE) is an important task that aims to identify the relationships between entities in texts. While large language models (LLMs) have revealed remarkable in-context learning (ICL) capability for general zero and few-shot \u2026"}, {"title": "Prompting-based Synthetic Data Generation for Few-Shot Question Answering", "link": "https://arxiv.org/pdf/2405.09335", "details": "M Schmidt, A Bartezzaghi, NT Vu - arXiv preprint arXiv:2405.09335, 2024", "abstract": "Although language models (LMs) have boosted the performance of Question Answering, they still need plenty of data. Data annotation, in contrast, is a time- consuming process. This especially applies to Question Answering, where possibly \u2026"}, {"title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers", "link": "https://arxiv.org/pdf/2404.18443", "details": "R Xu, W Shi, Y Yu, Y Zhuang, Y Zhu, MD Wang, JC Ho\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Developing effective biomedical retrieval models is important for excelling at knowledge-intensive biomedical tasks but still challenging due to the deficiency of sufficient publicly annotated biomedical data and computational resources. We \u2026"}, {"title": "Empowering Large Language Models for Textual Data Augmentation", "link": "https://arxiv.org/pdf/2404.17642", "details": "Y Li, K Ding, J Wang, K Lee - arXiv preprint arXiv:2404.17642, 2024", "abstract": "With the capabilities of understanding and executing natural language instructions, Large language models (LLMs) can potentially act as a powerful tool for textual data augmentation. However, the quality of augmented data depends heavily on the \u2026"}, {"title": "IAD: In-Context Learning Ability Decoupler of Large Language Models in Meta-Training", "link": "https://aclanthology.org/2024.lrec-main.749.pdf", "details": "Y Liu, X Chen, G Xing, J Zhang, R Yan - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) exhibit remarkable In-Context Learning (ICL) ability, where the model learns tasks from prompts consisting of input-output examples. However, the pre-training objectives of LLMs often misalign with ICL \u2026"}, {"title": "Benchmarking Benchmark Leakage in Large Language Models", "link": "https://arxiv.org/pdf/2404.18824%3Ftrk%3Dpublic_post_comment-text", "details": "R Xu, Z Wang, RZ Fan, P Liu - arXiv preprint arXiv:2404.18824, 2024", "abstract": "Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary \u2026"}]
