[{"title": "MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization", "link": "https://arxiv.org/pdf/2412.06141", "details": "K Zhu, P Xia, Y Li, H Zhu, S Wang, H Yao - arXiv preprint arXiv:2412.06141, 2024", "abstract": "The advancement of Large Vision-Language Models (LVLMs) has propelled their application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter factuality challenges due to modality misalignment, where the models prioritize \u2026"}, {"title": "Enhancing vision-language models for medical imaging: bridging the 3D gap with innovative slice selection", "link": "https://openreview.net/pdf%3Fid%3DJrJW21IP9p", "details": "Y Wang, Y Dai, C Jones, HI Sair, J Shen, N Loizou\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Recent approaches to vision-language tasks are built on the remarkable capabilities of large vision-language models (VLMs). These models excel in zero-shot and few- shot learning, enabling them to learn new tasks without parameter updates \u2026"}, {"title": "Domain knowledge boosted adaptation: Leveraging vision-language models for multi-source domain adaptation", "link": "https://www.sciencedirect.com/science/article/pii/S092523122401885X", "details": "Y He, J Feng, G Ding, Y Guo, T He - Neurocomputing, 2024", "abstract": "Multi-source domain adaptation (MSDA) aims to adapt a model trained on multiple labeled source domains to an unlabeled target domain. Existing MSDA methods primarily focus on reducing domain gaps by aligning the source domains with the \u2026"}, {"title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models", "link": "https://openreview.net/pdf%3Fid%3DLC1QAqhePv", "details": "D Zhang, Z Hu, S Zhoubian, Z Du, K Yang, Z Wang\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving \u2026"}, {"title": "Compositional Image Retrieval via Instruction-Aware Contrastive Learning", "link": "https://arxiv.org/pdf/2412.05756", "details": "W Zhong, W An, F Jiang, H Ma, Y Guo, J Huang - arXiv preprint arXiv:2412.05756, 2024", "abstract": "Composed Image Retrieval (CIR) involves retrieving a target image based on a composed query of an image paired with text that specifies modifications or changes to the visual reference. CIR is inherently an instruction-following task, as the model \u2026"}, {"title": "Micro-Bench: A Microscopy Benchmark for Vision-Language Understanding", "link": "https://openreview.net/pdf%3Fid%3DeRleg6vy0Y", "details": "A Lozano, JJ Nirschl, J Burgess, SR Gupte, Y Zhang\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Recent advances in microscopy have enabled the rapid generation of terabytes of image data in cell biology and biomedical research. Vision-language models (VLMs) offer a promising solution for large-scale biological image analysis, enhancing \u2026"}, {"title": "Espresso: High Compression For Rich Extraction From Videos for Your Vision-Language Model", "link": "https://arxiv.org/pdf/2412.04729", "details": "KP Yu, A Dave, R Ambrus, J Mercat - arXiv preprint arXiv:2412.04729, 2024", "abstract": "Most of the current vision-language models (VLMs) for videos struggle to understand videos longer than a few seconds. This is primarily due to the fact that they do not scale to utilizing a large number of frames. In order to address this limitation, we \u2026"}, {"title": "Assessing and Learning Alignment of Unimodal Vision and Language Models", "link": "https://arxiv.org/pdf/2412.04616", "details": "L Zhang, Q Yang, A Agrawal - arXiv preprint arXiv:2412.04616, 2024", "abstract": "How well are unimodal vision and language models aligned? Although prior work have approached answering this question, their assessment methods do not directly translate to how these models are used in practical vision-language tasks. In this \u2026"}, {"title": "Chain of Thought Prompting in Vision-Language Model for Vision Reasoning Tasks", "link": "https://link.springer.com/chapter/10.1007/978-981-96-0351-0_22", "details": "J Ou, J Zhou, Y Dong, F Chen - Australasian Joint Conference on Artificial Intelligence, 2024", "abstract": "The large language model has demonstrated its ability to reason and interpret in text- to-text applications. Current Chain of Thought (CoT) research focuses on either explaining reasoning steps or improving prediction results. This paper proposes a \u2026"}]
