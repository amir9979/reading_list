[{"title": "How Private are Language Models in Abstractive Summarization?", "link": "https://arxiv.org/pdf/2412.12040%3F", "details": "A Hughes, N Aletras, N Ma - arXiv preprint arXiv:2412.12040, 2024", "abstract": "Language models (LMs) have shown outstanding performance in text summarization including sensitive domains such as medicine and law. In these settings, it is important that personally identifying information (PII) included in the source \u2026"}, {"title": "HITgram: A Platform for Experimenting with n-gram Language Models", "link": "https://arxiv.org/pdf/2412.10717", "details": "S Dasgupta, C Maity, S Mukherjee, R Singh, D Dutta\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) are powerful but resource intensive, limiting accessibility. HITgram addresses this gap by offering a lightweight platform for n- gram model experimentation, ideal for resource-constrained environments. It \u2026"}]
