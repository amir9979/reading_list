[{"title": "Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction", "link": "https://arxiv.org/pdf/2505.00814", "details": "M S\u00e4nger, U Leser - arXiv preprint arXiv:2505.00814, 2025", "abstract": "Automatic relationship extraction (RE) from biomedical literature is critical for managing the vast amount of scientific knowledge produced each year. In recent years, utilizing pre-trained language models (PLMs) has become the prevalent \u2026", "entry_id": "http://arxiv.org/abs/2505.00814v1", "updated": "2025-05-01 19:16:18", "published": "2025-05-01 19:16:18", "authors": "Mario S\u00e4nger;Ulf Leser", "summary": "Automatic relationship extraction (RE) from biomedical literature is critical\nfor managing the vast amount of scientific knowledge produced each year. In\nrecent years, utilizing pre-trained language models (PLMs) has become the\nprevalent approach in RE. Several studies report improved performance when\nincorporating additional context information while fine-tuning PLMs for RE.\nHowever, variations in the PLMs applied, the databases used for augmentation,\nhyper-parameter optimization, and evaluation methods complicate direct\ncomparisons between studies and raise questions about the generalizability of\nthese findings. Our study addresses this research gap by evaluating PLMs\nenhanced with contextual information on five datasets spanning four relation\nscenarios within a consistent evaluation framework. We evaluate three baseline\nPLMs and first conduct extensive hyperparameter optimization. After selecting\nthe top-performing model, we enhance it with additional data, including textual\nentity descriptions, relational information from knowledge graphs, and\nmolecular structure encodings. Our findings illustrate the importance of i) the\nchoice of the underlying language model and ii) a comprehensive hyperparameter\noptimization for achieving strong extraction performance. Although inclusion of\ncontext information yield only minor overall improvements, an ablation study\nreveals substantial benefits for smaller PLMs when such external data was\nincluded during fine-tuning.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.00814v1;http://arxiv.org/pdf/2505.00814v1", "pdf_url": "http://arxiv.org/pdf/2505.00814v1"}, {"title": "Nemotron-Research-Tool-N1: Exploring Tool-Using Language Models with Reinforced Reasoning", "link": "https://arxiv.org/pdf/2505.00024", "details": "S Zhang, Y Dong, J Zhang, J Kautz, B Catanzaro\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Enabling large language models with external tools has become a pivotal strategy for extending their functionality beyond text generation tasks. Prior work typically enhances tool-use abilities by either applying supervised fine-tuning (SFT) to enforce \u2026", "entry_id": "http://arxiv.org/abs/2505.00024v2", "updated": "2025-05-12 03:01:39", "published": "2025-04-25 02:55:21", "authors": "Shaokun Zhang;Yi Dong;Jieyu Zhang;Jan Kautz;Bryan Catanzaro;Andrew Tao;Qingyun Wu;Zhiding Yu;Guilin Liu", "summary": "Enabling large language models with external tools has become a pivotal\nstrategy for extending their functionality beyond text space. To enhance LLMs'\ntool-calling abilities, previous approaches primarily rely on supervised\nfine-tuning (SFT) with trajectories distilled from stronger models, often\nresulting in imitative reasoning that limits generalization. In this work, we\nexplore rule-based reinforcement learning to enhance tool-calling in LLMs,\nresulting in Nemotron-Research-Tool-N1, a series of tool-calling reasoning\nmodels. Rather than enforcing supervision over intermediate distilled reasoning\ntraces, Tool-N1 is trained with a binary RL reward that assesses only the\nformat validity and functional correctness of tool invocations. This\nlightweight supervision allows the model to develop reasoning strategies\nindependently, without relying on annotated trajectories. Experiments on\nseveral major benchmarks show that Tool-N1-7B/14B clearly outperform GPT-4o. We\nconduct a systematic study on the design of rule-based reinforcement learning\nstrategies for training tool-calling models. Using 5,518 distilled reasoning\ntrajectories, we compare SFT, RL, and the SFT-then-RL pipeline, finding that\nthe widely adopted SFT-then-RL paradigm does not necessarily outperform pure\nRL.", "comment": "17 pages, 6 tables, 12 figures. - update new results - add more\n  details", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.00024v2;http://arxiv.org/pdf/2505.00024v2", "pdf_url": "http://arxiv.org/pdf/2505.00024v2"}, {"title": "Corner Cases: How Size and Position of Objects Challenge ImageNet-Trained Models", "link": "https://arxiv.org/pdf/2505.03569", "details": "M Fatima, S Jung, M Keuper - arXiv preprint arXiv:2505.03569, 2025", "abstract": "Backgrounds in images play a major role in contributing to spurious correlations among different data points. Owing to aesthetic preferences of humans capturing the images, datasets can exhibit positional (location of the object within a given frame) \u2026", "entry_id": "http://arxiv.org/abs/2505.03569v1", "updated": "2025-05-06 14:27:01", "published": "2025-05-06 14:27:01", "authors": "Mishal Fatima;Steffen Jung;Margret Keuper", "summary": "Backgrounds in images play a major role in contributing to spurious\ncorrelations among different data points. Owing to aesthetic preferences of\nhumans capturing the images, datasets can exhibit positional (location of the\nobject within a given frame) and size (region-of-interest to image ratio)\nbiases for different classes. In this paper, we show that these biases can\nimpact how much a model relies on spurious features in the background to make\nits predictions. To better illustrate our findings, we propose a synthetic\ndataset derived from ImageNet1k, Hard-Spurious-ImageNet, which contains images\nwith various backgrounds, object positions, and object sizes. By evaluating the\ndataset on different pretrained models, we find that most models rely heavily\non spurious features in the background when the region-of-interest (ROI) to\nimage ratio is small and the object is far from the center of the image.\nMoreover, we also show that current methods that aim to mitigate harmful\nspurious features, do not take into account these factors, hence fail to\nachieve considerable performance gains for worst-group accuracies when the size\nand location of core features in an image change.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.03569v1;http://arxiv.org/pdf/2505.03569v1", "pdf_url": "http://arxiv.org/pdf/2505.03569v1"}, {"title": "AEXL: Enhancing Path Prediction with Active Explainable Learning via Large Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3706599.3719963", "details": "A Doula, M M\u00fchlh\u00e4user, A Sanchez Guinea - \u2026 of the Extended Abstracts of the CHI \u2026, 2025", "abstract": "Active learning (AL) reduces annotation costs by selectively querying humans for uncertain data samples, thus improving data efficiency. In traditional AL, annotators simply label the data without gaining a deeper understanding of the reasoning of the \u2026"}, {"title": "Proactive Pseudo-Intervention: Pre-informed Contrastive Learning For Interpretable Vision Models", "link": "https://proceedings.mlr.press/v281/wang25a.html", "details": "D Wang, Y Yang, L Chen, Z Gan, R Henao, L Carin - AAAI Bridge Program on AI for \u2026, 2025", "abstract": "Deep neural networks excel at comprehending complex visual signals, delivering on par or even superior performance to that of human experts. However, ad-hoc visual explanations of model decisions often reveal an alarming level of reliance on \u2026"}]
