[{"title": "Language Models Can Predict Their Own Behavior", "link": "https://arxiv.org/pdf/2502.13329", "details": "D Ashok, J May - arXiv preprint arXiv:2502.13329, 2025", "abstract": "Autoregressive Language Models output text by sequentially predicting the next token to generate, with modern methods like Chain-of-Thought (CoT) prompting achieving state-of-the-art reasoning capabilities by scaling the number of generated \u2026"}, {"title": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models", "link": "https://arxiv.org/pdf/2501.17420", "details": "Y Li, H Shirado, S Das - arXiv preprint arXiv:2501.17420, 2025", "abstract": "While advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating \u2026"}, {"title": "The Multilingual Mind: A Survey of Multilingual Reasoning in Language Models", "link": "https://arxiv.org/pdf/2502.09457", "details": "A Ghosh, D Datta, S Saha, C Agarwal - arXiv preprint arXiv:2502.09457, 2025", "abstract": "While reasoning and multilingual capabilities in Language Models (LMs) have achieved remarkable progress in recent years, their integration into a unified paradigm, multilingual reasoning, is at a nascent stage. Multilingual reasoning \u2026"}, {"title": "Mordal: Automated Pretrained Model Selection for Vision Language Models", "link": "https://arxiv.org/pdf/2502.00241", "details": "S He, I Jang, M Chowdhury - arXiv preprint arXiv:2502.00241, 2025", "abstract": "Incorporating multiple modalities into large language models (LLMs) is a powerful way to enhance their understanding of non-textual data, enabling them to perform multimodal tasks. Vision language models (VLMs) form the fastest growing category \u2026"}, {"title": "Temporal and spatial self supervised learning methods for electrocardiograms", "link": "https://www.nature.com/articles/s41598-025-90084-2", "details": "W Chen, H Wang, L Zhang, M Zhang - Scientific Reports, 2025", "abstract": "The limited availability of labeled ECG data restricts the application of supervised deep learning methods in ECG detection. Although existing self-supervised learning approaches have been applied to ECG analysis, they are predominantly image \u2026"}, {"title": "Adapting Generative Large Language Models for Information Extraction from Unstructured Electronic Health Records in Residential Aged Care: A Comparative \u2026", "link": "https://link.springer.com/article/10.1007/s41666-025-00190-z", "details": "D Vithanage, C Deng, L Wang, M Yin, M Alkhalaf\u2026 - Journal of Healthcare \u2026, 2025", "abstract": "Abstract Information extraction (IE) of unstructured electronic health records is challenging due to the semantic complexity of textual data. Generative large language models (LLMs) offer promising solutions to address this challenge \u2026"}, {"title": "Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models", "link": "https://arxiv.org/pdf/2502.11559", "details": "Y Xu, C Fu, L Xiong, S Yang, W Wang - arXiv preprint arXiv:2502.11559, 2025", "abstract": "Pre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias \u2026"}, {"title": "A Survey on Large Language Models for Automated Planning", "link": "https://arxiv.org/pdf/2502.12435", "details": "M Aghzal, E Plaku, GJ Stein, Z Yao - arXiv preprint arXiv:2502.12435, 2025", "abstract": "The planning ability of Large Language Models (LLMs) has garnered increasing attention in recent years due to their remarkable capacity for multi-step reasoning and their ability to generalize across a wide range of domains. While some \u2026"}, {"title": "BoT: Breaking Long Thought Processes of o1-like Large Language Models through Backdoor Attack", "link": "https://arxiv.org/pdf/2502.12202", "details": "Z Zhu, H Zhang, M Zhang, R Wang, G Wu, K Xu, B Wu - arXiv preprint arXiv \u2026, 2025", "abstract": "Longer thought, better performance: large language models with deep reasoning capabilities, particularly o1-like models, have demonstrated remarkable performance by generating extensive thought processes during inference. This trade-off reveals a \u2026"}]
