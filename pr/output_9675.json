[{"title": "Accelerating Blockwise Parallel Language Models with Draft Refinement", "link": "https://openreview.net/pdf%3Fid%3DKT6F5Sw0eg", "details": "T Kim, AT Suresh, KA Papineni, M Riley, S Kumar\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Autoregressive language models have achieved remarkable advancements, yet their potential is often limited by the slow inference speeds associated with sequential token generation. Blockwise parallel decoding (BPD) was proposed by Stern et \u2026"}, {"title": "Probing Social Bias in Labor Market Text Generation by ChatGPT: A Masked Language Model Approach", "link": "https://openreview.net/pdf%3Fid%3DMP7j58lbWO", "details": "L Ding, Y Hu, N Denier, E Shi, J Zhang, Q Hu\u2026 - The Thirty-eighth Annual \u2026", "abstract": "As generative large language models (LLMs) such as ChatGPT gain widespread adoption in various domains, their potential to propagate and amplify social biases, particularly in high-stakes areas such as the labor market, has become a pressing \u2026"}, {"title": "Discovering sparsity allocation for layer-wise pruning of large language models", "link": "https://openreview.net/pdf%3Fid%3DrgtrYVC9n4", "details": "L Li, P Dong, Z Tang, X Liu, Q Wang, W Luo, W Xue\u2026 - The Thirty-eighth Annual \u2026, 2024", "abstract": "In this paper, we present DSA, the first automated framework for discovering sparsity allocation schemes for layer-wise pruning in Large Language Models (LLMs). LLMs have become increasingly powerful, but their large parameter counts make them \u2026"}, {"title": "Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language Models", "link": "https://openreview.net/pdf%3Fid%3Dnvn80cscVm", "details": "L Wei, Z Tan, C Li, J Wang, W Huang - The Thirty-eighth Annual Conference on \u2026, 2024", "abstract": "Large Language Models (LLMs) have transformed natural language processing and extended their powerful capabilities to multi-modal domains. As LLMs continue to advance, it is crucial to develop diverse and appropriate metrics for their evaluation \u2026"}, {"title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "link": "https://openreview.net/pdf%3Fid%3DIRXyPm9IPW", "details": "X Wu, S Huang, G Wang, J Xiong, F Wei - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts. Despite these advances \u2026"}]
