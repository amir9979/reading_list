[{"title": "How Chinese are Chinese Language Models? The Puzzling Lack of Language Policy in China's LLMs", "link": "https://arxiv.org/pdf/2407.09652", "details": "AW Wen-Yi, UES Jo, LJ Lin, D Mimno - arXiv preprint arXiv:2407.09652, 2024", "abstract": "Contemporary language models are increasingly multilingual, but Chinese LLM developers must navigate complex political and business considerations of language diversity. Language policy in China aims at influencing the public \u2026"}, {"title": "Steering Language Models with Game-Theoretic Solvers", "link": "https://openreview.net/pdf%3Fid%3D5QLtIodDmu", "details": "I Gemp, R Patel, Y Bachrach, M Lanctot, V Dasagi\u2026 - Agentic Markets Workshop at ICML \u2026", "abstract": "Mathematical models of strategic interactions among rational agents have long been studied in game theory. However the interactions studied are often over a small set of discrete actions which is very different from how humans communicate in natural \u2026"}, {"title": "RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent", "link": "https://arxiv.org/pdf/2407.16667", "details": "H Xu, W Zhang, Z Wang, F Xiao, R Zheng, Y Feng, Z Ba\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot. These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of \u2026"}, {"title": "Patch-Level Training for Large Language Models", "link": "https://arxiv.org/pdf/2407.12665", "details": "C Shao, F Meng, J Zhou - arXiv preprint arXiv:2407.12665, 2024", "abstract": "As Large Language Models (LLMs) achieve remarkable progress in language understanding and generation, their training efficiency has become a critical concern. Traditionally, LLMs are trained to predict the next token in a sequence \u2026"}, {"title": "Cross-Lingual Multi-Hop Knowledge Editing--Benchmarks, Analysis and a Simple Contrastive Learning based Approach", "link": "https://arxiv.org/pdf/2407.10275", "details": "A Khandelwal, H Singh, H Gu, T Chen, K Zhou - arXiv preprint arXiv:2407.10275, 2024", "abstract": "Large language models are often expected to constantly adapt to new sources of knowledge and knowledge editing techniques aim to efficiently patch the outdated model knowledge, with minimal modification. Most prior works focus on monolingual \u2026"}, {"title": "AutoM3L: An Automated Multimodal Machine Learning Framework with Large Language Models", "link": "https://openreview.net/pdf%3Fid%3DBkSgt0ClHJ", "details": "D Luo, C Feng, Y Nong, Y Shen - ACM Multimedia 2024", "abstract": "Automated Machine Learning (AutoML) offers a promising approach to streamline the training of machine learning models. However, existing AutoML frameworks are often limited to unimodal scenarios and require extensive manual configuration \u2026"}, {"title": "Not All Attention is Needed: Parameter and Computation Efficient Tuning for Multi-modal Large Language Models via Effective Attention Skipping", "link": "https://arxiv.org/pdf/2407.14093", "details": "Q Wu, Z Ke, Y Zhou, G Luo, X Sun, R Ji - arXiv preprint arXiv:2407.14093, 2024", "abstract": "Recently, mixture of experts (MoE) has become a popular paradigm for achieving the trade-off between modal capacity and efficiency of multi-modal large language models (MLLMs). Different from previous efforts, we are dedicated to exploring the \u2026"}, {"title": "Refusing Safe Prompts for Multi-modal Large Language Models", "link": "https://arxiv.org/pdf/2407.09050", "details": "Z Shao, H Liu, Y Hu, NZ Gong - arXiv preprint arXiv:2407.09050, 2024", "abstract": "Multimodal large language models (MLLMs) have become the cornerstone of today's generative AI ecosystem, sparking intense competition among tech giants and startups. In particular, an MLLM generates a text response given a prompt consisting \u2026"}, {"title": "CEIPA: Counterfactual Explainable Incremental Prompt Attack Analysis on Large Language Models", "link": "https://arxiv.org/pdf/2407.09292", "details": "D Shu, M Jin, T Chen, C Zhang, Y Zhang - arXiv preprint arXiv:2407.09292, 2024", "abstract": "This study sheds light on the imperative need to bolster safety and privacy measures in large language models (LLMs), such as GPT-4 and LLaMA-2, by identifying and mitigating their vulnerabilities through explainable analysis of prompt attacks. We \u2026"}]
