[{"title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use? CALM: A Unified Conversational Agentic Language Model", "link": "https://arxiv.org/pdf/2502.08820", "details": "EC Acikgoz, J Greer, A Datta, Z Yang, W Zeng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task- oriented dialogue (TOD) paradigm. However, current approaches face a critical \u2026"}, {"title": "DR. GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting with Demonstration and Reasoning", "link": "https://arxiv.org/pdf/2502.11603", "details": "H Qiu, Y Xu, M Qiu, W Wang - arXiv preprint arXiv:2502.11603, 2025", "abstract": "Large Language Models (LLMs) exhibit strong natural language processing capabilities but also inherit and amplify societal biases, including gender bias, raising fairness concerns. Existing debiasing methods face significant limitations \u2026"}, {"title": "Enhancing Out-of-Distribution Detection in Medical Imaging with Normalizing Flows", "link": "https://arxiv.org/pdf/2502.11638", "details": "D Lotfi, MAN Mahani, M Koohi-Moghadam, KT Bae - arXiv preprint arXiv:2502.11638, 2025", "abstract": "Out-of-distribution (OOD) detection is crucial in AI-driven medical imaging to ensure reliability and safety by identifying inputs outside a model's training distribution. Existing methods often require retraining or modifications to pre-trained models \u2026"}, {"title": "The need for balancing'black box'systems and explainable artificial intelligence: A necessary implementation in radiology", "link": "https://www.sciencedirect.com/science/article/pii/S0720048X25001007", "details": "F De-Giorgio, B Benedetti, M Mancino, E Sala\u2026 - European Journal of \u2026, 2025", "abstract": "Radiology is one of the medical specialties most significantly impacted by Artificial Intelligence (AI). AI systems, particularly those employing machine and deep learning, excel in processing large datasets and comparing images from similar \u2026"}, {"title": "LATTE-CXR: Locally Aligned TexT and imagE, Explainable dataset for Chest X-Rays", "link": "https://physionet.org/content/latte-cxr/", "details": "E Ghelichkhan, T Tasdizen", "abstract": "Local annotation of medical data is both expensive and time-consuming due to the high cost of expert annotators, the precision required for accurate annotation, and the inherent challenges of medical diagnosis. To address these problems, we developed \u2026"}, {"title": "Knowing When to Stop: Dynamic Context Cutoff for Large Language Models", "link": "https://arxiv.org/pdf/2502.01025", "details": "R Xie, J Wang, P Rosu, C Deng, B Sun, Z Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) process entire input contexts indiscriminately, which is inefficient in cases where the information required to answer a query is localized within the context. We present dynamic context cutoff, a human-inspired method \u2026"}, {"title": "Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models", "link": "https://arxiv.org/pdf/2502.08130", "details": "S Gupta, Y Nandwani, A Yehudai, D Khandelwal\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task \u2026"}, {"title": "Prompt-based Depth Pruning of Large Language Models", "link": "https://arxiv.org/pdf/2502.04348", "details": "J Wee, M Park, J Lee - arXiv preprint arXiv:2502.04348, 2025", "abstract": "Depth pruning aims to reduce the inference cost of a large language model without any hardware-specific complications, by simply removing several less important transformer blocks. However, our empirical findings suggest that the importance of a \u2026"}, {"title": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning", "link": "https://arxiv.org/pdf/2502.02390%3F", "details": "J Pan, S Deng, S Huang - arXiv preprint arXiv:2502.02390, 2025", "abstract": "Research on LLM technologies is rapidly emerging, with most of them employing a'fast thinking'approach to inference. Most LLMs generate the final result based solely on a single query and LLM's reasoning capabilities. However, with the advent \u2026"}]
