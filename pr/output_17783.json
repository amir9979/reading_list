[{"title": "Confidence-calibrated covariate shift correction for few-shot classification in Vision-Language Models", "link": "https://openaccess.thecvf.com/content/CVPR2025W/DG-EBF/papers/Khan_Confidence-calibrated_covariate_shift_correction_for_few-shot_classification_in_Vision-Language_Models_CVPRW_2025_paper.pdf", "details": "B Khan, R Qureshi, NM Durrani, TQ Syed - Proceedings of the Computer Vision and \u2026, 2025", "abstract": "Since the establishment of vision-language foundation models as the new mainstay in low-shot vision classification tasks, the question of domain generalization arising from insufficient target data is assuming more importance. This scarcity challenge \u2026"}, {"title": "Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2506.07064", "details": "K Xiong, X Ding, Y Cao, Y Yan, L Du, Y Zhang, J Gao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have mastered abundant simple and explicit commonsense knowledge through pre-training, enabling them to achieve human- like performance in simple commonsense reasoning. Nevertheless, LLMs struggle to \u2026", "entry_id": "http://arxiv.org/abs/2506.07064v1", "updated": "2025-06-08 09:53:08", "published": "2025-06-08 09:53:08", "authors": "Kai Xiong;Xiao Ding;Yixin Cao;Yuxiong Yan;Li Du;Yufei Zhang;Jinglong Gao;Jiaqian Liu;Bing Qin;Ting Liu", "summary": "Large language models (LLMs) have mastered abundant simple and explicit\ncommonsense knowledge through pre-training, enabling them to achieve human-like\nperformance in simple commonsense reasoning. Nevertheless, LLMs struggle to\nreason with complex and implicit commonsense knowledge that is derived from\nsimple ones (such as understanding the long-term effects of certain events), an\naspect humans tend to focus on more. Existing works focus on complex tasks like\nmath and code, while complex commonsense reasoning remains underexplored due to\nits uncertainty and lack of structure. To fill this gap and align with\nreal-world concerns, we propose a benchmark Com$^2$ focusing on complex\ncommonsense reasoning. We first incorporate causal event graphs to serve as\nstructured complex commonsense. Then we adopt causal theory~(e.g.,\nintervention) to modify the causal event graphs and obtain different scenarios\nthat meet human concerns. Finally, an LLM is employed to synthesize examples\nwith slow thinking, which is guided by the logical relationships in the\nmodified causal graphs. Furthermore, we use detective stories to construct a\nmore challenging subset. Experiments show that LLMs struggle in reasoning depth\nand breadth, while post-training and slow thinking can alleviate this. The code\nand data are available at https://github.com/Waste-Wood/Com2.", "comment": "Accepted by ACL 2025 Main Conference", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.07064v1;http://arxiv.org/pdf/2506.07064v1", "pdf_url": "http://arxiv.org/pdf/2506.07064v1"}, {"title": "Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models", "link": "https://arxiv.org/pdf/2506.06371", "details": "P Koletsis, C Panagiotopoulos, GT Papadopoulos\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Over the past few years, table interpretation tasks have made significant progress due to their importance and the introduction of new technologies and benchmarks in the field. This work experiments with a hybrid approach for detecting relationships \u2026", "entry_id": "http://arxiv.org/abs/2506.06371v1", "updated": "2025-06-04 12:11:05", "published": "2025-06-04 12:11:05", "authors": "Panagiotis Koletsis;Christos Panagiotopoulos;Georgios Th. Papadopoulos;Vasilis Efthymiou", "summary": "Over the past few years, table interpretation tasks have made significant\nprogress due to their importance and the introduction of new technologies and\nbenchmarks in the field. This work experiments with a hybrid approach for\ndetecting relationships among columns of unlabeled tabular data, using a\nKnowledge Graph (KG) as a reference point, a task known as CPA. This approach\nleverages large language models (LLMs) while employing statistical analysis to\nreduce the search space of potential KG relations. The main modules of this\napproach for reducing the search space are domain and range constraints\ndetection, as well as relation co-appearance analysis. The experimental\nevaluation on two benchmark datasets provided by the SemTab challenge assesses\nthe influence of each module and the effectiveness of different\nstate-of-the-art LLMs at various levels of quantization. The experiments were\nperformed, as well as at different prompting techniques. The proposed\nmethodology, which is publicly available on github, proved to be competitive\nwith state-of-the-art approaches on these datasets.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.06371v1;http://arxiv.org/pdf/2506.06371v1", "pdf_url": "http://arxiv.org/pdf/2506.06371v1"}]
