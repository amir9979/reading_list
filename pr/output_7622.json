[{"title": "Advancing Medical Radiograph Representation Learning: A Hybrid Pre-training Paradigm with Multilevel Semantic Granularity", "link": "https://arxiv.org/pdf/2410.00448", "details": "H Jiang, X Hao, Y Huang, C Ma, J Zhang, Y Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces an innovative approach to Medical Vision-Language Pre- training (Med-VLP) area in the specialized context of radiograph representation learning. While conventional methods frequently merge textual annotations into \u2026"}, {"title": "Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs", "link": "https://arxiv.org/pdf/2409.14988", "details": "C Christophe, T Raha, S Maslenkova, MU Salman\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated significant potential in transforming clinical applications. In this study, we investigate the efficacy of four techniques in adapting LLMs for clinical use-cases: continuous pretraining, instruct \u2026"}, {"title": "Evaluation of Large Language Model Performance on the Biomedical Language Understanding and Reasoning Benchmark: Comparative Study", "link": "https://www.medrxiv.org/content/10.1101/2024.05.17.24307411.pdf", "details": "H Feng, F Ronzano, J LaFleur, M Garber, R de Oliveira\u2026", "abstract": "Background: The availability of increasingly powerful large language models (LLMs) has attracted substantial interest in their potential for interpreting and generating human-like text for biomedical and clinical applications. However, there are often \u2026"}, {"title": "An Efficient Contrastive Unimodal Pretraining Method for EHR Time Series Data", "link": "https://openreview.net/pdf%3Fid%3DZN5vbwMpgX", "details": "R King, S Kodali, C Krueger, T Yang, BJ Mortazavi - IEEE-EMBS International Conference on \u2026", "abstract": "Machine learning has revolutionized the modeling of clinical timeseries data. Using machine learning, a Deep Neural Network (DNN) can be automatically trained to learn a complex mapping of its input features for a desired task. This is particularly \u2026"}, {"title": "Aligning Language Models Using Follow-up Likelihood as Reward Signal", "link": "https://arxiv.org/pdf/2409.13948", "details": "C Zhang, D Chong, F Jiang, C Tang, A Gao, G Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In natural human-to-human conversations, participants often receive feedback signals from one another based on their follow-up reactions. These reactions can include verbal responses, facial expressions, changes in emotional state, and other \u2026"}, {"title": "Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination", "link": "https://arxiv.org/pdf/2409.12746", "details": "ES Salido, R Morante, J Gonzalo, G Marco\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this article we present UNED-ACCESS 2024, a bilingual dataset that consists of 1003 multiple-choice questions of university entrance level exams in Spanish and English. Questions are originally formulated in Spanish and translated manually into \u2026"}, {"title": "Addition is All You Need for Energy-efficient Language Models", "link": "https://arxiv.org/pdf/2410.00907%3F", "details": "H Luo, W Sun - arXiv preprint arXiv:2410.00907, 2024", "abstract": "Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear \u2026"}, {"title": "Efficient Long-range Language Modeling with Self-supervised Causal Retrieval", "link": "https://arxiv.org/pdf/2410.01651", "details": "X Hu, Z Teng, W Wu, K Tu - arXiv preprint arXiv:2410.01651, 2024", "abstract": "Recently, retrieval-based language models (RLMs) have received much attention. However, most of them leverage a pre-trained retriever with fixed parameters, which may not adapt well to causal language models. In this work, we propose Grouped \u2026"}, {"title": "Enhancing Polyglot Voices by Leveraging Cross-Lingual Fine-Tuning in Any-to-One Voice Conversion", "link": "https://arxiv.org/pdf/2409.17387", "details": "G Ruggiero, M Testa, J Van de Walle, L Di Caro - arXiv preprint arXiv:2409.17387, 2024", "abstract": "The creation of artificial polyglot voices remains a challenging task, despite considerable progress in recent years. This paper investigates self-supervised learning for voice conversion to create native-sounding polyglot voices. We introduce \u2026"}]
