[{"title": "Privacy-Aware Visual Language Models", "link": "https://arxiv.org/pdf/2405.17423", "details": "L Samson, N Barazani, S Ghebreab, YM Asano - arXiv preprint arXiv:2405.17423, 2024", "abstract": "This paper aims to advance our understanding of how Visual Language Models (VLMs) handle privacy-sensitive information, a crucial concern as these technologies become integral to everyday life. To this end, we introduce a new benchmark \u2026"}, {"title": "Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models", "link": "https://arxiv.org/pdf/2406.09403", "details": "Y Hu, W Shi, X Fu, D Roth, M Ostendorf, L Zettlemoyer\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Humans draw to facilitate reasoning: we draw auxiliary lines when solving geometry problems; we mark and circle when reasoning on maps; we use sketches to amplify our ideas and relieve our limited-capacity working memory. However, such actions \u2026"}, {"title": "Calibrating Reasoning in Language Models with Internal Consistency", "link": "https://arxiv.org/pdf/2405.18711", "details": "Z Xie, J Guo, T Yu, S Li - arXiv preprint arXiv:2405.18711, 2024", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought (CoT) prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious \u2026"}, {"title": "Strengthening the use of artificial intelligence within healthcare delivery organizations: balancing regulatory compliance and patient safety", "link": "https://academic.oup.com/jamia/advance-article-abstract/doi/10.1093/jamia/ocae119/7676585", "details": "MP Sendak, VX Liu, A Beecy, DE Vidal, K Shaw\u2026 - Journal of the American \u2026, 2024", "abstract": "Objectives Surface the urgent dilemma that healthcare delivery organizations (HDOs) face navigating the US Food and Drug Administration (FDA) final guidance on the use of clinical decision support (CDS) software. Materials and Methods We \u2026"}, {"title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions", "link": "https://arxiv.org/pdf/2405.19744", "details": "C Li, W Yang, J Zhang, J Lu, S Wang, C Zong - arXiv preprint arXiv:2405.19744, 2024", "abstract": "Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples \u2026"}, {"title": "The Impact of Common Variations in Sequential Organ Failure Assessment Score Calculation on Sepsis Measurement Using Sepsis-3 Criteria: A Retrospective \u2026", "link": "https://journals.lww.com/ccmjournal/fulltext/9900/the_impact_of_common_variations_in_sequential.337.aspx", "details": "M Alrawashdeh, M Klompas, C Rhee - Critical Care Medicine, 2024", "abstract": "Objectives: To assess the impact of different methods of calculating Sequential Organ Failure Assessment (SOFA) scores using electronic health record data on the incidence, outcomes, agreement, and predictive validity of Sepsis-3 criteria. Design \u2026"}, {"title": "Exploring Activation Patterns of Parameters in Language Models", "link": "https://arxiv.org/pdf/2405.17799", "details": "Y Wang, D Dai, Z Sui - arXiv preprint arXiv:2405.17799, 2024", "abstract": "Most work treats large language models as black boxes without in-depth understanding of their internal working mechanism. In order to explain the internal representations of LLMs, we propose a gradient-based metric to assess the \u2026"}, {"title": "Direct Alignment of Language Models via Quality-Aware Self-Refinement", "link": "https://arxiv.org/pdf/2405.21040", "details": "R Yu, Y Wang, X Jiao, Y Zhang, JT Kwok - arXiv preprint arXiv:2405.21040, 2024", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been commonly used to align the behaviors of Large Language Models (LLMs) with human preferences. Recently, a popular alternative is Direct Policy Optimization (DPO), which replaces \u2026"}, {"title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective", "link": "https://arxiv.org/pdf/2405.16747", "details": "A Tomihari, I Sato - arXiv preprint arXiv:2405.16747, 2024", "abstract": "The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data. This success is largely \u2026"}]
