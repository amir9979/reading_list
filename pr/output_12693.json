[{"title": "Vulnerability Mitigation for Safety-Aligned Language Models via Debiasing", "link": "https://arxiv.org/pdf/2502.02153", "details": "TQ Tran, A Wachi, R Sato, T Tanabe, Y Akimoto - arXiv preprint arXiv:2502.02153, 2025", "abstract": "Safety alignment is an essential research topic for real-world AI applications. Despite the multifaceted nature of safety and trustworthiness in AI, current safety alignment methods often focus on a comprehensive notion of safety. By carefully assessing \u2026"}, {"title": "Online Preference Alignment for Language Models via Count-based Exploration", "link": "https://arxiv.org/pdf/2501.12735", "details": "C Bai, Y Zhang, S Qiu, Q Zhang, K Xu, X Li - arXiv preprint arXiv:2501.12735, 2025", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has shown great potential in fine-tuning Large Language Models (LLMs) to align with human preferences. Existing methods perform preference alignment from a fixed dataset, which can be \u2026"}, {"title": "Tool Learning in the Wild: Empowering Language Models as Automatic Tool Agents", "link": "https://openreview.net/pdf%3Fid%3DT4wMdeFEjX", "details": "Z Shi, S Gao, L Yan, Y Feng, X Chen, Z Chen, D Yin\u2026 - THE WEB CONFERENCE 2025", "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a promising approach to extend their utility, enabling them to solve practical tasks. Previous methods manually parse tool documentation and create in-context \u2026"}, {"title": "Personalization Toolkit: Training Free Personalization of Large Vision Language Models", "link": "https://arxiv.org/pdf/2502.02452", "details": "S Seifi, V Dorovatas, DO Reino, R Aljundi - arXiv preprint arXiv:2502.02452, 2025", "abstract": "Large Vision Language Models (LVLMs) have significant potential to deliver personalized assistance by adapting to individual users' unique needs and preferences. Personalization of LVLMs is an emerging area that involves \u2026"}, {"title": "Global Semantic-Guided Sub-image Feature Weight Allocation in High-Resolution Large Vision-Language Models", "link": "https://arxiv.org/pdf/2501.14276", "details": "Y Liang, X Li, X Chen, H Chen, Y Zheng, C Lai, B Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As the demand for high-resolution image processing in Large Vision-Language Models (LVLMs) grows, sub-image partitioning has become a popular approach for mitigating visual information loss associated with fixed-resolution processing \u2026"}, {"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models", "link": "https://arxiv.org/pdf/2502.03199", "details": "J Wu, Y Shen, S Liu, Y Tang, S Song, X Wang, L Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the \u2026"}, {"title": "Advancing Math Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages", "link": "https://arxiv.org/pdf/2501.14002", "details": "Z Chen, T Liu, M Tian, Q Tong, W Luo, Z Liu - arXiv preprint arXiv:2501.14002, 2025", "abstract": "Advancements in LLMs have significantly expanded their capabilities across various domains. However, mathematical reasoning remains a challenging area, prompting the development of math-specific LLMs. These models typically follow a two-stage \u2026"}, {"title": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques", "link": "https://arxiv.org/pdf/2501.14492", "details": "Z Tang, Z Li, Z Xiao, T Ding, R Sun, B Wang, D Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Critiques are important for enhancing the performance of Large Language Models (LLMs), enabling both self-improvement and constructive feedback for others by identifying flaws and suggesting improvements. However, evaluating the critique \u2026"}, {"title": "FinMoE: A MoE-based Large Chinese Financial Language Model", "link": "https://aclanthology.org/2025.finnlp-1.4.pdf", "details": "X Zhang, Q Yang - Proceedings of the Joint Workshop of the 9th Financial \u2026, 2025", "abstract": "Large-scale language models have demonstrated remarkable success, achieving strong performance across a variety of general tasks. However, when applied to domain-specific fields, such as finance, these models face challenges due to the \u2026"}]
