[{"title": "Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models", "link": "https://arxiv.org/pdf/2410.12011", "details": "K Tatariya, V Araujo, T Bauwens, M de Lhoneux - arXiv preprint arXiv:2410.12011, 2024", "abstract": "Pixel-based language models have emerged as a compelling alternative to subword- based language modelling, particularly because they can represent virtually any script. PIXEL, a canonical example of such a model, is a vision transformer that has \u2026"}, {"title": "LanFL: Differentially Private Federated Learning with Large Language Models using Synthetic Samples", "link": "https://arxiv.org/pdf/2410.19114", "details": "H Wu, D Klabjan - arXiv preprint arXiv:2410.19114, 2024", "abstract": "Federated Learning (FL) is a collaborative, privacy-preserving machine learning framework that enables multiple participants to train a single global model. However, the recent advent of powerful Large Language Models (LLMs) with tens to hundreds \u2026"}, {"title": "Category-guided multi-interest collaborative metric learning with representation uniformity constraints", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324002966", "details": "L Wang, T Lian - Information Processing & Management, 2025", "abstract": "Multi-interest collaborative metric learning has recently emerged as an effective approach to modeling the multifaceted interests of a user in recommender systems. However, two issues remain unexplored.(1) There is no explicit guidance for the \u2026"}, {"title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model Training Pipelines via Memoization-Awareness", "link": "https://arxiv.org/pdf/2411.03731", "details": "A Essofi, R Salahuddeen, M Nwadike, E Zhalieva\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The training or fine-tuning of machine learning, vision, and language models is often implemented as a pipeline: a sequence of stages encompassing data preparation, model training and evaluation. In this paper, we exploit pipeline structures to reduce \u2026"}, {"title": "Policy optimization of language models to align fidelity and efficiency of generative retrieval in multi-turn dialogues", "link": "https://jcur.github.io/publications/RecSysLLM_FallbackOptimization.pdf", "details": "J Curuksu - NeurIPS 2024 Workshop on Fine-Tuning in Modern \u2026, 2024", "abstract": "Combining large languages models (LM) with policy optimization based on human preferences has recently led to new generations of chatbots showing human-level capabilities in helpfulness and safety, with each new release often massively better \u2026"}, {"title": "Enabling Scalable Evaluation of Bias Patterns in Medical LLMs", "link": "https://arxiv.org/pdf/2410.14763", "details": "H Fayyaz, R Poulain, R Beheshti - arXiv preprint arXiv:2410.14763, 2024", "abstract": "Large language models (LLMs) have shown impressive potential in helping with numerous medical challenges. Deploying LLMs in high-stakes applications such as medicine, however, brings in many concerns. One major area of concern relates to \u2026"}, {"title": "Evolutionary Contrastive Distillation for Language Model Alignment", "link": "https://arxiv.org/pdf/2410.07513", "details": "J Katz-Samuels, Z Li, H Yun, P Nigam, Y Xu, V Petricek\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The ability of large language models (LLMs) to execute complex instructions is essential for their real-world applications. However, several recent studies indicate that LLMs struggle with challenging instructions. In this paper, we propose \u2026"}, {"title": "FactTest: Factuality Testing in Large Language Models with Statistical Guarantees", "link": "https://arxiv.org/pdf/2411.02603", "details": "F Nie, X Hou, S Lin, J Zou, H Yao, L Zhang - arXiv preprint arXiv:2411.02603, 2024", "abstract": "The propensity of Large Language Models (LLMs) to generate hallucinations and non-factual content undermines their reliability in high-stakes domains, where rigorous control over Type I errors (the conditional probability of incorrectly \u2026"}, {"title": "Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered Clues", "link": "https://arxiv.org/pdf/2410.10700", "details": "Q Ren, H Li, D Liu, Z Xie, X Lu, Y Qiao, L Sha, J Yan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study exposes the safety vulnerabilities of Large Language Models (LLMs) in multi-turn interactions, where malicious users can obscure harmful intents across several queries. We introduce ActorAttack, a novel multi-turn attack method inspired \u2026"}]
