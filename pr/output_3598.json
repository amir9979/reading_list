[{"title": "Aligning Language Models with the Human World", "link": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi%3Farticle%3D1241%26context%3Ddissertations", "details": "R LIU - 2024", "abstract": "Abstract The field of Natural Language Processing (NLP) has undergone a significant transformation with the emergence of large language models (LMs). These models have enabled the development of human-like conversational \u2026"}, {"title": "Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?", "link": "https://arxiv.org/pdf/2406.16316", "details": "Y Jinnai - arXiv preprint arXiv:2406.16316, 2024", "abstract": "Alignment of the language model with human preferences is a common approach to making a language model useful to end users. However, most alignment work is done in English, and human preference datasets are dominated by English \u2026"}, {"title": "Entropy-Based Decoding for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2406.17519", "details": "Z Qiu, Z Ou, B Wu, J Li, A Liu, I King - arXiv preprint arXiv:2406.17519, 2024", "abstract": "Augmenting Large Language Models (LLMs) with retrieved external knowledge has proven effective for improving the factual accuracy of generated responses. Despite their success, retrieval-augmented LLMs still face the distractibility issue, where the \u2026"}, {"title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models", "link": "https://arxiv.org/pdf/2407.02408", "details": "S Wang, P Wang, T Zhou, Y Dong, Z Tan, J Li - arXiv preprint arXiv:2407.02408, 2024", "abstract": "As Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases \u2026"}, {"title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application", "link": "https://arxiv.org/pdf/2407.01885", "details": "C Yang, W Lu, Y Zhu, Y Wang, Q Chen, C Gao, B Yan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have showcased exceptional capabilities in various domains, attracting significant interest from both academia and industry. Despite their impressive performance, the substantial size and computational demands of LLMs \u2026"}, {"title": "Banishing LLM Hallucinations Requires Rethinking Generalization", "link": "https://arxiv.org/pdf/2406.17642", "details": "J Li, S Consul, E Zhou, J Wong, N Farooqui, Y Ye\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their powerful chat, coding, and reasoning abilities, Large Language Models (LLMs) frequently hallucinate. Conventional wisdom suggests that hallucinations are a consequence of a balance between creativity and factuality, which can be \u2026"}, {"title": "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models", "link": "https://arxiv.org/pdf/2406.19999", "details": "X Chen, B Liao, J Qi, P Eustratiadis, C Monz, A Bisazza\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Following multiple instructions is a crucial ability for large language models (LLMs). Evaluating this ability comes with significant challenges:(i) limited coherence between multiple instructions,(ii) positional bias where the order of instructions \u2026"}, {"title": "Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization", "link": "https://arxiv.org/pdf/2406.16743", "details": "Z Zhao, X Zhang, K Xu, X Hu, R Zhang, Z Du, Q Guo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the widespread application of Large Language Models (LLMs), it has become a significant concern to ensure their safety and prevent harmful responses. While current safe-alignment methods based on instruction fine-tuning and Reinforcement \u2026"}, {"title": "Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models", "link": "https://openreview.net/pdf%3Fid%3DcQWsTeTSkZ", "details": "S Lotfi, Y Kuang, MA Finzi, B Amos, M Goldblum\u2026 - ICML 2024 Workshop on \u2026", "abstract": "Large language models (LLMs) with billions of parameters excel at predicting the next token in a sequence. Recent work computes non-vacuous compression-based generalization bounds for LLMs, but these bounds are vacuous for large models at \u2026"}]
