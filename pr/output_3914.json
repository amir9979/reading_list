[{"title": "Scaling Laws for Linear Complexity Language Models", "link": "https://arxiv.org/pdf/2406.16690", "details": "X Shen, D Li, R Leng, Z Qin, W Sun, Y Zhong - arXiv preprint arXiv:2406.16690, 2024", "abstract": "The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their \u2026"}, {"title": "Enhancing Robustness of Vision-Language Models through Orthogonality Learning and Cross-Regularization", "link": "https://arxiv.org/pdf/2407.08374", "details": "J Li, Z Jie, E Ricci, L Ma, N Sebe - arXiv preprint arXiv:2407.08374, 2024", "abstract": "Efficient finetuning of vision-language models (VLMs) like CLIP for specific downstream tasks is gaining significant attention. Previous works primarily focus on prompt learning to adapt the CLIP into a variety of downstream tasks, however \u2026"}, {"title": "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models", "link": "https://arxiv.org/pdf/2407.05131", "details": "P Xia, K Zhu, H Li, H Zhu, Y Li, G Li, L Zhang, H Yao - arXiv preprint arXiv:2407.05131, 2024", "abstract": "The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical \u2026"}, {"title": "DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning", "link": "https://arxiv.org/pdf/2406.17659", "details": "X Zhang, Z Altaweel, Y Hayamizu, Y Ding, S Amiri\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language models (VLMs) have been applied to robot task planning problems, where the robot receives a task in natural language and generates plans based on visual inputs. While current VLMs have demonstrated strong vision-language \u2026"}, {"title": "MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models", "link": "https://arxiv.org/pdf/2407.02775", "details": "Y Zhang, Z Yang, S Ji - arXiv preprint arXiv:2407.02775, 2024", "abstract": "Knowledge distillation is an effective technique for pre-trained language model compression. Although existing knowledge distillation methods perform well for the most typical model BERT, they could be further improved in two aspects: the relation \u2026"}, {"title": "DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs", "link": "https://arxiv.org/pdf/2406.17158", "details": "VVD Prabhu, A Anand - arXiv preprint arXiv:2406.17158, 2024", "abstract": "Open-domain complex Question Answering (QA) is a difficult task with challenges in evidence retrieval and reasoning. The complexity of such questions could stem from questions being compositional, hybrid evidence, or ambiguity in questions. While \u2026"}, {"title": "Information Guided Regularization for Fine-tuning Language Models", "link": "https://arxiv.org/pdf/2406.14005", "details": "M Sharma, N Muralidhar, S Xu, RB Yosuf\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The pretraining-fine-tuning paradigm has been the de facto strategy for transfer learning in modern language modeling. With the understanding that task adaptation in LMs is often a function of parameters shared across tasks, we argue that a more \u2026"}, {"title": "Semantic Compositions Enhance Vision-Language Contrastive Learning", "link": "https://arxiv.org/pdf/2407.01408", "details": "M Aladago, L Torresani, S Vosoughi - arXiv preprint arXiv:2407.01408, 2024", "abstract": "In the field of vision-language contrastive learning, models such as CLIP capitalize on matched image-caption pairs as positive examples and leverage within-batch non- matching pairs as negatives. This approach has led to remarkable outcomes in zero \u2026"}, {"title": "Large Language Model as a Universal Clinical Multi-task Decoder", "link": "https://arxiv.org/pdf/2406.12738", "details": "Y Wu, H Song, J Zhang, X Wen, S Zheng, J Bian - arXiv preprint arXiv:2406.12738, 2024", "abstract": "The development of effective machine learning methodologies for enhancing the efficiency and accuracy of clinical systems is crucial. Despite significant research efforts, managing a plethora of diversified clinical tasks and adapting to emerging \u2026"}]
