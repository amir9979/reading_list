[{"title": "SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities", "link": "https://arxiv.org/pdf/2502.12025%3F", "details": "F Jiang, Z Xu, Y Li, L Niu, Z Xiang, B Li, BY Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently \u2026"}, {"title": "Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models", "link": "https://arxiv.org/pdf/2502.14191", "details": "M Yasunaga, L Zettlemoyer, M Ghazvininejad - arXiv preprint arXiv:2502.14191, 2025", "abstract": "Reward models play an essential role in training vision-language models (VLMs) by assessing output quality to enable aligning with human preferences. Despite their importance, the research community lacks comprehensive open benchmarks for \u2026"}, {"title": "SAIF: A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models", "link": "https://arxiv.org/pdf/2502.11356", "details": "Z He, H Zhao, Y Qiao, F Yang, A Payani, J Ma, M Du - arXiv preprint arXiv \u2026, 2025", "abstract": "The ability of large language models (LLMs) to follow instructions is crucial for their practical applications, yet the underlying mechanisms remain poorly understood. This paper presents a novel framework that leverages sparse autoencoders (SAE) to \u2026"}, {"title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information", "link": "https://arxiv.org/pdf/2502.14258", "details": "Y Park, C Yoon, J Park, M Jeong, J Kang - arXiv preprint arXiv:2502.14258, 2025", "abstract": "While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads primarily responsible for processing \u2026"}, {"title": "Improved Unbiased Watermark for Large Language Models", "link": "https://arxiv.org/pdf/2502.11268", "details": "R Chen, Y Wu, J Guo, H Huang - arXiv preprint arXiv:2502.11268, 2025", "abstract": "As artificial intelligence surpasses human capabilities in text generation, the necessity to authenticate the origins of AI-generated content has become paramount. Unbiased watermarks offer a powerful solution by embedding statistical signals into \u2026"}, {"title": "Prompt-to-Leaderboard", "link": "https://arxiv.org/pdf/2502.14855", "details": "E Frick, C Chen, J Tennyson, T Li, WL Chiang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts. This averaging obscures user-and prompt-specific variations in model performance. To address this \u2026"}, {"title": "Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?", "link": "https://arxiv.org/pdf/2502.11501", "details": "Z Wen, Y Gao, W Li, C He, L Zhang - arXiv preprint arXiv:2502.11501, 2025", "abstract": "Multimodal large language models (MLLMs) have shown remarkable performance for cross-modal understanding and generation, yet still suffer from severe inference costs. Recently, abundant works have been proposed to solve this problem with \u2026"}, {"title": "Diversity-driven Data Selection for Language Model Tuning through Sparse Autoencoder", "link": "https://arxiv.org/pdf/2502.14050", "details": "X Yang, S Nie, L Liu, S Gururangan, U Karn, R Hou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Current pre-trained large language models typically need instruction tuning to align with human preferences. However, instruction tuning data is often quantity-saturated due to the large volume of data collection and fast model iteration, leaving coreset \u2026"}, {"title": "Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models", "link": "https://arxiv.org/pdf/2502.11881", "details": "H Kim, M Sclar, T Zhi-Xuan, L Ying, S Levine, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods-such \u2026"}]
