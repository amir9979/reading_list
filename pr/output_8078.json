[{"title": "Demonstration of a Multi-agent Framework for Text to SQL Applications with Large Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3627673.3679216", "details": "C Shen, J Wang, S Rahman, E Kandogan - \u2026 of the 33rd ACM International Conference \u2026, 2024", "abstract": "The Text-to-SQL problem aims at developing natural language query interfaces for relational database systems by converting the text input into executable SQL queries. Recently, using Large Language Models (LLM) has emerged as a new paradigm for \u2026"}, {"title": "Understanding the Effect of Algorithm Transparency of Model Explanations in Text-to-SQL Semantic Parsing", "link": "https://arxiv.org/pdf/2410.16283", "details": "D Rai, RR Weiland, KMG Herrera, TH Shaw, Z Yao - arXiv preprint arXiv:2410.16283, 2024", "abstract": "Explaining the decisions of AI has become vital for fostering appropriate user trust in these systems. This paper investigates explanations for a structured prediction task called``text-to-SQL Semantic Parsing'', which translates a natural language question \u2026"}, {"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "link": "https://arxiv.org/pdf/2410.05639", "details": "R Zhao, ZL Thai, Y Zhang, S Hu, Y Ba, J Zhou, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the \u2026"}, {"title": "Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop Question Answering", "link": "https://dl.acm.org/doi/pdf/10.1145/3627673.3679722", "details": "Y Shi, Q Tan, X Wu, S Zhong, K Zhou, N Liu - Proceedings of the 33rd ACM \u2026, 2024", "abstract": "Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging \u2026"}, {"title": "Do Vision-Language Models Really Understand Visual Language?", "link": "https://arxiv.org/pdf/2410.00193", "details": "B Giledereli, Y Hou, Y Tu, M Sachan - arXiv preprint arXiv:2410.00193, 2024", "abstract": "Visual language is a system of communication that conveys information through symbols, shapes, and spatial arrangements. Diagrams are a typical example of a visual language depicting complex concepts and their relationships in the form of an \u2026"}, {"title": "ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context Information in Multi-Turn Multimodal Medical Dialogue", "link": "https://arxiv.org/pdf/2409.17610", "details": "Z Li, C Zou, S Ma, Z Yang, C Du, Y Tang, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rocketing prosperity of large language models (LLMs) in recent years has boosted the prevalence of vision-language models (VLMs) in the medical sector. In our online medical consultation scenario, a doctor responds to the texts and images \u2026"}, {"title": "Zero-Shot Multi-Hop Question Answering via Monte-Carlo Tree Search with Large Language Models", "link": "https://arxiv.org/pdf/2409.19382", "details": "S Lee, J Shin, Y Ahn, S Seo, O Kwon, KE Kim - arXiv preprint arXiv:2409.19382, 2024", "abstract": "Recent advances in large language models (LLMs) have significantly impacted the domain of multi-hop question answering (MHQA), where systems are required to aggregate information and infer answers from disparate pieces of text. However, the \u2026"}, {"title": "Reducing Hallucinations in Vision-Language Models via Latent Space Steering", "link": "https://arxiv.org/abs/2410.15778", "details": "S Liu, H Ye, J Zou - arXiv preprint arXiv:2410.15778, 2024", "abstract": "Hallucination poses a challenge to the deployment of large vision-language models (LVLMs) in applications. Unlike in large language models (LLMs), hallucination in LVLMs often arises from misalignments between visual inputs and textual outputs \u2026"}, {"title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "link": "https://arxiv.org/pdf/2410.11829%3F", "details": "Y Cao, Y Liu, Z Chen, G Shi, W Wang, D Zhao, T Lu - arXiv preprint arXiv:2410.11829, 2024", "abstract": "Despite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating \u2026"}]
