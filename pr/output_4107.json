[{"title": "Composable Interventions for Language Models", "link": "https://arxiv.org/pdf/2407.06483", "details": "A Kolbeinsson, K O'Brien, T Huang, S Gao, S Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Test-time interventions for language models can enhance factual accuracy, mitigate harmful outputs, and improve model efficiency without costly retraining. But despite a flood of new methods, different types of interventions are largely developing \u2026"}, {"title": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation", "link": "https://arxiv.org/pdf/2407.08441", "details": "R Cantini, G Cosenza, A Orsino, D Talia - arXiv preprint arXiv:2407.08441, 2024", "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence, demonstrating remarkable computational power and linguistic capabilities. However, these models are inherently prone to various biases stemming from their training \u2026"}, {"title": "Patch-Level Training for Large Language Models", "link": "https://arxiv.org/pdf/2407.12665", "details": "C Shao, F Meng, J Zhou - arXiv preprint arXiv:2407.12665, 2024", "abstract": "As Large Language Models (LLMs) achieve remarkable progress in language understanding and generation, their training efficiency has become a critical concern. Traditionally, LLMs are trained to predict the next token in a sequence \u2026"}, {"title": "On the attribution of confidence to large language models", "link": "https://arxiv.org/pdf/2407.08388", "details": "G Keeling, W Street - arXiv preprint arXiv:2407.08388, 2024", "abstract": "Credences are mental states corresponding to degrees of confidence in propositions. Attribution of credences to Large Language Models (LLMs) is commonplace in the empirical literature on LLM evaluation. Yet the theoretical basis \u2026"}, {"title": "ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts", "link": "https://arxiv.org/pdf/2407.09447", "details": "AF Hardy, H Liu, B Lange, MJ Kochenderfer - arXiv preprint arXiv:2407.09447, 2024", "abstract": "Typical schemes for automated red-teaming large language models (LLMs) focus on discovering prompts that trigger a frozen language model (the defender) to generate toxic text. This often results in the prompting model (the adversary) producing text \u2026"}, {"title": "Beyond Instruction Following: Evaluating Rule Following of Large Language Models", "link": "https://arxiv.org/pdf/2407.08440", "details": "W Sun, C Zhang, X Zhang, Z Huang, H Xu, P Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Although Large Language Models (LLMs) have demonstrated strong instruction- following ability to be helpful, they are further supposed to be controlled and guided by rules in real-world scenarios to be safe, and accurate in responses. This demands \u2026"}, {"title": "Social Bias Evaluation for Large Language Models Requires Prompt Variations", "link": "https://arxiv.org/pdf/2407.03129", "details": "R Hida, M Kaneko, N Okazaki - arXiv preprint arXiv:2407.03129, 2024", "abstract": "Warning: This paper contains examples of stereotypes and biases. Large Language Models (LLMs) exhibit considerable social biases, and various studies have tried to evaluate and mitigate these biases accurately. Previous studies use downstream \u2026"}, {"title": "1+ 1> 2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?", "link": "https://arxiv.org/pdf/2406.14721", "details": "Y Huang, C Fan, Y Li, S Wu, T Zhou, X Zhang, L Sun - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have garnered significant attention due to their remarkable ability to process information across various languages. Despite their capabilities, they exhibit inconsistencies in handling identical queries in different \u2026"}, {"title": "DART: Deep Adversarial Automated Red Teaming for LLM Safety", "link": "https://arxiv.org/pdf/2407.03876", "details": "B Jiang, Y Jing, T Shen, Q Yang, D Xiong - arXiv preprint arXiv:2407.03876, 2024", "abstract": "Manual Red teaming is a commonly-used method to identify vulnerabilities in large language models (LLMs), which, is costly and unscalable. In contrast, automated red teaming uses a Red LLM to automatically generate adversarial prompts to the Target \u2026"}]
