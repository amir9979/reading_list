[{"title": "TabMedBERT: A Tabular Knowledge Enhanced Biomedical Pretrained Language Model", "link": "https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA240674", "details": "X Yan, L Geng, Z Cao, J Li, W Li, S Li, X Zhou, Y Yang\u2026 - ECAI 2024, 2024", "abstract": "Most existing biomedical language models are trained on plain text with general learning goals such as random word infilling, failing to capture the knowledge in the biomedical corpus sufficiently. Since biomedical articles usually contain many tables \u2026"}, {"title": "Improving clinical expertise in large language models using electronic medical records", "link": "https://www.researchsquare.com/article/rs-5285540/latest.pdf", "details": "L Zhu, J Liu, J Wang, W Zhang, S Jiang, H Yang\u2026 - 2024", "abstract": "Electronic medical records (EMRs) are essential in clinical practice. Although current medical large language models (LLMs) excel in tasks like US Medical Licensing Examination, they struggle with real-world clinical applications due to insufficient \u2026"}]
