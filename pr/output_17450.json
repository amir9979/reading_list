[{"title": "Current Applications of Chatbots Powered by **Large Language Models** in Oral and Maxillofacial Surgery: A Systematic Review", "link": "https://www.mdpi.com/2304-6767/13/6/261", "details": "V Ronsivalle, S Santonocito, U Cammarata, E Lo Muzio\u2026 - Dentistry Journal, 2025", "abstract": "\u2026 It allows us to **evaluate** the risk of bias through a structured approach to **evaluate** potential biases, using seven domains: confounding, \u2026 This process ensured objectivity and consistency in the **evaluations**. The ROBINS-I assessment provided a \u2026"}, {"title": "Automating and **Evaluating Large Language Models** for Accurate Text Summarization Under Zero-Shot Conditions", "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12150706/", "details": "MPM Rocha, HB Klasky - AMIA Summits on Translational Science Proceedings, 2025", "abstract": "Automated text summarization (ATS) is crucial for collecting specialized, domain-specific information. Zero-shot learning (ZSL) allows **large** **language** **models** (LLMs) to respond to prompts on information not included in their training, playing a vital role \u2026"}, {"title": "Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data", "link": "https://arxiv.org/pdf/2506.08750", "details": "M Anwar, D Lau, M de Costa, I Hammad - arXiv preprint arXiv:2506.08750, 2025", "abstract": "\u2026 The use of **Large** **Language** **Models** (LLMs) for synthetic \u2026 **evaluation** of the synthetically generated question-answer pairs. We assess the quality of these pairs along several dimensions: semantic diversity, relevance to the source text, and \u2026", "entry_id": "http://arxiv.org/abs/2506.08750v1", "updated": "2025-06-10 12:45:12", "published": "2025-06-10 12:45:12", "authors": "Muhammad Anwar;Daniel Lau;Mishca de Costa;Issam Hammad", "summary": "The nuclear industry possesses a wealth of valuable information locked away\nin unstructured text data. This data, however, is not readily usable for\nadvanced Large Language Model (LLM) applications that require clean, structured\nquestion-answer pairs for tasks like model training, fine-tuning, and\nevaluation. This paper explores how synthetic data generation can bridge this\ngap, enabling the development of robust LLMs for the nuclear domain. We discuss\nthe challenges of data scarcity and privacy concerns inherent in the nuclear\nindustry and how synthetic data provides a solution by transforming existing\ntext data into usable Q&A pairs. This approach leverages LLMs to analyze text,\nextract key information, generate relevant questions, and evaluate the quality\nof the resulting synthetic dataset. By unlocking the potential of LLMs in the\nnuclear industry, synthetic data can pave the way for improved information\nretrieval, enhanced knowledge sharing, and more informed decision-making in\nthis critical sector.", "comment": null, "journal_ref": "44th Annual CNS Conference and the 49th Annual CNS/CNA Student\n  Conference, Westin Harbour Castle Hotel, Toronto, ON, Canada, June 8-11, 2025", "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.08750v1;http://arxiv.org/pdf/2506.08750v1", "pdf_url": "http://arxiv.org/pdf/2506.08750v1"}, {"title": "Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens", "link": "https://arxiv.org/abs/2506.08410", "details": "Z Ma, Q Yuan, Z Wang, D Zhou - arXiv preprint arXiv:2506.08410, 2025", "abstract": "\u2026 on the cognitive error detection capabilities of **Large** **Language** **Models** (LLMs), often prompting them to \u2026 **evaluation** of LLM meta-cognition using the current lenses and how to improve these lenses. Specifically, we propose AutoMeco, an \u2026", "entry_id": "http://arxiv.org/abs/2506.08410v1", "updated": "2025-06-10 03:30:10", "published": "2025-06-10 03:30:10", "authors": "Ziyang Ma;Qingyue Yuan;Zhenglin Wang;Deyu Zhou", "summary": "Previous research has primarily focused on the cognitive error detection\ncapabilities of Large Language Models (LLMs), often prompting them to analyze\nmistakes in reasoning chains. However, few studies have examined the\nmeta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),\nwhich are crucial for their reliability. While studies on LLM self-evaluation\npresent some measures, such as perplexity, which can reflect the answer\ncorrectness and be viewed as the lens of meta-cognition, they lack step-level\nanalysis and adaptation. This paper studies the evaluation of LLM\nmeta-cognition using the current lenses and how to improve these lenses.\nSpecifically, we propose AutoMeco, an Automated Meta-cognition Evaluation\nframework for benchmarking the existing lenses. Furthermore, a training-free\nMarkovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost\ncurrent meta-cognition lenses. Experimental results on three mathematical\nreasoning datasets and three LLMs show the reasonableness of AutoMeco by\ncomparing it with Best-of-N verification. Moreover, the meta-cognition ability\nof LLMs can be better evaluated using MIRA.", "comment": "Preprint", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.08410v1;http://arxiv.org/pdf/2506.08410v1", "pdf_url": "http://arxiv.org/pdf/2506.08410v1"}, {"title": "EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models", "link": "https://arxiv.org/abs/2506.08375", "details": "T Zou, X Zhang, H Yu, M Wang, F Huang, Y Li - arXiv preprint arXiv:2506.08375, 2025", "abstract": "\u2026 Abstract:With the development and widespread application of **large** **language** **models** (LLMs), \u2026 crafted to facilitate a more realistic and robust **evaluation** of LLMs. EIFBENCH not only includes \u2026 **Evaluations** on EIFBENCH have unveiled \u2026", "entry_id": "http://arxiv.org/abs/2506.08375v1", "updated": "2025-06-10 02:39:55", "published": "2025-06-10 02:39:55", "authors": "Tao Zou;Xinghua Zhang;Haiyang Yu;Minzheng Wang;Fei Huang;Yongbin Li", "summary": "With the development and widespread application of large language models\n(LLMs), the new paradigm of \"Model as Product\" is rapidly evolving, and demands\nhigher capabilities to address complex user needs, often requiring precise\nworkflow execution which involves the accurate understanding of multiple tasks.\nHowever, existing benchmarks focusing on single-task environments with limited\nconstraints lack the complexity required to fully reflect real-world scenarios.\nTo bridge this gap, we present the Extremely Complex Instruction Following\nBenchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and\nrobust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that\nenable comprehensive assessment across diverse task types concurrently, but\nalso integrates a variety of constraints, replicating complex operational\nenvironments. Furthermore, we propose the Segment Policy Optimization (SegPO)\nalgorithm to enhance the LLM's ability to accurately fulfill multi-task\nworkflow. Evaluations on EIFBENCH have unveiled considerable performance\ndiscrepancies in existing LLMs when challenged with these extremely complex\ninstructions. This finding underscores the necessity for ongoing optimization\nto navigate the intricate challenges posed by LLM applications.", "comment": "24 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.08375v1;http://arxiv.org/pdf/2506.08375v1", "pdf_url": "http://arxiv.org/pdf/2506.08375v1"}, {"title": "Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models", "link": "https://arxiv.org/pdf/2506.08147", "details": "M Usman, M Ahmad, MS Tash, I Gelbukh, RQ Tellez\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 This section presents the performance **evaluation** of the multilingual hate speech detection framework across English, Spanish, Urdu, and \u2026 -based models, and **large** **language** **models** (LLMs), with macro F1-score as the primary **evaluation** \u2026", "entry_id": "http://arxiv.org/abs/2506.08147v1", "updated": "2025-06-09 18:53:56", "published": "2025-06-09 18:53:56", "authors": "Muhammad Usman;Muhammad Ahmad;M. Shahiki Tash;Irina Gelbukh;Rolando Quintero Tellez;Grigori Sidorov", "summary": "Social media platforms are critical spaces for public discourse, shaping\nopinions and community dynamics, yet their widespread use has amplified harmful\ncontent, particularly hate speech, threatening online safety and inclusivity.\nWhile hate speech detection has been extensively studied in languages like\nEnglish and Spanish, Urdu remains underexplored, especially using\ntranslation-based approaches. To address this gap, we introduce a trilingual\ndataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and\nSpanish (3,162 samples), collected via keyword filtering, with a balanced\ndistribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology\nleverages attention layers as a precursor to transformer-based models and large\nlanguage models (LLMs), enhancing feature extraction for multilingual hate\nspeech detection. For non-transformer models, we use TF-IDF for feature\nextraction. The dataset is benchmarked using state-of-the-art models, including\nGPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models\nlike SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,\nfollowing rigorous guidelines, ensured high dataset quality, achieving a\nFleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5\nTurbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of\n0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for\nUrdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).\nThese results reflect improvements of 8.75% in English (over SVM baseline\n0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM\nbaseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline\n0.82). Our framework offers a robust solution for multilingual hate speech\ndetection, fostering safer digital communities worldwide.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2506.08147v1;http://arxiv.org/pdf/2506.08147v1", "pdf_url": "http://arxiv.org/pdf/2506.08147v1"}, {"title": "Enhancing Pulmonary Disease Prediction Using **Large Language Models** With Feature Summarization and Hybrid Retrieval-Augmented Generation: Multicenter \u2026", "link": "https://www.jmir.org/2025/1/e72638/", "details": "R Li, S Mao, C Zhu, Y Yang, C Tan, L Li, X Mu, H Liu\u2026 - Journal of Medical Internet \u2026, 2025", "abstract": "\u2026 As shown in Table 1, we **evaluated** multiple different prompt engineering strategies by combining \u2026 across these strategies provides a more comprehensive **evaluation** of the F-Sum + CoT + RAG \u2026 of different prompt engineering strategies \u2026"}, {"title": "From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?", "link": "https://arxiv.org/abs/2506.08295", "details": "Z Zhou, X Feng, Z Zhu, J Yao, S Koyejo, B Han - arXiv preprint arXiv:2506.08295, 2025", "abstract": "\u2026 Abstract:While existing benchmarks probe the reasoning abilities of **large** **language** **models** (LLMs\u2026 -Bench, a novel benchmark designed explicitly to **evaluate** an LLM's active reasoning skills. AR-\u2026 Empirical **evaluation** on AR-Bench \u2026", "entry_id": "http://arxiv.org/abs/2506.08295v1", "updated": "2025-06-09 23:56:41", "published": "2025-06-09 23:56:41", "authors": "Zhanke Zhou;Xiao Feng;Zhaocheng Zhu;Jiangchao Yao;Sanmi Koyejo;Bo Han", "summary": "While existing benchmarks probe the reasoning abilities of large language\nmodels (LLMs) across diverse domains, they predominantly assess passive\nreasoning, providing models with all the information needed to reach a\nsolution. By contrast, active reasoning-where an LLM must interact with\nexternal systems to acquire missing evidence or data-has received little\nsystematic attention. To address this shortfall, we present AR-Bench, a novel\nbenchmark designed explicitly to evaluate an LLM's active reasoning skills.\nAR-Bench comprises three task families-detective cases, situation puzzles, and\nguessing numbers-that together simulate real-world, agentic scenarios and\nmeasure performance across commonsense, logical, and symbolic reasoning\nchallenges. Empirical evaluation on AR-Bench demonstrates that contemporary\nLLMs exhibit pronounced difficulties with active reasoning: they frequently\nfail to acquire or leverage the information needed to solve tasks. This gap\nhighlights a stark divergence between their passive and active reasoning\nabilities. Moreover, ablation studies indicate that even advanced strategies,\nsuch as tree-based searching or post-training approaches, yield only modest\ngains and fall short of the levels required for real-world deployment.\nCollectively, these findings highlight the critical need to advance methodology\nfor active reasoning, e.g., incorporating interactive learning, real-time\nfeedback loops, and environment-aware objectives for training. The benchmark is\npublicly available at: https://github.com/tmlr-group/AR-Bench.", "comment": "Accepted by ICML 2025", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CL", "links": "http://arxiv.org/abs/2506.08295v1;http://arxiv.org/pdf/2506.08295v1", "pdf_url": "http://arxiv.org/pdf/2506.08295v1"}, {"title": "Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models", "link": "https://arxiv.org/pdf/2506.09002", "details": "B Chu, Y Feng, K Liu, H Shi, Z Nan, Z Guo, B Xu - arXiv preprint arXiv:2506.09002, 2025", "abstract": "\u2026 Recent work has increasingly utilized **large** **language** **models** (LLMs) to generate test cases, \u2026 This paper presents PALM, an approach that leverages **large** **language** **models** (LLMs) to \u2026 We implement the approach and **evaluate** it in 10 \u2026", "entry_id": "http://arxiv.org/abs/2506.09002v2", "updated": "2025-06-11 03:16:55", "published": "2025-06-10 17:21:21", "authors": "Bei Chu;Yang Feng;Kui Liu;Hange Shi;Zifan Nan;Zhaoqiang Guo;Baowen Xu", "summary": "Unit testing is essential for ensuring software reliability and correctness.\nClassic Search-Based Software Testing (SBST) methods and concolic\nexecution-based approaches for generating unit tests often fail to achieve high\ncoverage due to difficulties in handling complex program units, such as\nbranching conditions and external dependencies. Recent work has increasingly\nutilized large language models (LLMs) to generate test cases, improving the\nquality of test generation by providing better context and correcting errors in\nthe model's output. However, these methods rely on fixed prompts, resulting in\nrelatively low compilation success rates and coverage. This paper presents\nPALM, an approach that leverages large language models (LLMs) to enhance the\ngeneration of high-coverage unit tests. PALM performs program analysis to\nidentify branching conditions within functions, which are then combined into\npath constraints. These constraints and relevant contextual information are\nused to construct prompts that guide the LLMs in generating unit tests. We\nimplement the approach and evaluate it in 10 open-source Rust crates.\nExperimental results show that within just two or three hours, PALM can\nsignificantly improves test coverage compared to classic methods, with\nincreases in overall project coverage exceeding 50% in some instances and its\ngenerated tests achieving an average coverage of 75.77%, comparable to human\neffort (71.30%), highlighting the potential of LLMs in automated test\ngeneration. We submitted 91 PALM-generated unit tests targeting new code. Of\nthese submissions, 80 were accepted, 5 were rejected, and 6 remain pending\nreview. The results demonstrate the effectiveness of integrating program\nanalysis with AI and open new avenues for future research in automated software\ntesting.", "comment": "10 pages, 5 figures", "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE", "links": "http://arxiv.org/abs/2506.09002v2;http://arxiv.org/pdf/2506.09002v2", "pdf_url": "http://arxiv.org/pdf/2506.09002v2"}]
