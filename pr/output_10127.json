[{"title": "Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers", "link": "https://arxiv.org/pdf/2412.00142", "details": "C Mitra, B Huang, T Chai, Z Lin, A Arbelle, R Feris\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a wide variety of vision-language (VL) tasks such as image captioning or visual question answering. Despite strong performance, LMMs are not directly suited for \u2026"}, {"title": "COSMOS: Cross-Modality Self-Distillation for Vision Language Pre-training", "link": "https://arxiv.org/pdf/2412.01814", "details": "S Kim, R Xiao, MI Georgescu, S Alaniz, Z Akata - arXiv preprint arXiv:2412.01814, 2024", "abstract": "Vision-Language Models (VLMs) trained with contrastive loss have achieved significant advancements in various vision and language tasks. However, the global nature of contrastive loss makes VLMs focus predominantly on foreground objects \u2026"}, {"title": "Liquid: Language Models are Scalable Multi-modal Generators", "link": "https://arxiv.org/pdf/2412.04332", "details": "J Wu, Y Jiang, C Ma, Y Liu, H Zhao, Z Yuan, S Bai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Liquid, an auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared \u2026"}, {"title": "Assessing and Learning Alignment of Unimodal Vision and Language Models", "link": "https://arxiv.org/pdf/2412.04616", "details": "L Zhang, Q Yang, A Agrawal - arXiv preprint arXiv:2412.04616, 2024", "abstract": "How well are unimodal vision and language models aligned? Although prior work have approached answering this question, their assessment methods do not directly translate to how these models are used in practical vision-language tasks. In this \u2026"}, {"title": "VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models", "link": "https://arxiv.org/pdf/2411.17451", "details": "L Li, Y Wei, Z Xie, X Yang, Y Song, P Wang, C An, T Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language generative reward models (VL-GenRMs) play a crucial role in aligning and evaluating multimodal AI systems, yet their own evaluation remains under-explored. Current assessment methods primarily rely on AI-annotated \u2026"}, {"title": "Enhancing Instruction-Following Capability of Visual-Language Models by Reducing Image Redundancy", "link": "https://arxiv.org/pdf/2411.15453", "details": "T Yang, J Jia, X Zhu, W Zhao, B Wang, Y Cheng, Y Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have strong instruction-following capability to interpret and execute tasks as directed by human commands. Multimodal Large Language Models (MLLMs) have inferior instruction-following ability compared to \u2026"}, {"title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference", "link": "https://arxiv.org/pdf/2412.04652", "details": "X Pei, T Huang, C Xu - arXiv preprint arXiv:2412.04652, 2024", "abstract": "KV cache pruning has emerged as a promising technique for reducing memory and computation costs in long-context auto-regressive generation. Existing methods for vision-language models (VLMs) typically rely on self-attention scores from large \u2026"}, {"title": "Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents", "link": "https://arxiv.org/pdf/2411.16740", "details": "J Chen, D Xu, J Fei, CM Feng, M Elhoseiny - arXiv preprint arXiv:2411.16740, 2024", "abstract": "Large multimodal models (LMMs) have achieved impressive progress in vision- language understanding, yet they face limitations in real-world applications requiring complex reasoning over a large number of images. Existing benchmarks for multi \u2026"}, {"title": "Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks", "link": "https://arxiv.org/pdf/2412.06224", "details": "J Zhang, K Wang, S Wang, M Li, H Liu, S Wei, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "A practical navigation agent must be capable of handling a wide range of interaction demands, such as following instructions, searching objects, answering questions, tracking people, and more. Existing models for embodied navigation fall short of \u2026"}]
