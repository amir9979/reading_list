[{"title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?", "link": "https://arxiv.org/pdf/2410.23856", "details": "Z Zhou, R Tao, J Zhu, Y Luo, Z Wang, B Han - arXiv preprint arXiv:2410.23856, 2024", "abstract": "This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We \u2026"}, {"title": "Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text", "link": "https://aclanthology.org/2024.conll-1.27.pdf", "details": "S Adak, D Agrawal, A Mukherjee, S Aditya - Proceedings of the 28th Conference on \u2026, 2024", "abstract": "We investigate the knowledge of object affordances in pre-trained language models (LMs) and pre-trained Vision-Language models (VLMs). A growing body of literature shows that PTLMs fail inconsistently and non-intuitively, demonstrating a lack of \u2026"}, {"title": "metaTextGrad: Learning to learn with language models as optimizers", "link": "https://openreview.net/pdf%3Fid%3DyzieYIT9hu", "details": "G Xu, M Yuksekgonul, C Guestrin, J Zou - Adaptive Foundation Models: Evolving AI for \u2026", "abstract": "Large language models (LLMs) are increasingly used in learning algorithms, evaluations, and optimization tasks. Recent studies have shown that incorporating self-criticism into LLMs can significantly enhance model performance, with \u2026"}, {"title": "From One to Zero: RAG-IM Adapts Language Models for Interpretable Zero-Shot Clinical Predictions", "link": "https://openreview.net/pdf%3Fid%3D3OYjWzqqC1", "details": "S Mahbub, C Ellington, S Alinejad, K Wen, Y Luo\u2026 - Adaptive Foundation Models \u2026", "abstract": "Clinical machine learning models must adapt to new settings such as different hospitals, clinicians, or patient populations. These differing environments present related but subtly distinct tasks, where diseases and medical interventions share \u2026"}, {"title": "Weakly Supervised Language Models for Automated Extraction of Critical Findings from Radiology Reports", "link": "https://www.researchsquare.com/article/rs-5060695/latest.pdf", "details": "A Das, I Talati, JMZ Chaves, D Rubin, I Banerjee - 2024", "abstract": "Critical findings in radiology reports are life threatening conditions that need to be communicated promptly to physicians (\u201ccritical findings\u201d) for timely man-agement of patients. Flagging radiology reports of such incidents could facilitate opportune \u2026"}, {"title": "Axes of Robustness of Neural Language Models", "link": "https://is.muni.cz/th/m805b/PhD_thesis_Michal_Stefanik.pdf", "details": "M \u0160TEF\u00c1NIK", "abstract": "In recent years, language models have emerged into a technology adopted in a wide variety of applications, nowadays largely exceeding traditional natural language processing tasks. Thanks to their versatility and adaptability, modern language \u2026"}, {"title": "Large Language Models Know What is Key Visual Entity: An LLM-assisted Multimodal Retrieval for VQA", "link": "https://aclanthology.org/2024.emnlp-main.613.pdf", "details": "P Jian, D Yu, J Zhang - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Visual question answering (VQA) tasks, often performed by visual language model (VLM), face challenges with long-tail knowledge. Recent retrieval-augmented VQA (RA-VQA) systems address this by retrieving and integrating external knowledge \u2026"}, {"title": "Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings", "link": "https://arxiv.org/pdf/2410.22685", "details": "YS Grewal, EV Bonilla, TD Bui - arXiv preprint arXiv:2410.22685, 2024", "abstract": "Accurately quantifying uncertainty in large language models (LLMs) is crucial for their reliable deployment, especially in high-stakes applications. Current state-of-the- art methods for measuring semantic uncertainty in LLMs rely on strict bidirectional \u2026"}, {"title": "How Far Can We Extract Diverse Perspectives from Large Language Models?", "link": "https://aclanthology.org/2024.emnlp-main.306.pdf", "details": "S Hayati, M Lee, D Rajagopal, D Kang - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "Collecting diverse human opinions is costly and challenging. This leads to a recent trend in exploiting large language models (LLMs) for generating diverse data for potential scalable and efficient solutions. However, the extent to which LLMs can \u2026"}]
