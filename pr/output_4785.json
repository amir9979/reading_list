[{"title": "An Innovative Ensemble Deep Learning Clinical Decision Support System for Diabetes Prediction", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10620301.pdf", "details": "MS Al Reshan, S Amin, MA Zeb, A Sulaiman\u2026 - IEEE Access, 2024", "abstract": "Diabetes is a significant global health concern, with an increasing number of individuals at risk. It leads to a significant number of fatalities annually. The early prediction of diabetes is vital for preventing complications and improving patient \u2026"}, {"title": "L-AutoDA: Large Language Models for Automatically Evolving Decision-based Adversarial Attacks", "link": "https://dl.acm.org/doi/abs/10.1145/3638530.3664121", "details": "P Guo, F Liu, X Lin, Q Zhao, Q Zhang - Proceedings of the Genetic and Evolutionary \u2026, 2024", "abstract": "In the rapidly evolving field of machine learning, adversarial attacks pose a significant threat to the robustness and security of models. Amongst these, decision- based attacks are particularly insidious due to their nature of requiring only the \u2026"}, {"title": "On Speeding Up Language Model Evaluation", "link": "https://arxiv.org/pdf/2407.06172", "details": "JP Zhou, CK Belardi, R Wu, T Zhang, CP Gomes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) currently dominate the field of natural language processing (NLP), representing the state-of-the-art across a diverse array of tasks. Developing a model of this nature, from training to inference, requires making \u2026"}, {"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "link": "https://arxiv.org/pdf/2408.00724", "details": "Y Wu, Z Sun, S Li, S Welleck, Y Yang - arXiv preprint arXiv:2408.00724, 2024", "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth \u2026"}, {"title": "Strong Copyright Protection for Language Models via Adaptive Model Fusion", "link": "https://arxiv.org/pdf/2407.20105", "details": "J Abad, K Donhauser, F Pinto, F Yang - arXiv preprint arXiv:2407.20105, 2024", "abstract": "The risk of language models unintentionally reproducing copyrighted material from their training data has led to the development of various protective measures. In this paper, we propose model fusion as an effective solution to safeguard against \u2026"}, {"title": "DECIDER: Leveraging Foundation Model Priors for Improved Model Failure Detection and Explanation", "link": "https://arxiv.org/pdf/2408.00331", "details": "R Subramanyam, K Thopalli, V Narayanaswamy\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Reliably detecting when a deployed machine learning model is likely to fail on a given input is crucial for ensuring safe operation. In this work, we propose DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel approach that leverages \u2026"}, {"title": "SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training", "link": "https://arxiv.org/pdf/2407.06654", "details": "N He, W Xiong, H Liu, Y Liao, L Ding, K Zhang, G Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The effectiveness of large language models (LLMs) is often hindered by duplicated data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, which risks the loss of valuable information and \u2026"}, {"title": "Pruning Large Language Models to Intra-module Low-rank Architecture with Transitional Activations", "link": "https://arxiv.org/pdf/2407.05690", "details": "B Shen, Z Lin, D Zha, W Liu, J Luan, B Wang, W Wang - arXiv preprint arXiv \u2026, 2024", "abstract": "Structured pruning fundamentally reduces computational and memory overheads of large language models (LLMs) and offers a feasible solution for end-side LLM deployment. Structurally pruned models remain dense and high-precision, highly \u2026"}, {"title": "Generalization vs Memorization: Tracing Language Models' Capabilities Back to Pretraining Data", "link": "https://arxiv.org/pdf/2407.14985", "details": "A Antoniades, X Wang, Y Elazar, A Amayuelas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the proven utility of large language models (LLMs) in real-world applications, there remains a lack of understanding regarding how they leverage their large-scale pretraining text corpora to achieve such capabilities. In this work, we investigate the \u2026"}]
