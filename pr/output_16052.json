[{"title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation", "link": "https://arxiv.org/pdf/2504.20859", "details": "G Hadad, H Roitman, Y Eshel, B Shapira, L Rokach - arXiv preprint arXiv:2504.20859, 2025", "abstract": "As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents``X-Cross''--a novel cross-domain sequential-recommendation model \u2026"}, {"title": "COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts", "link": "https://arxiv.org/pdf/2504.10158%3F", "details": "J Li, X Zhang, H Zou, Y Guo, R Xu, Y Liu, C Zhu, Y He\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Current object detectors often suffer significant perfor-mance degradation in real- world applications when encountering distributional shifts. Consequently, the out-of- distribution (OOD) generalization capability of object detectors has garnered \u2026"}, {"title": "Future Sight: Fine-Tuning Language Models for Dynamic Story Generation", "link": "https://link.springer.com/chapter/10.1007/978-3-031-90167-6_16", "details": "B Zimmerman, G Sahu, O Vechtomova - \u2026 Intelligence in Music, Sound, Art and Design \u2026, 2025", "abstract": "The recent surge in the development of attention mechanisms has made it possible for language models (LMs) to produce text on par with humans. Unfortunately, the autoregressive nature of attention-based LM decoders inhibits them from attending to \u2026"}, {"title": "DeepCritic: Deliberate Critique with Large Language Models", "link": "https://arxiv.org/pdf/2505.00662", "details": "W Yang, J Chen, Y Lin, JR Wen - arXiv preprint arXiv:2505.00662, 2025", "abstract": "As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a \u2026"}, {"title": "Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study", "link": "https://arxiv.org/pdf/2504.16414", "details": "M Khodadad, AS Kasmaee, M Astaraki, N Sherck\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a \u2026"}, {"title": "A Temporal Knowledge Graph Generation Dataset Supervised Distantly by Large Language Models", "link": "https://www.nature.com/articles/s41597-025-05062-0", "details": "J Zhu, Y Fu, J Zhou, D Chen - Scientific Data, 2025", "abstract": "Abstract Knowledge graphs can be constructed by extracting triples from documents, which denotes document-level relation extraction. Each triple illustrates a fact composed of two entities and a relation. However, temporal information \u2026"}, {"title": "ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data", "link": "https://arxiv.org/pdf/2504.16628", "details": "H Gu, H Wang, Y Mei, M Zhang, Y Jin - arXiv preprint arXiv:2504.16628, 2025", "abstract": "Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm \u2026"}, {"title": "prompt4vis: prompting large language models with example mining for tabular data visualization", "link": "https://link.springer.com/article/10.1007/s00778-025-00912-0", "details": "S Li, X Chen, Y Song, Y Song, CJ Zhang, F Hao\u2026 - The VLDB Journal, 2025", "abstract": "We are currently in the epoch of Large Language Models (LLMs), which have transformed numerous technological domains within the database community. In this paper, we examine the application of LLMs in text-to-visualization (text-to-vis). The \u2026"}, {"title": "Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics", "link": "https://arxiv.org/pdf/2504.17665", "details": "Z Al-Khalili, N Howell, D Klakow - arXiv preprint arXiv:2504.17665, 2025", "abstract": "Assisting LLMs with code generation improved their performance on mathematical reasoning tasks. However, the evaluation of code-assisted LLMs is generally restricted to execution correctness, lacking a rigorous evaluation of their generated \u2026"}]
