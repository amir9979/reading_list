[{"title": "Process-based Self-Rewarding Language Models", "link": "https://arxiv.org/pdf/2503.03746", "details": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' \u2026"}, {"title": "KGGen: Extracting Knowledge Graphs from Plain Text with Language Models", "link": "https://arxiv.org/pdf/2502.09956", "details": "B Mo, K Yu, J Kazdan, P Mpala, L Yu, C Cundy\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent interest in building foundation models for KGs has highlighted a fundamental challenge: knowledge-graph data is relatively scarce. The best-known KGs are primarily human-labeled, created by pattern-matching, or extracted using early NLP \u2026"}, {"title": "Balcony: A Lightweight Approach to Dynamic Inference of Generative Language Models", "link": "https://arxiv.org/pdf/2503.05005", "details": "B Jamialahmadi, P Kavehzadeh, M Rezagholizadeh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Deploying large language models (LLMs) in real-world applications is often hindered by strict computational and latency constraints. While dynamic inference offers the flexibility to adjust model behavior based on varying resource budgets, existing \u2026"}, {"title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "link": "https://arxiv.org/pdf/2502.21321", "details": "K Kumar, T Ashraf, O Thawakar, RM Anwer\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have transformed the natural language processing landscape and brought to life diverse applications. Pretraining on vast web-scale data has laid the foundation for these models, yet the research community is now \u2026"}, {"title": "Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks", "link": "https://arxiv.org/pdf/2503.04833", "details": "L Lu, S Pang, S Liang, H Zhu, X Zeng, A Liu, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal large language models (MLLMs) have made remarkable strides in cross- modal comprehension and generation tasks. However, they remain vulnerable to jailbreak attacks, where crafted perturbations bypass security guardrails and elicit \u2026"}, {"title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems", "link": "https://arxiv.org/pdf/2503.03686", "details": "R Ye, S Tang, R Ge, Y Du, Z Yin, S Chen, J Shao - arXiv preprint arXiv:2503.03686, 2025", "abstract": "LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability \u2026"}, {"title": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs", "link": "https://arxiv.org/pdf/2503.05139", "details": "L Team, B Zeng, C Huang, C Zhang, C Tian, C Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this technical report, we tackle the challenges of training large-scale Mixture of Experts (MoE) models, focusing on overcoming cost inefficiency and resource limitations prevalent in such systems. To address these issues, we present two \u2026"}, {"title": "Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety", "link": "https://arxiv.org/pdf/2503.05021", "details": "Y Zhang, M Li, W Han, Y Yao, Z Cen, D Zhao - arXiv preprint arXiv:2503.05021, 2025", "abstract": "Large Language Models (LLMs) are vulnerable to jailbreak attacks that exploit weaknesses in traditional safety alignment, which often relies on rigid refusal heuristics or representation engineering to block harmful outputs. While they are \u2026"}, {"title": "Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models", "link": "https://arxiv.org/pdf/2503.01742", "details": "A Purpura, S Wadhwa, J Zymet, A Gupta, A Luo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The rapid growth of Large Language Models (LLMs) presents significant privacy, security, and ethical concerns. While much research has proposed methods for defending LLM systems against misuse by malicious actors, researchers have \u2026"}]
