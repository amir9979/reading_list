[{"title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks", "link": "https://arxiv.org/pdf/2504.01308", "details": "J Wang, Y Zuo, Y Chai, Z Liu, Y Fu, Y Feng, K Lam - arXiv preprint arXiv:2504.01308, 2025", "abstract": "Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing \u2026"}, {"title": "From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.06260", "details": "M Dai, J Sun, Z Zhao, S Liu, R Li, J Gao, X Li - arXiv preprint arXiv:2503.06260, 2025", "abstract": "Aligning large vision-language models (LVLMs) with human preferences is challenging due to the scarcity of fine-grained, high-quality, and multimodal preference data without human annotations. Existing methods relying on direct \u2026"}, {"title": "No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language models", "link": "https://arxiv.org/pdf/2503.11985", "details": "CV Kumar, A Urlana, G Kanumolu, BM Garlapati\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Advancements in Large Language Models (LLMs) have increased the performance of different natural language understanding as well as generation tasks. Although LLMs have breached the state-of-the-art performance in various tasks, they often \u2026"}, {"title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding", "link": "https://arxiv.org/pdf/2503.05061", "details": "M Krumdick, C Lovering, V Reddy, S Ebner, C Tanner - arXiv preprint arXiv \u2026, 2025", "abstract": "LLM-as-a-Judge is a framework that uses an LLM (large language model) to evaluate the quality of natural language text-typically text that is also generated by an LLM. This framework holds great promise due to its relative low-cost, ease of use \u2026"}, {"title": "Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching", "link": "https://arxiv.org/pdf/2503.05179%3F", "details": "SA Aytes, J Baek, SJ Hwang - arXiv preprint arXiv:2503.05179, 2025", "abstract": "Recent advances in large language models have demonstrated remarkable reasoning capabilities through Chain of Thought (CoT) prompting, but often at the cost of excessive verbosity in their intermediate outputs, which increases \u2026"}, {"title": "\" Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding", "link": "https://arxiv.org/pdf/2503.10167", "details": "H Jin, JW Yeom, S Bae, T Kim - arXiv preprint arXiv:2503.10167, 2025", "abstract": "Large language models (LLMs) exhibit strong reasoning abilities, often attributed to few-shot or zero-shot chain-of-thought (CoT) prompting. While effective, these methods require labor-intensive prompt engineering, raising the question of whether \u2026"}, {"title": "A survey on post-training of large language models", "link": "https://arxiv.org/pdf/2503.06072", "details": "G Tie, Z Zhao, D Song, F Wei, R Zhou, Y Dai, W Yin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained \u2026"}, {"title": "Benchmarking Reasoning Robustness in Large Language Models", "link": "https://arxiv.org/pdf/2503.04550", "details": "T Yu, Y Jing, X Zhang, W Jiang, W Wu, Y Wang, W Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite the recent success of large language models (LLMs) in reasoning such as DeepSeek, we for the first time identify a key dilemma in reasoning robustness and generalization: significant performance degradation on novel or incomplete data \u2026"}, {"title": "A Weighted Cross-entropy Loss for Mitigating LLM Hallucinations in Cross-lingual Continual Pretraining", "link": "https://ieeexplore.ieee.org/abstract/document/10888877/", "details": "Y Fan, R Li, G Zhang, C Shi, X Wang - \u2026 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "Recently, due to the explosive advances of large language models (LLMs) on English, cross-lingual continual pretraining has been widely applied in obtaining Chinese LLMs. However, previous studies showed that these LLMs have suffered \u2026"}]
