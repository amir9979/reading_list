[{"title": "Few-shot Adaptation of Medical Vision-Language Models", "link": "https://arxiv.org/pdf/2409.03868", "details": "F Shakeri, Y Huang, J Silva-Rodr\u00edguez, H Bahig\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Integrating image and text data through multi-modal learning has emerged as a new approach in medical imaging research, following its successful deployment in computer vision. While considerable efforts have been dedicated to establishing \u2026"}, {"title": "Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis", "link": "https://arxiv.org/pdf/2409.00106", "details": "A Nagar, S Jaiswal, C Tan - arXiv preprint arXiv:2409.00106, 2024", "abstract": "Vision-language models (VLMs) have shown impressive zero-and few-shot performance on real-world visual question answering (VQA) benchmarks, alluding to their capabilities as visual reasoning engines. However, the benchmarks being used \u2026"}, {"title": "Designing Retrieval-Augmented Language Models for Clinical Decision", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DWcMbEQAAQBAJ%26oi%3Dfnd%26pg%3DPA159%26ots%3DtCwXsZRCat%26sig%3DS8CqflPFNjWX98qn6gS-_DmZx2w", "details": "K Quigley, T Koker, J Taylor, V Mancuso - AI for Health Equity and Fairness: Leveraging AI to \u2026", "abstract": "Ever-increasing demands for physician expertise drive the need for trust-worthy point- of-care tools that can help aid decision-making in all clinical settings. Retrieval- augmented language models carry potential to relieve the information burden on \u2026"}, {"title": "Language Models Don't Learn the Physical Manifestation of Language", "link": "https://aclanthology.org/2024.acl-long.195.pdf", "details": "B Lee, J Lim - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "We argue that language-only models don't learn the physical manifestation of language. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-Test. These tasks highlight a \u2026"}, {"title": "Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability", "link": "https://arxiv.org/pdf/2408.07852", "details": "J Hron, L Culp, G Elsayed, R Liu, B Adlam, M Bileschi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While many capabilities of language models (LMs) improve with increased training budget, the influence of scale on hallucinations is not yet fully understood. Hallucinations come in many forms, and there is no universally accepted definition \u2026"}, {"title": "Boosting entity recognition by leveraging cross-task domain models for weak supervision", "link": "https://www.amazon.science/publications/boosting-entity-recognition-by-leveraging-cross-task-domain-models-for-weak-supervision", "details": "S Agrawal, S Merugu, V Sembium - 2024", "abstract": "Entity Recognition (ER) is a common natural language processing task encountered in a number of real-world applications. For common domains and named entities such as places and organisations, there exists sufficient high quality annotated data \u2026"}, {"title": "E-code: Mastering Efficient Code Generation through Pretrained Models and Expert Encoder Group", "link": "https://arxiv.org/pdf/2408.12948", "details": "Y Pan, C Lyu, Z Yang, L Li, Q Liu, X Shao - arXiv preprint arXiv:2408.12948, 2024", "abstract": "Context: With the waning of Moore's Law, the software industry is placing increasing importance on finding alternative solutions for continuous performance enhancement. The significance and research results of software performance \u2026"}, {"title": "The advantages of context specific language models: the case of the Erasmian Language Model", "link": "https://arxiv.org/pdf/2408.06931", "details": "J Gon\u00e7alves, N Jelicic, M Murgia, E Stamhuis - arXiv preprint arXiv:2408.06931, 2024", "abstract": "The current trend to improve language model performance seems to be based on scaling up with the number of parameters (eg the state of the art GPT4 model has approximately 1.7 trillion parameters) or the amount of training data fed into the \u2026"}, {"title": "Cyclical Contrastive Learning Based on Geodesic for Zero-shot Cross-lingual Spoken Language Understanding", "link": "https://aclanthology.org/2024.findings-acl.106.pdf", "details": "X Cheng, Z Zhu, B Yang, X Zhuang, H Li, Y Zou - Findings of the Association for \u2026, 2024", "abstract": "Owing to the scarcity of labeled training data, Spoken Language Understanding (SLU) is still a challenging task in low-resource languages. Therefore, zero-shot cross-lingual SLU attracts more and more attention. Contrastive learning is widely \u2026"}]
