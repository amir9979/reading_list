[{"title": "Effective prompt extraction from language models", "link": "https://openreview.net/pdf%3Fid%3D0o95CVdNuz", "details": "Y Zhang, N Carlini, D Ippolito - First Conference on Language Modeling, 2024", "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden \u2026"}, {"title": "Understanding Defects in Generated Codes by Language Models", "link": "https://arxiv.org/pdf/2408.13372", "details": "AM Esfahani, N Kahani, SA Ajila - arXiv preprint arXiv:2408.13372, 2024", "abstract": "This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the \u2026"}, {"title": "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws", "link": "https://arxiv.org/pdf/2408.08310", "details": "R Li, Y Wei, M Zhang, N Yu, H Hu, H Peng - arXiv preprint arXiv:2408.08310, 2024", "abstract": "High-quality data is crucial for the pre-training performance of large language models. Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity. In \u2026"}, {"title": "MobileQuant: Mobile-friendly Quantization for On-device Language Models", "link": "https://arxiv.org/pdf/2408.13933", "details": "F Tan, R Lee, \u0141 Dudziak, SX Hu, S Bhattacharya\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have revolutionized language processing, delivering outstanding results across multiple applications. However, deploying LLMs on edge devices poses several challenges with respect to memory, energy, and compute \u2026"}, {"title": "LLMEmbed: Rethinking Lightweight LLM's Genuine Function in Text Classification", "link": "https://aclanthology.org/2024.acl-long.433.pdf", "details": "CL ChunLiu, H Zhang, K Zhao, X Ju, L Yang - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "With the booming of Large Language Models (LLMs), prompt-learning has become a promising method mainly researched in various research areas. Recently, many attempts based on prompt-learning have been made to improve the performance of \u2026"}, {"title": "Persona is a Double-edged Sword: Enhancing the Zero-shot Reasoning by Ensembling the Role-playing and Neutral Prompts", "link": "https://arxiv.org/pdf/2408.08631", "details": "J Kim, N Yang, K Jung - arXiv preprint arXiv:2408.08631, 2024", "abstract": "Recent studies demonstrate that prompting an appropriate role-playing persona to an LLM improves its reasoning capability. However, assigning a proper persona is difficult since an LLM's performance is extremely sensitive to assigned prompts; \u2026"}, {"title": "Tad: A plug-and-play task-aware decoding method to better adapt llms on downstream tasks", "link": "https://www.ijcai.org/proceedings/2024/0728.pdf", "details": "X Xu, H Chen, Z Lin, J Han, L Gong, G Wang, Y Bao\u2026 - Proceedings of the Thirty \u2026, 2024", "abstract": "Fine-tuning pre-trained models on downstream tasks is a common practice in leveraging large language models (LLMs) today. A critical issue is how to adapt pre- trained models to downstream tasks better, thereby enhancing their performance \u2026"}, {"title": "Monotonic Representation of Numeric Attributes in Language Models", "link": "https://aclanthology.org/2024.acl-short.18.pdf", "details": "B Heinzerling, K Inui - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Abstract Language models (LMs) can express factual knowledge involving numeric properties such as Karl Popper was born in 1902. However, how this information is encoded in the model's internal representations is not understood well. Here, we \u2026"}, {"title": "CogLM: Tracking Cognitive Development of Large Language Models", "link": "https://arxiv.org/pdf/2408.09150", "details": "X Wang, P Yuan, S Feng, Y Li, B Pan, H Wang, Y Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Piaget's Theory of Cognitive Development (PTC) posits that the development of cognitive levels forms the foundation for human learning across various abilities. As Large Language Models (LLMs) have recently shown remarkable abilities across a \u2026"}]
