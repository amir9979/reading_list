[{"title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision", "link": "https://arxiv.org/pdf/2405.04086", "details": "Y Tong, S Wang, D Li, Y Wang, S Han, Z Lin, C Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations \u2026"}, {"title": "Causal Evaluation of Language Models", "link": "https://arxiv.org/pdf/2405.00622", "details": "S Chen, B Peng, M Chen, R Wang, M Xu, X Zeng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for \u2026"}, {"title": "MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning", "link": "https://arxiv.org/pdf/2405.03000", "details": "W Shi, R Xu, Y Zhuang, Y Yu, H Wu, C Yang, MD Wang - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their improved capabilities in generation and reasoning, adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and corporate privacy. In this work, we propose MedAdapter, a unified \u2026"}, {"title": "Bridging Operator Learning and Conditioned Neural Fields: A Unifying Perspective", "link": "https://arxiv.org/pdf/2405.13998", "details": "S Wang, JH Seidman, S Sankaran, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Operator learning is an emerging area of machine learning which aims to learn mappings between infinite dimensional function spaces. Here we uncover a connection between operator learning architectures and conditioned neural fields \u2026"}, {"title": "MetaEarth: A Generative Foundation Model for Global-Scale Remote Sensing Image Generation", "link": "https://arxiv.org/pdf/2405.13570", "details": "Z Yu, C Liu, L Liu, Z Shi, Z Zou - arXiv preprint arXiv:2405.13570, 2024", "abstract": "The recent advancement of generative foundational models has ushered in a new era of image generation in the realm of natural images, revolutionizing art design, entertainment, environment simulation, and beyond. Despite producing high-quality \u2026"}, {"title": "Flow Priors for Linear Inverse Problems via Iterative Corrupted Trajectory Matching", "link": "https://arxiv.org/pdf/2405.18816", "details": "Y Zhang, P Yu, Y Zhu, Y Chang, F Gao, YN Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generative models based on flow matching have attracted significant attention for their simplicity and superior performance in high-resolution image synthesis. By leveraging the instantaneous change-of-variables formula, one can directly compute \u2026"}, {"title": "Matryoshka Query Transformer for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2405.19315", "details": "W Hu, ZY Dou, LH Li, A Kamath, N Peng, KW Chang - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) typically encode an image into a fixed number of visual tokens (eg, 576) and process these tokens with a language model. Despite their strong performance, LVLMs face challenges in adapting to varying \u2026"}, {"title": "Autonomous Data Selection with Language Models for Mathematical Texts", "link": "https://openreview.net/pdf%3Fid%3DbBF077z8LF", "details": "Y Zhang, Y Luo, Y Yuan, AC Yao - ICLR 2024 Workshop on Navigating and \u2026, 2024", "abstract": "To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or \u2026"}, {"title": "Disease-informed Adaptation of Vision-Language Models", "link": "https://arxiv.org/pdf/2405.15728", "details": "J Zhang, G Wang, MK Kalra, P Yan - arXiv preprint arXiv:2405.15728, 2024", "abstract": "In medical image analysis, the expertise scarcity and the high cost of data annotation limits the development of large artificial intelligence models. This paper investigates the potential of transfer learning with pre-trained vision-language models (VLMs) in \u2026"}]
