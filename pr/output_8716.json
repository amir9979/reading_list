[{"title": "A General-Purpose Multimodal Foundation Model for Dermatology", "link": "https://arxiv.org/pdf/2410.15038", "details": "S Yan, Z Yu, C Primiero, C Vico-Alonso, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Diagnosing and treating skin diseases require advanced visual skills across multiple domains and the ability to synthesize information from various imaging modalities. Current deep learning models, while effective at specific tasks such as diagnosing \u2026"}, {"title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models", "link": "https://arxiv.org/pdf/2410.18252", "details": "M Noukhovitch, S Huang, S Xhonneux, A Hosseini\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this \u2026"}, {"title": "Multi-granularity Semantic Guided Transformer for Radiology Report Generation", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9437-9_36", "details": "Y Song, X Hua, K Zhang, H Zan, R Li - \u2026 on Natural Language Processing and Chinese \u2026, 2024", "abstract": "Abstract Radiology Report Generation aims to generate accurate diagnostic reports based on medical images. Existing approaches based on the Transformer paradigm and grid features had achieved significant performance. However, this paradigm \u2026"}, {"title": "You Don't Need Domain-Specific Data Augmentations When Scaling Self-Supervised Learning", "link": "https://openreview.net/pdf%3Fid%3D7RwKMRMNrc", "details": "T Moutakanni, M Oquab, M Szafraniec\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Self-Supervised learning (SSL) with Joint-Embedding Architectures (JEA) has led to outstanding performances. All instantiations of this paradigm were trained using strong and well-established hand-crafted data augmentations, leading to the general \u2026"}, {"title": "EchoApex: A General-Purpose Vision Foundation Model for Echocardiography", "link": "https://arxiv.org/pdf/2410.11092", "details": "AA Amadou, Y Zhang, S Piat, P Klein, I Schmuecking\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Quantitative evaluation of echocardiography is essential for precise assessment of cardiac condition, monitoring disease progression, and guiding treatment decisions. The diverse nature of echo images, including variations in probe types \u2026"}, {"title": "Large-Scale 3D Medical Image Pre-training with Geometric Context Priors", "link": "https://arxiv.org/pdf/2410.09890", "details": "L Wu, J Zhuang, H Chen - arXiv preprint arXiv:2410.09890, 2024", "abstract": "The scarcity of annotations poses a significant challenge in medical image analysis. Large-scale pre-training has emerged as a promising label-efficient solution, owing to the utilization of large-scale data, large models, and advanced pre-training \u2026"}, {"title": "MI-VisionShot: Few-shot adaptation of vision-language models for slide-level classification of histopathological images", "link": "https://arxiv.org/pdf/2410.15881", "details": "P Meseguer, R del Amor, V Naranjo - arXiv preprint arXiv:2410.15881, 2024", "abstract": "Vision-language supervision has made remarkable strides in learning visual representations from textual guidance. In digital pathology, vision-language models (VLM), pre-trained on curated datasets of histological image-captions, have been \u2026"}, {"title": "Foundation Models for Slide-level Cancer Subtyping in Digital Pathology", "link": "https://arxiv.org/pdf/2410.15886", "details": "P Meseguer, R del Amor, A Colomer, V Naranjo - arXiv preprint arXiv:2410.15886, 2024", "abstract": "Since the emergence of the ImageNet dataset, the pretraining and fine-tuning approach has become widely adopted in computer vision due to the ability of ImageNet-pretrained models to learn a wide variety of visual features. However, a \u2026"}, {"title": "Exploring the Reliability of Foundation Model-Based Frontier Selection in Zero-Shot Object Goal Navigation", "link": "https://arxiv.org/pdf/2410.21037", "details": "S Yuan, HU Unlu, H Huang, C Wen, A Tzes, Y Fang - arXiv preprint arXiv:2410.21037, 2024", "abstract": "In this paper, we present a novel method for reliable frontier selection in Zero-Shot Object Goal Navigation (ZS-OGN), enhancing robotic navigation systems with foundation models to improve commonsense reasoning in indoor environments. Our \u2026"}]
