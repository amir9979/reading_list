[{"title": "Improving Deep Assertion Generation via Fine-Tuning Retrieval-Augmented Pre-trained Language Models", "link": "https://arxiv.org/pdf/2502.16071", "details": "Q Zhang, C Fang, Y Zheng, Y Zhang, Y Zhao, R Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Unit testing validates the correctness of the units of the software system under test and serves as the cornerstone in improving software quality and reliability. To reduce manual efforts in writing unit tests, some techniques have been proposed to \u2026"}, {"title": "Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Ownership Verification with Reasoning", "link": "https://arxiv.org/pdf/2502.10440", "details": "J Guo, Y Li, R Chen, Y Wu, C Liu, Y Chen, H Huang - arXiv preprint arXiv:2502.10440, 2025", "abstract": "Large language models (LLMs) are increasingly integrated into real-world applications through retrieval-augmented generation (RAG) mechanisms to supplement their responses with up-to-date and domain-specific knowledge \u2026"}, {"title": "Mapping 1,000+ Language Models via the Log-Likelihood Vector", "link": "https://arxiv.org/pdf/2502.16173", "details": "M Oyama, H Yamagiwa, Y Takase, H Shimodaira - arXiv preprint arXiv:2502.16173, 2025", "abstract": "To compare autoregressive language models at scale, we propose using log- likelihood vectors computed on a predefined text set as model features. This approach has a solid theoretical basis: when treated as model coordinates, their \u2026"}, {"title": "BARE: Combining Base and Instruction-Tuned Language Models for Better Synthetic Data Generation", "link": "https://arxiv.org/pdf/2502.01697", "details": "A Zhu, P Asawa, JQ Davis, L Chen, I Stoica\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. A common assumption about synthetic data is that sampling from instruct-tuned models \u2026"}, {"title": "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization", "link": "https://arxiv.org/pdf/2502.13146", "details": "S Xing, Y Wang, P Li, R Bai, Y Wang, C Qian, H Yao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The emergence of large Vision Language Models (VLMs) has broadened the scope and capabilities of single-modal Large Language Models (LLMs) by integrating visual modalities, thereby unlocking transformative cross-modal applications in a \u2026"}, {"title": "Quantifying Memorization and Retriever Performance in Retrieval-Augmented Vision-Language Models", "link": "https://arxiv.org/pdf/2502.13836", "details": "P Carragher, A Jha, R Raghav, KM Carley - arXiv preprint arXiv:2502.13836, 2025", "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in question answering (QA), but metrics for assessing their reliance on memorization versus retrieval remain underdeveloped. Moreover, while finetuned models are state-of-the \u2026"}, {"title": "Large Language Models are Powerful EHR Encoders", "link": "https://arxiv.org/pdf/2502.17403", "details": "S Hegselmann, G von Arnim, T Rheude, N Kronenberg\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific EHR foundation models trained on \u2026"}, {"title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey", "link": "https://arxiv.org/pdf/2502.06872", "details": "B Ni, Z Liu, L Wang, Y Lei, Y Zhao, X Cheng, Q Zeng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to \u2026"}, {"title": "Leveraging Medical Knowledge Graphs Into Large Language Models for Diagnosis Prediction: Design and Application Study", "link": "https://ai.jmir.org/2025/1/e58670", "details": "Y Gao, R Li, E Croxford, J Caskey, BW Patterson\u2026 - JMIR AI, 2025", "abstract": "Background Electronic health records (EHRs) and routine documentation practices play a vital role in patients' daily care, providing a holistic record of health, diagnoses, and treatment. However, complex and verbose EHR narratives can \u2026"}]
