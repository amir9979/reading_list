[{"title": "Prompting, Decoding, Embedding: Leveraging Pretrained Language Models for High-quality and Diverse Open Rule Induction", "link": "https://ieeexplore.ieee.org/abstract/document/10906469/", "details": "W Sun, S He, J Zhao, K Liu - IEEE Transactions on Audio, Speech and Language \u2026, 2025", "abstract": "Open rule induction (OpenRI) is devoted to obtaining reasoning rules expressed in natural languages. Compared with traditional rules with predefined logical symbols and domain predicates, open rules are more expressive and easier to use in real \u2026"}, {"title": "Attention-Based Synthetic Data Generation for Calibration-Enhanced Survival Analysis: A Case Study for Chronic Kidney Disease Using Electronic Health Records", "link": "https://arxiv.org/pdf/2503.06096", "details": "NI Kuo, B Gallego, L Jorm - arXiv preprint arXiv:2503.06096, 2025", "abstract": "Access to real-world healthcare data is limited by stringent privacy regulations and data imbalances, hindering advancements in research and clinical applications. Synthetic data presents a promising solution, yet existing methods often fail to ensure \u2026"}, {"title": "Health Equity in the Era of Large Language? Models.", "link": "https://search.ebscohost.com/login.aspx%3Fdirect%3Dtrue%26profile%3Dehost%26scope%3Dsite%26authtype%3Dcrawler%26jrnl%3D10880224%26AN%3D183764921%26h%3DA8GzP6qfAgEieVO3KveeFVjkzGy4t5hzZjCNyIPJidR64p%252ButgrAlGH46Qb0dowqu5QmiOYHiMxXuvyipYyRzA%253D%253D%26crl%3Dc", "details": "AA Tierney, ME Reed, RW Grant, FX Doo, DD Pay\u00e1n\u2026 - American Journal of \u2026, 2025", "abstract": "This commentary presents a summary of 8 major regulations and guidelines that have direct implications for the equitable design, implementation, and maintenance of health care-focused large language models (LLMs) deployed in the US. We \u2026"}, {"title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning", "link": "https://arxiv.org/pdf/2503.15621", "details": "F Cocchi, N Moratelli, D Caffagni, S Sarto, L Baraldi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted the critical roles of both the visual backbone and the underlying language model. While prior work has primarily focused on scaling these components to billions of \u2026"}, {"title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for Multilingual Medical Question Answering", "link": "https://arxiv.org/pdf/2503.16131", "details": "F Li, Y Chen, H Liu, R Yang, H Yuan, Y Jiang, T Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have shown remarkable progress in medical question answering (QA), yet their effectiveness remains predominantly limited to English due to imbalanced multilingual training data and scarce medical resources \u2026"}]
