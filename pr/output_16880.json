[{"title": "MMaDA: Multimodal Large Diffusion Language Models", "link": "https://arxiv.org/pdf/2505.15809", "details": "L Yang, Y Tian, B Li, X Zhang, K Shen, Y Tong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is \u2026", "entry_id": "http://arxiv.org/abs/2505.15809v1", "updated": "2025-05-21 17:59:05", "published": "2025-05-21 17:59:05", "authors": "Ling Yang;Ye Tian;Bowen Li;Xinchen Zhang;Ke Shen;Yunhai Tong;Mengdi Wang", "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA", "comment": "Project: https://github.com/Gen-Verse/MMaDA", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.15809v1;http://arxiv.org/pdf/2505.15809v1", "pdf_url": "http://arxiv.org/pdf/2505.15809v1"}, {"title": "Do Language Models Use Their Depth Efficiently?", "link": "https://arxiv.org/pdf/2505.13898", "details": "R Csord\u00e1s, CD Manning, C Potts - arXiv preprint arXiv:2505.13898, 2025", "abstract": "Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible \u2026", "entry_id": "http://arxiv.org/abs/2505.13898v1", "updated": "2025-05-20 04:00:56", "published": "2025-05-20 04:00:56", "authors": "R\u00f3bert Csord\u00e1s;Christopher D. Manning;Christopher Potts", "summary": "Modern LLMs are increasingly deep, and depth correlates with performance,\nalbeit with diminishing returns. However, do these models use their depth\nefficiently? Do they compose more features to create higher-order computations\nthat are impossible in shallow models, or do they merely spread the same kinds\nof computation out over more layers? To address these questions, we analyze the\nresidual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,\ncomparing the output of the sublayers to the residual stream reveals that\nlayers in the second half contribute much less than those in the first half,\nwith a clear phase transition between the two halves. Second, skipping layers\nin the second half has a much smaller effect on future computations and output\npredictions. Third, for multihop tasks, we are unable to find evidence that\nmodels are using increased depth to compose subresults in examples involving\nmany hops. Fourth, we seek to directly address whether deeper models are using\ntheir additional layers to perform new kinds of computation. To do this, we\ntrain linear maps from the residual stream of a shallow model to a deeper one.\nWe find that layers with the same relative depth map best to each other,\nsuggesting that the larger model simply spreads the same computations out over\nits many layers. All this evidence suggests that deeper models are not using\ntheir depth to learn new kinds of computation, but only using the greater depth\nto perform more fine-grained adjustments to the residual. This may help explain\nwhy increasing scale leads to diminishing returns for stacked Transformer\narchitectures.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.NE", "links": "http://arxiv.org/abs/2505.13898v1;http://arxiv.org/pdf/2505.13898v1", "pdf_url": "http://arxiv.org/pdf/2505.13898v1"}, {"title": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models", "link": "https://arxiv.org/pdf/2505.15634", "details": "Z Li, X Wang, Y Yang, Z Yao, H Xiong, M Du - arXiv preprint arXiv:2505.15634, 2025", "abstract": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT length, as seen in models such as DeepSeek-R1, significantly enhances this \u2026", "entry_id": "http://arxiv.org/abs/2505.15634v2", "updated": "2025-05-24 15:20:30", "published": "2025-05-21 15:17:59", "authors": "Zihao Li;Xu Wang;Yuzhe Yang;Ziyu Yao;Haoyi Xiong;Mengnan Du", "summary": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and\nmathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT\nlength, as seen in models such as DeepSeek-R1, significantly enhances this\nreasoning for complex problems, but requires costly and high-quality long CoT\ndata and fine-tuning. This work, inspired by the deep thinking paradigm of\nDeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of\nan LLM without external datasets. Our method first employs Sparse Autoencoders\n(SAEs) to extract interpretable features from vanilla CoT. These features are\nthen used to steer the LLM's internal states during generation. Recognizing\nthat many LLMs do not have corresponding pre-trained SAEs, we further introduce\na novel SAE-free steering algorithm, which directly computes steering\ndirections from the residual activations of an LLM, obviating the need for an\nexplicit SAE. Experimental results demonstrate that both our SAE-based and\nsubsequent SAE-free steering algorithms significantly enhance the reasoning\ncapabilities of LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.15634v2;http://arxiv.org/pdf/2505.15634v2", "pdf_url": "http://arxiv.org/pdf/2505.15634v2"}, {"title": "FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain", "link": "https://arxiv.org/pdf/2505.14826", "details": "R Deb, K Thekumparampil, K Kalantari, G Hiranandani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Supervised fine-tuning (SFT) is a standard approach to adapting large language models (LLMs) to new domains. In this work, we improve the statistical efficiency of SFT by selecting an informative subset of training examples. Specifically, for a fixed \u2026", "entry_id": "http://arxiv.org/abs/2505.14826v1", "updated": "2025-05-20 18:41:34", "published": "2025-05-20 18:41:34", "authors": "Rohan Deb;Kiran Thekumparampil;Kousha Kalantari;Gaurush Hiranandani;Shoham Sabach;Branislav Kveton", "summary": "Supervised fine-tuning (SFT) is a standard approach to adapting large\nlanguage models (LLMs) to new domains. In this work, we improve the statistical\nefficiency of SFT by selecting an informative subset of training examples.\nSpecifically, for a fixed budget of training examples, which determines the\ncomputational cost of fine-tuning, we determine the most informative ones. The\nkey idea in our method is to select examples that maximize information gain,\nmeasured by the Hessian of the log-likelihood of the LLM. We approximate it\nefficiently by linearizing the LLM at the last layer using multinomial logistic\nregression models. Our approach is computationally efficient, analyzable, and\nperforms well empirically. We demonstrate this on several problems, and back\nour claims with both quantitative results and an LLM evaluation.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CL;stat.ML", "links": "http://arxiv.org/abs/2505.14826v1;http://arxiv.org/pdf/2505.14826v1", "pdf_url": "http://arxiv.org/pdf/2505.14826v1"}, {"title": "Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack", "link": "https://arxiv.org/pdf/2505.15323", "details": "S Cappelletti, T Poppi, S Poppi, ZX Yong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) are increasingly evaluated on multiple-choice question answering (MCQA) tasks using* first-token probability*(FTP), which selects the answer option whose initial token has the highest likelihood. While efficient, FTP \u2026", "entry_id": "http://arxiv.org/abs/2505.15323v1", "updated": "2025-05-21 09:58:38", "published": "2025-05-21 09:58:38", "authors": "Silvia Cappelletti;Tobia Poppi;Samuele Poppi;Zheng-Xin Yong;Diego Garcia-Olano;Marcella Cornia;Lorenzo Baraldi;Rita Cucchiara", "summary": "Large Language Models (LLMs) are increasingly evaluated on multiple-choice\nquestion answering (MCQA) tasks using *first-token probability* (FTP), which\nselects the answer option whose initial token has the highest likelihood. While\nefficient, FTP can be fragile: models may assign high probability to unrelated\ntokens (*misalignment*) or use a valid token merely as part of a generic\npreamble rather than as a clear answer choice (*misinterpretation*),\nundermining the reliability of symbolic evaluation. We propose a simple\nsolution: the *prefilling attack*, a structured natural-language prefix (e.g.,\n\"*The correct option is:*\") prepended to the model output. Originally explored\nin AI safety, we repurpose prefilling to steer the model to respond with a\nclean, valid option, without modifying its parameters. Empirically, the FTP\nwith prefilling strategy substantially improves accuracy, calibration, and\noutput consistency across a broad set of LLMs and MCQA benchmarks. It\noutperforms standard FTP and often matches the performance of open-ended\ngeneration approaches that require full decoding and external classifiers,\nwhile being significantly more efficient. Our findings suggest that prefilling\nis a simple, robust, and low-cost method to enhance the reliability of\nFTP-based evaluation in multiple-choice settings.", "comment": "13 pages, 5 figures, 7 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15323v1;http://arxiv.org/pdf/2505.15323v1", "pdf_url": "http://arxiv.org/pdf/2505.15323v1"}, {"title": "Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training", "link": "https://arxiv.org/pdf/2504.20484", "details": "L Wu, H Wei, H Lin, T Li, B Yang, W Lu - arXiv preprint arXiv:2504.20484, 2025", "abstract": "Large language models (LLMs) exhibit remarkable multilingual capabilities despite English-dominated pre-training, attributed to cross-lingual mechanisms during pre- training. Existing methods for enhancing cross-lingual transfer remain constrained by \u2026", "entry_id": "http://arxiv.org/abs/2504.20484v1", "updated": "2025-04-29 07:24:25", "published": "2025-04-29 07:24:25", "authors": "Linjuan Wu;Haoran Wei;Huan Lin;Tianhao Li;Baosong Yang;Weiming Lu", "summary": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite English-dominated pre-training, attributed to cross-lingual mechanisms\nduring pre-training. Existing methods for enhancing cross-lingual transfer\nremain constrained by parallel resources, suffering from limited linguistic and\ndomain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT),\na simple and scalable approach that enhances cross-lingual transfer by\nleveraging semantically related bilingual texts via simple next-word\nprediction. We construct CrossIC-PT samples by interleaving semantic-related\nbilingual Wikipedia documents into a single context window. To access window\nsize constraints, we implement a systematic segmentation policy to split long\nbilingual document pairs into chunks while adjusting the sliding window\nmechanism to preserve contextual coherence. We further extend data availability\nthrough a semantic retrieval framework to construct CrossIC-PT samples from\nweb-crawled corpus. Experimental results demonstrate that CrossIC-PT improves\nmultilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and\nQwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%,\n3.99%, and 1.95%, respectively, with additional improvements after data\naugmentation.", "comment": "12 pages, 6 figures, Under Review", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2504.20484v1;http://arxiv.org/pdf/2504.20484v1", "pdf_url": "http://arxiv.org/pdf/2504.20484v1"}, {"title": "Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought", "link": "https://arxiv.org/pdf/2505.15431", "details": "A Liu, B Zhou, C Xu, C Zhou, CC Zhang, C Xu, C Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS, a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It synergistically combines Mamba's long-sequence processing efficiency with \u2026", "entry_id": "http://arxiv.org/abs/2505.15431v2", "updated": "2025-05-22 06:44:25", "published": "2025-05-21 12:11:53", "authors": "Tencent Hunyuan Team;Ao Liu;Botong Zhou;Can Xu;Chayse Zhou;ChenChen Zhang;Chengcheng Xu;Chenhao Wang;Decheng Wu;Dengpeng Wu;Dian Jiao;Dong Du;Dong Wang;Feng Zhang;Fengzong Lian;Guanghui Xu;Guanwei Zhang;Hai Wang;Haipeng Luo;Han Hu;Huilin Xu;Jiajia Wu;Jianchen Zhu;Jianfeng Yan;Jiaqi Zhu;Jihong Zhang;Jinbao Xue;Jun Xia;Junqiang Zheng;Kai Liu;Kai Zhang;Kai Zheng;Kejiao Li;Keyao Wang;Lan Jiang;Lixin Liu;Lulu Wu;Mengyuan Huang;Peijie Yu;Peiqi Wang;Qian Wang;Qianbiao Xiang;Qibin Liu;Qingfeng Sun;Richard Guo;Ruobing Xie;Saiyong Yang;Shaohua Chen;Shihui Hu;Shuai Li;Shuaipeng Li;Shuang Chen;Suncong Zheng;Tao Yang;Tian Zhang;Tinghao Yu;Weidong Han;Weijie Liu;Weijin Zhou;Weikang Wang;Wesleye Chen;Xiao Feng;Xiaoqin Ren;Xingwu Sun;Xiong Kuang;Xuemeng Huang;Xun Cao;Yanfeng Chen;Yang Du;Yang Zhen;Yangyu Tao;Yaping Deng;Yi Shen;Yigeng Hong;Yiqi Chen;Yiqing Huang;Yuchi Deng;Yue Mao;Yulong Wang;Yuyuan Zeng;Zenan Xu;Zhanhui Kang;Zhe Zhao;ZhenXiang Yan;Zheng Fang;Zhichao Hu;Zhongzhi Chen;Zhuoyu Li;Zongwei Li;Alex Yan;Ande Liang;Baitong Liu;Beiping Pan;Bin Xing;Binghong Wu;Bingxin Qu;Bolin Ni;Boyu Wu;Chen Li;Cheng Jiang;Cheng Zhang;Chengjun Liu;Chengxu Yang;Chengzhong Xu;Chiyu Wang;Chong Zha;Daisy Yi;Di Wang;Fanyang Lu;Fei Chen;Feifei Liu;Feng Zheng;Guanghua Yu;Guiyang Li;Guohua Wang;Haisheng Lin;Han Liu;Han Wang;Hao Fei;Hao Lu;Haoqing Jiang;Haoran Sun;Haotian Zhu;Huangjin Dai;Huankui Chen;Huawen Feng;Huihui Cai;Huxin Peng;Jackson Lv;Jiacheng Shi;Jiahao Bu;Jianbo Li;Jianglu Hu;Jiangtao Guan;Jianing Xu;Jianwei Cai;Jiarong Zhang;Jiawei Song;Jie Jiang;Jie Liu;Jieneng Yang;Jihong Zhang;Jin lv;Jing Zhao;Jinjian Li;Jinxing Liu;Jun Zhao;Juntao Guo;Kai Wang;Kan Wu;Lei Fu;Lei He;Lei Wang;Li Liu;Liang Dong;Liya Zhan;Long Cheng;Long Xu;Mao Zheng;Meng Liu;Mengkang Hu;Nanli Chen;Peirui Chen;Peng He;Pengju Pan;Pengzhi Wei;Qi Yang;Qi Yi;Roberts Wang;Rongpeng Chen;Rui Sun;Rui Yang;Ruibin Chen;Ruixu Zhou;Shaofeng Zhang;Sheng Zhang;Shihao Xu;Shuaishuai Chang;Shulin Liu;SiQi Wang;Songjia Feng;Songling Yuan;Tao Zhang;Tianjiao Lang;Tongkai Li;Wei Deng;Wei Li;Weichao Wang;Weigang Zhang;Weixuan Sun;Wen Ouyang;Wenxiang Jiao;Wenzhi Sun;Wenzhuo Jia;Xiang Zhang;Xiangyu He;Xianshun Ren;XiaoYing Zhu;Xiaolong Guo;Xiaoxue Li;Xiaoyu Ma;Xican Lu;Xinhua Feng;Xinting Huang;Xinyu Guan;Xirui Li;Xu Zhang;Xudong Gao;Xun Luo;Xuxiang Qi;Yangkun Chen;Yangyu Tao;Yanling Xiao;Yantao Mai;Yanze Chen;Yao Ding;Yeting Yang;YiFan Song;Yifan Yang;Yijiao Zhu;Yinhe Wu;Yixian Liu;Yong Yang;Yuanjun Cai;Yuanlin Tu;Yue Zhang;Yufei Huang;Yuhang Zhou;Yuhao Jiang;Yuhong Liu;Yuhui Hu;Yujin Lin;Yun Yang;Yunhao Wang;Yusong Zhang;Zekun Wu;Zelong Zhang;Zhan Yu;Zhaoliang Yang;Zhe Zhao;Zheng Li;Zhenyu Huang;Zhiguang Liu;Zhijiang Xu;Zhiqing Kui;Zhiyin Zeng;Zhiyuan Xiong;Zhuo Han;Zifan Wu;Zigang Geng;Zilong Zhao;Ziyan Tang;Ziyuan Zhu;Zonglei Zhu;Zhijiang Xu", "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15431v2;http://arxiv.org/pdf/2505.15431v2", "pdf_url": "http://arxiv.org/pdf/2505.15431v2"}, {"title": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science", "link": "https://arxiv.org/pdf/2505.13220", "details": "J Ying, Z Chen, Z Wang, W Jiang, C Wang, Z Yuan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Seed science is essential for modern agriculture, directly influencing crop yields and global food security. However, challenges such as interdisciplinary complexity and high costs with limited returns hinder progress, leading to a shortage of experts and \u2026", "entry_id": "http://arxiv.org/abs/2505.13220v1", "updated": "2025-05-19 15:02:59", "published": "2025-05-19 15:02:59", "authors": "Jie Ying;Zihong Chen;Zhefan Wang;Wanli Jiang;Chenyang Wang;Zhonghang Yuan;Haoyang Su;Huanjun Kong;Fan Yang;Nanqing Dong", "summary": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design.", "comment": "Accepted by ACL 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.13220v1;http://arxiv.org/pdf/2505.13220v1", "pdf_url": "http://arxiv.org/pdf/2505.13220v1"}, {"title": "Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses", "link": "https://arxiv.org/pdf/2505.15738", "details": "X Yang, B Stevanoski, M Meeus, YA de Montjoye - arXiv preprint arXiv:2505.15738, 2025", "abstract": "Large language models (LLMs) are rapidly deployed in real-world applications ranging from chatbots to agentic systems. Alignment is one of the main approaches used to defend against attacks such as prompt injection and jailbreaks. Recent \u2026", "entry_id": "http://arxiv.org/abs/2505.15738v1", "updated": "2025-05-21 16:43:17", "published": "2025-05-21 16:43:17", "authors": "Xiaoxue Yang;Bozhidar Stevanoski;Matthieu Meeus;Yves-Alexandre de Montjoye", "summary": "Large language models (LLMs) are rapidly deployed in real-world applications\nranging from chatbots to agentic systems. Alignment is one of the main\napproaches used to defend against attacks such as prompt injection and\njailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even\nagainst Greedy Coordinate Gradient (GCG), a white-box attack that generates\nadversarial suffixes to induce attacker-desired outputs. However, this search\nspace over discrete tokens is extremely large, making the task of finding\nsuccessful attacks difficult. GCG has, for instance, been shown to converge to\nlocal minima, making it sensitive to initialization choices. In this paper, we\nassess the future-proof robustness of these defenses using a more informed\nthreat model: attackers who have access to some information about the alignment\nprocess. Specifically, we propose an informed white-box attack leveraging the\nintermediate model checkpoints to initialize GCG, with each checkpoint acting\nas a stepping stone for the next one. We show this approach to be highly\neffective across state-of-the-art (SOTA) defenses and models. We further show\nour informed initialization to outperform other initialization methods and show\na gradient-informed checkpoint selection strategy to greatly improve attack\nperformance and efficiency. Importantly, we also show our method to\nsuccessfully find universal adversarial suffixes -- single suffixes effective\nacross diverse inputs. Our results show that, contrary to previous beliefs,\neffective adversarial suffixes do exist against SOTA alignment-based defenses,\nthat these can be found by existing attack methods when adversaries exploit\nalignment knowledge, and that even universal suffixes exist. Taken together,\nour results highlight the brittleness of current alignment-based methods and\nthe need to consider stronger threat models when testing the safety of LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR;cs.AI;cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.15738v1;http://arxiv.org/pdf/2505.15738v1", "pdf_url": "http://arxiv.org/pdf/2505.15738v1"}]
