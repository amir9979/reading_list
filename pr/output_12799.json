[{"title": "ELAINE-medLLM: Lightweight English Japanese Chinese Trilingual Large Language Model for Bio-medical Domain", "link": "https://aclanthology.org/2025.coling-main.313.pdf", "details": "K Yano, Z Luo, J Huang, Q Xie, M Asada, C Yuan\u2026 - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract We propose ELAINE (EngLish-jApanese-chINesE)-medLLM, a trilingual (English, Japanese, Chinese) large language model adapted for the bio-medical domain based on Llama-3-8B. The training dataset was carefully curated in terms of \u2026"}, {"title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06788", "details": "H Diao, X Li, Y Cui, Y Wang, H Deng, T Pan, W Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient \u2026"}, {"title": "Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding", "link": "https://arxiv.org/pdf/2501.07888", "details": "L Yuan, J Wang, H Sun, Y Zhang, Y Lin - arXiv preprint arXiv:2501.07888, 2025", "abstract": "We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM) designed for generating detailed and accurate video descriptions, while also exhibiting superior general video understanding capabilities. Tarsier2 achieves \u2026"}, {"title": "Chest X-ray Foundation Model with Global and Local Representations Integration", "link": "https://arxiv.org/pdf/2502.05142", "details": "Z Yang, X Xu, J Zhang, G Wang, MK Kalra, P Yan - arXiv preprint arXiv:2502.05142, 2025", "abstract": "Chest X-ray (CXR) is the most frequently ordered imaging test, supporting diverse clinical tasks from thoracic disease detection to postoperative monitoring. However, task-specific classification models are limited in scope, require costly labeled data \u2026"}, {"title": "Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06130", "details": "C Zhang, Z Wan, Z Kan, MQ Ma, S Stepputtis\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While recent Large Vision-Language Models (LVLMs) have shown remarkable performance in multi-modal tasks, they are prone to generating hallucinatory text responses that do not align with the given visual input, which restricts their practical \u2026"}, {"title": "Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement", "link": "https://arxiv.org/pdf/2502.02573%3F", "details": "S Abbasloo - arXiv preprint arXiv:2502.02573, 2025", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem- solving, a crucial, ubiquitous, and complex domain. This paper explores the \u2026"}, {"title": "MedS $^ 3$: Towards Medical Small Language Models with Self-Evolved Slow Thinking", "link": "https://arxiv.org/pdf/2501.12051%3F", "details": "S Jiang, Y Liao, Z Chen, Y Zhang, Y Wang, Y Wang - arXiv preprint arXiv:2501.12051, 2025", "abstract": "Medical language models (MLMs) have become pivotal in advancing medical natural language processing. However, prior models that rely on pre-training or supervised fine-tuning often exhibit low data efficiency and limited practicality in real \u2026"}, {"title": "Hierarchical Autoregressive Transformers: Combining Byte-and Word-Level Processing for Robust, Adaptable Language Models", "link": "https://arxiv.org/pdf/2501.10322", "details": "P Neitemeier, B Deiseroth, C Eichenberg, L Balles - arXiv preprint arXiv:2501.10322, 2025", "abstract": "Tokenization is a fundamental step in natural language processing, breaking text into units that computational models can process. While learned subword tokenizers have become the de-facto standard, they present challenges such as large \u2026"}, {"title": "Evaluating Small Language Models for News Summarization: Implications and Factors Influencing Performance", "link": "https://arxiv.org/pdf/2502.00641", "details": "B Xu, Y Chen, Z Wen, W Liu, B He - arXiv preprint arXiv:2502.00641, 2025", "abstract": "The increasing demand for efficient summarization tools in resource-constrained environments highlights the need for effective solutions. While large language models (LLMs) deliver superior summarization quality, their high computational \u2026"}]
