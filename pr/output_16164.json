[{"title": "Spectral Bias Correction in PINNs for Myocardial Image Registration of Pathological Data", "link": "https://arxiv.org/pdf/2504.17945", "details": "BC Baluyot, M Varela, C Qin - arXiv preprint arXiv:2504.17945, 2025", "abstract": "Accurate myocardial image registration is essential for cardiac strain analysis and disease diagnosis. However, spectral bias in neural networks impedes modeling high-frequency deformations, producing inaccurate, biomechanically implausible \u2026"}, {"title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability", "link": "https://arxiv.org/pdf/2504.07416%3F", "details": "J Park, S Kim, B Yoon, K Choi - arXiv preprint arXiv:2504.07416, 2025", "abstract": "Recent advancements in multi-modal models have significantly improved vision- language alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning, rely on low-resolution \u2026"}, {"title": "Zero-shot 3D Scene Representation with Invertible Generative Neural Radiance Fields", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10967257.pdf", "details": "K Ko, S Kim, M Lee - IEEE Access, 2025", "abstract": "Generative Neural Radiance Fields (NeRFs) have recently enabled efficient synthesis of 3D scenes by training on unposed real image sets. However, existing methods for generating multi-view images of specific input images have limitations \u2026"}, {"title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation", "link": "https://arxiv.org/pdf/2504.06861%3F", "details": "D Jagpal, X Chen, VP Namboodiri - arXiv preprint arXiv:2504.06861, 2025", "abstract": "Zero-shot, training-free, image-based text-to-video generation is an emerging area that aims to generate videos using existing image-based diffusion models. Current methods in this space require specific architectural changes to image generation \u2026"}, {"title": "SRG-Net: A Self-supervised 3D Scene Representation Method via Graph Contrastive Learning for Novel View Synthesis", "link": "https://www.jstage.jst.go.jp/article/transfun/advpub/0/advpub_2024EAL2091/_pdf", "details": "Q Qi, Z Liu, Y Guo - IEICE Transactions on Fundamentals of Electronics \u2026, 2025", "abstract": "SUMMARYAccurate scene representation holds practical significance for autonomous driving and virtual reality. This letter proposes a network to optimize images encoding and features learning for better scene representations \u2026"}, {"title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer", "link": "https://arxiv.org/pdf/2504.10462", "details": "W Lei, J Wang, H Wang, X Li, JH Liew, J Feng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained \u2026"}, {"title": "Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training", "link": "https://arxiv.org/pdf/2504.13123%3F", "details": "X Zhang, Y Zeng, X Huang, H Hu, R Xie, H Hu, Z Kang - arXiv preprint arXiv \u2026, 2025", "abstract": "In recent years, the field of vision-language model pre-training has experienced rapid advancements, driven primarily by the continuous enhancement of textual capabilities in large language models. However, existing training paradigms for \u2026"}, {"title": "SVLTA: Benchmarking Vision-Language Temporal Alignment via Synthetic Video Situation", "link": "https://arxiv.org/pdf/2504.05925", "details": "H Du, B Wu, Y Lu, Z Mao - arXiv preprint arXiv:2504.05925, 2025", "abstract": "Vision-language temporal alignment is a crucial capability for human dynamic recognition and cognition in real-world scenarios. While existing research focuses on capturing vision-language relevance, it faces limitations due to biased temporal \u2026"}, {"title": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting", "link": "https://arxiv.org/pdf/2504.10486", "details": "Z Jiang, S Wang, S Tang - arXiv preprint arXiv:2504.10486, 2025", "abstract": "Creating relightable and animatable human avatars from monocular videos is a rising research topic with a range of applications, eg virtual reality, sports, and video games. Previous works utilize neural fields together with physically based rendering \u2026"}]
