[{"title": "Attention-enriched deeper UNet (ADU-NET) for disease diagnosis in breast ultrasound and retina fundus images", "link": "https://link.springer.com/article/10.1007/s13748-024-00340-1", "details": "CJ Ejiyi, Z Qin, VK Agbesi, MB Ejiyi, IA Chikwendu\u2026 - Progress in Artificial \u2026, 2024", "abstract": "In image segmentation, effective upsampling plays a pivotal role in recovering lost spatial information during the process of downsampling. Standard skip connections designed to mitigate this and prevalent in most models, often fall short of maintaining \u2026"}, {"title": "Comparison of Pathologist and Artificial Intelligence\u2013based Grading for Prediction of Metastatic Outcomes After Radical Prostatectomy", "link": "https://www.sciencedirect.com/science/article/pii/S2588931124001871", "details": "LD Oliveira, J Lu, E Erak, AA Mendes, O Dairo\u2026 - European Urology Oncology, 2024", "abstract": "Gleason grade group (GG) is the most powerful prognostic variable in localized prostate cancer; however, interobserver variability remains a challenge. Artificial intelligence algorithms applied to histopathologic images standardize grading, but \u2026"}, {"title": "Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis", "link": "https://arxiv.org/pdf/2409.00106", "details": "A Nagar, S Jaiswal, C Tan - arXiv preprint arXiv:2409.00106, 2024", "abstract": "Vision-language models (VLMs) have shown impressive zero-and few-shot performance on real-world visual question answering (VQA) benchmarks, alluding to their capabilities as visual reasoning engines. However, the benchmarks being used \u2026"}, {"title": "Customized Convolutional Neural Network for Glaucoma Detection in Retinal Fundus Images", "link": "https://jppipa.unram.ac.id/index.php/jppipa/article/download/7614/5561", "details": "F Islami, S Defit - Jurnal Penelitian Pendidikan IPA, 2024", "abstract": "Glaucoma is one of the leading causes of permanent blindness and remains a current challenge in the field of ophthalmology. This research aims to present a comprehensive investigation into the development and evaluation of new technology \u2026"}, {"title": "How Does Diverse Interpretability of Textual Prompts Impact Medical Vision-Language Zero-Shot Tasks?", "link": "https://arxiv.org/pdf/2409.00543", "details": "S Wang, C Liu, R Arcucci - arXiv preprint arXiv:2409.00543, 2024", "abstract": "Recent advancements in medical vision-language pre-training (MedVLP) have significantly enhanced zero-shot medical vision tasks such as image classification by leveraging large-scale medical image-text pair pre-training. However, the \u2026"}, {"title": "Language Models Benefit from Preparation with Elicited Knowledge", "link": "https://arxiv.org/pdf/2409.01345", "details": "J Yu, H An, LK Schubert - arXiv preprint arXiv:2409.01345, 2024", "abstract": "The zero-shot chain of thought (CoT) approach is often used in question answering (QA) by language models (LMs) for tasks that require multiple reasoning steps, typically enhanced by the prompt\" Let's think step by step.\" However, some QA tasks \u2026"}]
