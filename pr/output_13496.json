[{"title": "Revealing the Pragmatic Dilemma for Moral Reasoning Acquisition in Language Models", "link": "https://arxiv.org/pdf/2502.16600", "details": "G Liu, L Jiang, X Zhang, KM Johnson - arXiv preprint arXiv:2502.16600, 2025", "abstract": "Ensuring that Large Language Models (LLMs) return just responses which adhere to societal values is crucial for their broader application. Prior research has shown that LLMs often fail to perform satisfactorily on tasks requiring moral cognizance, such as \u2026"}, {"title": "GenTool: Enhancing Tool Generalization in Language Models through Zero-to-One and Weak-to-Strong Simulation", "link": "https://arxiv.org/pdf/2502.18990", "details": "J He, J Neville, M Wan, L Yang, H Liu, X Xu, X Song\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) can enhance their capabilities as AI assistants by integrating external tools, allowing them to access a wider range of information. While recent LLMs are typically fine-tuned with tool usage examples during \u2026"}, {"title": "Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI", "link": "https://arxiv.org/pdf/2502.16691", "details": "E Noh, J Baek - arXiv preprint arXiv:2502.16691, 2025", "abstract": "Recent research has increasingly focused on training large language models (LLMs) using federated learning, known as FedLLM. However, responsible AI (RAI), which aims to ensure safe responses, remains underexplored in the context of FedLLM. In \u2026"}, {"title": "MTDP: Modulated Transformer Diffusion Policy Model", "link": "https://arxiv.org/pdf/2502.09029", "details": "Q Wang, Y Sun, E Lu, Q Zhang, Y Zeng - arXiv preprint arXiv:2502.09029, 2025", "abstract": "Recent research on robot manipulation based on Behavior Cloning (BC) has made significant progress. By combining diffusion models with BC, diffusion policiy has been proposed, enabling robots to quickly learn manipulation tasks with high \u2026"}, {"title": "IPO: Your Language Model is Secretly a Preference Classifier", "link": "https://arxiv.org/pdf/2502.16182", "details": "S Garg, A Singh, S Singh, P Chopra - arXiv preprint arXiv:2502.16182, 2025", "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. While it enables LLMs to achieve human-level alignment, it often incurs significant \u2026"}, {"title": "ManiTrend: Bridging Future Generation and Action Prediction with 3D Flow for Robotic Manipulation", "link": "https://arxiv.org/pdf/2502.10028", "details": "Y He, Q Nie - arXiv preprint arXiv:2502.10028, 2025", "abstract": "Language-conditioned manipulation is a vital but challenging robotic task due to the high-level abstraction of language. To address this, researchers have sought improved goal representations derived from natural language. In this paper, we \u2026"}, {"title": "Can Large Language Models Unveil the Mysteries? An Exploration of Their Ability to Unlock Information in Complex Scenarios", "link": "https://arxiv.org/pdf/2502.19973", "details": "C Wang, L Zhang, Z Wang, Y Zhou - arXiv preprint arXiv:2502.19973, 2025", "abstract": "Combining multiple perceptual inputs and performing combinatorial reasoning in complex scenarios is a sophisticated cognitive function in humans. With advancements in multi-modal large language models, recent benchmarks tend to \u2026"}, {"title": "From System 1 to System 2: A Survey of Reasoning Large Language Models", "link": "https://arxiv.org/pdf/2502.17419", "details": "ZZ Li, D Zhang, ML Zhang, J Zhang, Z Liu, Y Yao, H Xu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more \u2026"}, {"title": "Towards label-only membership inference attack against pre-trained large language models", "link": "https://arxiv.org/pdf/2502.18943", "details": "Y He, B Li, L Liu, Z Ba, W Dong, Y Li, Z Qin, K Ren\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Membership Inference Attacks (MIAs) aim to predict whether a data sample belongs to the model's training set or not. Although prior research has extensively explored MIAs in Large Language Models (LLMs), they typically require accessing to complete \u2026"}]
