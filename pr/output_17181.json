[{"title": "TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine", "link": "https://arxiv.org/pdf/2505.24063", "details": "J Xie, Y Yu, Z Zhang, S Zeng, J He, A Vasireddy\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 To further assess the models' capability in visual understanding tasks within Traditional Chinese **Medicine** (TCM), we evaluated 10 **large** **language** **models** (LLMs) on two image-based benchmarks: Herbs classification and Tongue image diagnosis \u2026", "entry_id": "http://arxiv.org/abs/2505.24063v1", "updated": "2025-05-29 23:13:57", "published": "2025-05-29 23:13:57", "authors": "Jiacheng Xie;Yang Yu;Ziyang Zhang;Shuai Zeng;Jiaxuan He;Ayush Vasireddy;Xiaoting Tang;Congyu Guo;Lening Zhao;Congcong Jing;Guanghui An;Dong Xu", "summary": "Traditional Chinese Medicine (TCM), as an effective alternative medicine, has\nbeen receiving increasing attention. In recent years, the rapid development of\nlarge language models (LLMs) tailored for TCM has underscored the need for an\nobjective and comprehensive evaluation framework to assess their performance on\nreal-world tasks. However, existing evaluation datasets are limited in scope\nand primarily text-based, lacking a unified and standardized multimodal\nquestion-answering (QA) benchmark. To address this issue, we introduce\nTCM-Ladder, the first multimodal QA dataset specifically designed for\nevaluating large TCM language models. The dataset spans multiple core\ndisciplines of TCM, including fundamental theory, diagnostics, herbal formulas,\ninternal medicine, surgery, pharmacognosy, and pediatrics. In addition to\ntextual content, TCM-Ladder incorporates various modalities such as images and\nvideos. The datasets were constructed using a combination of automated and\nmanual filtering processes and comprise 52,000+ questions in total. These\nquestions include single-choice, multiple-choice, fill-in-the-blank, diagnostic\ndialogue, and visual comprehension tasks. We trained a reasoning model on\nTCM-Ladder and conducted comparative experiments against 9 state-of-the-art\ngeneral domain and 5 leading TCM-specific LLMs to evaluate their performance on\nthe datasets. Moreover, we propose Ladder-Score, an evaluation method\nspecifically designed for TCM question answering that effectively assesses\nanswer quality regarding terminology usage and semantic expression. To our\nknowledge, this is the first work to evaluate mainstream general domain and\nTCM-specific LLMs on a unified multimodal benchmark. The datasets and\nleaderboard are publicly available at https://tcmladder.com or\nhttps://54.211.107.106 and will be continuously updated.", "comment": "22 pages, 4 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.DB", "links": "http://arxiv.org/abs/2505.24063v1;http://arxiv.org/pdf/2505.24063v1", "pdf_url": "http://arxiv.org/pdf/2505.24063v1"}, {"title": "MedBLIP: A Multimodal Method of **Medical Question** - **Answering** Based on Fine-Tuning Large Language Model", "link": "https://www.sciencedirect.com/science/article/pii/S0895611125000904", "details": "L Gong, J Yang, S Han, Y Ji - Computerized **Medical** Imaging and Graphics, 2025", "abstract": "**Medical** visual **question** **answering** is crucial for effectively interpreting **medical** images containing clinically relevant information. This study proposes a method called MedBLIP ( **Medical** Treatment Bootstrapping Language-Image Pretraining) to \u2026"}, {"title": "Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs", "link": "https://arxiv.org/pdf/2505.24830", "details": "J Vladika, A Domres, M Nguyen, R Moser, J Nano\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) exhibit extensive **medical** knowledge.However, LLMs are prone to hallucinations that may lead to harmful **medical** advice, and their often inaccurate citations reduce overall explainability.Such limitations also \u2026", "entry_id": "http://arxiv.org/abs/2505.24830v1", "updated": "2025-05-30 17:33:07", "published": "2025-05-30 17:33:07", "authors": "Juraj Vladika;Annika Domres;Mai Nguyen;Rebecca Moser;Jana Nano;Felix Busch;Lisa C. Adams;Keno K. Bressem;Denise Bernhardt;Stephanie E. Combs;Kai J. Borm;Florian Matthes;Jan C. Peeken", "summary": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare.", "comment": "11 pages, 4 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.24830v1;http://arxiv.org/pdf/2505.24830v1", "pdf_url": "http://arxiv.org/pdf/2505.24830v1"}, {"title": "MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering", "link": "https://arxiv.org/pdf/2505.24040", "details": "Y Hao, K Alhamoud, H Jeong, H Zhang, I Puri, P Torr\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) have shown strong performance across a range of **medical** tasks, with systems like GPT-4 and MedPaLM outperforming human averages on standardized **medical** examinations [9, 42]. However, many tasks do \u2026", "entry_id": "http://arxiv.org/abs/2505.24040v1", "updated": "2025-05-29 22:23:48", "published": "2025-05-29 22:23:48", "authors": "Yuexing Hao;Kumail Alhamoud;Hyewon Jeong;Haoran Zhang;Isha Puri;Philip Torr;Mike Schaekermann;Ariel D. Stern;Marzyeh Ghassemi", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious medical question-answering (QA) benchmarks, including standardized\nmedical exams. However, correct answers alone do not ensure correct logic, and\nmodels may reach accurate conclusions through flawed processes. In this study,\nwe introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance\nEstimation and Question Answering) dataset to evaluate how physician trainees\nand LLMs prioritize relevant information when answering QA questions. We obtain\nannotations on 1,300 QA pairs from 36 physician trainees, labeling each\nsentence within the question components for relevance. We compare these\nrelevance estimates to those for LLMs, and further evaluate the impact of these\n\"relevant\" subsets on downstream task performance for both physician trainees\nand LLMs. We find that LLMs are frequently not aligned with the content\nrelevance estimates of physician trainees. After filtering out physician\ntrainee-labeled irrelevant sentences, accuracy improves for both the trainees\nand the LLMs. All LLM and physician trainee-labeled data are available at:\nhttp://medpair.csail.mit.edu/.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.24040v1;http://arxiv.org/pdf/2505.24040v1", "pdf_url": "http://arxiv.org/pdf/2505.24040v1"}, {"title": "Integrating **Large Language Models** in Biostatistical Workflows for Clinical and Translational Research", "link": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/44E8054B0DFFB978CB005ED42CC0C9CC/S2059866125100642a.pdf/div-class-title-integrating-large-language-models-in-biostatistical-workflows-for-clinical-and-translational-research-div.pdf", "details": "SC Grambow, M Desai, KP Weinfurt, CJ Lindsell\u2026 - Journal of Clinical and \u2026", "abstract": "\u2026 **Large** **language** **models** (LLMs) are advanced artificial intelligence systems trained on vast amounts of text data\u2026 A survey of **large** **language** **models** for **healthcare** : from data, technology, and applications to \u2026 Evaluating the application \u2026"}, {"title": "Automated Response Generation Using Language Models: An Approach to Enhancing User Interaction", "link": "https://link.springer.com/chapter/10.1007/978-3-031-92967-0_11", "details": "C Asaju, H Vadapalli - International Conference on Human-Computer \u2026, 2025", "abstract": "\u2026 of smart **reply** recommendations in **medical** situations. The study generated responses to patient communications in online **medical** chat \u2026 to customer reviews using retrieval-augmented generation (RAG) and **large** **language** **models** (LLMs) \u2026"}, {"title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models", "link": "https://arxiv.org/pdf/2505.24302", "details": "Y Wang, S Feng, Y Tsvetkov, H Hajishirzi - arXiv preprint arXiv:2505.24302, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) are increasingly used to support scientific research, but \u2026 and 30,888 scientific claims from ten domains including **medicine** , biology, materials science, and \u2026 The transformative impact of **large** **language** \u2026", "entry_id": "http://arxiv.org/abs/2505.24302v1", "updated": "2025-05-30 07:28:20", "published": "2025-05-30 07:28:20", "authors": "Yike Wang;Shangbin Feng;Yulia Tsvetkov;Hannaneh Hajishirzi", "summary": "Large Language Models (LLMs) are increasingly used to support scientific\nresearch, but their knowledge of scientific advancements can quickly become\noutdated. We introduce ScienceMeter, a new framework for evaluating scientific\nknowledge update methods over scientific knowledge spanning the past, present,\nand future. ScienceMeter defines three metrics: knowledge preservation, the\nextent to which models' understanding of previously learned papers are\npreserved; knowledge acquisition, how well scientific claims from newly\nintroduced papers are acquired; and knowledge projection, the ability of the\nupdated model to anticipate or generalize to related scientific claims that may\nemerge in the future. Using ScienceMeter, we examine the scientific knowledge\nof LLMs on claim judgment and generation tasks across a curated dataset of\n15,444 scientific papers and 30,888 scientific claims from ten domains\nincluding medicine, biology, materials science, and computer science. We\nevaluate five representative knowledge update approaches including training-\nand inference-time methods. With extensive experiments, we find that the\nbest-performing knowledge update methods can preserve only 85.9% of existing\nknowledge, acquire 71.7% of new knowledge, and project 37.7% of future\nknowledge. Inference-based methods work for larger models, whereas smaller\nmodels require training to achieve comparable performance. Cross-domain\nanalysis reveals that performance on these objectives is correlated. Even when\napplying on specialized scientific LLMs, existing knowledge update methods fail\nto achieve these objectives collectively, underscoring that developing robust\nscientific knowledge update mechanisms is both crucial and challenging.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.24302v1;http://arxiv.org/pdf/2505.24302v1", "pdf_url": "http://arxiv.org/pdf/2505.24302v1"}, {"title": "Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation", "link": "https://arxiv.org/pdf/2505.23867", "details": "Z Liu, Z Hou, Y Di, K Yang, Z Sang, C Xie, J Yang, S Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Med** -PaLM 2 fine-tunes on advanced **medical** **question** - **answering** tasks using domain-\u2026 to achieve competitive results in multilingual **medical** **question** **answering** [11], while Radiology\u2026 -art multimodal **large** **language** **models** (MLLMs), including \u2026", "entry_id": "http://arxiv.org/abs/2505.23867v1", "updated": "2025-05-29 10:31:57", "published": "2025-05-29 10:31:57", "authors": "Zeyu Liu;Zhitian Hou;Yining Di;Kejing Yang;Zhijie Sang;Congkai Xie;Jingwen Yang;Siyuan Liu;Jialu Wang;Chunming Li;Ming Li;Hongxia Yang", "summary": "Multimodal large language models (MLLMs) have demonstrated promising\nprospects in healthcare, particularly for addressing complex medical tasks,\nsupporting multidisciplinary treatment (MDT), and enabling personalized\nprecision medicine. However, their practical deployment faces critical\nchallenges in resource efficiency, diagnostic accuracy, clinical\nconsiderations, and ethical privacy. To address these limitations, we propose\nInfi-Med, a comprehensive framework for medical MLLMs that introduces three key\ninnovations: (1) a resource-efficient approach through curating and\nconstructing high-quality supervised fine-tuning (SFT) datasets with minimal\nsample requirements, with a forward-looking design that extends to both\npretraining and posttraining phases; (2) enhanced multimodal reasoning\ncapabilities for cross-modal integration and clinical task understanding; and\n(3) a systematic evaluation system that assesses model performance across\nmedical modalities and task types. Our experiments demonstrate that Infi-Med\nachieves state-of-the-art (SOTA) performance in general medical reasoning while\nmaintaining rapid adaptability to clinical scenarios. The framework establishes\na solid foundation for deploying MLLMs in real-world healthcare settings by\nbalancing model effectiveness with operational constraints.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.23867v1;http://arxiv.org/pdf/2505.23867v1", "pdf_url": "http://arxiv.org/pdf/2505.23867v1"}, {"title": "Causal Reasoning with **Large Language Models** \u2013A ChatGPT Case Study", "link": "https://link.springer.com/chapter/10.1007/978-3-031-93429-2_24", "details": "A Rawal, J Rawal, A Raglin, Q Wang, Z Tang - International Conference on Human \u2026, 2025", "abstract": "\u2026 led to the boom of generative artificial intelligence (GenAI) models such as **large** **language** **models** (LLMs) [1, 2]. Both commercial and open-\u2026 GenAI/LLMs are now widely used in many fields ranging from sensitive domains such as **medicine** to \u2026"}]
