[{"title": "Refining Skewed Perceptions in Vision-Language Models through Visual Representations", "link": "https://arxiv.org/pdf/2405.14030", "details": "H Dai, S Joshi - arXiv preprint arXiv:2405.14030, 2024", "abstract": "Large vision-language models (VLMs), such as CLIP, have become foundational, demonstrating remarkable success across a variety of downstream tasks. Despite their advantages, these models, akin to other foundational systems, inherit biases \u2026"}, {"title": "Worldwide Federated Training of Language Models", "link": "https://arxiv.org/pdf/2405.14446", "details": "A Iacob, L Sani, B Marino, P Aleksandrov, ND Lane - arXiv preprint arXiv:2405.14446, 2024", "abstract": "The reliance of language model training on massive amounts of computation and vast datasets scraped from potentially low-quality, copyrighted, or sensitive data has come into question practically, legally, and ethically. Federated learning provides a \u2026"}, {"title": "Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation", "link": "https://arxiv.org/pdf/2405.13622", "details": "G Guinet, B Omidvar-Tehrani, A Deoras, L Callot - arXiv preprint arXiv:2405.13622, 2024", "abstract": "We propose a new method to measure the task-specific accuracy of Retrieval- Augmented Large Language Models (RAG). Evaluation is performed by scoring the RAG on an automatically-generated synthetic exam composed of multiple choice \u2026"}, {"title": "Implicit Personalization in Language Models: A Systematic Study", "link": "https://arxiv.org/pdf/2405.14808", "details": "Z Jin, N Heil, J Liu, S Dhuliawala, Y Qi, B Sch\u00f6lkopf\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Implicit Personalization (IP) is a phenomenon of language models inferring a user's background from the implicit cues in the input prompts and tailoring the response based on this inference. While previous work has touched upon various instances of \u2026"}, {"title": "Federated Learning in Healthcare: Model Misconducts, Security, Challenges, Applications, and Future Research Directions--A Systematic Review", "link": "https://arxiv.org/pdf/2405.13832", "details": "MS Ali, MM Ahsan, L Tasnim, S Afrin, K Biswas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Data privacy has become a major concern in healthcare due to the increasing digitization of medical records and data-driven medical research. Protecting sensitive patient information from breaches and unauthorized access is critical, as such \u2026"}, {"title": "Domain Generalization and Multidimensional Approach for Brain MRI Segmentation Using Contrastive Representation Transfer Learning Algorithm", "link": "https://www.igi-global.com/chapter/domain-generalization-and-multidimensional-approach-for-brain-mri-segmentation-using-contrastive-representation-transfer-learning-algorithm/346188", "details": "SSS Ramesh, A Jose, PR Samraysh, H Mulabagala\u2026 - Advancements in Clinical \u2026, 2024", "abstract": "Quantitative examination of human brain development when the individual is still in the womb is essential for aberrant characterisation. Therefore, the segmentation of magnetic resonance images (MRI) is a valuable asset for quantitative analysis \u2026"}, {"title": "Multi-turn Reinforcement Learning from Preference Human Feedback", "link": "https://arxiv.org/pdf/2405.14655", "details": "L Shani, A Rosenberg, A Cassel, O Lang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks. Existing methods \u2026"}, {"title": "Large Language Models Can Self-Correct with Minimal Effort", "link": "https://arxiv.org/pdf/2405.14092", "details": "Z Wu, Q Zeng, Z Zhang, Z Tan, C Shen, M Jiang - arXiv preprint arXiv:2405.14092, 2024", "abstract": "Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple \u2026"}]
