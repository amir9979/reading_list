[{"title": "Are Vision Language Models Ready for Clinical Diagnosis? A 3D Medical Benchmark for Tumor-centric Visual Question Answering", "link": "https://arxiv.org/pdf/2505.18915", "details": "Y Chen, W Xiao, PRAS Bassi, X Zhou, S Er\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 To systematically evaluate these dimensions, we present DeepTumorVQA, a diagnostic visual **question** **answering** (VQA) benchmark targeting abdominal tumors in CT scans. It comprises 9,262 CT volumes (3.7M slices) from 17 public datasets \u2026", "entry_id": "http://arxiv.org/abs/2505.18915v1", "updated": "2025-05-25 00:50:15", "published": "2025-05-25 00:50:15", "authors": "Yixiong Chen;Wenjie Xiao;Pedro R. A. S. Bassi;Xinze Zhou;Sezgin Er;Ibrahim Ethem Hamamci;Zongwei Zhou;Alan Yuille", "summary": "Vision-Language Models (VLMs) have shown promise in various 2D visual tasks,\nyet their readiness for 3D clinical diagnosis remains unclear due to stringent\ndemands for recognition precision, reasoning ability, and domain knowledge. To\nsystematically evaluate these dimensions, we present DeepTumorVQA, a diagnostic\nvisual question answering (VQA) benchmark targeting abdominal tumors in CT\nscans. It comprises 9,262 CT volumes (3.7M slices) from 17 public datasets,\nwith 395K expert-level questions spanning four categories: Recognition,\nMeasurement, Visual Reasoning, and Medical Reasoning. DeepTumorVQA introduces\nunique challenges, including small tumor detection and clinical reasoning\nacross 3D anatomy. Benchmarking four advanced VLMs (RadFM, M3D, Merlin,\nCT-CHAT), we find current models perform adequately on measurement tasks but\nstruggle with lesion recognition and reasoning, and are still not meeting\nclinical needs. Two key insights emerge: (1) large-scale multimodal pretraining\nplays a crucial role in DeepTumorVQA testing performance, making RadFM stand\nout among all VLMs. (2) Our dataset exposes critical differences in VLM\ncomponents, where proper image preprocessing and design of vision modules\nsignificantly affect 3D perception. To facilitate medical multimodal research,\nwe have released DeepTumorVQA as a rigorous benchmark:\nhttps://github.com/Schuture/DeepTumorVQA.", "comment": "NeurIPS 2025 datasets&benchmarks track submission", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.18915v1;http://arxiv.org/pdf/2505.18915v1", "pdf_url": "http://arxiv.org/pdf/2505.18915v1"}, {"title": "Medical Large Vision Language Models with Multi-Image Visual Ability", "link": "https://arxiv.org/pdf/2505.19031", "details": "X Yang, J Miao, Y Yuan, J Wang, Q Dou, J Li, PA Heng - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 each image and generate the corresponding **question** **answer** pairs. Unlike the composed **Med** -MIM instruction dataset, which can be created by simply appending location-specific suffixes (eg, \u201cin the first image\u201d) to the original QA pairs, the \u2026", "entry_id": "http://arxiv.org/abs/2505.19031v1", "updated": "2025-05-25 08:31:22", "published": "2025-05-25 08:31:22", "authors": "Xikai Yang;Juzheng Miao;Yuchen Yuan;Jiaze Wang;Qi Dou;Jinpeng Li;Pheng-Ann Heng", "summary": "Medical large vision-language models (LVLMs) have demonstrated promising\nperformance across various single-image question answering (QA) benchmarks, yet\ntheir capability in processing multi-image clinical scenarios remains\nunderexplored. Unlike single image based tasks, medical tasks involving\nmultiple images often demand sophisticated visual understanding capabilities,\nsuch as temporal reasoning and cross-modal analysis, which are poorly supported\nby current medical LVLMs. To bridge this critical gap, we present the Med-MIM\ninstruction dataset, comprising 83.2K medical multi-image QA pairs that span\nfour types of multi-image visual abilities (temporal understanding, reasoning,\ncomparison, co-reference). Using this dataset, we fine-tune Mantis and\nLLaVA-Med, resulting in two specialized medical VLMs: MIM-LLaVA-Med and\nMed-Mantis, both optimized for multi-image analysis. Additionally, we develop\nthe Med-MIM benchmark to comprehensively evaluate the medical multi-image\nunderstanding capabilities of LVLMs. We assess eight popular LVLMs, including\nour two models, on the Med-MIM benchmark. Experimental results show that both\nMed-Mantis and MIM-LLaVA-Med achieve superior performance on the held-in and\nheld-out subsets of the Med-MIM benchmark, demonstrating that the Med-MIM\ninstruction dataset effectively enhances LVLMs' multi-image understanding\ncapabilities in the medical domain.", "comment": "10 pages, 4 figures", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.19031v1;http://arxiv.org/pdf/2505.19031v1", "pdf_url": "http://arxiv.org/pdf/2505.19031v1"}, {"title": "CSE 7850: Final Project (Group 4) MedBench, a Benchmark Dataset for **Medical Large Language Models**", "link": "https://adityaamk.com/projects/MCB_Final_Project.pdf", "details": "S Bonam, AM Kumar, A Venkatesan, M Rastogi", "abstract": "\u2026 While there exist benchmark datasets for **medical** **Large** **Language** **Models** , our literature review only found datasets that either tested a \u2026 more than **question** **answering** ). A similar dataset, MultiMedQA, was created a group of researchers at \u2026"}, {"title": "Making Sense of the Unsensible: Reflection, Survey, and Challenges for XAI in Large Language Models Toward Human-Centered AI", "link": "https://arxiv.org/pdf/2505.20305", "details": "F Herrera - arXiv preprint arXiv:2505.20305, 2025", "abstract": "\u2026 As **large** **language** **models** (LLMs) become embedded in sensitive domains such as **healthcare** , law, and \u2026 **question** **answering** by prompting a model to (i) explain a concept, (ii) generate multiple-choice **questions** from its own explanation, and (iii) \u2026", "entry_id": "http://arxiv.org/abs/2505.20305v1", "updated": "2025-05-18 17:30:10", "published": "2025-05-18 17:30:10", "authors": "Francisco Herrera", "summary": "As large language models (LLMs) are increasingly deployed in sensitive\ndomains such as healthcare, law, and education, the demand for transparent,\ninterpretable, and accountable AI systems becomes more urgent. Explainable AI\n(XAI) acts as a crucial interface between the opaque reasoning of LLMs and the\ndiverse stakeholders who rely on their outputs in high-risk decisions. This\npaper presents a comprehensive reflection and survey of XAI for LLMs, framed\naround three guiding questions: Why is explainability essential? What technical\nand ethical dimensions does it entail? And how can it fulfill its role in\nreal-world deployment?\n  We highlight four core dimensions central to explainability in LLMs,\nfaithfulness, truthfulness, plausibility, and contrastivity, which together\nexpose key design tensions and guide the development of explanation strategies\nthat are both technically sound and contextually appropriate. The paper\ndiscusses how XAI can support epistemic clarity, regulatory compliance, and\naudience-specific intelligibility across stakeholder roles and decision\nsettings.\n  We further examine how explainability is evaluated, alongside emerging\ndevelopments in audience-sensitive XAI, mechanistic interpretability, causal\nreasoning, and adaptive explanation systems. Emphasizing the shift from\nsurface-level transparency to governance-ready design, we identify critical\nchallenges and future research directions for ensuring the responsible use of\nLLMs in complex societal contexts. We argue that explainability must evolve\ninto a civic infrastructure fostering trust, enabling contestability, and\naligning AI systems with institutional accountability and human-centered\ndecision-making.", "comment": "30 pages, 1 figure", "journal_ref": null, "primary_category": "cs.CY", "categories": "cs.CY", "links": "http://arxiv.org/abs/2505.20305v1;http://arxiv.org/pdf/2505.20305v1", "pdf_url": "http://arxiv.org/pdf/2505.20305v1"}, {"title": "Toward Human Centered Interactive Clinical Question Answering System", "link": "https://arxiv.org/pdf/2505.18928", "details": "D Albassam - arXiv preprint arXiv:2505.18928, 2025", "abstract": "\u2026 Electronic health records (EHRs) have become foundational to modern **healthcare** , serving as digital repositories of patient information. By \u2026 ideal target for **large** **language** **models** (LLMs), which excel at processing freetext inputs [1]. As a result \u2026", "entry_id": "http://arxiv.org/abs/2505.18928v1", "updated": "2025-05-25 01:31:31", "published": "2025-05-25 01:31:31", "authors": "Dina Albassam", "summary": "Unstructured clinical notes contain essential patient information but are\nchallenging for physicians to search and interpret efficiently. Although large\nlanguage models (LLMs) have shown promise in question answering (QA), most\nexisting systems lack transparency, usability, and alignment with clinical\nworkflows. This work introduces an interactive QA system that enables\nphysicians to query clinical notes via text or voice and receive extractive\nanswers highlighted directly in the note for traceability.\n  The system was built using OpenAI models with zero-shot prompting and\nevaluated across multiple metrics, including exact string match, word overlap,\nSentenceTransformer similarity, and BERTScore. Results show that while exact\nmatch scores ranged from 47 to 62 percent, semantic similarity scores exceeded\n87 percent, indicating strong contextual alignment even when wording varied.\n  To assess usability, the system was also evaluated using simulated clinical\npersonas. Seven diverse physician and nurse personas interacted with the system\nacross scenario-based tasks and provided structured feedback. The evaluations\nhighlighted strengths in intuitive design and answer accessibility, alongside\nopportunities for enhancing explanation clarity.", "comment": null, "journal_ref": null, "primary_category": "cs.HC", "categories": "cs.HC", "links": "http://arxiv.org/abs/2505.18928v1;http://arxiv.org/pdf/2505.18928v1", "pdf_url": "http://arxiv.org/pdf/2505.18928v1"}, {"title": "Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts", "link": "https://arxiv.org/pdf/2505.21324", "details": "Y Zhu, Y Guo, N Marchuck, A Sarker, Y Wang - arXiv preprint arXiv:2505.21324, 2025", "abstract": "\u2026 **healthcare** data pipelines. For example, ensemble approaches with multiple LLMs have been used to improve **medical** **question** **answering** \u2026 Xiao et al. introduced an LLM ensemble that uses a weighted majority voting among different \u2026", "entry_id": "http://arxiv.org/abs/2505.21324v1", "updated": "2025-05-27 15:22:01", "published": "2025-05-27 15:22:01", "authors": "Yuxin Zhu;Yuting Guo;Noah Marchuck;Abeed Sarker;Yun Wang", "summary": "Despite rapid advances in large language models (LLMs), their integration\nwith traditional supervised machine learning (ML) techniques that have proven\napplicability to medical data remains underexplored. This is particularly true\nfor psychiatric applications, where narrative data often exhibit nuanced\nlinguistic and contextual complexity, and can benefit from the combination of\nmultiple models with differing characteristics. In this study, we introduce an\nensemble framework for automatically classifying\nAttention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using\nnarrative transcripts. Our approach integrates three complementary models:\nLLaMA3, an open-source LLM that captures long-range semantic structure;\nRoBERTa, a pre-trained transformer model fine-tuned on labeled clinical\nnarratives; and a Support Vector Machine (SVM) classifier trained using\nTF-IDF-based lexical features. These models are aggregated through a majority\nvoting mechanism to enhance predictive robustness. The dataset includes 441\ninstances, including 352 for training and 89 for validation. Empirical results\nshow that the ensemble outperforms individual models, achieving an F$_1$ score\nof 0.71 (95\\% CI: [0.60-0.80]). Compared to the best-performing individual\nmodel (SVM), the ensemble improved recall while maintaining competitive\nprecision. This indicates the strong sensitivity of the ensemble in identifying\nADHD-related linguistic cues. These findings demonstrate the promise of hybrid\narchitectures that leverage the semantic richness of LLMs alongside the\ninterpretability and pattern recognition capabilities of traditional supervised\nML, offering a new direction for robust and generalizable psychiatric text\nclassification.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.21324v1;http://arxiv.org/pdf/2505.21324v1", "pdf_url": "http://arxiv.org/pdf/2505.21324v1"}, {"title": "Test-Time Learning for Large Language Models", "link": "https://arxiv.org/pdf/2505.20633", "details": "J Hu, Z Zhang, G Chen, X Wen, C Shuai, W Luo, B Xiao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 The datasets cover various task types, such as **question** **answering** , text classification, and summarization, while introducing variations in \u2026 create the **Medicine** dataset, aiming to assess the model\u2019s performance in **medical** dialogue \u2026", "entry_id": "http://arxiv.org/abs/2505.20633v1", "updated": "2025-05-27 02:18:59", "published": "2025-05-27 02:18:59", "authors": "Jinwu Hu;Zhitian Zhang;Guohao Chen;Xutao Wen;Chao Shuai;Wei Luo;Bin Xiao;Yuanqing Li;Mingkui Tan", "summary": "While Large Language Models (LLMs) have exhibited remarkable emergent\ncapabilities through extensive pre-training, they still face critical\nlimitations in generalizing to specialized domains and handling diverse\nlinguistic variations, known as distribution shifts. In this paper, we propose\na Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically\nadapts LLMs to target domains using only unlabeled test data during testing.\nSpecifically, we first provide empirical evidence and theoretical insights to\nreveal that more accurate predictions from LLMs can be achieved by minimizing\nthe input perplexity of the unlabeled test data. Based on this insight, we\nformulate the Test-Time Learning process of LLMs as input perplexity\nminimization, enabling self-supervised enhancement of LLM performance.\nFurthermore, we observe that high-perplexity samples tend to be more\ninformative for model optimization. Accordingly, we introduce a Sample\nEfficient Learning Strategy that actively selects and emphasizes these\nhigh-perplexity samples for test-time updates. Lastly, to mitigate catastrophic\nforgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA)\ninstead of full-parameter optimization, which allows lightweight model updates\nwhile preserving more original knowledge from the model. We introduce the\nAdaptEval benchmark for TTL and demonstrate through experiments that TLM\nimproves performance by at least 20% compared to original LLMs on domain\nknowledge adaptation.", "comment": "Accepted by ICML2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.20633v1;http://arxiv.org/pdf/2505.20633v1", "pdf_url": "http://arxiv.org/pdf/2505.20633v1"}, {"title": "Evaluating **Large Language Models** for Enhancing Radiology Specialty Examination: A Comparative Study with Human Performance", "link": "https://www.sciencedirect.com/science/article/pii/S1076633225004519", "details": "HY Liu, SJ Chen, W Wang, CH Lee, HH Hsu, SH Shen\u2026 - Academic Radiology, 2025", "abstract": "\u2026 **medical** knowledge, traditional test design faces challenges in maintaining accuracy and relevance. **Large** **language** **models** (LLMs) demonstrate potential in **medical** \u2026 This study evaluates LLM performance in radiology specialty exams \u2026"}, {"title": "Will Large Language Models Transform Clinical Prediction?", "link": "https://arxiv.org/pdf/2505.18246", "details": "Y Yildiz, G Nenadic, M Jani, DA Jenkins - arXiv preprint arXiv:2505.18246, 2025", "abstract": "\u2026 Background: **Large** **language** **models** (LLMs) are attracting increasing interest in **healthcare**. Their ability to summarise large datasets effectively, **answer** **questions** accurately, and generate synthesised text is \u2026 These capabilities are already \u2026", "entry_id": "http://arxiv.org/abs/2505.18246v1", "updated": "2025-05-23 17:02:04", "published": "2025-05-23 17:02:04", "authors": "Yusuf Yildiz;Goran Nenadic;Meghna Jani;David A. Jenkins", "summary": "Background: Large language models (LLMs) are attracting increasing interest\nin healthcare. Their ability to summarise large datasets effectively, answer\nquestions accurately, and generate synthesised text is widely recognised. These\ncapabilities are already finding applications in healthcare. Body: This\ncommentary discusses LLMs usage in the clinical prediction context and\nhighlight potential benefits and existing challenges. In these early stages,\nthe focus should be on extending the methodology, specifically on validation,\nfairness and bias evaluation, survival analysis and development of regulations.\nConclusion: We conclude that further work and domain-specific considerations\nneed to be made for full integration into the clinical prediction workflows.", "comment": "Submitted to: BMC Diagnostic and Prognostic Research", "journal_ref": null, "primary_category": "cs.CY", "categories": "cs.CY;cs.CL", "links": "http://arxiv.org/abs/2505.18246v1;http://arxiv.org/pdf/2505.18246v1", "pdf_url": "http://arxiv.org/pdf/2505.18246v1"}]
