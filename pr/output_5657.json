[{"title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge", "link": "https://arxiv.org/pdf/2407.19594", "details": "T Wu, W Yuan, O Golovneva, J Xu, Y Tian, J Jiao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can \u2026"}, {"title": "Multi-task heterogeneous graph learning on electronic health records", "link": "https://arxiv.org/pdf/2408.07569", "details": "TH Chan, G Yin, K Bae, L Yu - Neural Networks, 2024", "abstract": "Learning electronic health records (EHRs) has received emerging attention because of its capability to facilitate accurate medical diagnosis. Since the EHRs contain enriched information specifying complex interactions between entities, modeling \u2026"}, {"title": "Fine-tuning Language Models for Joint Rewriting and Completion of Code with Potential Bugs", "link": "https://aclanthology.org/2024.findings-acl.938.pdf", "details": "D Wang, J Zhao, H Pei, S Tan, S Zha - Findings of the Association for Computational \u2026, 2024", "abstract": "Handling drafty partial code remains a notable challenge in real-time code suggestion applications. Previous work has demonstrated shortcomings of large language models of code (CodeLLMs) in completing partial code with potential bugs \u2026"}, {"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "link": "https://arxiv.org/pdf/2408.12337", "details": "KS Phogat, SA Puranam, S Dasaratha, C Harsha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain \u2026"}, {"title": "Prompting Encoder Models for Zero-Shot Classification: A Cross-Domain Study in Italian", "link": "https://arxiv.org/pdf/2407.20654", "details": "S Auriemma, M Miliani, M Madeddu, A Bondielli\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Addressing the challenge of limited annotated data in specialized fields and low- resource languages is crucial for the effective use of Language Models (LMs). While most Large Language Models (LLMs) are trained on general-purpose English \u2026"}, {"title": "MedSyn: LLM-based Synthetic Medical Text Generation Framework", "link": "https://arxiv.org/pdf/2408.02056", "details": "G Kumichev, P Blinov, Y Kuzkina, V Goncharov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating synthetic text addresses the challenge of data availability in privacy- sensitive domains such as healthcare. This study explores the applicability of synthetic data in real-world medical settings. We introduce MedSyn, a novel medical \u2026"}, {"title": "Prompt Based CVAE Data Augmentation for Few-Shot Intention Detection", "link": "https://link.springer.com/chapter/10.1007/978-981-97-5498-4_24", "details": "J Xue, C Yin, C Li, J Bai, H Chen, W Rong - International Conference on Knowledge \u2026, 2024", "abstract": "Intent detection is an important task for AI assistants when communicating with users. However, in real life, the number of intents that need to be recognized in the intent recognition task continues to increase. It is often difficult to manually label new \u2026"}, {"title": "Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution", "link": "https://arxiv.org/pdf/2408.10548", "details": "Y Ruan, X Lan, J Ma, Y Dong, K He, M Feng - arXiv preprint arXiv:2408.10548, 2024", "abstract": "Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds \u2026"}, {"title": "Envisioning Class Entity Reasoning by Large Language Models for Few-shot Learning", "link": "https://arxiv.org/pdf/2408.12469", "details": "M Liu, F Wu, B Li, Z Lu, Y Yu, X Li - arXiv preprint arXiv:2408.12469, 2024", "abstract": "Few-shot learning (FSL) aims to recognize new concepts using a limited number of visual samples. Existing approaches attempt to incorporate semantic information into the limited visual data for category understanding. However, these methods often \u2026"}]
