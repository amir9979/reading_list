[{"title": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering", "link": "https://arxiv.org/pdf/2405.12701", "details": "M Jeong, H Hwang, C Yoon, T Lee, J Kang - arXiv preprint arXiv:2405.12701, 2024", "abstract": "In the medical domain, numerous scenarios necessitate the long-form generation ability of large language models (LLMs). Specifically, when addressing patients' questions, it is essential that the model's response conveys factual claims \u2026"}, {"title": "StatBot. Swiss: Bilingual Open Data Exploration in Natural Language", "link": "https://arxiv.org/pdf/2406.03170", "details": "F Nooralahzadeh, Y Zhang, E Smith, S Maennel\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The potential for improvements brought by Large Language Models (LLMs) in Text-to- SQL systems is mostly assessed on monolingual English datasets. However, LLMs' performance for other languages remains vastly unexplored. In this work, we release \u2026"}, {"title": "Interpretable Multi-task Learning with Shared Variable Embeddings", "link": "https://arxiv.org/pdf/2405.06330", "details": "M \u017belaszczyk, J Ma\u0144dziuk - arXiv preprint arXiv:2405.06330, 2024", "abstract": "This paper proposes a general interpretable predictive system with shared information. The system is able to perform predictions in a multi-task setting where distinct tasks are not bound to have the same input/output structure. Embeddings of \u2026"}, {"title": "TAeKD: Teacher Assistant Enhanced Knowledge Distillation for Closed-Source Multilingual Neural Machine Translation", "link": "https://aclanthology.org/2024.lrec-main.1350.pdf", "details": "B Lv, X Liu, K Wei, P Luo, Y Yu - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Knowledge Distillation (KD) serves as an efficient method for transferring language knowledge from open-source large language models (LLMs) to more computationally efficient models. However, challenges arise when attempting to \u2026"}, {"title": "An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation", "link": "https://arxiv.org/pdf/2405.07673", "details": "L Pan, D Xiong - arXiv preprint arXiv:2405.07673, 2024", "abstract": "Massively multilingual neural machine translation (MMNMT) has been proven to enhance the translation quality of low-resource languages. In this paper, we empirically investigate the translation robustness of Indonesian-Chinese translation \u2026"}, {"title": "Exploring and Mitigating Shortcut Learning for Generative Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.602.pdf", "details": "Z Sun, Y Xiao, J Li, Y Ji, W Chen, M Zhang - Proceedings of the 2024 Joint \u2026, 2024", "abstract": "Recent generative large language models (LLMs) have exhibited incredible instruction-following capabilities while keeping strong task completion ability, even without task-specific fine-tuning. Some works attribute this to the bonus of the new \u2026"}, {"title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective", "link": "https://arxiv.org/pdf/2405.16747", "details": "A Tomihari, I Sato - arXiv preprint arXiv:2405.16747, 2024", "abstract": "The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data. This success is largely \u2026"}, {"title": "Towards Better Vision-Inspired Vision-Language Models", "link": "https://www.lamda.nju.edu.cn/caoyh/files/VIVL.pdf", "details": "YH Cao, K Ji, Z Huang, C Zheng, J Liu, J Wang, J Chen\u2026", "abstract": "Vision-language (VL) models have achieved unprecedented success recently, in which the connection module is the key to bridge the modality gap. Nevertheless, the abundant visual clues are not sufficiently exploited in most existing methods. On the \u2026"}, {"title": "Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning", "link": "https://arxiv.org/pdf/2405.05615", "details": "S Jie, Y Tang, N Ding, ZH Deng, K Han, Y Wang - arXiv preprint arXiv:2405.05615, 2024", "abstract": "Current solutions for efficiently constructing large vision-language (VL) models follow a two-step paradigm: projecting the output of pre-trained vision encoders to the input space of pre-trained language models as visual prompts; and then transferring the \u2026"}]
