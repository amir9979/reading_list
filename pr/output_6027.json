[{"title": "Beyond the Hype: A dispassionate look at vision-language models in medical scenario", "link": "https://arxiv.org/pdf/2408.08704", "details": "Y Nan, H Zhou, X Xing, G Yang - arXiv preprint arXiv:2408.08704, 2024", "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across diverse tasks, garnering significant attention in AI communities. However, their performance and reliability in specialized \u2026"}, {"title": "Towards Holistic Disease Risk Prediction using Small Language Models", "link": "https://arxiv.org/pdf/2408.06943", "details": "L Bj\u00f6rkdahl, O Pauli, J \u00d6stman, C Ceccobello\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Data in the healthcare domain arise from a variety of sources and modalities, such as x-ray images, continuous measurements, and clinical notes. Medical practitioners integrate these diverse data types daily to make informed and accurate decisions \u2026"}, {"title": "VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models", "link": "https://aclanthology.org/2024.acl-long.658.pdf", "details": "Q Cao, J Cheng, X Liang, L Lin - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Despite the significant success of large vision-language models (LVLMs), some studies have revealed that LVLMs suffer from the hallucination problem, where the LVLMs' response contains descriptions of non-existent objects. Although various \u2026"}, {"title": "Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models", "link": "https://aclanthology.org/2024.acl-long.404.pdf", "details": "Q Ai, J Li, J Dai, J Zhou, L Liu, H Jiang, S Shi - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields. In this paper, we propose a new paradigm for interactive and instructional graph data \u2026"}, {"title": "Making Large Vision Language Models to be Good Few-shot Learners", "link": "https://arxiv.org/pdf/2408.11297", "details": "F Liu, W Cai, J Huo, C Zhang, D Chen, J Zhou - arXiv preprint arXiv:2408.11297, 2024", "abstract": "Few-shot classification (FSC) is a fundamental yet challenging task in computer vision that involves recognizing novel classes from limited data. While previous methods have focused on enhancing visual features or incorporating additional \u2026"}, {"title": "Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2408.02032", "details": "F Huo, W Xu, Z Zhang, H Wang, Z Chen, P Zhao - arXiv preprint arXiv:2408.02032, 2024", "abstract": "While Large Vision-Language Models (LVLMs) have rapidly advanced in recent years, the prevalent issue known as thehallucination'problem has emerged as a significant bottleneck, hindering their real-world deployments. Existing methods \u2026"}, {"title": "Evaluating Attribute Comprehension in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2408.13898", "details": "H Zhang, Z Yang, Y Liu, X Wang, Z He, K Liang, Z Ma - arXiv preprint arXiv \u2026, 2024", "abstract": "Currently, large vision-language models have gained promising progress on many downstream tasks. However, they still suffer many challenges in fine-grained visual understanding tasks, such as object attribute comprehension. Besides, there have \u2026"}, {"title": "SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for Large-scale Vision-Language Models", "link": "https://arxiv.org/pdf/2408.12114", "details": "Y Yu, S Chung, BK Lee, YM Ro - arXiv preprint arXiv:2408.12114, 2024", "abstract": "Large-scale Vision-Language Models (LVLMs) have significantly advanced with text- aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate \u2026"}, {"title": "Towards Analyzing and Mitigating Sycophancy in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2408.11261", "details": "Y Zhao, R Zhang, J Xiao, C Ke, R Hou, Y Hao, Q Guo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) have shown significant capability in vision- language understanding. However, one critical issue that persists in these models is sycophancy, which means models are unduly influenced by leading or deceptive \u2026"}]
