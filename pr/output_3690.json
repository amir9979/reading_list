[{"title": "WSI-VQA: Interpreting Whole Slide Images by Generative Visual Question Answering", "link": "https://arxiv.org/pdf/2407.05603", "details": "P Chen, C Zhu, S Zheng, H Li, L Yang - arXiv preprint arXiv:2407.05603, 2024", "abstract": "Whole slide imaging is routinely adopted for carcinoma diagnosis and prognosis. Abundant experience is required for pathologists to achieve accurate and reliable diagnostic results of whole slide images (WSI). The huge size and heterogeneous \u2026"}, {"title": "Inverse Probability of Treatment Weighting with Deep Sequence Models Enables Accurate treatment effect Estimation from Electronic Health Records", "link": "https://arxiv.org/pdf/2406.08851", "details": "J Lee, S Ma, N Serban, S Yang - arXiv preprint arXiv:2406.08851, 2024", "abstract": "Observational data have been actively used to estimate treatment effect, driven by the growing availability of electronic health records (EHRs). However, EHRs typically consist of longitudinal records, often introducing time-dependent confoundings that \u2026"}, {"title": "Vision-Language Models Meet Meteorology: Developing Models for Extreme Weather Events Detection with Heatmaps", "link": "https://arxiv.org/pdf/2406.09838", "details": "J Chen, P Zhou, Y Hua, D Chong, M Cao, Y Li, Z Yuan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Real-time detection and prediction of extreme weather protect human lives and infrastructure. Traditional methods rely on numerical threshold setting and manual interpretation of weather heatmaps with Geographic Information Systems (GIS) \u2026"}, {"title": "On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning", "link": "https://arxiv.org/pdf/2406.14197", "details": "F Nowak, A Svete, A Butoi, R Cotterell - arXiv preprint arXiv:2406.14197, 2024", "abstract": "The performance of modern language models (LMs) has been improved by chain-of- thought (CoT) reasoning, ie, the process of generating intermediate results that guide the model towards a final answer. A possible explanation for this improvement is that \u2026"}, {"title": "Vision-Language Models under Cultural and Inclusive Considerations", "link": "https://arxiv.org/pdf/2407.06177", "details": "A Karamolegkou, P Rust, Y Cao, R Cui, A S\u00f8gaard\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision-language models (VLMs) can assist visually impaired people by describing images from their daily lives. Current evaluation datasets may not reflect diverse cultural user backgrounds or the situational context of this use case. To \u2026"}, {"title": "Merlin: A Vision Language Foundation Model for 3D Computed Tomography", "link": "https://www.researchsquare.com/article/rs-4546309/latest.pdf", "details": "A Chaudhari, L Blankemeier, JP Cohen, A Kumar\u2026 - 2024", "abstract": "Over 85 million computed tomography (CT) scans are performed annually in the US, of which approximately one quarter focus on the abdomen. Given the current shortage of both general and specialized radiologists, there is a large impetus to use \u2026"}]
