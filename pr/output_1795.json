[{"title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning", "link": "https://arxiv.org/pdf/2405.10292", "details": "Y Zhai, H Bai, Z Lin, J Pan, S Tong, Y Zhou, A Suhr\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision-language models (VLMs) fine-tuned on specialized visual instruction- following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently \u2026"}, {"title": "Large Language Multimodal Models for New-Onset Type 2 Diabetes Prediction using Five-Year Cohort Electronic Health Records", "link": "https://www.researchsquare.com/article/rs-4414387/latest.pdf", "details": "JE Ding, NMT Phan, WC Peng, JZ Wang, CC Chug\u2026 - 2024", "abstract": "Type 2 diabetes mellitus (T2DM) is a prevalent health challenge faced by countries worldwide. In this study, we propose a novel large language multimodal models (LLMMs) framework incorporating multimodal data from clinical notes and laboratory \u2026"}, {"title": "Elements of World Knowledge (EWOK): A cognition-inspired framework for evaluating basic world knowledge in language models", "link": "https://arxiv.org/pdf/2405.09605", "details": "AA Ivanova, A Sathe, B Lipkin, U Kumar, S Radkani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The ability to build and leverage world models is essential for a general-purpose AI agent. Testing such capabilities is hard, in part because the building blocks of world models are ill-defined. We present Elements of World Knowledge (EWOK), a \u2026"}, {"title": "GRAMMAR: Grounded and Modular Evaluation of Domain-Specific Retrieval-Augmented Language Models", "link": "https://arxiv.org/pdf/2404.19232", "details": "X Li, M Liu, S Gao - arXiv preprint arXiv:2404.19232, 2024", "abstract": "Retrieval-augmented Generation (RAG) systems have been actively studied and deployed across various industries to query on domain-specific knowledge base. However, evaluating these systems presents unique challenges due to the scarcity of \u2026"}, {"title": "An in-depth evaluation of federated learning on biomedical natural language processing for information extraction", "link": "https://www.nature.com/articles/s41746-024-01126-4", "details": "L Peng, G Luo, S Zhou, J Chen, Z Xu, J Sun, R Zhang - NPJ Digital Medicine, 2024", "abstract": "Abstract Language models (LMs) such as BERT and GPT have revolutionized natural language processing (NLP). However, the medical field faces challenges in training LMs due to limited data access and privacy constraints imposed by \u2026"}, {"title": "Large Language Models for Social Determinants of Health Information Extraction from Clinical Notes-A Generalizable Approach across Institutions", "link": "https://www.medrxiv.org/content/10.1101/2024.05.21.24307726.full.pdf", "details": "VK Keloth, S Selek, Q Chen, C Gilman, S Fu, Y Dang\u2026 - medRxiv, 2024", "abstract": "The consistent and persuasive evidence illustrating the influence of social determinants on health has prompted a growing realization throughout the health care sector that enhancing health and health equity will likely depend, at least to \u2026"}, {"title": "ExcluIR: Exclusionary Neural Information Retrieval", "link": "https://arxiv.org/pdf/2404.17288", "details": "W Zhang, M Zhang, S Wu, J Pei, Z Ren, M de Rijke\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Exclusion is an important and universal linguistic skill that humans use to express what they do not want. However, in information retrieval community, there is little research on exclusionary retrieval, where users express what they do not want in \u2026"}, {"title": "Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "link": "https://arxiv.org/pdf/2405.10431", "details": "S Furniturewala, S Jandial, A Java, P Banerjee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing debiasing techniques are typically training-based or require access to the model's internals and output distributions, so they are inaccessible to end-users looking to adapt LLM outputs for their particular needs. In this study, we examine \u2026"}, {"title": "Characterizing the Accuracy-Efficiency Trade-off of Low-rank Decomposition in Language Models", "link": "https://arxiv.org/pdf/2405.06626", "details": "C Moar, M Pellauer, H Kwon - arXiv preprint arXiv:2405.06626, 2024", "abstract": "Large language models (LLMs) have emerged and presented their general problem- solving capabilities with one model. However, the model size has increased dramatically with billions of parameters to enable such broad problem-solving \u2026"}]
