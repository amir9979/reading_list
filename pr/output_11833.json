[{"title": "Disentangled Contrastive Learning from Synthetic Matching Pairs for Targeted Chest X-ray Generation", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10844299.pdf", "details": "E Kim, S Lee, KM Lee - IEEE Access, 2025", "abstract": "Disentangled generation enables the synthesis of images with explicit control over disentangled attributes. However, traditional generative models often struggle to independently disentangle these attributes while maintaining the ability to generate \u2026"}, {"title": "MatchAnything: Universal Cross-Modality Image Matching with Large-Scale Pre-Training", "link": "https://arxiv.org/pdf/2501.07556%3F", "details": "X He, H Yu, S Peng, D Tan, Z Shen, H Bao, X Zhou - arXiv preprint arXiv:2501.07556, 2025", "abstract": "Image matching, which aims to identify corresponding pixel locations between images, is crucial in a wide range of scientific disciplines, aiding in image registration, fusion, and analysis. In recent years, deep learning-based image \u2026"}, {"title": "Historical facts learning from Long-Short Terms with Language Model for Temporal Knowledge Graph Reasoning", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324004060", "details": "W Xu, B Liu, M Peng, Z Jiang, X Jia, K Liu, L Liu\u2026 - Information Processing & \u2026, 2025", "abstract": "Abstract Temporal Knowledge Graph Reasoning (TKGR) aims to reason the missing parts in TKGs based on historical facts from different time periods. Traditional GCN- based TKGR models depend on structured relations between entities. To utilize the \u2026"}, {"title": "On Linear Representations and Pretraining Data Frequency in Language Models", "link": "https://openreview.net/pdf%3Fid%3D90tmmXyaaV", "details": "J Merullo, NA Smith, S Wiegreffe, Y Elazar", "abstract": "Pretraining data has a direct impact on the behaviors and quality of language models (LMs), but we only understand the most basic principles of this relationship. While most work focuses on pretraining data and downstream task behavior, we look at the \u2026"}, {"title": "Efficient GPT-4V Level Multimodal Large Language Model for Deployment on Edge Devices", "link": "https://www.researchsquare.com/article/rs-5830327/latest.pdf", "details": "Y Yao, T Yu, A Zhang, C Wang, J Cui, H Zhu, T Cai\u2026 - 2025", "abstract": "The recent surge of Multimodal Large Language Models (MLLMs) has fundamentally reshaped the landscape of AI research and industry, shedding light on a promising path toward the next AI milestone. However, significant challenges remain \u2026"}, {"title": "AmalREC: A Dataset for Relation Extraction and Classification Leveraging Amalgamation of Large Language Models", "link": "https://arxiv.org/pdf/2412.20427", "details": "P Pandya, MB Vora, S Bharadwaj, A Anand - arXiv preprint arXiv:2412.20427, 2024", "abstract": "Existing datasets for relation classification and extraction often exhibit limitations such as restricted relation types and domain-specific biases. This work presents a generic framework to generate well-structured sentences from given tuples with the \u2026"}]
