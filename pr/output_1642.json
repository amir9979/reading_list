'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Autonomous Data Selection with Language Models for Mat'
[{"title": "THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2405.05256", "details": "P Kaul, Z Li, H Yang, Y Dukler, A Swaminathan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term\" Type I hallucinations\". Instead, they focus on \u2026"}, {"title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision", "link": "https://arxiv.org/pdf/2405.04086", "details": "Y Tong, S Wang, D Li, Y Wang, S Han, Z Lin, C Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations \u2026"}, {"title": "Enhancing Data Quality in Federated Fine-Tuning of Large Language Models", "link": "https://openreview.net/pdf%3Fid%3DaHD3WJ7gQ5", "details": "W Zhao, Y Du, ND Lane, S Chen, Y Wang - ICLR 2024 Workshop on Navigating and \u2026", "abstract": "In the current landscape of large language model training, there is a significant reliance on public domain data, which is nearing exhaustion according to recent research. To further scale up, it is crucial to incorporate collaboration among multiple \u2026"}, {"title": "Argumentative Large Language Models for Explainable and Contestable Decision-Making", "link": "https://arxiv.org/pdf/2405.02079", "details": "G Freedman, A Dejl, D Gorur, X Yin, A Rago, F Toni - arXiv preprint arXiv:2405.02079, 2024", "abstract": "The diversity of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them a promising candidate for use in decision-making. However, they are currently limited \u2026"}, {"title": "Graph learning with label attention and hyperbolic embedding for temporal event prediction in healthcare", "link": "https://napier-repository.worktribe.com/preview/3635477/1-s2.0-S0925231224005071-main.pdf", "details": "U Naseem, S Thapa, Q Zhang, S Wang, J Rashid, L Hu\u2026 - Neurocomputing, 2024", "abstract": "The digitization of healthcare systems has led to the proliferation of electronic health records (EHRs), serving as comprehensive repositories of patient information. However, the vast volume and complexity of EHR data present challenges in \u2026"}, {"title": "Comparison of Pretrained Models for Optimized Transformer Based Question Answering System", "link": "https://ieeexplore.ieee.org/abstract/document/10527306/", "details": "T Gokcimen, B Das - 2024 12th International Symposium on Digital \u2026, 2024", "abstract": "This study delves into the evaluation and optimization of transformer-based models for question-answering systems, focusing on health-related inquiries. Utilizing a specialized dataset extracted from Wikipedia articles, transformer models, namely \u2026"}, {"title": "MedDr: Diagnosis-Guided Bootstrapping for Large-Scale Medical Vision-Language Learning", "link": "https://arxiv.org/pdf/2404.15127", "details": "S He, Y Nie, Z Chen, Z Cai, H Wang, S Yang, H Chen - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancement of large-scale vision-language models has showcased remarkable capabilities across various tasks. However, the lack of extensive and high-quality image-text data in medicine has greatly hindered the development of \u2026"}, {"title": "Model & Data Insights using Pre-trained Language Models", "link": "https://openreview.net/pdf%3Fid%3DL5T3ZqsD0j", "details": "S Asgari, A Khani, AH Khasahmadi, A Sanghi\u2026 - ICLR 2024 Workshop on \u2026", "abstract": "We propose TExplain, using language models to interpret pre-trained image classifiers' features. Our approach connects the feature space of image classifiers with language models, generating explanatory sentences during inference. By \u2026"}, {"title": "Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for Multi-hop Question Answering", "link": "https://arxiv.org/pdf/2404.14464", "details": "L Jiapeng, L Runze, L Yabo, Z Tong, L Mingling\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multi-hop question answering is a knowledge-intensive complex problem. Large Language Models (LLMs) use their Chain of Thoughts (CoT) capability to reason complex problems step by step, and retrieval-augmentation can effectively alleviate \u2026"}]
