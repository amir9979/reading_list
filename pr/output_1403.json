'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Detecting Edited Knowledge in Language Models](https:/'
[{"title": "Recall Them All: Retrieval-Augmented Language Models for Long Object List Extraction from Long Documents", "link": "https://arxiv.org/pdf/2405.02732", "details": "S Singhania, S Razniewski, G Weikum - arXiv preprint arXiv:2405.02732, 2024", "abstract": "Methods for relation extraction from text mostly focus on high precision, at the cost of limited recall. High recall is crucial, though, to populate long lists of object entities that stand in a specific relation with a given subject. Cues for relevant objects can be \u2026"}, {"title": "LMD3: Language Model Data Density Dependence", "link": "https://arxiv.org/pdf/2405.06331", "details": "J Kirchenbauer, G Honke, G Somepalli, J Geiping\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We develop a methodology for analyzing language model task performance at the individual example level based on training data density estimation. Experiments with paraphrasing as a controlled intervention on finetuning data demonstrate that \u2026"}, {"title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2405.02659", "details": "T Zhang, D Li, Q Chen, C Wang, L Huang, H Xue, X He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Retrieval-augmented large language models (LLMs) leverage relevant content retrieved by information retrieval systems to generate correct responses, aiming to alleviate the hallucination problem. However, existing retriever-responder methods \u2026"}, {"title": "Redefining Information Retrieval of Structured Database via Large Language Models", "link": "https://arxiv.org/pdf/2405.05508", "details": "M Wang, Y Zhang, Q Zhao, J Yang, H Zhang - arXiv preprint arXiv:2405.05508, 2024", "abstract": "Retrieval augmentation is critical when Language Models (LMs) exploit non- parametric knowledge related to the query through external knowledge bases before reasoning. The retrieved information is incorporated into LMs as context alongside \u2026"}, {"title": "Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games", "link": "https://openreview.net/pdf%3Fid%3Ddhy1NBeusb", "details": "Y Zhang, J Lu, N Jaitly - ICLR 2024 Workshop: How Far Are We From AGI", "abstract": "Large language models (LLMs) are effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of \u2026"}, {"title": "Backdoor Removal for Generative Large Language Models", "link": "https://arxiv.org/pdf/2405.07667", "details": "H Li, Y Chen, Z Zheng, Q Hu, C Chan, H Liu, Y Song - arXiv preprint arXiv \u2026, 2024", "abstract": "With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased \u2026"}, {"title": "A Survey on RAG Meets LLMs: Towards Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2405.06211", "details": "Y Ding, W Fan, L Ning, S Wang, H Li, D Yin, TS Chua\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) techniques can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-generated content \u2026"}, {"title": "NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli", "link": "https://arxiv.org/pdf/2405.02814", "details": "X Wang, C Li, Y Chang, J Wang, Y Wu - arXiv preprint arXiv:2405.02814, 2024", "abstract": "Large Language Models (LLMs) have become integral to a wide spectrum of applications, ranging from traditional computing tasks to advanced artificial intelligence (AI) applications. This widespread adoption has spurred extensive \u2026"}, {"title": "Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense", "link": "https://arxiv.org/pdf/2405.04655", "details": "S Shen, L Logeswaran, M Lee, H Lee, S Poria\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated substantial commonsense understanding through numerous benchmark evaluations. However, their understanding of cultural commonsense remains largely unexamined. In this paper \u2026"}]
