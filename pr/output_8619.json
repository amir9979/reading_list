[{"title": "Reducing the Scope of Language Models with Circuit Breakers", "link": "https://arxiv.org/pdf/2410.21597", "details": "D Yunis, S Huo, C Gunasekara, D Contractor - arXiv preprint arXiv:2410.21597, 2024", "abstract": "Language models are now deployed in a wide variety of user-facing applications, often for specific purposes like answering questions about documentation or acting as coding assistants. As these models are intended for particular purposes, they \u2026"}, {"title": "Do Advanced Language Models Eliminate the Need for Prompt Engineering in Software Engineering?", "link": "https://arxiv.org/pdf/2411.02093", "details": "G Wang, Z Sun, Z Gong, S Ye, Y Chen, Y Zhao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have significantly advanced software engineering (SE) tasks, with prompt engineering techniques enhancing their performance in code- related areas. However, the rapid development of foundational LLMs such as the \u2026"}, {"title": "EEE-Bench: A Comprehensive Multimodal Electrical And Electronics Engineering Benchmark", "link": "https://arxiv.org/pdf/2411.01492", "details": "M Li, J Zhong, T Chen, Y Lai, K Psounis - arXiv preprint arXiv:2411.01492, 2024", "abstract": "Recent studies on large language models (LLMs) and large multimodal models (LMMs) have demonstrated promising skills in various domains including science and mathematics. However, their capability in more challenging and real-world \u2026"}, {"title": "Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios", "link": "https://arxiv.org/pdf/2411.02708", "details": "Y Dang, M Gao, Y Yan, X Zou, Y Gu, A Liu, X Hu - arXiv preprint arXiv:2411.02708, 2024", "abstract": "Ensuring that Multimodal Large Language Models (MLLMs) maintain consistency in their responses is essential for developing trustworthy multimodal intelligence. However, existing benchmarks include many samples where all MLLMs\\textit {exhibit \u2026"}, {"title": "SafeBench: A Safety Evaluation Framework for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2410.18927", "details": "Z Ying, A Liu, S Liang, L Huang, J Guo, W Zhou, X Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal Large Language Models (MLLMs) are showing strong safety concerns (eg, generating harmful outputs for users), which motivates the development of safety evaluation benchmarks. However, we observe that existing safety benchmarks for \u2026"}, {"title": "CLR-Bench: Evaluating Large Language Models in College-level Reasoning", "link": "https://arxiv.org/pdf/2410.17558", "details": "J Dong, Z Hong, Y Bei, F Huang, X Wang, X Huang - arXiv preprint arXiv:2410.17558, 2024", "abstract": "Large language models (LLMs) have demonstrated their remarkable performance across various language understanding tasks. While emerging benchmarks have been proposed to evaluate LLMs in various domains such as mathematics and \u2026"}, {"title": "Sparsing Law: Towards Large Language Models with Greater Activation Sparsity", "link": "https://arxiv.org/pdf/2411.02335", "details": "Y Luo, C Song, X Han, Y Chen, C Xiao, Z Liu, M Sun - arXiv preprint arXiv \u2026, 2024", "abstract": "Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting \u2026"}, {"title": "Textual Aesthetics in Large Language Models", "link": "https://arxiv.org/pdf/2411.02930", "details": "L Jiang, S Huang, X Wu, F Wei - arXiv preprint arXiv:2411.02930, 2024", "abstract": "Image aesthetics is a crucial metric in the field of image generation. However, textual aesthetics has not been sufficiently explored. With the widespread application of large language models (LLMs), previous work has primarily focused on the \u2026"}, {"title": "Adversarial Attacks on Large Language Models Using Regularized Relaxation", "link": "https://arxiv.org/pdf/2410.19160", "details": "SJ Chacko, S Biswas, CM Islam, FT Liza, X Liu - arXiv preprint arXiv:2410.19160, 2024", "abstract": "As powerful Large Language Models (LLMs) are now widely used for numerous practical applications, their safety is of critical importance. While alignment techniques have significantly improved overall safety, LLMs remain vulnerable to \u2026"}]
