[{"title": "EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models", "link": "https://arxiv.org/pdf/2502.06663", "details": "X Xing, Z Liu, S Xiao, B Gao, Y Liang, W Zhang, H Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Modern large language models (LLMs) driven by scaling laws, achieve intelligence emergency in large model sizes. Recently, the increasing concerns about cloud costs, latency, and privacy make it an urgent requirement to develop compact edge \u2026"}, {"title": "Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More", "link": "https://arxiv.org/pdf/2502.11494", "details": "Z Wen, Y Gao, S Wang, J Zhang, Q Zhang, W Li, C He\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision tokens in multimodal large language models often dominate huge computational overhead due to their excessive length compared to linguistic modality. Abundant recent methods aim to solve this problem with token pruning \u2026"}, {"title": "SKI Models: Skeleton Induced Vision-Language Embeddings for Understanding Activities of Daily Living", "link": "https://arxiv.org/pdf/2502.03459", "details": "A Sinha, D Reilly, F Bremond, P Wang, S Das - arXiv preprint arXiv:2502.03459, 2025", "abstract": "The introduction of vision-language models like CLIP has enabled the development of foundational video models capable of generalizing to unseen videos and human actions. However, these models are typically trained on web videos, which often fail \u2026"}, {"title": "Str-GCL: Structural Commonsense Driven Graph Contrastive Learning", "link": "https://openreview.net/pdf%3Fid%3DzefCoSncYR", "details": "D He, Y Huang, J Zhao, X Wang, Z Wang - THE WEB CONFERENCE 2025", "abstract": "Graph Contrastive Learning (GCL) is a widely adopted approach in unsupervised representation learning, utilizing representational constraints to derive effective embeddings. However, current GCL methods primarily focus on capturing implicit \u2026"}, {"title": "Rethinking Homogeneity of Vision and Text Tokens in Large Vision-and-Language Models", "link": "https://arxiv.org/pdf/2502.01906", "details": "CW Kuo, S Zhu, F Chen, X Shen, L Wen - arXiv preprint arXiv:2502.01906, 2025", "abstract": "Large vision-and-language models (LVLMs) typically treat visual and textual embeddings as homogeneous inputs to a large language model (LLM). However, these inputs are inherently different: visual inputs are multi-dimensional and \u2026"}, {"title": "Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning", "link": "https://arxiv.org/pdf/2502.11751", "details": "Y Pang, B Yang, H Tu, Y Cao, Z Zhang - arXiv preprint arXiv:2502.11751, 2025", "abstract": "Although Large Language Models (LLMs) excel in reasoning and generation for language tasks, they are not specifically designed for multimodal challenges. Training Multimodal Large Language Models (MLLMs), however, is resource \u2026"}, {"title": "Prediction of molecular subtypes for endometrial cancer based on hierarchical foundation model", "link": "https://academic.oup.com/bioinformatics/advance-article-pdf/doi/10.1093/bioinformatics/btaf059/61841826/btaf059.pdf", "details": "H Cui, Q Guo, J Xu, X Wu, C Cai, Y Jiao, W Ming\u2026 - Bioinformatics, 2025", "abstract": "Motivation Endometrial cancer is a prevalent gynecological malignancy that requires accurate identification of its molecular subtypes for effective diagnosis and treatment. Four molecular subtypes with different clinical outcomes have been identified: POLE \u2026"}, {"title": "AIDE: Agentically Improve Visual Language Model with Domain Experts", "link": "https://arxiv.org/pdf/2502.09051", "details": "MC Chiu, F Liu, K Sapra, A Tao, Y Jacoob, X Ma, Z Yu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The enhancement of Visual Language Models (VLMs) has traditionally relied on knowledge distillation from larger, more capable models. This dependence creates a fundamental bottleneck for improving state-of-the-art systems, particularly when no \u2026"}, {"title": "Mind the Gap: Evaluating Patch Embeddings from General-Purpose and Histopathology Foundation Models for Cell Segmentation and Classification", "link": "https://arxiv.org/pdf/2502.02471%3F", "details": "V Vadori, A Peruffo, JM Gra\u00c3\u014ac, L Finos, E Grisan - arXiv preprint arXiv:2502.02471, 2025", "abstract": "Recent advancements in foundation models have transformed computer vision, driving significant performance improvements across diverse domains, including digital histopathology. However, the advantages of domain-specific histopathology \u2026"}]
