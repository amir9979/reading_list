[{"title": "Surgical-VQLA++: Adversarial contrastive learning for calibrated robust visual question-localized answering in robotic surgery", "link": "https://www.sciencedirect.com/science/article/pii/S1566253524003804", "details": "L Bai, G Wang, M Islam, L Seenivasan, A Wang, H Ren - Information Fusion, 2024", "abstract": "Medical visual question answering (VQA) bridges the gap between visual information and clinical decision-making, enabling doctors to extract understanding from clinical images and videos. In particular, surgical VQA can enhance the \u2026"}, {"title": "Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models", "link": "https://arxiv.org/pdf/2407.11717", "details": "C Ju, H Wang, H Cheng, X Chen, Z Zhai, W Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-Language Large Models (VLMs) recently become primary backbone of AI, due to the impressive performance. However, their expensive computation costs, ie, throughput and delay, impede potentials in the real-world scenarios. To achieve \u2026"}, {"title": "Cross-Class Domain Adaptive Semantic Segmentation with Visual Language Models", "link": "https://openreview.net/pdf%3Fid%3DTjFn6xktTm", "details": "W Ren, R Xia, M Zheng, Z Wu, Y Tang, N Sebe - ACM Multimedia 2024", "abstract": "This paper addresses the issue of cross-class domain adaptation (CCDA) in semantic segmentation, where the target domain contains both shared and novel classes that are either unlabeled or unseen in the source domain. This problem is \u2026"}]
