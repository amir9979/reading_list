[{"title": "Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality", "link": "https://arxiv.org/pdf/2411.11531", "details": "V Chekalina, A Razzigaev, E Goncharova, A Kuznetsov - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper we present an approach to reduce hallucinations in Large Language Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional modality. Our method involves transforming input text into a set of KG embeddings and using \u2026"}, {"title": "Dual attention model with reinforcement learning for classification of histology whole-slide images", "link": "https://www.sciencedirect.com/science/article/pii/S0895611124001435", "details": "M Raza, R Awan, RMS Bashir, T Qaiser, NM Rajpoot - Computerized Medical Imaging \u2026, 2024", "abstract": "Digital whole slide images (WSIs) are generally captured at microscopic resolution and encompass extensive spatial data (several billions of pixels per image). Directly feeding these images to deep learning models is computationally intractable due to \u2026"}, {"title": "HistoEncoder: a digital pathology foundation model for prostate cancer", "link": "https://arxiv.org/pdf/2411.11458", "details": "J Pohjonen, AO Batouche, A Rannikko, K Sandeman\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Foundation models are trained on massive amounts of data to distinguish complex patterns and can be adapted to a wide range of downstream tasks with minimal computational resources. Here, we develop a foundation model for prostate cancer \u2026"}, {"title": "Foundation models in healthcare require rethinking reliability", "link": "https://www.nature.com/articles/s42256-024-00924-5", "details": "T Grote, T Freiesleben, P Berens - Nature Machine Intelligence, 2024", "abstract": "A new class of AI models, called foundation models, has entered healthcare. Foundation models violate several basic principles of the standard machine learning paradigm for assessing reliability, making it necessary to rethink what guarantees \u2026"}, {"title": "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "link": "https://aclanthology.org/2024.findings-emnlp.61.pdf", "details": "X Wei, H Chen, H Yu, H Fei, Q Liu - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have achieved notable success in commonsense reasoning tasks, benefiting from their extensive world knowledge acquired through extensive pretraining. While approaches like Chain-of-Thought \u2026"}]
