[{"title": "A fine-grained self-adapting prompt learning approach for few-shot learning with pre-trained language models", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124006026", "details": "X Chen, T Liu, P Fournier-Viger, B Zhang, G Long\u2026 - Knowledge-Based Systems, 2024", "abstract": "Pre-trained language models have demonstrated remarkable performance in few- shot learning through the emergence of \u201cprompt-based learning\u201d methods, where the performance of these tasks highly rely on the quality of prompts. Existing prompt \u2026"}, {"title": "Wonder at Chemotimelines 2024: MedTimeline: An End-to-End NLP System for Timeline Extraction from Clinical Narratives", "link": "https://aclanthology.org/2024.clinicalnlp-1.48.pdf", "details": "L Wang, Q Lu, R Li, S Fu, H Liu - Proceedings of the 6th Clinical Natural Language \u2026, 2024", "abstract": "Extracting timeline information from clinical narratives is critical for cancer research and practice using electronic health records (EHRs). In this study, we apply MedTimeline, our end-to-end hybrid NLP system combining large language model \u2026"}, {"title": "Overview of the 2024 shared task on chemotherapy treatment timeline extraction", "link": "https://aclanthology.org/2024.clinicalnlp-1.53.pdf", "details": "J Yao, H Hochheiser, WJ Yoon, E Goldner, G Savova - Proceedings of the 6th \u2026, 2024", "abstract": "Abstract The 2024 Shared Task on Chemotherapy Treatment Timeline Extraction aims to advance the state of the art of clinical event timeline extraction from the Electronic Health Records (EHRs). Specifically, this edition focuses on \u2026"}, {"title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models", "link": "https://arxiv.org/pdf/2406.02224", "details": "T Fan, G Ma, Y Kang, H Gu, L Fan, Q Yang - arXiv preprint arXiv:2406.02224, 2024", "abstract": "Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small \u2026"}, {"title": "BERTastic at SemEval-2024 Task 4: State-of-the-Art Multilingual Propaganda Detection in Memes via Zero-Shot Learning with Vision-Language Models", "link": "https://aclanthology.org/2024.semeval-1.77.pdf", "details": "T Mahmoud, P Nakov - Proceedings of the 18th International Workshop on \u2026, 2024", "abstract": "Analyzing propagandistic memes in a multilingual, multimodal dataset is a challenging problem due to the inherent complexity of memes' multimodal content, which combines images, text, and often, nuanced context. In this paper, we use a \u2026"}, {"title": "Evaluating Copyright Takedown Methods for Language Models", "link": "https://arxiv.org/pdf/2406.18664", "details": "B Wei, W Shi, Y Huang, NA Smith, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) derive their capabilities from extensive training on diverse data, including potentially copyrighted material. These models can memorize and generate content similar to their training data, posing potential concerns. Therefore \u2026"}, {"title": "ViGLUE: A Vietnamese General Language Understanding Benchmark and Analysis of Vietnamese Language Models", "link": "https://aclanthology.org/2024.findings-naacl.261.pdf", "details": "MN Tran, PV Nguyen, L Nguyen, D Dien - Findings of the Association for \u2026, 2024", "abstract": "As the number of language models has increased, various benchmarks have been suggested to assess the proficiency of the models in natural language understanding. However, there is a lack of such a benchmark in Vietnamese due to \u2026"}, {"title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models", "link": "https://aclanthology.org/2024.findings-naacl.18.pdf", "details": "Z Su, Z Lin, B Baixue, H Chen, S Hu, W Zhou, G Ding\u2026 - Findings of the Association \u2026, 2024", "abstract": "Generative language models are usually pre-trained on large text corpus via predicting the next token (ie, sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language \u2026"}, {"title": "A Critical Look At Tokenwise Reward-Guided Text Generation", "link": "https://arxiv.org/pdf/2406.07780", "details": "A Rashid, R Wu, J Grosse, A Kristiadi, P Poupart - arXiv preprint arXiv:2406.07780, 2024", "abstract": "Large language models (LLMs) can significantly be improved by aligning to human preferences--the so-called reinforcement learning from human feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive for many users. Due to their \u2026"}]
