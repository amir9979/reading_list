'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [An Efficient Approach for Studying Cross-Lingual Trans'
[{"title": "Handling Class Imbalance and Overlap with a Hesitation-based Instance Selection Method", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124003800", "details": "M Moradi, J Hamidzadeh - Knowledge-Based Systems, 2024", "abstract": "Class imbalance is a common problem in machine learning, particularly in classification tasks. When the distribution of instances across known classes is biased or skewed, this issue leads to poor predictive performance. This is especially \u2026"}, {"title": "Rethinking transformers pre-training for multi-spectral satellite imagery", "link": "https://arxiv.org/html/2403.05419v1", "details": "M Noman, M Naseer, H Cholakkal, RM Anwar, S Khan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in unsupervised learning have demonstrated the ability of large vision models to achieve promising results on downstream tasks by pre-training on large amount of unlabelled data. Such pre-training techniques have also been \u2026"}, {"title": "Gecko: Versatile Text Embeddings Distilled from Large Language Models", "link": "https://arxiv.org/html/2403.20327v1", "details": "J Lee, Z Dai, X Ren, B Chen, D Cer, JR Cole, K Hui\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Gecko, a compact and versatile text embedding model. Gecko achieves strong retrieval performance by leveraging a key idea: distilling knowledge from large language models (LLMs) into a retriever. Our two-step distillation process \u2026"}, {"title": "Distilling Named Entity Recognition Models for Endangered Species from Large Language Models", "link": "https://arxiv.org/pdf/2403.15430", "details": "J Atuhurra, SC Dujohn, H Kamigaito, H Shindo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Natural language processing (NLP) practitioners are leveraging large language models (LLM) to create structured datasets from semi-structured and unstructured data sources such as patents, papers, and theses, without having domain-specific \u2026"}, {"title": "GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation", "link": "https://arxiv.org/html/2403.19754v1", "details": "M Gholami, M Akbari, C Hu, V Masrani, ZJ Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Knowledge distillation from LLMs is essential for the efficient deployment of language models. Prior works have proposed data generation using LLMs for preparing distilled models. We argue that generating data with LLMs is prone to \u2026"}, {"title": "Envisioning MedCLIP: A Deep Dive into Explainability for Medical Vision-Language Models", "link": "https://arxiv.org/html/2403.18996v1", "details": "AUR Hashmi, D Mahapatra, M Yaqub - arXiv preprint arXiv:2403.18996, 2024", "abstract": "Explaining Deep Learning models is becoming increasingly important in the face of daily emerging multimodal models, particularly in safety-critical domains like medical imaging. However, the lack of detailed investigations into the performance of \u2026"}, {"title": "Construction of a Japanese Financial Benchmark for Large Language Models", "link": "https://arxiv.org/pdf/2403.15062", "details": "M Hirano - arXiv preprint arXiv:2403.15062, 2024", "abstract": "With the recent development of large language models (LLMs), models that focus on certain domains and languages have been discussed for their necessity. There is also a growing need for benchmarks to evaluate the performance of current LLMs in \u2026"}, {"title": "Time Series Representation Learning with Supervised Contrastive Temporal Transformer", "link": "https://arxiv.org/html/2403.10787v1", "details": "Y Liu, S Wijewickrema, C Bester, S O'Leary, J Bailey - arXiv preprint arXiv \u2026, 2024", "abstract": "Finding effective representations for time series data is a useful but challenging task. Several works utilize self-supervised or unsupervised learning methods to address this. However, there still remains the open question of how to leverage available \u2026"}, {"title": "HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction", "link": "https://arxiv.org/html/2403.05396v1", "details": "Z Guo, J Ma, Y Xu, Y Wang, L Wang, H Chen - arXiv preprint arXiv:2403.05396, 2024", "abstract": "Histopathology serves as the gold standard in cancer diagnosis, with clinical reports being vital in interpreting and understanding this process, guiding cancer treatment and patient care. The automation of histopathology report generation with deep \u2026"}]
