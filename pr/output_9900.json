[{"title": "From Language Models to Medical Diagnoses: Assessing the Potential of GPT-4 and GPT-3.5-Turbo in Digital Health", "link": "https://www.mdpi.com/2673-2688/5/4/128", "details": "J Roos, TI Wilhelm, R Martin, R Kaczmarczyk - AI, 2024", "abstract": "Background: Large language models (LLMs) like GPT-3.5-Turbo and GPT-4 show potential to transform medical diagnostics through their linguistic and analytical capabilities. This study evaluates their diagnostic proficiency using English and \u2026"}, {"title": "Towards Explainable Computerized Adaptive Testing with Large Language Model", "link": "https://aclanthology.org/2024.findings-emnlp.149.pdf", "details": "C Cheng, GH Zhao, Z Huang, Y Zhuang, Z Pan, Q Liu\u2026 - Findings of the Association \u2026, 2024", "abstract": "As intelligent education evolves, it will provide students with multiple personalized learning services based on their individual abilities. Computerized adaptive testing (CAT) is designed to accurately measure a student's ability using the least questions \u2026"}, {"title": "PPLqa: An Unsupervised Information-Theoretic Quality Metric for Comparing Generative Large Language Models", "link": "https://arxiv.org/pdf/2411.15320", "details": "G Friedland, X Huang, Y Cui, V Kapoor, A Khetan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We propose PPLqa, an easy to compute, language independent, information- theoretic metric to measure the quality of responses of generative Large Language Models (LLMs) in an unsupervised way, without requiring ground truth annotations or \u2026"}, {"title": "BGTplanner: Maximizing Training Accuracy for Differentially Private Federated Recommenders via Strategic Privacy Budget Allocation", "link": "https://arxiv.org/pdf/2412.02934", "details": "X Zhang, Y Zhou, M Hu, D Wu, P Liao, M Guizani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To mitigate the rising concern about privacy leakage, the federated recommender (FR) paradigm emerges, in which decentralized clients co-train the recommendation model without exposing their raw user-item rating data. The differentially private \u2026"}, {"title": "MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation", "link": "https://arxiv.org/pdf/2411.17636", "details": "H Singh, RJ Das, M Han, P Nakov, I Laptev - arXiv preprint arXiv:2411.17636, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable planning abilities across various domains, including robotics manipulation and navigation. While recent efforts in robotics have leveraged LLMs both for high-level and low-level \u2026"}]
