[{"title": "Hyperdimensional Intelligent Sensing for Efficient Real-Time Audio Processing on Extreme Edge", "link": "https://arxiv.org/pdf/2502.10718", "details": "S Yun, R Masukawa, H Chen, SH Jeong, W Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The escalating challenges of managing vast sensor-generated data, particularly in audio applications, necessitate innovative solutions. Current systems face significant computational and storage demands, especially in real-time applications like \u2026"}, {"title": "SAIF: A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models", "link": "https://arxiv.org/pdf/2502.11356", "details": "Z He, H Zhao, Y Qiao, F Yang, A Payani, J Ma, M Du - arXiv preprint arXiv \u2026, 2025", "abstract": "The ability of large language models (LLMs) to follow instructions is crucial for their practical applications, yet the underlying mechanisms remain poorly understood. This paper presents a novel framework that leverages sparse autoencoders (SAE) to \u2026"}, {"title": "Cost-Efficient Domain-Adaptive Pretraining of Language Models for Optoelectronics Applications", "link": "https://pubs.acs.org/doi/full/10.1021/acs.jcim.4c02029", "details": "D Huang, JM Cole - Journal of Chemical Information and Modeling, 2025", "abstract": "Pretrained language models have demonstrated strong capability and versatility in natural language processing (NLP) tasks, and they have important applications in optoelectronics research, such as data mining and topic modeling. Many language \u2026"}, {"title": "Evaluating the effectiveness of XAI techniques for encoder-based language models", "link": "https://arxiv.org/pdf/2501.15374", "details": "MA Mersha, MG Yigezu, J Kalita - Knowledge-Based Systems, 2025", "abstract": "The black-box nature of large language models (LLMs) necessitates the development of eXplainable AI (XAI) techniques for transparency and trustworthiness. However, evaluating these techniques remains a challenge. This \u2026"}, {"title": "Parameters vs flops: Scaling laws for optimal sparsity for mixture-of-experts language models", "link": "https://arxiv.org/pdf/2501.12370", "details": "S Abnar, H Shah, D Busbridge, AME Ali, J Susskind\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Scaling the capacity of language models has consistently proven to be a reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the \u2026"}, {"title": "VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic Health Records", "link": "https://arxiv.org/pdf/2501.16672%3F", "details": "P Chung, A Swaminathan, AJ Goodell, Y Kim\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Methods to ensure factual accuracy of text generated by large language models (LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence system that combines retrieval-augmented generation and LLM-as-a-Judge to verify whether \u2026"}, {"title": "CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language Model Question Answering", "link": "https://arxiv.org/pdf/2501.18457%3F", "details": "Y Wang, Z Fan, Q Wang, M Fung, H Ji - arXiv preprint arXiv:2501.18457, 2025", "abstract": "Large Language Models (LLMs) are pretrained on extensive multilingual corpora to acquire both language-specific cultural knowledge and general knowledge. Ideally, while LLMs should provide consistent responses to culture-independent questions \u2026"}, {"title": "MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding", "link": "https://arxiv.org/pdf/2501.18362%3F", "details": "Y Zuo, S Qu, Y Li, Z Chen, X Zhu, E Hua, K Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce MedXpertQA, a highly challenging and comprehensive benchmark to evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA includes 4,460 questions spanning 17 specialties and 11 body systems. It includes \u2026"}, {"title": "ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language Interactions for Computer-Aided Diagnosis", "link": "https://arxiv.org/pdf/2502.10620", "details": "X Li, X Hou, Z Huang, Y Gan - arXiv preprint arXiv:2502.10620, 2025", "abstract": "Recent advancements in large language models (LLMs) have demonstrated extraordinary comprehension capabilities with remarkable breakthroughs on various vision-language tasks. However, the application of LLMs in generating reliable \u2026"}]
