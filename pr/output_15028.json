[{"title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models", "link": "https://arxiv.org/pdf/2504.02821", "details": "M Pach, S Karthik, Q Bouniot, S Belongie, Z Akata - arXiv preprint arXiv:2504.02821, 2025", "abstract": "Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce \u2026"}, {"title": "Selecting and Pruning: A Differentiable Causal Sequentialized State-Space Model for Two-View Correspondence Learning", "link": "https://arxiv.org/pdf/2503.17938", "details": "X Fang, S Zhang, H Zhang, T Lu, H Zhou, J Ma - arXiv preprint arXiv:2503.17938, 2025", "abstract": "Two-view correspondence learning aims to discern true and false correspondences between image pairs by recognizing their underlying different information. Previous methods either treat the information equally or require the explicit storage of the \u2026"}]
