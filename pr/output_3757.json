[{"title": "Composable Interventions for Language Models", "link": "https://arxiv.org/pdf/2407.06483", "details": "A Kolbeinsson, K O'Brien, T Huang, S Gao, S Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Test-time interventions for language models can enhance factual accuracy, mitigate harmful outputs, and improve model efficiency without costly retraining. But despite a flood of new methods, different types of interventions are largely developing \u2026"}, {"title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models", "link": "https://arxiv.org/pdf/2407.06460", "details": "W Shi, J Lee, Y Huang, S Malladi, J Zhao, A Holtzman\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly \u2026"}, {"title": "BeHonest: Benchmarking Honesty of Large Language Models", "link": "https://arxiv.org/pdf/2406.13261", "details": "S Chern, Z Hu, Y Yang, E Chern, Y Guo, J Jin, B Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Previous works on Large Language Models (LLMs) have mainly focused on evaluating their helpfulness or harmlessness. However, honesty, another crucial alignment criterion, has received relatively less attention. Dishonest behaviors in \u2026"}, {"title": "Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence", "link": "https://arxiv.org/pdf/2407.07061", "details": "W Chen, Z You, R Li, Y Guan, C Qian, C Zhao, C Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to \u2026"}, {"title": "Collaborative Performance Prediction for Large Language Models", "link": "https://arxiv.org/pdf/2407.01300", "details": "Q Zhang, F Lyu, X Liu, C Ma - arXiv preprint arXiv:2407.01300, 2024", "abstract": "Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works \u2026"}, {"title": "A Single Transformer for Scalable Vision-Language Modeling", "link": "https://arxiv.org/pdf/2407.06438", "details": "Y Chen, X Wang, H Peng, H Ji - arXiv preprint arXiv:2407.06438, 2024", "abstract": "We present SOLO, a single transformer for Scalable visiOn-Language mOdeling. Current large vision-language models (LVLMs) such as LLaVA mostly employ heterogeneous architectures that connect pre-trained visual encoders with large \u2026"}, {"title": "Universal Approximation Theory: The basic theory for large language models", "link": "https://arxiv.org/pdf/2407.00958", "details": "W Wang, Q Li - arXiv preprint arXiv:2407.00958, 2024", "abstract": "Language models have emerged as a critical area of focus in artificial intelligence, particularly with the introduction of groundbreaking innovations like ChatGPT. Large- scale Transformer networks have quickly become the leading approach for \u2026"}, {"title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs", "link": "https://arxiv.org/pdf/2407.00653", "details": "Y Zhang, X Wang, J Liang, S Xia, L Chen, Y Xiao - arXiv preprint arXiv:2407.00653, 2024", "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new \u2026"}, {"title": "Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement", "link": "https://arxiv.org/pdf/2407.01461", "details": "Z Huang, X Wang, F Zhang, Z Xu, C Zhang, X Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The capacity of large language models (LLMs) to generate honest, harmless, and helpful responses heavily relies on the quality of user prompts. However, these prompts often tend to be brief and vague, thereby significantly limiting the full \u2026"}]
