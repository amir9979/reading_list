[{"title": "Multidimensional Consistency Improves Reasoning in Language Models", "link": "https://arxiv.org/pdf/2503.02670", "details": "H Lai, X Zhang, M Nissim - arXiv preprint arXiv:2503.02670, 2025", "abstract": "While Large language models (LLMs) have proved able to address some complex reasoning tasks, we also know that they are highly sensitive to input variation, which can lead to different solution paths and final answers. Answer consistency across \u2026"}, {"title": "SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label Recognition in Vision-Language Models", "link": "https://arxiv.org/pdf/2502.16911", "details": "K Miller, S Mishra, A Gangrade, K Saenko, V Saligrama - arXiv preprint arXiv \u2026, 2025", "abstract": "Zero-shot multi-label recognition (MLR) with Vision-Language Models (VLMs) faces significant challenges without training data, model tuning, or architectural modifications. Existing approaches require prompt tuning or architectural \u2026"}, {"title": "One ruler to measure them all: Benchmarking multilingual long-context language models", "link": "https://arxiv.org/pdf/2503.01996", "details": "Y Kim, J Russell, M Karpinska, M Iyyer - arXiv preprint arXiv:2503.01996, 2025", "abstract": "We present ONERULER, a multilingual benchmark designed to evaluate long- context language models across 26 languages. ONERULER adapts the English-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic tasks that test \u2026"}, {"title": "Enhancing Multi-hop Reasoning in Vision-Language Models via Self-Distillation with Multi-Prompt Ensembling", "link": "https://arxiv.org/pdf/2503.01754", "details": "G Wu, H Song, Y Wang, Q Yan, Y Tian, LL Cheong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multi-modal large language models have seen rapid advancement alongside large language models. However, while language models can effectively leverage chain- of-thought prompting for zero or few-shot learning, similar prompting strategies are \u2026"}, {"title": "In-context Learning vs. Instruction Tuning: The Case of Small and Multilingual Language Models", "link": "https://arxiv.org/pdf/2503.01611", "details": "D Ponce, T Etchegoyhen - arXiv preprint arXiv:2503.01611, 2025", "abstract": "Instruction following is a critical ability for Large Language Models to perform downstream tasks. The standard approach to instruction alignment has relied on a specific phase of model tuning over curated instruction datasets, optionally \u2026"}, {"title": "Leveraging Self-Supervised Learning Methods for Remote Screening of Subjects with Paroxysmal Atrial Fibrillation", "link": "https://arxiv.org/pdf/2503.02621", "details": "A Atienza, G Manimaran, S Puthusserypady\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The integration of Artificial Intelligence (AI) into clinical research has great potential to reveal patterns that are difficult for humans to detect, creating impactful connections between inputs and clinical outcomes. However, these methods often \u2026"}, {"title": "OFF-CLIP: Improving Normal Detection Confidence in Radiology CLIP with Simple Off-Diagonal Term Auto-Adjustment", "link": "https://arxiv.org/pdf/2503.01794", "details": "J Park, C Moon, D Lee, K Kim, M Hwang - arXiv preprint arXiv:2503.01794, 2025", "abstract": "Contrastive Language-Image Pre-Training (CLIP) has enabled zero-shot classification in radiology, reducing reliance on manual annotations. However, conventional contrastive learning struggles with normal case detection due to its \u2026"}, {"title": "Argument Summarization and its Evaluation in the Era of Large Language Models", "link": "https://arxiv.org/pdf/2503.00847", "details": "M Altemeyer, S Eger, J Daxenberger, T Altendorf\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have revolutionized various Natural Language Generation (NLG) tasks, including Argument Summarization (ArgSum), a key subfield of Argument Mining (AM). This paper investigates the integration of state-of \u2026"}, {"title": "Rehearse With User: Personalized Opinion Summarization via Role-Playing based on Large Language Models", "link": "https://arxiv.org/pdf/2503.00449", "details": "Y Zhang, Y He, D Zhou - arXiv preprint arXiv:2503.00449, 2025", "abstract": "Personalized opinion summarization is crucial as it considers individual user interests while generating product summaries. Recent studies show that although large language models demonstrate powerful text summarization and evaluation \u2026"}]
