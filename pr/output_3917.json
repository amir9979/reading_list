[{"title": "Evaluating Deep Neural Networks in Deployment (A Comparative and Replicability Study)", "link": "https://arxiv.org/pdf/2407.08730", "details": "E Pinconschi, D Gopinath, R Abreu, CS Pasareanu - arXiv preprint arXiv:2407.08730, 2024", "abstract": "As deep neural networks (DNNs) are increasingly used in safety-critical applications, there is a growing concern for their reliability. Even highly trained, high-performant networks are not 100% accurate. However, it is very difficult to predict their behavior \u2026"}, {"title": "mDPO: Conditional Preference Optimization for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2406.11839", "details": "F Wang, W Zhou, JY Huang, N Xu, S Zhang, H Poon\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Direct preference optimization (DPO) has shown to be an effective method for large language model (LLM) alignment. Recent works have attempted to apply DPO to multimodal scenarios but have found it challenging to achieve consistent \u2026"}, {"title": "From text to treatment: the crucial role of validation for generative large language models in health care", "link": "https://www.thelancet.com/journals/landig/article/PIIS2589-7500\\(24\\)00111-0/fulltext", "details": "A de Hond, T Leeuwenberg, R Bartels, M van Buchem\u2026 - The Lancet Digital Health, 2024", "abstract": "Generative large language models (LLMs) have made incredible progress and are speculated to become the next big revolution in health care. Researchers have described several compelling uses for LLMs in health care, including the automatic \u2026"}, {"title": "Information Guided Regularization for Fine-tuning Language Models", "link": "https://arxiv.org/pdf/2406.14005", "details": "M Sharma, N Muralidhar, S Xu, RB Yosuf\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The pretraining-fine-tuning paradigm has been the de facto strategy for transfer learning in modern language modeling. With the understanding that task adaptation in LMs is often a function of parameters shared across tasks, we argue that a more \u2026"}]
