[{"title": "A Systematic Analysis on the Temporal Generalization of Language Models in Social Media", "link": "https://arxiv.org/pdf/2405.13017", "details": "A Ushio, J Camacho-Collados - arXiv preprint arXiv:2405.13017, 2024", "abstract": "In machine learning, temporal shifts occur when there are differences between training and test splits in terms of time. For streaming data such as news or social media, models are commonly trained on a fixed corpus from a certain period of time \u2026"}, {"title": "MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning", "link": "https://arxiv.org/pdf/2405.05189", "details": "I Nair, L Wang - arXiv preprint arXiv:2405.05189, 2024", "abstract": "We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs). Previous approaches have explored various prompting schemes, yet they suffer from error \u2026"}, {"title": "Zero-shot LLM-guided Counterfactual Generation for Text", "link": "https://arxiv.org/pdf/2405.04793", "details": "A Bhattacharjee, R Moraffah, J Garland, H Liu - arXiv preprint arXiv:2405.04793, 2024", "abstract": "Counterfactual examples are frequently used for model development and evaluation in many natural language processing (NLP) tasks. Although methods for automated counterfactual generation have been explored, such methods depend on models \u2026"}, {"title": "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "link": "https://arxiv.org/pdf/2405.07551", "details": "S Yin, W You, Z Ji, G Zhong, J Bai - arXiv preprint arXiv:2405.07551, 2024", "abstract": "The tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs, while tool-free methods chose another track: augmenting math \u2026"}, {"title": "Backdoor Removal for Generative Large Language Models", "link": "https://arxiv.org/pdf/2405.07667", "details": "H Li, Y Chen, Z Zheng, Q Hu, C Chan, H Liu, Y Song - arXiv preprint arXiv \u2026, 2024", "abstract": "With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased \u2026"}, {"title": "SuperST: Superficial Self-Training for Few-Shot Text Classification", "link": "https://aclanthology.org/2024.lrec-main.1341.pdf", "details": "JH Lee, J Hahn, HT Seo, J Park, YS Han - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "In few-shot text classification, self-training is a popular tool in semi-supervised learning (SSL). It relies on pseudo-labels to expand data, which has demonstrated success. However, these pseudo-labels contain potential noise and provoke a risk of \u2026"}, {"title": "Towards Comprehensive and Efficient Post Safety Alignment of Large Language Models via Safety Patching", "link": "https://arxiv.org/pdf/2405.13820", "details": "W Zhao, Y Hu, Z Li, Y Deng, Y Zhao, B Qin, TS Chua - arXiv preprint arXiv \u2026, 2024", "abstract": "Safety alignment of large language models (LLMs) has been gaining increasing attention. However, current safety-aligned LLMs suffer from the fragile and imbalanced safety mechanisms, which can still be induced to generate unsafe \u2026"}, {"title": "A Minimalist Prompt for Zero-Shot Policy Learning", "link": "https://arxiv.org/pdf/2405.06063", "details": "M Song, X Wang, T Biradar, Y Qin, M Chandraker - arXiv preprint arXiv:2405.06063, 2024", "abstract": "Transformer-based methods have exhibited significant generalization ability when prompted with target-domain demonstrations or example solutions during inference. Although demonstrations, as a way of task specification, can capture rich information \u2026"}, {"title": "QCRD: Quality-guided Contrastive Rationale Distillation for Large Language Models", "link": "https://arxiv.org/pdf/2405.13014", "details": "W Wang, Z Li, Q Xu, Y Cai, H Song, Q Qi, R Zhou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Deploying large language models (LLMs) poses challenges in terms of resource limitations and inference efficiency. To address these challenges, recent research has focused on using smaller task-specific language models, which are enhanced by \u2026"}]
