[{"title": "Are Vision Language Models Ready for Clinical Diagnosis? A 3D Medical Benchmark for Tumor-centric Visual Question Answering", "link": "https://arxiv.org/pdf/2505.18915", "details": "Y Chen, W Xiao, PRAS Bassi, X Zhou, S Er\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 To systematically evaluate these dimensions, we present DeepTumorVQA, a diagnostic visual **question** **answering** (VQA) benchmark targeting abdominal tumors in CT scans. It comprises 9,262 CT volumes (3.7M slices) from 17 public datasets \u2026", "entry_id": "http://arxiv.org/abs/2505.18915v1", "updated": "2025-05-25 00:50:15", "published": "2025-05-25 00:50:15", "authors": "Yixiong Chen;Wenjie Xiao;Pedro R. A. S. Bassi;Xinze Zhou;Sezgin Er;Ibrahim Ethem Hamamci;Zongwei Zhou;Alan Yuille", "summary": "Vision-Language Models (VLMs) have shown promise in various 2D visual tasks,\nyet their readiness for 3D clinical diagnosis remains unclear due to stringent\ndemands for recognition precision, reasoning ability, and domain knowledge. To\nsystematically evaluate these dimensions, we present DeepTumorVQA, a diagnostic\nvisual question answering (VQA) benchmark targeting abdominal tumors in CT\nscans. It comprises 9,262 CT volumes (3.7M slices) from 17 public datasets,\nwith 395K expert-level questions spanning four categories: Recognition,\nMeasurement, Visual Reasoning, and Medical Reasoning. DeepTumorVQA introduces\nunique challenges, including small tumor detection and clinical reasoning\nacross 3D anatomy. Benchmarking four advanced VLMs (RadFM, M3D, Merlin,\nCT-CHAT), we find current models perform adequately on measurement tasks but\nstruggle with lesion recognition and reasoning, and are still not meeting\nclinical needs. Two key insights emerge: (1) large-scale multimodal pretraining\nplays a crucial role in DeepTumorVQA testing performance, making RadFM stand\nout among all VLMs. (2) Our dataset exposes critical differences in VLM\ncomponents, where proper image preprocessing and design of vision modules\nsignificantly affect 3D perception. To facilitate medical multimodal research,\nwe have released DeepTumorVQA as a rigorous benchmark:\nhttps://github.com/Schuture/DeepTumorVQA.", "comment": "NeurIPS 2025 datasets&benchmarks track submission", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.18915v1;http://arxiv.org/pdf/2505.18915v1", "pdf_url": "http://arxiv.org/pdf/2505.18915v1"}, {"title": "Toward Human Centered Interactive Clinical Question Answering System", "link": "https://arxiv.org/pdf/2505.18928", "details": "D Albassam - arXiv preprint arXiv:2505.18928, 2025", "abstract": "\u2026 Abstract\u2014Unstructured **clinical** notes contain essential patient information but are challenging for physicians to \u2026 **large** **language** **models** (LLMs) have shown promise in **question** **answering** (QA), most existing systems lack transparency, usability, and \u2026", "entry_id": "http://arxiv.org/abs/2505.18928v1", "updated": "2025-05-25 01:31:31", "published": "2025-05-25 01:31:31", "authors": "Dina Albassam", "summary": "Unstructured clinical notes contain essential patient information but are\nchallenging for physicians to search and interpret efficiently. Although large\nlanguage models (LLMs) have shown promise in question answering (QA), most\nexisting systems lack transparency, usability, and alignment with clinical\nworkflows. This work introduces an interactive QA system that enables\nphysicians to query clinical notes via text or voice and receive extractive\nanswers highlighted directly in the note for traceability.\n  The system was built using OpenAI models with zero-shot prompting and\nevaluated across multiple metrics, including exact string match, word overlap,\nSentenceTransformer similarity, and BERTScore. Results show that while exact\nmatch scores ranged from 47 to 62 percent, semantic similarity scores exceeded\n87 percent, indicating strong contextual alignment even when wording varied.\n  To assess usability, the system was also evaluated using simulated clinical\npersonas. Seven diverse physician and nurse personas interacted with the system\nacross scenario-based tasks and provided structured feedback. The evaluations\nhighlighted strengths in intuitive design and answer accessibility, alongside\nopportunities for enhancing explanation clarity.", "comment": null, "journal_ref": null, "primary_category": "cs.HC", "categories": "cs.HC", "links": "http://arxiv.org/abs/2505.18928v1;http://arxiv.org/pdf/2505.18928v1", "pdf_url": "http://arxiv.org/pdf/2505.18928v1"}, {"title": "Medical Large Vision Language Models with Multi-Image Visual Ability", "link": "https://arxiv.org/pdf/2505.19031", "details": "X Yang, J Miao, Y Yuan, J Wang, Q Dou, J Li, PA Heng - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 After collecting the multi-image sets, we integrate the textual data for each image and generate the corresponding **question** **answer** pairs. Unlike \u2026 For this, we prompt language-only GPT-4o to generate multi-image QA pairs by leveraging the **medical** \u2026", "entry_id": "http://arxiv.org/abs/2505.19031v1", "updated": "2025-05-25 08:31:22", "published": "2025-05-25 08:31:22", "authors": "Xikai Yang;Juzheng Miao;Yuchen Yuan;Jiaze Wang;Qi Dou;Jinpeng Li;Pheng-Ann Heng", "summary": "Medical large vision-language models (LVLMs) have demonstrated promising\nperformance across various single-image question answering (QA) benchmarks, yet\ntheir capability in processing multi-image clinical scenarios remains\nunderexplored. Unlike single image based tasks, medical tasks involving\nmultiple images often demand sophisticated visual understanding capabilities,\nsuch as temporal reasoning and cross-modal analysis, which are poorly supported\nby current medical LVLMs. To bridge this critical gap, we present the Med-MIM\ninstruction dataset, comprising 83.2K medical multi-image QA pairs that span\nfour types of multi-image visual abilities (temporal understanding, reasoning,\ncomparison, co-reference). Using this dataset, we fine-tune Mantis and\nLLaVA-Med, resulting in two specialized medical VLMs: MIM-LLaVA-Med and\nMed-Mantis, both optimized for multi-image analysis. Additionally, we develop\nthe Med-MIM benchmark to comprehensively evaluate the medical multi-image\nunderstanding capabilities of LVLMs. We assess eight popular LVLMs, including\nour two models, on the Med-MIM benchmark. Experimental results show that both\nMed-Mantis and MIM-LLaVA-Med achieve superior performance on the held-in and\nheld-out subsets of the Med-MIM benchmark, demonstrating that the Med-MIM\ninstruction dataset effectively enhances LVLMs' multi-image understanding\ncapabilities in the medical domain.", "comment": "10 pages, 4 figures", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.19031v1;http://arxiv.org/pdf/2505.19031v1", "pdf_url": "http://arxiv.org/pdf/2505.19031v1"}, {"title": "CSE 7850: Final Project (Group 4) MedBench, a Benchmark Dataset for **Medical Large Language Models**", "link": "https://adityaamk.com/projects/MCB_Final_Project.pdf", "details": "S Bonam, AM Kumar, A Venkatesan, M Rastogi", "abstract": "\u2026 Given that we curated datasets for **question** **answering** and natural language, the final dataset we identified was for a multi-classification task. We found a popular dataset on GitHub for disease prognosis created by Anuj Dutt [2], which we\u2019ll refer to \u2026"}, {"title": "Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts", "link": "https://arxiv.org/pdf/2505.21324", "details": "Y Zhu, Y Guo, N Marchuck, A Sarker, Y Wang - arXiv preprint arXiv:2505.21324, 2025", "abstract": "\u2026 in **large** **language** **models** (LLMs), their integration with traditional supervised machine learning (ML) techniques that have proven applicability to **medical** \u2026 For example, ensemble approaches with multiple LLMs have been used to improve \u2026", "entry_id": "http://arxiv.org/abs/2505.21324v1", "updated": "2025-05-27 15:22:01", "published": "2025-05-27 15:22:01", "authors": "Yuxin Zhu;Yuting Guo;Noah Marchuck;Abeed Sarker;Yun Wang", "summary": "Despite rapid advances in large language models (LLMs), their integration\nwith traditional supervised machine learning (ML) techniques that have proven\napplicability to medical data remains underexplored. This is particularly true\nfor psychiatric applications, where narrative data often exhibit nuanced\nlinguistic and contextual complexity, and can benefit from the combination of\nmultiple models with differing characteristics. In this study, we introduce an\nensemble framework for automatically classifying\nAttention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using\nnarrative transcripts. Our approach integrates three complementary models:\nLLaMA3, an open-source LLM that captures long-range semantic structure;\nRoBERTa, a pre-trained transformer model fine-tuned on labeled clinical\nnarratives; and a Support Vector Machine (SVM) classifier trained using\nTF-IDF-based lexical features. These models are aggregated through a majority\nvoting mechanism to enhance predictive robustness. The dataset includes 441\ninstances, including 352 for training and 89 for validation. Empirical results\nshow that the ensemble outperforms individual models, achieving an F$_1$ score\nof 0.71 (95\\% CI: [0.60-0.80]). Compared to the best-performing individual\nmodel (SVM), the ensemble improved recall while maintaining competitive\nprecision. This indicates the strong sensitivity of the ensemble in identifying\nADHD-related linguistic cues. These findings demonstrate the promise of hybrid\narchitectures that leverage the semantic richness of LLMs alongside the\ninterpretability and pattern recognition capabilities of traditional supervised\nML, offering a new direction for robust and generalizable psychiatric text\nclassification.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.21324v1;http://arxiv.org/pdf/2505.21324v1", "pdf_url": "http://arxiv.org/pdf/2505.21324v1"}, {"title": "Making Sense of the Unsensible: Reflection, Survey, and Challenges for XAI in Large Language Models Toward Human-Centered AI", "link": "https://arxiv.org/pdf/2505.20305", "details": "F Herrera - arXiv preprint arXiv:2505.20305, 2025", "abstract": "\u2026 and **question** - **answering**. The EQT method decouples explanation generation from subsequent **question** **answering** by prompting a model to (i) explain a concept, (ii) generate multiple-choice **questions** from its own explanation, and (iii) **answer** those \u2026", "entry_id": "http://arxiv.org/abs/2505.20305v1", "updated": "2025-05-18 17:30:10", "published": "2025-05-18 17:30:10", "authors": "Francisco Herrera", "summary": "As large language models (LLMs) are increasingly deployed in sensitive\ndomains such as healthcare, law, and education, the demand for transparent,\ninterpretable, and accountable AI systems becomes more urgent. Explainable AI\n(XAI) acts as a crucial interface between the opaque reasoning of LLMs and the\ndiverse stakeholders who rely on their outputs in high-risk decisions. This\npaper presents a comprehensive reflection and survey of XAI for LLMs, framed\naround three guiding questions: Why is explainability essential? What technical\nand ethical dimensions does it entail? And how can it fulfill its role in\nreal-world deployment?\n  We highlight four core dimensions central to explainability in LLMs,\nfaithfulness, truthfulness, plausibility, and contrastivity, which together\nexpose key design tensions and guide the development of explanation strategies\nthat are both technically sound and contextually appropriate. The paper\ndiscusses how XAI can support epistemic clarity, regulatory compliance, and\naudience-specific intelligibility across stakeholder roles and decision\nsettings.\n  We further examine how explainability is evaluated, alongside emerging\ndevelopments in audience-sensitive XAI, mechanistic interpretability, causal\nreasoning, and adaptive explanation systems. Emphasizing the shift from\nsurface-level transparency to governance-ready design, we identify critical\nchallenges and future research directions for ensuring the responsible use of\nLLMs in complex societal contexts. We argue that explainability must evolve\ninto a civic infrastructure fostering trust, enabling contestability, and\naligning AI systems with institutional accountability and human-centered\ndecision-making.", "comment": "30 pages, 1 figure", "journal_ref": null, "primary_category": "cs.CY", "categories": "cs.CY", "links": "http://arxiv.org/abs/2505.20305v1;http://arxiv.org/pdf/2505.20305v1", "pdf_url": "http://arxiv.org/pdf/2505.20305v1"}, {"title": "Test-Time Learning for Large Language Models", "link": "https://arxiv.org/pdf/2505.20633", "details": "J Hu, Z Zhang, G Chen, X Wen, C Shuai, W Luo, B Xiao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 While **Large** **Language** **Models** (LLMs) have exhibited remarkable emergent capabilities \u2026 as named entity recognition, judgment, and **question** **answering**. By incorporating domain-specific \u2026 The datasets cover various task types, such as \u2026", "entry_id": "http://arxiv.org/abs/2505.20633v1", "updated": "2025-05-27 02:18:59", "published": "2025-05-27 02:18:59", "authors": "Jinwu Hu;Zhitian Zhang;Guohao Chen;Xutao Wen;Chao Shuai;Wei Luo;Bin Xiao;Yuanqing Li;Mingkui Tan", "summary": "While Large Language Models (LLMs) have exhibited remarkable emergent\ncapabilities through extensive pre-training, they still face critical\nlimitations in generalizing to specialized domains and handling diverse\nlinguistic variations, known as distribution shifts. In this paper, we propose\na Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically\nadapts LLMs to target domains using only unlabeled test data during testing.\nSpecifically, we first provide empirical evidence and theoretical insights to\nreveal that more accurate predictions from LLMs can be achieved by minimizing\nthe input perplexity of the unlabeled test data. Based on this insight, we\nformulate the Test-Time Learning process of LLMs as input perplexity\nminimization, enabling self-supervised enhancement of LLM performance.\nFurthermore, we observe that high-perplexity samples tend to be more\ninformative for model optimization. Accordingly, we introduce a Sample\nEfficient Learning Strategy that actively selects and emphasizes these\nhigh-perplexity samples for test-time updates. Lastly, to mitigate catastrophic\nforgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA)\ninstead of full-parameter optimization, which allows lightweight model updates\nwhile preserving more original knowledge from the model. We introduce the\nAdaptEval benchmark for TTL and demonstrate through experiments that TLM\nimproves performance by at least 20% compared to original LLMs on domain\nknowledge adaptation.", "comment": "Accepted by ICML2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.20633v1;http://arxiv.org/pdf/2505.20633v1", "pdf_url": "http://arxiv.org/pdf/2505.20633v1"}, {"title": "Benchmarking Large Multimodal Models for Ophthalmic Visual Question Answering with OphthalWeChat", "link": "https://arxiv.org/pdf/2505.19624", "details": "P Xu, X Gong, X Chen, W Zhang, J Yang, B Yan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Journal of **medical** Internet research. 2024;26:e59505. \u2026 **Large** **language** **models** and their impact in ophthalmology. The Lancet Digital Health. 2023;5(12):e917-e924. \u2026 Evaluating **large** **language** **models** and agents in healthcare: key challenges in \u2026", "entry_id": "http://arxiv.org/abs/2505.19624v1", "updated": "2025-05-26 07:45:42", "published": "2025-05-26 07:45:42", "authors": "Pusheng Xu;Xia Gong;Xiaolan Chen;Weiyi Zhang;Jiancheng Yang;Bingjie Yan;Meng Yuan;Yalin Zheng;Mingguang He;Danli Shi", "summary": "Purpose: To develop a bilingual multimodal visual question answering (VQA)\nbenchmark for evaluating VLMs in ophthalmology. Methods: Ophthalmic image posts\nand associated captions published between January 1, 2016, and December 31,\n2024, were collected from WeChat Official Accounts. Based on these captions,\nbilingual question-answer (QA) pairs in Chinese and English were generated\nusing GPT-4o-mini. QA pairs were categorized into six subsets by question type\nand language: binary (Binary_CN, Binary_EN), single-choice (Single-choice_CN,\nSingle-choice_EN), and open-ended (Open-ended_CN, Open-ended_EN). The benchmark\nwas used to evaluate the performance of three VLMs: GPT-4o, Gemini 2.0 Flash,\nand Qwen2.5-VL-72B-Instruct. Results: The final OphthalWeChat dataset included\n3,469 images and 30,120 QA pairs across 9 ophthalmic subspecialties, 548\nconditions, 29 imaging modalities, and 68 modality combinations. Gemini 2.0\nFlash achieved the highest overall accuracy (0.548), outperforming GPT-4o\n(0.522, P < 0.001) and Qwen2.5-VL-72B-Instruct (0.514, P < 0.001). It also led\nin both Chinese (0.546) and English subsets (0.550). Subset-specific\nperformance showed Gemini 2.0 Flash excelled in Binary_CN (0.687),\nSingle-choice_CN (0.666), and Single-choice_EN (0.646), while GPT-4o ranked\nhighest in Binary_EN (0.717), Open-ended_CN (BLEU-1: 0.301; BERTScore: 0.382),\nand Open-ended_EN (BLEU-1: 0.183; BERTScore: 0.240). Conclusions: This study\npresents the first bilingual VQA benchmark for ophthalmology, distinguished by\nits real-world context and inclusion of multiple examinations per patient. The\ndataset reflects authentic clinical decision-making scenarios and enables\nquantitative evaluation of VLMs, supporting the development of accurate,\nspecialized, and trustworthy AI systems for eye care.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.19624v1;http://arxiv.org/pdf/2505.19624v1", "pdf_url": "http://arxiv.org/pdf/2505.19624v1"}, {"title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue", "link": "https://arxiv.org/pdf/2505.19630", "details": "Y Feng, J Wang, L Zhou, Y Li - arXiv preprint arXiv:2505.19630, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) have demonstrated excellent capabilities in the field of biomedical **question** **answering** , but their application in real-world **clinical** consultations still faces core challenges. Existing systems rely on a one-way information \u2026", "entry_id": "http://arxiv.org/abs/2505.19630v1", "updated": "2025-05-26 07:48:14", "published": "2025-05-26 07:48:14", "authors": "Yichun Feng;Jiawei Wang;Lu Zhou;Yixue Li", "summary": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Existing systems rely on a\none-way information transmission mode where patients must fully describe their\nsymptoms in a single round, leading to nonspecific diagnostic recommendations\nwhen complaints are vague. Traditional multi-turn dialogue methods based on\nsupervised learning are constrained by static data-driven paradigms, lacking\ngeneralizability and struggling to intelligently extract key clinical\ninformation. To address these limitations, we propose DoctorAgent-RL, a\nreinforcement learning (RL)-based multi-agent collaborative framework that\nmodels medical consultations as a dynamic decision-making process under\nuncertainty. The doctor agent continuously optimizes its questioning strategy\nwithin the RL framework through multi-turn interactions with the patient agent,\ndynamically adjusting its information-gathering path based on comprehensive\nrewards from the Consultation Evaluator. This RL fine-tuning mechanism enables\nLLMs to autonomously develop interaction strategies aligned with clinical\nreasoning logic, rather than superficially imitating patterns in existing\ndialogue data. Notably, we constructed MTMedDialog, the first English\nmulti-turn medical consultation dataset capable of simulating patient\ninteractions. Experiments demonstrate that DoctorAgent-RL outperforms existing\nmodels in both multi-turn reasoning capability and final diagnostic\nperformance, demonstrating practical value in assisting clinical consultations.\nhttps://github.com/JarvisUSTC/DoctorAgent-RL", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.19630v1;http://arxiv.org/pdf/2505.19630v1", "pdf_url": "http://arxiv.org/pdf/2505.19630v1"}]
