[{"title": "Small Language Models (SLMs) Can Still Pack a Punch: A survey", "link": "https://arxiv.org/pdf/2501.05465", "details": "S Subramanian, V Elango, M Gungor - arXiv preprint arXiv:2501.05465, 2025", "abstract": "As foundation AI models continue to increase in size, an important question arises-is massive scale the only path forward? This survey of about 160 papers presents a family of Small Language Models (SLMs) in the 1 to 8 billion parameter range that \u2026"}, {"title": "Challenges and recommendations for Electronic Health Records data extraction and preparation for dynamic prediction modelling in hospitalized patients--a practical \u2026", "link": "https://ui.adsabs.harvard.edu/abs/2025arXiv250110240A/abstract", "details": "E Albu, S Gao, P Stijnen, FE Rademakers\u2026 - arXiv e-prints, 2025", "abstract": "Dynamic predictive modeling using electronic health record (EHR) data has gained significant attention in recent years. The reliability and trustworthiness of such models depend heavily on the quality of the underlying data, which is largely \u2026"}, {"title": "3VL: Using Trees to Improve Vision-Language Models' Interpretability", "link": "https://ieeexplore.ieee.org/abstract/document/10829542/", "details": "N Yellinek, L Karlinsky, R Giryes - IEEE Transactions on Image Processing, 2025", "abstract": "Vision-Language models (VLMs) have proven to be effective at aligning image and text representations, producing superior zero-shot results when transferred to many downstream tasks. However, these representations suffer from some key \u2026"}, {"title": "Reasoning Language Models: A Blueprint", "link": "https://arxiv.org/pdf/2501.11223", "details": "M Besta, J Barth, E Schreiber, A Kubicek, A Catarino\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have redefined AI's problem-solving capabilities by extending large language models \u2026"}, {"title": "Eve: Efficient Multimodal Vision Language Models with Elastic Visual Experts", "link": "https://ui.adsabs.harvard.edu/abs/2025arXiv250104322R/abstract", "details": "M Rang, Z Bi, C Liu, Y Tang, K Han, Y Wang - arXiv e-prints, 2025", "abstract": "Multimodal vision language models (VLMs) have made significant progress with the support of continuously increasing model sizes and data volumes. Running VLMs on edge devices has become a challenge for their widespread application. There are \u2026"}, {"title": "Bactrainus: Optimizing Large Language Models for Multi-hop Complex Question Answering Tasks", "link": "https://arxiv.org/pdf/2501.06286", "details": "I Barati, A Ghafouri, B Minaei-Bidgoli - arXiv preprint arXiv:2501.06286, 2025", "abstract": "In recent years, the use of large language models (LLMs) has significantly increased, and these models have demonstrated remarkable performance in a variety of general language tasks. However, the evaluation of their performance in domain \u2026"}, {"title": "Awakening Augmented Generation: Learning to Awaken Internal Knowledge of Large Language Models for Question Answering", "link": "https://aclanthology.org/2025.coling-main.89.pdf", "details": "H Liao, S He, Y Xu, Y Zhang, S Liu, K Liu, J Zhao - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract Retrieval-Augmented-Generation and Generation-Augmented-Generation have been proposed to enhance the knowledge required for question answering with Large Language Models (LLMs) by leveraging richer context. However, the \u2026"}, {"title": "Double Visual Defense: Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness", "link": "https://arxiv.org/pdf/2501.09446", "details": "Z Wang, C Xie, B Bartoldson, B Kailkhura - arXiv preprint arXiv:2501.09446, 2025", "abstract": "This paper investigates the robustness of vision-language models against adversarial visual perturbations and introduces a novel``double visual defense\" to enhance this robustness. Unlike previous approaches that resort to lightweight \u2026"}, {"title": "Synthetic Data Generation Using Large Language Models for Financial Question Answering", "link": "https://aclanthology.org/2025.finnlp-1.7.pdf", "details": "C Harsha, KS Phogat, S Dasaratha, SA Puranam\u2026 - \u2026 of the Joint Workshop of the \u2026, 2025", "abstract": "Recent research has shown excellent performance of large language models (LLMs) for answering questions requiring multi-step financial reasoning. While the larger models have been used with zero-shot or few-shot prompting, the smaller variants \u2026"}]
