[{"title": "DomCLP: Domain-wise Contrastive Learning with Prototype Mixup for Unsupervised Domain Generalization", "link": "https://arxiv.org/pdf/2412.09074", "details": "JS Lee, N Kim, JH Lee - arXiv preprint arXiv:2412.09074, 2024", "abstract": "Self-supervised learning (SSL) methods based on the instance discrimination tasks with InfoNCE have achieved remarkable success. Despite their success, SSL models often struggle to generate effective representations for unseen-domain data. To \u2026"}, {"title": "UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for Diverse Medical Imaging Modalities", "link": "https://arxiv.org/pdf/2412.10372", "details": "MU Khattak, S Kunhimon, M Naseer, S Khan, FS Khan - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-Language Models (VLMs) trained via contrastive learning have achieved notable success in natural image tasks. However, their application in the medical domain remains limited due to the scarcity of openly accessible, large-scale medical \u2026"}, {"title": "LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation", "link": "https://arxiv.org/pdf/2412.15188", "details": "W Shi, X Han, C Zhou, W Liang, XV Lin, L Zettlemoyer\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present LlamaFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LlamaFusion \u2026"}, {"title": "Do language models understand time?", "link": "https://arxiv.org/pdf/2412.13845", "details": "X Ding, L Wang - arXiv preprint arXiv:2412.13845, 2024", "abstract": "Large language models (LLMs) have revolutionized video-based computer vision applications, including action recognition, anomaly detection, and video summarization. Videos inherently pose unique challenges, combining spatial \u2026"}, {"title": "GIRAFFE: Design Choices for Extending the Context Length of Visual Language Models", "link": "https://arxiv.org/pdf/2412.12735", "details": "M Li, L Li, S Gong, Q Liu - arXiv preprint arXiv:2412.12735, 2024", "abstract": "Visual Language Models (VLMs) demonstrate impressive capabilities in processing multimodal inputs, yet applications such as visual agents, which require handling multiple images and high-resolution videos, demand enhanced long-range \u2026"}, {"title": "CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology", "link": "https://arxiv.org/pdf/2412.12077", "details": "Y Sun, Y Si, C Zhu, X Gong, K Zhang, P Chen, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence of large multimodal models (LMMs) has brought significant advancements to pathology. Previous research has primarily focused on separately training patch-level and whole-slide image (WSI)-level models, limiting the \u2026"}, {"title": "Hybrid Vision Transformer and Convolutional Neural Network for Multi-Class and Multi-Label Classification of Tuberculosis Anomalies on Chest X-Ray", "link": "https://www.mdpi.com/2073-431X/13/12/343", "details": "R Yulvina, SA Putra, M Rizkinia, A Pujitresnani\u2026 - Computers, 2024", "abstract": "Tuberculosis (TB), caused by Mycobacterium tuberculosis, remains a leading cause of global mortality. While TB detection can be performed through chest X-ray (CXR) analysis, numerous studies have leveraged AI to automate and enhance the \u2026"}, {"title": "Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach", "link": "https://arxiv.org/pdf/2412.18108", "details": "J Bi, J Guo, Y Tang, LB Wen, Z Liu, C Xu - arXiv preprint arXiv:2412.18108, 2024", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated remarkable progress in visual understanding. This impressive leap raises a compelling question: how can language models, initially trained solely on \u2026"}, {"title": "Token Preference Optimization with Self-Calibrated Visual-Anchored Rewards for Hallucination Mitigation", "link": "https://arxiv.org/pdf/2412.14487", "details": "J Gu, Y Wang, M Cao, P Bu, J Song, Y He, S Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Direct Preference Optimization (DPO) has been demonstrated to be highly effective in mitigating hallucinations in Large Vision Language Models (LVLMs) by aligning their outputs more closely with human preferences. Despite the recent progress \u2026"}]
