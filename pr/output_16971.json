[{"title": "An Analytical Review of **Large Language Models** Leveraging KDGI Fine-Tuning, Quantum Embedding's, and Multimodal Architectures", "link": "https://www.sciencedirect.com/org/science/article/pii/S1546221825004850", "details": "U Sirisha, CK Kumar, R Durgam, P Eswaraiah\u2026 - Computers, Materials and \u2026, 2025", "abstract": "A complete examination of **Large** **Language** **Models** \u2019 strengths, problems, and applications is needed due to their rising use across disciplines. Current studies frequently focus on single-use situations and lack a comprehensive understanding \u2026"}, {"title": "Using **Large Language Models** to Enhance Exercise Recommendations and Physical Activity in Clinical and Healthy Populations: Scoping Review", "link": "https://medinform.jmir.org/2025/1/e59309/", "details": "X Lai, J Chen, Y Lai, S Huang, Y Cai, Z Sun, X Wang\u2026 - JMIR Medical Informatics, 2025", "abstract": "\u2026 used expert **evaluations** and user feedback to assess model usability, and 45% (5/11) of the studies used experimental designs to **evaluate** LLM \u2026 Notably, the linguistic understanding and generation abilities of **large** **language** **models** (LLMs), such as \u2026"}, {"title": "Large Language Models for IT Automation Tasks: Are We There Yet?", "link": "https://arxiv.org/pdf/2505.20505", "details": "MM Hassan, J Salvador, A Rahman, S Karmaker - arXiv preprint arXiv:2505.20505, 2025", "abstract": "\u2026 We **evaluated** a diverse set of fourteen **large** **language** **models** (LLMs), including both code-specialized models (Code LLMs) and generalpurpose instruction-tuned models (General LLMs). Our selection spans multiple model families, parameter \u2026", "entry_id": "http://arxiv.org/abs/2505.20505v1", "updated": "2025-05-26 20:15:00", "published": "2025-05-26 20:15:00", "authors": "Md Mahadi Hassan;John Salvador;Akond Rahman;Santu Karmaker", "summary": "LLMs show promise in code generation, yet their effectiveness for IT\nautomation tasks, particularly for tools like Ansible, remains understudied.\nExisting benchmarks rely primarily on synthetic tasks that fail to capture the\nneeds of practitioners who use IT automation tools, such as Ansible. We present\nITAB (IT Automation Task Benchmark), a benchmark of 126 diverse tasks (e.g.,\nconfiguring servers, managing files) where each task accounts for state\nreconciliation: a property unique to IT automation tools. ITAB evaluates LLMs'\nability to generate functional Ansible automation scripts via dynamic execution\nin controlled environments. We evaluate 14 open-source LLMs, none of which\naccomplish pass@10 at a rate beyond 12%. To explain these low scores, we\nanalyze 1,411 execution failures across the evaluated LLMs and identify two\nmain categories of prevalent semantic errors: failures in state reconciliation\nrelated reasoning (44.87% combined from variable (11.43%), host (11.84%),\npath(11.63%), and template (9.97%) issues) and deficiencies in module-specific\nexecution knowledge (24.37% combined from Attribute and parameter (14.44%) and\nmodule (9.93%) errors). Our findings reveal key limitations in open-source\nLLMs' ability to track state changes and apply specialized module knowledge,\nindicating that reliable IT automation will require major advances in state\nreasoning and domain-specific execution understanding.", "comment": "8 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.SE", "links": "http://arxiv.org/abs/2505.20505v1;http://arxiv.org/pdf/2505.20505v1", "pdf_url": "http://arxiv.org/pdf/2505.20505v1"}, {"title": "Performance **evaluation** of **large language models** for the national nursing examination in Japan", "link": "https://journals.sagepub.com/doi/full/10.1177/20552076251346571", "details": "T Kuribara, K Hirayama, K Hirata - DIGITAL HEALTH, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) are increasingly used in healthcare, with the potential for various applications. However, the performance of different LLMs on nursing license exams and their tendencies to make errors remain unclear. This \u2026"}, {"title": "STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models", "link": "https://arxiv.org/pdf/2505.20645", "details": "K Chen, Z He, T Shi, K Lerman - arXiv preprint arXiv:2505.20645, 2025", "abstract": "\u2026 focuses on systematically measuring the steering capabilities of **large** **language** **models**. \u2026 **Evaluating** **large** **language** **models** in generating synthetic hci research data: a case study. In \u2026 instruction following ability of **large** **language** **models**. arXiv \u2026", "entry_id": "http://arxiv.org/abs/2505.20645v1", "updated": "2025-05-27 02:47:56", "published": "2025-05-27 02:47:56", "authors": "Kai Chen;Zihao He;Taiwei Shi;Kristina Lerman", "summary": "Steerability, or the ability of large language models (LLMs) to adapt outputs\nto align with diverse community-specific norms, perspectives, and communication\nstyles, is critical for real-world applications but remains under-evaluated. We\nintroduce Steer-Bench, a benchmark for assessing population-specific steering\nusing contrasting Reddit communities. Covering 30 contrasting subreddit pairs\nacross 19 domains, Steer-Bench includes over 10,000 instruction-response pairs\nand validated 5,500 multiple-choice question with corresponding silver labels\nto test alignment with diverse community norms. Our evaluation of 13 popular\nLLMs using Steer-Bench reveals that while human experts achieve an accuracy of\n81% with silver labels, the best-performing models reach only around 65%\naccuracy depending on the domain and configuration. Some models lag behind\nhuman-level alignment by over 15 percentage points, highlighting significant\ngaps in community-sensitive steerability. Steer-Bench is a benchmark to\nsystematically assess how effectively LLMs understand community-specific\ninstructions, their resilience to adversarial steering attempts, and their\nability to accurately represent diverse cultural and ideological perspectives.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.20645v1;http://arxiv.org/pdf/2505.20645v1", "pdf_url": "http://arxiv.org/pdf/2505.20645v1"}, {"title": "What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2505.20405", "details": "L Baraldi, D Bucciarelli, F Betti, M Cornia, N Sebe\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Prior work has approached the **evaluation** of image editing models by developing annotated \u2026 Other methods leverage pre-trained **large** **language** **models** to assess the quality of image \u2026 the original image and then (ii) we **evaluate** the coherence of \u2026", "entry_id": "http://arxiv.org/abs/2505.20405v1", "updated": "2025-05-26 18:00:10", "published": "2025-05-26 18:00:10", "authors": "Lorenzo Baraldi;Davide Bucciarelli;Federico Betti;Marcella Cornia;Lorenzo Baraldi;Nicu Sebe;Rita Cucchiara", "summary": "Instruction-based image editing models offer increased personalization\nopportunities in generative tasks. However, properly evaluating their results\nis challenging, and most of the existing metrics lag in terms of alignment with\nhuman judgment and explainability. To tackle these issues, we introduce DICE\n(DIfference Coherence Estimator), a model designed to detect localized\ndifferences between the original and the edited image and to assess their\nrelevance to the given modification request. DICE consists of two key\ncomponents: a difference detector and a coherence estimator, both built on an\nautoregressive Multimodal Large Language Model (MLLM) and trained using a\nstrategy that leverages self-supervision, distillation from inpainting\nnetworks, and full supervision. Through extensive experiments, we evaluate each\nstage of our pipeline, comparing different MLLMs within the proposed framework.\nWe demonstrate that DICE effectively identifies coherent edits, effectively\nevaluating images generated by different editing models with a strong\ncorrelation with human judgment. We publicly release our source code, models,\nand data.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI;cs.CL;cs.MM", "links": "http://arxiv.org/abs/2505.20405v1;http://arxiv.org/pdf/2505.20405v1", "pdf_url": "http://arxiv.org/pdf/2505.20405v1"}, {"title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models", "link": "https://arxiv.org/pdf/2505.21334", "details": "K Shao, K Tao, C Qin, H You, Y Sui, H Wang - arXiv preprint arXiv:2505.21334, 2025", "abstract": "\u2026 2.1 Video **Large** **Language** **Models** The rapid progress of multimodal **large** **language** **models** has led to the integration of video encoders, \u2026 We **evaluate** the inference cost of transformer layers, each composed of multi-head attention (MHA) \u2026", "entry_id": "http://arxiv.org/abs/2505.21334v2", "updated": "2025-05-28 10:49:18", "published": "2025-05-27 15:28:45", "authors": "Kele Shao;Keda Tao;Can Qin;Haoxuan You;Yang Sui;Huan Wang", "summary": "Video large language models (video LLMs) excel at video comprehension but\nface significant computational inefficiency due to redundant video tokens.\nExisting token pruning methods offer solutions. However, approaches operating\nwithin the LLM (inner-LLM pruning), such as FastV, incur intrinsic\ncomputational overhead in shallow layers. In contrast, methods performing token\npruning before the LLM (outer-LLM pruning) primarily address spatial redundancy\nwithin individual frames or limited temporal windows, neglecting the crucial\nglobal temporal dynamics and correlations across longer video sequences. This\nleads to sub-optimal spatio-temporal reduction and does not leverage video\ncompressibility fully. Crucially, the synergistic potential and mutual\ninfluence of combining these strategies remain unexplored. To further reduce\nredundancy, we introduce HoliTom, a novel training-free holistic token merging\nframework. HoliTom employs outer-LLM pruning through global redundancy-aware\ntemporal segmentation, followed by spatial-temporal merging to reduce visual\ntokens by over 90%, significantly alleviating the LLM's computational burden.\nComplementing this, we introduce a robust inner-LLM token similarity-based\nmerging approach, designed for superior performance and compatibility with\nouter-LLM pruning. Evaluations demonstrate our method's promising\nefficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational\ncosts to 6.9% of FLOPs while maintaining 99.1% of the original performance.\nFurthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a\n1.32x acceleration in decoding throughput, highlighting the practical benefits\nof our integrated pruning approach for efficient video LLMs inference.", "comment": "version provides code link: https://github.com/cokeshao/HoliTom", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.21334v2;http://arxiv.org/pdf/2505.21334v2", "pdf_url": "http://arxiv.org/pdf/2505.21334v2"}, {"title": "Evaluating Large Language Models for Code Review", "link": "https://arxiv.org/pdf/2505.20206", "details": "U Cihan, A \u0130\u00e7\u00f6z, V Haratian, E T\u00fcz\u00fcn - arXiv preprint arXiv:2505.20206, 2025", "abstract": "\u2026 **large** **language** **models** (LLMs) **evaluate** code changes for approval or rejection? RQ2: How effective are the code improvement suggestions generated by **large** **language** **models** (\u2026 We **evaluate** the LLM\u2019s assessment against unit test results \u2026", "entry_id": "http://arxiv.org/abs/2505.20206v1", "updated": "2025-05-26 16:47:29", "published": "2025-05-26 16:47:29", "authors": "Umut Cihan;Arda \u0130\u00e7\u00f6z;Vahid Haratian;Eray T\u00fcz\u00fcn", "summary": "Context: Code reviews are crucial for software quality. Recent AI advances\nhave allowed large language models (LLMs) to review and fix code; now, there\nare tools that perform these reviews. However, their reliability and accuracy\nhave not yet been systematically evaluated. Objective: This study compares\ndifferent LLMs' performance in detecting code correctness and suggesting\nimprovements. Method: We tested GPT4o and Gemini 2.0 Flash on 492 AI generated\ncode blocks of varying correctness, along with 164 canonical code blocks from\nthe HumanEval benchmark. To simulate the code review task objectively, we\nexpected LLMs to assess code correctness and improve the code if needed. We ran\nexperiments with different configurations and reported on the results. Results:\nWith problem descriptions, GPT4o and Gemini 2.0 Flash correctly classified code\ncorrectness 68.50% and 63.89% of the time, respectively, and corrected the code\n67.83% and 54.26% of the time for the 492 code blocks of varying correctness.\nWithout problem descriptions, performance declined. The results for the 164\ncanonical code blocks differed, suggesting that performance depends on the type\nof code. Conclusion: LLM code reviews can help suggest improvements and assess\ncorrectness, but there is a risk of faulty outputs. We propose a process that\ninvolves humans, called the \"Human in the loop LLM Code Review\" to promote\nknowledge sharing while mitigating the risk of faulty outputs.", "comment": null, "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE;cs.AI", "links": "http://arxiv.org/abs/2505.20206v1;http://arxiv.org/pdf/2505.20206v1", "pdf_url": "http://arxiv.org/pdf/2505.20206v1"}, {"title": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models", "link": "https://arxiv.org/pdf/2505.21409", "details": "D Satriani, E Veltri, D Santoro, P Papotti - arXiv preprint arXiv:2505.21409, 2025", "abstract": "Factuality in **Large** **Language** **Models** (LLMs) is a persistent challenge. Current benchmarks often assess short factual answers, overlooking the critical ability to generate structured, multi-record tabular outputs from parametric knowledge. We \u2026", "entry_id": "http://arxiv.org/abs/2505.21409v1", "updated": "2025-05-27 16:33:38", "published": "2025-05-27 16:33:38", "authors": "Dario Satriani;Enzo Veltri;Donatello Santoro;Paolo Papotti", "summary": "Factuality in Large Language Models (LLMs) is a persistent challenge. Current\nbenchmarks often assess short factual answers, overlooking the critical ability\nto generate structured, multi-record tabular outputs from parametric knowledge.\nWe demonstrate that this relational fact retrieval is substantially more\ndifficult than isolated point-wise queries, even when individual facts are\nknown to the model, exposing distinct failure modes sensitive to output\ndimensionality (e.g., number of attributes or records). To systematically\nevaluate this under-explored capability, we introduce RelationalFactQA, a new\nbenchmark featuring diverse natural language questions (paired with SQL) and\ngold-standard tabular answers, specifically designed to assess knowledge\nretrieval in a structured format. RelationalFactQA enables analysis across\nvarying query complexities, output sizes, and data characteristics. Our\nexperiments reveal that even state-of-the-art LLMs struggle significantly, not\nexceeding 25% factual accuracy in generating relational outputs, with\nperformance notably degrading as output dimensionality increases. These\nfindings underscore critical limitations in current LLMs' ability to synthesize\nstructured factual knowledge and establish RelationalFactQA as a crucial\nresource for measuring future progress in LLM factuality.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.DB", "links": "http://arxiv.org/abs/2505.21409v1;http://arxiv.org/pdf/2505.21409v1", "pdf_url": "http://arxiv.org/pdf/2505.21409v1"}]
