[{"title": "FATE: Feature-Adapted Parameter Tuning for Vision-Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/32975/35130", "details": "Z Xu, Z Peng, X Yang, W Shen - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Following the recent popularity of vision language models, several attempts, eg, parameter-efficient fine-tuning (PEFT), have been made to extend them to different downstream tasks. Previous PEFT works motivate their methods from the view of \u2026"}, {"title": "Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning", "link": "https://arxiv.org/pdf/2504.05632", "details": "S Kabra, A Jha, C Reddy - arXiv preprint arXiv:2504.05632, 2025", "abstract": "Recent advances in large-scale generative language models have shown that reasoning capabilities can significantly improve model performance across a variety of tasks. However, the impact of reasoning on a model's ability to mitigate \u2026"}, {"title": "DART: Disease-aware Image-Text Alignment and Self-correcting Re-alignment for Trustworthy Radiology Report Generation", "link": "https://arxiv.org/pdf/2504.11786", "details": "SJ Park, KS Heo, DH Shin, YH Son, JH Oh, TE Kam - arXiv preprint arXiv:2504.11786, 2025", "abstract": "The automatic generation of radiology reports has emerged as a promising solution to reduce a time-consuming task and accurately capture critical disease-relevant findings in X-ray images. Previous approaches for radiology report generation have \u2026"}, {"title": "DDIR: Domain-Disentangled Invariant Representation learning for tailored predictions", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125004691", "details": "Y Ma, Y Gu, X Qin, S Guo, F Fan, F Dong, Y Chen - Knowledge-Based Systems, 2025", "abstract": "Traditional training methods often struggle to scale effectively with large datasets due to significant distributional differences. Domain generalization (DG) aims to address the challenge of generalizing across multiple-source domains and improving \u2026"}, {"title": "Overcoming Heterogeneous Data in Federated Medical Vision-Language Pre-training: A Triple-Embedding Model Selector Approach", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/32807/34962", "details": "A Wang, Z Zhang, D Wang, F Wang, H Hu, J Guo\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "The scarcity data of medical field brings the collaborative training in medical vision- language pre-training (VLP) cross different clients. Therefore, the collaborative training in medical VLP faces two challenges: First, the medical data requires \u2026"}, {"title": "HalluShift: Measuring Distribution Shifts towards Hallucination Detection in LLMs", "link": "https://arxiv.org/pdf/2504.09482", "details": "S Dasgupta, S Nath, A Basu, P Shamsolmoali, S Das - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have recently garnered widespread attention due to their adeptness at generating innovative responses to the given prompts across a multitude of domains. However, LLMs often suffer from the inherent limitation of \u2026"}, {"title": "Robust Classification with Noisy Labels Based on Posterior Maximization", "link": "https://arxiv.org/pdf/2504.06805", "details": "N Novello, AM Tonello - arXiv preprint arXiv:2504.06805, 2025", "abstract": "Designing objective functions robust to label noise is crucial for real-world classification algorithms. In this paper, we investigate the robustness to label noise of an $ f $-divergence-based class of objective functions recently proposed for \u2026"}, {"title": "Bi-modality Individual-aware Prompt tuning for Visual-Language Model", "link": "https://ieeexplore.ieee.org/abstract/document/10949734/", "details": "H Yao, R Zhang, H Lyu, Y Zhang, C Xu - IEEE Transactions on Pattern Analysis and \u2026, 2025", "abstract": "Prompt tuning is a valuable technique for adapting visual language models (VLMs) to different downstream tasks, such as domain generalization and learning from a few examples. Previous methods have utilized Context Optimization approaches to \u2026"}, {"title": "Stress-Testing of Multimodal Models in Medical Image-Based Report Generation", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/35203/37358", "details": "F Carvalhido, HL Cardoso, V Cerqueira - Proceedings of the AAAI Conference on \u2026, 2025", "abstract": "Multimodal models, namely vision-language models, present unique possibilities through the seamless integration of different information mediums for data generation. These models mostly act as a black-box, making them lack transparency \u2026"}]
