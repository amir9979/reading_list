[{"title": "GraphWiz: An Instruction-Following Language Model for Graph Computational Problems", "link": "https://dl.acm.org/doi/abs/10.1145/3637528.3672010", "details": "N Chen, Y Li, J Tang, J Li - Proceedings of the 30th ACM SIGKDD Conference on \u2026, 2024", "abstract": "Large language models (LLMs) have achieved impressive success across various domains, but their capability in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel \u2026"}, {"title": "Does Knowledge Localization Hold True? Surprising Differences Between Entity and Relation Perspectives in Language Models", "link": "https://arxiv.org/pdf/2409.00617", "details": "Y Wei, X Yu, Y Weng, H Ma, Y Zhang, J Zhao, K Liu - arXiv preprint arXiv:2409.00617, 2024", "abstract": "Large language models encapsulate knowledge and have demonstrated superior performance on various natural language processing tasks. Recent studies have localized this knowledge to specific model parameters, such as the MLP weights in \u2026"}, {"title": "Factual and Tailored Recommendation Endorsements using Language Models and Reinforcement Learning", "link": "https://openreview.net/pdf%3Fid%3DxI8C7sfN1H", "details": "J Jeong, Y Chow, G Tennenholtz, CW Hsu\u2026 - First Conference on Language \u2026", "abstract": "Recommender systems (RSs) play a central role in matching candidate items to users based on their preferences. While traditional RSs rely on user feed-back signals, conversational RSs interact with users in natural language. In this work, we \u2026"}, {"title": "CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks", "link": "https://arxiv.org/pdf/2409.03381", "details": "Y Deng, X Qiu, X Tan, C Qu, J Pan, Y Cheng, Y Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Cognitive psychology investigates perception, attention, memory, language, problem- solving, decision-making, and reasoning. Kahneman's dual-system theory elucidates the human decision-making process, distinguishing between the rapid, intuitive \u2026"}, {"title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models", "link": "https://arxiv.org/pdf/2408.12798", "details": "Y Li, H Huang, Y Zhao, X Ma, J Sun - arXiv preprint arXiv:2408.12798, 2024", "abstract": "Generative Large Language Models (LLMs) have made significant strides across various tasks, but they remain vulnerable to backdoor attacks, where specific triggers in the prompt cause the LLM to generate adversary-desired responses. While most \u2026"}, {"title": "Reasoning and Planning with Large Language Models in Code Development", "link": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671452", "details": "H Ding, Z Fan, I Guehring, G Gupta, W Ha, J Huan\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code \u2026"}, {"title": "Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback", "link": "https://arxiv.org/pdf/2409.00162", "details": "J Zhou, J Ji, J Dai, Y Yang - arXiv preprint arXiv:2409.00162, 2024", "abstract": "Aligning the behavior of Large language models (LLMs) with human intentions and values remains a critical challenge. Reinforcement learning from human feedback (RLHF) aligns LLMs by training a reward model (RM) on human preferences and fine \u2026"}, {"title": "SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large Language Models", "link": "https://arxiv.org/pdf/2408.15565", "details": "D Yu, B Peng, Y Tian, L Song, H Mi, D Yu - arXiv preprint arXiv:2408.15565, 2024", "abstract": "There is a growing trend of teaching large language models (LLMs) to solve mathematical problems through coding. Existing studies primarily focus on prompting powerful, closed-source models to generate seed training data followed by in \u2026"}, {"title": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models", "link": "https://arxiv.org/pdf/2408.15915", "details": "Y Yang, Y Qin, T Wu, Z Xu, G Li, P Guo, H Shao, Y Shi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The cultivation of expertise for large language models (LLMs) to solve tasks of specific areas often requires special-purpose tuning with calibrated behaviors on the expected stable outputs. To avoid huge cost brought by manual preparation of \u2026"}]
