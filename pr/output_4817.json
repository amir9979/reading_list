[{"title": "LIONs: An Empirically Optimized Approach to Align Language Models", "link": "https://arxiv.org/pdf/2407.06542", "details": "X Yu, Q Wu, Y Li, Z Yu - arXiv preprint arXiv:2407.06542, 2024", "abstract": "Alignment is a crucial step to enhance the instruction-following and conversational abilities of language models. Despite many recent work proposing new algorithms, datasets, and training pipelines, there is a lack of comprehensive studies measuring \u2026"}, {"title": "Igea: a Decoder-Only Language Model for Biomedical Text Generation in Italian", "link": "https://arxiv.org/pdf/2407.06011", "details": "TM Buonocore, S Rancati, E Parimbelli - arXiv preprint arXiv:2407.06011, 2024", "abstract": "The development of domain-specific language models has significantly advanced natural language processing applications in various specialized fields, particularly in biomedicine. However, the focus has largely been on English-language models \u2026"}, {"title": "Can Language Models Safeguard Themselves, Instantly and For Free?", "link": "https://openreview.net/pdf%3Fid%3DALRWSxT1rl", "details": "D Adila, C Shin, Y Zhang, F Sala - ICML 2024 Next Generation of AI Safety Workshop", "abstract": "Aligning pretrained language models (LMs) to handle a new safety scenario is normally difficult and expensive, often requiring access to large amounts of ground- truth preference data and substantial compute. Are these costs necessary? That is, is \u2026"}, {"title": "AutoTutor meets Large Language Models: A Language Model Tutor with Rich Pedagogy and Guardrails", "link": "https://dl.acm.org/doi/abs/10.1145/3657604.3662041", "details": "S Pal Chowdhury, V Zouhar, M Sachan - \u2026 of the Eleventh ACM Conference on \u2026, 2024", "abstract": "Large Language Models (LLMs) have found several use cases in education, ranging from automatic question generation to essay evaluation. In this paper, we explore the potential of using LLMs to author Intelligent Tutoring Systems. A common pitfall of \u2026"}, {"title": "A pre-trained language model for emergency department intervention prediction using routine physiological data and clinical narratives", "link": "https://www.sciencedirect.com/science/article/pii/S1386505624002272", "details": "TY Huang, CF Chong, HY Lin, TY Chen, YC Chang\u2026 - International Journal of \u2026, 2024", "abstract": "Introduction The urgency and complexity of emergency room (ER) settings require precise and swift decision-making processes for patient care. Ensuring the timely execution of critical examinations and interventions is vital for reducing diagnostic \u2026"}, {"title": "Pseudo-triplet Guided Few-shot Composed Image Retrieval", "link": "https://arxiv.org/pdf/2407.06001", "details": "B Hou, H Lin, H Wen, M Liu, X Song - arXiv preprint arXiv:2407.06001, 2024", "abstract": "Composed Image Retrieval (CIR) is a challenging task that aims to retrieve the target image based on a multimodal query, ie, a reference image and its corresponding modification text. While previous supervised or zero-shot learning paradigms all fail \u2026"}, {"title": "LLMBox: A Comprehensive Library for Large Language Models", "link": "https://arxiv.org/pdf/2407.05563", "details": "T Tang, Y Hu, B Li, W Luo, Z Qin, H Sun, J Wang, S Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To facilitate the research on large language models (LLMs), this paper presents a comprehensive and unified library, LLMBox, to ease the development, use, and evaluation of LLMs. This library is featured with three main merits:(1) a unified data \u2026"}, {"title": "The diagnostic and triage accuracy of the GPT-3 artificial intelligence model: an observational study", "link": "https://www.thelancet.com/journals/landig/article/PIIS2589-7500\\(24\\)00097-9/fulltext", "details": "DM Levine, R Tuwani, B Kompa, A Varma\u2026 - The Lancet Digital Health, 2024", "abstract": "Background Artificial intelligence (AI) applications in health care have been effective in many areas of medicine, but they are often trained for a single task using labelled data, making deployment and generalisability challenging. How well a general \u2026"}, {"title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated", "link": "https://arxiv.org/pdf/2407.10969", "details": "H Wang, S Ma, R Wang, F Wei - arXiv preprint arXiv:2407.10969, 2024", "abstract": "We introduce, Q-Sparse, a simple yet effective approach to training sparsely- activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is \u2026"}]
