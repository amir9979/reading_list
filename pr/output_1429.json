'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [SuRe: Summarizing Retrievals using Answer Candidates f'
[{"title": "MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems", "link": "https://arxiv.org/pdf/2404.09486", "details": "K Li, Y Tian, Q Hu, Z Luo, J Ma - arXiv preprint arXiv:2404.09486, 2024", "abstract": "Programming often involves converting detailed and complex specifications into code, a process during which developers typically utilize visual aids to more effectively convey concepts. While recent developments in Large Multimodal Models \u2026"}, {"title": "LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought", "link": "https://arxiv.org/pdf/2405.06705", "details": "Z Jiang, H Peng, S Feng, F Li, D Li - arXiv preprint arXiv:2405.06705, 2024", "abstract": "Self-correction is emerging as a promising approach to mitigate the issue of hallucination in Large Language Models (LLMs). To facilitate effective self-correction, recent research has proposed mistake detection as its initial step. However, current \u2026"}, {"title": "Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance", "link": "https://arxiv.org/pdf/2405.06703", "details": "G Muric, B Delay, S Minton - arXiv preprint arXiv:2405.06703, 2024", "abstract": "In this paper, we introduce the Interpretable Cross-Examination Technique (ICE-T), a novel approach that leverages structured multi-prompt techniques with Large Language Models (LLMs) to improve classification performance over zero-shot and \u2026"}, {"title": "Enhancing Confidence Expression in Large Language Models Through Learning from Past Experience", "link": "https://arxiv.org/pdf/2404.10315", "details": "H Han, T Li, S Chen, J Shi, C Du, Y Xiao, J Liang, X Lin - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have exhibited remarkable performance across various downstream tasks, but they may generate inaccurate or false information with a confident tone. One of the possible solutions is to empower the LLM confidence \u2026"}, {"title": "LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks", "link": "https://seclab.bu.edu/people/gianluca/papers/llm-oakland2024.pdf", "details": "S Ullah, M Han, S Pujar, H Pearce, A Coskun\u2026 - 2024 IEEE Symposium on \u2026, 2024", "abstract": "Large Language Models (LLMs) have been suggested for use in automated vulnerability repair, but benchmarks showing they can consistently identify security- related bugs are lacking. We thus develop SecLLMHolmes, a fully automated \u2026"}, {"title": "OpenLLM-Ro--Technical Report on Open-source Romanian LLMs trained starting from Llama 2", "link": "https://arxiv.org/pdf/2405.07703", "details": "M Masala, DC Ilie-Ablachim, D Corlatescu, M Zavelca\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, Large Language Models (LLMs) have achieved almost human-like performance on various tasks. While some LLMs have been trained on multilingual data, most of the training data is in English. Hence, their performance in English \u2026"}, {"title": "Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games", "link": "https://openreview.net/pdf%3Fid%3Ddhy1NBeusb", "details": "Y Zhang, J Lu, N Jaitly - ICLR 2024 Workshop: How Far Are We From AGI", "abstract": "Large language models (LLMs) are effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of \u2026"}, {"title": "Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots", "link": "https://arxiv.org/pdf/2405.07990", "details": "C Wu, Y Ge, Q Guo, J Wang, Z Liang, Z Lu, Y Shan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has attracted significant attention due to their superior performance in visual contexts. However, their capabilities in turning visual figure to executable code, have not been \u2026"}, {"title": "Backdoor Removal for Generative Large Language Models", "link": "https://arxiv.org/pdf/2405.07667", "details": "H Li, Y Chen, Z Zheng, Q Hu, C Chan, H Liu, Y Song - arXiv preprint arXiv \u2026, 2024", "abstract": "With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased \u2026"}]
