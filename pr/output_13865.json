[{"title": "Mapping 1,000+ Language Models via the Log-Likelihood Vector", "link": "https://arxiv.org/pdf/2502.16173", "details": "M Oyama, H Yamagiwa, Y Takase, H Shimodaira - arXiv preprint arXiv:2502.16173, 2025", "abstract": "To compare autoregressive language models at scale, we propose using log- likelihood vectors computed on a predefined text set as model features. This approach has a solid theoretical basis: when treated as model coordinates, their \u2026"}, {"title": "When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning", "link": "https://arxiv.org/pdf/2503.07588", "details": "J Luo, Y Zhang, X Yang, K Wu, Q Zhu, L Liang, J Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Efficient vision-language understanding of large Remote Sensing Images (RSIs) is meaningful but challenging. Current Large Vision-Language Models (LVLMs) typically employ limited pre-defined grids to process images, leading to information \u2026"}, {"title": "LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle Zero-shot Radiology Recognition?", "link": "https://arxiv.org/pdf/2503.07487", "details": "B Li, W Huang, Y Shen, Y Wang, S Lin, J Lin, L You\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, multimodal large models (MLLMs) have demonstrated exceptional capabilities in visual understanding and reasoning across various vision-language tasks. However, MLLMs usually perform poorly in zero-shot medical disease \u2026"}, {"title": "IDEA Prune: An Integrated Enlarge-and-Prune Pipeline in Generative Language Model Pretraining", "link": "https://arxiv.org/pdf/2503.05920", "details": "Y Li, X Du, A Jaiswal, T Lei, T Zhao, C Wang, J Wang - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in large language models have intensified the need for efficient and deployable models within limited inference budgets. Structured pruning pipelines have shown promise in token efficiency compared to training target-size \u2026"}, {"title": "Language Model Personalization via Reward Factorization", "link": "https://arxiv.org/pdf/2503.06358", "details": "I Shenfeld, F Faltings, P Agrawal, A Pacchiano - arXiv preprint arXiv:2503.06358, 2025", "abstract": "Modern large language models (LLMs) are optimized for human-aligned responses using Reinforcement Learning from Human Feedback (RLHF). However, existing RLHF approaches assume a universal preference model and fail to account for \u2026"}, {"title": "Mitigation of outcome conflation in predicting patient outcomes using electronic health records", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocaf033/8064347", "details": "SM Reincke, C Espinosa, P Chung, T James, E Berson\u2026 - Journal of the American \u2026, 2025", "abstract": "Objectives Artificial intelligence (AI) models utilizing electronic health record data for disease prediction can enhance risk stratification but may lack specificity, which is crucial for reducing the economic and psychological burdens associated with false \u2026"}, {"title": "Towards label-only membership inference attack against pre-trained large language models", "link": "https://arxiv.org/pdf/2502.18943", "details": "Y He, B Li, L Liu, Z Ba, W Dong, Y Li, Z Qin, K Ren\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Membership Inference Attacks (MIAs) aim to predict whether a data sample belongs to the model's training set or not. Although prior research has extensively explored MIAs in Large Language Models (LLMs), they typically require accessing to complete \u2026"}, {"title": "What's in a Latent? Leveraging Diffusion Latent Space for Domain Generalization", "link": "https://arxiv.org/pdf/2503.06698", "details": "X Thomas, D Ghadiyaram - arXiv preprint arXiv:2503.06698, 2025", "abstract": "Domain Generalization aims to develop models that can generalize to novel and unseen data distributions. In this work, we study how model architectures and pre- training objectives impact feature richness and propose a method to effectively \u2026"}, {"title": "Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation", "link": "https://arxiv.org/pdf/2502.11306", "details": "H Nguyen, Z He, SA Gandre, U Pasupulety\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) often suffer from hallucination, generating factually incorrect or ungrounded content, which limits their reliability in high-stakes applications. A key factor contributing to hallucination is the use of hard labels during \u2026"}]
