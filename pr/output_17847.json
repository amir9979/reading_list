[{"title": "Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break Large Models", "link": "https://arxiv.org/pdf/2506.00457", "details": "J Park, H Lee, D Lee, D Gwak, J Choo - arXiv preprint arXiv:2506.00457, 2025", "abstract": "Large Language Models (LLMs) have shown remarkable performance across diverse tasks without domain-specific training, fueling interest in their potential for time-series forecasting. While LLMs have shown potential in zero-shot forecasting \u2026", "entry_id": "http://arxiv.org/abs/2506.00457v1", "updated": "2025-05-31 08:24:01", "published": "2025-05-31 08:24:01", "authors": "Junwoo Park;Hyuck Lee;Dohyun Lee;Daehoon Gwak;Jaegul Choo", "summary": "Large Language Models (LLMs) have shown remarkable performance across diverse\ntasks without domain-specific training, fueling interest in their potential for\ntime-series forecasting. While LLMs have shown potential in zero-shot\nforecasting through prompting alone, recent studies suggest that LLMs lack\ninherent effectiveness in forecasting. Given these conflicting findings, a\nrigorous validation is essential for drawing reliable conclusions. In this\npaper, we evaluate the effectiveness of LLMs as zero-shot forecasters compared\nto state-of-the-art domain-specific models. Our experiments show that LLM-based\nzero-shot forecasters often struggle to achieve high accuracy due to their\nsensitivity to noise, underperforming even simple domain-specific models. We\nhave explored solutions to reduce LLMs' sensitivity to noise in the zero-shot\nsetting, but improving their robustness remains a significant challenge. Our\nfindings suggest that rather than emphasizing zero-shot forecasting, a more\npromising direction would be to focus on fine-tuning LLMs to better process\nnumerical sequences. Our experimental code is available at\nhttps://github.com/junwoopark92/revisiting-LLMs-zeroshot-forecaster.", "comment": "Annual Meeting of the Association for Computational Linguistics\n  (ACL), 2025, Accepted as Short Paper", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2506.00457v1;http://arxiv.org/pdf/2506.00457v1", "pdf_url": "http://arxiv.org/pdf/2506.00457v1"}, {"title": "Optimizing Data Augmentation through Bayesian Model Selection", "link": "https://arxiv.org/pdf/2505.21813", "details": "M Matymov, BH Tran, M Kampffmeyer, M Heinonen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Data Augmentation (DA) has become an essential tool to improve robustness and generalization of modern machine learning. However, when deciding on DA strategies it is critical to choose parameters carefully, and this can be a daunting task \u2026", "entry_id": "http://arxiv.org/abs/2505.21813v1", "updated": "2025-05-27 22:44:36", "published": "2025-05-27 22:44:36", "authors": "Madi Matymov;Ba-Hien Tran;Michael Kampffmeyer;Markus Heinonen;Maurizio Filippone", "summary": "Data Augmentation (DA) has become an essential tool to improve robustness and\ngeneralization of modern machine learning. However, when deciding on DA\nstrategies it is critical to choose parameters carefully, and this can be a\ndaunting task which is traditionally left to trial-and-error or expensive\noptimization based on validation performance. In this paper, we counter these\nlimitations by proposing a novel framework for optimizing DA. In particular, we\ntake a probabilistic view of DA, which leads to the interpretation of\naugmentation parameters as model (hyper)-parameters, and the optimization of\nthe marginal likelihood with respect to these parameters as a Bayesian model\nselection problem. Due to its intractability, we derive a tractable Evidence\nLower BOund (ELBO), which allows us to optimize augmentation parameters jointly\nwith model parameters. We provide extensive theoretical results on variational\napproximation quality, generalization guarantees, invariance properties, and\nconnections to empirical Bayes. Through experiments on computer vision tasks,\nwe show that our approach improves calibration and yields robust performance\nover fixed or no augmentation. Our work provides a rigorous foundation for\noptimizing DA through Bayesian principles with significant potential for robust\nmachine learning.", "comment": "26 pages, 3 figures", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;stat.ML;62F15, 68T07 (Primary) 62M45, 62C10, 65C60 (Secondary)", "links": "http://arxiv.org/abs/2505.21813v1;http://arxiv.org/pdf/2505.21813v1", "pdf_url": "http://arxiv.org/pdf/2505.21813v1"}, {"title": "Cluster-Aware Causal Mixer for Online Anomaly Detection in Multivariate Time Series", "link": "https://arxiv.org/pdf/2506.00188", "details": "MMN Murad, Y Yilmaz - arXiv preprint arXiv:2506.00188, 2025", "abstract": "Early and accurate detection of anomalies in time series data is critical, given the significant risks associated with false or missed detections. While MLP-based mixer models have shown promise in time series analysis, they lack a causality mechanism \u2026", "entry_id": "http://arxiv.org/abs/2506.00188v1", "updated": "2025-05-30 19:56:54", "published": "2025-05-30 19:56:54", "authors": "Md Mahmuddun Nabi Murad;Yasin Yilmaz", "summary": "Early and accurate detection of anomalies in time series data is critical,\ngiven the significant risks associated with false or missed detections. While\nMLP-based mixer models have shown promise in time series analysis, they lack a\ncausality mechanism to preserve temporal dependencies inherent in the system.\nMoreover, real-world multivariate time series often contain numerous channels\nwith diverse inter-channel correlations. A single embedding mechanism for all\nchannels does not effectively capture these complex relationships. To address\nthese challenges, we propose a novel cluster-aware causal mixer to effectively\ndetect anomalies in multivariate time series. Our model groups channels into\nclusters based on their correlations, with each cluster processed through a\ndedicated embedding layer. In addition, we introduce a causal mixer in our\nmodel, which mixes the information while maintaining causality. Furthermore, we\npresent an anomaly detection framework that accumulates the anomaly evidence\nover time to prevent false positives due to nominal outliers. Our proposed\nmodel operates in an online fashion, making it suitable for real-time\ntime-series anomaly detection tasks. Experimental evaluations across six public\nbenchmark datasets demonstrate that our model consistently achieves superior F1\nscores.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;stat.ML", "links": "http://arxiv.org/abs/2506.00188v1;http://arxiv.org/pdf/2506.00188v1", "pdf_url": "http://arxiv.org/pdf/2506.00188v1"}]
