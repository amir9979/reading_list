[{"title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models", "link": "https://arxiv.org/pdf/2406.19146", "details": "T Porian, M Wortsman, J Jitsev, L Schmidt, Y Carmon - arXiv preprint arXiv \u2026, 2024", "abstract": "Kaplan et al. and Hoffmann et al. developed influential scaling laws for the optimal model size as a function of the compute budget, but these laws yield substantially different predictions. We explain the discrepancy by reproducing the Kaplan scaling \u2026"}, {"title": "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models", "link": "https://arxiv.org/pdf/2406.18880", "details": "V Rathore, A Deb, A Chandresh, P Singla - arXiv preprint arXiv:2406.18880, 2024", "abstract": "Recently, very large language models (LLMs) have shown exceptional performance on several English NLP tasks with just in-context learning (ICL), but their utility in other languages is still underexplored. We investigate their effectiveness for NLP \u2026"}, {"title": "Electronic Health Record\u2013Oriented Knowledge Graph System for Collaborative Clinical Decision Support Using Multicenter Fragmented Medical Data: Design and \u2026", "link": "https://www.jmir.org/2024/1/e54263/", "details": "Y Shang, Y Tian, K Lyu, T Zhou, P Zhang, J Chen, J Li - Journal of Medical Internet \u2026, 2024", "abstract": "Background The medical knowledge graph provides explainable decision support, helping clinicians with prompt diagnosis and treatment suggestions. However, in real- world clinical practice, patients visit different hospitals seeking various medical \u2026"}, {"title": "Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks", "link": "https://arxiv.org/pdf/2406.12066", "details": "J Gallifant, S Chen, P Moreira, N Munch, M Gao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical knowledge is context-dependent and requires consistent reasoning across various natural language expressions of semantically equivalent phrases. This is particularly crucial for drug names, where patients often use brand names like Advil \u2026"}, {"title": "Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations", "link": "https://arxiv.org/pdf/2406.11801", "details": "R Hazra, S Layek, S Banerjee, S Poria - arXiv preprint arXiv:2406.11801, 2024", "abstract": "Ensuring the safe alignment of large language models (LLMs) with human values is critical as they become integral to applications like translation and question answering. Current alignment methods struggle with dynamic user intentions and \u2026"}, {"title": "Memory Augmented Language Models through Mixture of Word Experts", "link": "https://aclanthology.org/2024.naacl-long.249.pdf", "details": "C dos Santos, J Lee-Thorp, I Noble, CC Chang\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Scaling up the number of parameters of language models has proven to be an effective approach to improve performance. For dense models, increasing their size proportionally increases their computational footprint. In this work, we seek to \u2026"}, {"title": "An Empirical Study of Mamba-based Language Models", "link": "https://arxiv.org/pdf/2406.07887", "details": "R Waleffe, W Byeon, D Riach, B Norick, V Korthikanti\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value \u2026"}, {"title": "Protecting Privacy Through Approximating Optimal Parameters for Sequence Unlearning in Language Models", "link": "https://arxiv.org/pdf/2406.14091", "details": "D Lee, D Rim, M Choi, J Choo - arXiv preprint arXiv:2406.14091, 2024", "abstract": "Although language models (LMs) demonstrate exceptional capabilities on various tasks, they are potentially vulnerable to extraction attacks, which represent a significant privacy risk. To mitigate the privacy concerns of LMs, machine unlearning \u2026"}, {"title": "Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?", "link": "https://arxiv.org/pdf/2406.13121", "details": "J Lee, A Chen, Z Dai, D Dua, DS Sachan, M Boratko\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. Leveraging LCLMs' ability to natively ingest and process entire corpora of \u2026"}]
