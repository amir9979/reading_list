[{"title": "Causal-Informed Contrastive Learning: Towards Bias-Resilient Pre-training under Concept Drift", "link": "https://arxiv.org/pdf/2502.07620%3F", "details": "X Yang, J Lu, E Yu - arXiv preprint arXiv:2502.07620, 2025", "abstract": "The evolution of large-scale contrastive pre-training propelled by top-tier datasets has reached a transition point in the scaling law. Consequently, sustaining and enhancing a model's pre-training capabilities in drift environments have surfaced as \u2026"}, {"title": "Deep Out-of-Distribution Uncertainty Quantification via Weight Entropy Maximization", "link": "http://www.jmlr.org/papers/volume26/23-1359/23-1359.pdf", "details": "A de Mathelin, F Deheeger, M Mougeot, N Vayatis - Journal of Machine Learning \u2026, 2025", "abstract": "This paper deals with uncertainty quantification and out-of-distribution detection in deep learning using Bayesian and ensemble methods. It proposes a practical solution to the lack of prediction diversity observed recently for standard approaches \u2026"}]
