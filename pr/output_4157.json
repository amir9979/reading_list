[{"title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models", "link": "https://arxiv.org/pdf/2407.11484", "details": "N Chen, Y Wang, Y Deng, J Li - arXiv preprint arXiv:2407.11484, 2024", "abstract": "This survey explores the burgeoning field of role-playing with language models, focusing on their development from early persona-based models to advanced character-driven simulations facilitated by Large Language Models (LLMs). Initially \u2026"}, {"title": "Aligning Language Models with the Human World", "link": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi%3Farticle%3D1241%26context%3Ddissertations", "details": "R LIU - 2024", "abstract": "Abstract The field of Natural Language Processing (NLP) has undergone a significant transformation with the emergence of large language models (LMs). These models have enabled the development of human-like conversational \u2026"}, {"title": "Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization", "link": "https://arxiv.org/pdf/2407.07880", "details": "J Wu, Y Xie, Z Yang, J Wu, J Chen, J Gao, B Ding\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study addresses the challenge of noise in training datasets for Direct Preference Optimization (DPO), a method for aligning Large Language Models (LLMs) with human preferences. We categorize noise into pointwise noise, which includes low \u2026"}, {"title": "Evaluating Large Language Models with fmeval", "link": "https://arxiv.org/pdf/2407.12872", "details": "P Schw\u00f6bel, L Franceschi, MB Zafar, K Vasist\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "fmeval is an open source library to evaluate large language models (LLMs) in a range of tasks. It helps practitioners evaluate their model for task performance and along multiple responsible AI dimensions. This paper presents the library and \u2026"}, {"title": "Unlocking Continual Learning Abilities in Language Models", "link": "https://arxiv.org/pdf/2406.17245", "details": "W Du, S Cheng, T Luo, Z Qiu, Z Huang, KC Cheung\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) exhibit impressive performance and generalization capabilities. However, LMs struggle with the persistent challenge of catastrophic forgetting, which undermines their long-term sustainability in continual learning (CL) \u2026"}, {"title": "Optimizing Tourism Accommodation Offers by Integrating Language Models and Knowledge Graph Technologies", "link": "https://www.mdpi.com/2078-2489/15/7/398", "details": "A Cadeddu, A Chessa, V De Leo, G Fenu, E Motta\u2026 - Information, 2024", "abstract": "Online platforms have become the primary means for travellers to search, compare, and book accommodations for their trips. Consequently, online platforms and revenue managers must acquire a comprehensive comprehension of these \u2026"}, {"title": "Benchmarking Mental State Representations in Language Models", "link": "https://arxiv.org/pdf/2406.17513", "details": "M Bortoletto, C Ruhdorfer, L Shi, A Bulling - arXiv preprint arXiv:2406.17513, 2024", "abstract": "While numerous works have assessed the generative performance of language models (LMs) on tasks requiring Theory of Mind reasoning, research into the models' internal representation of mental states remains limited. Recent work has used \u2026"}, {"title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2407.11681", "details": "H Cheng, M Zhang, JQ Shi - arXiv preprint arXiv:2407.11681, 2024", "abstract": "As Large Language Models (LLMs) grow dramatically in size, there is an increasing trend in compressing and speeding up these models. Previous studies have highlighted the usefulness of gradients for importance scoring in neural network \u2026"}, {"title": "PAG-LLM: Paraphrase and Aggregate with Large Language Models for Minimizing Intent Classification Errors", "link": "https://dl.acm.org/doi/pdf/10.1145/3626772.3657959", "details": "V Yadav, Z Tang, V Srinivasan - Proceedings of the 47th International ACM SIGIR \u2026, 2024", "abstract": "Large language models (LLM) have achieved remarkable success in natural language generation but lesser focus has been given to their applicability in key tasks such as intent-classification. We show that LLMs like LLaMa can achieve high \u2026"}]
