[{"title": "Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models", "link": "https://arxiv.org/pdf/2407.02716", "details": "X Han, L Jin, X Ma, X Liu - arXiv preprint arXiv:2407.02716, 2024", "abstract": "Fine-tuning pre-trained Vision-Language Models (VLMs) has shown remarkable capabilities in medical image and textual depiction synergy. Nevertheless, many pre- training datasets are restricted by patient privacy concerns, potentially containing \u2026"}, {"title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models", "link": "https://arxiv.org/pdf/2407.03181", "details": "H Puerto, T Chubakov, X Zhu, HT Madabushi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model \u2026"}, {"title": "Semantic Compositions Enhance Vision-Language Contrastive Learning", "link": "https://arxiv.org/pdf/2407.01408", "details": "M Aladago, L Torresani, S Vosoughi - arXiv preprint arXiv:2407.01408, 2024", "abstract": "In the field of vision-language contrastive learning, models such as CLIP capitalize on matched image-caption pairs as positive examples and leverage within-batch non- matching pairs as negatives. This approach has led to remarkable outcomes in zero \u2026"}, {"title": "UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization", "link": "https://arxiv.org/pdf/2407.03525", "details": "MN Uddin, A Saeidi, D Handa, A Seth, TC Son\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces UnSeenTimeQA, a novel time-sensitive question-answering (TSQA) benchmark that diverges from traditional TSQA benchmarks by avoiding factual and web-searchable queries. We present a series of time-sensitive event \u2026"}, {"title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs", "link": "https://arxiv.org/pdf/2407.00653", "details": "Y Zhang, X Wang, J Liang, S Xia, L Chen, Y Xiao - arXiv preprint arXiv:2407.00653, 2024", "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new \u2026"}, {"title": "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models", "link": "https://arxiv.org/pdf/2407.01009", "details": "J Pan, Y Zhang, C Zhang, Z Liu, H Wang, H Li - arXiv preprint arXiv:2407.01009, 2024", "abstract": "Large language models (LLMs) have demonstrated emergent capabilities across diverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However, such a simple and fast COT approach often encounters limitations in dealing with \u2026"}, {"title": "Universal Approximation Theory: The basic theory for large language models", "link": "https://arxiv.org/pdf/2407.00958", "details": "W Wang, Q Li - arXiv preprint arXiv:2407.00958, 2024", "abstract": "Language models have emerged as a critical area of focus in artificial intelligence, particularly with the introduction of groundbreaking innovations like ChatGPT. Large- scale Transformer networks have quickly become the leading approach for \u2026"}, {"title": "GraphArena: Benchmarking Large Language Models on Graph Computational Problems", "link": "https://arxiv.org/pdf/2407.00379", "details": "J Tang, Q Zhang, Y Li, J Li - arXiv preprint arXiv:2407.00379, 2024", "abstract": "The\" arms race\" of Large Language Models (LLMs) demands novel, challenging, and diverse benchmarks to faithfully examine their progresses. We introduce GraphArena, a benchmarking tool designed to evaluate LLMs on graph \u2026"}, {"title": "LLM-Select: Feature Selection with Large Language Models", "link": "https://arxiv.org/pdf/2407.02694", "details": "DP Jeong, ZC Lipton, P Ravikumar - arXiv preprint arXiv:2407.02694, 2024", "abstract": "In this paper, we demonstrate a surprising capability of large language models (LLMs): given only input feature names and a description of a prediction task, they are capable of selecting the most predictive features, with performance rivaling the \u2026"}]
