'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Consistency and Uncertainty: Identifying Unreliable Re'
[{"title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2404.05567", "details": "B Pan, Y Shen, H Liu, M Mishra, G Zhang, A Oliva\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) language models can reduce computational costs by 2- 4$\\times $ compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios. However, MoE models generally \u2026"}, {"title": "Can Language Models Solve Olympiad Programming?", "link": "https://arxiv.org/pdf/2404.10952", "details": "Q Shi, M Tang, K Narasimhan, S Yao - arXiv preprint arXiv:2404.10952, 2024", "abstract": "Computing olympiads contain some of the most challenging problems for humans, requiring complex algorithmic reasoning, puzzle solving, in addition to generating efficient code. However, it has been understudied as a domain to evaluate language \u2026"}, {"title": "ReCycle: Fast and Efficient Long Time Series Forecasting with Residual Cyclic Transformers", "link": "https://arxiv.org/pdf/2405.03429", "details": "A Weyrauch, T Steens, O Taubert, B Hanke, A Eqbal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Transformers have recently gained prominence in long time series forecasting by elevating accuracies in a variety of use cases. Regrettably, in the race for better predictive performance the overhead of model architectures has grown onerous \u2026"}, {"title": "Multi-Resolution Expansion of Analysis in Time-Frequency Domain for Time Series Forecasting", "link": "https://ieeexplore.ieee.org/abstract/document/10520822/", "details": "K Yan, C Long, H Wu, Z Wen - IEEE Transactions on Knowledge and Data \u2026, 2024", "abstract": "Time series forecasting plays a crucial role in various real-world applications, such as finance, energy, traffic, and healthcare, providing valuable insights for decision- making processes. The aggregation of information windows with different resolutions \u2026"}, {"title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training", "link": "https://arxiv.org/pdf/2405.03133", "details": "Z Zhong, M Xia, D Chen, M Lewis - arXiv preprint arXiv:2405.03133, 2024", "abstract": "Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture, SMEAR, was proposed \u2026"}, {"title": "CoE-SQL: In-Context Learning for Multi-Turn Text-to-SQL with Chain-of-Editions", "link": "https://arxiv.org/pdf/2405.02712", "details": "H Zhang, R Cao, H Xu, L Chen, K Yu - arXiv preprint arXiv:2405.02712, 2024", "abstract": "Recently, Large Language Models (LLMs) have been demonstrated to possess impressive capabilities in a variety of domains and tasks. We investigate the issue of prompt design in the multi-turn text-to-SQL task and attempt to enhance the LLMs' \u2026"}, {"title": "Self-playing Adversarial Language Game Enhances LLM Reasoning", "link": "https://arxiv.org/pdf/2404.10642", "details": "P Cheng, T Hu, H Xu, Z Zhang, Y Dai, L Han, N Du - arXiv preprint arXiv:2404.10642, 2024", "abstract": "We explore the self-play training procedure of large language models (LLMs) in a two-player adversarial language game called Adversarial Taboo. In this game, an attacker and a defender communicate with respect to a target word only visible to the \u2026"}, {"title": "ZeroEA: A Zero-Training Entity Alignment Framework via Pre-Trained Language Model", "link": "https://www.vldb.org/pvldb/vol17/p1765-huo.pdf", "details": "N Huo, R Cheng, B Kao, W Ning, N Al Hasan, X Li, J Li\u2026", "abstract": "Entity alignment (EA), a crucial task in knowledge graph (KG) research, aims to identify equivalent entities across different KGs to support downstream tasks like KG integration, text-to-SQL, and question-answering systems. Given rich semantic \u2026"}, {"title": "FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations", "link": "https://arxiv.org/pdf/2404.06619", "details": "J Dwivedi-Yu, R Dwivedi, T Schick - arXiv preprint arXiv:2404.06619, 2024", "abstract": "The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes \u2026"}]
