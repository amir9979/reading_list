[{"title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "link": "https://arxiv.org/pdf/2405.00402", "details": "L Ranaldi, A Freitas - arXiv preprint arXiv:2405.00402, 2024", "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches \u2026"}, {"title": "Coactive Learning for Large Language Models using Implicit User Feedback", "link": "https://openreview.net/pdf%3Fid%3DQ7cwVnRWAs", "details": "AD Tucker, K Brantley, A Cahall, T Joachims - 2024", "abstract": "We propose coactive learning as a model and feedback mechanism for training large language models (LLMs). The key insight is that users provide implicit feedback whenever they edit the text $ y $ proposed by an LLM. While the edited text $\\bar y \u2026"}, {"title": "Self-play preference optimization for language model alignment", "link": "https://arxiv.org/pdf/2405.00675", "details": "Y Wu, Z Sun, H Yuan, K Ji, Y Yang, Q Gu - arXiv preprint arXiv:2405.00675, 2024", "abstract": "Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest \u2026"}, {"title": "Navigating the Modern Evaluation Landscape: Considerations in Benchmarks and Frameworks for Large Language Models (LLMs)", "link": "https://aclanthology.org/2024.lrec-tutorials.4.pdf", "details": "L Choshen, A Gera, Y Perlitz, M Shmueli-Scheuer\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract General-Purpose Language Models have changed the world of Natural Language Processing, if not the world itself. The evaluation of such versatile models, while supposedly similar to evaluation of generation models before them, in fact \u2026"}]
