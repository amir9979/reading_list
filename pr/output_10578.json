[{"title": "Minerva LLMs: The First Family of Large Language Models Trained from Scratch on Italian Data", "link": "https://clic2024.ilc.cnr.it/wp-content/uploads/2024/12/76_main_long.pdf", "details": "R Orlando, L Moroni, PLH Cabot, E Barba, S Conia\u2026 - Proceedings of the Tenth \u2026, 2024", "abstract": "The growing interest in Large Language Models (LLMs) has accelerated research efforts to adapt these models for various languages. Despite this, pretraining LLMs from scratch for non-English languages remains underexplored. This is the case for \u2026"}, {"title": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models", "link": "https://arxiv.org/pdf/2412.02830", "details": "H Tran, Z Yao, J Wang, Y Zhang, Z Yang, H Yu - arXiv preprint arXiv:2412.02830, 2024", "abstract": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a versatile extension to the mutual reasoning framework (rStar), aimed at enhancing reasoning accuracy and factual integrity across large language models (LLMs) for \u2026"}, {"title": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures", "link": "https://arxiv.org/pdf/2411.16260", "details": "FC Chang, PY Wu - arXiv preprint arXiv:2411.16260, 2024", "abstract": "Large language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions. This approach has enabled \u2026"}]
