[{"title": "Finetuning Language Models to Emit Linguistic Expressions of Uncertainty", "link": "https://arxiv.org/pdf/2409.12180", "details": "A Chaudhry, S Thiagarajan, D Gorur - arXiv preprint arXiv:2409.12180, 2024", "abstract": "Large language models (LLMs) are increasingly employed in information-seeking and decision-making tasks. Despite their broad utility, LLMs tend to generate information that conflicts with real-world facts, and their persuasive style can make \u2026"}, {"title": "Mutual Prompt Leaning for Vision Language Models", "link": "https://link.springer.com/article/10.1007/s11263-024-02243-z", "details": "S Long, Z Zhao, J Yuan, Z Tan, J Liu, J Feng, S Wang\u2026 - International Journal of \u2026, 2024", "abstract": "Large pre-trained vision language models (VLMs) have demonstrated impressive representation learning capabilities, but their transferability across various downstream tasks heavily relies on prompt learning. Since VLMs consist of text and \u2026"}, {"title": "Gauging, enriching and applying geography knowledge in Pre-trained Language Models", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324002516", "details": "N Ramrakhiyani, V Varma, GK Palshikar, S Pawar - Information Processing & \u2026, 2025", "abstract": "Abstract To employ Pre-trained Language Models (PLMs) as knowledge containers in niche domains it is important to gauge the knowledge of these PLMs about facts in these domains. It is also an important pre-requisite to know how much enrichment \u2026"}, {"title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions", "link": "https://arxiv.org/pdf/2409.18042", "details": "K Chen, Y Gou, R Huang, Z Liu, D Tan, J Xu, C Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and \u2026"}, {"title": "Inference-Time Language Model Alignment via Integrated Value Guidance", "link": "https://arxiv.org/pdf/2409.17819", "details": "Z Liu, Z Zhou, Y Wang, C Yang, Y Qiao - arXiv preprint arXiv:2409.17819, 2024", "abstract": "Large language models are typically fine-tuned to align with human preferences, but tuning large models is computationally intensive and complex. In this work, we introduce $\\textit {Integrated Value Guidance} $(IVG), a method that uses implicit and \u2026"}, {"title": "Parameter Efficiency, Few-Shot, Zero-Shot, Prompting", "link": "https://jonmay.github.io/USC-CS662/assets/files/llm.pdf", "details": "J May - 2024", "abstract": "The models we've discussed so far follow the paradigm that, out of the box, they don't do too much, but when you expose them to some supervised data that is an exemplar of a task and fine-tune their parameters they can do the task when given \u2026"}, {"title": "Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia", "link": "https://arxiv.org/pdf/2409.17391", "details": "Z Zhou, J Wang, D Lin, K Chen - arXiv preprint arXiv:2409.17391, 2024", "abstract": "Though Large Language Models (LLMs) have shown remarkable abilities in mathematics reasoning, they are still struggling with performing numeric operations accurately, such as addition and multiplication. Numbers can be tokenized into \u2026"}, {"title": "A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B", "link": "https://arxiv.org/pdf/2409.11055", "details": "J Lee, S Park, J Kwon, J Oh, Y Kwon - arXiv preprint arXiv:2409.11055, 2024", "abstract": "Prior research works have evaluated quantized LLMs using limited metrics such as perplexity or a few basic knowledge tasks and old datasets. Additionally, recent large- scale models such as Llama 3.1 with up to 405B have not been thoroughly \u2026"}, {"title": "Graph Reasoning with Large Language Models via Pseudo-code Prompting", "link": "https://arxiv.org/pdf/2409.17906", "details": "K Skianis, G Nikolentzos, M Vazirgiannis - arXiv preprint arXiv:2409.17906, 2024", "abstract": "Large language models (LLMs) have recently achieved remarkable success in various reasoning tasks in the field of natural language processing. This success of LLMs has also motivated their use in graph-related tasks. Among others, recent work \u2026"}]
