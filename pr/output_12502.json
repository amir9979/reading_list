[{"title": "ELAINE-medLLM: Lightweight English Japanese Chinese Trilingual Large Language Model for Bio-medical Domain", "link": "https://aclanthology.org/2025.coling-main.313.pdf", "details": "K Yano, Z Luo, J Huang, Q Xie, M Asada, C Yuan\u2026 - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract We propose ELAINE (EngLish-jApanese-chINesE)-medLLM, a trilingual (English, Japanese, Chinese) large language model adapted for the bio-medical domain based on Llama-3-8B. The training dataset was carefully curated in terms of \u2026"}, {"title": "EHealth: A Chinese Biomedical Language Model Built via Multi-Level Text Discrimination", "link": "https://ieeexplore.ieee.org/abstract/document/10857372/", "details": "Q Wang, S Dai, B Xu, Y Lyu, H Wu, H Wang - IEEE Transactions on Audio, Speech \u2026, 2025", "abstract": "Pre-trained language models (PLMs) have recently revolutionized the field of natural language processing, impacting not only the general domain but also the biomedical domain. Most previous studies on constructing biomedical PLMs relied simply on \u2026"}, {"title": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models", "link": "https://arxiv.org/pdf/2501.17420", "details": "Y Li, H Shirado, S Das - arXiv preprint arXiv:2501.17420, 2025", "abstract": "While advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating \u2026"}]
