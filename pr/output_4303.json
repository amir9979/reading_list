[{"title": "Compact Language Models via Pruning and Knowledge Distillation", "link": "https://arxiv.org/pdf/2407.14679", "details": "S Muralidharan, ST Sreenivas, R Joshi, M Chochowski\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute- intensive. In this paper, we investigate if pruning an existing LLM and then re-training \u2026"}, {"title": "Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models", "link": "https://arxiv.org/pdf/2407.15504", "details": "A Girish, A Nagle, M Bondaschi, M Gastpar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We formalize the problem of prompt compression for large language models (LLMs) and present a framework to unify token-level prompt compression methods which create hard prompts for black-box models. We derive the distortion-rate function for \u2026"}, {"title": "Leveraging Language Models and Automatic Summarization in Online Programming Learning Environments", "link": "https://dl.acm.org/doi/full/10.1145/3653323", "details": "C Areces, L Benotti, F Bulgarelli, E Echeveste, N Finzi - Communications of the ACM", "abstract": "Objective A. Enhance the interaction between tutors, the Mumuki platform, and the group of trainee programmers. By utilizing the stochastic language models of learners' errors in each programming language, training errors in the exercise are \u2026"}, {"title": "Human-Interpretable Adversarial Prompt Attack on Large Language Models with Situational Context", "link": "https://arxiv.org/pdf/2407.14644", "details": "N Das, E Raff, M Gaur - arXiv preprint arXiv:2407.14644, 2024", "abstract": "Previous research on testing the vulnerabilities in Large Language Models (LLMs) using adversarial attacks has primarily focused on nonsensical prompt injections, which are easily detected upon manual or automated review (eg, via byte entropy) \u2026"}, {"title": "Imposter. AI: Adversarial Attacks with Hidden Intentions towards Aligned Large Language Models", "link": "https://arxiv.org/pdf/2407.15399", "details": "X Liu, L Li, T Xiang, F Ye, L Wei, W Li, N Garcia - arXiv preprint arXiv:2407.15399, 2024", "abstract": "With the development of large language models (LLMs) like ChatGPT, both their vast applications and potential vulnerabilities have come to the forefront. While developers have integrated multiple safety mechanisms to mitigate their misuse, a \u2026"}]
