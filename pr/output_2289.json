[{"title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment", "link": "https://arxiv.org/pdf/2405.19332", "details": "S Zhang, D Yu, H Sharma, Z Yang, S Wang, H Hassan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed \u2026"}, {"title": "Calibrating Reasoning in Language Models with Internal Consistency", "link": "https://arxiv.org/pdf/2405.18711", "details": "Z Xie, J Guo, T Yu, S Li - arXiv preprint arXiv:2405.18711, 2024", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought (CoT) prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious \u2026"}, {"title": "EFTNAS: Searching for Efficient Language Models in First-Order Weight-Reordered Super-Networks", "link": "https://aclanthology.org/2024.lrec-main.497.pdf", "details": "JP Munoz, Y Zheng, N Jain - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Transformer-based models have demonstrated outstanding performance in natural language processing (NLP) tasks and many other domains, eg, computer vision. Depending on the size of these models, which have grown exponentially in the past \u2026"}, {"title": "A Philosophical Introduction to Language Models-Part II: The Way Forward", "link": "https://arxiv.org/pdf/2405.03207", "details": "R Milli\u00e8re, C Buckner - arXiv preprint arXiv:2405.03207, 2024", "abstract": "In this paper, the second of two companion pieces, we explore novel philosophical questions raised by recent progress in large language models (LLMs) that go beyond the classical debates covered in the first part. We focus particularly on issues \u2026"}, {"title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models", "link": "https://arxiv.org/pdf/2405.16057", "details": "X Lu, A Zhou, Y Xu, R Zhang, P Gao, H Li - arXiv preprint arXiv:2405.16057, 2024", "abstract": "Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine- tuning and deployment. Current post-training pruning methods, while reducing the \u2026"}, {"title": "Kestrel: Point Grounding Multimodal LLM for Part-Aware 3D Vision-Language Understanding", "link": "https://arxiv.org/pdf/2405.18937", "details": "J Fei, M Ahmed, J Ding, EM Bakr, M Elhoseiny - arXiv preprint arXiv:2405.18937, 2024", "abstract": "While 3D MLLMs have achieved significant progress, they are restricted to object and scene understanding and struggle to understand 3D spatial structures at the part level. In this paper, we introduce Kestrel, representing a novel approach that \u2026"}, {"title": "TEII: Think, Explain, Interact and Iterate with Large Language Models to Solve Cross-lingual Emotion Detection", "link": "https://arxiv.org/pdf/2405.17129", "details": "L Cheng, Q Shao, C Zhao, S Bi, GA Levow - arXiv preprint arXiv:2405.17129, 2024", "abstract": "Cross-lingual emotion detection allows us to analyze global trends, public opinion, and social phenomena at scale. We participated in the Explainability of Cross-lingual Emotion Detection (EXALT) shared task, achieving an F1-score of 0.6046 on the \u2026"}, {"title": "On the Noise Robustness of In-Context Learning for Text Generation", "link": "https://arxiv.org/pdf/2405.17264", "details": "H Gao, F Zhang, W Jiang, J Shu, F Zheng, H Wei - arXiv preprint arXiv:2405.17264, 2024", "abstract": "Large language models (LLMs) have shown impressive performance on downstream tasks by in-context learning (ICL), which heavily relies on the quality of demonstrations selected from a large set of annotated examples. Recent works claim \u2026"}, {"title": "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis", "link": "https://arxiv.org/pdf/2405.05496", "details": "X Ding, J Zhou, L Dou, Q Chen, Y Wu, C Chen, L He - arXiv preprint arXiv \u2026, 2024", "abstract": "Aspect-based sentiment analysis (ABSA) is an important subtask of sentiment analysis, which aims to extract the aspects and predict their sentiments. Most existing studies focus on improving the performance of the target domain by fine-tuning \u2026"}]
