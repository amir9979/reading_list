[{"title": "Alphazero-like tree-search can guide large language model decoding and training", "link": "https://openreview.net/pdf%3Fid%3DC4OpREezgj", "details": "Z Wan, X Feng, M Wen, SM McAleer, Y Wen, W Zhang\u2026 - Forty-first International \u2026, 2024", "abstract": "Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the multi-step reasoning capabilities of LLMs by using tree-search algorithms. These methods rely on prompting a pre-trained model to serve as a value \u2026"}, {"title": "Risk Factors for Diabetic Retinopathy and Neural Retinal Degeneration in Youth with Type 1 Diabetes", "link": "https://iovs.arvojournals.org/article.aspx%3Farticleid%3D2800267", "details": "K Sampani, J Keady, PS Silva, L Laffel, LP Aiello\u2026 - \u2026 Ophthalmology & Visual \u2026, 2024", "abstract": "Purpose: To evaluate risk factors associated with diabetic retinopathy and neural retinal layer thickness in youth with type 1 diabetes (T1D). Methods: Retrospective cross-sectional chart review was conducted of youth (2-21.9 years old) medical \u2026"}, {"title": "Language Models can be Deductive Solvers", "link": "https://aclanthology.org/2024.findings-naacl.254.pdf", "details": "J Feng, R Xu, J Hao, H Sharma, Y Shen, D Zhao\u2026 - Findings of the Association \u2026, 2024", "abstract": "Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit \u2026"}, {"title": "Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations", "link": "https://arxiv.org/pdf/2406.11801", "details": "R Hazra, S Layek, S Banerjee, S Poria - arXiv preprint arXiv:2406.11801, 2024", "abstract": "Ensuring the safe alignment of large language models (LLMs) with human values is critical as they become integral to applications like translation and question answering. Current alignment methods struggle with dynamic user intentions and \u2026"}, {"title": "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference", "link": "https://arxiv.org/pdf/2406.17626", "details": "E Yu, J Li, M Liao, S Wang, Z Gao, F Mi, L Hong - arXiv preprint arXiv:2406.17626, 2024", "abstract": "As large language models (LLMs) constantly evolve, ensuring their safety remains a critical research problem. Previous red-teaming approaches for LLM safety have primarily focused on single prompt attacks or goal hijacking. To the best of our \u2026"}]
