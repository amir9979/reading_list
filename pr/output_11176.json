[{"title": "Discriminative Image Generation with Diffusion Models for Zero-Shot Learning", "link": "https://arxiv.org/pdf/2412.17219", "details": "D Fu, W Hou, S Chen, S Chen, X You, S Khan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generative Zero-Shot Learning (ZSL) methods synthesize class-related features based on predefined class semantic prototypes, showcasing superior performance. However, this feature generation paradigm falls short of providing interpretable \u2026"}, {"title": "STNMamba: Mamba-based Spatial-Temporal Normality Learning for Video Anomaly Detection", "link": "https://arxiv.org/pdf/2412.20084", "details": "Z Li, M Zhao, X Yang, Y Liu, J Sheng, X Zeng, T Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Video anomaly detection (VAD) has been extensively researched due to its potential for intelligent video systems. However, most existing methods based on CNNs and transformers still suffer from substantial computational burdens and have room for \u2026"}, {"title": "Differentiable Prompt Learning for Vision Language Models", "link": "https://arxiv.org/pdf/2501.00457", "details": "Z Huang, T Pedapati, PY Chen, J Gao - arXiv preprint arXiv:2501.00457, 2024", "abstract": "Prompt learning is an effective way to exploit the potential of large-scale pre-trained foundational models. Continuous prompts parameterize context tokens in prompts by turning them into differentiable vectors. Deep continuous prompts insert prompts not \u2026"}, {"title": "BudgetFusion: Perceptually-Guided Adaptive Diffusion Models", "link": "https://arxiv.org/pdf/2412.05780", "details": "K Chen, Q Sun - arXiv preprint arXiv:2412.05780, 2024", "abstract": "Diffusion models have shown unprecedented success in the task of text-to-image generation. While these models are capable of generating high-quality and realistic images, the complexity of sequential denoising has raised societal concerns \u2026"}, {"title": "LoRACLR: Contrastive Adaptation for Customization of Diffusion Models", "link": "https://arxiv.org/pdf/2412.09622", "details": "E Simsar, T Hofmann, F Tombari, P Yanardag - arXiv preprint arXiv:2412.09622, 2024", "abstract": "Recent advances in text-to-image customization have enabled high-fidelity, context- rich generation of personalized images, allowing specific concepts to appear in a variety of scenarios. However, current methods struggle with combining multiple \u2026"}, {"title": "Enhancing Fine-Tuning Performance of Text-to-Image Diffusion Models for Few-Shot Image Generation Through", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3Dsuw5EQAAQBAJ%26oi%3Dfnd%26pg%3DPA133%26ots%3DNITWM_GbuP%26sig%3DHNslpquuieXWxqYBjjqP7Zjp_WY", "details": "YL Zhu, P Yang - Image and Graphics Technologies and Applications \u2026", "abstract": "Recent significant progress in the field of few-shot image generation has been achieved by fine-tuning pretrained text-to-image mod-els, notably methods such as Dreambooth and Textual Inversion. To enhance the performance of existing \u2026"}, {"title": "Multimodal base distributions in conditional flow matching generative models", "link": "https://bmva-archive.org.uk/bmvc/2024/papers/Paper_492/paper.pdf", "details": "S Josias, W Brink - 2024", "abstract": "Normalising flows are a flexible class of generative models that provide exact likelihoods, and are often trained through maximum likelihood estimation. Recent work suggests that these models can assign undesirably high likelihood to out-of \u2026"}]
