[{"title": "BiasICL: In-Context Learning and Demographic Biases of Vision Language Models", "link": "https://arxiv.org/pdf/2503.02334", "details": "S Xu, J Janizek, Y Jiang, R Daneshjou - arXiv preprint arXiv:2503.02334, 2025", "abstract": "Vision language models (VLMs) show promise in medical diagnosis, but their performance across demographic subgroups when using in-context learning (ICL) remains poorly understood. We examine how the demographic composition of \u2026"}, {"title": "A Tale of Two Classes: Adapting Supervised Contrastive Learning to Binary Imbalanced Datasets", "link": "https://arxiv.org/pdf/2503.17024", "details": "D Mildenberger, P Hager, D Rueckert, MJ Menten - arXiv preprint arXiv:2503.17024, 2025", "abstract": "Supervised contrastive learning (SupCon) has proven to be a powerful alternative to the standard cross-entropy loss for classification of multi-class balanced datasets. However, it struggles to learn well-conditioned representations of datasets with long \u2026"}, {"title": "X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography from Chest Radiography via Tri-Modal Contrastive Learning", "link": "https://arxiv.org/pdf/2503.02162", "details": "J You, Y Gao, S Kim, C Mcintosh - arXiv preprint arXiv:2503.02162, 2025", "abstract": "Computed tomography (CT) is a key imaging modality for diagnosis, yet its clinical utility is marred by high radiation exposure and long turnaround times, restricting its use for larger-scale screening. Although chest radiography (CXR) is more accessible \u2026"}, {"title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models", "link": "https://arxiv.org/pdf/2503.18931", "details": "Y Chen, L Meng, W Peng, Z Wu, YG Jiang - arXiv preprint arXiv:2503.18931, 2025", "abstract": "Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of \u2026"}, {"title": "BPQA Dataset: Evaluating How Well Language Models Leverage Blood Pressures to Answer Biomedical Questions", "link": "https://arxiv.org/pdf/2503.04155", "details": "C Hang, R Deng, LY Jiang, Z Yang, A Alyakin, D Alber\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Clinical measurements such as blood pressures and respiration rates are critical in diagnosing and monitoring patient outcomes. It is an important component of biomedical data, which can be used to train transformer-based language models \u2026"}, {"title": "Few-Shot Whole Slide Pathology Classification with Multi-Granular Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3DnJZtYrOeoV", "details": "AT Nguyen, DMH Nguyen, NT Diep, TQ Nguyen, N Ho\u2026 - \u2026 on Foundation Models in the Wild", "abstract": "In this study, we propose a novel architecture for a large vision-language model adapted with a multi-granular prompt learning method to advance few-shot pathol- ogy classification. Starting with the Prov-GigaPath foundation model-pre-trained on \u2026"}, {"title": "Unsupervised Parameter Efficient Source-free Post-pretraining", "link": "https://arxiv.org/pdf/2502.21313%3F", "details": "A Jha, T Tuytelaars, YM Asano - arXiv preprint arXiv:2502.21313, 2025", "abstract": "Following the success in NLP, the best vision models are now in the billion parameter ranges. Adapting these large models to a target distribution has become computationally and economically prohibitive. Addressing this challenge, we \u2026"}, {"title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems", "link": "https://arxiv.org/pdf/2503.03686", "details": "R Ye, S Tang, R Ge, Y Du, Z Yin, S Chen, J Shao - arXiv preprint arXiv:2503.03686, 2025", "abstract": "LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability \u2026"}, {"title": "Towards a holistic framework for multimodal LLM in 3D brain CT radiology report generation", "link": "https://www.nature.com/articles/s41467-025-57426-0", "details": "CY Li, KJ Chang, CF Yang, HY Wu, W Chen, H Bansal\u2026 - Nature Communications, 2025", "abstract": "Multi-modal large language models (MLLMs) have transformed the landscape of modern healthcare, with automated radiology report generation (RRG) emerging as a cutting-edge application. While 2D MLLM-based RRG has been well established \u2026"}]
