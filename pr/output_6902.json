[{"title": "Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language Models", "link": "https://arxiv.org/pdf/2409.01584", "details": "S Ozaki, K Hayashi, Y Sakai, H Kamigaito, K Hayashi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As the performance of Large-scale Vision Language Models (LVLMs) improves, they are increasingly capable of responding in multiple languages, and there is an expectation that the demand for explanations generated by LVLMs will grow \u2026"}, {"title": "Effective prompt extraction from language models", "link": "https://openreview.net/pdf%3Fid%3D0o95CVdNuz", "details": "Y Zhang, N Carlini, D Ippolito - First Conference on Language Modeling, 2024", "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden \u2026"}, {"title": "Evaluating Attribute Comprehension in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2408.13898", "details": "H Zhang, Z Yang, Y Liu, X Wang, Z He, K Liang, Z Ma - arXiv preprint arXiv \u2026, 2024", "abstract": "Currently, large vision-language models have gained promising progress on many downstream tasks. However, they still suffer many challenges in fine-grained visual understanding tasks, such as object attribute comprehension. Besides, there have \u2026"}, {"title": "Poly-Visual-Expert Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3D7QaEO9WYMa", "details": "X Fan, T Ji, S Li, S Jin, S Song, J Wang, B Hong\u2026 - First Conference on Language \u2026", "abstract": "Current large vision-language models (VLMs) frequently face challenges such as the limited capabilities of a single visual component and the excessive length of visual tokens. These issues can limit the model's ability to interpret complex visual \u2026"}, {"title": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments", "link": "https://arxiv.org/pdf/2408.15903", "details": "R Chen, W Jiang, C Qin, IS Rawal, C Tan, D Choi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid obsolescence of information in Large Language Models (LLMs) has driven the development of various techniques to incorporate new facts. However, existing methods for knowledge editing still face difficulties with multi-hop questions that \u2026"}, {"title": "VTPL: Visual and Text Prompt Learning for visual-language models", "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002360", "details": "B Sun, Z Wu, H Zhang, J He - Journal of Visual Communication and Image \u2026, 2024", "abstract": "Visual-language (VL) models have achieved remarkable success in learning combined visual\u2013textual representations from large web datasets. Prompt learning, as a solution for downstream tasks, can address the forgetting of knowledge \u2026"}, {"title": "How Does Diverse Interpretability of Textual Prompts Impact Medical Vision-Language Zero-Shot Tasks?", "link": "https://arxiv.org/pdf/2409.00543", "details": "S Wang, C Liu, R Arcucci - arXiv preprint arXiv:2409.00543, 2024", "abstract": "Recent advancements in medical vision-language pre-training (MedVLP) have significantly enhanced zero-shot medical vision tasks such as image classification by leveraging large-scale medical image-text pair pre-training. However, the \u2026"}, {"title": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models", "link": "https://arxiv.org/pdf/2408.15915", "details": "Y Yang, Y Qin, T Wu, Z Xu, G Li, P Guo, H Shao, Y Shi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The cultivation of expertise for large language models (LLMs) to solve tasks of specific areas often requires special-purpose tuning with calibrated behaviors on the expected stable outputs. To avoid huge cost brought by manual preparation of \u2026"}, {"title": "Focused Large Language Models are Stable Many-Shot Learners", "link": "https://arxiv.org/pdf/2408.13987", "details": "P Yuan, S Feng, Y Li, X Wang, Y Zhang, C Tan, B Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL \u2026"}]
