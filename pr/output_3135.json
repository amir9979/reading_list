[{"title": "$\\texttt {MoE-RBench} $: Towards Building Reliable Language Models with Sparse Mixture-of-Experts", "link": "https://arxiv.org/pdf/2406.11353", "details": "G Chen, X Zhao, T Chen, Y Cheng - arXiv preprint arXiv:2406.11353, 2024", "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new \u2026"}, {"title": "Word Embeddings Are Steers for Language Models", "link": "https://blender.cs.illinois.edu/paper/lmsteer2024.pdf", "details": "C Han, J Xu, M Li, Y Fung, C Sun, N Jiang\u2026", "abstract": "Abstract Language models (LMs) automatically learn word embeddings during pre- training on language corpora. Although word embeddings are usually interpreted as feature vectors for individual words, their roles in language model generation remain \u2026"}, {"title": "Language Models can be Deductive Solvers", "link": "https://aclanthology.org/2024.findings-naacl.254.pdf", "details": "J Feng, R Xu, J Hao, H Sharma, Y Shen, D Zhao\u2026 - Findings of the Association \u2026, 2024", "abstract": "Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit \u2026"}, {"title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space", "link": "https://arxiv.org/pdf/2406.09325", "details": "T Ashuach, M Tutek, Y Belinkov - arXiv preprint arXiv:2406.09325, 2024", "abstract": "Large language models (LLMs) risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in training data, causing privacy concerns. Current approaches to address this issue involve costly dataset \u2026"}, {"title": "Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain", "link": "https://arxiv.org/pdf/2406.06435", "details": "B Hu, B Ray, A Leung, A Summerville, D Joy, C Funk\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In difficult decision-making scenarios, it is common to have conflicting opinions among expert human decision-makers as there may not be a single right answer. Such decisions may be guided by different attributes that can be used to characterize \u2026"}, {"title": "Advancing High Resolution Vision-Language Models in Biomedicine", "link": "https://arxiv.org/pdf/2406.09454", "details": "Z Chen, A Pekis, K Brown - arXiv preprint arXiv:2406.09454, 2024", "abstract": "Multi-modal learning has significantly advanced generative AI, especially in vision- language modeling. Innovations like GPT-4V and open-source projects such as LLaVA have enabled robust conversational agents capable of zero-shot task \u2026"}, {"title": "PharmGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry", "link": "https://arxiv.org/pdf/2406.18045", "details": "L Chen, W Wang, Z Bai, P Xu, Y Fang, J Fang, W Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have revolutionized Natural Language Processing (NLP) by by minimizing the need for complex feature engineering. However, the application of LLMs in specialized domains like biopharmaceuticals and chemistry \u2026"}, {"title": "Language Models are Crossword Solvers", "link": "https://arxiv.org/pdf/2406.09043", "details": "S Saha, S Chakraborty, S Saha, U Garain - arXiv preprint arXiv:2406.09043, 2024", "abstract": "Crosswords are a form of word puzzle that require a solver to demonstrate a high degree of proficiency in natural language understanding, wordplay, reasoning, and world knowledge, along with adherence to character and length constraints. In this \u2026"}, {"title": "AutoCAP: Towards Automatic Cross-lingual Alignment Planning for Zero-shot Chain-of-Thought", "link": "https://arxiv.org/pdf/2406.13940", "details": "Y Zhang, Q Chen, M Li, W Che, L Qin - arXiv preprint arXiv:2406.13940, 2024", "abstract": "Cross-lingual chain-of-thought can effectively complete reasoning tasks across languages, which gains increasing attention. Recently, dominant approaches in the literature improve cross-lingual alignment capabilities by integrating reasoning \u2026"}]
