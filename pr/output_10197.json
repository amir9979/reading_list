[{"title": "VLM-Social-Nav: Socially Aware Robot Navigation through Scoring using Vision-Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10777573/", "details": "D Song, J Liang, A Payandeh, AH Raj, X Xiao\u2026 - IEEE Robotics and \u2026, 2024", "abstract": "We propose VLM-Social-Nav, a novel Vision-Language Model (VLM) based navigation approach to compute a robot's motion in human-centered environments. Our goal is to make real-time decisions on robot actions that are socially compliant \u2026"}, {"title": "Do Large Language Models have Shared Weaknesses in Medical Question Answering?", "link": "https://openreview.net/pdf%3Fid%3DZjQ04tsRQl", "details": "AM Bean, K Korgul, F Krones, R McCraith, A Mahdi - Advancements In Medical \u2026, 2024", "abstract": "Large language models (LLMs) have made rapid improvement on medical benchmarks, but their unreliability remains a persistent challenge for safe real-world uses. To design for the use LLMs as a category, rather than for specific models \u2026"}, {"title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information", "link": "https://arxiv.org/pdf/2412.00947", "details": "R Kamoi, Y Zhang, SSS Das, RH Zhang, R Zhang - arXiv preprint arXiv:2412.00947, 2024", "abstract": "Errors in understanding visual information in images (ie, visual perception errors) remain a major source of mistakes in Large Vision Language Models (LVLMs). While further analysis is essential, there is a deficiency in datasets for evaluating the visual \u2026"}, {"title": "ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models", "link": "https://arxiv.org/pdf/2412.07012", "details": "J Zhang, L Xue, L Song, J Wang, W Huang, M Shu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the rise of multimodal applications, instruction data has become critical for training multimodal language models capable of understanding complex image- based queries. Existing practices rely on powerful but costly large language models \u2026"}, {"title": "PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.09613", "details": "C Yang, X Dong, X Zhu, W Su, J Wang, H Tian, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (VLMs) have been extended to understand both images and videos. Visual token compression is leveraged to reduce the considerable token length of visual inputs. To meet the needs of different tasks \u2026"}, {"title": "V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding", "link": "https://arxiv.org/pdf/2412.09616", "details": "J Ge, Z Chen, J Lin, J Zhu, X Liu, J Dai, X Zhu - arXiv preprint arXiv:2412.09616, 2024", "abstract": "Vision-Language Models (VLMs) have shown promising capabilities in handling various multimodal tasks, yet they struggle in long-context scenarios, particularly in tasks involving videos, high-resolution images, or lengthy image-text documents. In \u2026"}, {"title": "ITA-Bench: Towards a More Comprehensive Evaluation for Italian LLMs", "link": "https://clic2024.ilc.cnr.it/wp-content/uploads/2024/12/66_main_long.pdf", "details": "L Moroni, S Conia, F Martelli, R Navigli - \u2026 of the Tenth Italian Conference on \u2026, 2024", "abstract": "Abstract Recent Large Language Models (LLMs) have shown impressive performance in addressing complex aspects of human language. These models have also demonstrated significant capabilities in processing and generating Italian \u2026"}, {"title": "DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through Diverse Perspectives and Multi-Agent Interaction", "link": "https://arxiv.org/pdf/2412.09572", "details": "Y Feng, PM Htut, Z Qi, W Xiao, M Mager, N Pappas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Quantifying the uncertainty in the factual parametric knowledge of Large Language Models (LLMs), especially in a black-box setting, poses a significant challenge. Existing methods, which gauge a model's uncertainty through evaluating self \u2026"}, {"title": "VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models", "link": "https://arxiv.org/pdf/2412.01822", "details": "BK Lee, R Hachiuma, YCF Wang, YM Ro, YH Wu - arXiv preprint arXiv:2412.01822, 2024", "abstract": "The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve \u2026"}]
