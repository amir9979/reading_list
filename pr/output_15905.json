[{"title": "Enhancing Multimodal In-Context Learning for Image Classification through Coreset Optimization", "link": "https://arxiv.org/pdf/2504.14200", "details": "H Chen, J Peng, K Tang, X Geng, X Yang - arXiv preprint arXiv:2504.14200, 2025", "abstract": "In-context learning (ICL) enables Large Vision-Language Models (LVLMs) to adapt to new tasks without parameter updates, using a few demonstrations from a large support set. However, selecting informative demonstrations leads to high \u2026"}, {"title": "Window Token Concatenation for Efficient Visual Large Language Models", "link": "https://arxiv.org/pdf/2504.04024", "details": "Y Li, W Bao, B Ye, Z Tan, T Chen, H Liu, Y Kong - arXiv preprint arXiv:2504.04024, 2025", "abstract": "To effectively reduce the visual tokens in Visual Large Language Models (VLLMs), we propose a novel approach called Window Token Concatenation (WiCo). Specifically, we employ a sliding window to concatenate spatially adjacent visual \u2026"}, {"title": "Foundation models for electronic health records: representation dynamics and transferability", "link": "https://arxiv.org/pdf/2504.10422", "details": "MC Burkhart, B Ramadan, Z Liao, K Chhikara\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Foundation models (FMs) trained on electronic health records (EHRs) have shown strong performance on a range of clinical prediction tasks. However, adapting these models to local health systems remains challenging due to limited data availability \u2026"}, {"title": "Unitoken: Harmonizing multimodal understanding and generation through unified visual encoding", "link": "https://arxiv.org/pdf/2504.04423", "details": "Y Jiao, H Qiu, Z Jie, S Chen, J Chen, L Ma, YG Jiang - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce UniToken, an auto-regressive generation model that encodes visual inputs through a combination of discrete and continuous representations, enabling seamless integration of unified visual understanding and image generation tasks \u2026"}, {"title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "link": "https://arxiv.org/pdf/2504.10479%3F", "details": "J Zhu, W Wang, Z Chen, Z Liu, S Ye, L Gu, Y Duan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that \u2026"}, {"title": "BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text", "link": "https://arxiv.org/pdf/2504.19467", "details": "J Wu, B Gu, R Zhou, K Xie, D Snyder, Y Jiang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) hold great promise for medical applications and are evolving rapidly, with new models being released at an accelerated pace. However, current evaluations of LLMs in clinical contexts remain limited. Most existing \u2026"}]
