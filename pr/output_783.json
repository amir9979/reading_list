'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [ConvBench: A Multi-Turn Conversation Evaluation Benchm'
[{"title": "Beyond Simple Text Style Transfer: Unveiling Compound Text Style Transfer with Prompt-Based Pre-Trained Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10447801/", "details": "S Ju, C Wang - ICASSP 2024-2024 IEEE International Conference on \u2026, 2024", "abstract": "Compound text style transfer is an innovative task that seeks to merge textual elements from distinct styles, themes, or attributes to create diverse and distinctive textual content. This technique plays an important role in various fields, such as \u2026"}, {"title": "Few-shot Named Entity Recognition via Superposition Concept Discrimination", "link": "https://arxiv.org/pdf/2403.16463", "details": "J Chen, H Lin, X Han, Y Lu, S Jiang, B Dong, L Sun - arXiv preprint arXiv:2403.16463, 2024", "abstract": "Few-shot NER aims to identify entities of target types with only limited number of illustrative instances. Unfortunately, few-shot NER is severely challenged by the intrinsic precise generalization problem, ie, it is hard to accurately determine the \u2026"}, {"title": "Using Clustering to Improve the Performance of few-shot Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10447442/", "details": "Y Zhang, C Wu, R Shi, Y Zhang - ICASSP 2024-2024 IEEE International Conference \u2026, 2024", "abstract": "With the rise of pre-trained language models, few-shot learning has experienced significant progress in terms of performance. Nevertheless, there remains considerable scope for improvement. The objective of this study is to improve the \u2026"}, {"title": "MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering", "link": "https://arxiv.org/pdf/2403.19116", "details": "C Guan, M Huang, P Zhang - arXiv preprint arXiv:2403.19116, 2024", "abstract": "In today's fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested \u2026"}, {"title": "MING-MOE: Enhancing Medical Multi-Task Learning in Large Language Models with Sparse Mixture of Low-Rank Adapter Experts", "link": "https://arxiv.org/pdf/2404.09027", "details": "Y Liao, S Jiang, Y Wang, Y Wang - arXiv preprint arXiv:2404.09027, 2024", "abstract": "Large language models like ChatGPT have shown substantial progress in natural language understanding and generation, proving valuable across various disciplines, including the medical field. Despite advancements, challenges persist \u2026"}, {"title": "Evaluation of large language models performance against humans for summarizing MRI knee radiology reports: A feasibility study", "link": "https://www.sciencedirect.com/science/article/pii/S1386505624001060", "details": "P L\u00f3pez-\u00dabeda, T Mart\u00edn-Noguerol, C D\u00edaz-Angulo\u2026 - International Journal of \u2026, 2024", "abstract": "Objectives This study addresses the critical need for accurate summarization in radiology by comparing various Large Language Model (LLM)-based approaches for automatic summary generation. With the increasing volume of patient information \u2026"}, {"title": "Self-Training Large Language Models for Improved Visual Program Synthesis With Visual Reinforcement", "link": "https://arxiv.org/pdf/2404.04627", "details": "Z Khan, VK BG, S Schulter, Y Fu, M Chandraker - arXiv preprint arXiv:2404.04627, 2024", "abstract": "Visual program synthesis is a promising approach to exploit the reasoning abilities of large language models for compositional computer vision tasks. Previous work has used few-shot prompting with frozen LLMs to synthesize visual programs. Training \u2026"}]
