[{"title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation", "link": "https://arxiv.org/pdf/2505.11754", "details": "W Huang, P Vougiouklis, M Lapata, JZ Pan - arXiv preprint arXiv:2505.11754, 2025", "abstract": "Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant \u2026", "entry_id": "http://arxiv.org/abs/2505.11754v1", "updated": "2025-05-16 23:29:47", "published": "2025-05-16 23:29:47", "authors": "Wenyu Huang;Pavlos Vougiouklis;Mirella Lapata;Jeff Z. Pan", "summary": "Multi-hop Question Answering (MHQA) adds layers of complexity to question\nanswering, making it more challenging. When Language Models (LMs) are prompted\nwith multiple search results, they are tasked not only with retrieving relevant\ninformation but also employing multi-hop reasoning across the information\nsources. Although LMs perform well on traditional question-answering tasks, the\ncausal mask can hinder their capacity to reason across complex contexts. In\nthis paper, we explore how LMs respond to multi-hop questions by permuting\nsearch results (retrieved documents) under various configurations. Our study\nreveals interesting findings as follows: 1) Encoder-decoder models, such as the\nones in the Flan-T5 family, generally outperform causal decoder-only LMs in\nMHQA tasks, despite being significantly smaller in size; 2) altering the order\nof gold documents reveals distinct trends in both Flan T5 models and fine-tuned\ndecoder-only models, with optimal performance observed when the document order\naligns with the reasoning chain order; 3) enhancing causal decoder-only models\nwith bi-directional attention by modifying the causal mask can effectively\nboost their end performance. In addition to the above, we conduct a thorough\ninvestigation of the distribution of LM attention weights in the context of\nMHQA. Our experiments reveal that attention weights tend to peak at higher\nvalues when the resulting answer is correct. We leverage this finding to\nheuristically improve LMs' performance on this task. Our code is publicly\navailable at https://github.com/hwy9855/MultiHopQA-Reasoning.", "comment": "ACL 2025 main", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.11754v1;http://arxiv.org/pdf/2505.11754v1", "pdf_url": "http://arxiv.org/pdf/2505.11754v1"}, {"title": "LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades", "link": "https://arxiv.org/pdf/2505.13515", "details": "Y Li, F Meng, M Zhang, S Zhu, S Wang, M Xu - arXiv preprint arXiv:2505.13515, 2025", "abstract": "As Large Language Models (LLMs) are frequently updated, LoRA weights trained on earlier versions quickly become obsolete. The conventional practice of retraining LoRA weights from scratch on the latest model is costly, time-consuming, and \u2026", "entry_id": "http://arxiv.org/abs/2505.13515v1", "updated": "2025-05-17 04:11:17", "published": "2025-05-17 04:11:17", "authors": "Yanan Li;Fanxu Meng;Muhan Zhang;Shiai Zhu;Shangguang Wang;Mengwei Xu", "summary": "As Large Language Models (LLMs) are frequently updated, LoRA weights trained\non earlier versions quickly become obsolete. The conventional practice of\nretraining LoRA weights from scratch on the latest model is costly,\ntime-consuming, and environmentally detrimental, particularly as the diversity\nof LLMs and downstream tasks expands. This motivates a critical question: \"How\ncan we efficiently leverage existing LoRA weights to adapt to newer model\nversions?\" To address this, we propose LoRASuite, a modular approach tailored\nspecifically to various types of LLM updates. First, we compute a transfer\nmatrix utilizing known parameters from both old and new LLMs. Next, we allocate\ncorresponding layers and attention heads based on centered kernel alignment and\ncosine similarity metrics, respectively. A subsequent small-scale, skillful\nfine-tuning step ensures numerical stability. Experimental evaluations\ndemonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA\nmethods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even\nexceeds the performance of full-scale LoRA retraining, with average\nimprovements of +1.4 and +6.6 points on math tasks, respectively. Additionally,\nLoRASuite significantly reduces memory consumption by 5.5 GB and computational\ntime by 78.23%.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CL", "links": "http://arxiv.org/abs/2505.13515v1;http://arxiv.org/pdf/2505.13515v1", "pdf_url": "http://arxiv.org/pdf/2505.13515v1"}, {"title": "AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation", "link": "https://arxiv.org/pdf/2505.11887", "details": "X Zhang, Z Ouyang, L Wang, G de Melo, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the proliferation of large language models (LLMs) in the medical domain, there is increasing demand for improved evaluation techniques to assess their capabilities. However, traditional metrics like F1 and ROUGE, which rely on token overlaps to \u2026", "entry_id": "http://arxiv.org/abs/2505.11887v1", "updated": "2025-05-17 07:44:54", "published": "2025-05-17 07:44:54", "authors": "Xiechi Zhang;Zetian Ouyang;Linlin Wang;Gerard de Melo;Zhu Cao;Xiaoling Wang;Ya Zhang;Yanfeng Wang;Liang He", "summary": "With the proliferation of large language models (LLMs) in the medical domain,\nthere is increasing demand for improved evaluation techniques to assess their\ncapabilities. However, traditional metrics like F1 and ROUGE, which rely on\ntoken overlaps to measure quality, significantly overlook the importance of\nmedical terminology. While human evaluation tends to be more reliable, it can\nbe very costly and may as well suffer from inaccuracies due to limits in human\nexpertise and motivation. Although there are some evaluation methods based on\nLLMs, their usability in the medical field is limited due to their proprietary\nnature or lack of expertise. To tackle these challenges, we present\nAutoMedEval, an open-sourced automatic evaluation model with 13B parameters\nspecifically engineered to measure the question-answering proficiency of\nmedical LLMs. The overarching objective of AutoMedEval is to assess the quality\nof responses produced by diverse models, aspiring to significantly reduce the\ndependence on human evaluation. Specifically, we propose a hierarchical\ntraining method involving curriculum instruction tuning and an iterative\nknowledge introspection mechanism, enabling AutoMedEval to acquire professional\nmedical assessment capabilities with limited instructional data. Human\nevaluations indicate that AutoMedEval surpasses other baselines in terms of\ncorrelation with human judgments.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.11887v1;http://arxiv.org/pdf/2505.11887v1", "pdf_url": "http://arxiv.org/pdf/2505.11887v1"}, {"title": "Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models", "link": "https://arxiv.org/pdf/2505.11271", "details": "C Couturier, S Mastorakis, H Shen, S Rajmohan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) are increasingly deployed across edge and cloud platforms for real-time question-answering and retrieval-augmented generation. However, processing lengthy contexts in distributed systems incurs high \u2026", "entry_id": "http://arxiv.org/abs/2505.11271v1", "updated": "2025-05-16 14:04:31", "published": "2025-05-16 14:04:31", "authors": "Camille Couturier;Spyros Mastorakis;Haiying Shen;Saravan Rajmohan;Victor R\u00fchle", "summary": "Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants.", "comment": "Preprint. Paper accepted at ICCCN 2025, the final version will appear\n  in the proceedings", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.IR;cs.LG;I.2.7", "links": "http://arxiv.org/abs/2505.11271v1;http://arxiv.org/pdf/2505.11271v1", "pdf_url": "http://arxiv.org/pdf/2505.11271v1"}, {"title": "CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models", "link": "https://arxiv.org/pdf/2505.12504", "details": "Z Liu, F Meng, L Du, Z Zhou, C Yu, W Shao, Q Zhang - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methods--such as GRPO, REINFORCE++, and RLOO \u2026", "entry_id": "http://arxiv.org/abs/2505.12504v1", "updated": "2025-05-18 17:44:53", "published": "2025-05-18 17:44:53", "authors": "Zongkai Liu;Fanqing Meng;Lingxiao Du;Zhixiang Zhou;Chao Yu;Wenqi Shao;Qiaosheng Zhang", "summary": "Recent advances in rule-based reinforcement learning (RL) have significantly\nimproved the reasoning capability of language models (LMs) with rule-based\nrewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO --\noften suffer from training instability, where large policy updates and improper\nclipping can lead to training collapse. To address this issue, we propose\nClipped Policy Gradient Optimization with Policy Drift (CPGD), a novel\nalgorithm designed to stabilize policy learning in LMs. CPGD introduces a\npolicy drift constraint based on KL divergence to dynamically regularize policy\nupdates, and leverages a clip mechanism on the logarithm of the ratio to\nprevent excessive policy updates. We provide theoretical justification for CPGD\nand demonstrate through empirical analysis that it mitigates the instability\nobserved in prior approaches. Furthermore, we show that CPGD significantly\nimproves performance while maintaining training stability. Our implementation\nbalances theoretical rigor with practical usability, offering a robust\nalternative for RL in the post-training of LMs. We release our code at\nhttps://github.com/ModalMinds/MM-EUREKA.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.12504v1;http://arxiv.org/pdf/2505.12504v1", "pdf_url": "http://arxiv.org/pdf/2505.12504v1"}, {"title": "I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models", "link": "https://arxiv.org/pdf/2505.13302", "details": "A Plebe, T Douglas, D Riazi, RM del Rio-Chanona - arXiv preprint arXiv:2505.13302, 2025", "abstract": "Large language models are increasingly integrated into news recommendation systems, raising concerns about their role in spreading misinformation. In humans, visual content is known to boost credibility and shareability of information, yet its \u2026", "entry_id": "http://arxiv.org/abs/2505.13302v1", "updated": "2025-05-19 16:20:54", "published": "2025-05-19 16:20:54", "authors": "Alice Plebe;Timothy Douglas;Diana Riazi;R. Maria del Rio-Chanona", "summary": "Large language models are increasingly integrated into news recommendation\nsystems, raising concerns about their role in spreading misinformation. In\nhumans, visual content is known to boost credibility and shareability of\ninformation, yet its effect on vision-language models (VLMs) remains unclear.\nWe present the first study examining how images influence VLMs' propensity to\nreshare news content, whether this effect varies across model families, and how\npersona conditioning and content attributes modulate this behavior. To support\nthis analysis, we introduce two methodological contributions: a\njailbreaking-inspired prompting strategy that elicits resharing decisions from\nVLMs while simulating users with antisocial traits and political alignments;\nand a multimodal dataset of fact-checked political news from PolitiFact, paired\nwith corresponding images and ground-truth veracity labels. Experiments across\nmodel families reveal that image presence increases resharing rates by 4.8% for\ntrue news and 15.0% for false news. Persona conditioning further modulates this\neffect: Dark Triad traits amplify resharing of false news, whereas\nRepublican-aligned profiles exhibit reduced veracity sensitivity. Of all the\ntested models, only Claude-3-Haiku demonstrates robustness to visual\nmisinformation. These findings highlight emerging risks in multimodal model\nbehavior and motivate the development of tailored evaluation frameworks and\nmitigation strategies for personalized AI systems. Code and dataset are\navailable at: https://github.com/3lis/misinfo_vlm", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.13302v1;http://arxiv.org/pdf/2505.13302v1", "pdf_url": "http://arxiv.org/pdf/2505.13302v1"}, {"title": "Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning", "link": "https://arxiv.org/pdf/2505.14471", "details": "T Li, J Wang, Y Zhang, S Li, L Chen - arXiv preprint arXiv:2505.14471, 2025", "abstract": "Citation classification, which identifies the intention behind academic citations, is pivotal for scholarly analysis. Previous works suggest fine-tuning pretrained language models (PLMs) on citation classification datasets, reaping the reward of the \u2026", "entry_id": "http://arxiv.org/abs/2505.14471v1", "updated": "2025-05-20 15:05:27", "published": "2025-05-20 15:05:27", "authors": "Tong Li;Jiachuan Wang;Yongqi Zhang;Shuangyin Li;Lei Chen", "summary": "Citation classification, which identifies the intention behind academic\ncitations, is pivotal for scholarly analysis. Previous works suggest\nfine-tuning pretrained language models (PLMs) on citation classification\ndatasets, reaping the reward of the linguistic knowledge they gained during\npretraining. However, directly fine-tuning for citation classification is\nchallenging due to labeled data scarcity, contextual noise, and spurious\nkeyphrase correlations. In this paper, we present a novel framework, Citss,\nthat adapts the PLMs to overcome these challenges. Citss introduces\nself-supervised contrastive learning to alleviate data scarcity, and is\nequipped with two specialized strategies to obtain the contrastive pairs:\nsentence-level cropping, which enhances focus on target citations within long\ncontexts, and keyphrase perturbation, which mitigates reliance on specific\nkeyphrases. Compared with previous works that are only designed for\nencoder-based PLMs, Citss is carefully developed to be compatible with both\nencoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged\npretraining. Experiments with three benchmark datasets with both encoder-based\nPLMs and decoder-based LLMs demonstrate our superiority compared to the\nprevious state of the art. Our code is available at: github.com/LITONG99/Citss", "comment": "Manuscripts, accepted to KDD 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.14471v1;http://arxiv.org/pdf/2505.14471v1", "pdf_url": "http://arxiv.org/pdf/2505.14471v1"}, {"title": "Understanding Cross-Lingual Inconsistency in Large Language Models", "link": "https://arxiv.org/pdf/2505.13141", "details": "ZW Lim, AF Aji, T Cohn - arXiv preprint arXiv:2505.13141, 2025", "abstract": "Large language models (LLMs) are demonstrably capable of cross-lingual transfer, but can produce inconsistent output when prompted with the same queries written in different languages. To understand how language models are able to generalize \u2026", "entry_id": "http://arxiv.org/abs/2505.13141v1", "updated": "2025-05-19 14:10:15", "published": "2025-05-19 14:10:15", "authors": "Zheng Wei Lim;Alham Fikri Aji;Trevor Cohn", "summary": "Large language models (LLMs) are demonstrably capable of cross-lingual\ntransfer, but can produce inconsistent output when prompted with the same\nqueries written in different languages. To understand how language models are\nable to generalize knowledge from one language to the others, we apply the\nlogit lens to interpret the implicit steps taken by LLMs to solve multilingual\nmulti-choice reasoning questions. We find LLMs predict inconsistently and are\nless accurate because they rely on subspaces of individual languages, rather\nthan working in a shared semantic space. While larger models are more\nmultilingual, we show their hidden states are more likely to dissociate from\nthe shared representation compared to smaller models, but are nevertheless more\ncapable of retrieving knowledge embedded across different languages. Finally,\nwe demonstrate that knowledge sharing can be modulated by steering the models'\nlatent processing towards the shared semantic space. We find reinforcing\nutilization of the shared space improves the models' multilingual reasoning\nperformance, as a result of more knowledge transfer from, and better output\nconsistency with English.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.13141v1;http://arxiv.org/pdf/2505.13141v1", "pdf_url": "http://arxiv.org/pdf/2505.13141v1"}, {"title": "The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models", "link": "https://arxiv.org/pdf/2505.12287", "details": "L Huang, H Jin, Z Bi, P Yang, P Zhao, T Chen, X Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have seen widespread applications across various domains, yet remain vulnerable to adversarial prompt injections. While most existing research on jailbreak attacks and hallucination phenomena has focused primarily on \u2026", "entry_id": "http://arxiv.org/abs/2505.12287v1", "updated": "2025-05-18 07:51:19", "published": "2025-05-18 07:51:19", "authors": "Linghan Huang;Haolin Jin;Zhaoge Bi;Pengyue Yang;Peizhou Zhao;Taozhao Chen;Xiongfei Wu;Lei Ma;Huaming Chen", "summary": "Large language models (LLMs) have seen widespread applications across various\ndomains, yet remain vulnerable to adversarial prompt injections. While most\nexisting research on jailbreak attacks and hallucination phenomena has focused\nprimarily on open-source models, we investigate the frontier of closed-source\nLLMs under multilingual attack scenarios. We present a first-of-its-kind\nintegrated adversarial framework that leverages diverse attack techniques to\nsystematically evaluate frontier proprietary solutions, including GPT-4o,\nDeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories\nof security contents in both English and Chinese, generating 38,400 responses\nacross 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as\nthe quantitative metric to assess performance from three dimensions: prompt\ndesign, model architecture, and language environment. Our findings suggest that\nQwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.\nNotably, prompts in Chinese consistently yield higher ASRs than their English\ncounterparts, and our novel Two-Sides attack technique proves to be the most\neffective across all models. This work highlights a dire need for\nlanguage-aware alignment and robust cross-lingual defenses in LLMs, and we hope\nit will inspire researchers, developers, and policymakers toward more robust\nand inclusive AI systems.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.12287v1;http://arxiv.org/pdf/2505.12287v1", "pdf_url": "http://arxiv.org/pdf/2505.12287v1"}]
