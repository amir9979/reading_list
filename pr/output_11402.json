[{"title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules", "link": "https://arxiv.org/pdf/2412.13536", "details": "K Chen, L Wang, Q Zhang, R Xu - arXiv preprint arXiv:2412.13536, 2024", "abstract": "Recent studies have highlighted the limitations of large language models in mathematical reasoning, particularly their inability to capture the underlying logic. Inspired by meta-learning, we propose that models should acquire not only task \u2026"}, {"title": "Adaptive Few-shot Prompting for Machine Translation with Pre-trained Language Models", "link": "https://arxiv.org/pdf/2501.01679", "details": "L Tang, J Qin, W Ye, H Tan, Z Yang - arXiv preprint arXiv:2501.01679, 2025", "abstract": "Recently, Large language models (LLMs) with in-context learning have demonstrated remarkable potential in handling neural machine translation. However, existing evidence shows that LLMs are prompt-sensitive and it is sub-optimal to \u2026"}, {"title": "SMIR: Efficient Synthetic Data Pipeline To Improve Multi-Image Reasoning", "link": "https://arxiv.org/pdf/2501.03675", "details": "A Li, R Thapa, R Chalamala, Q Wu, K Chen, J Zou - arXiv preprint arXiv:2501.03675, 2025", "abstract": "Vision-Language Models (VLMs) have shown strong performance in understanding single images, aided by numerous high-quality instruction datasets. However, multi- image reasoning tasks are still under-explored in the open-source community due to \u2026"}, {"title": "Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces", "link": "https://arxiv.org/pdf/2412.14171%3F", "details": "J Yang, S Yang, AW Gupta, R Han, L Fei-Fei, S Xie - arXiv preprint arXiv:2412.14171, 2024", "abstract": "Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also``think in space''from videos? We present \u2026"}, {"title": "Smoothed Embeddings for Robust Language Models", "link": "https://merl.com/publications/docs/TR2024-170.pdf", "details": "H Ryo, MRU Rashid, A Lewis, J Liu, T Koike-Akino\u2026", "abstract": "Improving the safety and reliability of large language models (LLMs) is a crucial aspect of realizing trustworthy AI systems. Although alignment methods aim to suppress harmful content generation, LLMs are often still vulnerable to jail-breaking \u2026"}, {"title": "Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models", "link": "https://arxiv.org/pdf/2412.15287", "details": "Y Chow, G Tennenholtz, I Gur, V Zhuang, B Dai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent studies have indicated that effectively utilizing inference-time compute is crucial for attaining better performance from large language models (LLMs). In this work, we propose a novel inference-aware fine-tuning paradigm, in which the model \u2026"}, {"title": "Aligning Crowd-Sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10818581/", "details": "MF Wong, CW Tan - IEEE Transactions on Big Data, 2024", "abstract": "This paper studies how AI-assisted programming and large language models (LLM) improve software developers' ability via AI tools (LLM agents) like Github Copilot and Amazon CodeWhisperer, while integrating human feedback to enhance \u2026"}, {"title": "Dynamic Skill Adaptation for Large Language Models", "link": "https://arxiv.org/pdf/2412.19361%3F", "details": "J Chen, D Yang - arXiv preprint arXiv:2412.19361, 2024", "abstract": "We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework to adapt novel and complex skills to Large Language Models (LLMs). Compared with previous work which learns from human-curated and static data in random orders \u2026"}, {"title": "How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models", "link": "https://arxiv.org/pdf/2501.01741", "details": "S Corbo, L Bancale, V De Gennaro, L Lestingi, V Scotti\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Language is a deep-rooted means of perpetration of stereotypes and discrimination. Large Language Models (LLMs), now a pervasive technology in our everyday lives, can cause extensive harm when prone to generating toxic responses. The standard \u2026"}]
