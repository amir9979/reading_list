[{"title": "Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference", "link": "https://arxiv.org/pdf/2408.01935", "details": "K Shen, M Kejriwal - arXiv preprint arXiv:2408.01935, 2024", "abstract": "Despite their impressive performance, large language models (LLMs) such as ChatGPT are known to pose important risks. One such set of risks arises from misplaced confidence, whether over-confidence or under-confidence, that the \u2026"}, {"title": "Towards inclusive biodesign and innovation: lowering barriers to entry in medical device development through large language model tools", "link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11268064/", "details": "JT Moon, NJ Lima, E Froula, H Li, J Newsome\u2026 - BMJ Health & Care \u2026, 2024", "abstract": "In the following narrative review, we discuss the potential role of large language models (LLMs) in medical device innovation, specifically examples using generative pretrained transformer-4. Throughout the biodesign process, LLMs can offer prompt \u2026"}, {"title": "Citekit: A Modular Toolkit for Large Language Model Citation Generation", "link": "https://arxiv.org/pdf/2408.04662", "details": "J Shen, T Zhou, S Zhao, Y Chen, K Liu - arXiv preprint arXiv:2408.04662, 2024", "abstract": "Enabling Large Language Models (LLMs) to generate citations in Question- Answering (QA) tasks is an emerging paradigm aimed at enhancing the verifiability of their responses when LLMs are utilizing external references to generate an \u2026"}, {"title": "Towards Effective and Efficient Continual Pre-training of Large Language Models", "link": "https://arxiv.org/pdf/2407.18743", "details": "J Chen, Z Chen, J Wang, K Zhou, Y Zhu, J Jiang, Y Min\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Continual pre-training (CPT) has been an important approach for adapting language models to specific domains or tasks. To make the CPT approach more traceable, this paper presents a technical report for continually pre-training Llama-3 (8B), which \u2026"}]
