[{"title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?", "link": "https://arxiv.org/pdf/2410.23856", "details": "Z Zhou, R Tao, J Zhu, Y Luo, Z Wang, B Han - arXiv preprint arXiv:2410.23856, 2024", "abstract": "This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We \u2026"}, {"title": "Shareable artificial intelligence to extract cancer outcomes from electronic health records for precision oncology research", "link": "https://www.nature.com/articles/s41467-024-54071-x", "details": "KL Kehl, J Jee, K Pichotta, MA Paul, P Trukhanov\u2026 - Nature Communications, 2024", "abstract": "Databases that link molecular data to clinical outcomes can inform precision cancer research into novel prognostic and predictive biomarkers. However, outside of clinical trials, cancer outcomes are typically recorded only in text form within \u2026"}, {"title": "Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings", "link": "https://arxiv.org/pdf/2410.22685", "details": "YS Grewal, EV Bonilla, TD Bui - arXiv preprint arXiv:2410.22685, 2024", "abstract": "Accurately quantifying uncertainty in large language models (LLMs) is crucial for their reliable deployment, especially in high-stakes applications. Current state-of-the- art methods for measuring semantic uncertainty in LLMs rely on strict bidirectional \u2026"}, {"title": "Darwin: A DRAM-Based Multi-Level Processing-in-Memory Architecture for Column-Oriented Database", "link": "https://ieeexplore.ieee.org/abstract/document/10752898/", "details": "D Kim, JY Kim, W Han, J Won, H Choi, Y Kwon, JY Kim - IEEE Transactions on \u2026, 2024", "abstract": "We propose Darwin, a practical LRDIMM-based multi-level Processing-in-memory (PIM) architecture for data analytics, which exploits the internal bandwidth of DRAM using the bank-, bank group-, chip-, and rank-level parallelisms. Considering the \u2026"}, {"title": "Energy-Based Diffusion Language Models for Text Generation", "link": "https://arxiv.org/pdf/2410.21357", "details": "M Xu, T Geffner, K Kreis, W Nie, Y Xu, J Leskovec\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite remarkable progress in autoregressive language models, alternative generative paradigms beyond left-to-right generation are still being actively explored. Discrete diffusion models, with the capacity for parallel generation, have recently \u2026"}, {"title": "Mixed Distillation Helps Smaller Language Models Reason Better", "link": "https://aclanthology.org/2024.findings-emnlp.91.pdf", "details": "L Chenglin, Q Chen, L Li, C Wang, F Tao, Y Li, Z Chen\u2026 - Findings of the Association \u2026, 2024", "abstract": "As large language models (LLMs) have demonstrated impressive multiple step-by- step reasoning capabilities in recent natural language processing (NLP) reasoning tasks, many studies are interested in distilling reasoning abilities into smaller \u2026"}, {"title": "Mathematical Reasoning via Multi-step Self Questioning and Answering for Small Language Models", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9440-9_7", "details": "K Chen, J Wang, X Zhang - CCF International Conference on Natural Language \u2026, 2024", "abstract": "Mathematical reasoning is challenging for large language models (LLMs), while the scaling relationship concerning LLM capacity is under-explored. Existing works have tried to leverage the rationales of LLMs to train small language models (SLMs) for \u2026"}, {"title": "Label correlated contrastive learning for medical report generation", "link": "https://www.sciencedirect.com/science/article/pii/S0169260724004759", "details": "X Liu, J Xin, B Dai, Q Shen, Z Huang, Z Wang - Computer Methods and Programs in \u2026, 2024", "abstract": "Abstract Background and Objective: Automatic generation of medical reports reduces both the burden on radiologists and the possibility of errors due to the inexperience of radiologists. The model that utilizes attention mechanism and contrastive learning \u2026"}, {"title": "Language-based reasoning graph neural network for commonsense question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007408", "details": "M Yang, Y Wang, Y Gu - Neural Networks, 2024", "abstract": "Abstract Language model (LM) has played an increasingly important role in the common-sense understanding and reasoning in the CSQA task (Common Sense Question Answering). However, due to the amount of model parameters, increasing \u2026"}]
