[{"title": "Attention-enriched deeper UNet (ADU-NET) for disease diagnosis in breast ultrasound and retina fundus images", "link": "https://link.springer.com/article/10.1007/s13748-024-00340-1", "details": "CJ Ejiyi, Z Qin, VK Agbesi, MB Ejiyi, IA Chikwendu\u2026 - Progress in Artificial \u2026, 2024", "abstract": "In image segmentation, effective upsampling plays a pivotal role in recovering lost spatial information during the process of downsampling. Standard skip connections designed to mitigate this and prevalent in most models, often fall short of maintaining \u2026"}, {"title": "LLMs for clinical risk prediction", "link": "https://arxiv.org/pdf/2409.10191", "details": "M Rezk, PC Silva, FM Dahlweid - arXiv preprint arXiv:2409.10191, 2024", "abstract": "This study compares the efficacy of GPT-4 and clinalytix Medical AI in predicting the clinical risk of delirium development. Findings indicate that GPT-4 exhibited significant deficiencies in identifying positive cases and struggled to provide reliable \u2026"}, {"title": "Evaluation of AI-Enhanced Non-Mydriatic Fundus Photography for Diabetic Retinopathy Screening", "link": "https://www.sciencedirect.com/science/article/pii/S1572100024003685", "details": "CL Hu, YC Wang, WF Wu, Y Xi - Photodiagnosis and Photodynamic Therapy, 2024", "abstract": "Objective To assess the feasibility of using non-mydriatic fundus photography in conjunction with an artificial intelligence (AI) reading platform for large-scale screening of diabetic retinopathy (DR). Methods In this study, we selected 120 \u2026"}, {"title": "Context-Aware Optimal Transport Learning for Retinal Fundus Image Enhancement", "link": "https://arxiv.org/pdf/2409.07862", "details": "VK Vasa, P Qiu, W Zhu, Y Xiong, O Dumitrascu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Retinal fundus photography offers a non-invasive way to diagnose and monitor a variety of retinal diseases, but is prone to inherent quality glitches arising from systemic imperfections or operator/patient-related factors. However, high-quality \u2026"}, {"title": "VU Research Portal", "link": "https://research.vu.nl/ws/portalfiles/portal/355675396/d1ab2185232d59b73/docs/_source/tutorials/notebooks/deploying-textclassification-colab-activelearning.ipynb", "details": "M Laurer", "abstract": "From millions of social media posts, to decades of legal text-more and more relevant information is hidden in digital text corpora that are too large for manual analyses. The key promise of machine learning is to automate parts of the manual analysis \u2026"}, {"title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles", "link": "https://arxiv.org/pdf/2409.10502", "details": "K Shah, N Dikkala, X Wang, R Panigrahy - arXiv preprint arXiv:2409.10502, 2024", "abstract": "Causal language modeling using the Transformer architecture has yielded remarkable capabilities in Large Language Models (LLMs) over the last few years. However, the extent to which fundamental search and reasoning capabilities \u2026"}, {"title": "Electrocardiogram Report Generation and Question Answering via Retrieval-Augmented Self-Supervised Modeling", "link": "https://arxiv.org/pdf/2409.08788", "details": "J Tang, T Xia, Y Lu, C Mascolo, A Saeed - arXiv preprint arXiv:2409.08788, 2024", "abstract": "Interpreting electrocardiograms (ECGs) and generating comprehensive reports remain challenging tasks in cardiology, often requiring specialized expertise and significant time investment. To address these critical issues, we propose ECG \u2026"}, {"title": "The Use of Large Language Models Tuned with Socratic Methods on the Impact of Medical Students' Learning: A Randomised Controlled Trial", "link": "https://s3.ca-central-1.amazonaws.com/assets.jmir.org/assets/preprints/preprint-57995-submitted.pdf", "details": "CL Yong, MS Furqan, JWK Lee, A Makmur\u2026", "abstract": "Abstract Background: Large Language Models (LLM) are AI models that can generate conversational content based on a trained specified source of information (corpus). Objective: The aim is to use these corpus-trained LLMs to limit the content \u2026"}, {"title": "Large Language Model is not a (Multilingual) Compositional Relation Reasoner", "link": "https://openreview.net/pdf%3Fid%3DwLQ3I0F1oj", "details": "J Zhao, X Zhang - First Conference on Language Modeling", "abstract": "We present a comprehensive evaluation of large language models' capability to reason compositional relations through a benchmark encompassing 1,800 test cases in both English and Chinese, covering six distinct categories of composition \u2026"}]
