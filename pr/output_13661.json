[{"title": "Improving Clinical Question Answering with Multi-Task Learning: A Joint Approach for Answer Extraction and Medical Categorization", "link": "https://arxiv.org/pdf/2502.13108%3F", "details": "P Pattnayak, HL Patel, A Agarwal, B Kumar, S Panda\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Clinical Question Answering (CQA) plays a crucial role in medical decision-making, enabling physicians to extract relevant information from Electronic Medical Records (EMRs). While transformer-based models such as BERT, BioBERT, and \u2026"}, {"title": "Mapping 1,000+ Language Models via the Log-Likelihood Vector", "link": "https://arxiv.org/pdf/2502.16173", "details": "M Oyama, H Yamagiwa, Y Takase, H Shimodaira - arXiv preprint arXiv:2502.16173, 2025", "abstract": "To compare autoregressive language models at scale, we propose using log- likelihood vectors computed on a predefined text set as model features. This approach has a solid theoretical basis: when treated as model coordinates, their \u2026"}, {"title": "CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport Alignment for Language Models with Different Tokenizers", "link": "https://arxiv.org/pdf/2502.16806", "details": "AD Le, T Vu, NL Hai, NTN Diep, LN Van, T Le\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) achieve state-of-the-art performance across various NLP tasks but face deployment challenges due to high computational costs and memory constraints. Knowledge distillation (KD) is a promising solution, transferring \u2026"}, {"title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?", "link": "https://arxiv.org/pdf/2503.02199", "details": "A Deng, T Cao, Z Chen, B Hooi - arXiv preprint arXiv:2503.02199, 2025", "abstract": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual \u2026"}, {"title": "Enhancing Multi-hop Reasoning in Vision-Language Models via Self-Distillation with Multi-Prompt Ensembling", "link": "https://arxiv.org/pdf/2503.01754", "details": "G Wu, H Song, Y Wang, Q Yan, Y Tian, LL Cheong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multi-modal large language models have seen rapid advancement alongside large language models. However, while language models can effectively leverage chain- of-thought prompting for zero or few-shot learning, similar prompting strategies are \u2026"}, {"title": "Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images", "link": "https://arxiv.org/pdf/2502.13928", "details": "S Wu, FY Sun, K Wen, N Haber - arXiv preprint arXiv:2502.13928, 2025", "abstract": "Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises \u2026"}, {"title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large Vision-Language Models", "link": "https://arxiv.org/pdf/2502.14504", "details": "Y Meng, K Li, C Huang, C Gao, X Chen, Y Li, X Zhang - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across a range of multimodal tasks. However, their inference efficiency is constrained by the large number of visual tokens processed during decoding. To address this \u2026"}, {"title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models", "link": "https://arxiv.org/pdf/2502.17387", "details": "A Albalak, D Phung, N Lile, R Rafailov, K Gandhi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written \u2026"}, {"title": "Compositional Causal Reasoning Evaluation in Language Models", "link": "https://arxiv.org/abs/2503.04556", "details": "JRMA Maasch, A H\u00fcy\u00fck, X Xu, AV Nori, J Gonzalez - arXiv preprint arXiv:2503.04556, 2025", "abstract": "Causal reasoning and compositional reasoning are two core aspirations in generative AI. Measuring the extent of these behaviors requires principled evaluation methods. We explore a unified perspective that considers both behaviors \u2026"}]
