[{"title": "Unlocking language boundaries: AraCLIP-transforming Arabic language and image understanding through cross-lingual models", "link": "https://www.sciencedirect.com/science/article/pii/S0952197625005779", "details": "M Al-Barham, I Afyouni, K Almubarak, A Turky\u2026 - Engineering Applications of \u2026, 2025", "abstract": "In the domain of image retrieval, the integration of text and images has been transformative, facilitating models that transcend language barriers. This paper introduces Arabic Contrastive Language-Image Pre-training (AraCLIP), an extension \u2026"}, {"title": "Leveraging Robust Optimization for LLM Alignment under Distribution Shifts", "link": "https://arxiv.org/pdf/2504.05831", "details": "M Zhu, Y Liu, J Guo, Q Wang, Y Zhang, Z Mao - arXiv preprint arXiv:2504.05831, 2025", "abstract": "Large language models (LLMs) increasingly rely on preference alignment methods to steer outputs toward human values, yet these methods are often constrained by the scarcity of high-quality human-annotated data. To tackle this, recent approaches \u2026"}, {"title": "Think twice: Enhancing llm reasoning by scaling multi-round test-time thinking", "link": "https://arxiv.org/pdf/2503.19855%3F", "details": "X Tian, S Zhao, H Wang, S Chen, Y Ji, Y Peng, H Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite \u2026"}, {"title": "A Survey on Mixture of Experts in Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10937907/", "details": "W Cai, J Jiang, F Wang, J Tang, S Kim, J Huang - IEEE Transactions on Knowledge \u2026, 2025", "abstract": "Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size \u2026"}, {"title": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark", "link": "https://arxiv.org/pdf/2503.20786", "details": "SM Bsharat, M Ranjan, A Myrzakhan, J Liu, B Guo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Rapid advancements in large language models (LLMs) have increased interest in deploying them on mobile devices for on-device AI applications. Mobile users interact differently with LLMs compared to desktop users, creating unique \u2026"}]
