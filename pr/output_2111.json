[{"title": "Privacy-Aware Visual Language Models", "link": "https://arxiv.org/pdf/2405.17423", "details": "L Samson, N Barazani, S Ghebreab, YM Asano - arXiv preprint arXiv:2405.17423, 2024", "abstract": "This paper aims to advance our understanding of how Visual Language Models (VLMs) handle privacy-sensitive information, a crucial concern as these technologies become integral to everyday life. To this end, we introduce a new benchmark \u2026"}, {"title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment", "link": "https://arxiv.org/pdf/2405.19332", "details": "S Zhang, D Yu, H Sharma, Z Yang, S Wang, H Hassan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed \u2026"}, {"title": "Calibrating Reasoning in Language Models with Internal Consistency", "link": "https://arxiv.org/pdf/2405.18711", "details": "Z Xie, J Guo, T Yu, S Li - arXiv preprint arXiv:2405.18711, 2024", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought (CoT) prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious \u2026"}, {"title": "Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models", "link": "https://arxiv.org/pdf/2405.16282", "details": "A Kumar, R Morabito, S Umbet, J Kabbara, A Emami - arXiv preprint arXiv \u2026, 2024", "abstract": "As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models \u2026"}, {"title": "Learning diverse attacks on large language models for robust red-teaming and safety tuning", "link": "https://arxiv.org/pdf/2405.18540", "details": "S Lee, M Kim, L Cherif, D Dobre, J Lee, SJ Hwang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Red-teaming, or identifying prompts that elicit harmful responses, is a critical step in ensuring the safe and responsible deployment of large language models (LLMs). Developing effective protection against many modes of attack prompts requires \u2026"}, {"title": "LLMs and Memorization: On Quality and Specificity of Copyright Compliance", "link": "https://arxiv.org/pdf/2405.18492", "details": "FB Mueller, R G\u00f6rge, AK Bernzen, JC Pirk\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Memorization in large language models (LLMs) is a growing concern. LLMs have been shown to easily reproduce parts of their training data, including copyrighted work. This is an important problem to solve, as it may violate existing copyright laws \u2026"}, {"title": "Mosaic Memory: Fuzzy Duplication in Copyright Traps for Large Language Models", "link": "https://arxiv.org/pdf/2405.15523", "details": "I Shilov, M Meeus, YA de Montjoye - arXiv preprint arXiv:2405.15523, 2024", "abstract": "The immense datasets used to develop Large Language Models (LLMs) often include copyright-protected content, typically without the content creator's consent. Copyright traps have been proposed to be injected into the original content \u2026"}, {"title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models", "link": "https://arxiv.org/pdf/2405.16057", "details": "X Lu, A Zhou, Y Xu, R Zhang, P Gao, H Li - arXiv preprint arXiv:2405.16057, 2024", "abstract": "Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine- tuning and deployment. Current post-training pruning methods, while reducing the \u2026"}, {"title": "Adversarial Robustness for Visual Grounding of Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2405.09981", "details": "K Gao, Y Bai, J Bai, Y Yang, ST Xia - arXiv preprint arXiv:2405.09981, 2024", "abstract": "Multi-modal Large Language Models (MLLMs) have recently achieved enhanced performance across various vision-language tasks including visual grounding capabilities. However, the adversarial robustness of visual grounding remains \u2026"}]
