'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Smaller Language Models are Better Zero-shot Machine-G'
[{"title": "Can 3D Vision-Language Models Truly Understand Natural Language?", "link": "https://arxiv.org/pdf/2403.14760", "details": "W Deng, R Ding, J Yang, J Liu, Y Li, X Qi, E Ngai - arXiv preprint arXiv:2403.14760, 2024", "abstract": "Rapid advancements in 3D vision-language (3D-VL) tasks have opened up new avenues for human interaction with embodied agents or robots using natural language. Despite this progress, we find a notable limitation: existing 3D-VL models \u2026"}, {"title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models", "link": "https://arxiv.org/pdf/2403.08281", "details": "N Ding, Y Chen, G Cui, X Lv, R Xie, B Zhou, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three \u2026"}, {"title": "Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts", "link": "https://arxiv.org/html/2403.12918v1", "details": "SA Somayajula, Y Liang, A Singh, L Zhang, P Xie - arXiv preprint arXiv:2403.12918, 2024", "abstract": "Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle \u2026"}, {"title": "Adaptive Prompt Routing for Arbitrary Text Style Transfer with Pre-trained Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29832/31446", "details": "Q Liu, J Qin, W Ye, H Mou, Y He, K Wang - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Recently, arbitrary text style transfer (TST) has made significant progress with the paradigm of prompt learning. In this paradigm, researchers often design or search for a fixed prompt for any input. However, existing evidence shows that large language \u2026"}, {"title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", "link": "https://arxiv.org/html/2404.05567v1", "details": "B Pan, Y Shen, H Liu, M Mishra, G Zhang, A Oliva\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) language models can reduce computational costs by 2- 4$\\times $ compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios. However, MoE models generally \u2026"}, {"title": "Language Models on a Diet: Cost-Efficient Development of Encoders for Closely-Related Languages via Additional Pretraining", "link": "https://arxiv.org/pdf/2404.05428", "details": "N Ljube\u0161i\u0107, V Suchomel, P Rupnik, T Kuzman\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The world of language models is going through turbulent times, better and ever larger models are coming out at an unprecedented speed. However, we argue that, especially for the scientific community, encoder models of up to 1 billion parameters \u2026"}, {"title": "Seme at semeval-2024 task 2: Comparing masked and generative language models on natural language inference for clinical trials", "link": "https://arxiv.org/pdf/2404.03977", "details": "M Aguiar, P Zweigenbaum, N Naderi - arXiv preprint arXiv:2404.03977, 2024", "abstract": "This paper describes our submission to Task 2 of SemEval-2024: Safe Biomedical Natural Language Inference for Clinical Trials. The Multi-evidence Natural Language Inference for Clinical Trial Data (NLI4CT) consists of a Textual Entailment (TE) task \u2026"}, {"title": "Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data", "link": "https://arxiv.org/pdf/2404.03862", "details": "J Zhang, M Marone, T Li, B Van Durme, D Khashabi - arXiv preprint arXiv:2404.03862, 2024", "abstract": "For humans to trust the fluent generations of large language models (LLMs), they must be able to verify their correctness against trusted, external sources. Recent efforts aim to increase verifiability through citations of retrieved documents or post \u2026"}, {"title": "Personalized Federated Graph Learning on Non-IID Electronic Health Records", "link": "https://scholarsmine.mst.edu/cgi/viewcontent.cgi%3Farticle%3D2465%26context%3Dcomsci_facwork", "details": "T Tang, Z Han, Z Cai, S Yu, X Zhou, T Oseni, SK Das - IEEE Transactions on Neural \u2026, 2024", "abstract": "Understanding the latent disease patterns embedded in electronic health records (EHRs) is crucial for making precise and proactive healthcare decisions. Federated graph learning-based methods are commonly employed to extract complex disease \u2026"}]
