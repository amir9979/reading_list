[{"title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?", "link": "https://arxiv.org/pdf/2410.23856", "details": "Z Zhou, R Tao, J Zhu, Y Luo, Z Wang, B Han - arXiv preprint arXiv:2410.23856, 2024", "abstract": "This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We \u2026"}, {"title": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning", "link": "https://arxiv.org/pdf/2411.05193", "details": "J Hong, A Dragan, S Levine - arXiv preprint arXiv:2411.05193, 2024", "abstract": "Value-based reinforcement learning (RL) can in principle learn effective policies for a wide range of multi-turn problems, from games to dialogue to robotic control, including via offline RL from static previously collected datasets. However, despite \u2026"}, {"title": "Code-switching finetuning: Bridging multilingual pretrained language models for enhanced cross-lingual performance", "link": "https://www.sciencedirect.com/science/article/pii/S0952197624016907", "details": "C Zan, L Ding, L Shen, Y Cao, W Liu - Engineering Applications of Artificial \u2026, 2025", "abstract": "In recent years, the development of pre-trained models has significantly propelled advancements in natural language processing. However, multilingual sequence-to- sequence pretrained language models (Seq2Seq PLMs) are pretrained on a wide \u2026"}, {"title": "Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings", "link": "https://arxiv.org/pdf/2410.22685", "details": "YS Grewal, EV Bonilla, TD Bui - arXiv preprint arXiv:2410.22685, 2024", "abstract": "Accurately quantifying uncertainty in large language models (LLMs) is crucial for their reliable deployment, especially in high-stakes applications. Current state-of-the- art methods for measuring semantic uncertainty in LLMs rely on strict bidirectional \u2026"}, {"title": "Neuron-level knowledge attribution in large language models", "link": "https://aclanthology.org/2024.emnlp-main.191.pdf", "details": "Z Yu, S Ananiadou - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a \u2026"}, {"title": "Research on Fine-Tuning Optimization Strategies for Large Language Models in Tabular Data Processing", "link": "https://www.mdpi.com/2313-7673/9/11/708", "details": "X Zhao, X Leng, L Wang, N Wang - Biomimetics, 2024", "abstract": "Recent advancements in natural language processing (NLP) have been significantly driven by the development of large language models (LLMs). Despite their impressive performance across various language tasks, these models still encounter \u2026"}, {"title": "Web-Scale Visual Entity Recognition: An LLM-Driven Data Approach", "link": "https://arxiv.org/pdf/2410.23676", "details": "M Caron, A Fathi, C Schmid, A Iscen - arXiv preprint arXiv:2410.23676, 2024", "abstract": "Web-scale visual entity recognition, the task of associating images with their corresponding entities within vast knowledge bases like Wikipedia, presents significant challenges due to the lack of clean, large-scale training data. In this paper \u2026"}]
