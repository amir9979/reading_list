[{"title": "Does Knowledge Localization Hold True? Surprising Differences Between Entity and Relation Perspectives in Language Models", "link": "https://arxiv.org/pdf/2409.00617", "details": "Y Wei, X Yu, Y Weng, H Ma, Y Zhang, J Zhao, K Liu - arXiv preprint arXiv:2409.00617, 2024", "abstract": "Large language models encapsulate knowledge and have demonstrated superior performance on various natural language processing tasks. Recent studies have localized this knowledge to specific model parameters, such as the MLP weights in \u2026"}, {"title": "Exploiting Pre-trained Language Models for Black-box Attack against Knowledge Graph Embeddings", "link": "https://dl.acm.org/doi/pdf/10.1145/3688850", "details": "G Yang, L Zhang, Y Liu, H Xie, Z Mao - ACM Transactions on Knowledge Discovery \u2026, 2024", "abstract": "Despite the emerging research on adversarial attacks against Knowledge Graph Embedding (KGE) models, most of them focus on white-box attack settings. However, white-box attacks are difficult to apply in practice compared to black-box attacks \u2026"}, {"title": "Eureka: Evaluating and Understanding Large Foundation Models", "link": "https://arxiv.org/pdf/2409.10566", "details": "V Balachandran, J Chen, N Joshi, B Nushi, H Palangi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Rigorous and reproducible evaluation is critical for assessing the state of the art and for guiding scientific advances in Artificial Intelligence. Evaluation is challenging in practice due to several reasons, including benchmark saturation, lack of \u2026"}, {"title": "Safety Layers of Aligned Large Language Models: The Key to LLM Security", "link": "https://arxiv.org/pdf/2408.17003", "details": "S Li, L Yao, L Zhang, Y Li - arXiv preprint arXiv:2408.17003, 2024", "abstract": "Aligned LLMs are highly secure, capable of recognizing and refusing to answer malicious questions. However, the role of internal parameters in maintaining this security is not well understood, further these models are vulnerable to security \u2026"}, {"title": "Towards a Unified View of Preference Learning for Large Language Models: A Survey", "link": "https://arxiv.org/pdf/2409.02795", "details": "B Gao, F Song, Y Miao, Z Cai, Z Yang, L Chen, H Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of the crucial factors to achieve success is aligning the LLM's output with human preferences. This alignment process often requires only a small amount of data to \u2026"}, {"title": "Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback", "link": "https://arxiv.org/pdf/2409.00162", "details": "J Zhou, J Ji, J Dai, Y Yang - arXiv preprint arXiv:2409.00162, 2024", "abstract": "Aligning the behavior of Large language models (LLMs) with human intentions and values remains a critical challenge. Reinforcement learning from human feedback (RLHF) aligns LLMs by training a reward model (RM) on human preferences and fine \u2026"}, {"title": "An Empirical Study on Self-correcting Large Language Models for Data Science Code Generation", "link": "https://arxiv.org/pdf/2408.15658", "details": "TT Quoc, DH Minh, TQ Thanh, A Nguyen-Duc - arXiv preprint arXiv:2408.15658, 2024", "abstract": "Large Language Models (LLMs) have recently advanced many applications on software engineering tasks, particularly the potential for code generation. Among contemporary challenges, code generated by LLMs often suffers from inaccuracies \u2026"}, {"title": "AutoTQA: Towards Autonomous Tabular Question Answering through Multi-Agent Large Language Models", "link": "https://www.vldb.org/pvldb/vol17/p3920-zhu.pdf", "details": "JP Zhu, P Cai, K Xu, L Li, Y Sun, S Zhou, H Su, L Tang\u2026", "abstract": "With the growing significance of data analysis, several studies aim to provide precise answers to users' natural language questions from tables, a task referred to as tabular question answering (TQA). The state-of-the-art TQA approaches are limited to \u2026"}, {"title": "The Dark Side of Human Feedback: Poisoning Large Language Models via User Inputs", "link": "https://arxiv.org/pdf/2409.00787", "details": "B Chen, H Guo, G Wang, Y Wang, Q Yan - arXiv preprint arXiv:2409.00787, 2024", "abstract": "Large Language Models (LLMs) have demonstrated great capabilities in natural language understanding and generation, largely attributed to the intricate alignment process using human feedback. While alignment has become an essential training \u2026"}]
