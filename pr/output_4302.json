[{"title": "LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models", "link": "https://arxiv.org/pdf/2406.19486", "details": "S Guo, S Damani, K Chang - arXiv preprint arXiv:2406.19486, 2024", "abstract": "In prompt tuning, a prefix or suffix text is added to the prompt, and the embeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix are optimized to gain more control over language models for specific tasks. This approach eliminates the \u2026"}, {"title": "When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?", "link": "https://arxiv.org/pdf/2407.15211", "details": "R Schaeffer, D Valentine, L Bailey, J Chua\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The integration of new modalities into frontier AI systems offers exciting capabilities, but also increases the possibility such systems can be adversarially manipulated in undesirable ways. In this work, we focus on a popular class of vision-language \u2026"}, {"title": "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation", "link": "https://arxiv.org/pdf/2407.02056", "details": "X Wang, Y Li, S Feng, P Yuan, B Pan, H Wang, Y Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-consistency (SC), leveraging multiple samples from LLMs, shows significant gains on various reasoning tasks but struggles with free-form generation due to the difficulty of aggregating answers. Its variants, UCS and USC, rely on sample \u2026"}, {"title": "Zero-Shot Embeddings Inform Learning and Forgetting with Vision-Language Encoders", "link": "https://arxiv.org/pdf/2407.15731", "details": "L Niss, K Vogt-Lowell, T Tsiligkaridis - arXiv preprint arXiv:2407.15731, 2024", "abstract": "Despite the proliferation of large vision-language foundation models, estimation of the learning and forgetting outcomes following fine-tuning of these models remains largely unexplored. Inspired by work highlighting the significance of the modality gap \u2026"}, {"title": "XAI meets LLMs: A Survey of the Relation between Explainable AI and Large Language Models", "link": "https://arxiv.org/pdf/2407.15248", "details": "E Cambria, L Malandri, F Mercorio, N Nobani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this survey, we address the key challenges in Large Language Models (LLM) research, focusing on the importance of interpretability. Driven by increasing interest from AI and business sectors, we highlight the need for transparency in LLMs. We \u2026"}, {"title": "Entropy-Based Decoding for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2406.17519", "details": "Z Qiu, Z Ou, B Wu, J Li, A Liu, I King - arXiv preprint arXiv:2406.17519, 2024", "abstract": "Augmenting Large Language Models (LLMs) with retrieved external knowledge has proven effective for improving the factual accuracy of generated responses. Despite their success, retrieval-augmented LLMs still face the distractibility issue, where the \u2026"}, {"title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application", "link": "https://arxiv.org/pdf/2407.01885", "details": "C Yang, W Lu, Y Zhu, Y Wang, Q Chen, C Gao, B Yan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have showcased exceptional capabilities in various domains, attracting significant interest from both academia and industry. Despite their impressive performance, the substantial size and computational demands of LLMs \u2026"}, {"title": "Mental Modeling of Reinforcement Learning Agents by Language Models", "link": "https://arxiv.org/pdf/2406.18505", "details": "W Lu, X Zhao, J Spisak, JH Lee, S Wermter - arXiv preprint arXiv:2406.18505, 2024", "abstract": "Can emergent language models faithfully model the intelligence of decision-making agents? Though modern language models exhibit already some reasoning ability, and theoretically can potentially express any probable distribution over tokens, it \u2026"}, {"title": "Chronologically Accurate Retrieval for Temporal Grounding of Motion-Language Models", "link": "https://arxiv.org/pdf/2407.15408", "details": "K Fujiwara, M Tanaka, Q Yu - arXiv preprint arXiv:2407.15408, 2024", "abstract": "With the release of large-scale motion datasets with textual annotations, the task of establishing a robust latent space for language and 3D human motion has recently witnessed a surge of interest. Methods have been proposed to convert human \u2026"}]
