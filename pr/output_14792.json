[{"title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models", "link": "https://arxiv.org/pdf/2504.02821", "details": "M Pach, S Karthik, Q Bouniot, S Belongie, Z Akata - arXiv preprint arXiv:2504.02821, 2025", "abstract": "Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce \u2026"}, {"title": "ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models", "link": "https://arxiv.org/pdf/2503.19355", "details": "D Ko, S Kim, Y Suh, M Yoon, M Chandraker, HJ Kim - arXiv preprint arXiv \u2026, 2025", "abstract": "Spatio-temporal reasoning is essential in understanding real-world environments in various fields, eg, autonomous driving and sports analytics. Recent advances have improved the spatial reasoning ability of Vision-Language Models (VLMs) by \u2026"}, {"title": "Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving", "link": "https://arxiv.org/pdf/2503.21505%3F", "details": "Y Li, M Tian, Z Lin, J Zhu, D Zhu, H Liu, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing benchmarks for Vision-Language Model (VLM) on autonomous driving (AD) primarily assess interpretability through open-form visual question answering (QA) within coarse-grained tasks, which remain insufficient to assess capabilities in \u2026"}, {"title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme", "link": "https://arxiv.org/pdf/2504.02587", "details": "Y Ma, S Chern, X Shen, Y Zhong, P Liu - arXiv preprint arXiv:2504.02587, 2025", "abstract": "Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often \u2026"}, {"title": "Evolution-based Region Adversarial Prompt Learning for Robustness Enhancement in Vision-Language Models", "link": "https://arxiv.org/pdf/2503.12874", "details": "X Jia, S Gao, S Qin, K Ma, X Li, Y Huang, W Dong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large pre-trained vision-language models (VLMs), such as CLIP, demonstrate impressive generalization but remain highly vulnerable to adversarial examples (AEs). Previous work has explored robust text prompts through adversarial training \u2026"}, {"title": "Can Vision-Language Models Answer Face to Face Questions in the Real-World?", "link": "https://arxiv.org/pdf/2503.19356", "details": "R Pourreza, R Dagli, A Bhattacharyya, S Panchal\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "AI models have made significant strides in recent years in their ability to describe and answer questions about real-world images. They have also made progress in the ability to converse with users in real-time using audio input. This raises the question \u2026"}, {"title": "Beyond Standard MoE: Mixture of Latent Experts for Resource-Efficient Language Models", "link": "https://arxiv.org/pdf/2503.23100", "details": "Z Liu, H Wu, R She, X Fu, X Han, T Zhong, M Yuan - arXiv preprint arXiv:2503.23100, 2025", "abstract": "Mixture of Experts (MoE) has emerged as a pivotal architectural paradigm for efficient scaling of Large Language Models (LLMs), operating through selective activation of parameter subsets for each input token. Nevertheless, conventional MoE \u2026"}, {"title": "Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models", "link": "https://media.roboflow.com/rf100vl/rf100vl.pdf", "details": "P Robicheaux, M Popov, A Madan, I Robinson\u2026", "abstract": "Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to outof-distribution tasks \u2026"}, {"title": "Federated Continual Instruction Tuning", "link": "https://arxiv.org/pdf/2503.12897", "details": "H Guo, F Zeng, F Zhu, W Liu, DH Wang, J Xu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "A vast amount of instruction tuning data is crucial for the impressive performance of Large Multimodal Models (LMMs), but the associated computational costs and data collection demands during supervised fine-tuning make it impractical for most \u2026"}]
