'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Understanding Emergent Abilities of Language Models fr'
[{"title": "Towards a Robust Retrieval-Based Summarization System", "link": "https://arxiv.org/pdf/2403.19889", "details": "S Liu, J Wu, J Bao, W Wang, N Hovakimyan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper describes an investigation of the robustness of large language models (LLMs) for retrieval augmented generation (RAG)-based summarization tasks. While LLMs provide summarization capabilities, their performance in complex, real-world \u2026"}, {"title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.07860", "details": "S Zhao, S Hao, B Zi, H Xu, KYK Wong - arXiv preprint arXiv:2403.07860, 2024", "abstract": "Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding \u2026"}, {"title": "A Comprehensive Overhaul of Multimodal Assistant with Small Language Models", "link": "https://arxiv.org/html/2403.06199v3", "details": "M Zhu, Y Zhu, X Liu, N Liu, Z Xu, C Shen, Y Peng, Z Ou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal Large Language Models (MLLMs) have showcased impressive skills in tasks related to visual understanding and reasoning. Yet, their widespread application faces obstacles due to the high computational demands during both the \u2026"}, {"title": "Are We on the Right Way for Evaluating Large Vision-Language Models?", "link": "https://arxiv.org/pdf/2403.20330", "details": "L Chen, J Li, X Dong, P Zhang, Y Zang, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is \u2026"}, {"title": "ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2403.20194", "details": "S Liu, K Ying, H Zhang, Y Yang, Y Lin, T Zhang, C Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper presents ConvBench, a novel multi-turn conversation evaluation benchmark tailored for Large Vision-Language Models (LVLMs). Unlike existing benchmarks that assess individual capabilities in single-turn dialogues, ConvBench \u2026"}, {"title": "$\\mathbf {(N, K)} $-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model", "link": "https://arxiv.org/html/2403.07191v1", "details": "Y Zhang, L Chen, B Liu, Y Yang, Q Cui, Y Tao, H Yang - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these \u2026"}, {"title": "Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous Driving", "link": "https://arxiv.org/html/2403.05907v1", "details": "J Cao, Z Li, N Wang, C Ma - arXiv preprint arXiv:2403.05907, 2024", "abstract": "Recent studies have highlighted the promising application of NeRF in autonomous driving contexts. However, the complexity of outdoor environments, combined with the restricted viewpoints in driving scenarios, complicates the task of precisely \u2026"}, {"title": "Grounding and Enhancing Grid-based Models for Neural Fields", "link": "https://arxiv.org/html/2403.20002v1", "details": "Z Zhao, F Fan, W Liao, J Yan - arXiv preprint arXiv:2403.20002, 2024", "abstract": "Many contemporary studies utilize grid-based models for neural field representation, but a systematic analysis of grid-based models is still missing, hindering the improvement of those models. Therefore, this paper introduces a theoretical \u2026"}, {"title": "Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models", "link": "https://arxiv.org/pdf/2403.20331", "details": "A Miyai, J Yang, J Zhang, Y Ming, Q Yu, G Irie, Y Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of \u2026"}]
