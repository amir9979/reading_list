[{"title": "Elucidating the Design Space of Multimodal Protein Language Models", "link": "https://arxiv.org/pdf/2504.11454", "details": "X Wang, D Zhang, D Xue, F Ye, S Huang, Z Zheng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal protein language models (PLMs) integrate sequence and token-based structural information, serving as a powerful foundation for protein modeling, generation, and design. However, the reliance on tokenizing 3D structures into \u2026"}, {"title": "A Dual-Space Framework for General Knowledge Distillation of Large Language Models", "link": "https://arxiv.org/pdf/2504.11426", "details": "X Zhang, S Zhang, Y Liang, F Meng, Y Chen, J Xu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Knowledge distillation (KD) is a promising solution to compress large language models (LLMs) by transferring their knowledge to smaller models. During this process, white-box KD methods usually minimize the distance between the output \u2026"}, {"title": "FLUE: Streamlined Uncertainty Estimation for Large Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/33840/35995", "details": "S Gao, T Gong, Z Lin, R Xu, H Zhou, J Li - Proceedings of the AAAI Conference on \u2026, 2025", "abstract": "Uncertainty estimation is essential for practical applications such as decision- making, risk assessment, and human-AI collaboration. However, Uncertainty estimation in open-ended question-answering (QA) tasks presents unique \u2026"}, {"title": "Large language models could be rote learners", "link": "https://arxiv.org/pdf/2504.08300", "details": "Y Xu, R Hu, H Ying, J Wu, X Shi, W Lin - arXiv preprint arXiv:2504.08300, 2025", "abstract": "Multiple-choice question (MCQ) benchmarks are widely used for evaluating Large Language Models (LLMs), yet their reliability is undermined by benchmark contamination. In this study, we reframe contamination as an inherent aspect of \u2026"}, {"title": "Exploring Conversational Adaptability: Assessing the Proficiency of Large Language Models in Dynamic Alignment with Updated User Intent", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/34534/36689", "details": "YC Chen, HH Huang - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "This paper presents a practical problem in dialogue systems: the capability to adapt to changing user intentions and resolve inconsistencies in conversation histories. It is crucial in scenarios like train ticket booking, where travel plans often change \u2026"}, {"title": "Fast-Slow-Thinking: Complex Task Solving with Large Language Models", "link": "https://arxiv.org/pdf/2504.08690", "details": "Y Sun, Y Zhang, Z Zhao, S Wan, D Tao, C Gong - arXiv preprint arXiv:2504.08690, 2025", "abstract": "Nowadays, Large Language Models (LLMs) have been gradually employed to solve complex tasks. To face the challenge, task decomposition has become an effective way, which proposes to divide a complex task into multiple simpler subtasks and \u2026"}, {"title": "Teaching Large Language Models to Reason through Learning and Forgetting", "link": "https://arxiv.org/pdf/2504.11364", "details": "T Ni, A Nie, S Chaudhary, Y Liu, H Rangwala, R Fakoor - arXiv preprint arXiv \u2026, 2025", "abstract": "Leveraging inference-time search in large language models has proven effective in further enhancing a trained model's capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational \u2026"}, {"title": "Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning", "link": "https://arxiv.org/pdf/2504.11409", "details": "A Taghibakhshi, ST Sreenivas, S Muralidharan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve state-of-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models \u2026"}, {"title": "Explore What LLM Does Not Know in Complex Question Answering", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/34638/36793", "details": "X Lin, Z Huang, Z Zhang, J Zhou, E Chen - Proceedings of the AAAI Conference on \u2026, 2025", "abstract": "Complex question answering (QA) is a challenging task in artificial intelligence research which requires reasoning based on related knowledge. The retrieval- augmented generation (RAG) based on large language models (LLMs) have \u2026"}]
