[{"title": "Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies", "link": "https://arxiv.org/pdf/2505.19337", "details": "K Li, M Zitnik - arXiv preprint arXiv:2505.19337, 2025", "abstract": "Offline goal-conditioned reinforcement learning methods have shown promise for reach-avoid tasks, where an agent must reach a target state while avoiding undesirable regions of the state space. Existing approaches typically encode avoid \u2026", "entry_id": "http://arxiv.org/abs/2505.19337v2", "updated": "2025-05-27 02:56:11", "published": "2025-05-25 22:00:38", "authors": "Kevin Li;Marinka Zitnik", "summary": "Offline goal-conditioned reinforcement learning methods have shown promise\nfor reach-avoid tasks, where an agent must reach a target state while avoiding\nundesirable regions of the state space. Existing approaches typically encode\navoid-region information into an augmented state space and cost function, which\nprevents flexible, dynamic specification of novel avoid-region information at\nevaluation time. They also rely heavily on well-designed reward and cost\nfunctions, limiting scalability to complex or poorly structured environments.\nWe introduce RADT, a decision transformer model for offline, reward-free,\ngoal-conditioned, avoid region-conditioned RL. RADT encodes goals and avoid\nregions directly as prompt tokens, allowing any number of avoid regions of\narbitrary size to be specified at evaluation time. Using only suboptimal\noffline trajectories from a random policy, RADT learns reach-avoid behavior\nthrough a novel combination of goal and avoid-region hindsight relabeling. We\nbenchmark RADT against 3 existing offline goal-conditioned RL models across 11\ntasks, environments, and experimental settings. RADT generalizes in a zero-shot\nmanner to out-of-distribution avoid region sizes and counts, outperforming\nbaselines that require retraining. In one such zero-shot setting, RADT achieves\n35.7% improvement in normalized cost over the best retrained baseline while\nmaintaining high goal-reaching success. We apply RADT to cell reprogramming in\nbiology, where it reduces visits to undesirable intermediate gene expression\nstates during trajectories to desired target states, despite stochastic\ntransitions and discrete, structured state dynamics.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;q-bio.QM", "links": "http://arxiv.org/abs/2505.19337v2;http://arxiv.org/pdf/2505.19337v2", "pdf_url": "http://arxiv.org/pdf/2505.19337v2"}, {"title": "A feature matching-based method for few-shot multivariate time series anomaly detection with symmetric patch mask Siam Transformer", "link": "https://www.sciencedirect.com/science/article/pii/S0952197625008942", "details": "J Yu, X Gao, T Wang, H Lu, B Li, F Zhai, B Xue, Z Meng - Engineering Applications of \u2026, 2025", "abstract": "Accurate anomaly detection of industrial system operating status based on multivariate time series data is an important means to ensure the stable operation of the system. However if there is insufficient training data for the objects to be detected \u2026"}, {"title": "Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based Continual Learning", "link": "https://arxiv.org/pdf/2505.20135", "details": "W Liao, Q Wang, Y Wu, R Wang, D Meng - arXiv preprint arXiv:2505.20135, 2025", "abstract": "Replay-based continual learning (CL) methods assume that models trained on a small subset can also effectively minimize the empirical risk of the complete dataset. These methods maintain a memory buffer that stores a sampled subset of data from \u2026", "entry_id": "http://arxiv.org/abs/2505.20135v2", "updated": "2025-05-28 16:33:14", "published": "2025-05-26 15:37:10", "authors": "Wenyang Liao;Quanziang Wang;Yichen Wu;Renzhen Wang;Deyu Meng", "summary": "Replay-based continual learning (CL) methods assume that models trained on a\nsmall subset can also effectively minimize the empirical risk of the complete\ndataset. These methods maintain a memory buffer that stores a sampled subset of\ndata from previous tasks to consolidate past knowledge. However, this\nassumption is not guaranteed in practice due to the limited capacity of the\nmemory buffer and the heuristic criteria used for buffer data selection. To\naddress this issue, we propose a new dataset distillation framework tailored\nfor CL, which maintains a learnable memory buffer to distill the global\ninformation from the current task data and accumulated knowledge preserved in\nthe previous memory buffer. Moreover, to avoid the computational overhead and\noverfitting risks associated with parameterizing the entire buffer during\ndistillation, we introduce a lightweight distillation module that can achieve\nglobal information distillation solely by generating learnable soft labels for\nthe memory buffer data. Extensive experiments show that, our method can achieve\ncompetitive results and effectively mitigates forgetting across various\ndatasets. The source code will be publicly available.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.20135v2;http://arxiv.org/pdf/2505.20135v2", "pdf_url": "http://arxiv.org/pdf/2505.20135v2"}, {"title": "Adversarial Bandit over Bandits: Hierarchical Bandits for Online Configuration Management", "link": "https://arxiv.org/pdf/2505.19061", "details": "C Avin, Z Lotker, S Mannor, G Shabat, H Shteingart\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Motivated by dynamic parameter optimization in finite, but large action (configurations) spaces, this work studies the nonstochastic multi-armed bandit (MAB) problem in metric action spaces with oblivious Lipschitz adversaries. We \u2026", "entry_id": "http://arxiv.org/abs/2505.19061v1", "updated": "2025-05-25 09:30:47", "published": "2025-05-25 09:30:47", "authors": "Chen Avin;Zvi Lotker;Shie Mannor;Gil Shabat;Hanan Shteingart;Roey Yadgar", "summary": "Motivated by dynamic parameter optimization in finite, but large action\n(configurations) spaces, this work studies the nonstochastic multi-armed bandit\n(MAB) problem in metric action spaces with oblivious Lipschitz adversaries. We\npropose ABoB, a hierarchical Adversarial Bandit over Bandits algorithm that can\nuse state-of-the-art existing \"flat\" algorithms, but additionally clusters\nsimilar configurations to exploit local structures and adapt to changing\nenvironments. We prove that in the worst-case scenario, such clustering\napproach cannot hurt too much and ABoB guarantees a standard worst-case regret\nbound of $O\\left(k^{\\frac{1}{2}}T^{\\frac{1}{2}}\\right)$, where $T$ is the\nnumber of rounds and $k$ is the number of arms, matching the traditional flat\napproach. However, under favorable conditions related to the algorithm\nproperties, clusters properties, and certain Lipschitz conditions, the regret\nbound can be improved to $O\\left(k^{\\frac{1}{4}}T^{\\frac{1}{2}}\\right)$.\nSimulations and experiments on a real storage system demonstrate that ABoB,\nusing standard algorithms like EXP3 and Tsallis-INF, achieves lower regret and\nfaster convergence than the flat method, up to 50% improvement in known\nprevious setups, nonstochastic and stochastic, as well as in our settings.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.MA;stat.ML", "links": "http://arxiv.org/abs/2505.19061v1;http://arxiv.org/pdf/2505.19061v1", "pdf_url": "http://arxiv.org/pdf/2505.19061v1"}, {"title": "Unsupervised Specific Emitter Identification: A Multi-Scale Feature Adaptive Fusion Contrastive Learning Algorithm", "link": "https://ieeexplore.ieee.org/abstract/document/11002864/", "details": "J Wang, G Ding, Y Jiao, D Zhang, P Tang, G Wei - IEEE Wireless Communications \u2026, 2025", "abstract": "Specific Emitter Identification (SEI) plays a vital role in the fields of electromagnetic spectrum management and electronic warfare. However, existing SEI algorithms typically require labeled information, which is often unavailable in non-cooperative \u2026"}, {"title": "MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention", "link": "https://arxiv.org/pdf/2505.18698", "details": "C Yaras, AS Xu, P Abillama, C Lee, L Balzano - arXiv preprint arXiv:2505.18698, 2025", "abstract": "Transformers have achieved state-of-the-art performance across various tasks, but suffer from a notable quadratic complexity in sequence length due to the attention mechanism. In this work, we propose MonarchAttention--a novel approach to sub \u2026", "entry_id": "http://arxiv.org/abs/2505.18698v1", "updated": "2025-05-24 13:44:44", "published": "2025-05-24 13:44:44", "authors": "Can Yaras;Alec S. Xu;Pierre Abillama;Changwoo Lee;Laura Balzano", "summary": "Transformers have achieved state-of-the-art performance across various tasks,\nbut suffer from a notable quadratic complexity in sequence length due to the\nattention mechanism. In this work, we propose MonarchAttention -- a novel\napproach to sub-quadratic attention approximation via Monarch matrices, an\nexpressive class of structured matrices. Based on the variational form of\nsoftmax, we describe an efficient optimization-based algorithm to compute an\napproximate projection of softmax attention onto the class of Monarch matrices\nwith $\\Theta(N\\sqrt{N} d)$ computational complexity and $\\Theta(Nd)$ memory/IO\ncomplexity. Unlike previous approaches, MonarchAttention is both (1)\ntransferable, yielding minimal performance loss with no additional training,\neven when replacing every attention layer of the transformer, and (2)\nhardware-efficient, utilizing the highest-throughput tensor core units on\nmodern GPUs. With optimized kernels, MonarchAttention achieves substantial\nspeed-ups in wall-time over FlashAttention-2: $1.4\\times$ for shorter sequences\n$(N=256)$, $4.5\\times$ for medium-length sequences $(N=4K)$, and $8.2\\times$\nfor longer sequences $(N=16K)$. We demonstrate the quality of MonarchAttention\non diverse tasks and architectures in vision and language problems, showing\nthat it flexibly and accurately approximates softmax attention in a variety of\ncontexts. Our code is available at\nhttps://github.com/cjyaras/monarch-attention.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.18698v1;http://arxiv.org/pdf/2505.18698v1", "pdf_url": "http://arxiv.org/pdf/2505.18698v1"}, {"title": "CiTranGAN: Channel-Independent Based-Anomaly Detection for Multivariate Time Series Data", "link": "https://www.mdpi.com/2079-9292/14/9/1857", "details": "X Chen, T Li, Z Ma, J Chen, J Guo, Z Liu - Electronics, 2025", "abstract": "Anomaly detection, as a critical task in time series data analysis, plays a pivotal role in ensuring industrial production safety, enhancing the precision of climate predictions and improving early warning for ocean disaster. However, due to the high \u2026"}]
