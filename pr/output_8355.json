[{"title": "Reducing the Scope of Language Models with Circuit Breakers", "link": "https://arxiv.org/pdf/2410.21597", "details": "D Yunis, S Huo, C Gunasekara, D Contractor - arXiv preprint arXiv:2410.21597, 2024", "abstract": "Language models are now deployed in a wide variety of user-facing applications, often for specific purposes like answering questions about documentation or acting as coding assistants. As these models are intended for particular purposes, they \u2026"}, {"title": "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes", "link": "https://arxiv.org/pdf/2410.16930", "details": "BR Christ, Z Gottesman, J Kropko, T Hartvigsen - arXiv preprint arXiv:2410.16930, 2024", "abstract": "Math reasoning is a highly active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be \u2026"}, {"title": "Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop Question Answering", "link": "https://dl.acm.org/doi/pdf/10.1145/3627673.3679722", "details": "Y Shi, Q Tan, X Wu, S Zhong, K Zhou, N Liu - Proceedings of the 33rd ACM \u2026, 2024", "abstract": "Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging \u2026"}, {"title": "Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Models Alignment", "link": "https://arxiv.org/pdf/2410.16714", "details": "M Wang, C Ma, Q Chen, L Meng, Y Han, J Xiao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-play methods have demonstrated remarkable success in enhancing model capabilities across various domains. In the context of Reinforcement Learning from Human Feedback (RLHF), self-play not only boosts Large Language Model (LLM) \u2026"}, {"title": "Energy-Based Diffusion Language Models for Text Generation", "link": "https://arxiv.org/pdf/2410.21357", "details": "M Xu, T Geffner, K Kreis, W Nie, Y Xu, J Leskovec\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite remarkable progress in autoregressive language models, alternative generative paradigms beyond left-to-right generation are still being actively explored. Discrete diffusion models, with the capacity for parallel generation, have recently \u2026"}, {"title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models", "link": "https://arxiv.org/pdf/2410.18252", "details": "M Noukhovitch, S Huang, S Xhonneux, A Hosseini\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this \u2026"}, {"title": "Vision Language Model is NOT All You Need: Augmentation Strategies for Molecule Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3627673.3679607", "details": "N Lee, S Laghuvarapu, C Park, J Sun - Proceedings of the 33rd ACM International \u2026, 2024", "abstract": "Recently, there has been a growing interest among researchers in understanding molecules and their textual descriptions through molecule language models (MoLM). However, despite some early promising developments, the advancement of MoLM \u2026"}, {"title": "Faster Language Models with Better Multi-Token Prediction Using Tensor Decomposition", "link": "https://arxiv.org/pdf/2410.17765", "details": "A Basharin, A Chertkov, I Oseledets - arXiv preprint arXiv:2410.17765, 2024", "abstract": "We propose a new model for multi-token prediction in transformers, aiming to enhance sampling efficiency without compromising accuracy. Motivated by recent work that predicts the probabilities of subsequent tokens using multiple heads, we \u2026"}, {"title": "TableRAG: Million-Token Table Understanding with Language Models", "link": "https://arxiv.org/pdf/2410.04739", "details": "SA Chen, L Miculicich, JM Eisenschlos, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in language models (LMs) have notably enhanced their ability to reason with tabular data, primarily through program-aided mechanisms that manipulate and analyze tables. However, these methods often require the entire \u2026"}]
