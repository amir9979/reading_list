[{"title": "From Language Models to Medical Diagnoses: Assessing the Potential of GPT-4 and GPT-3.5-Turbo in Digital Health", "link": "https://www.mdpi.com/2673-2688/5/4/128", "details": "J Roos, TI Wilhelm, R Martin, R Kaczmarczyk - AI, 2024", "abstract": "Background: Large language models (LLMs) like GPT-3.5-Turbo and GPT-4 show potential to transform medical diagnostics through their linguistic and analytical capabilities. This study evaluates their diagnostic proficiency using English and \u2026"}, {"title": "A dataset and benchmark for hospital course summarization with adapted large language models", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae312/7934937", "details": "A Aali, D Van Veen, YI Arefeen, J Hom, C Bluethgen\u2026 - Journal of the American \u2026, 2024", "abstract": "Objective Brief hospital course (BHC) summaries are clinical documents that summarize a patient's hospital stay. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for \u2026"}, {"title": "Medec: A benchmark for medical error detection and correction in clinical notes", "link": "https://arxiv.org/pdf/2412.19260", "details": "AB Abacha, W Yim, Y Fu, Z Sun, M Yetisgen, F Xia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Several studies showed that Large Language Models (LLMs) can answer medical questions correctly, even outperforming the average human score in some medical exams. However, to our knowledge, no study has been conducted to assess the \u2026"}, {"title": "The potential of Generative Pre-trained Transformer 4 (GPT-4) to analyse medical notes in three different languages: a retrospective model-evaluation study", "link": "https://www.thelancet.com/journals/landig/article/PIIS2589-7500\\(24\\)00246-2/fulltext", "details": "MCS Menezes, AF Hoffmann, ALM Tan, M Nalbandyan\u2026 - The Lancet Digital Health, 2025", "abstract": "Background Patient notes contain substantial information but are difficult for computers to analyse due to their unstructured format. Large-language models (LLMs), such as Generative Pre-trained Transformer 4 (GPT-4), have changed our \u2026"}, {"title": "Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models", "link": "https://arxiv.org/pdf/2412.02980", "details": "A Havrilla, A Dai, L O'Mahony, K Oostermeijer, V Zisler\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it \u2026"}, {"title": "Training large language models to reason in a continuous latent space", "link": "https://arxiv.org/pdf/2412.06769%3F", "details": "S Hao, S Sukhbaatar, DJ Su, X Li, Z Hu, J Weston\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) are restricted to reason in the\" language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may \u2026"}, {"title": "LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation", "link": "https://arxiv.org/pdf/2412.15188", "details": "W Shi, X Han, C Zhou, W Liang, XV Lin, L Zettlemoyer\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present LlamaFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LlamaFusion \u2026"}, {"title": "Multi-LLM Text Summarization", "link": "https://arxiv.org/pdf/2412.15487", "details": "J Fang, CT Liu, J Kim, Y Bhedaru, E Liu, N Singh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we propose a Multi-LLM summarization framework, and investigate two different multi-LLM strategies including centralized and decentralized. Our multi-LLM summarization framework has two fundamentally important steps at each round of \u2026"}, {"title": "Find the Intention of Instruction: Comprehensive Evaluation of Instruction Understanding for Large Language Models", "link": "https://arxiv.org/pdf/2412.19450", "details": "H Moon, J Seo, S Lee, C Park, H Lim - arXiv preprint arXiv:2412.19450, 2024", "abstract": "One of the key strengths of Large Language Models (LLMs) is their ability to interact with humans by generating appropriate responses to given instructions. This ability, known as instruction-following capability, has established a foundation for the use of \u2026"}]
