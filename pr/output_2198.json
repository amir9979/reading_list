[{"title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision", "link": "https://arxiv.org/pdf/2405.04086", "details": "Y Tong, S Wang, D Li, Y Wang, S Han, Z Lin, C Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations \u2026"}, {"title": "MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning", "link": "https://arxiv.org/pdf/2405.03000", "details": "W Shi, R Xu, Y Zhuang, Y Yu, H Wu, C Yang, MD Wang - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their improved capabilities in generation and reasoning, adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and corporate privacy. In this work, we propose MedAdapter, a unified \u2026"}, {"title": "Bridging Operator Learning and Conditioned Neural Fields: A Unifying Perspective", "link": "https://arxiv.org/pdf/2405.13998", "details": "S Wang, JH Seidman, S Sankaran, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Operator learning is an emerging area of machine learning which aims to learn mappings between infinite dimensional function spaces. Here we uncover a connection between operator learning architectures and conditioned neural fields \u2026"}, {"title": "MetaEarth: A Generative Foundation Model for Global-Scale Remote Sensing Image Generation", "link": "https://arxiv.org/pdf/2405.13570", "details": "Z Yu, C Liu, L Liu, Z Shi, Z Zou - arXiv preprint arXiv:2405.13570, 2024", "abstract": "The recent advancement of generative foundational models has ushered in a new era of image generation in the realm of natural images, revolutionizing art design, entertainment, environment simulation, and beyond. Despite producing high-quality \u2026"}, {"title": "Evaluating Vision-Language Models on Bistable Images", "link": "https://arxiv.org/pdf/2405.19423", "details": "A Panagopoulou, C Melkin, C Callison-Burch - arXiv preprint arXiv:2405.19423, 2024", "abstract": "Bistable images, also known as ambiguous or reversible images, present visual stimuli that can be seen in two distinct interpretations, though not simultaneously by the observer. In this study, we conduct the most extensive examination of vision \u2026"}, {"title": "Disease-informed Adaptation of Vision-Language Models", "link": "https://arxiv.org/pdf/2405.15728", "details": "J Zhang, G Wang, MK Kalra, P Yan - arXiv preprint arXiv:2405.15728, 2024", "abstract": "In medical image analysis, the expertise scarcity and the high cost of data annotation limits the development of large artificial intelligence models. This paper investigates the potential of transfer learning with pre-trained vision-language models (VLMs) in \u2026"}, {"title": "Impact of high-quality, mixed-domain data on the performance of medical language models", "link": "https://academic.oup.com/jamia/advance-article-abstract/doi/10.1093/jamia/ocae120/7680487", "details": "M Griot, C Hemptinne, J Vanderdonckt, D Yuksel - Journal of the American Medical \u2026, 2024", "abstract": "Objective To optimize the training strategy of large language models for medical applications, focusing on creating clinically relevant systems that efficiently integrate into healthcare settings, while ensuring high standards of accuracy and reliability \u2026"}, {"title": "Pseudo-Prompt Generating in Pre-trained Vision-Language Models for Multi-Label Medical Image Classification", "link": "https://arxiv.org/pdf/2405.06468", "details": "Y Ye, J Zhang, H Shi - arXiv preprint arXiv:2405.06468, 2024", "abstract": "The task of medical image recognition is notably complicated by the presence of varied and multiple pathological indications, presenting a unique challenge in multi- label classification with unseen labels. This complexity underlines the need for \u2026"}, {"title": "Super Tiny Language Models", "link": "https://arxiv.org/pdf/2405.14159", "details": "D Hillier, L Guertler, C Tan, P Agrawal, C Ruirui\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancement of large language models (LLMs) has led to significant improvements in natural language processing but also poses challenges due to their high computational and energy demands. This paper introduces a series of research \u2026"}]
