'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Language Models for Text Classification: Is In-Context'
[{"title": "Few-Shot Recalibration of Language Models", "link": "https://arxiv.org/html/2403.18286v1", "details": "XL Li, U Khandelwal, K Guu - arXiv preprint arXiv:2403.18286, 2024", "abstract": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over \u2026"}, {"title": "An Efficient Approach for Studying Cross-Lingual Transfer in Multilingual Language Models", "link": "https://arxiv.org/pdf/2403.20088", "details": "F Faisal, A Anastasopoulos - arXiv preprint arXiv:2403.20088, 2024", "abstract": "The capacity and effectiveness of pre-trained multilingual models (MLMs) for zero- shot cross-lingual transfer is well established. However, phenomena of positive or negative transfer, and the effect of language choice still need to be fully understood \u2026"}, {"title": "A Comparison of Parameter-Efficient ASR Domain Adaptation Methods for Universal Speech and Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10445894/", "details": "KC Sim, Z Huo, T Munkhdalai, N Siddhartha, A Stooke\u2026 - ICASSP 2024-2024 IEEE \u2026, 2024", "abstract": "A recent paradigm shift in artificial intelligence has seen the rise of foundation models, such as the large language models and the universal speech models. With billions of model parameters and trained with a wide range of data, these foundation \u2026"}, {"title": "Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records", "link": "https://arxiv.org/pdf/2403.09226", "details": "A Ziletti, L D'Ambrosi - arXiv preprint arXiv:2403.09226, 2024", "abstract": "Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. Querying these databases to answer epidemiological questions is challenging due to the intricacy of medical \u2026"}, {"title": "DINGO: Towards Diverse and Fine-Grained Instruction-Following Evaluation", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29768/31322", "details": "Z Gu, X Sun, F Lian, Z Kang, C Xu, J Fan - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Instruction-following is particularly crucial for large language models (LLMs) to support diverse user requests. While existing work has made progress in aligning LLMs with human preferences, evaluating their capabilities on instruction-following \u2026"}, {"title": "Single Domain Generalization Method for Remote Sensing Image Segmentation via Category Consistency on Domain Randomization", "link": "https://ieeexplore.ieee.org/abstract/document/10476506/", "details": "C Liang, W Li, Y Dong, WL Fu - IEEE Transactions on Geoscience and Remote \u2026, 2024", "abstract": "Single Domain Generalization (SDG) is a more realistic setting than Domain Generalization (DG) and Domain Adaptation (DA). It aims to train a domain-agnostic model in the presence of a single source domain to perform well on arbitrary unseen \u2026"}, {"title": "Understanding emergent abilities of language models from the loss perspective", "link": "https://arxiv.org/pdf/2403.15796", "details": "Z Du, A Zeng, Y Dong, J Tang - arXiv preprint arXiv:2403.15796, 2024", "abstract": "Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) \u2026"}, {"title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models", "link": "https://arxiv.org/html/2403.19647v1", "details": "S Marks, C Rager, EJ Michaud, Y Belinkov, D Bau\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic \u2026"}, {"title": "Learning To Guide Human Decision Makers With Vision-Language Models", "link": "https://arxiv.org/pdf/2403.16501", "details": "D Banerjee, S Teso, BS Grunel, A Passerini - arXiv preprint arXiv:2403.16501, 2024", "abstract": "There is increasing interest in developing AIs for assisting human decision making in\\textit {high-stakes} tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain.% Mainstream approaches team up an \u2026"}]
