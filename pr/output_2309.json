[{"title": "CASE: Curricular Data Pre-training for Building Generative and Discriminative Assistive Psychology Expert Models", "link": "https://arxiv.org/pdf/2406.00314", "details": "S Harne, MN Choudhury, M Rao, TK Srikanth\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The limited availability of psychologists necessitates efficient identification of individuals requiring urgent mental healthcare. This study explores the use of Natural Language Processing (NLP) pipelines to analyze text data from online mental health \u2026"}, {"title": "Representing Animatable Avatar via Factorized Neural Fields", "link": "https://arxiv.org/pdf/2406.00637", "details": "C Song, Z Wu, B Wandt, L Sigal, H Rhodin - arXiv preprint arXiv:2406.00637, 2024", "abstract": "For reconstructing high-fidelity human 3D models from monocular videos, it is crucial to maintain consistent large-scale body shapes along with finely matched subtle wrinkles. This paper explores the observation that the per-frame rendering results \u2026"}, {"title": "CSAMDT: Conditional Self Attention Memory-Driven Transformers for Radiology Report Generation from Chest X-Ray", "link": "https://link.springer.com/article/10.1007/s10278-024-01126-6", "details": "I Shahzadi, TM Madni, UI Janjua, G Batool, B Naz\u2026 - Journal of Imaging \u2026, 2024", "abstract": "A radiology report plays a crucial role in guiding patient treatment, but writing these reports is a time-consuming task that demands a radiologist's expertise. In response to this challenge, researchers in the subfields of artificial intelligence for healthcare \u2026"}, {"title": "Super Tiny Language Models", "link": "https://arxiv.org/pdf/2405.14159", "details": "D Hillier, L Guertler, C Tan, P Agrawal, C Ruirui\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancement of large language models (LLMs) has led to significant improvements in natural language processing but also poses challenges due to their high computational and energy demands. This paper introduces a series of research \u2026"}, {"title": "SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model", "link": "https://arxiv.org/pdf/2406.01584", "details": "AC Cheng, H Yin, Y Fu, Q Guo, R Yang, J Kautz\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision Language Models (VLMs) have demonstrated remarkable performance in 2D vision and language tasks. However, their ability to reason about spatial arrangements remains limited. In this work, we introduce Spatial Region GPT \u2026"}, {"title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training", "link": "https://arxiv.org/pdf/2405.03133", "details": "Z Zhong, M Xia, D Chen, M Lewis - arXiv preprint arXiv:2405.03133, 2024", "abstract": "Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture, SMEAR, was proposed \u2026"}, {"title": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records", "link": "https://aclanthology.org/2024.cl4health-1.23.pdf", "details": "J Lov\u00f3n-Melgarejo, T Ben-Haddi, J Di Scala\u2026 - Proceedings of the First \u2026, 2024", "abstract": "The lack of standardized evaluation benchmarks in the medical domain for text inputs can be a barrier to widely adopting and leveraging the potential of natural language models for health-related downstream tasks. This paper revisited an \u2026"}, {"title": "Score-based generative models are provably robust: an uncertainty quantification perspective", "link": "https://arxiv.org/pdf/2405.15754", "details": "N Mimikos-Stamatopoulos, BJ Zhang, MA Katsoulakis - arXiv preprint arXiv \u2026, 2024", "abstract": "Through an uncertainty quantification (UQ) perspective, we show that score-based generative models (SGMs) are provably robust to the multiple sources of error in practical implementation. Our primary tool is the Wasserstein uncertainty propagation \u2026"}, {"title": "BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models", "link": "https://arxiv.org/pdf/2405.04756", "details": "CF Luo, A Ghawanmeh, X Zhu, FK Khattak - arXiv preprint arXiv:2405.04756, 2024", "abstract": "Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly. The language model can also \u2026"}]
