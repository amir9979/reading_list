[{"title": "Probing Fundamental Visual Comprehend Capabilities on Vision Language Models via Visual Phrases from Structural Data", "link": "https://link.springer.com/article/10.1007/s12559-024-10351-8", "details": "P Xie, B Liu - Cognitive Computation, 2024", "abstract": "Does the model demonstrate exceptional proficiency in \u201citem counting,\u201d\u201ccolor recognition,\u201d or other Fundamental Visual Comprehension Capability (FVCC)? There have been remarkable advancements in the field of multimodal, the pretrained \u2026"}, {"title": "CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs", "link": "https://arxiv.org/pdf/2408.10433", "details": "Y Ouali, A Bulat, B Martinez, G Tzimiropoulos - arXiv preprint arXiv:2408.10433, 2024", "abstract": "Despite recent successes, LVLMs or Large Vision Language Models are prone to hallucinating details like objects and their properties or relations, limiting their real- world deployment. To address this and improve their robustness, we present CLIP \u2026"}, {"title": "Tuning Vision-Language Models with Multiple Prototypes Clustering", "link": "https://ieeexplore.ieee.org/abstract/document/10679902/", "details": "MH Guo, Y Zhang, TJ Mu, SX Huang, SM Hu - IEEE Transactions on Pattern Analysis \u2026, 2024", "abstract": "Benefiting from advances in large-scale pre-training, foundation models, have demonstrated remarkable capability in the fields of natural language processing, computer vision, among others. However, to achieve expert-level performance in \u2026"}, {"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "link": "https://arxiv.org/pdf/2408.12337", "details": "KS Phogat, SA Puranam, S Dasaratha, C Harsha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain \u2026"}, {"title": "CoDi: Conversational Distillation for Grounded Question Answering", "link": "https://arxiv.org/pdf/2408.11219", "details": "P Huber, A Einolghozati, R Conway, K Narang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Distilling conversational skills into Small Language Models (SLMs) with approximately 1 billion parameters presents significant challenges. Firstly, SLMs have limited capacity in their model parameters to learn extensive knowledge \u2026"}, {"title": "DetoxBench: Benchmarking Large Language Models for Multitask Fraud & Abuse Detection", "link": "https://arxiv.org/pdf/2409.06072", "details": "J Chakraborty, W Xia, A Majumder, D Ma, W Chaabene\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks. However, their practical application in high-stake domains, such as fraud and abuse detection, remains an area that requires further \u2026"}, {"title": "Balancing Diversity and Risk in LLM Sampling: How to Select Your Method and Parameter for Open-Ended Text Generation", "link": "https://arxiv.org/pdf/2408.13586", "details": "Y Zhou, M Keuper, M Fritz - arXiv preprint arXiv:2408.13586, 2024", "abstract": "Sampling-based decoding strategies have been widely adopted for Large Language Models (LLMs) in numerous applications, which target a balance between diversity and quality via temperature tuning and tail truncation (eg, top-k and top-p sampling) \u2026"}, {"title": "Multi-task Multi-attention Transformer for Generative Named Entity Recognition", "link": "https://ieeexplore.ieee.org/abstract/document/10679732/", "details": "Y Mo, J Liu, H Tang, Q Wang, Z Xu, J Wang, X Quan\u2026 - IEEE/ACM Transactions on \u2026, 2024", "abstract": "Most previous sequential labeling models are taskspecific, while recent years have witnessed the rise of generative models due to the advantage of unifying all named entity recognition (NER) tasks into the encoder-decoder framework. Although \u2026"}, {"title": "Towards Harnessing Large Language Models as Autonomous Agents for Semantic Triple Extraction from Unstructured Text", "link": "https://ceur-ws.org/Vol-3747/text2kg_paper1.pdf", "details": "A Ananya, S Tiwari, N Mihindukulasooriya, T Soru\u2026 - 2024", "abstract": "Abstract The use of Large Language Models as autonomous agents interacting with tools has shown to improve the performance of several tasks from code generation to API calling and sequencing. This paper proposes a framework for using Large \u2026"}]
