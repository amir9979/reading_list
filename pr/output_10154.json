[{"title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step", "link": "https://arxiv.org/pdf/2411.10440%3F", "details": "G Xu, P Jin, L Hao, Y Song, L Sun, L Yuan - arXiv preprint arXiv:2411.10440, 2024", "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to \u2026"}, {"title": "Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning", "link": "https://arxiv.org/pdf/2412.08614", "details": "F Lu, W Wu, K Zheng, S Ma, B Gong, J Liu, W Zhai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating detailed captions comprehending text-rich visual content in images has received growing attention for Large Vision-Language Models (LVLMs). However, few studies have developed benchmarks specifically tailored for detailed captions to \u2026"}, {"title": "AdvDreamer Unveils: Are Vision-Language Models Truly Ready for Real-World 3D Variations?", "link": "https://arxiv.org/pdf/2412.03002", "details": "S Ruan, H Liu, Y Huang, X Wang, C Kang, H Su\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision Language Models (VLMs) have exhibited remarkable generalization capabilities, yet their robustness in dynamic real-world scenarios remains largely unexplored. To systematically evaluate VLMs' robustness to real-world 3D variations \u2026"}, {"title": "Progressive Multi-granular Alignments for Grounded Reasoning in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.08125", "details": "QH Le, LH Dang, N Le, T Tran, TM Le - arXiv preprint arXiv:2412.08125, 2024", "abstract": "Existing Large Vision-Language Models (LVLMs) excel at matching concepts across multi-modal inputs but struggle with compositional concepts and high-level relationships between entities. This paper introduces Progressive multi-granular \u2026"}, {"title": "Lifelong Knowledge Editing for Vision Language Models with Low-Rank Mixture-of-Experts", "link": "https://arxiv.org/pdf/2411.15432", "details": "Q Chen, C Wang, D Wang, T Zhang, W Li, X He - arXiv preprint arXiv:2411.15432, 2024", "abstract": "Model editing aims to correct inaccurate knowledge, update outdated information, and incorporate new data into Large Language Models (LLMs) without the need for retraining. This task poses challenges in lifelong scenarios where edits must be \u2026"}, {"title": "An Approach to Complex Visual Data Interpretation with Vision-Language Models", "link": "https://openaccess.thecvf.com/content/ACCV2024W/LAVA/papers/Nguyen_An_Approach_to_Complex_Visual_Data_Interpretation_with_Vision-Language_Models_ACCVW_2024_paper.pdf", "details": "TS Nguyen, VT Huynh, VL Nguyen, MT Tran - \u2026 of the Asian Conference on Computer \u2026, 2024", "abstract": "The LAVA Workshop 2024 challenge aimed to assess the capability of Large Vision- Language Models (VLMs) to interpret and understand complex visual data accurately. This includes intricate visual formats such as data flow diagrams, class \u2026"}, {"title": "Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.06775", "details": "YL Lee, YH Tsai, WC Chiu - arXiv preprint arXiv:2412.06775, 2024", "abstract": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To \u2026"}, {"title": "A cascaded retrieval-while-reasoning multi-document comprehension framework with incremental attention for medical question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0957417424025685", "details": "J Liu, J Ren, R Bai, Z Zhang, Z Lu - Expert Systems with Applications, 2024", "abstract": "Abstract Clinical Machine Reading Comprehension (MRC) is challenging due to the need for medical expertise and comprehensive reasoning chains for diagnosis. This paper introduces a novel cascade retrieval-while-reasoning framework for clinical \u2026"}, {"title": "Align-KD: Distilling Cross-Modal Alignment Knowledge for Mobile Vision-Language Model", "link": "https://arxiv.org/pdf/2412.01282", "details": "Q Feng, W Li, T Lin, X Chen - arXiv preprint arXiv:2412.01282, 2024", "abstract": "Vision-Language Models (VLMs) bring powerful understanding and reasoning capabilities to multimodal tasks. Meanwhile, the great need for capable aritificial intelligence on mobile devices also arises, such as the AI assistant software. Some \u2026"}]
