[{"title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures", "link": "https://arxiv.org/pdf/2410.07698", "details": "Y Chen, Y Zhang, L Cao, K Yuan, Z Wen - arXiv preprint arXiv:2410.07698, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs when adapting large language models (LLMs) for downstream applications. However, traditional first-order (FO) fine-tuning algorithms incur substantial memory overhead \u2026"}, {"title": "Belief in the Machine: Investigating Epistemological Blind Spots of Language Models", "link": "https://arxiv.org/pdf/2410.21195", "details": "M Suzgun, T Gur, F Bianchi, DE Ho, T Icard, D Jurafsky\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As language models (LMs) become integral to fields like healthcare, law, and journalism, their ability to differentiate between fact, belief, and knowledge is essential for reliable decision-making. Failure to grasp these distinctions can lead to \u2026"}, {"title": "Bypassing the Exponential Dependency: Looped Transformers Efficiently Learn In-context by Multi-step Gradient Descent", "link": "https://arxiv.org/pdf/2410.11268", "details": "B Chen, X Li, Y Liang, Z Shi, Z Song - arXiv preprint arXiv:2410.11268, 2024", "abstract": "In-context learning has been recognized as a key factor in the success of Large Language Models (LLMs). It refers to the model's ability to learn patterns on the fly from provided in-context examples in the prompt during inference. Previous studies \u2026"}, {"title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style", "link": "https://arxiv.org/pdf/2410.16184%3F", "details": "Y Liu, Z Yao, R Min, Y Cao, L Hou, J Li - arXiv preprint arXiv:2410.16184, 2024", "abstract": "Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward \u2026"}, {"title": "Beyond Graphs: Can Large Language Models Comprehend Hypergraphs?", "link": "https://arxiv.org/pdf/2410.10083", "details": "Y Feng, C Yang, X Hou, S Du, S Ying, Z Wu, Y Gao - arXiv preprint arXiv:2410.10083, 2024", "abstract": "Existing benchmarks like NLGraph and GraphQA evaluate LLMs on graphs by focusing mainly on pairwise relationships, overlooking the high-order correlations found in real-world data. Hypergraphs, which can model complex beyond-pairwise \u2026"}, {"title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs", "link": "https://arxiv.org/pdf/2410.12881%3F", "details": "SN Akter, S Prabhumoye, J Kamalu, S Satheesh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models (LLMs). Yet, these approaches fall inadequate in complex, multi-hop and \u2026"}, {"title": "Ada-K Routing: Boosting the Efficiency of MoE-based LLMs", "link": "https://arxiv.org/pdf/2410.10456", "details": "T Yue, L Guo, J Cheng, X Gao, J Liu - arXiv preprint arXiv:2410.10456, 2024", "abstract": "In the era of Large Language Models (LLMs), Mixture-of-Experts (MoE) architectures offer a promising approach to managing computational costs while scaling up model parameters. Conventional MoE-based LLMs typically employ static Top-K routing \u2026"}, {"title": "OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2410.09671", "details": "J Wang, M Fang, Z Wan, M Wen, J Zhu, A Liu, Z Gong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this technical report, we introduce OpenR, an open-source framework designed to integrate key components for enhancing the reasoning capabilities of large language models (LLMs). OpenR unifies data acquisition, reinforcement learning training (both \u2026"}, {"title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for Enhancing Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2410.08068", "details": "W Tan, D Chen, J Xue, Z Wang, T Chen - arXiv preprint arXiv:2410.08068, 2024", "abstract": "Large Language Models (LLMs) exhibit impressive performance across various domains but still struggle with arithmetic reasoning tasks. Recent work shows the effectiveness of prompt design methods in enhancing reasoning capabilities \u2026"}]
