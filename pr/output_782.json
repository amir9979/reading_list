'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Adapting transformer-based language models for heart '
[{"title": "Emergent Abilities in Reduced-Scale Generative Language Models", "link": "https://arxiv.org/pdf/2404.02204", "details": "S Muckatira, V Deshpande, V Lialin, A Rumshisky - arXiv preprint arXiv:2404.02204, 2024", "abstract": "Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study \u2026"}, {"title": "Mining Clinical Notes for Physical Rehabilitation Exercise Information: Natural Language Processing Algorithm Development and Validation Study", "link": "https://medinform.jmir.org/2024/1/e52289/", "details": "S Sivarajkumar, F Gao, P Denny, B Aldhahwani\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background: The rehabilitation of a patient who had a stroke requires precise, personalized treatment plans. Natural language processing (NLP) offers the potential to extract valuable exercise information from clinical notes, aiding in the development \u2026"}, {"title": "Question-answering system extracts information on injection drug use from clinical notes", "link": "https://www.nature.com/articles/s43856-024-00470-6", "details": "M Mahbub, I Goethert, I Danciu, K Knight, S Srinivasan\u2026 - Communications Medicine, 2024", "abstract": "Background Injection drug use (IDU) can increase mortality and morbidity. Therefore, identifying IDU early and initiating harm reduction interventions can benefit individuals at risk. However, extracting IDU behaviors from patients' electronic health \u2026"}, {"title": "Self-Supervised Pulse-Aware Interpretable Disentangled ECG Representation Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10447945/", "details": "CT Chou, VS Tseng - ICASSP 2024-2024 IEEE International Conference on \u2026, 2024", "abstract": "Electrocardiography (ECG) is a widely used cardiac measurement for detecting cardiovascular conditions, while self-supervised learning leverages unlabeled data for model pre-training. However, current self-supervised frameworks for ECG signals \u2026"}, {"title": "Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models using Minimal Pairs", "link": "https://arxiv.org/pdf/2403.17299", "details": "L He, P Chen, E Nie, Y Li, JR Brennan - arXiv preprint arXiv:2403.17299, 2024", "abstract": "Inspired by cognitive neuroscience studies, we introduce a noveldecoding probing'method that uses minimal pairs benchmark (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer. By treating the \u2026"}, {"title": "Context versus Prior Knowledge in Language Models", "link": "https://arxiv.org/pdf/2404.04633", "details": "K Du, V Sn\u00e6bjarnarson, N Stoehr, JC White, A Schein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To answer a question, language models often need to integrate prior knowledge learned during pretraining and new information presented in context. We hypothesize that models perform this integration in a predictable way across different \u2026"}, {"title": "Investigating Regularization of Self-Play Language Models", "link": "https://arxiv.org/pdf/2404.04291", "details": "R Alami, A Abubaker, M Achab, MEA Seddik, S Lahlou - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper explores the effects of various forms of regularization in the context of language model alignment via self-play. While both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) require to collect \u2026"}, {"title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models", "link": "https://arxiv.org/pdf/2404.02936", "details": "J Zhang, J Sun, E Yeats, Y Ouyang, M Kuo, J Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. The current state-of-the-art approach, Min-K \u2026"}, {"title": "Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context", "link": "https://arxiv.org/pdf/2404.02000", "details": "A Caubri\u00e8re, E Gauthier - arXiv preprint arXiv:2404.02000, 2024", "abstract": "We present the first self-supervised multilingual speech model trained exclusively on African speech. The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa. On the SSA \u2026"}]
