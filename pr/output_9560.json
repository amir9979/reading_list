[{"title": "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "link": "https://aclanthology.org/2024.findings-emnlp.61.pdf", "details": "X Wei, H Chen, H Yu, H Fei, Q Liu - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have achieved notable success in commonsense reasoning tasks, benefiting from their extensive world knowledge acquired through extensive pretraining. While approaches like Chain-of-Thought \u2026"}, {"title": "Memory-Tuning: A Unified Parameter-Efficient Tuning Method for Pre-trained Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10769026/", "details": "W Qi, R Liu, Y Zuo, F Li, Y Chen, J Wu - IEEE/ACM Transactions on Audio, Speech \u2026, 2024", "abstract": "Conventional fine-tuning encounters increasing difficulties given the size of current Pre-trained Language Models, which makes parameter-efficient tuning become the focal point of frontier research. Recent advances in this field is the unified tuning \u2026"}, {"title": "Can Language Models Learn to Skip Steps?", "link": "https://arxiv.org/pdf/2411.01855%3F", "details": "T Liu, Q Guo, X Hu, C Jiayang, Y Zhang, X Qiu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Trained on vast corpora of human language, language models demonstrate emergent human-like reasoning abilities. Yet they are still far from true intelligence, which opens up intriguing opportunities to explore the parallels of humans and \u2026"}, {"title": "LM2: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.920.pdf", "details": "G Juneja, S Dutta, T Chakraborty - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}, {"title": "Training and Evaluating Language Models with Template-based Data Generation", "link": "https://arxiv.org/pdf/2411.18104", "details": "Y Zhang - arXiv preprint arXiv:2411.18104, 2024", "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, these \u2026"}, {"title": "ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large Language Models for Code Generation", "link": "https://arxiv.org/pdf/2411.07112", "details": "X Jiang, Y Dong, Y Tao, H Liu, Z Jin, W Jiao, G Li - arXiv preprint arXiv:2411.07112, 2024", "abstract": "Large language models (LLMs) have achieved impressive performance in code generation recently, offering programmers revolutionary assistance in software development. However, due to the auto-regressive nature of LLMs, they are \u2026"}, {"title": "CoCoP: Enhancing Text Classification with LLM through Code Completion Prompt", "link": "https://arxiv.org/pdf/2411.08979", "details": "MM Mohajeri, MJ Dousti, MN Ahmadabadi - arXiv preprint arXiv:2411.08979, 2024", "abstract": "Text classification is a fundamental task in natural language processing (NLP), and large language models (LLMs) have demonstrated their capability to perform this task across various domains. However, the performance of LLMs heavily depends on \u2026"}, {"title": "DART-LLM: Dependency-Aware Multi-Robot Task Decomposition and Execution using Large Language Models", "link": "https://arxiv.org/pdf/2411.09022", "details": "Y Wang, R Xiao, JYL Kasahara, R Yajima, K Nagatani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated significant reasoning capabilities in robotic systems. However, their deployment in multi-robot systems remains fragmented and struggles to handle complex task dependencies and \u2026"}, {"title": "Large Language Models Can Self-Improve in Long-context Reasoning", "link": "https://arxiv.org/pdf/2411.08147", "details": "S Li, C Yang, Z Cheng, L Liu, M Yu, Y Yang, W Lam - arXiv preprint arXiv:2411.08147, 2024", "abstract": "Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations \u2026"}]
