[{"title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective", "link": "https://arxiv.org/pdf/2405.16747", "details": "A Tomihari, I Sato - arXiv preprint arXiv:2405.16747, 2024", "abstract": "The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data. This success is largely \u2026"}, {"title": "Leveraging Large Language Models for Knowledge-free Weak Supervision in Clinical Natural Language Processing", "link": "https://arxiv.org/pdf/2406.06723", "details": "E Hsu, K Roberts - arXiv preprint arXiv:2406.06723, 2024", "abstract": "The performance of deep learning-based natural language processing systems is based on large amounts of labeled training data which, in the clinical domain, are not easily available or affordable. Weak supervision and in-context learning offer partial \u2026"}, {"title": "Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?", "link": "https://arxiv.org/pdf/2406.13121", "details": "J Lee, A Chen, Z Dai, D Dua, DS Sachan, M Boratko\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. Leveraging LCLMs' ability to natively ingest and process entire corpora of \u2026"}, {"title": "Memory Augmented Language Models through Mixture of Word Experts", "link": "https://aclanthology.org/2024.naacl-long.249.pdf", "details": "C dos Santos, J Lee-Thorp, I Noble, CC Chang\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Scaling up the number of parameters of language models has proven to be an effective approach to improve performance. For dense models, increasing their size proportionally increases their computational footprint. In this work, we seek to \u2026"}, {"title": "$\\texttt {MoE-RBench} $: Towards Building Reliable Language Models with Sparse Mixture-of-Experts", "link": "https://arxiv.org/pdf/2406.11353", "details": "G Chen, X Zhao, T Chen, Y Cheng - arXiv preprint arXiv:2406.11353, 2024", "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new \u2026"}, {"title": "Dialogue Action Tokens: Steering Language Models in Goal-Directed Dialogue with a Multi-Turn Planner", "link": "https://arxiv.org/pdf/2406.11978", "details": "K Li, Y Wang, F Vi\u00e9gas, M Wattenberg - arXiv preprint arXiv:2406.11978, 2024", "abstract": "We present an approach called Dialogue Action Tokens (DAT) that adapts language model agents to plan goal-directed dialogues. The core idea is to treat each utterance as an action, thereby converting dialogues into games where existing \u2026"}, {"title": "Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt", "link": "https://arxiv.org/pdf/2406.04031", "details": "Z Ying, A Liu, T Zhang, Z Yu, S Liang, X Liu, D Tao - arXiv preprint arXiv:2406.04031, 2024", "abstract": "In the realm of large vision language models (LVLMs), jailbreak attacks serve as a red-teaming approach to bypass guardrails and uncover safety implications. Existing jailbreaks predominantly focus on the visual modality, perturbing solely visual inputs \u2026"}, {"title": "When Parts are Greater Than Sums: Individual LLM Components Can Outperform Full Models", "link": "https://arxiv.org/pdf/2406.13131", "details": "TY Chang, J Thomason, R Jia - arXiv preprint arXiv:2406.13131, 2024", "abstract": "This paper studies in-context learning (ICL) by decomposing the output of large language models into the individual contributions of attention heads and MLPs (components). We observe curious components: good-performing ones that \u2026"}, {"title": "Assessing Real-World Data From Electronic Health Records for Health Technology Assessment: The SUITABILITY Checklist: A Good Practices Report of an ISPOR \u2026", "link": "https://www.valueinhealthjournal.com/article/S1098-3015\\(24\\)00069-X/pdf", "details": "RL Fleurence, S Kent, B Adamson, J Tcheng, R Balicer\u2026 - Value in Health, 2024", "abstract": "Abstract This ISPOR Good Practices report provides a framework for assessing the suitability of electronic health records data for use in health technology assessments (HTAs). Although electronic health record (EHR) data can fill evidence gaps and \u2026"}]
