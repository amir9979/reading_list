[{"title": "Confidence Regulation Neurons in Language Models", "link": "https://arxiv.org/pdf/2406.16254", "details": "A Stolfo, B Wu, W Gurnee, Y Belinkov, X Song\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their widespread use, the mechanisms by which large language models (LLMs) represent and regulate uncertainty in next-token predictions remain largely unexplored. This study investigates two critical components believed to influence this \u2026"}, {"title": "Exploring Safety-Utility Trade-Offs in Personalized Language Models", "link": "https://arxiv.org/pdf/2406.11107", "details": "AR Vijjini, SBR Chowdhury, S Chaturvedi - arXiv preprint arXiv:2406.11107, 2024", "abstract": "As large language models (LLMs) become increasingly integrated into daily applications, it is essential to ensure they operate fairly across diverse user demographics. In this work, we show that LLMs suffer from personalization bias \u2026"}, {"title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models", "link": "https://arxiv.org/pdf/2406.16714", "details": "J Cheng, Y Lu, X Gu, P Ke, X Liu, Y Dong, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Although Large Language Models (LLMs) are becoming increasingly powerful, they still exhibit significant but subtle weaknesses, such as mistakes in instruction- following or coding tasks. As these unexpected errors could lead to severe \u2026"}, {"title": "Unlocking Continual Learning Abilities in Language Models", "link": "https://arxiv.org/pdf/2406.17245", "details": "W Du, S Cheng, T Luo, Z Qiu, Z Huang, KC Cheung\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) exhibit impressive performance and generalization capabilities. However, LMs struggle with the persistent challenge of catastrophic forgetting, which undermines their long-term sustainability in continual learning (CL) \u2026"}, {"title": "ICLEval: Evaluating In-Context Learning Ability of Large Language Models", "link": "https://arxiv.org/pdf/2406.14955", "details": "W Chen, Y Lin, ZH Zhou, HY Huang, Y Jia, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) is a critical capability of Large Language Models (LLMs) as it empowers them to comprehend and reason across interconnected inputs. Evaluating the ICL ability of LLMs can enhance their utilization and deepen our \u2026"}, {"title": "Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?", "link": "https://arxiv.org/pdf/2406.16316", "details": "Y Jinnai - arXiv preprint arXiv:2406.16316, 2024", "abstract": "Alignment of the language model with human preferences is a common approach to making a language model useful to end users. However, most alignment work is done in English, and human preference datasets are dominated by English \u2026"}, {"title": "TAGLAS: An atlas of text-attributed graph datasets in the era of large graph and language models", "link": "https://arxiv.org/pdf/2406.14683", "details": "J Feng, H Liu, L Kong, Y Chen, M Zhang - arXiv preprint arXiv:2406.14683, 2024", "abstract": "In this report, we present TAGLAS, an atlas of text-attributed graph (TAG) datasets and benchmarks. TAGs are graphs with node and edge features represented in text, which have recently gained wide applicability in training graph-language or graph \u2026"}, {"title": "Multilingual Nonce Dependency Treebanks: Understanding how Language Models Represent and Process Syntactic Structure", "link": "https://aclanthology.org/2024.naacl-long.433.pdf", "details": "D Arps, L Kallmeyer, Y Samih, H Sajjad - Proceedings of the 2024 Conference of the \u2026, 2024", "abstract": "Abstract We introduce SPUD (Semantically Perturbed Universal Dependencies), a framework for creating nonce treebanks for the multilingual Universal Dependencies (UD) corpora. SPUD data satisfies syntactic argument structure, provides syntactic \u2026"}, {"title": "LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models", "link": "https://aclanthology.org/2024.naacl-long.360.pdf", "details": "Y Xu, W Wang - Proceedings of the 2024 Conference of the North \u2026, 2024", "abstract": "Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks \u2026"}]
