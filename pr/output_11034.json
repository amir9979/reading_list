[{"title": "Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models", "link": "https://arxiv.org/pdf/2412.16545", "details": "Z Zhang, Y Wang, X Huang, T Fang, H Zhang, C Deng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models have shown remarkable performance across a wide range of language tasks, owing to their exceptional capabilities in context modeling. The most commonly used method of context modeling is full self-attention, as seen in \u2026"}, {"title": "A pen mark is all you need-Incidental prompt injection attacks on Vision Language Models in real-life histopathology", "link": "https://www.medrxiv.org/content/10.1101/2024.12.11.24318840.full.pdf", "details": "J Clusmann, SJK Schulz, D Ferber, IC Wiest\u2026 - medRxiv, 2024", "abstract": "Vision-language models (VLMs) can analyze multimodal medical data. However, a significant weakness of VLMs, as we have recently described, is their susceptibility to prompt injection attacks. Here, the model receives conflicting instructions, leading to \u2026"}, {"title": "Generalizing Fairness to Generative Language Models via Reformulation of Non-discrimination Criteria", "link": "https://orbit.dtu.dk/files/382532948/2403.08564v3.pdf", "details": "S Sterlie, N Weng, A Feragen - Fairness and ethics towards transparent AI: facing the \u2026, 2024", "abstract": "Generative AI, such as large language models, has undergone rapid development within recent years. As these models become increasingly available to the public, concerns arise about perpetuating and amplifying harmful biases in applications \u2026"}, {"title": "Weak-to-Strong Generalization Through the Data-Centric Lens", "link": "https://arxiv.org/pdf/2412.03881%3F", "details": "C Shin, J Cooper, F Sala - arXiv preprint arXiv:2412.03881, 2024", "abstract": "The weak-to-strong generalization phenomenon is the driver for important machine learning applications including highly data-efficient learning and, most recently, performing superalignment. While decades of research have resulted in numerous \u2026"}, {"title": "A comprehensive analysis of gender, racial, and prompt-induced biases in large language models", "link": "https://link.springer.com/article/10.1007/s41060-024-00696-6", "details": "N Torres, C Ulloa, I Araya, M Ayala, S Jara - International Journal of Data Science and \u2026, 2024", "abstract": "This study presents a comprehensive analysis of gender and racial bias in large language models (LLMs), examining their manifestation across different model versions, languages, and modalities. Through a series of eight experiments, we \u2026"}, {"title": "Prompting Large Language Models for Clinical Temporal Relation Extraction", "link": "https://arxiv.org/pdf/2412.04512", "details": "J He, L Rasmy, H Li, J Li, Z Sun, E Yu, D Zhi, C Tao - arXiv preprint arXiv:2412.04512, 2024", "abstract": "Objective: This paper aims to prompt large language models (LLMs) for clinical temporal relation extraction (CTRE) in both few-shot and fully supervised settings. Materials and Methods: This study utilizes four LLMs: Encoder-based GatorTron \u2026"}, {"title": "LLM2: Let Large Language Models Harness System 2 Reasoning", "link": "https://arxiv.org/pdf/2412.20372", "details": "C Yang, C Shi, S Li, B Shui, Y Yang, W Lam - arXiv preprint arXiv:2412.20372, 2024", "abstract": "Large language models (LLMs) have exhibited impressive capabilities across a myriad of tasks, yet they occasionally yield undesirable outputs. We posit that these limitations are rooted in the foundational autoregressive architecture of LLMs, which \u2026"}]
