[{"title": "Language Models Encode the Value of Numbers Linearly", "link": "https://aclanthology.org/2025.coling-main.47.pdf", "details": "F Zhu, D Dai, Z Sui - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Large language models (LLMs) have exhibited impressive competence in various tasks, but their internal mechanisms on mathematical problems are still under- explored. In this paper, we study a fundamental question: how language models \u2026"}, {"title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models", "link": "https://arxiv.org/pdf/2501.05752", "details": "S Lee, H Park, J Kim, J Ok - arXiv preprint arXiv:2501.05752, 2025", "abstract": "Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer \u2026"}, {"title": "Factual Knowledge Assessment of Language Models Using Distractors", "link": "https://aclanthology.org/2025.coling-main.537.pdf", "details": "HA Khodja, F Bechet, Q Brabant, A Nasr, G Lecorv\u00e9 - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract Language models encode extensive factual knowledge within their parameters. The accurate assessment of this knowledge is crucial for understanding and improving these models. In the literature, factual knowledge assessment often \u2026"}, {"title": "Foundations of Large Language Models", "link": "https://arxiv.org/pdf/2501.09223", "details": "T Xiao, J Zhu - arXiv preprint arXiv:2501.09223, 2025", "abstract": "This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting- edge technologies. The book is structured into four main chapters, each exploring a \u2026"}, {"title": "CALM: Curiosity-Driven Auditing for Large Language Models", "link": "https://arxiv.org/pdf/2501.02997%3F", "details": "X Zheng, L Wang, Y Liu, X Ma, C Shen, C Wang - arXiv preprint arXiv:2501.02997, 2025", "abstract": "Auditing Large Language Models (LLMs) is a crucial and challenging task. In this study, we focus on auditing black-box LLMs without access to their parameters, only to the provided service. We treat this type of auditing as a black-box optimization \u2026"}, {"title": "Cascaded Self-Evaluation Augmented Training for Efficient Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2501.05662", "details": "Z Lv, W Wang, J Wang, S Zhang, F Wu - arXiv preprint arXiv:2501.05662, 2025", "abstract": "Efficient Multimodal Large Language Models (EMLLMs) have rapidly advanced recently. Incorporating Chain-of-Thought (CoT) reasoning and step-by-step self- evaluation has improved their performance. However, limited parameters often \u2026"}, {"title": "Topology-of-Question-Decomposition: Enhancing Large Language Models with Information Retrieval for Knowledge-Intensive Tasks", "link": "https://aclanthology.org/2025.coling-main.191.pdf", "details": "W Li, J Wang, LC Yu, X Zhang - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Large language models (LLMs) are increasingly deployed for general problem- solving across various domains yet remain constrained to chaining immediate reasoning steps and depending solely on parametric knowledge. Integrating an \u2026"}, {"title": "Chain-of-Specificity: Enhancing Task-Specific Constraint Adherence in Large Language Models", "link": "https://aclanthology.org/2025.coling-main.164.pdf", "details": "K Wei, J Zhong, H Zhang, F Zhang, D Zhang, L Jin\u2026 - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract Large Language Models (LLMs) exhibit remarkable generative capabilities, enabling the generation of valuable information. Despite these advancements, previous research found that LLMs sometimes struggle with adhering to specific \u2026"}, {"title": "ArithmeticGPT: empowering small-size large language models with advanced arithmetic skills", "link": "https://link.springer.com/article/10.1007/s10994-024-06681-1", "details": "Z Liu, Y Zheng, Z Yin, J Chen, T Liu, M Tian, W Luo - Machine Learning, 2025", "abstract": "Large language models (LLMs) have shown remarkable capabilities in understanding and generating language across a wide range of domains. However, their performance in advanced arithmetic calculation remains a significant challenge \u2026"}]
