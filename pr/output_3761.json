[{"title": "KU-DMIS at MEDIQA-CORR 2024: Exploring the Reasoning Capabilities of Small Language Models in Medical Error Correction", "link": "https://aclanthology.org/2024.clinicalnlp-1.51.pdf", "details": "H Hwang, T Lee, H Kim, J Kang - Proceedings of the 6th Clinical Natural Language \u2026, 2024", "abstract": "Recent advancements in large language models (LM) like OpenAI's GPT-4 have shown promise in healthcare, particularly in medical question answering and clinical applications. However, their deployment raises privacy concerns and their size limits \u2026"}, {"title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models", "link": "https://arxiv.org/pdf/2407.06460", "details": "W Shi, J Lee, Y Huang, S Malladi, J Zhao, A Holtzman\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly \u2026"}, {"title": "Memory Augmented Language Models through Mixture of Word Experts", "link": "https://aclanthology.org/2024.naacl-long.249.pdf", "details": "C dos Santos, J Lee-Thorp, I Noble, CC Chang\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Scaling up the number of parameters of language models has proven to be an effective approach to improve performance. For dense models, increasing their size proportionally increases their computational footprint. In this work, we seek to \u2026"}, {"title": "ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models", "link": "https://arxiv.org/pdf/2406.13342", "details": "H Jo, H Lee, T Park - arXiv preprint arXiv:2406.13342, 2024", "abstract": "The recent advancements in large language models (LLMs) have brought significant progress in solving NLP tasks. Notably, in-context learning (ICL) is the key enabling mechanism for LLMs to understand specific tasks and grasping nuances. In this \u2026"}, {"title": "Learning to Complement and to Defer to Multiple Users", "link": "https://arxiv.org/pdf/2407.07003", "details": "Z Zhang, W Ai, K Wells, D Rosewarne, TT Do\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the development of Human-AI Collaboration in Classification (HAI-CC), integrating users and AI predictions becomes challenging due to the complex decision-making process. This process has three options: 1) AI autonomously \u2026"}, {"title": "A Survey on Human Preference Learning for Large Language Models", "link": "https://arxiv.org/pdf/2406.11191", "details": "R Jiang, K Chen, X Bai, Z He, J Li, M Yang, T Zhao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The recent surge of versatile large language models (LLMs) largely depends on aligning increasingly capable foundation models with human intentions by preference learning, enhancing LLMs with excellent applicability and effectiveness in \u2026"}, {"title": "Evaluating machine learning approaches for multi-label classification of unstructured electronic health records with a generative large language model", "link": "https://www.medrxiv.org/content/10.1101/2024.06.24.24309441.full.pdf", "details": "D Vithanage, C Deng, L Wang, M Yin, M Alkhalaf\u2026 - medRxiv, 2024", "abstract": "Multi-label classification of unstructured electronic health records (EHR) poses challenges due to the inherent semantic complexity in textual data. Advances in natural language processing (NLP) using large language models (LLMs) show \u2026"}, {"title": "Large Language Models are Interpretable Learners", "link": "https://arxiv.org/pdf/2406.17224", "details": "R Wang, S Si, F Yu, D Wiesmann, CJ Hsieh, I Dhillon - arXiv preprint arXiv \u2026, 2024", "abstract": "The trade-off between expressiveness and interpretability remains a core challenge when building human-centric predictive models for classification and decision- making. While symbolic rules offer interpretability, they often lack expressiveness \u2026"}, {"title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs", "link": "https://arxiv.org/pdf/2407.00653", "details": "Y Zhang, X Wang, J Liang, S Xia, L Chen, Y Xiao - arXiv preprint arXiv:2407.00653, 2024", "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new \u2026"}]
