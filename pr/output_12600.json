[{"title": "Efficient extraction of medication information from clinical notes: an evaluation in two languages", "link": "https://arxiv.org/pdf/2502.03257", "details": "T Fabacher, EA Sauleau, E Arcay, B Faye, M Alter\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Objective: To evaluate the accuracy, computational cost and portability of a new Natural Language Processing (NLP) method for extracting medication information from clinical narratives. Materials and Methods: We propose an original transformer \u2026"}, {"title": "SmolLM2: When Smol Goes Big--Data-Centric Training of a Small Language Model", "link": "https://arxiv.org/pdf/2502.02737", "details": "LB Allal, A Lozhkov, E Bakouch, GM Bl\u00e1zquez\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While large language models have facilitated breakthroughs in many applications of artificial intelligence, their inherent largeness makes them computationally expensive and challenging to deploy in resource-constrained settings. In this paper, we \u2026"}, {"title": "Knowing When to Stop: Dynamic Context Cutoff for Large Language Models", "link": "https://arxiv.org/pdf/2502.01025", "details": "R Xie, J Wang, P Rosu, C Deng, B Sun, Z Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) process entire input contexts indiscriminately, which is inefficient in cases where the information required to answer a query is localized within the context. We present dynamic context cutoff, a human-inspired method \u2026"}]
