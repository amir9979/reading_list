[{"title": "CyberRL: Brain-Inspired Reinforcement Learning for Efficient Network Intrusion Detection", "link": "https://ieeexplore.ieee.org/abstract/document/10579883/", "details": "MA Issa, H Chen, J Wang, M Imani - IEEE Transactions on Computer-Aided Design of \u2026, 2024", "abstract": "Due to the rapidly evolving landscape of cybersecurity, the risks in securing cloud networks and devices are attesting to be an increasingly prevalent research challenge. Reinforcement learning is a subfield of machine learning that has \u2026"}, {"title": "MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction", "link": "https://arxiv.org/pdf/2406.12950", "details": "Y Liu, S Ding, S Zhou, W Fan, Q Tan - arXiv preprint arXiv:2406.12950, 2024", "abstract": "Molecular property prediction (MPP) is a fundamental and crucial task in drug discovery. However, prior methods are limited by the requirement for a large number of labeled molecules and their restricted ability to generalize for unseen and new \u2026"}, {"title": "GenderBias-\\emph {VL}: Benchmarking Gender Bias in Vision Language Models via Counterfactual Probing", "link": "https://arxiv.org/pdf/2407.00600", "details": "Y Xiao, A Liu, QJ Cheng, Z Yin, S Liang, J Li, J Shao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) have been widely adopted in various applications; however, they exhibit significant gender biases. Existing benchmarks primarily evaluate gender bias at the demographic group level, neglecting individual \u2026"}, {"title": "Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2407.00569", "details": "W Zhong, X Feng, L Zhao, Q Li, L Huang, Y Gu, W Ma\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations \u2026"}, {"title": "From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models", "link": "https://arxiv.org/pdf/2407.00263", "details": "M Bhatia, S Ravi, A Chinchure, E Hwang, V Shwartz - arXiv preprint arXiv:2407.00263, 2024", "abstract": "Despite recent advancements in vision-language models, their performance remains suboptimal on images from non-western cultures due to underrepresentation in training datasets. Various benchmarks have been proposed to test models' cultural \u2026"}, {"title": "GEARS: Generalizable Multi-Purpose Embeddings for Gaze and Hand Data in VR Interactions", "link": "https://dl.acm.org/doi/pdf/10.1145/3627043.3659551", "details": "P Hallgarten, N Sendhilnathan, T Zhang, E Sood\u2026 - Proceedings of the 32nd \u2026, 2024", "abstract": "Machine learning models using users' gaze and hand data to encode user interaction behavior in VR are often tailored to a single task and sensor set, limiting their applicability in settings with constrained compute resources. We propose \u2026"}]
