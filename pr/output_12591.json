[{"title": "Factual Knowledge Assessment of Language Models Using Distractors", "link": "https://aclanthology.org/2025.coling-main.537.pdf", "details": "HA Khodja, F Bechet, Q Brabant, A Nasr, G Lecorv\u00e9 - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract Language models encode extensive factual knowledge within their parameters. The accurate assessment of this knowledge is crucial for understanding and improving these models. In the literature, factual knowledge assessment often \u2026"}, {"title": "Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration", "link": "https://arxiv.org/pdf/2502.01969", "details": "Y Zhu, L Tao, M Dong, C Xu - arXiv preprint arXiv:2502.01969, 2025", "abstract": "Large Vision-Language Models (LVLMs) exhibit impressive multimodal reasoning capabilities but remain highly susceptible to object hallucination, where models generate responses that are not factually aligned with the visual content. Recent \u2026"}, {"title": "RelCAT: Advancing Extraction of Clinical Inter-Entity Relationships from Unstructured Electronic Health Records", "link": "https://arxiv.org/pdf/2501.16077", "details": "S Agarwal, V Dinu, T Searle, M Ratas, A Shek, DF Stein\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This study introduces RelCAT (Relation Concept Annotation Toolkit), an interactive tool, library, and workflow designed to classify relations between entities extracted from clinical narratives. Building upon the CogStack MedCAT framework, RelCAT \u2026"}, {"title": "Knowledge Enhanced Language Model for Biomedical Natural Language Processing: Introducing a New Language Model for BioNLP", "link": "https://ieeexplore.ieee.org/abstract/document/10836827/", "details": "U Naseem, Q Zhang, L Hu, S Hussain, S Wang - IEEE Systems, Man, and \u2026, 2025", "abstract": "Following the success of pre-trained language models (PLMs), the biomedical research community has presented various domain-specific PLMs trained on a large biomedical and clinical corpus for biomedical natural language processing (BioNLP) \u2026"}, {"title": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models", "link": "https://arxiv.org/pdf/2501.17420", "details": "Y Li, H Shirado, S Das - arXiv preprint arXiv:2501.17420, 2025", "abstract": "While advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating \u2026"}, {"title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models", "link": "https://arxiv.org/pdf/2501.05752", "details": "S Lee, H Park, J Kim, J Ok - arXiv preprint arXiv:2501.05752, 2025", "abstract": "Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer \u2026"}, {"title": "Scaling Laws for Upcycling Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2502.03009", "details": "SP Liew, T Kato, S Takase - arXiv preprint arXiv:2502.03009, 2025", "abstract": "Pretraining large language models (LLMs) is resource-intensive, often requiring months of training time even with high-end GPU clusters. There are two approaches of mitigating such computational demands: reusing smaller models to train larger \u2026"}, {"title": "Leveraging large language models to generate clinical histories for oncologic imaging requisitions", "link": "https://pubs.rsna.org/doi/abs/10.1148/radiol.242134", "details": "R Bhayana, O Alwahbi, AM Ladak, Y Deng\u2026 - Radiology, 2025", "abstract": "Background Clinical information improves imaging interpretation, but physician- provided histories on requisitions for oncologic imaging often lack key details. Purpose To evaluate large language models (LLMs) for automatically generating \u2026"}, {"title": "Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models", "link": "https://arxiv.org/pdf/2501.08248%3F", "details": "Y Qiu, V Embar, Y Zhang, N Jaitly, SB Cohen, B Han - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in long-context language models (LCLMs) promise to transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With their expanded context windows, LCLMs can process entire knowledge bases and \u2026"}]
