[{"title": "PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.09613%3F", "details": "C Yang, X Dong, X Zhu, W Su, J Wang, H Tian, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (VLMs) have been extended to understand both images and videos. Visual token compression is leveraged to reduce the considerable token length of visual inputs. To meet the needs of different tasks \u2026"}, {"title": "ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models", "link": "https://arxiv.org/pdf/2412.00447", "details": "X Ye, Y Gan, Y Ge, XP Zhang, Y Tang - arXiv preprint arXiv:2412.00447, 2024", "abstract": "Large Vision Language Models (LVLMs) have achieved significant success across multi-modal tasks. However, the computational cost of processing long visual tokens can be prohibitively expensive on resource-limited devices. Previous methods have \u2026"}, {"title": "CTPT: Continual Test-time Prompt Tuning for vision-language models", "link": "https://www.sciencedirect.com/science/article/pii/S0031320324010513", "details": "F Wang, Z Han, X Liu, Y Yin, X Gao - Pattern Recognition, 2024", "abstract": "Abstract Test-time Prompt Tuning (TPT) aims to further enhance the generalization capabilities of pre-trained vision-language models, eg, CLIP, on streaming test samples from a new distribution. Current TPT methods primarily utilize self-training \u2026"}, {"title": "Compositional Image Retrieval via Instruction-Aware Contrastive Learning", "link": "https://arxiv.org/pdf/2412.05756", "details": "W Zhong, W An, F Jiang, H Ma, Y Guo, J Huang - arXiv preprint arXiv:2412.05756, 2024", "abstract": "Composed Image Retrieval (CIR) involves retrieving a target image based on a composed query of an image paired with text that specifies modifications or changes to the visual reference. CIR is inherently an instruction-following task, as the model \u2026"}, {"title": "Espresso: High Compression For Rich Extraction From Videos for Your Vision-Language Model", "link": "https://arxiv.org/pdf/2412.04729", "details": "KP Yu, A Dave, R Ambrus, J Mercat - arXiv preprint arXiv:2412.04729, 2024", "abstract": "Most of the current vision-language models (VLMs) for videos struggle to understand videos longer than a few seconds. This is primarily due to the fact that they do not scale to utilizing a large number of frames. In order to address this limitation, we \u2026"}, {"title": "Efficient and Comprehensive Feature Extraction in Large Vision-Language Model for Clinical Pathology Analysis", "link": "https://arxiv.org/pdf/2412.09521", "details": "S Zhang, W Li, T Gao, J Hu, H Luo, M Song, X Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pathological diagnosis is vital for determining disease characteristics, guiding treatment, and assessing prognosis, relying heavily on detailed, multi-scale analysis of high-resolution whole slide images (WSI). However, traditional pure vision models \u2026"}, {"title": "Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers", "link": "https://arxiv.org/pdf/2412.00142", "details": "C Mitra, B Huang, T Chai, Z Lin, A Arbelle, R Feris\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a wide variety of vision-language (VL) tasks such as image captioning or visual question answering. Despite strong performance, LMMs are not directly suited for \u2026"}, {"title": "ConMix: Contrastive Learning with Mixup Augmentation for Dialogue Summarization", "link": "https://link.springer.com/chapter/10.1007/978-981-96-0847-8_18", "details": "Z Chen, J Xiao - International Conference on Advanced Data Mining \u2026, 2024", "abstract": "Seq2seq models have achieved remarkable performance on dialogue summarization, but the exposure bias problem still remains. Contrastive learning has been widely adopted to address this issue. However, previous contrastive learning \u2026"}, {"title": "Text-Guided Zero-Shot 3D Style Transfer of Neural Radiance Fields", "link": "https://link.springer.com/chapter/10.1007/978-3-031-78186-5_9", "details": "W Li, WS Zheng - International Conference on Pattern Recognition, 2024", "abstract": "Abstract 3D style transfer aims to generate novel, stylized views while maintaining multi-view consistency. However, current approaches primarily focus on uniformly stylizing entire 3D scenes, limiting the versatility of 3D style transfer. To address this \u2026"}]
