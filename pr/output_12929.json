[{"title": "CrossVideoMAE: Self-Supervised Image-Video Representation Learning with Masked Autoencoders", "link": "https://arxiv.org/pdf/2502.07811", "details": "SA Ahamed, M Gunawardhana, L David, M Sidorov\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Current video-based Masked Autoencoders (MAEs) primarily focus on learning effective spatiotemporal representations from a visual perspective, which may lead the model to prioritize general spatial-temporal patterns but often overlook nuanced \u2026"}, {"title": "An Experimental Study on Exploring Strong Lightweight Vision Transformers via Masked Image Modeling Pre-training", "link": "https://link.springer.com/article/10.1007/s11263-024-02327-w", "details": "J Gao, S Lin, S Wang, Y Kou, Z Li, L Li, C Zhang\u2026 - International Journal of \u2026, 2025", "abstract": "Masked image modeling (MIM) pre-training for large-scale vision transformers (ViTs) has enabled promising downstream performance on top of the learned self- supervised ViT features. In this paper, we question if the extremely simple lightweight \u2026"}, {"title": "MAA: Meticulous Adversarial Attack against Vision-Language Pre-trained Models", "link": "https://arxiv.org/pdf/2502.08079", "details": "PF Zhang, G Bai, Z Huang - arXiv preprint arXiv:2502.08079, 2025", "abstract": "Current adversarial attacks for evaluating the robustness of vision-language pre- trained (VLP) models in multi-modal tasks suffer from limited transferability, where attacks crafted for a specific model often struggle to generalize effectively across \u2026"}, {"title": "$\\texttt {LucidAtlas} $: Learning Uncertainty-Aware, Covariate-Disentangled, Individualized Atlas Representations", "link": "https://arxiv.org/pdf/2502.08445", "details": "Y Jiao, S Bhamidi, H Qu, C Zdanski, J Kimbell, A Prince\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The goal of this work is to develop principled techniques to extract information from high dimensional data sets with complex dependencies in areas such as medicine that can provide insight into individual as well as population level variation. We \u2026"}, {"title": "Discovering Chunks in Neural Embeddings for Interpretability", "link": "https://arxiv.org/pdf/2502.01803", "details": "S Wu, S Alaniz, E Schulz, Z Akata - arXiv preprint arXiv:2502.01803, 2025", "abstract": "Understanding neural networks is challenging due to their high-dimensional, interacting components. Inspired by human cognition, which processes complex sensory data by chunking it into recurring entities, we propose leveraging this \u2026"}, {"title": "Slot-Guided Adaptation of Pre-trained Diffusion Models for Object-Centric Learning and Compositional Generation", "link": "https://arxiv.org/pdf/2501.15878", "details": "AK Akan, Y Yemez - arXiv preprint arXiv:2501.15878, 2025", "abstract": "We present SlotAdapt, an object-centric learning method that combines slot attention with pretrained diffusion models by introducing adapters for slot-based conditioning. Our method preserves the generative power of pretrained diffusion models, while \u2026"}]
