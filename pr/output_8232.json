[{"title": "AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation", "link": "https://arxiv.org/pdf/2410.00371", "details": "J Duan, W Pumacay, N Kumar, YR Wang, S Tian\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Robotic manipulation in open-world settings requires not only task execution but also the ability to detect and learn from failures. While recent advances in vision-language models (VLMs) and large language models (LLMs) have improved robots' spatial \u2026"}, {"title": "BUMBLE: Unifying Reasoning and Acting with Vision-Language Models for Building-wide Mobile Manipulation", "link": "https://arxiv.org/pdf/2410.06237", "details": "R Shah, A Yu, Y Zhu, Y Zhu, R Mart\u00edn-Mart\u00edn - arXiv preprint arXiv:2410.06237, 2024", "abstract": "To operate at a building scale, service robots must perform very long-horizon mobile manipulation tasks by navigating to different rooms, accessing different floors, and interacting with a wide and unseen range of everyday objects. We refer to these \u2026"}]
