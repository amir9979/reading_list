[{"title": "Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models", "link": "https://ieeexplore.ieee.org/iel8/42/4359023/11021425.pdf", "details": "C Lian, HY Zhou, D Liang, J Qin, L Wang - IEEE Transactions on Medical Imaging, 2025", "abstract": "Medical vision-language alignment through cross-modal contrastive learning shows promising performance in image-text matching tasks, such as retrieval and zero-shot classification. However, conventional cross-modal contrastive learning (CLIP-based) \u2026"}, {"title": "Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models", "link": "https://arxiv.org/pdf/2505.19037", "details": "KH Lu, CY Kuan, H Lee - arXiv preprint arXiv:2505.19037, 2025", "abstract": "We introduce Speech-IFeval, an evaluation framework designed to assess instruction-following capabilities and quantify catastrophic forgetting in speech- aware language models (SLMs). Recent SLMs integrate speech perception with \u2026", "entry_id": "http://arxiv.org/abs/2505.19037v1", "updated": "2025-05-25 08:37:55", "published": "2025-05-25 08:37:55", "authors": "Ke-Han Lu;Chun-Yi Kuan;Hung-yi Lee", "summary": "We introduce Speech-IFeval, an evaluation framework designed to assess\ninstruction-following capabilities and quantify catastrophic forgetting in\nspeech-aware language models (SLMs). Recent SLMs integrate speech perception\nwith large language models (LLMs), often degrading textual capabilities due to\nspeech-centric training. Existing benchmarks conflate speech perception with\ninstruction-following, hindering evaluation of these distinct skills. To\naddress this gap, we provide a benchmark for diagnosing the\ninstruction-following abilities of SLMs. Our findings show that most SLMs\nstruggle with even basic instructions, performing far worse than text-based\nLLMs. Additionally, these models are highly sensitive to prompt variations,\noften yielding inconsistent and unreliable outputs. We highlight core\nchallenges and provide insights to guide future research, emphasizing the need\nfor evaluation beyond task-level metrics.", "comment": "Accecpted by Interspeech 2025;\n  https://github.com/kehanlu/Speech-IFEval", "journal_ref": null, "primary_category": "eess.AS", "categories": "eess.AS;cs.CL", "links": "http://arxiv.org/abs/2505.19037v1;http://arxiv.org/pdf/2505.19037v1", "pdf_url": "http://arxiv.org/pdf/2505.19037v1"}, {"title": "MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning", "link": "https://arxiv.org/pdf/2506.00555", "details": "P Xia, J Wang, Y Peng, K Zeng, X Wu, X Tang, H Zhu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent \u2026", "entry_id": "http://arxiv.org/abs/2506.00555v1", "updated": "2025-05-31 13:22:55", "published": "2025-05-31 13:22:55", "authors": "Peng Xia;Jinglu Wang;Yibo Peng;Kaide Zeng;Xian Wu;Xiangru Tang;Hongtu Zhu;Yun Li;Shujie Liu;Yan Lu;Huaxiu Yao", "summary": "Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential\nin multimodal diagnostic tasks. However, existing single-agent models struggle\nto generalize across diverse medical specialties, limiting their performance.\nRecent efforts introduce multi-agent collaboration frameworks inspired by\nclinical workflows, where general practitioners (GPs) and specialists interact\nin a fixed sequence. Despite improvements, these static pipelines lack\nflexibility and adaptability in reasoning. To address this, we propose\nMMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that\nenables dynamic, optimized collaboration among medical agents. Specifically, we\ntrain two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to\nassign patients to appropriate specialties, while the attending physician\nintegrates the judgments from multi-specialists and its own knowledge to make\nfinal decisions. To address the inconsistency in specialist outputs, we\nintroduce a curriculum learning (CL)-guided RL strategy that progressively\nteaches the attending physician to balance between imitating specialists and\ncorrecting their mistakes. Experiments on five medical VQA benchmarks\ndemonstrate that MMedAgent-RL not only outperforms both open-source and\nproprietary Med-LVLMs, but also exhibits human-like reasoning patterns.\nNotably, it achieves an average performance gain of 18.4% over supervised\nfine-tuning baselines.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CL;cs.CV", "links": "http://arxiv.org/abs/2506.00555v1;http://arxiv.org/pdf/2506.00555v1", "pdf_url": "http://arxiv.org/pdf/2506.00555v1"}, {"title": "Synthetic Visual Genome", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Park_Synthetic_Visual_Genome_CVPR_2025_paper.pdf", "details": "JS Park, Z Ma, L Li, C Zheng, CY Hsieh, X Lu\u2026 - Proceedings of the \u2026, 2025", "abstract": "Abstract Reasoning over visual relationships--spatial, functional, interactional, social, etc.--are considered to be a fundamental component of human cognition. Yet, despite the major advances in visual comprehension in multimodal language models \u2026"}, {"title": "ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases", "link": "https://arxiv.org/pdf/2506.00095", "details": "Y Li, X Zeng, C Fang, J Yang, L Zhang - arXiv preprint arXiv:2506.00095, 2025", "abstract": "Hepato-pancreato-biliary (HPB) disorders represent a global public health challenge due to their high morbidity and mortality. Although large language models (LLMs) have shown promising performance in general medical question-answering tasks \u2026", "entry_id": "http://arxiv.org/abs/2506.00095v3", "updated": "2025-06-04 03:25:49", "published": "2025-05-30 11:35:05", "authors": "Yuchong Li;Xiaojun Zeng;Chihua Fang;Jian Yang;Fucang Jia;Lei Zhang", "summary": "Hepato-pancreato-biliary (HPB) disorders represent a global public health\nchallenge due to their high morbidity and mortality. Although large language\nmodels (LLMs) have shown promising performance in general medical\nquestion-answering tasks, the current evaluation benchmarks are mostly derived\nfrom standardized examinations or manually designed questions, lacking HPB\ncoverage and clinical cases. To address these issues, we systematically\neatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended\nmultiple-choice questions and 337 open-ended real diagnosis cases, which\nencompasses all the 33 main categories and 465 subcategories of HPB diseases\ndefined in the International Statistical Classification of Diseases, 10th\nRevision (ICD-10). The multiple-choice questions are curated from public\ndatasets and synthesized data, and the clinical cases are collected from\nprestigious medical journals, case-sharing platforms, and collaborating\nhospitals. By evalauting commercial and open-source general and medical LLMs on\nour established benchmark, namely ClinBench-HBP, we find that while commercial\nLLMs perform competently on medical exam questions, they exhibit substantial\nperformance degradation on HPB diagnosis tasks, especially on complex,\ninpatient clinical cases. Those medical LLMs also show limited generalizability\nto HPB diseases. Our results reveal the critical limitations of current LLMs in\nthe domain of HPB diseases, underscoring the imperative need for future medical\nLLMs to handle real, complex clinical diagnostics rather than simple medical\nexam questions. The benchmark will be released at\nhttps://clinbench-hpb.github.io.", "comment": null, "journal_ref": null, "primary_category": "cs.CY", "categories": "cs.CY;cs.AI;cs.CL", "links": "http://arxiv.org/abs/2506.00095v3;http://arxiv.org/pdf/2506.00095v3", "pdf_url": "http://arxiv.org/pdf/2506.00095v3"}, {"title": "Knowledge-enhanced Parameter-efficient Transfer Learning with METER for medical vision-language tasks", "link": "https://www.sciencedirect.com/science/article/pii/S1532046425000693", "details": "X Liang, J Xie, J Wei, M Zhang, H Zhang - Journal of Biomedical Informatics, 2025", "abstract": "Objective: The full fine-tuning paradigm becomes impractical when applying pre- trained models to downstream tasks due to significant computational and storage costs. Parameter-efficient fine-tuning (PEFT) methods can alleviate the issue \u2026"}, {"title": "Chitrakshara: A Large Multilingual Multimodal Dataset for Indian languages", "link": "https://openreview.net/pdf%3Fid%3DCHrzyIKfPd", "details": "S Khan, A Faraz, A Ravi, M Nauman, M Sarfraz\u2026 - CVPR 2025 Workshop Vision \u2026", "abstract": "Multimodal research has predominantly focused on single-image reasoning, with limited exploration of multi-image scenarios. Recent models have sought to enhance multi-image understanding through large-scale pretraining on interleaved image-text \u2026"}, {"title": "PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain", "link": "https://arxiv.org/pdf/2506.00250", "details": "MJR Kalahroodi, A Sheikholselami, S Karimi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have achieved remarkable performance on a wide range of NLP benchmarks, often surpassing human-level accuracy. However, their reliability in high-stakes domains such as medicine, particularly in low-resource \u2026", "entry_id": "http://arxiv.org/abs/2506.00250v2", "updated": "2025-06-03 00:22:37", "published": "2025-05-30 21:34:30", "authors": "Mohammad Javad Ranjbar Kalahroodi;Amirhossein Sheikholselami;Sepehr Karimi;Sepideh Ranjbar Kalahroodi;Heshaam Faili;Azadeh Shakery", "summary": "Large Language Models (LLMs) have achieved remarkable performance on a wide\nrange of NLP benchmarks, often surpassing human-level accuracy. However, their\nreliability in high-stakes domains such as medicine, particularly in\nlow-resource languages, remains underexplored. In this work, we introduce\nPersianMedQA, a large-scale, expert-validated dataset of multiple-choice\nPersian medical questions, designed to evaluate LLMs across both Persian and\nEnglish. We benchmark over 40 state-of-the-art models, including\ngeneral-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and\nchain-of-thought (CoT) settings. Our results show that closed-source general\nmodels (e.g., GPT-4.1) consistently outperform all other categories, achieving\n83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models\nsuch as Dorna underperform significantly (e.g., 35.9% in Persian), often\nstruggling with both instruction-following and domain reasoning. We also\nanalyze the impact of translation, showing that while English performance is\ngenerally higher, Persian responses are sometimes more accurate due to cultural\nand clinical contextual cues. Finally, we demonstrate that model size alone is\ninsufficient for robust performance without strong domain or language\nadaptation. PersianMedQA provides a foundation for evaluating multilingual and\nculturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be\naccessed at: https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.IT;math.IT", "links": "http://arxiv.org/abs/2506.00250v2;http://arxiv.org/pdf/2506.00250v2", "pdf_url": "http://arxiv.org/pdf/2506.00250v2"}, {"title": "Evaluation and Bias Analysis of Large Language Models in Generating Synthetic Electronic Health Records: Comparative Study", "link": "https://www.jmir.org/2025/1/e65317/", "details": "R Huang, H Wu, Y Yuan, Y Xu, H Qian, C Zhang, X Wei\u2026 - Journal of Medical Internet \u2026, 2025", "abstract": "Background Synthetic electronic health records (EHRs) generated by large language models (LLMs) offer potential for clinical education and model training while addressing privacy concerns. However, performance variations and demographic \u2026"}]
