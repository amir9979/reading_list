[{"title": "Oasis: One Image is All You Need for Multimodal Instruction Data Synthesis", "link": "https://arxiv.org/pdf/2503.08741", "details": "L Zhang, Q Cui, B Zhao, C Yang - arXiv preprint arXiv:2503.08741, 2025", "abstract": "The success of multi-modal large language models (MLLMs) has been largely attributed to the large-scale training data. However, the training data of many MLLMs is unavailable due to privacy concerns. The expensive and labor-intensive process \u2026"}, {"title": "Generative Binary Memory: Pseudo-Replay Class-Incremental Learning on Binarized Embeddings", "link": "https://arxiv.org/pdf/2503.10333", "details": "Y Basso-Bert, A Molnos, R Lemaire, W Guicquero\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In dynamic environments where new concepts continuously emerge, Deep Neural Networks (DNNs) must adapt by learning new classes while retaining previously acquired ones. This challenge is addressed by Class-Incremental Learning (CIL) \u2026"}, {"title": "Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models", "link": "https://arxiv.org/pdf/2503.02318", "details": "Z Xie, M Lin, Z Liu, P Wu, S Yan, C Miao - arXiv preprint arXiv:2503.02318, 2025", "abstract": "Recent advancements in multimodal reasoning have largely overlooked the audio modality. We introduce Audio-Reasoner, a large-scale audio language model for deep reasoning in audio tasks. We meticulously curated a large-scale and diverse \u2026"}, {"title": "Tradeoffs Between Alignment and Helpfulness in Language Models with Steering Methods", "link": "https://openreview.net/pdf%3Fid%3DAoTFSkUfLp", "details": "Y Wolf, N Wies, D Shteyman, B Rothberg, Y Levine\u2026 - \u2026 on Foundation Models in the Wild", "abstract": "Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model \u2026"}, {"title": "Understanding the Logical Capabilities of Large Language Models via Out-of-Context Representation Learning", "link": "https://arxiv.org/pdf/2503.10408", "details": "J Shaki, E La Malfa, M Wooldridge, S Kraus - arXiv preprint arXiv:2503.10408, 2025", "abstract": "We study the capabilities of Large Language Models (LLM) on binary relations, a ubiquitous concept in math employed in most reasoning, math and logic benchmarks. This work focuses on equality, inequality, and inclusion, along with the \u2026"}, {"title": "LLM-IE: a python package for biomedical generative information extraction with large language models", "link": "https://academic.oup.com/jamiaopen/article/8/2/ooaf012/8071856", "details": "E Hsu, K Roberts - JAMIA open, 2025", "abstract": "Objectives Despite the recent adoption of large language models (LLMs) for biomedical information extraction (IE), challenges in prompt engineering and algorithms persist, with no dedicated software available. To address this, we \u2026"}, {"title": "Graph out-of-distribution generalization through contrastive learning paradigm", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125003636", "details": "H Du, X Li, M Shao - Knowledge-Based Systems, 2025", "abstract": "The problem we want to address is graph generalization in the out-of-distribution (OOD) scenario. Mainstream approaches to OOD generalization tasks specific to graph data primarily emphasize domain adaptation and invariant learning and do not \u2026"}, {"title": "Self-Training Elicits Concise Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2502.20122", "details": "T Munkhbat, N Ho, S Kim, Y Yang, Y Kim, SY Yun - arXiv preprint arXiv:2502.20122, 2025", "abstract": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens \u2026"}]
