'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Hufu: A Modality-Agnositc Watermarking System for Pre'
[{"title": "IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages", "link": "https://arxiv.org/pdf/2403.06350", "details": "MSUR Khan, P Mehta, A Sankar, U Kumaravelan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the considerable advancements in English LLMs, the progress in building comparable models for other languages has been hindered due to the scarcity of tailored resources. Our work aims to bridge this divide by introducing an expansive \u2026"}, {"title": "Drop the shortcuts: image augmentation improves fairness and decreases AI detection of race and other demographics from medical images", "link": "https://www.thelancet.com/journals/ebiom/article/PIIS2352-3964\\(24\\)00082-3/fulltext", "details": "R Wang, PC Kuo, LC Chen, KP Seastedt, JW Gichoya\u2026 - EBioMedicine, 2024", "abstract": "Background It has been shown that AI models can learn race on medical images, leading to algorithmic bias. Our aim in this study was to enhance the fairness of medical image models by eliminating bias related to race, age, and sex. We \u2026"}, {"title": "KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques", "link": "https://arxiv.org/pdf/2403.05881", "details": "R Yang, H Liu, Q Zeng, YH Ke, W Li, L Cheng, Q Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have significantly advanced healthcare innovation on generation capabilities. However, their application in real clinical settings is challenging due to potential deviations from medical facts and inherent biases. In this \u2026"}, {"title": "Zero-Shot Pediatric Tuberculosis Detection in Chest X-Rays using Self-Supervised Learning", "link": "https://arxiv.org/pdf/2402.14741", "details": "D Capell\u00e1n-Mart\u00edn, A Parida, JJ G\u00f3mez-Valverde\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Tuberculosis (TB) remains a significant global health challenge, with pediatric cases posing a major concern. The World Health Organization (WHO) advocates for chest X-rays (CXRs) for TB screening. However, visual interpretation by radiologists can be \u2026"}, {"title": "Towards Reducing Diagnostic Errors with Interpretable Risk Prediction", "link": "https://arxiv.org/pdf/2402.10109", "details": "DJ McInerney, W Dickinson, L Flynn, A Young\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate \u2026"}, {"title": "Pruning Quantized Unsupervised Meta-Learning DegradingNet Solution for Industrial Equipment and Semiconductor Process Anomaly Detection and Prediction", "link": "https://www.mdpi.com/2076-3417/14/5/1708", "details": "YC Yu, SR Yang, SW Chuang, JT Chien, CY Lee - Applied Sciences, 2024", "abstract": "Machine-and deep-learning methods are used for industrial applications in prognostics and health management (PHM) for semiconductor processing and equipment anomaly detection to achieve proactive equipment maintenance and \u2026"}, {"title": "Rethinking Generative Large Language Model Evaluation for Semantic Comprehension", "link": "https://arxiv.org/pdf/2403.07872", "details": "F Wei, X Chen, L Luo - arXiv preprint arXiv:2403.07872, 2024", "abstract": "Despite their sophisticated capabilities, large language models (LLMs) encounter a major hurdle in effective assessment. This paper first revisits the prevalent evaluation method-multiple choice question answering (MCQA), which allows for \u2026"}, {"title": "Analysing The Impact of Sequence Composition on Language Model Pre-Training", "link": "https://arxiv.org/html/2402.13991v1", "details": "Y Zhao, Y Qu, K Staniszewski, S Tworkowski, W Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and \u2026"}, {"title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains", "link": "https://arxiv.org/pdf/2402.10373.pdf%3Ftrk%3Dpublic_post_comment-text", "details": "Y Labrak, A Bazoge, E Morin, PA Gourraud, M Rouvier\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for \u2026"}]
