'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Overview of the EHRSQL 2024 Shared Task on Reliable Te'
[{"title": "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models", "link": "https://arxiv.org/pdf/2404.15846", "details": "Q He, J Zeng, Q He, J Liang, Y Xiao - arXiv preprint arXiv:2404.15846, 2024", "abstract": "It is imperative for Large language models (LLMs) to follow instructions with elaborate requirements (ie Complex Instructions Following). Yet, it remains under- explored how to enhance the ability of LLMs to follow complex instructions with \u2026"}, {"title": "Continuous patient state attention model for addressing irregularity in electronic health records", "link": "https://link.springer.com/article/10.1186/s12911-024-02514-2", "details": "VK Chauhan, A Thakur, O O'Donoghue, O Rohanian\u2026 - BMC Medical Informatics \u2026, 2024", "abstract": "Background Irregular time series (ITS) are common in healthcare as patient data is recorded in an electronic health record (EHR) system as per clinical guidelines/ requirements but not for research and depends on a patient's health status. Due to \u2026"}, {"title": "Phi-3 technical report: A highly capable language model locally on your phone", "link": "https://arxiv.org/pdf/2404.14219%3Ftrk%3Dpublic_post_comment-text", "details": "M Abdin, SA Jacobs, AA Awan, J Aneja, A Awadallah\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT \u2026"}]
