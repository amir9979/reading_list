[{"title": "NLPrompt: Noise-Label Prompt Learning for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.01256", "details": "B Pan, Q Li, X Tang, W Huang, Z Fang, F Liu, J Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence of vision-language foundation models, such as CLIP, has revolutionized image-text representation, enabling a broad range of applications via prompt learning. Despite its promise, real-world datasets often contain noisy labels \u2026"}, {"title": "PRIMAL: Prompting Multiple Language Models for Low-resource Diverse Response Generation", "link": "https://ieeexplore.ieee.org/abstract/document/10768975/", "details": "Z Wen, Z Tian, S Pan, K Zhu, X Meng, Y Song, D Li - IEEE/ACM Transactions on \u2026, 2024", "abstract": "Low-resource conversation models are becoming increasingly important. Existing conversation models tend to generate uninformative responses that lack diversity, especially when the training data are limited. Researchers address this issue by \u2026"}, {"title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators", "link": "https://arxiv.org/pdf/2412.02467", "details": "T Afonja, HP Wang, R Kerkouche, M Fritz - arXiv preprint arXiv:2412.02467, 2024", "abstract": "Generating tabular data under differential privacy (DP) protection ensures theoretical privacy guarantees but poses challenges for training machine learning models, primarily due to the need to capture complex structures under noisy supervision \u2026"}, {"title": "TRAPL: Transformer-Based Patch Learning for Enhancing Semantic Representations Using Aggregated Features to Estimate Patch-Class Distribution", "link": "https://link.springer.com/chapter/10.1007/978-3-031-77915-2_9", "details": "SR Jyhne, PA Andersen, I Oveland, M Goodwin - International Conference on \u2026, 2024", "abstract": "We introduce TRAPL, a Transformer-based Patch Learning technique that enhances semantic representations in segmentation models. TRAPL leverages aggregated features for precise patch-class distribution estimation, gathering features at key \u2026"}, {"title": "CPLLM: Clinical prediction with large language models", "link": "https://journals.plos.org/digitalhealth/article%3Fid%3D10.1371/journal.pdig.0000680", "details": "O Ben Shoham, N Rappoport - PLOS Digital Health, 2024", "abstract": "We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for predicting clinical disease and readmission. We utilized quantization and fine-tuned the LLM using \u2026"}, {"title": "CNNSum: Exploring Long-Conext Summarization with Large Language Models in Chinese Novels", "link": "https://arxiv.org/pdf/2412.02819", "details": "L Wei, H Yan, X Lu, J Zhu, J Wang, W Zhang - arXiv preprint arXiv:2412.02819, 2024", "abstract": "Large Language Models (LLMs) have been well-researched in many long-context tasks. However, due to high annotation costs, high-quality long-context summary datasets for training or evaluation are scarce, limiting further research. In this work \u2026"}, {"title": "Data augmentation based on large language models for radiological report classification", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124013790", "details": "J Collado-Monta\u00f1ez, MT Mart\u00edn-Valdivia\u2026 - Knowledge-Based Systems, 2024", "abstract": "Abstract The International Classification of Diseases (ICD) is fundamental in the field of healthcare as it provides a standardized framework for the classification and coding of medical diagnoses and procedures, enabling the understanding of \u2026"}, {"title": "Lost in Inference: Rediscovering the Role of Natural Language Inference for Large Language Models", "link": "https://arxiv.org/pdf/2411.14103%3F", "details": "L Madaan, D Esiobu, P Stenetorp, B Plank, D Hupkes - arXiv preprint arXiv \u2026, 2024", "abstract": "In the recent past, a popular way of evaluating natural language understanding (NLU), was to consider a model's ability to perform natural language inference (NLI) tasks. In this paper, we investigate if NLI tasks, that are rarely used for LLM \u2026"}, {"title": "RedStone: Curating General, Code, Math, and QA Data for Large Language Models", "link": "https://arxiv.org/pdf/2412.03398", "details": "Y Chang, L Cui, L Dong, S Huang, Y Huang, Y Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-training Large Language Models (LLMs) on high-quality, meticulously curated datasets is widely recognized as critical for enhancing their performance and generalization capabilities. This study explores the untapped potential of Common \u2026"}]
