[{"title": "Evaluating **Large Language Models** for Preoperative Patient Education in Superior Capsular Reconstruction: Comparative Study of Claude, GPT, and Gemini", "link": "https://periop.jmir.org/2025/1/e70047", "details": "Y Liu, H Li, J Ouyang, Z Xue, M Wang, H He, B Song\u2026 - \u2026 Perioperative **Medicine** , 2025", "abstract": "\u2026 The subjects of this study are LLMs ( **large** **language** **models** ). Besides being used as operational models, LLMs also serve as tools for translating Chinese content into English, as detailed in Multimedia Appendix 1. The specific types of models used \u2026"}, {"title": "\u2026 the Quality, Usefulness, and Reliability of **Large Language Models** (ChatGPT, DeepSeek, and Gemini) in **Answering** General **Questions** Regarding Dyslexia and \u2026", "link": "https://link.springer.com/article/10.1007/s11126-025-10170-6", "details": "A Alrubaian - Psychiatric Quarterly, 2025", "abstract": "\u2026 reliability of three **large** **language** **models** (LLMs)\u2014ChatGPT-4, DeepSeek, and Gemini\u2014in **answering** general **questions** about specific \u2026 One significant application of novel computational technology is its ability to address **medical** and \u2026"}, {"title": "Advancing **medical** education in cervical cancer control with **large language models** for multiple-choice **question** generation", "link": "https://www.tandfonline.com/doi/abs/10.1080/0142159X.2025.2513419", "details": "M Chen, J Ma, X Cui, Q Dai, H Hu, Y Wu, S Husaiyin\u2026 - **Medical** Teacher, 2025", "abstract": "\u2026 being excluded from standard **medical** school curricula, relying \u2026 **healthcare** examinations and generating human-like text, **large** **language** **models** (LLMs) present a potential solution for automating the creation of **medical** training exercises \u2026"}, {"title": " **Large language models** and **questions** from older adults: a human and machine\u2011based evaluation study", "link": "https://search.proquest.com/openview/0bfdd222cb451e3cc31343c4bd405b90/1%3Fpq-origsite%3Dgscholar%26cbl%3D5642945", "details": "N Watanabe, Y Kuriya, M Araki - 2025", "abstract": "\u2026 **questions** , we first asked three LLMs, namely CG, GE and CL, the following **question** : \u201cWhat are some of the **questions** that older people ask in search engines or in **large** **language** **models** \u2026 For example, with the **question** H3 ie \u201cWhat are the \u2026"}, {"title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy", "link": "https://arxiv.org/pdf/2506.09958", "details": "S Gautam, MA Riegler, P Halvorsen - arXiv preprint arXiv:2506.09958, 2025", "abstract": "\u2026 159,549 new **question** - **answer** pairs that are designed to test deeper clinical reasoning. We developed a systematic method using **large** **language** **models** to generate these **questions** , which are \u2026 Toward expert-level **medical** **question** \u2026", "entry_id": "http://arxiv.org/abs/2506.09958v1", "updated": "2025-06-11 17:31:38", "published": "2025-06-11 17:31:38", "authors": "Sushant Gautam;Michael A. Riegler;P\u00e5l Halvorsen", "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.LG;68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing) 68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing);I.2.10; I.2.6; J.3", "links": "http://arxiv.org/abs/2506.09958v1;http://arxiv.org/pdf/2506.09958v1", "pdf_url": "http://arxiv.org/pdf/2506.09958v1"}, {"title": "Towards Efficient and Effective Alignment of Large Language Models", "link": "https://arxiv.org/pdf/2506.09329", "details": "Y Jiang - arXiv preprint arXiv:2506.09329, 2025", "abstract": "\u2026 The recent advances in **large** **language** **models** (LLMs) have demonstrated extraordinary breakthroughs in various real-world applications. These models, typically \u2026 \u2022 Our work is the first attempt to adopt the idea of adversarial knowledge \u2026", "entry_id": "http://arxiv.org/abs/2506.09329v1", "updated": "2025-06-11 02:08:52", "published": "2025-06-11 02:08:52", "authors": "Yuxin Jiang", "summary": "Large language models (LLMs) exhibit remarkable capabilities across diverse\ntasks, yet aligning them efficiently and effectively with human expectations\nremains a critical challenge. This thesis advances LLM alignment by introducing\nnovel methodologies in data collection, training, and evaluation. We first\naddress alignment data collection. Existing approaches rely heavily on manually\ncurated datasets or proprietary models. To overcome these limitations, we\npropose Lion, an adversarial distillation framework that iteratively refines\ntraining data by identifying and generating challenging instructions, enabling\nstate-of-the-art zero-shot reasoning. Additionally, we introduce Web\nReconstruction (WebR), a fully automated framework that synthesizes\ninstruction-tuning data directly from raw web documents, significantly\nimproving data diversity and scalability over existing synthetic data methods.\nNext, we enhance alignment training through novel optimization techniques. We\ndevelop Learning to Edit (LTE), a framework that enables LLMs to efficiently\nintegrate new knowledge while preserving existing information. LTE leverages\nmeta-learning to improve both real-time and batch knowledge updates.\nFurthermore, we introduce Bridging and Modeling Correlations (BMC), a\nrefinement of Direct Preference Optimization (DPO) that explicitly captures\ntoken-level correlations in preference data, leading to superior alignment\nacross QA and mathematical reasoning tasks. Finally, we tackle the challenge of\nevaluating alignment. Existing benchmarks emphasize response quality but\noverlook adherence to specific constraints. To bridge this gap, we introduce\nFollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to\nfollow complex constraints across diverse instruction types. Our results expose\nkey weaknesses in current models' constraint adherence, offering insights for\nfuture improvements.", "comment": "PhD thesis", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.09329v1;http://arxiv.org/pdf/2506.09329v1", "pdf_url": "http://arxiv.org/pdf/2506.09329v1"}, {"title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning", "link": "https://arxiv.org/pdf/2506.09513", "details": "Y Sun, X Qian, W Xu, H Zhang, C Xiao, L Li, Y Rong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **large** **language** **models** (LLMs) have excelled in mathematics and programming, their capabilities in knowledge-intensive **medical** **question** **answering** \u2026 the integration of comprehensive **medical** knowledge and multi-step reasoning into \u2026", "entry_id": "http://arxiv.org/abs/2506.09513v1", "updated": "2025-06-11 08:36:55", "published": "2025-06-11 08:36:55", "authors": "Yu Sun;Xingyu Qian;Weiwen Xu;Hao Zhang;Chenghao Xiao;Long Li;Yu Rong;Wenbing Huang;Qifeng Bai;Tingyang Xu", "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a \\textit{multi-agent\nverification and refinement process}, where we design an \\textit{Error Refiner}\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.", "comment": "24 pages, 6 figures, 7 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.MA", "links": "http://arxiv.org/abs/2506.09513v1;http://arxiv.org/pdf/2506.09513v1", "pdf_url": "http://arxiv.org/pdf/2506.09513v1"}, {"title": "HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding", "link": "https://arxiv.org/pdf/2506.09634", "details": "Y Shi, X Zhang, J Ji, H Jiang, C Zheng, Y Wang, L Qu - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Recently, multi-modal **large** **language** **models** (MLLMs) have emerged as a powerful tool in **medical** image analysis, including diagnostic tasks such as **medical** report generation (MRG) and visual **question** **answering** (VQA). Current works \u2026", "entry_id": "http://arxiv.org/abs/2506.09634v1", "updated": "2025-06-11 11:46:57", "published": "2025-06-11 11:46:57", "authors": "Yanzhao Shi;Xiaodan Zhang;Junzhong Ji;Haoning Jiang;Chengxin Zheng;Yinong Wang;Liangqiong Qu", "summary": "Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based\ndecisions by enhancing diagnostic accuracy and workflow efficiency. While\nmultimodal large language models (MLLMs) exhibit promising performance in\nvisual-language understanding, existing methods mainly focus on 2D medical\nimages, which fundamentally limits their ability to capture complex 3D\nanatomical structures. This limitation often leads to misinterpretation of\nsubtle pathologies and causes diagnostic hallucinations. In this paper, we\npresent Hybrid Spatial Encoding Network (HSENet), a framework that exploits\nenriched 3D medical visual cues by effective visual perception and projection\nfor accurate and robust vision-language understanding. Specifically, HSENet\nemploys dual-3D vision encoders to perceive both global volumetric contexts and\nfine-grained anatomical details, which are pre-trained by dual-stage alignment\nwith diagnostic reports. Furthermore, we propose Spatial Packer, an efficient\nmultimodal projector that condenses high-resolution 3D spatial regions into a\ncompact set of informative visual tokens via centroid-based compression. By\nassigning spatial packers with dual-3D vision encoders, HSENet can seamlessly\nperceive and transfer hybrid visual representations to LLM's semantic space,\nfacilitating accurate diagnostic text generation. Experimental results\ndemonstrate that our method achieves state-of-the-art performance in 3D\nlanguage-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report\ngeneration (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering\n(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.\nOur code is available at https://github.com/YanzhaoShi/HSENet.", "comment": "27 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2410.14200 by other authors", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2506.09634v1;http://arxiv.org/pdf/2506.09634v1", "pdf_url": "http://arxiv.org/pdf/2506.09634v1"}, {"title": "Applying Language Models To Patient Health Records: Acronym Expansion, Long Document Classification and Explainable Predictions", "link": "https://repository.upenn.edu/bitstreams/3ae58c00-9a3d-49fc-afbb-e9f6f2212d78/download", "details": "A Kashyap - 2025", "abstract": "\u2026 be deployed across different **healthcare** settings and institutions. Our methods leverage recent advances in **large** **language** **models** while maintaining interpretability \u2026 An example could include providing the model with a few sentence \u2026"}]
