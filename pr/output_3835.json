[{"title": "Memory Augmented Language Models through Mixture of Word Experts", "link": "https://aclanthology.org/2024.naacl-long.249.pdf", "details": "C dos Santos, J Lee-Thorp, I Noble, CC Chang\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Scaling up the number of parameters of language models has proven to be an effective approach to improve performance. For dense models, increasing their size proportionally increases their computational footprint. In this work, we seek to \u2026"}, {"title": "Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level", "link": "https://arxiv.org/pdf/2406.11817", "details": "J Liu, Z Zhou, J Liu, X Bu, C Yang, HS Zhong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Direct Preference Optimization (DPO), a standard method for aligning language models with human preferences, is traditionally applied to offline preferences. Recent studies show that DPO benefits from iterative training with online preferences \u2026"}, {"title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models", "link": "https://aclanthology.org/2024.findings-naacl.18.pdf", "details": "Z Su, Z Lin, B Baixue, H Chen, S Hu, W Zhou, G Ding\u2026 - Findings of the Association \u2026, 2024", "abstract": "Generative language models are usually pre-trained on large text corpus via predicting the next token (ie, sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language \u2026"}, {"title": "Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs", "link": "https://arxiv.org/pdf/2406.11514", "details": "Y Fang, M Li, W Wang, H Lin, F Feng - arXiv preprint arXiv:2406.11514, 2024", "abstract": "Large Language Models (LLMs) excel in various natural language processing tasks but struggle with hallucination issues. Existing solutions have considered utilizing LLMs' inherent reasoning abilities to alleviate hallucination, such as self-correction \u2026"}, {"title": "Developing an explainable diagnosis system utilizing deep learning model: a case study of spontaneous pneumothorax", "link": "https://iopscience.iop.org/article/10.1088/1361-6560/ad5e31/pdf", "details": "FCF Lin, CJ Wei, ZR Bai, CC Chang, MC Chiu - Physics in Medicine and Biology, 2024", "abstract": "Objective. The trend in the medical field is towards intelligent detection-based medical diagnostic systems. However, these methods are often seen as\" black boxes\" due to their lack of interpretability. This situation presents challenges in \u2026"}, {"title": "FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model", "link": "https://arxiv.org/pdf/2406.17706", "details": "F Wu, Z Li, Y Li, B Ding, J Gao - arXiv preprint arXiv:2406.17706, 2024", "abstract": "Large language models (LLMs) show amazing performance on many domain- specific tasks after fine-tuning with some appropriate data. However, many domain- specific data are privately distributed across multiple owners. Thus, this dilemma \u2026"}, {"title": "Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications", "link": "https://aclanthology.org/2024.naacl-long.198.pdf", "details": "Y Liu, S Gautam, J Ma, H Lakkaraju - Proceedings of the 2024 Conference of the \u2026, 2024", "abstract": "Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society \u2026"}, {"title": "UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages", "link": "https://arxiv.org/pdf/2406.09717", "details": "T Pham, KM Le, LA Tuan - arXiv preprint arXiv:2406.09717, 2024", "abstract": "In this paper, we introduce UniBridge (Cross-Lingual Transfer Learning with Optimized Embeddings and Vocabulary), a comprehensive approach developed to improve the effectiveness of Cross-Lingual Transfer Learning, particularly in \u2026"}, {"title": "Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for Information Retrieval", "link": "https://arxiv.org/pdf/2406.12169", "details": "Z Li, H Zhang, J Zhang - arXiv preprint arXiv:2406.12169, 2024", "abstract": "Recent research has explored distilling knowledge from large language models (LLMs) to optimize retriever models, especially within the retrieval-augmented generation (RAG) framework. However, most existing training methods rely on \u2026"}]
