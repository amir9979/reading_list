'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [EFTNAS: Searching for Efficient Language Models in Fir'
[{"title": "Understanding the influence of AI autonomy on AI explainability levels in human-AI teams using a mixed methods approach", "link": "https://link.springer.com/article/10.1007/s10111-024-00765-7", "details": "AI Hauptman, BG Schelble, W Duan, C Flathmann\u2026 - Cognition, Technology & \u2026, 2024", "abstract": "An obstacle to effective teaming between humans and AI is the agent's\" black box\" design. AI explanations have proven benefits, but few studies have explored the effects that explanations can have in a teaming environment with AI agents operating \u2026"}, {"title": "Prompt Tuning for Few-shot Relation Extraction via Modeling Global and Local Graphs", "link": "https://aclanthology.org/2024.lrec-main.1158.pdf", "details": "Z Zhang, Y Yang, B Chen - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Recently, prompt-tuning has achieved very significant results for few-shot tasks. The core idea of prompt-tuning is to insert prompt templates into the input, thus converting the classification task into a masked language modeling problem. However, for few \u2026"}, {"title": "PDAMeta: Meta-Learning Framework with Progressive Data Augmentation for Few-Shot Text Classification", "link": "https://aclanthology.org/2024.lrec-main.1109.pdf", "details": "X Li, K Song, T Lin, Y Kang, F Zhao, C Sun, X Liu - Proceedings of the 2024 Joint \u2026, 2024", "abstract": "Recently, we have witnessed the breakthroughs of meta-learning for few-shot learning scenario. Data augmentation is essential for meta-learning, particularly in situations where data is extremely scarce. However, existing text data augmentation \u2026"}, {"title": "Exploring and Mitigating Shortcut Learning for Generative Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.602.pdf", "details": "Z Sun, Y Xiao, J Li, Y Ji, W Chen, M Zhang - Proceedings of the 2024 Joint \u2026, 2024", "abstract": "Recent generative large language models (LLMs) have exhibited incredible instruction-following capabilities while keeping strong task completion ability, even without task-specific fine-tuning. Some works attribute this to the bonus of the new \u2026"}, {"title": "Phi-3 technical report: A highly capable language model locally on your phone", "link": "https://arxiv.org/pdf/2404.14219%3Ftrk%3Dpublic_post_comment-text", "details": "M Abdin, SA Jacobs, AA Awan, J Aneja, A Awadallah\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT \u2026"}, {"title": "Probe Then Retrieve and Reason: Distilling Probing and Reasoning Capabilities into Smaller Language Models", "link": "https://aclanthology.org/2024.lrec-main.1140.pdf", "details": "Y Zhao, S Zhou, H Zhu - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Step-by-step reasoning methods, such as the Chain-of-Thought (CoT), have been demonstrated to be highly effective in harnessing the reasoning capabilities of Large Language Models (LLMs). Recent research efforts have sought to distill LLMs into \u2026"}, {"title": "Correcting Language Model Bias for Text Classification in True Zero-Shot Learning", "link": "https://aclanthology.org/2024.lrec-main.359.pdf", "details": "F Zhao, W Xianlin, C Yan, CK Loo - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Combining pre-trained language models (PLMs) and manual templates is a common practice for text classification in zero-shot scenarios. However, the effect of this approach is highly volatile, ranging from random guesses to near state-of-the-art \u2026"}, {"title": "Analyzing Chain-of-thought Prompting in Black-Box Large Language Models via Estimated V-information", "link": "https://aclanthology.org/2024.lrec-main.81.pdf", "details": "Z Wang, C Li, Z Yang, Q Liu, Y Hao, X Chen, D Chu\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract Chain-of-Thought (CoT) prompting combined with large language models (LLM) has shown great potential in improving performance on challenging reasoning tasks. While understanding why CoT prompting is effective is crucial for the \u2026"}, {"title": "Self-training improves few-shot learning in legal artificial intelligence tasks", "link": "https://link.springer.com/article/10.1007/s10506-024-09403-z", "details": "Y Zhou, Y Qin, R Huang, Y Chen, C Lin, Y Zhou - Artificial Intelligence and Law, 2024", "abstract": "As the labeling costs in legal artificial intelligence tasks are expensive. Therefore, it becomes a challenge to utilize low cost to train a robust model. In this paper, we propose a LAIAugment approach, which aims to enhance the few-shot learning \u2026"}]
