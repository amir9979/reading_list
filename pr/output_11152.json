[{"title": "OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes", "link": "https://arxiv.org/pdf/2501.00962", "details": "S Dehdashtian, G Sreekumar, VN Boddeti - arXiv preprint arXiv:2501.00962, 2025", "abstract": "Images generated by text-to-image (T2I) models often exhibit visual biases and stereotypes of concepts such as culture and profession. Existing quantitative measures of stereotypes are based on statistical parity that does not align with the \u2026"}, {"title": "Generalizing Trust: Weak-to-Strong Trustworthiness in Language Models", "link": "https://arxiv.org/pdf/2501.00418", "details": "M Pawelczyk, L Sun, Z Qi, A Kumar, H Lakkaraju - arXiv preprint arXiv:2501.00418, 2024", "abstract": "The rapid proliferation of generative AI, especially large language models, has led to their integration into a variety of applications. A key phenomenon known as weak-to- strong generalization-where a strong model trained on a weak model's outputs \u2026"}, {"title": "A Data-Centric Approach to Detecting and Mitigating Demographic Bias in Pediatric Mental Health Text: A Case Study in Anxiety Detection", "link": "https://arxiv.org/pdf/2501.00129", "details": "J Ive, P Bondaronek, V Yadav, D Santel, T Glauser\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Introduction: Healthcare AI models often inherit biases from their training data. While efforts have primarily targeted bias in structured data, mental health heavily depends on unstructured data. This study aims to detect and mitigate linguistic differences \u2026"}, {"title": "Titans: Learning to Memorize at Test Time", "link": "https://arxiv.org/pdf/2501.00663", "details": "A Behrouz, P Zhong, V Mirrokni - arXiv preprint arXiv:2501.00663, 2024", "abstract": "Over more than a decade there has been an extensive research effort on how to effectively utilize recurrent models and attention. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows \u2026"}]
