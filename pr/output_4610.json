[{"title": "Cognitive Assessment of Language Models", "link": "https://openreview.net/pdf%3Fid%3DpxRh1meUvN", "details": "D McDuff, D Munday, X Liu, I Galatzer-Levy - ICML 2024 Workshop on LLMs and Cognition", "abstract": "Large language models (LLMs) are a subclass of generative artificial intelligence that can interpret language inputs to generate novel responses. These capabilities are conceptualized as a significant step forward in artificial intelligence because the \u2026"}, {"title": "Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education", "link": "https://arxiv.org/pdf/2407.17022", "details": "S Kim, S Kim - arXiv preprint arXiv:2407.17022, 2024", "abstract": "Large language model (LLM)-based evaluation pipelines have demonstrated their capability to robustly evaluate machine-generated text. Extending this methodology to assess human-written text could significantly benefit educational settings by \u2026"}, {"title": "Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models", "link": "https://arxiv.org/pdf/2407.19474", "details": "N Bitton-Guetta, A Slobodkin, A Maimon, E Habba\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Imagine observing someone scratching their arm; to understand why, additional context would be necessary. However, spotting a mosquito nearby would immediately offer a likely explanation for the person's discomfort, thereby alleviating \u2026"}, {"title": "RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent", "link": "https://arxiv.org/pdf/2407.16667", "details": "H Xu, W Zhang, Z Wang, F Xiao, R Zheng, Y Feng, Z Ba\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot. These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of \u2026"}, {"title": "AutoScale: Automatic Prediction of Compute-optimal Data Composition for Training LLMs", "link": "https://arxiv.org/pdf/2407.20177", "details": "F Kang, Y Sun, B Wen, S Chen, D Song, R Mahmood\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To ensure performance on a diverse set of downstream tasks, LLMs are pretrained via data mixtures over different domains. In this work, we demonstrate that the optimal data composition for a fixed compute budget varies depending on the scale \u2026"}, {"title": "Patch-Level Training for Large Language Models", "link": "https://arxiv.org/pdf/2407.12665", "details": "C Shao, F Meng, J Zhou - arXiv preprint arXiv:2407.12665, 2024", "abstract": "As Large Language Models (LLMs) achieve remarkable progress in language understanding and generation, their training efficiency has become a critical concern. Traditionally, LLMs are trained to predict the next token in a sequence \u2026"}, {"title": "Fine-tuning language models for joint rewriting and completion of code with potential bugs", "link": "https://www.amazon.science/publications/fine-tuning-language-models-for-joint-rewriting-and-completion-of-code-with-potential-bugs", "details": "D Wang, J Zhao, H Pei, S Tan, S Zha - 2024", "abstract": "Handling drafty partial code remains a notable challenge in real-time code suggestion applications. Previous work has demonstrated shortcomings of large language models of code (CodeLLMs) in completing partial code with potential bugs \u2026"}, {"title": "Not All Attention is Needed: Parameter and Computation Efficient Tuning for Multi-modal Large Language Models via Effective Attention Skipping", "link": "https://arxiv.org/pdf/2407.14093", "details": "Q Wu, Z Ke, Y Zhou, G Luo, X Sun, R Ji - arXiv preprint arXiv:2407.14093, 2024", "abstract": "Recently, mixture of experts (MoE) has become a popular paradigm for achieving the trade-off between modal capacity and efficiency of multi-modal large language models (MLLMs). Different from previous efforts, we are dedicated to exploring the \u2026"}, {"title": "AutoM3L: An Automated Multimodal Machine Learning Framework with Large Language Models", "link": "https://openreview.net/pdf%3Fid%3DBkSgt0ClHJ", "details": "D Luo, C Feng, Y Nong, Y Shen - ACM Multimedia 2024", "abstract": "Automated Machine Learning (AutoML) offers a promising approach to streamline the training of machine learning models. However, existing AutoML frameworks are often limited to unimodal scenarios and require extensive manual configuration \u2026"}]
