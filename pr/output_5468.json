[{"title": "MixPrompt: Enhancing Generalizability and Adversarial Robustness for Vision-Language Models via Prompt Fusion", "link": "https://link.springer.com/chapter/10.1007/978-981-97-5606-3_28", "details": "H Fan, Z Ma, Y Li, R Tian, Y Chen, C Gao - International Conference on Intelligent \u2026, 2024", "abstract": "Abstract Pretrained Vision-Language Models (VLMs) like CLIP have exhibited remarkable capacities across downstream tasks, while their image encoders are vulnerable to adversarial examples. A recently introduced lightweight approach \u2026"}, {"title": "CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning", "link": "https://arxiv.org/pdf/2407.21011", "details": "Y Du, B Chang, NC Dvornek - arXiv preprint arXiv:2407.21011, 2024", "abstract": "Recent advancements in Contrastive Language-Image Pre-training (CLIP) have demonstrated notable success in self-supervised representation learning across various tasks. However, the existing CLIP-like approaches often demand extensive \u2026"}, {"title": "CADRL: Category-aware Dual-agent Reinforcement Learning for Explainable Recommendations over Knowledge Graphs", "link": "https://arxiv.org/pdf/2408.03166", "details": "S Zheng, H Yin, T Chen, X Kong, J Hou, P Zhao - arXiv preprint arXiv:2408.03166, 2024", "abstract": "Knowledge graphs (KGs) have been widely adopted to mitigate data sparsity and address cold-start issues in recommender systems. While existing KGs-based recommendation methods can predict user preferences and demands, they fall short \u2026"}, {"title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT", "link": "https://arxiv.org/pdf/2407.21058", "details": "M Ali, S Panda, Q Shen, M Wick, A Kobren - arXiv preprint arXiv:2407.21058, 2024", "abstract": "In the current landscape of language model research, larger models, larger datasets and more compute seems to be the only way to advance towards intelligence. While there have been extensive studies of scaling laws and models' scaling behaviors, the \u2026"}, {"title": "Evaluating Language Models for Efficient Code Generation", "link": "https://arxiv.org/pdf/2408.06450", "details": "J Liu, S Xie, J Wang, Y Wei, Y Ding, L Zhang - arXiv preprint arXiv:2408.06450, 2024", "abstract": "We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. Traditional coding benchmarks often fail to provide reliable insights into code \u2026"}, {"title": "Improving Relational Database Interactions with Large Language Models: Column Descriptions and Their Impact on Text-to-SQL Performance", "link": "https://arxiv.org/pdf/2408.04691", "details": "N Wretblad, O Holmstr\u00f6m, E Larsson, A Wiks\u00e4ter\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Relational databases often suffer from uninformative descriptors of table contents, such as ambiguous columns and hard-to-interpret values, impacting both human users and Text-to-SQL models. This paper explores the use of large language \u2026"}, {"title": "SSuieBERT: Domain Adaptation Model for Chinese Space Science Text Mining and Information Extraction", "link": "https://www.mdpi.com/2079-9292/13/15/2949", "details": "Y Liu, S Li, Y Deng, S Hao, L Wang - Electronics, 2024", "abstract": "With the continuous exploration of space science, a large number of domain-related materials and scientific literature are constantly generated, mostly in the form of text, which contains rich and unexplored domain knowledge. Natural language \u2026"}, {"title": "Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment", "link": "https://arxiv.org/pdf/2408.00137", "details": "S Yu, J Song, B Hwang, H Kang, S Cho, J Choi, S Joe\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "A binary decision task, like yes-no questions or answer verification, reflects a significant real-world scenario such as where users look for confirmation about the correctness of their decisions on specific issues. In this work, we observe that \u2026"}]
