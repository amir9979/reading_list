[{"title": "The Architecture and Evaluation of Bayesian Neural Networks", "link": "https://arxiv.org/pdf/2503.11808", "details": "A Sheinkman, S Wade - arXiv preprint arXiv:2503.11808, 2025", "abstract": "As modern neural networks get more complex, specifying a model with high predictive performance and sound uncertainty quantification becomes a more challenging task. Despite some promising theoretical results on the true posterior \u2026"}, {"title": "Sparseformer: a Transferable Transformer with Multi-granularity Token Sparsification for Medical Time Series Classification", "link": "https://arxiv.org/pdf/2503.15578%3F", "details": "J Ye, W Zhang, Z Li, J Li, F Tsung - arXiv preprint arXiv:2503.15578, 2025", "abstract": "Medical time series (MedTS) classification is crucial for improved diagnosis in healthcare, and yet it is challenging due to the varying granularity of patterns, intricate inter-channel correlation, information redundancy, and label scarcity. While \u2026"}, {"title": "MSNet: Multi-task self-supervised network for time series classification", "link": "https://www.sciencedirect.com/science/article/pii/S0167865525000923", "details": "D Huang, X Lv, Y Zhang - Pattern Recognition Letters, 2025", "abstract": "Learning rich representations from unlabeled temporal data is essential for effective time series classification. Most existing self-supervised learning methods for time series focus on a single task, often relying on contrastive learning or reconstruction \u2026"}, {"title": "Hardware-Aware Iterative One-Shot Neural Architecture Search with Adaptable Knowledge Distillation for Efficient Edge Computing", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10938148.pdf", "details": "OTC Chen, YX Chang, CY Chung, YY Cheng, MH Ha - IEEE Access, 2025", "abstract": "The growing demand for edge applications calls for efficient and optimized deep neural network models. Neural Architecture Search (NAS) is instrumental in designing such models, but achieving optimal architectures quickly remains a key \u2026"}, {"title": "MAD-DGTD: Multivariate time series Anomaly Detection based on Dynamic Graph structure learning with Time Delay", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225005594", "details": "K Wang, J Kong, M Zhang, M Jiang, T Liu - Neurocomputing, 2025", "abstract": "Anomaly detection of multivariate time series data is extremely important in the industrial operation maintenance of Internet of Things (IoT). Researchers have found that the relationship between multiple sensors can be modeled as graph structure \u2026"}, {"title": "Language Model Uncertainty Quantification with Attention Chain", "link": "https://arxiv.org/pdf/2503.19168", "details": "Y Li, R Qiang, L Moukheiber, C Zhang - arXiv preprint arXiv:2503.19168, 2025", "abstract": "Accurately quantifying a large language model's (LLM) predictive uncertainty is crucial for judging the reliability of its answers. While most existing research focuses on short, directly answerable questions with closed-form outputs (eg, multiple \u2026"}, {"title": "TSRM: ALightweight TEMPORAL FEATURE ENCODING ARCHITECTURE FOR TIME SERIES FORECASTING AND IMPUTATION", "link": "https://www.researchgate.net/profile/Robert-Leppich-2/publication/389759430_TSRM_A_Lightweight_Temporal_Feature_Encoding_Architecture_for_Time_Series_Forecasting_and_Imputation/links/67d1452d32265243f58520ab/TSRM-A-Lightweight-Temporal-Feature-Encoding-Architecture-for-Time-Series-Forecasting-and-Imputation.pdf", "details": "R Leppich, M Stenger, D Grillmeyer, V Borst, S Kounev", "abstract": "We introduce a temporal feature encoding architecture called Time Series Representation Model (TSRM) for multivariate time series forecasting and imputation. The architecture is structured around CNN-based representation layers \u2026"}, {"title": "Teacher privileged distillation: How to deal with imperfect teachers?", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125003855", "details": "M Mart\u00ednez-Garc\u00eda, I Inza, JA Lozano - Knowledge-Based Systems, 2025", "abstract": "The paradigm of learning using privileged information leverages privileged features present at training time, but not at prediction, as additional training information. The privileged learning process is addressed through a knowledge distillation \u2026"}, {"title": "Scale-wise Distillation of Diffusion Models", "link": "https://arxiv.org/pdf/2503.16397%3F", "details": "N Starodubcev, D Kuznedelev, A Babenko\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present SwD, a scale-wise distillation framework for diffusion models (DMs), which effectively employs next-scale prediction ideas for diffusion-based few-step generators. In more detail, SwD is inspired by the recent insights relating diffusion \u2026"}]
