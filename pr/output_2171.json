[{"title": "SLM as Guardian: Pioneering AI Safety with Small Language Models", "link": "https://arxiv.org/pdf/2405.19795", "details": "O Kwon, D Jeon, N Choi, GH Cho, C Kim, H Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Most prior safety research of large language models (LLMs) has focused on enhancing the alignment of LLMs to better suit the safety requirements of humans. However, internalizing such safeguard features into larger models brought \u2026"}, {"title": "Comparative Analysis of Open-Source Language Models in Summarizing Medical Text Data", "link": "https://arxiv.org/pdf/2405.16295", "details": "Y Chen, Z Wang, B Wen, F Zulkernine - arXiv preprint arXiv:2405.16295, 2024", "abstract": "Unstructured text in medical notes and dialogues contains rich information. Recent advancements in Large Language Models (LLMs) have demonstrated superior performance in question answering and summarization tasks on unstructured text \u2026"}, {"title": "Large Language Model Watermark Stealing With Mixed Integer Programming", "link": "https://arxiv.org/pdf/2405.19677", "details": "Z Zhang, X Zhang, Y Zhang, LY Zhang, C Chen, S Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI- generated text, and preventing its misuse. The LLM watermark scheme commonly \u2026"}, {"title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions", "link": "https://arxiv.org/pdf/2405.19744", "details": "C Li, W Yang, J Zhang, J Lu, S Wang, C Zong - arXiv preprint arXiv:2405.19744, 2024", "abstract": "Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples \u2026"}, {"title": "Language Models Need Inductive Biases to Count Inductively", "link": "https://arxiv.org/pdf/2405.20131", "details": "Y Chang, Y Bisk - arXiv preprint arXiv:2405.20131, 2024", "abstract": "Counting is a fundamental example of generalization, whether viewed through the mathematical lens of Peano's axioms defining the natural numbers or the cognitive science literature for children learning to count. The argument holds for both cases \u2026"}, {"title": "A Dataset for Evaluating Contextualized Representation of Biomedical Concepts in Language Models", "link": "https://www.nature.com/articles/s41597-024-03317-w", "details": "H Rouhizadeh, I Nikishina, A Yazdani, A Bornet\u2026 - Scientific Data, 2024", "abstract": "Due to the complexity of the biomedical domain, the ability to capture semantically meaningful representations of terms in context is a long-standing challenge. Despite important progress in the past years, no evaluation benchmark has been developed \u2026"}, {"title": "GECKO: Generative Language Model for English, Code and Korean", "link": "https://arxiv.org/pdf/2405.15640", "details": "S Oh, D Kim - arXiv preprint arXiv:2405.15640, 2024", "abstract": "We introduce GECKO, a bilingual large language model (LLM) optimized for Korean and English, along with programming languages. GECKO is pretrained on the balanced, high-quality corpus of Korean and English employing LLaMA architecture \u2026"}, {"title": "TAIA: Large Language Models are Out-of-Distribution Data Learners", "link": "https://arxiv.org/pdf/2405.20192", "details": "S Jiang, Y Liao, Y Zhang, Y Wang, Y Wang - arXiv preprint arXiv:2405.20192, 2024", "abstract": "Fine-tuning on task-specific question-answer pairs is a predominant method for enhancing the performance of instruction-tuned large language models (LLMs) on downstream tasks. However, in certain specialized domains, such as healthcare or \u2026"}, {"title": "I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models", "link": "https://arxiv.org/pdf/2405.17849", "details": "X Hu, Y Chen, D Yang, S Zhou, Z Yuan, J Yu, C Xu - arXiv preprint arXiv:2405.17849, 2024", "abstract": "Post-training quantization (PTQ) serves as a potent technique to accelerate the inference of large language models (LLMs). Nonetheless, existing works still necessitate a considerable number of floating-point (FP) operations during inference \u2026"}]
