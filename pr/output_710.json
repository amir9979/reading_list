'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Spatial-Aware Image Retrieval: A Hyperdimensional Comp'
[{"title": "Federated Learning For Heterogeneous Electronic Health Records Utilising Augmented Temporal Graph Attention Networks", "link": "https://proceedings.mlr.press/v238/molaei24a/molaei24a.pdf", "details": "S Molaei, A Thakur, G Niknam, A Soltan, H Zare\u2026 - International Conference on \u2026, 2024", "abstract": "The proliferation of decentralised electronic healthcare records (EHRs) across medical institutions requires innovative federated learning strategies for collaborative data analysis and global model training, prioritising data privacy. A prevalent issue \u2026"}, {"title": "Language Models Still Struggle to Zero-shot Reason about Time Series", "link": "https://arxiv.org/pdf/2404.11757", "details": "MA Merrill, M Tan, V Gupta, T Hartvigsen, T Althoff - arXiv preprint arXiv:2404.11757, 2024", "abstract": "Time series are critical for decision-making in fields like finance and healthcare. Their importance has driven a recent influx of works passing time series into language models, leading to non-trivial forecasting on some datasets. But it remains \u2026"}, {"title": "Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models", "link": "https://arxiv.org/pdf/2403.19521", "details": "A Lv, K Zhang, Y Chen, Y Wang, L Liu, JR Wen, J Xie\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like\" The capital of France is,\" task-specific attention heads extract the topic entity, such \u2026"}]
