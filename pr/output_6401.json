[{"title": "Untie the Knots: An Efficient Data Augmentation Strategy for Long-Context Pre-Training in Language Models", "link": "https://arxiv.org/pdf/2409.04774", "details": "J Tian, D Zheng, Y Cheng, R Wang, C Zhang, D Zhang - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLM) have prioritized expanding the context window from which models can incorporate more information. However, training models to handle long contexts presents significant challenges. These include the scarcity of high \u2026"}, {"title": "KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models", "link": "https://arxiv.org/pdf/2409.05370", "details": "Y Li, Z Wang, Y Liu, L Wang, L Liu, L Zhou - arXiv preprint arXiv:2409.05370, 2024", "abstract": "Harnessing the robust capabilities of Large Language Models (LLMs) for narrative generation, logical reasoning, and common-sense knowledge integration, this study delves into utilizing LLMs to enhance automated radiology report generation \u2026"}, {"title": "Cross-Modal Learning for Chemistry Property Prediction: Large Language Models Meet Graph Machine Learning", "link": "https://arxiv.org/pdf/2408.14964", "details": "SS Srinivas, V Runkana - arXiv preprint arXiv:2408.14964, 2024", "abstract": "In the field of chemistry, the objective is to create novel molecules with desired properties, facilitating accurate property predictions for applications such as material design and drug screening. However, existing graph deep learning methods face \u2026"}, {"title": "How Does Diverse Interpretability of Textual Prompts Impact Medical Vision-Language Zero-Shot Tasks?", "link": "https://arxiv.org/pdf/2409.00543", "details": "S Wang, C Liu, R Arcucci - arXiv preprint arXiv:2409.00543, 2024", "abstract": "Recent advancements in medical vision-language pre-training (MedVLP) have significantly enhanced zero-shot medical vision tasks such as image classification by leveraging large-scale medical image-text pair pre-training. However, the \u2026"}, {"title": "Fine-Tuned Transformers and Large Language Models for Entity Recognition in Complex Eligibility Criteria for Clinical Trials", "link": "https://aisel.aisnet.org/isd2014/proceedings2024/datascience/19/", "details": "K Kantor, M Morzy - 2024", "abstract": "This paper evaluates the\\texttt {gpt-4-turbo} model's proficiency in recognizing named entities within the clinical trial eligibility criteria. We employ prompt learning to a dataset comprising $49\\, 903$ criteria from $3\\, 314$ trials, with $120\\, 906 \u2026"}, {"title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk of Language Models", "link": "https://arxiv.org/pdf/2408.08926", "details": "AK Zhang, N Perry, R Dulepet, E Jones, JW Lin, J Ji\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have the potential to cause real- world impact. Policymakers, model providers, and other researchers in the AI and \u2026"}, {"title": "Large language models (LLMs): survey, technical frameworks, and future challenges", "link": "https://link.springer.com/article/10.1007/s10462-024-10888-y", "details": "P Kumar - Artificial Intelligence Review, 2024", "abstract": "Artificial intelligence (AI) has significantly impacted various fields. Large language models (LLMs) like GPT-4, BARD, PaLM, Megatron-Turing NLG, Jurassic-1 Jumbo etc., have contributed to our understanding and application of AI in these domains \u2026"}, {"title": "Focused Large Language Models are Stable Many-Shot Learners", "link": "https://arxiv.org/pdf/2408.13987", "details": "P Yuan, S Feng, Y Li, X Wang, Y Zhang, C Tan, B Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL \u2026"}, {"title": "The representation landscape of few-shot learning and fine-tuning in large language models", "link": "https://arxiv.org/pdf/2409.03662", "details": "D Doimo, A Serra, A Ansuini, A Cazzaniga - arXiv preprint arXiv:2409.03662, 2024", "abstract": "In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to \u2026"}]
