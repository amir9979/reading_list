[{"title": "Leveraging Foundation Language Models (FLMs) for Automated Cohort Extraction from Large EHR Databases", "link": "https://arxiv.org/pdf/2412.11472", "details": "P Mugambi, A Meliou, M Fiterau - arXiv preprint arXiv:2412.11472, 2024", "abstract": "A crucial step in cohort studies is to extract the required cohort from one or more study datasets. This step is time-consuming, especially when a researcher is presented with a dataset that they have not previously worked with. When the cohort \u2026"}, {"title": "Filipino Benchmarks for Measuring Sexist and Homophobic Bias in Multilingual Language Models from Southeast Asia", "link": "https://arxiv.org/pdf/2412.07303", "details": "LCL Gamboa, M Lee - arXiv preprint arXiv:2412.07303, 2024", "abstract": "Bias studies on multilingual models confirm the presence of gender-related stereotypes in masked models processing languages with high NLP resources. We expand on this line of research by introducing Filipino CrowS-Pairs and Filipino \u2026"}, {"title": "Training large language models to reason in a continuous latent space", "link": "https://arxiv.org/pdf/2412.06769%3F", "details": "S Hao, S Sukhbaatar, DJ Su, X Li, Z Hu, J Weston\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) are restricted to reason in the\" language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may \u2026"}, {"title": "Training Free Adaptive Text Classification through Aggregated Large Language Models", "link": "https://openreview.net/pdf%3Fid%3DqfcBET24sl", "details": "IZ Estakhraji, A Li, AM Loening, AS Chaudhari\u2026 - Advancements In Medical \u2026", "abstract": "Class-incremental classification problems typically requires continual learning of the underlying algorithm to adapt to new classes. While current-generation large language models (LLMs) can have excellent few shot performance on several tasks \u2026"}, {"title": "QAPyramid: Fine-grained Evaluation of Content Selection for Text Summarization", "link": "https://arxiv.org/pdf/2412.07096", "details": "S Zhang, D Wan, A Cattan, A Klein, I Dagan, M Bansal - arXiv preprint arXiv \u2026, 2024", "abstract": "How to properly conduct human evaluations for text summarization is a longstanding challenge. The Pyramid human evaluation protocol, which assesses content selection by breaking the reference summary into sub-units and verifying their \u2026"}, {"title": "DnDScore: Decontextualization and Decomposition for Factuality Verification in Long-Form Text Generation", "link": "https://arxiv.org/pdf/2412.13175", "details": "M Wanner, B Van Durme, M Dredze - arXiv preprint arXiv:2412.13175, 2024", "abstract": "The decompose-then-verify strategy for verification of Large Language Model (LLM) generations decomposes claims that are then independently verified. Decontextualization augments text (claims) to ensure it can be verified outside of the \u2026"}, {"title": "MedVLM: Medical Vision-Language Model for Consumer Devices", "link": "https://ieeexplore.ieee.org/abstract/document/10816095/", "details": "M Ayaz, M Khan, M Saqib, A Khelifi, M Sajjad\u2026 - IEEE Consumer Electronics \u2026, 2024", "abstract": "Generative Artificial Intelligence (GenAI) has enabled significant advancements in healthcare by supporting complex medical tasks through multimodal data processing. However, existing models often lack the adaptability required for diverse \u2026"}, {"title": "Knowledge Graph as Pre-training Corpus for Structural Reasoning via Multi-hop Linearization", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10817607.pdf", "details": "W Kim, H Jung, W Kim - IEEE Access, 2024", "abstract": "Large language models have demonstrated exceptional performance across various natural language processing tasks. However, their reliance on unstructured text corpora for pre-training limits their effectiveness in tasks requiring structured \u2026"}, {"title": "RedStone: Curating general, code, math, and QA data for large language models", "link": "https://arxiv.org/pdf/2412.03398", "details": "Y Chang, L Cui, L Dong, S Huang, Y Huang, Y Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-training Large Language Models (LLMs) on high-quality, meticulously curated datasets is widely recognized as critical for enhancing their performance and generalization capabilities. This study explores the untapped potential of Common \u2026"}]
