[{"title": "Direct Alignment of Language Models via Quality-Aware Self-Refinement", "link": "https://arxiv.org/pdf/2405.21040", "details": "R Yu, Y Wang, X Jiao, Y Zhang, JT Kwok - arXiv preprint arXiv:2405.21040, 2024", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been commonly used to align the behaviors of Large Language Models (LLMs) with human preferences. Recently, a popular alternative is Direct Policy Optimization (DPO), which replaces \u2026"}, {"title": "Calibrating Reasoning in Language Models with Internal Consistency", "link": "https://arxiv.org/pdf/2405.18711", "details": "Z Xie, J Guo, T Yu, S Li - arXiv preprint arXiv:2405.18711, 2024", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought (CoT) prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious \u2026"}, {"title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment", "link": "https://arxiv.org/pdf/2405.19332", "details": "S Zhang, D Yu, H Sharma, Z Yang, S Wang, H Hassan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed \u2026"}, {"title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning", "link": "https://arxiv.org/pdf/2406.00645", "details": "Y Fu, H Zhang, D Wu, W Xu, B Boulet - arXiv preprint arXiv:2406.00645, 2024", "abstract": "In this work, we investigate how to leverage pre-trained visual-language models (VLM) for online Reinforcement Learning (RL). In particular, we focus on sparse reward tasks with pre-defined textual task descriptions. We first identify the problem \u2026"}, {"title": "Why are Visually-Grounded Language Models Bad at Image Classification?", "link": "https://arxiv.org/pdf/2405.18415", "details": "Y Zhang, A Unell, X Wang, D Ghosh, Y Su, L Schmidt\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Image classification is one of the most fundamental capabilities of machine vision intelligence. In this work, we revisit the image classification task using visually- grounded language models (VLMs) such as GPT-4V and LLaVA. We find that \u2026"}, {"title": "Language Models Need Inductive Biases to Count Inductively", "link": "https://arxiv.org/pdf/2405.20131", "details": "Y Chang, Y Bisk - arXiv preprint arXiv:2405.20131, 2024", "abstract": "Counting is a fundamental example of generalization, whether viewed through the mathematical lens of Peano's axioms defining the natural numbers or the cognitive science literature for children learning to count. The argument holds for both cases \u2026"}, {"title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models", "link": "https://arxiv.org/pdf/2406.02224", "details": "T Fan, G Ma, Y Kang, H Gu, L Fan, Q Yang - arXiv preprint arXiv:2406.02224, 2024", "abstract": "Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small \u2026"}, {"title": "Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models", "link": "https://arxiv.org/pdf/2406.04274", "details": "X Ji, S Kulkarni, M Wang, T Xie - arXiv preprint arXiv:2406.04274, 2024", "abstract": "This work studies the challenge of aligning large language models (LLMs) with offline preference data. We focus on alignment by Reinforcement Learning from Human Feedback (RLHF) in particular. While popular preference optimization \u2026"}, {"title": "Gaussian Process Optimization for Adaptable Multi-Objective Text Generation using Linearly-Weighted Language Models", "link": "https://ssanner.github.io/papers/naacl24_gpllm.pdf", "details": "MMA Pour, A Pesaranghader, E Cohen, S Sanner - Findings of the Association for \u2026, 2024", "abstract": "In multi-objective text generation, we aim to optimize over multiple weighted aspects (eg, toxicity, semantic preservation, fluency) of the generated text. However, multi- objective weighting schemes may change dynamically in practice according to \u2026"}]
