[{"title": "Distilling Large Language Models for Efficient Clinical Information Extraction", "link": "https://arxiv.org/pdf/2501.00031", "details": "KS Vedula, A Gupta, A Swaminathan, I Lopez, S Bedi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) excel at clinical information extraction but their computational demands limit practical deployment. Knowledge distillation--the process of transferring knowledge from larger to smaller models--offers a potential \u2026"}]
