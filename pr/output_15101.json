[{"title": "Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization", "link": "https://arxiv.org/pdf/2503.22577%3F", "details": "I Pikabea, I Lacunza, O Pareras, C Escolano\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Rapid advancements in Visual Language Models (VLMs) have transformed multimodal understanding but are often constrained by generating English responses regardless of the input language. This phenomenon has been termed as \u2026"}, {"title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks", "link": "https://arxiv.org/pdf/2504.01308", "details": "J Wang, Y Zuo, Y Chai, Z Liu, Y Fu, Y Feng, K Lam - arXiv preprint arXiv:2504.01308, 2025", "abstract": "Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing \u2026"}, {"title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability", "link": "https://arxiv.org/pdf/2504.07416", "details": "J Park, S Kim, B Yoon, K Choi - arXiv preprint arXiv:2504.07416, 2025", "abstract": "Recent advancements in multi-modal models have significantly improved vision- language alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning, rely on low-resolution \u2026"}, {"title": "Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image Analysis", "link": "https://arxiv.org/pdf/2503.20047%3F", "details": "Y Xin, GC Ates, K Gong, W Shao - arXiv preprint arXiv:2503.20047, 2025", "abstract": "Vision-language models (VLMs) have shown promise in 2D medical image analysis, but extending them to 3D remains challenging due to the high computational demands of volumetric data and the difficulty of aligning 3D spatial features with \u2026"}, {"title": "Communication-Efficient and Personalized Federated Foundation Model Fine-Tuning via Tri-Matrix Adaptation", "link": "https://arxiv.org/pdf/2503.23869", "details": "Y Li, B Liu, S Huang, ZH ZHang, X Yuan, R Hong - arXiv preprint arXiv:2503.23869, 2025", "abstract": "In federated learning, fine-tuning pre-trained foundation models poses significant challenges, particularly regarding high communication cost and suboptimal model performance due to data heterogeneity between the clients. To address these issues \u2026"}, {"title": "Unveiling the mist over 3d vision-language understanding: Object-centric evaluation with chain-of-analysis", "link": "https://arxiv.org/pdf/2503.22420", "details": "J Huang, B Jia, Y Wang, Z Zhu, X Linghu, Q Li, SC Zhu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VL models, creating a\" mist\" that obscures rigorous insights into model capabilities and 3D-VL tasks. This mist persists due to three key limitations. First, flawed test data, like \u2026"}, {"title": "Hybrid-Driving: An Autonomous Driving Decision Framework Integrating Large Language Models, Knowledge Graphs and Driving Rules", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/32066/34221", "details": "J Wang, Z Wu, Q Dong, L Meng, Y Xue, Y Yang - \u2026 of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Recent advancements have underscored the exceptional analytical and situational understanding capabilities of Large Language Models (LLMs) in autonomous driving decisions. However, the inherent hallucination issues of LLMs pose significant safety \u2026"}, {"title": "Text to image generation with bidirectional multiway transformers", "link": "https://ieeexplore.ieee.org/iel8/10750449/10901938/10960474.pdf", "details": "H Bao, L Dong, S Piao, F Wei - Computational Visual Media, 2025", "abstract": "In this study, we explore the potential of Multiway Transformers for text-to-image generation to achieve performance improvements through a concise and efficient decoupled model design and the inference efficiency provided by bidirectional \u2026"}, {"title": "SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding", "link": "https://arxiv.org/pdf/2504.07745", "details": "Y Hu, Z Song, N Feng, Y Luo, J Yu, YPP Chen, W Yang - arXiv preprint arXiv \u2026, 2025", "abstract": "Video-based Large Language Models (Video-LLMs) have witnessed substantial advancements in recent years, propelled by the advancement in multi-modal LLMs. Although these models have demonstrated proficiency in providing the overall \u2026"}]
