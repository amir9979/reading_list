[{"title": "Cyclical Contrastive Learning Based on Geodesic for Zero-shot Cross-lingual Spoken Language Understanding", "link": "https://aclanthology.org/2024.findings-acl.106.pdf", "details": "X Cheng, Z Zhu, B Yang, X Zhuang, H Li, Y Zou - Findings of the Association for \u2026, 2024", "abstract": "Owing to the scarcity of labeled training data, Spoken Language Understanding (SLU) is still a challenging task in low-resource languages. Therefore, zero-shot cross-lingual SLU attracts more and more attention. Contrastive learning is widely \u2026"}, {"title": "Enhancing Discriminative Tasks by Guiding the Pre-trained Language Model with Large Language Model's Experience", "link": "https://arxiv.org/pdf/2408.08553", "details": "X Yin, C Ni, X Xu, X Li, X Yang - arXiv preprint arXiv:2408.08553, 2024", "abstract": "Large Language Models (LLMs) and pre-trained Language Models (LMs) have achieved impressive success on many software engineering tasks (eg, code completion and code generation). By leveraging huge existing code corpora (eg \u2026"}]
