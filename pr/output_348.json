'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Griffin: Mixing Gated Linear Recurrences with Local A'
[{"title": "Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing", "link": "https://arxiv.org/pdf/2402.18909", "details": "X Wu, L Pan, WY Wang, AT Luu - arXiv preprint arXiv:2402.18909, 2024", "abstract": "Knowledge editing aims to inject knowledge updates into language models to keep them correct and up-to-date. However, its current evaluation strategies are notably impractical: they solely update with well-curated structured facts (triplets with \u2026"}, {"title": "Identifying prognostic factors for survival in intensive care unit patients with SIRS or sepsis by machine learning analysis on electronic health records", "link": "https://journals.plos.org/digitalhealth/article%3Fid%3D10.1371/journal.pdig.0000459", "details": "M Mollura, D Chicco, A Paglialonga, R Barbieri - PLOS Digital Health, 2024", "abstract": "Background Systemic inflammatory response syndrome (SIRS) and sepsis are the most common causes of in-hospital death. However, the characteristics associated with the improvement in the patient conditions during the ICU stay were not fully \u2026"}, {"title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models", "link": "https://arxiv.org/html/2403.19647v1", "details": "S Marks, C Rager, EJ Michaud, Y Belinkov, D Bau\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic \u2026"}, {"title": "A Comparison of Parameter-Efficient ASR Domain Adaptation Methods for Universal Speech and Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10445894/", "details": "KC Sim, Z Huo, T Munkhdalai, N Siddhartha, A Stooke\u2026 - ICASSP 2024-2024 IEEE \u2026, 2024", "abstract": "A recent paradigm shift in artificial intelligence has seen the rise of foundation models, such as the large language models and the universal speech models. With billions of model parameters and trained with a wide range of data, these foundation \u2026"}, {"title": "A Novel Corpus of Annotated Medical Imaging Reports and Information Extraction Results Using BERT-based Language Models", "link": "https://arxiv.org/pdf/2403.18975", "details": "N Park, K Lybarger, GK Ramachandran, S Lewis\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical imaging is critical to the diagnosis, surveillance, and treatment of many health conditions, including oncological, neurological, cardiovascular, and musculoskeletal disorders, among others. Radiologists interpret these complex \u2026"}, {"title": "Information Retrieval Techniques for Question Answering based on Pre-Trained Language Models", "link": "https://rcs.cic.ipn.mx/2023_152_12/Information%2520Retrieval%2520Techniques%2520for%2520Question%2520Answering%2520based%2520on%2520Pre-Trained.pdf", "details": "A Cadena, FF L\u00f3pez-Ponce, G Sierra, J L\u00e1zaro\u2026", "abstract": "This paper presents a comparative study between two prominent pre-trained language models, RoBERTa and GPT-3, focused on their performance in Question Answering (QA). Broker exams serve as a rigorous evaluation guide, with which we \u2026"}, {"title": "Consprompt: Exploiting Contrastive Samples for Few-Shot Prompt Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10448403/", "details": "J Weng, Y Deng, D Li, H You, Y Hu, H Huang - ICASSP 2024-2024 IEEE International \u2026, 2024", "abstract": "Prompt has become an effective linguistic tool for utilizing pre-trained language models. However, in few-shot scenarios, subtle changes of prompt's design always make the result widely different, and the prompt learning methods are also easy to \u2026"}, {"title": "ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler", "link": "https://arxiv.org/pdf/2403.17536", "details": "P Mirza, V Sudhi, SR Sahoo, SR Bhat - arXiv preprint arXiv:2403.17536, 2024", "abstract": "State-of-the-art intent classification (IC) and slot filling (SF) methods often rely on data-intensive deep learning models, limiting their practicality for industry applications. Large language models on the other hand, particularly instruction \u2026"}]
