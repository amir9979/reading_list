[{"title": "Data-free Knowledge Distillation with Diffusion Models", "link": "https://arxiv.org/pdf/2504.00870", "details": "X Qi, R Li, L Peng, Q Ling, J Yu, Z Chen, P Chang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently Data-Free Knowledge Distillation (DFKD) has garnered attention and can transfer knowledge from a teacher neural network to a student neural network without requiring any access to training data. Although diffusion models are adept at \u2026"}]
