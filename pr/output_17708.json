[{"title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective", "link": "https://arxiv.org/pdf/2506.14965", "details": "Z Cheng, S Hao, T Liu, F Zhou, Y Xie, F Yao, Y Bian\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning, yet most open efforts focus narrowly on math and code, limiting our understanding of its broader applicability to general \u2026", "entry_id": "http://arxiv.org/abs/2506.14965v1", "updated": "2025-06-17 20:24:00", "published": "2025-06-17 20:24:00", "authors": "Zhoujun Cheng;Shibo Hao;Tianyang Liu;Fan Zhou;Yutao Xie;Feng Yao;Yuexin Bian;Yonghao Zhuang;Nilabjo Dey;Yuheng Zha;Yi Gu;Kun Zhou;Yuqi Wang;Yuan Li;Richard Fan;Jianshu She;Chengqian Gao;Abulhair Saparov;Haonan Li;Taylor W. Killian;Mikhail Yurochkin;Zhengzhong Liu;Eric P. Xing;Zhiting Hu", "summary": "Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360", "comment": "38 pages, 9 figures. Under review", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CL", "links": "http://arxiv.org/abs/2506.14965v1;http://arxiv.org/pdf/2506.14965v1", "pdf_url": "http://arxiv.org/pdf/2506.14965v1"}, {"title": "From Logs to Insights: Optimizing LLM Parameters for Root Cause Detection in CI/CD Build Failures", "link": "https://lup.lub.lu.se/luur/download%3Ffunc%3DdownloadFile%26recordOId%3D9202420%26fileOId%3D9202423", "details": "E Skeppner, K Thorsteinsson - LU-CS/HBG-EX, 2025", "abstract": "A significant time-consuming aspect of software development is diagnosing and understanding errors. While tools exist to streamline this process, their effectiveness often depends on predefined error patterns. For instance, some tools rely on users to \u2026"}, {"title": "MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs", "link": "https://arxiv.org/pdf/2506.15215", "details": "Y Fan, Y Wang, G Wang, J Zhai, J Liu, Q Ye, T Ruan - arXiv preprint arXiv:2506.15215, 2025", "abstract": "\u2026 In addition, we evaluate three naive **LLM** **evaluation** methods, including Pointwise, Pairwise, and Listwise, together with the recent LINKAGE approach (Yang et al.\u2026 We plan to opensource this project in hopes of contributing to more effective **LLM** \u2026", "entry_id": "http://arxiv.org/abs/2506.15215v1", "updated": "2025-06-18 07:49:13", "published": "2025-06-18 07:49:13", "authors": "Yongqi Fan;Yating Wang;Guandong Wang;Jie Zhai;Jingping Liu;Qi Ye;Tong Ruan", "summary": "Open-ended question answering (QA) is a key task for evaluating the\ncapabilities of large language models (LLMs). Compared to closed-ended QA, it\ndemands longer answer statements, more nuanced reasoning processes, and diverse\nexpressions, making refined and interpretable automatic evaluation both crucial\nand challenging. Traditional metrics like ROUGE and BERTScore struggle to\ncapture semantic similarities due to different patterns between model responses\nand reference answers. Current LLM-based evaluation approaches, such as\npairwise or listwise comparisons of candidate answers, lack intuitive\ninterpretability. While pointwise scoring of each response provides some\ndescriptions, it fails to adapt across different question contents. Most\nnotably, existing methods overlook the distinction between factoid and\nnon-factoid questions. To address these challenges, we propose\n\\textbf{MinosEval}, a novel evaluation method that first distinguishes\nopen-ended questions and then ranks candidate answers using different\nevaluation strategies. For factoid questions, it applies an adaptive key-point\nscoring strategy, while for non-factoid questions, it uses an instance-aware\nlistwise ranking strategy. Experiments on multiple open-ended QA datasets,\nincluding self-built ones with more candidate responses to complement community\nresources, show that MinosEval better aligns with human annotations and offers\nmore interpretable results.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.15215v1;http://arxiv.org/pdf/2506.15215v1", "pdf_url": "http://arxiv.org/pdf/2506.15215v1"}, {"title": "Evaluating the Intelligence of large language models: A comparative study using verbal and visual IQ tests", "link": "https://www.sciencedirect.com/science/article/pii/S2949882125000544", "details": "S Abdelkarim, D Lu, DL Flores, S Jaeggi, P Baldi - Computers in Human Behavior \u2026, 2025", "abstract": "Large language models (LLMs) excel on many specialised benchmarks, yet their general-reasoning ability remains opaque. We therefore test 18 models\u2014including GPT-4, Claude 3 and Gemini Pro\u2014on a 14-section IQ suite spanning verbal \u2026"}, {"title": "Are large", "link": "https://www.nature.com/articles/s41557-025-01865-1", "details": "KM Jablonka", "abstract": "\u2026 Owing to the absence of comprehensive **LLM** **evaluation** frameworks, it has been impossible to systematically understand the potential benefits and risks of LLMs in scientific domains and to develop systematic ways to improve them. \u2026"}, {"title": "A Survey of LLM\u00d7 DATA", "link": "https://dbgroup.cs.tsinghua.edu.cn/ligl/papers/DataAI-2025.pdf", "details": "X Zhou, J He, W Zhou, H Chen, Z Tang, H Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains. In this survey, we comprehensively review the bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale data \u2026"}, {"title": "Performance of ChatGPT-4o and Four Open-Source Large Language Models in Generating Diagnoses Based on China's Rare Disease Catalog: Comparative Study", "link": "https://www.jmir.org/2025/1/e69929/", "details": "W Zhong, YF Liu, Y Liu, K Yang, HM Gao, HH Yan\u2026 - Journal of Medical Internet \u2026, 2025", "abstract": "\u2026 For **LLM** **evaluation** , we systematically curated deidentified symptom descriptions from NRDRS, removing disease-specific identifiers (nosology, pathogenic variants, and subtype classifications) to simulate patient narratives. Given genetic basis of \u2026"}, {"title": "DeVisE: Behavioral Testing of Medical Large Language Models", "link": "https://arxiv.org/pdf/2506.15339", "details": "CZ Tagliabue, HO Boll, A Erdem, E Erdem, I Calixto - arXiv preprint arXiv:2506.15339, 2025", "abstract": "\u2026 To the best of our knowledge, by comparing model behavior on both raw and template-based clinical notes we address a relatively unexplored aspect of medical **LLM** **evaluation** since most prior approaches have focused solely on structured \u2026", "entry_id": "http://arxiv.org/abs/2506.15339v1", "updated": "2025-06-18 10:42:22", "published": "2025-06-18 10:42:22", "authors": "Camila Zurdo Tagliabue;Heloisa Oss Boll;Aykut Erdem;Erkut Erdem;Iacer Calixto", "summary": "Large language models (LLMs) are increasingly used in clinical decision\nsupport, yet current evaluation methods often fail to distinguish genuine\nmedical reasoning from superficial patterns. We introduce DeVisE (Demographics\nand Vital signs Evaluation), a behavioral testing framework for probing\nfine-grained clinical understanding. We construct a dataset of ICU discharge\nnotes from MIMIC-IV, generating both raw (real-world) and template-based\n(synthetic) versions with controlled single-variable counterfactuals targeting\ndemographic (age, gender, ethnicity) and vital sign attributes. We evaluate\nfive LLMs spanning general-purpose and medically fine-tuned variants, under\nboth zero-shot and fine-tuned settings. We assess model behavior via (1)\ninput-level sensitivity - how counterfactuals alter the likelihood of a note;\nand (2) downstream reasoning - how they affect predicted hospital\nlength-of-stay. Our results show that zero-shot models exhibit more coherent\ncounterfactual reasoning patterns, while fine-tuned models tend to be more\nstable yet less responsive to clinically meaningful changes. Notably,\ndemographic factors subtly but consistently influence outputs, emphasizing the\nimportance of fairness-aware evaluation. This work highlights the utility of\nbehavioral testing in exposing the reasoning strategies of clinical LLMs and\ninforming the design of safer, more transparent medical AI systems.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.15339v1;http://arxiv.org/pdf/2506.15339v1", "pdf_url": "http://arxiv.org/pdf/2506.15339v1"}, {"title": "video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models", "link": "https://arxiv.org/pdf/2506.15220", "details": "C Tang, Y Li, Y Yang, J Zhuang, G Sun, W Li, Z Ma\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Videos contain a wealth of information, and generating detailed and accurate descriptions in natural language is a key aspect of video understanding. In this paper, we present video-SALMONN 2, an advanced audio-visual large language model (LLM) \u2026", "entry_id": "http://arxiv.org/abs/2506.15220v1", "updated": "2025-06-18 07:58:41", "published": "2025-06-18 07:58:41", "authors": "Changli Tang;Yixuan Li;Yudong Yang;Jimin Zhuang;Guangzhi Sun;Wei Li;Zejun Ma;Chao Zhang", "summary": "Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimisation (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimised using DPO. To further improve training, we\npropose a novel multi-round DPO (MrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initialising the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilise the\nprocess. Experimental results show that MrDPO significantly enhances\nvideo-SALMONN 2's captioning accuracy, reducing the captioning error rates by\n28\\%. The final video-SALMONN 2 model, with just 7 billion parameters,\nsurpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning\ntasks, while maintaining highly competitive performance to the state-of-the-art\non widely used video question-answering benchmarks among models of similar\nsize. Codes are available at\n\\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.CL;cs.SD", "links": "http://arxiv.org/abs/2506.15220v1;http://arxiv.org/pdf/2506.15220v1", "pdf_url": "http://arxiv.org/pdf/2506.15220v1"}]
