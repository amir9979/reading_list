'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Hufu: A Modality-Agnositc Watermarking System for Pre'
[{"title": "A Diffusion-Based Pre-training Framework for Crystal Property Prediction", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/28748/29440", "details": "Z Song, Z Meng, I King - Proceedings of the AAAI Conference on Artificial \u2026, 2024", "abstract": "Many significant problems involving crystal property prediction from 3D structures have limited labeled data due to expensive and time-consuming physical simulations or lab experiments. To overcome this challenge, we propose a pretrain-finetune \u2026"}, {"title": "AI for Biomedicine in the Era of Large Language Models", "link": "https://arxiv.org/pdf/2403.15673", "details": "Z Bi, SA Dip, D Hajialigol, S Kommu, H Liu, M Lu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The capabilities of AI for biomedicine span a wide spectrum, from the atomic level, where it solves partial differential equations for quantum systems, to the molecular level, predicting chemical or protein structures, and further extending to societal \u2026"}, {"title": "Smaller Language Models are Better Zero-shot Machine-Generated Text Detectors", "link": "https://aclanthology.org/2024.eacl-short.25.pdf", "details": "N Mireshghallah, J Mattern, S Gao, R Shokri\u2026 - Proceedings of the 18th \u2026, 2024", "abstract": "As large language models are becoming more embedded in different user-facing services, it is important to be able to distinguish between human-written and machine- generated text to verify the authenticity of news articles, product reviews, etc. Thus, in \u2026"}, {"title": "An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis", "link": "https://arxiv.org/pdf/2403.12402", "details": "Y Peng, I Kulikov, Y Yang, S Popuri, H Lu, C Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Speech language models (LMs) are promising for high-quality speech synthesis through in-context learning. A typical speech LM takes discrete semantic units as content and a short utterance as prompt, and synthesizes speech which preserves \u2026"}, {"title": "Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts", "link": "https://arxiv.org/html/2403.12918v1", "details": "SA Somayajula, Y Liang, A Singh, L Zhang, P Xie - arXiv preprint arXiv:2403.12918, 2024", "abstract": "Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle \u2026"}, {"title": "A medical report generation method integrating teacher\u2013student model and encoder\u2013decoder network", "link": "https://www.sciencedirect.com/science/article/pii/S1746809424003094", "details": "S Zhang, Q Han, J Li, Y Sun, Y Qin - Biomedical Signal Processing and Control, 2024", "abstract": "The automatic medical report generation task can reduce the burden of radiologists and improve the intelligence of auxiliary diagnosis, but still faces the following challenges:(1) The small lesions are easily overlooked, leading to loss of crucial \u2026"}, {"title": "MSLM-S2ST: A Multitask Speech Language Model for Textless Speech-to-Speech Translation with Speaker Style Preservation", "link": "https://arxiv.org/pdf/2403.12408", "details": "Y Peng, I Kulikov, Y Yang, S Popuri, H Lu, C Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "There have been emerging research interest and advances in speech-to-speech translation (S2ST), translating utterances from one language to another. This work proposes Multitask Speech Language Model (MSLM), which is a decoder-only \u2026"}, {"title": "Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity", "link": "https://arxiv.org/pdf/2403.05257", "details": "S Hoshino, A Kato, S Murakami, P Zhang - arXiv preprint arXiv:2403.05257, 2024", "abstract": "Learning better sentence embeddings leads to improved performance for natural language understanding tasks including semantic textual similarity (STS) and natural language inference (NLI). As prior studies leverage large-scale labeled NLI datasets \u2026"}]
