[{"title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "link": "https://arxiv.org/pdf/2504.14194", "details": "X Zhuang, J Peng, R Ma, Y Wang, T Bai, X Wei, J Qiu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural \u2026"}, {"title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "link": "https://arxiv.org/pdf/2504.14366", "details": "P Haller, J Golde, A Akbik - arXiv preprint arXiv:2504.14366, 2025", "abstract": "Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures \u2026"}, {"title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language Models", "link": "https://arxiv.org/pdf/2504.14245", "details": "Y Ji, Y Hong, J Zhan, H Chen, H Zhu, W Wang, L Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Progress in image generation raises significant public security concerns. We argue that fake image detection should not operate as a\" black box\". Instead, an ideal approach must ensure both strong generalization and transparency. Recent \u2026"}, {"title": "Improving clinical decision support through interpretable machine learning and error handling in electronic health records", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocaf058/8117850", "details": "M Arora, H Mortagy, N Dwarshuis, J Wang, P Yang\u2026 - Journal of the American \u2026, 2025", "abstract": "Objective To develop an electronic medical record (EMR) data processing tool that confers clinical context to machine learning (ML) algorithms for error handling, bias mitigation, and interpretability. Materials and Methods We present Trust-MAPS, an \u2026"}, {"title": "Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for Large Language Models", "link": "https://arxiv.org/pdf/2504.02273%3F", "details": "H Le, D Do, D Nguyen, S Venkatesh - arXiv preprint arXiv:2504.02273, 2025", "abstract": "Recent advances in fine-tuning large language models (LLMs) with reinforcement learning (RL) have shown promising improvements in complex reasoning tasks, particularly when paired with chain-of-thought (CoT) prompting. However, these \u2026"}, {"title": "OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding", "link": "https://arxiv.org/pdf/2504.14692", "details": "S Jiang, Y Wang, S Song, Y Zhang, Z Meng, B Lei\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The practical deployment of medical vision-language models (Med-VLMs) necessitates seamless integration of textual data with diverse visual modalities, including 2D/3D images and videos, yet existing models typically employ separate \u2026"}, {"title": "Think twice: Enhancing llm reasoning by scaling multi-round test-time thinking", "link": "https://arxiv.org/pdf/2503.19855%3F", "details": "X Tian, S Zhao, H Wang, S Chen, Y Ji, Y Peng, H Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite \u2026"}, {"title": "A Survey on Mixture of Experts in Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10937907/", "details": "W Cai, J Jiang, F Wang, J Tang, S Kim, J Huang - IEEE Transactions on Knowledge \u2026, 2025", "abstract": "Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size \u2026"}, {"title": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scale Test-Time Compute", "link": "https://arxiv.org/pdf/2504.00762%3F", "details": "J Chen, Z Xun, B Zhou, H Qi, Q Zhang, Y Chen, W Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper presents a simple, effective, and cost-efficient strategy to improve LLM performance by scaling test-time compute. Our strategy builds upon the repeated- sampling-then-voting framework, with a novel twist: incorporating multiple models \u2026"}]
