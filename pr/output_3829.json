[{"title": "Language Models can be Deductive Solvers", "link": "https://aclanthology.org/2024.findings-naacl.254.pdf", "details": "J Feng, R Xu, J Hao, H Sharma, Y Shen, D Zhao\u2026 - Findings of the Association \u2026, 2024", "abstract": "Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit \u2026"}, {"title": "KU-DMIS at MEDIQA-CORR 2024: Exploring the Reasoning Capabilities of Small Language Models in Medical Error Correction", "link": "https://aclanthology.org/2024.clinicalnlp-1.51.pdf", "details": "H Hwang, T Lee, H Kim, J Kang - Proceedings of the 6th Clinical Natural Language \u2026, 2024", "abstract": "Recent advancements in large language models (LM) like OpenAI's GPT-4 have shown promise in healthcare, particularly in medical question answering and clinical applications. However, their deployment raises privacy concerns and their size limits \u2026"}, {"title": "Memory Augmented Language Models through Mixture of Word Experts", "link": "https://aclanthology.org/2024.naacl-long.249.pdf", "details": "C dos Santos, J Lee-Thorp, I Noble, CC Chang\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Scaling up the number of parameters of language models has proven to be an effective approach to improve performance. For dense models, increasing their size proportionally increases their computational footprint. In this work, we seek to \u2026"}, {"title": "Improving In-context Learning of Multilingual Generative Language Models with Cross-lingual Alignment", "link": "https://aclanthology.org/2024.naacl-long.445.pdf", "details": "C Li, S Wang, J Zhang, C Zong - Proceedings of the 2024 Conference of the North \u2026, 2024", "abstract": "Multilingual generative models obtain remarkable cross-lingual in-context learning capabilities through pre-training on large-scale corpora. However, they still exhibit a performance bias toward high-resource languages and learn isolated distributions of \u2026"}, {"title": "ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models", "link": "https://arxiv.org/pdf/2406.13342", "details": "H Jo, H Lee, T Park - arXiv preprint arXiv:2406.13342, 2024", "abstract": "The recent advancements in large language models (LLMs) have brought significant progress in solving NLP tasks. Notably, in-context learning (ICL) is the key enabling mechanism for LLMs to understand specific tasks and grasping nuances. In this \u2026"}, {"title": "A Survey on Human Preference Learning for Large Language Models", "link": "https://arxiv.org/pdf/2406.11191", "details": "R Jiang, K Chen, X Bai, Z He, J Li, M Yang, T Zhao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The recent surge of versatile large language models (LLMs) largely depends on aligning increasingly capable foundation models with human intentions by preference learning, enhancing LLMs with excellent applicability and effectiveness in \u2026"}, {"title": "Evaluating machine learning approaches for multi-label classification of unstructured electronic health records with a generative large language model", "link": "https://www.medrxiv.org/content/10.1101/2024.06.24.24309441.full.pdf", "details": "D Vithanage, C Deng, L Wang, M Yin, M Alkhalaf\u2026 - medRxiv, 2024", "abstract": "Multi-label classification of unstructured electronic health records (EHR) poses challenges due to the inherent semantic complexity in textual data. Advances in natural language processing (NLP) using large language models (LLMs) show \u2026"}, {"title": "Large Language Models are Interpretable Learners", "link": "https://arxiv.org/pdf/2406.17224", "details": "R Wang, S Si, F Yu, D Wiesmann, CJ Hsieh, I Dhillon - arXiv preprint arXiv \u2026, 2024", "abstract": "The trade-off between expressiveness and interpretability remains a core challenge when building human-centric predictive models for classification and decision- making. While symbolic rules offer interpretability, they often lack expressiveness \u2026"}, {"title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs", "link": "https://arxiv.org/pdf/2407.00653", "details": "Y Zhang, X Wang, J Liang, S Xia, L Chen, Y Xiao - arXiv preprint arXiv:2407.00653, 2024", "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new \u2026"}]
