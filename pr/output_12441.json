[{"title": "Contrastive Representation Learning Helps Cross-institutional Knowledge Transfer: A Study in Pediatric Ventilation Management", "link": "https://arxiv.org/pdf/2501.13587", "details": "J Han, P Ramnarayan, AA Faisal - arXiv preprint arXiv:2501.13587, 2025", "abstract": "Clinical machine learning deployment across institutions faces significant challenges when patient populations and clinical practices differ substantially. We present a systematic framework for cross-institutional knowledge transfer in clinical time series \u2026"}, {"title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers", "link": "https://arxiv.org/pdf/2501.16961%3F", "details": "M Raza, N Milic-Frayling - arXiv preprint arXiv:2501.16961, 2025", "abstract": "Robustness of reasoning remains a significant challenge for large language models, and addressing it is essential for the practical applicability of AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a novel approach that \u2026"}, {"title": "MedFILIP: Medical Fine-Grained Language-Image Pre-Training", "link": "https://arxiv.org/pdf/2501.10775", "details": "X Liang, X Li, F Li, J Jiang, Q Dong, W Wang, K Wang\u2026 - IEEE Journal of Biomedical \u2026, 2025", "abstract": "Medical vision-language pretraining (VLP) that leverages naturally-paired medical image-report data is crucial for medical image analysis. However, existing methods struggle to accurately characterize associations between images and diseases \u2026"}, {"title": "Privacy-ensuring open-weights large language models are competitive with closed-weights GPT-4o in extracting chest radiography findings from free-text reports", "link": "https://pubs.rsna.org/doi/pdf/10.1148/radiol.240895", "details": "S Nowak, B Wulff, YC Layer, M Theis, A Isaak, B Salam\u2026 - Radiology, 2025", "abstract": "Background Large-scale secondary use of clinical databases requires automated tools for retrospective extraction of structured content from free-text radiology reports. Purpose To share data and insights on the application of privacy-preserving open \u2026"}, {"title": "The Power of Negative Zero: Datatype Customization for Quantized Large Language Models", "link": "https://arxiv.org/pdf/2501.04052", "details": "Y Chen, X Dai, C Chang, Y Akhauri, MS Abdelfattah - arXiv preprint arXiv:2501.04052, 2025", "abstract": "Large language models (LLMs) have demonstrated remarkable performance across various machine learning tasks, quickly becoming one of the most prevalent AI workloads. Yet the substantial memory requirement of LLMs significantly hinders \u2026"}, {"title": "Foundations of Large Language Models", "link": "https://arxiv.org/pdf/2501.09223", "details": "T Xiao, J Zhu - arXiv preprint arXiv:2501.09223, 2025", "abstract": "This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting- edge technologies. The book is structured into four main chapters, each exploring a \u2026"}, {"title": "Cascaded Self-Evaluation Augmented Training for Efficient Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2501.05662", "details": "Z Lv, W Wang, J Wang, S Zhang, F Wu - arXiv preprint arXiv:2501.05662, 2025", "abstract": "Efficient Multimodal Large Language Models (EMLLMs) have rapidly advanced recently. Incorporating Chain-of-Thought (CoT) reasoning and step-by-step self- evaluation has improved their performance. However, limited parameters often \u2026"}, {"title": "A Sequential Optimal Learning Approach to Automated Prompt Engineering in Large Language Models", "link": "https://arxiv.org/pdf/2501.03508", "details": "S Wang, S Moazeni, D Klabjan - arXiv preprint arXiv:2501.03508, 2025", "abstract": "Designing effective prompts is essential to guiding large language models (LLMs) toward desired responses. Automated prompt engineering aims to reduce reliance on manual effort by streamlining the design, refinement, and optimization of natural \u2026"}]
