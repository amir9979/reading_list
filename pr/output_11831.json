[{"title": "Are Vision-Language Models Truly Understanding Multi-vision Sensor?", "link": "https://arxiv.org/pdf/2412.20750", "details": "S Chung, Y Yu, Y Chee, SY Kim, BK Lee, YM Ro - arXiv preprint arXiv:2412.20750, 2024", "abstract": "Large-scale Vision-Language Models (VLMs) have advanced by aligning vision inputs with text, significantly improving performance in computer vision tasks. Moreover, for VLMs to be effectively utilized in real-world applications, an \u2026"}, {"title": "Instruction-Guided Fusion of Multi-Layer Visual Features in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2501.08443", "details": "X Li, Y Zheng, H Chen, X Chen, Y Liang, C Lai - arXiv preprint arXiv:2501.08443, 2024", "abstract": "Large Vision-Language Models (LVLMs) have achieved significant success in multimodal tasks by combining pre-trained vision encoders and large language models. However, current LVLMs mainly rely on features from the final layers of the \u2026"}, {"title": "Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach", "link": "https://arxiv.org/pdf/2412.18108", "details": "J Bi, J Guo, Y Tang, LB Wen, Z Liu, C Xu - arXiv preprint arXiv:2412.18108, 2024", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated remarkable progress in visual understanding. This impressive leap raises a compelling question: how can language models, initially trained solely on \u2026"}, {"title": "Bactrainus: Optimizing Large Language Models for Multi-hop Complex Question Answering Tasks", "link": "https://arxiv.org/pdf/2501.06286", "details": "I Barati, A Ghafouri, B Minaei-Bidgoli - arXiv preprint arXiv:2501.06286, 2025", "abstract": "In recent years, the use of large language models (LLMs) has significantly increased, and these models have demonstrated remarkable performance in a variety of general language tasks. However, the evaluation of their performance in domain \u2026"}, {"title": "Enhancing Financial VQA in Vision Language Models using Intermediate Structured Representations", "link": "https://arxiv.org/pdf/2501.04675", "details": "A Srivastava, A Kumar, R Kumar, P Srinivasan - arXiv preprint arXiv:2501.04675, 2025", "abstract": "Chart interpretation is crucial for visual data analysis, but accurately extracting information from charts poses significant challenges for automated models. This study investigates the fine-tuning of DEPLOT, a modality conversion module that \u2026"}, {"title": "From Models to Microtheories: Distilling a Model's Topical Knowledge for Grounded Question Answering", "link": "https://arxiv.org/pdf/2412.17701", "details": "N Weir, BD Mishra, O Weller, O Tafjord, S Hornstein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent reasoning methods (eg, chain-of-thought, entailment reasoning) help users understand how language models (LMs) answer a single question, but they do little to reveal the LM's overall understanding, or\" theory,\" about the question's $\\textit \u2026"}, {"title": "PersianMHQA: A Dataset for Open Domain Persian Multi-hop Question Answering Based on Wikipedia Encyclopedia", "link": "https://dl.acm.org/doi/pdf/10.1145/3711826", "details": "M Taji, A Ghafouri, H Naderi, B Minaei-Bidgoli - ACM Transactions on Asian and Low \u2026, 2025", "abstract": "Today, one of the most important tasks in natural language processing is answering user questions. Especially, users' questions nowadays moved from simple questions to complex questions. In recent years, several question answering datasets have \u2026"}, {"title": "Fairness-driven federated learning-based spam email detection using clustering techniques", "link": "https://link.springer.com/article/10.1007/s00521-024-10969-7", "details": "V Kaushal, S Sharma - Neural Computing and Applications, 2025", "abstract": "In the world of emails, spam messages present a significant challenge, leading to inconveniences and potential security risks. Addressing this issue, the task of spotting spam in emails is critical for ensuring secure and trustworthy \u2026"}, {"title": "Clinical Decision Support using Pseudo-notes from multiple streams of EHR Data", "link": "https://openreview.net/pdf%3Fid%3DFG45RbP9Ka", "details": "SA Lee, S Jain, A Chen, K Ono, A Biswas, A Rudas\u2026", "abstract": "Electronic health records (EHR) contain data from disparate sources, spanning various biological and temporal scales. In this work, we introduce the Multiple Embedding Model for EHR (MEME), a deep learning framework for clinical decision \u2026"}]
