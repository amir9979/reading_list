'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Pre-trained Vision-Language Models Learn Discoverable '
[{"title": "GPT for medical entity recognition in Spanish", "link": "https://link.springer.com/article/10.1007/s11042-024-19209-5", "details": "\u00c1 Garc\u00eda-Barrag\u00e1n, A Gonz\u00e1lez Calatayud\u2026 - Multimedia Tools and \u2026, 2024", "abstract": "In recent years, there has been a remarkable surge in the development of Natural Language Processing (NLP) models, particularly in the realm of Named Entity Recognition (NER). Models such as BERT have demonstrated exceptional \u2026"}, {"title": "VALOR-EVAL: Holistic Coverage and Faithfulness Evaluation of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2404.13874", "details": "H Qiu, W Hu, ZY Dou, N Peng - arXiv preprint arXiv:2404.13874, 2024", "abstract": "Large Vision-Language Models (LVLMs) suffer from hallucination issues, wherein the models generate plausible-sounding but factually incorrect outputs, undermining their reliability. A comprehensive quantitative evaluation is necessary to identify and \u2026"}, {"title": "Gaze-infused BERT: Do human gaze signals help pre-trained language models?", "link": "https://link.springer.com/article/10.1007/s00521-024-09725-8", "details": "B Wang, B Liang, L Zhou, R Xu - Neural Computing and Applications, 2024", "abstract": "This research delves into the intricate connection between self-attention mechanisms in large-scale pre-trained language models, like BERT, and human gaze patterns, with the aim of harnessing gaze information to enhance the performance of natural \u2026"}, {"title": "REXEL: An End-to-end Model for Document-Level Relation Extraction and Entity Linking", "link": "https://arxiv.org/pdf/2404.12788", "details": "N Bouziani, S Tyagi, J Fisher, J Lehmann, A Pierleoni - arXiv preprint arXiv \u2026, 2024", "abstract": "Extracting structured information from unstructured text is critical for many downstream NLP applications and is traditionally achieved by closed information extraction (cIE). However, existing approaches for cIE suffer from two limitations:(i) \u2026"}]
