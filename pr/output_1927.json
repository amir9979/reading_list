[{"title": "Adaptive Reinforcement Tuning Language Models as Hard Data Generators for Sentence Representation", "link": "https://aclanthology.org/2024.lrec-main.33.pdf", "details": "B Xu, Y Wu, S Wei, M Du, H Wang - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Sentence representation learning is a fundamental task in NLP. Existing methods use contrastive learning (CL) to learn effective sentence representations, which benefit from high-quality contrastive data but require extensive human annotation \u2026"}, {"title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models", "link": "https://arxiv.org/pdf/2405.14366", "details": "A Liu, J Liu, Z Pan, Y He, G Haffari, B Zhuang - arXiv preprint arXiv:2405.14366, 2024", "abstract": "A critical approach for efficiently deploying computationally demanding large language models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive \u2026"}, {"title": "Large Language Models Can Self-Correct with Minimal Effort", "link": "https://arxiv.org/pdf/2405.14092", "details": "Z Wu, Q Zeng, Z Zhang, Z Tan, C Shen, M Jiang - arXiv preprint arXiv:2405.14092, 2024", "abstract": "Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple \u2026"}, {"title": "CIVICS: Building a Dataset for Examining Culturally-Informed Values in Large Language Models", "link": "https://arxiv.org/pdf/2405.13974", "details": "G Pistilli, A Leidinger, Y Jernite, A Kasirzadeh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces the\" CIVICS: Culturally-Informed & Values-Inclusive Corpus for Societal impacts\" dataset, designed to evaluate the social and cultural variation of Large Language Models (LLMs) across multiple languages and value-sensitive \u2026"}, {"title": "Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data", "link": "https://arxiv.org/pdf/2405.14212", "details": "H Li, X Zhao, D Guo, H Gu, Z Zeng, Y Han, Y Song\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As large language models (LLMs) demonstrate unparalleled performance and generalization ability, LLMs are widely used and integrated into various applications. When it comes to sensitive domains, as commonly described in federated learning \u2026"}, {"title": "Large Language Models are Good Spontaneous Multilingual Learners: Is the Multilingual Annotated Data Necessary?", "link": "https://arxiv.org/pdf/2405.13816", "details": "S Zhang, C Gao, W Zhu, J Chen, X Huang, X Han\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, Large Language Models (LLMs) have shown impressive language capabilities. However, most of the existing LLMs are all English-centric, which have very unstable and unbalanced performance across different languages. Multilingual \u2026"}, {"title": "Enhancing Text-to-SQL Capabilities of Large Language Models through Tailored Promptings", "link": "https://aclanthology.org/2024.lrec-main.539.pdf", "details": "Z Tan, X Liu, Q Shu, X Li, C Wan, D Liu, Q Wan, G Liao - Proceedings of the 2024 \u2026, 2024", "abstract": "Large language models (LLMs) with prompting have achieved encouraging results on many natural language processing (NLP) tasks based on task-tailored promptings. Text-to-SQL is a critical task that generates SQL queries from natural \u2026"}, {"title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation", "link": "https://arxiv.org/pdf/2405.13769", "details": "C Chhun, FM Suchanek, C Clavel - arXiv preprint arXiv:2405.13769, 2024", "abstract": "Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high \u2026"}, {"title": "Distributed Speculative Inference of Large Language Models", "link": "https://arxiv.org/pdf/2405.14105", "details": "N Timor, J Mamou, D Korat, M Berchansky, O Pereg\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Accelerating the inference of large language models (LLMs) is an important challenge in artificial intelligence. This paper introduces distributed speculative inference (DSI), a novel distributed inference algorithm that is provably faster than \u2026"}]
