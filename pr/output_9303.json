[{"title": "Neural Fields in Robotics: A Survey", "link": "https://arxiv.org/pdf/2410.20220", "details": "MZ Irshad, M Comi, YC Lin, N Heppert, A Valada\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Neural Fields have emerged as a transformative approach for 3D scene representation in computer vision and robotics, enabling accurate inference of geometry, 3D semantics, and dynamics from posed 2D data. Leveraging \u2026"}, {"title": "Domain Adaptation with a Single Vision-Language Embedding", "link": "https://arxiv.org/pdf/2410.21361", "details": "M Fahes, TH Vu, A Bursuc, P P\u00e9rez, R de Charette - arXiv preprint arXiv:2410.21361, 2024", "abstract": "Domain adaptation has been extensively investigated in computer vision but still requires access to target data at the training time, which might be difficult to obtain in some uncommon conditions. In this paper, we present a new framework for domain \u2026"}, {"title": "Med-2E3: A 2D-Enhanced 3D Medical Multimodal Large Language Model", "link": "https://arxiv.org/pdf/2411.12783", "details": "Y Shi, X Zhu, Y Hu, C Guo, M Li, J Wu - arXiv preprint arXiv:2411.12783, 2024", "abstract": "The analysis of 3D medical images is crucial for modern healthcare, yet traditional task-specific models are becoming increasingly inadequate due to limited generalizability across diverse clinical scenarios. Multimodal large language models \u2026"}, {"title": "Human-level information extraction from clinical reports with fine-tuned language models", "link": "https://www.medrxiv.org/content/10.1101/2024.11.18.24317466.full.pdf", "details": "L Liu, L Lian, Y Hao, A Pace, E Kim, N Homsi\u2026 - medRxiv, 2024", "abstract": "Background: Extracting structured data from clinical notes is a key bottleneck in developing AI tools for radiology and pathology. Manual annotation is labor-intensive and unscalable. An efficient, automated method for clinical information extraction \u2026"}, {"title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding", "link": "https://aclanthology.org/2024.emnlp-main.870.pdf", "details": "L Tu, S Yavuz, J Qu, J Xu, R Meng, C Xiong, Y Zhou - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired \u2026"}, {"title": "Large Language Models Can Be Contextual Privacy Protection Learners", "link": "https://aclanthology.org/2024.emnlp-main.785.pdf", "details": "Y Xiao, Y Jin, Y Bai, Y Wu, X Yang, X Luo, W Yu\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data \u2026"}, {"title": "Language Model Self-improvement by Reinforcement Learning Contemplation without External Supervision", "link": "https://jingchengpang.github.io/files/pdf/jair_rlc.pdf", "details": "JC Pang, K Li, P Wang, XH Chen, J Xu, Z Zhang, Y Yu", "abstract": "Abstract Language model self-improvement (LMSI) techniques have recently gained significant attention as they improve language models without requiring external supervision. A notable approach is reinforcement learning from AI feedback (RLAIF) \u2026"}]
