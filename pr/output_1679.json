'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Sentence salience contrastive learning for abstractive text '
[{"title": "HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models", "link": "https://arxiv.org/abs/2405.10299", "details": "RS Sukthanker, A Zela, B Staffler, JKH Franke, F Hutter - arXiv preprint arXiv \u2026, 2024", "abstract": "The expanding size of language models has created the necessity for a comprehensive examination across various dimensions that reflect the desiderata with respect to the tradeoffs between various hardware metrics, such as latency \u2026"}, {"title": "Representation Degeneration Problem in Prompt-based Models for Natural Language Understanding", "link": "https://aclanthology.org/2024.lrec-main.1217.pdf", "details": "Q Zhao, R He, J Zhang, C Liu, B Wang - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Prompt-based fine-tuning (PF), by aligning with the training objective of pre-trained language models (PLMs), has shown improved performance on many few-shot natural language understanding (NLU) benchmarks. However, the word embedding \u2026"}]
