[{"title": "Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models", "link": "https://arxiv.org/pdf/2410.02681", "details": "S Wang, Y Li, H Wei - arXiv preprint arXiv:2410.02681, 2024", "abstract": "Confidence calibration is critical for the safe deployment of machine learning models in the real world. However, such issue in vision-language models like CLIP, particularly after fine-tuning, has not been fully addressed. In this work, we \u2026"}, {"title": "Fast Training of Sinusoidal Neural Fields via Scaling Initialization", "link": "https://arxiv.org/pdf/2410.04779", "details": "T Yeom, S Lee, J Lee - arXiv preprint arXiv:2410.04779, 2024", "abstract": "Neural fields are an emerging paradigm that represent data as continuous functions parameterized by neural networks. Despite many advantages, neural fields often have a high training cost, which prevents a broader adoption. In this paper, we focus \u2026"}, {"title": "Bi-VLGM: Bi-Level Class-Severity-Aware Vision-Language Graph Matching for Text Guided Medical Image Segmentation", "link": "https://link.springer.com/article/10.1007/s11263-024-02246-w", "details": "W Chen, J Liu, T Liu, Y Yuan - International Journal of Computer Vision, 2024", "abstract": "Medical reports containing specific diagnostic results and additional information not present in medical images can be effectively employed to assist image understanding tasks, and the modality gap between vision and language can be \u2026"}, {"title": "Distillation of Vision Language Models for Enhancing End-to-End Autonomous Driving", "link": "https://mllmav.github.io/papers/Distillation%2520of%2520Vision%2520Language%2520Models%2520for%2520Enhancing%2520End-to-End%2520Autonomous%2520Driving.pdf", "details": "F Tao, A Mallik, C Pan, X Ye, Y Guo, B Yaman, L Ren", "abstract": "In recent years, large vision and language models (VLMs) have been investigated in autonomous driving to address long-standing issues including, reasoning, generalization, and long-tail scenarios. However, efficient integration of VLMs into \u2026"}, {"title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models", "link": "https://arxiv.org/pdf/2410.01335", "details": "L Bandarkar, B Muller, P Yuvraj, R Hou, N Singhal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large \u2026"}]
