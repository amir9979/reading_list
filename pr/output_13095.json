[{"title": "A Distributional Perspective on Word Learning in Neural Language Models", "link": "https://arxiv.org/pdf/2502.05892", "details": "F Ficarra, R Cotterell, A Warstadt - arXiv preprint arXiv:2502.05892, 2025", "abstract": "Language models (LMs) are increasingly being studied as models of human language learners. Due to the nascency of the field, it is not well-established whether LMs exhibit similar learning dynamics to humans, and there are few direct \u2026"}, {"title": "SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities", "link": "https://arxiv.org/pdf/2502.12025", "details": "F Jiang, Z Xu, Y Li, L Niu, Z Xiang, B Li, BY Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently \u2026"}, {"title": "Language Models Can Predict Their Own Behavior", "link": "https://arxiv.org/pdf/2502.13329", "details": "D Ashok, J May - arXiv preprint arXiv:2502.13329, 2025", "abstract": "Autoregressive Language Models output text by sequentially predicting the next token to generate, with modern methods like Chain-of-Thought (CoT) prompting achieving state-of-the-art reasoning capabilities by scaling the number of generated \u2026"}, {"title": "Evaluating the Paperclip Maximizer: Are RL-Based Language Models More Likely to Pursue Instrumental Goals?", "link": "https://arxiv.org/pdf/2502.12206", "details": "Y He, Y Li, J Wu, Y Sui, Y Chen, B Hooi - arXiv preprint arXiv:2502.12206, 2025", "abstract": "As large language models (LLMs) continue to evolve, ensuring their alignment with human goals and values remains a pressing challenge. A key concern is\\textit {instrumental convergence}, where an AI system, in optimizing for a given objective \u2026"}, {"title": "A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models", "link": "https://arxiv.org/pdf/2502.13942", "details": "H Huang, S Yuan, Y Hao, C Wen, Y Fang - arXiv preprint arXiv:2502.13942, 2025", "abstract": "A large-scale vision and language model that has been pretrained on massive data encodes visual and linguistic prior, which makes it easier to generate images and language that are more natural and realistic. Despite this, there is still a significant \u2026"}, {"title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers", "link": "https://arxiv.org/pdf/2501.16961%3F", "details": "M Raza, N Milic-Frayling - arXiv preprint arXiv:2501.16961, 2025", "abstract": "Robustness of reasoning remains a significant challenge for large language models, and addressing it is essential for the practical applicability of AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a novel approach that \u2026"}, {"title": "Adapting Generative Large Language Models for Information Extraction from Unstructured Electronic Health Records in Residential Aged Care: A Comparative \u2026", "link": "https://link.springer.com/article/10.1007/s41666-025-00190-z", "details": "D Vithanage, C Deng, L Wang, M Yin, M Alkhalaf\u2026 - Journal of Healthcare \u2026, 2025", "abstract": "Abstract Information extraction (IE) of unstructured electronic health records is challenging due to the semantic complexity of textual data. Generative large language models (LLMs) offer promising solutions to address this challenge \u2026"}, {"title": "Game Theory Meets Large Language Models: A Systematic Survey", "link": "https://arxiv.org/pdf/2502.09053", "details": "H Sun, Y Wu, Y Cheng, X Chu - arXiv preprint arXiv:2502.09053, 2025", "abstract": "Game theory establishes a fundamental framework for analyzing strategic interactions among rational decision-makers. The rapid advancement of large language models (LLMs) has sparked extensive research exploring the intersection \u2026"}, {"title": "Large language models improve transferability of electronic health record-based predictions across countries and coding systems", "link": "https://www.medrxiv.org/content/medrxiv/early/2025/02/04/2025.02.03.25321597.full.pdf", "details": "M Kirchler, M Ferro, V Lorenzini, FinnGen, C Lippert\u2026 - medRxiv, 2025", "abstract": "Variation in medical practices and reporting standards across healthcare systems limits the transferability of prediction models based on structured electronic health record (EHR) data. We introduce GRASP, a novel transformer-based architecture \u2026"}]
