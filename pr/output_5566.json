[{"title": "Refining Packing and Shuffling Strategies for Enhanced Performance in Generative Language Models", "link": "https://arxiv.org/pdf/2408.09621", "details": "Y Chen, R Wang, Z Yang, LY Jiang, EK Oermann - arXiv preprint arXiv:2408.09621, 2024", "abstract": "Packing and shuffling tokens is a common practice in training auto-regressive language models (LMs) to prevent overfitting and improve efficiency. Typically documents are concatenated to chunks of maximum sequence length (MSL) and \u2026"}, {"title": "R2GenCSR: Retrieving Context Samples for Large Language Model based X-ray Medical Report Generation", "link": "https://arxiv.org/pdf/2408.09743", "details": "X Wang, Y Li, F Wang, S Wang, C Li, B Jiang - arXiv preprint arXiv:2408.09743, 2024", "abstract": "Inspired by the tremendous success of Large Language Models (LLMs), existing X- ray medical report generation methods attempt to leverage large models to achieve better performance. They usually adopt a Transformer to extract the visual features of \u2026"}, {"title": "Improving VTE Identification through Language Models from Radiology Reports: A Comparative Study of Mamba, Phi-3 Mini, and BERT", "link": "https://arxiv.org/pdf/2408.09043", "details": "J Deng, Y Wu, Y Yesha, P Nguyen - arXiv preprint arXiv:2408.09043, 2024", "abstract": "Venous thromboembolism (VTE) is a critical cardiovascular condition, encompassing deep vein thrombosis (DVT) and pulmonary embolism (PE). Accurate and timely identification of VTE is essential for effective medical care. This study builds upon our \u2026"}, {"title": "Goldfish: Monolingual Language Models for 350 Languages", "link": "https://arxiv.org/pdf/2408.10441", "details": "TA Chang, C Arnett, Z Tu, BK Bergen - arXiv preprint arXiv:2408.10441, 2024", "abstract": "For many low-resource languages, the only available language models are large multilingual models trained on many languages simultaneously. However, using FLORES perplexity as a metric, we find that these models perform worse than \u2026"}, {"title": "Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference", "link": "https://arxiv.org/pdf/2408.01935", "details": "K Shen, M Kejriwal - arXiv preprint arXiv:2408.01935, 2024", "abstract": "Despite their impressive performance, large language models (LLMs) such as ChatGPT are known to pose important risks. One such set of risks arises from misplaced confidence, whether over-confidence or under-confidence, that the \u2026"}]
