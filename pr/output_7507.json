[{"title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks", "link": "https://arxiv.org/pdf/2409.07353", "details": "MZ Hossain, A Imteaj - arXiv preprint arXiv:2409.07353, 2024", "abstract": "Large Vision-Language Models (LVLMs), trained on multimodal big datasets, have significantly advanced AI by excelling in vision-language tasks. However, these models remain vulnerable to adversarial attacks, particularly jailbreak attacks, which \u2026"}, {"title": "Egalitarian Language Representation in Language Models: It All Begins with Tokenizers", "link": "https://arxiv.org/pdf/2409.11501", "details": "M Velayuthan, K Sarveswaran - arXiv preprint arXiv:2409.11501, 2024", "abstract": "Tokenizers act as a bridge between human language and the latent space of language models, influencing how language is represented in these models. Due to the immense popularity of English-Centric Large Language Models (LLMs), efforts \u2026"}, {"title": "Heterogeneous graph contrastive learning with adaptive data augmentation for semi\u2010supervised short text classification", "link": "https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.13744", "details": "M Wu, Z Xu, L Zheng - Expert Systems", "abstract": "Short text classification has been widely used in many fields. Due to the scarcity of labelled data, implementing short text classification under semi\u2010supervised learning setting has become increasingly popular. Semi\u2010supervised short text classification \u2026"}, {"title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2409.17539", "details": "T Liu, W Xu, W Huang, X Wang, J Wang, H Yang, J Li - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can \u2026"}]
