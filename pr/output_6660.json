[{"title": "CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks", "link": "https://arxiv.org/pdf/2409.03381", "details": "Y Deng, X Qiu, X Tan, C Qu, J Pan, Y Cheng, Y Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Cognitive psychology investigates perception, attention, memory, language, problem- solving, decision-making, and reasoning. Kahneman's dual-system theory elucidates the human decision-making process, distinguishing between the rapid, intuitive \u2026"}, {"title": "Selective Self-Rehearsal: A Fine-Tuning Approach to Improve Generalization in Large Language Models", "link": "https://arxiv.org/pdf/2409.04787", "details": "S Gupta, Y Nandwani, A Yehudai, M Mishra, G Pandey\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task \u2026"}, {"title": "Revolutionizing Database Q&A with Large Language Models: Comprehensive Benchmark and Evaluation", "link": "https://arxiv.org/pdf/2409.04475", "details": "Y Zheng, B Li, Z Lin, Y Luo, X Zhou, C Lin, J Su, G Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The development of Large Language Models (LLMs) has revolutionized Q&A across various industries, including the database domain. However, there is still a lack of a comprehensive benchmark to evaluate the capabilities of different LLMs and their \u2026"}, {"title": "Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications", "link": "https://arxiv.org/pdf/2409.05314", "details": "A Maatouk, KC Ampudia, R Ying, L Tassiulas - arXiv preprint arXiv:2409.05314, 2024", "abstract": "The emergence of large language models (LLMs) has significantly impacted various fields, from natural language processing to sectors like medicine and finance. However, despite their rapid proliferation, the applications of LLMs in \u2026"}, {"title": "PiTe: Pixel-Temporal Alignment for Large Video-Language Model", "link": "https://arxiv.org/pdf/2409.07239", "details": "Y Liu, P Ding, S Huang, M Zhang, H Zhao, D Wang - arXiv preprint arXiv:2409.07239, 2024", "abstract": "Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models (LVLMs) have emerged as a pivotal advancement, bridging the gap between image and text. However, video making it challenging for LVLMs to perform \u2026"}, {"title": "Booster: Tackling Harmful Fine-tuing for Large Language Models via Attenuating Harmful Perturbation", "link": "https://arxiv.org/pdf/2409.01586", "details": "T Huang, S Hu, F Ilhan, SF Tekin, L Liu - arXiv preprint arXiv:2409.01586, 2024", "abstract": "Harmful fine-tuning issue\\citep {qi2023fine} poses serious safety concerns for Large language models' fine-tuning-as-a-service. While existing defenses\\citep {huang2024vaccine, rosati2024representation} have been proposed to mitigate the \u2026"}, {"title": "Targeted training for numerical reasoning with large language models", "link": "https://link.springer.com/article/10.1007/s10115-024-02216-1", "details": "X Li, S Liu, Y Zhu, G Cheng - Knowledge and Information Systems, 2024", "abstract": "After recent gains achieved by large language models (LLMs) on numerical reasoning tasks, it has become of interest to have LLMs teach small models to improve on numerical reasoning. Instructing LLMs to generate Chains of Thought to \u2026"}, {"title": "Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language Models", "link": "https://arxiv.org/pdf/2409.01584", "details": "S Ozaki, K Hayashi, Y Sakai, H Kamigaito, K Hayashi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As the performance of Large-scale Vision Language Models (LVLMs) improves, they are increasingly capable of responding in multiple languages, and there is an expectation that the demand for explanations generated by LVLMs will grow \u2026"}, {"title": "ELMS: Elasticized Large Language Models On Mobile Devices", "link": "https://arxiv.org/pdf/2409.09071", "details": "W Yin, R Yi, D Xu, G Huang, M Xu, X Liu - arXiv preprint arXiv:2409.09071, 2024", "abstract": "On-device Large Language Models (LLMs) are revolutionizing mobile AI, enabling applications such as UI automation while addressing privacy concerns. Currently, the standard approach involves deploying a single, robust LLM as a universal solution \u2026"}]
