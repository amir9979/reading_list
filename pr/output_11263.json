[{"title": "Language Models as Continuous Self-Evolving Data Engineers", "link": "https://arxiv.org/pdf/2412.15151%3F", "details": "P Wang, M Wang, Z Ma, X Yang, S Feng, D Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert \u2026"}, {"title": "Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.06775", "details": "YL Lee, YH Tsai, WC Chiu - arXiv preprint arXiv:2412.06775, 2024", "abstract": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To \u2026"}, {"title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules", "link": "https://arxiv.org/pdf/2412.13536", "details": "K Chen, L Wang, Q Zhang, R Xu - arXiv preprint arXiv:2412.13536, 2024", "abstract": "Recent studies have highlighted the limitations of large language models in mathematical reasoning, particularly their inability to capture the underlying logic. Inspired by meta-learning, we propose that models should acquire not only task \u2026"}, {"title": "FastVLM: Efficient Vision Encoding for Vision Language Models", "link": "https://arxiv.org/pdf/2412.13303", "details": "PKA Vasu, F Faghri, CL Li, C Koc, N True, A Antony\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high \u2026"}, {"title": "Are Vision-Language Models Truly Understanding Multi-vision Sensor?", "link": "https://arxiv.org/pdf/2412.20750", "details": "S Chung, Y Yu, Y Chee, SY Kim, BK Lee, YM Ro - arXiv preprint arXiv:2412.20750, 2024", "abstract": "Large-scale Vision-Language Models (VLMs) have advanced by aligning vision inputs with text, significantly improving performance in computer vision tasks. Moreover, for VLMs to be effectively utilized in real-world applications, an \u2026"}, {"title": "HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.08378", "details": "S Zhu, W Dong, J Song, Y Guo, B Zheng - arXiv preprint arXiv:2412.08378, 2024", "abstract": "Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into \u2026"}, {"title": "Retaining and Enhancing Pre-trained Knowledge in Vision-Language Models with Prompt Ensembling", "link": "https://arxiv.org/pdf/2412.07077", "details": "D Kim, Y Jo, M Lee, T Kim - arXiv preprint arXiv:2412.07077, 2024", "abstract": "The advancement of vision-language models, particularly the Contrastive Language- Image Pre-training (CLIP) model, has revolutionized the field of machine learning by enabling robust zero-shot learning capabilities. These capabilities allow models to \u2026"}, {"title": "From Uncertainty to Trust: Enhancing Reliability in Vision-Language Models with Uncertainty-Guided Dropout Decoding", "link": "https://arxiv.org/pdf/2412.06474", "details": "Y Fang, Z Yang, Z Chen, Z Zhao, J Zhou - arXiv preprint arXiv:2412.06474, 2024", "abstract": "Large vision-language models (LVLMs) demonstrate remarkable capabilities in multimodal tasks but are prone to misinterpreting visual inputs, often resulting in hallucinations and unreliable outputs. To address these challenges, we propose \u2026"}, {"title": "Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach", "link": "https://arxiv.org/pdf/2412.18108", "details": "J Bi, J Guo, Y Tang, LB Wen, Z Liu, C Xu - arXiv preprint arXiv:2412.18108, 2024", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated remarkable progress in visual understanding. This impressive leap raises a compelling question: how can language models, initially trained solely on \u2026"}]
