[{"title": "SearchLVLMs: A Plug-and-Play Framework for Augmenting Large Vision-Language Models by Searching Up-to-Date Internet Knowledge", "link": "https://openreview.net/pdf%3Fid%3Dleeosk2RAM", "details": "C Li, Z Li, C Jing, S Liu, W Shao, Y Wu, P Luo, Y Qiao\u2026 - The Thirty-eighth Annual \u2026, 2024", "abstract": "Large vision-language models (LVLMs) are ignorant of the up-to-date knowledge, such as LLaVA series, because they cannot be updated frequently due to the large amount of resources required, and therefore fail in many cases. For example, if a \u2026"}, {"title": "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties", "link": "https://aclanthology.org/2024.emnlp-main.1137.pdf", "details": "K Yu, Z Zhang, F Hu, S Storks, J Chai - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "A major reason behind the recent success of large language models (LLMs) is their incontext learning capability, which makes it possible to rapidly adapt them to downstream textbased tasks by prompting them with a small number of relevant \u2026"}, {"title": "FoPru: Focal Pruning for Efficient Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.14164", "details": "L Jiang, W Huang, T Liu, Y Zeng, J Li, L Cheng, X Xu - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) represent a significant advancement toward achieving superior multimodal capabilities by enabling powerful Large Language Models (LLMs) to understand visual input. Typically, LVLMs utilize visual \u2026"}, {"title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step", "link": "https://arxiv.org/pdf/2411.10440%3F", "details": "G Xu, P Jin, L Hao, Y Song, L Sun, L Yuan - arXiv preprint arXiv:2411.10440, 2024", "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to \u2026"}, {"title": "Lifelong Knowledge Editing for Vision Language Models with Low-Rank Mixture-of-Experts", "link": "https://arxiv.org/pdf/2411.15432", "details": "Q Chen, C Wang, D Wang, T Zhang, W Li, X He - arXiv preprint arXiv:2411.15432, 2024", "abstract": "Model editing aims to correct inaccurate knowledge, update outdated information, and incorporate new data into Large Language Models (LLMs) without the need for retraining. This task poses challenges in lifelong scenarios where edits must be \u2026"}, {"title": "Exploring Visual Vulnerabilities via Multi-Loss Adversarial Search for Jailbreaking Vision-Language Models", "link": "https://arxiv.org/pdf/2411.18000", "details": "S Hao, B Hooi, J Liu, KW Chang, Z Huang, Y Cai - arXiv preprint arXiv:2411.18000, 2024", "abstract": "Despite inheriting security measures from underlying language models, Vision- Language Models (VLMs) may still be vulnerable to safety alignment issues. Through empirical analysis, we uncover two critical findings: scenario-matched \u2026"}, {"title": "A cascaded retrieval-while-reasoning multi-document comprehension framework with incremental attention for medical question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0957417424025685", "details": "J Liu, J Ren, R Bai, Z Zhang, Z Lu - Expert Systems with Applications, 2024", "abstract": "Abstract Clinical Machine Reading Comprehension (MRC) is challenging due to the need for medical expertise and comprehensive reasoning chains for diagnosis. This paper introduces a novel cascade retrieval-while-reasoning framework for clinical \u2026"}, {"title": "PreAdapter: Pre-training Language Models on Knowledge Graphs", "link": "https://link.springer.com/chapter/10.1007/978-3-031-77850-6_12", "details": "J Omeliyanenko, A Hotho, D Schl\u00f6r - International Semantic Web Conference, 2024", "abstract": "Pre-trained language models have demonstrated state-of-the-art performance in various downstream tasks such as summarization, sentiment classification, and question answering. Leveraging vast amounts of textual data during training, these \u2026"}, {"title": "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.884.pdf", "details": "P Gao, T Yamasaki, K Imoto - Findings of the Association for Computational \u2026, 2024", "abstract": "We propose VE-KD, a novel method that balances knowledge distillation and vocabulary expansion with the aim of training efficient domain-specific language models. Compared with traditional pre-training approaches, VE-KD exhibits \u2026"}]
