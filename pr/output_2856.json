[{"title": "An exploratory study of self-supervised pre-training on partially supervised multi-label classification on chest X-ray images", "link": "https://www.sciencedirect.com/science/article/pii/S156849462400629X", "details": "N Dong, M Kampffmeyer, H Su, E Xing - Applied Soft Computing, 2024", "abstract": "This paper serves as the first empirical study on self-supervised pre-training on partially supervised learning, an emerging yet unexplored learning paradigm with missing annotations. This is particularly important in the medical imaging domain \u2026"}, {"title": "MedExQA: Medical Question Answering Benchmark with Multiple Explanations", "link": "https://arxiv.org/pdf/2406.06331", "details": "Y Kim, J Wu, Y Abdulle, H Wu - arXiv e-prints, 2024", "abstract": "This paper introduces MedExQA, a novel benchmark in medical question-answering, to evaluate large language models'(LLMs) understanding of medical knowledge through explanations. By constructing datasets across five distinct medical \u2026"}, {"title": "White-box Multimodal Jailbreaks Against Large Vision-Language Models", "link": "https://arxiv.org/pdf/2405.17894", "details": "R Wang, X Ma, H Zhou, C Ji, G Ye, YG Jiang - arXiv preprint arXiv:2405.17894, 2024", "abstract": "Recent advancements in Large Vision-Language Models (VLMs) have underscored their superiority in various multimodal tasks. However, the adversarial robustness of VLMs has not been fully explored. Existing methods mainly assess robustness \u2026"}, {"title": "Super Tiny Language Models", "link": "https://arxiv.org/pdf/2405.14159", "details": "D Hillier, L Guertler, C Tan, P Agrawal, C Ruirui\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancement of large language models (LLMs) has led to significant improvements in natural language processing but also poses challenges due to their high computational and energy demands. This paper introduces a series of research \u2026"}, {"title": "VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers", "link": "https://arxiv.org/pdf/2406.05370", "details": "S Chen, S Liu, L Zhou, Y Liu, X Tan, J Li, S Zhao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration \u2026"}, {"title": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records", "link": "https://aclanthology.org/2024.cl4health-1.23.pdf", "details": "J Lov\u00f3n-Melgarejo, T Ben-Haddi, J Di Scala\u2026 - Proceedings of the First \u2026, 2024", "abstract": "The lack of standardized evaluation benchmarks in the medical domain for text inputs can be a barrier to widely adopting and leveraging the potential of natural language models for health-related downstream tasks. This paper revisited an \u2026"}, {"title": "Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding", "link": "https://arxiv.org/pdf/2405.19763", "details": "K Liao, S Li, M Zhao, L Liu, M Xue, Z Hu, H Han, C Yin - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent strides in large language models (LLMs) have yielded remarkable performance, leveraging reinforcement learning from human feedback (RLHF) to significantly enhance generation and alignment capabilities. However, RLHF \u2026"}, {"title": "Unified Editing of Panorama, 3D Scenes, and Videos Through Disentangled Self-Attention Injection", "link": "https://arxiv.org/pdf/2405.16823", "details": "G Kwon, J Park, JC Ye - arXiv preprint arXiv:2405.16823, 2024", "abstract": "While text-to-image models have achieved impressive capabilities in image generation and editing, their application across various modalities often necessitates training separate models. Inspired by existing method of single image editing with \u2026"}, {"title": "Multi-Prompting Decoder Helps Better Language Understanding", "link": "https://arxiv.org/pdf/2406.06279", "details": "Z Cheng, Z Chen, Z Jiang, Y Yin, S Ge, Y Liu, Q Gu - arXiv preprint arXiv:2406.06279, 2024", "abstract": "Recent Pre-trained Language Models (PLMs) usually only provide users with the inference APIs, namely the emerging Model-as-a-Service (MaaS) setting. To adapt MaaS PLMs to downstream tasks without accessing their parameters and gradients \u2026"}]
