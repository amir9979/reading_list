[{"title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation", "link": "https://arxiv.org/pdf/2505.00831", "details": "QPM Pham, KTN Nguyen, NH Doan, CA Pham, K Inui\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Efficient path planning in robotics, particularly within large-scale, dynamic environments, remains a significant hurdle. While Large Language Models (LLMs) offer strong reasoning capabilities, their high computational cost and limited \u2026"}, {"title": "Transferable Adversarial Attacks on Black-Box Vision-Language Models", "link": "https://arxiv.org/pdf/2505.01050", "details": "K Hu, W Yu, L Zhang, A Robey, A Zou, C Xu, H Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision Large Language Models (VLLMs) are increasingly deployed to offer advanced capabilities on inputs comprising both text and images. While prior research has shown that adversarial attacks can transfer from open-source to \u2026"}, {"title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models", "link": "https://arxiv.org/pdf/2504.12526%3F", "details": "J Zhang, T Zhu, C Luo, A Anandkumar - arXiv preprint arXiv:2504.12526, 2025", "abstract": "Long-context language models exhibit impressive performance but remain challenging to deploy due to high GPU memory demands during inference. We propose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that \u2026"}, {"title": "GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models", "link": "https://aclanthology.org/2025.naacl-long.420.pdf", "details": "H Kohli, S Kumar, H Sun - Proceedings of the 2025 Conference of the Nations of \u2026, 2025", "abstract": "The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their reasoning to \u2026"}, {"title": "Exploring Multimodal Language Models for Sustainability Disclosure Extraction: A Comparative Study", "link": "https://aclanthology.org/2025.insights-1.13.pdf", "details": "T Gupta, T Goel, I Verma - The Sixth Workshop on Insights from Negative Results \u2026, 2025", "abstract": "Sustainability metrics have increasingly become a crucial non-financial criterion in investment decision-making. Organizations worldwide are recognizing the importance of sustainability and are proactively highlighting their efforts through \u2026"}, {"title": "Defending Code Language Models against Backdoor Attacks with Deceptive Cross-Entropy Loss", "link": "https://dl.acm.org/doi/abs/10.1145/3728639", "details": "G Yang, Y Zhou, X Zhang, X Chen, T Zhuo, D Lo\u2026 - ACM Transactions on Software \u2026", "abstract": "Code Language Models (CLMs), particularly those leveraging deep learning, have achieved significant success in code intelligence domain. However, the issue of security, particularly backdoor attacks, is often overlooked in this process. The \u2026"}, {"title": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "link": "https://www.biorxiv.org/content/10.1101/2025.04.14.648850.full.pdf", "details": "SA Rizvi, D Levine, A Patel, S Zhang, E Wang, S He\u2026 - bioRxiv, 2025", "abstract": "Single-cell RNA sequencing has transformed our understanding of cellular diversity, yet current single-cell foundation models (scFMs) remain limited in their scalability, flexibility across diverse tasks, and ability to natively integrate textual information. In \u2026"}, {"title": "Developing safe and responsible large language model: can we balance bias reduction and language understanding?", "link": "https://link.springer.com/article/10.1007/s10994-025-06767-4", "details": "S Raza, O Bamgbose, S Ghuge, F Tavakoli, DJ Reji\u2026 - Machine Learning, 2025", "abstract": "Abstract Large Language Models (LLMs) have advanced various Natural Language Processing (NLP) tasks, such as text generation and translation, among others. However, these models often generate texts that can perpetuate biases. Existing \u2026"}, {"title": "Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society", "link": "https://arxiv.org/pdf/2504.17404%3F", "details": "F Zhao, Y Wang, E Lu, D Zhao, B Han, H Tong, Y Liang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Artificial Intelligence (AI) systems are becoming increasingly powerful and autonomous, and may progress to surpass human intelligence levels, namely Artificial Superintelligence (ASI). During the progression from AI to ASI, it may exceed \u2026"}]
