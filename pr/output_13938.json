[{"title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs", "link": "https://arxiv.org/pdf/2503.01743%3F", "details": "A Abouelenin, A Ashfaq, A Atkinson, H Awadalla\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent \u2026"}, {"title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing", "link": "https://arxiv.org/pdf/2503.08588", "details": "X Xu, W Xu, N Zhang, J McAuley - arXiv preprint arXiv:2503.08588, 2025", "abstract": "Previous studies have established that language models manifest stereotyped biases. Existing debiasing strategies, such as retraining a model with counterfactual data, representation projection, and prompting often fail to efficiently eliminate bias or \u2026"}, {"title": "Rethinking Data: Towards Better Performing Domain-Specific Small Language Models", "link": "https://arxiv.org/pdf/2503.01464", "details": "B Nazarov, D Frolova, Y Lubarsky, A Gaissinski\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Fine-tuning of Large Language Models (LLMs) for downstream tasks, performed on domain-specific data has shown significant promise. However, commercial use of such LLMs is limited by the high computational cost required for their deployment at \u2026"}, {"title": "Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?", "link": "https://arxiv.org/pdf/2502.11895", "details": "J Nielsen, P Schneider-Kamp, L Galke - arXiv preprint arXiv:2502.11895, 2025", "abstract": "Large language models (LLMs) require immense resources for training and inference. Quantization, a technique that reduces the precision of model parameters, offers a promising solution for improving LLM efficiency and sustainability. While post \u2026"}, {"title": "TINY LongProLIP: A Probabilistic Vision-Language Model with Long Context Text", "link": "https://openreview.net/pdf%3Fid%3DjfwxBVBCiD", "details": "S Chun, S Yun - \u2026 Workshop: Quantify Uncertainty and Hallucination in \u2026", "abstract": "Recently, Probabilistic Language-Image Pre-Training (ProLIP) has been proposed to tackle the multiplicity issue of vision-language (VL) tasks. Despite their success in probabilistic representation learning at a scale, the ProLIP models cannot handle \u2026"}, {"title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning", "link": "https://arxiv.org/pdf/2502.18080", "details": "W Yang, S Ma, Y Lin, F Wei - arXiv preprint arXiv:2502.18080, 2025", "abstract": "Recent studies have shown that making a model spend more time thinking through longer Chain of Thoughts (CoTs) enables it to gain significant improvements in complex reasoning tasks. While current researches continue to explore the benefits \u2026"}, {"title": "PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection", "link": "https://arxiv.org/pdf/2502.12119", "details": "J Bi, Y Wang, D Yan, X Xiao, A Hecker, V Tresp, Y Ma - arXiv preprint arXiv \u2026, 2025", "abstract": "Visual instruction tuning refines pre-trained Multimodal Large Language Models (MLLMs) to enhance their real-world task performance. However, the rapid expansion of visual instruction datasets introduces significant data redundancy \u2026"}, {"title": "Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation", "link": "https://arxiv.org/pdf/2502.15040", "details": "YW Chu, K Zhang, C Malon, MR Min - arXiv preprint arXiv:2502.15040, 2025", "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive performance in vision and text tasks. However, hallucination remains a major challenge, especially in fields like healthcare where details are critical. In this work, we show \u2026"}, {"title": "EAGER-LLM: Enhancing Large Language Models as Recommenders through Exogenous Behavior-Semantic Integration", "link": "https://arxiv.org/pdf/2502.14735", "details": "M Hong, Y Xia, Z Wang, J Zhu, Y Wang, S Cai, X Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) are increasingly leveraged as foundational backbones in the development of advanced recommender systems, offering enhanced capabilities through their extensive knowledge and reasoning. Existing llm \u2026"}]
