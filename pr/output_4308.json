[{"title": "Fuzzy Multi-view Graph Learning on Sparse Electronic Health Records", "link": "https://ieeexplore.ieee.org/abstract/document/10572354/", "details": "T Tang, Z Han, S Yu, A Bagirov, Q Zhang - IEEE Transactions on Fuzzy Systems, 2024", "abstract": "Extracting latent disease patterns from electronic health records (EHRs) is a crucial solution for disease analysis, significantly facilitating healthcare decision-making. Multiview learning presents itself as a promising approach that offers a \u2026"}, {"title": "Factors Influencing Data Quality in Electronic Health Record Systems in 50 Health Facilities in Rwanda and the Role of Clinical Alerts: Cross-Sectional Observational \u2026", "link": "https://publichealth.jmir.org/2024/1/e49127/", "details": "HSF Fraser, M Mugisha, I Bacher, JL Ngenzi\u2026 - JMIR Public Health and \u2026, 2024", "abstract": "Background: Electronic health records (EHRs) play an increasingly important role in delivering HIV care in low-and middle-income countries. The data collected are used for direct clinical care, quality improvement, program monitoring, public health \u2026"}, {"title": "Mental Modeling of Reinforcement Learning Agents by Language Models", "link": "https://arxiv.org/pdf/2406.18505", "details": "W Lu, X Zhao, J Spisak, JH Lee, S Wermter - arXiv preprint arXiv:2406.18505, 2024", "abstract": "Can emergent language models faithfully model the intelligence of decision-making agents? Though modern language models exhibit already some reasoning ability, and theoretically can potentially express any probable distribution over tokens, it \u2026"}, {"title": "Language models, like humans, show content effects on reasoning tasks", "link": "https://academic.oup.com/pnasnexus/article/3/7/pgae233/7712372", "details": "AK Lampinen, I Dasgupta, SCY Chan, HR Sheahan\u2026 - PNAS nexus, 2024", "abstract": "Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks but exhibit many imperfections. However, human abstract reasoning is also imperfect. Human \u2026"}, {"title": "Suri: Multi-constraint Instruction Following for Long-form Text Generation", "link": "https://arxiv.org/pdf/2406.19371", "details": "CM Pham, S Sun, M Iyyer - arXiv preprint arXiv:2406.19371, 2024", "abstract": "Existing research on instruction following largely focuses on tasks with simple instructions and short responses. In this work, we explore multi-constraint instruction following for generating long-form text. We create Suri, a dataset with 20K human \u2026"}, {"title": "Aligning Language Models with the Human World", "link": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi%3Farticle%3D1241%26context%3Ddissertations", "details": "R LIU - 2024", "abstract": "Abstract The field of Natural Language Processing (NLP) has undergone a significant transformation with the emergence of large language models (LMs). These models have enabled the development of human-like conversational \u2026"}, {"title": "Two Stacks Are Better Than One: A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "link": "https://arxiv.org/pdf/2407.15489", "details": "Z Li, S Ji, T Mickus, V Segonne, J Tiedemann - arXiv preprint arXiv:2407.15489, 2024", "abstract": "Pretrained language models (PLMs) display impressive performances and have captured the attention of the NLP community. Establishing the best practices in pretraining has therefore become a major point of focus for much of NLP research \u2026"}, {"title": "Rgat at semeval-2024 task 2: Biomedical natural language inference using graph attention network", "link": "https://aclanthology.org/2024.semeval-1.19.pdf", "details": "A Chakraborty - Proceedings of the 18th International Workshop on \u2026, 2024", "abstract": "In this work, we (team RGAT) describe our approaches for the SemEval 2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials (NLI4CT). The objective of this task is multi-evidence natural language inference based on different \u2026"}, {"title": "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "link": "https://arxiv.org/pdf/2406.17681", "details": "K Qian, S Wan, C Tang, Y Wang, X Zhang, M Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As large language models achieve impressive scores on traditional benchmarks, an increasing number of researchers are becoming concerned about benchmark data leakage during pre-training, commonly known as the data contamination problem. To \u2026"}]
