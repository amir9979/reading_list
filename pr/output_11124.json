[{"title": "Comparison of the inverted internal limiting membrane flap technique without versus with an autologous blood clot for treating macular hole-associated retinal \u2026", "link": "https://link.springer.com/article/10.1186/s40662-024-00417-x", "details": "K Zhu, Y Wang, B Lei, L Chen, Y Zhang, Q Chang\u2026 - Eye and Vision, 2025", "abstract": "Background To investigate the anatomical and functional outcomes of macular hole- associated retinal detachment (MHRD) after vitrectomy using the inverted internal limiting membrane (ILM) flap technique with autologous blood clot (ABC). Methods \u2026"}, {"title": "PLZero: placeholder based approach to generalized zero-shot learning for multi-label recognition in chest radiographs", "link": "https://link.springer.com/article/10.1007/s40747-024-01717-4", "details": "C Yang, Q Jin, F Du, J Guo, Y Zhou - Complex & Intelligent Systems, 2025", "abstract": "By leveraging large-scale image-text paired data for pre-training, the model can efficiently learn the alignment between images and text, significantly advancing the development of zero-shot learning (ZSL) in the field of intelligent medical image \u2026"}, {"title": "Impact of Mydriasis on Image Gradability and Automated Diabetic Retinopathy Screening with a Handheld Camera in Real-World Settings", "link": "https://www.medrxiv.org/content/10.1101/2025.01.02.25319898", "details": "IDA Costa, D Restrepo, LZ Ribeiro, AK Aragaki\u2026 - medRxiv, 2025", "abstract": "Purpose: Diabetic retinopathy (DR) screening in low-and middle-income countries (LMICs) faces challenges due to limited access to specialized care. Portable retinal cameras provide a practical alternative, but image quality, influenced by mydriasis \u2026"}, {"title": "Real-world Deployment and Evaluation of PErioperative AI CHatbot (PEACH)--a Large Language Model Chatbot for Perioperative Medicine", "link": "https://arxiv.org/pdf/2412.18096", "details": "YH Ke, L Jin, K Elangovan, BWX Ong, CY Oh, J Sim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) are emerging as powerful tools in healthcare, particularly for complex, domain-specific tasks. This study describes the development and evaluation of the PErioperative AI CHatbot (PEACH), a secure LLM-based \u2026"}, {"title": "Low-Rank Adaptation with Task-Relevant Feature Enhancement for Fine-tuning Language Models", "link": "https://arxiv.org/pdf/2412.09827", "details": "C Li, C Ding, K Luan, X Di - arXiv preprint arXiv:2412.09827, 2024", "abstract": "Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. LoRA is one of the most widely used methods, which assumes that the optimization process is essentially low \u2026"}, {"title": "Enhancing the Reasoning Capabilities of Small Language Models via Solution Guidance Fine-Tuning", "link": "https://arxiv.org/pdf/2412.09906", "details": "J Bi, Y Wu, W Xing, Z Wei - arXiv preprint arXiv:2412.09906, 2024", "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks. Advances in prompt engineering and fine-tuning techniques have further enhanced their ability to address complex reasoning challenges \u2026"}, {"title": "Can Language Models Rival Mathematics Students? Evaluating Mathematical Reasoning through Textual Manipulation and Human Experiments", "link": "https://arxiv.org/pdf/2412.11908", "details": "A Nikolaiev, Y Stathopoulos, S Teufel - arXiv preprint arXiv:2412.11908, 2024", "abstract": "In this paper we look at the ability of recent large language models (LLMs) at solving mathematical problems in combinatorics. We compare models LLaMA-2, LLaMA-3.1, GPT-4, and Mixtral against each other and against human pupils and \u2026"}, {"title": "Improving intermediate reasoning in zero-shot chain-of-thought for large language models with filter supervisor-self correction", "link": "https://www.sciencedirect.com/science/article/pii/S0925231224019908", "details": "J Sun, Y Pan, X Yan - Neurocomputing, 2024", "abstract": "Abstract Chain of Thought (CoT) prompting enables Large Language Models (LLMs) to generate detailed intermediate reasoning steps to solve problems, demonstrating excellent performance across various fields. However, when LLMs encounter \u2026"}]
