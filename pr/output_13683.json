[{"title": "EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models", "link": "https://arxiv.org/pdf/2502.06663", "details": "X Xing, Z Liu, S Xiao, B Gao, Y Liang, W Zhang, H Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Modern large language models (LLMs) driven by scaling laws, achieve intelligence emergency in large model sizes. Recently, the increasing concerns about cloud costs, latency, and privacy make it an urgent requirement to develop compact edge \u2026"}, {"title": "MMSciBench: Benchmarking Language Models on Multimodal Scientific Problems", "link": "https://arxiv.org/pdf/2503.01891", "details": "X Ye, C Li, S Chen, X Tang, W Wei - arXiv preprint arXiv:2503.01891, 2025", "abstract": "Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present \u2026"}, {"title": "Compositional Causal Reasoning Evaluation in Language Models", "link": "https://arxiv.org/abs/2503.04556", "details": "JRMA Maasch, A H\u00fcy\u00fck, X Xu, AV Nori, J Gonzalez - arXiv preprint arXiv:2503.04556, 2025", "abstract": "Causal reasoning and compositional reasoning are two core aspirations in generative AI. Measuring the extent of these behaviors requires principled evaluation methods. We explore a unified perspective that considers both behaviors \u2026"}, {"title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models", "link": "https://arxiv.org/pdf/2502.09604", "details": "YS Chuang, B Cohen-Wang, SZ Shen, Z Wu, H Xu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive \u2026"}, {"title": "Shaping Shared Languages: Human and Large Language Models' Inductive Biases in Emergent Communication", "link": "https://arxiv.org/pdf/2503.04395", "details": "T Kouwenhoven, M Peeperkorn, R de Kleijn, T Verhoef - arXiv preprint arXiv \u2026, 2025", "abstract": "Languages are shaped by the inductive biases of their users. Using a classical referential game, we investigate how artificial languages evolve when optimised for inductive biases in humans and large language models (LLMs) via Human-Human \u2026"}, {"title": "Large Language Models as Attribution Regularizers for Efficient Model Training", "link": "https://arxiv.org/pdf/2502.20268", "details": "D Vukadin, M \u0160ili\u0107, G Dela\u010d - arXiv preprint arXiv:2502.20268, 2025", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains. However, effectively leveraging their vast knowledge for training smaller downstream models remains an open challenge, especially in domains like \u2026"}, {"title": "Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models via Watermarking", "link": "https://arxiv.org/pdf/2503.04636", "details": "Y Xu, A Liu, X Hu, L Wen, H Xiong - arXiv preprint arXiv:2503.04636, 2025", "abstract": "As open-source large language models (LLMs) like Llama3 become more capable, it is crucial to develop watermarking techniques to detect their potential misuse. Existing watermarking methods either add watermarks during LLM inference, which \u2026"}, {"title": "From Text to Trust: Empowering AI-assisted Decision Making with Adaptive LLM-powered Analysis", "link": "https://arxiv.org/pdf/2502.11919", "details": "Z Li, H Zhu, Z Lu, Z Xiao, M Yin - arXiv preprint arXiv:2502.11919, 2025", "abstract": "AI-assisted decision making becomes increasingly prevalent, yet individuals often fail to utilize AI-based decision aids appropriately especially when the AI explanations are absent, potentially as they do not% understand reflect on AI's \u2026"}, {"title": "Detecting LLM Fact-conflicting Hallucinations Enhanced by Temporal-logic-based Reasoning", "link": "https://arxiv.org/pdf/2502.13416", "details": "N Li, Y Song, K Wang, Y Li, L Shi, Y Liu, H Wang - arXiv preprint arXiv:2502.13416, 2025", "abstract": "Large language models (LLMs) face the challenge of hallucinations--outputs that seem coherent but are actually incorrect. A particularly damaging type is fact- conflicting hallucination (FCH), where generated content contradicts established \u2026"}]
