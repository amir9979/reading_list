[{"title": "Learning with Enriched Inductive Biases for Vision-Language Models", "link": "https://ruyuanzhang.github.io/files/2501_indctbiasVisLangModel_IJCV.pdf", "details": "L Yang, RY Zhang, Q Chen, X Xie - International Journal of Computer Vision, 2025", "abstract": "Abstract Vision-Language Models, pre-trained on large-scale image-text pairs, serve as strong foundation models for transfer learning across a variety of downstream tasks. For few-shot generalization tasks, ie., when the model is trained on few-shot \u2026"}, {"title": "MDFCL: Multimodal data fusion-based graph contrastive learning framework for molecular property prediction", "link": "https://www.sciencedirect.com/science/article/pii/S0031320325001232", "details": "X Gong, M Liu, Q Liu, Y Guo, G Wang - Pattern Recognition, 2025", "abstract": "Molecular property prediction is a critical task with substantial applications for drug design and repositioning. The multiplicity of molecular data modalities and paucity of labeled data present significant challenges that affect algorithmic performance in this \u2026"}, {"title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06788", "details": "H Diao, X Li, Y Cui, Y Wang, H Deng, T Pan, W Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient \u2026"}, {"title": "Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences", "link": "https://arxiv.org/pdf/2502.01126", "details": "V Shrivastava, A Kumar, P Liang - arXiv preprint arXiv:2502.01126, 2025", "abstract": "Language models (LMs) should provide reliable confidence estimates to help users detect mistakes in their outputs and defer to human experts when necessary. Asking a language model to assess its confidence (\" Score your confidence from 0-1.\") is a \u2026"}, {"title": "Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding", "link": "https://arxiv.org/pdf/2502.11492", "details": "KH Huang, C Qin, H Qiu, P Laban, S Joty, C Xiong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision Language Models (VLMs) have achieved remarkable progress in multimodal tasks, yet they often struggle with visual arithmetic, seemingly simple capabilities like object counting or length comparison, which are essential for relevant complex tasks \u2026"}, {"title": "Refine Knowledge of Large Language Models via Adaptive Contrastive Learning", "link": "https://arxiv.org/pdf/2502.07184", "details": "Y Li, H Huang, J Kuang, Y Li, SY Guo, C Qu, X Tan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "How to alleviate the hallucinations of Large Language Models (LLMs) has always been the fundamental goal pursued by the LLMs research community. Looking through numerous hallucination-related studies, a mainstream category of methods \u2026"}, {"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}, {"title": "Is Self-Supervised Pre-training on Satellite Imagery Better than ImageNet? A Systematic Study with Sentinel-2", "link": "https://arxiv.org/pdf/2502.10669", "details": "S Lahrichi, Z Sheng, S Xia, K Bradbury, J Malof - arXiv preprint arXiv:2502.10669, 2025", "abstract": "Self-supervised learning (SSL) has demonstrated significant potential in pre-training robust models with limited labeled data, making it particularly valuable for remote sensing (RS) tasks. A common assumption is that pre-training on domain-aligned \u2026"}, {"title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training", "link": "https://arxiv.org/pdf/2502.06589", "details": "Y Zhuang, J Yang, H Jiang, X Liu, K Cheng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce \u2026"}]
