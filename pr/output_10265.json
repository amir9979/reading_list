[{"title": "FlashSloth: Lightning Multimodal Large Language Models via Embedded Visual Compression", "link": "https://arxiv.org/pdf/2412.04317", "details": "B Tong, B Lai, Y Zhou, G Luo, Y Shen, K Li, X Sun, R Ji - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite a big leap forward in capability, multimodal large language models (MLLMs) tend to behave like a sloth in practical use, ie, slow response and large latency. Recent efforts are devoted to building tiny MLLMs for better efficiency, but the \u2026"}, {"title": "CNNSum: Exploring Long-Conext Summarization with Large Language Models in Chinese Novels", "link": "https://arxiv.org/pdf/2412.02819", "details": "L Wei, H Yan, X Lu, J Zhu, J Wang, W Zhang - arXiv preprint arXiv:2412.02819, 2024", "abstract": "Large Language Models (LLMs) have been well-researched in many long-context tasks. However, due to high annotation costs, high-quality long-context summary datasets for training or evaluation are scarce, limiting further research. In this work \u2026"}, {"title": "GeoTool-GPT: a trainable method for facilitating Large Language Models to master GIS tools", "link": "https://www.tandfonline.com/doi/abs/10.1080/13658816.2024.2438937", "details": "C Wei, Y Zhang, X Zhao, Z Zeng, Z Wang, J Lin\u2026 - International Journal of \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) excel in natural language-relevant tasks like text generation and question answering Q&A. To further expand their application, efforts focus on enabling LLMs to utilize real-world tools. However, their tool-use \u2026"}, {"title": "Training Large Language Models to Reason in a Continuous Latent Space", "link": "https://arxiv.org/pdf/2412.06769%3F", "details": "S Hao, S Sukhbaatar, DJ Su, X Li, Z Hu, J Weston\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) are restricted to reason in the\" language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may \u2026"}, {"title": "Enhancing Trust in Large Language Models with Uncertainty-Aware Fine-Tuning", "link": "https://arxiv.org/pdf/2412.02904", "details": "R Krishnan, P Khanna, O Tickoo - arXiv preprint arXiv:2412.02904, 2024", "abstract": "Large language models (LLMs) have revolutionized the field of natural language processing with their impressive reasoning and question-answering capabilities. However, these models are sometimes prone to generating credible-sounding but \u2026"}, {"title": "Can Large Language Models Serve as Evaluators for Code Summarization?", "link": "https://arxiv.org/pdf/2412.01333", "details": "Y Wu, Y Wan, Z Chu, W Zhao, Y Liu, H Zhang, X Shi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Code summarization facilitates program comprehension and software maintenance by converting code snippets into natural-language descriptions. Over the years, numerous methods have been developed for this task, but a key challenge remains \u2026"}, {"title": "Pre-train, Align, and Disentangle: Empowering Sequential Recommendation with Large Language Models", "link": "https://arxiv.org/pdf/2412.04107", "details": "Y Wang, J Pan, X Zhao, P Jia, W Wang, Y Wang, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Sequential recommendation (SR) aims to model the sequential dependencies in users' historical interactions to better capture their evolving interests. However, existing SR approaches primarily rely on collaborative data, which leads to \u2026"}, {"title": "Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models", "link": "https://arxiv.org/pdf/2412.08615", "details": "J Li, Y Hao, H Xu, X Wang, Y Hong - arXiv preprint arXiv:2412.08615, 2024", "abstract": "Despite the advancements in training Large Language Models (LLMs) with alignment techniques to enhance the safety of generated content, these models remain susceptible to jailbreak, an adversarial attack method that exposes security \u2026"}, {"title": "Multi-Objective Alignment of Large Language Models Through Hypervolume Maximization", "link": "https://arxiv.org/pdf/2412.05469", "details": "S Mukherjee, A Lalitha, S Sengupta, A Deshmukh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multi-objective alignment from human feedback (MOAHF) in large language models (LLMs) is a challenging problem as human preferences are complex, multifaceted, and often conflicting. Recent works on MOAHF considered a-priori multi-objective \u2026"}]
