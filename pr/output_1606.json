'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Correcting Language Model Bias for Text Classification'
[{"title": "Language Models for Hierarchical Classification of Radiology Reports with Attention Mechanisms, BERT and GPT-4", "link": "https://ieeexplore.ieee.org/iel7/6287639/6514899/10531266.pdf", "details": "M Olivato, L Putelli, N Arici, AE Gerevini, A Lavelli\u2026 - IEEE Access, 2024", "abstract": "Radiology reports are a valuable source of textual information used to improve clinical care and support research. In recent years, deep learning techniques have been shown to be effective in classifying radiology reports. This article investigates \u2026"}, {"title": "Prompt Tuning for Few-shot Relation Extraction via Modeling Global and Local Graphs", "link": "https://aclanthology.org/2024.lrec-main.1158.pdf", "details": "Z Zhang, Y Yang, B Chen - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Recently, prompt-tuning has achieved very significant results for few-shot tasks. The core idea of prompt-tuning is to insert prompt templates into the input, thus converting the classification task into a masked language modeling problem. However, for few \u2026"}, {"title": "Leveraging Information Redundancy of Real-World Data through Distant Supervision", "link": "https://aclanthology.org/2024.lrec-main.905.pdf", "details": "A Cohen, A Lanson, E Kempf, X Tannier - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "We explore the task of event extraction and classification by harnessing the power of distant supervision. We present a novel text labeling method that leverages the redundancy of temporal information in a data lake. This method enables the creation \u2026"}, {"title": "Dynamic Prompt-Driven Zero-Shot Relation Extraction", "link": "https://ieeexplore.ieee.org/abstract/document/10531644/", "details": "L Xu, X Bu, X Tian - IEEE/ACM Transactions on Audio, Speech, and \u2026, 2024", "abstract": "The task of zero-shot relation extraction is a very important research topic in the field of information extraction, which can effectively alleviate the issue of no training samples for some relations. Existing zero-shot relation extraction methods based on \u2026"}, {"title": "Probe Then Retrieve and Reason: Distilling Probing and Reasoning Capabilities into Smaller Language Models", "link": "https://aclanthology.org/2024.lrec-main.1140.pdf", "details": "Y Zhao, S Zhou, H Zhu - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Step-by-step reasoning methods, such as the Chain-of-Thought (CoT), have been demonstrated to be highly effective in harnessing the reasoning capabilities of Large Language Models (LLMs). Recent research efforts have sought to distill LLMs into \u2026"}, {"title": "Multi-hop Question Answering over Knowledge Graphs using Large Language Models", "link": "https://arxiv.org/pdf/2404.19234", "details": "A Chakraborty - arXiv preprint arXiv:2404.19234, 2024", "abstract": "Knowledge graphs (KGs) are large datasets with specific structures representing large knowledge bases (KB) where each node represents a key entity and relations amongst them are typed edges. Natural language queries formed to extract \u2026"}, {"title": "Redefining Information Retrieval of Structured Database via Large Language Models", "link": "https://arxiv.org/pdf/2405.05508", "details": "M Wang, Y Zhang, Q Zhao, J Yang, H Zhang - arXiv preprint arXiv:2405.05508, 2024", "abstract": "Retrieval augmentation is critical when Language Models (LMs) exploit non- parametric knowledge related to the query through external knowledge bases before reasoning. The retrieved information is incorporated into LMs as context alongside \u2026"}, {"title": "IT5: Text-to-text Pretraining for Italian Language Understanding and Generation", "link": "https://aclanthology.org/2024.lrec-main.823.pdf", "details": "G Sarti, M Nissim - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "We introduce IT5, the first family of encoder-decoder transformer models pretrained specifically on Italian. We document and perform a thorough cleaning procedure for a large Italian corpus and use it to pretrain four IT5 model sizes. We then introduce \u2026"}, {"title": "Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.708.pdf", "details": "O Ignat, Z Jin, A Abzaliev, L Biester, S Castro, N Deng\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Recent progress in large language models (LLMs) has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that \u201cit's all been solved.\u201d Not surprisingly, this has, in turn, made \u2026"}]
