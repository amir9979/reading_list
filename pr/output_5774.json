[{"title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios", "link": "https://aclanthology.org/2024.findings-acl.230.pdf", "details": "L Lin, J Fu, P Liu, Q Li, Y Gong, J Wan, F Zhang\u2026 - Findings of the Association \u2026, 2024", "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local \u2026"}, {"title": "Teaching Small Language Models to Reason for Knowledge-Intensive Multi-Hop Question Answering", "link": "https://aclanthology.org/2024.findings-acl.464.pdf", "details": "X Li, S He, F Lei, JY JunYang, T Su, K Liu, J Zhao - Findings of the Association for \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) can teach small language models (SLMs) to solve complex reasoning tasks (eg, mathematical question answering) by Chain-of- thought Distillation (CoTD). Specifically, CoTD fine-tunes SLMs by utilizing rationales \u2026"}, {"title": "Fast Randomized Low-Rank Adaptation of Pre-trained Language Models with PAC Regularization", "link": "https://aclanthology.org/2024.findings-acl.310.pdf", "details": "Z Lei, D Qian, W Cheung - Findings of the Association for Computational \u2026, 2024", "abstract": "Low-rank adaptation (LoRA) achieves parameter efficient fine-tuning for large language models (LLMs) by decomposing the model weight update into a pair of low- rank projection matrices. Yet, the memory overhead restricts it to scale up when the \u2026"}, {"title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.06663", "details": "K Sun, M Dredze - arXiv preprint arXiv:2408.06663, 2024", "abstract": "The development of large language models leads to the formation of a pre-train-then- align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream \u2026"}, {"title": "InstructCoder: Instruction Tuning Large Language Models for Code Editing", "link": "https://aclanthology.org/2024.acl-srw.6.pdf", "details": "K Li, Q Hu, J Zhao, H Chen, Y Xie, T Liu, M Shieh, J He - Proceedings of the 62nd \u2026, 2024", "abstract": "Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data \u2026"}, {"title": "T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step", "link": "https://aclanthology.org/2024.acl-long.515.pdf", "details": "Z Chen, W Du, W Zhang, K Liu, J Liu, M Zheng, J Zhuo\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Large language models (LLMs) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool utilization capability of LLMs is still under-explored. In contrast \u2026"}, {"title": "Rescue: Ranking llm responses with partial ordering to improve response generation", "link": "https://aclanthology.org/2024.acl-srw.32.pdf", "details": "Y Wang, R Zheng, H Li, Q Zhang, T Gui, F Liu - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Customizing LLMs for a specific task involves separating high-quality responses from lower-quality ones. This skill can be developed using supervised fine-tuning with extensive human preference data. However, obtaining a large volume of expert \u2026"}, {"title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Trustworthy Response Generation in Chinese", "link": "https://dl.acm.org/doi/pdf/10.1145/3686807", "details": "H Wang, S Zhao, Z Qiang, Z Li, C Liu, N Xi, Y Du, B Qin\u2026 - ACM Transactions on \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to \u2026"}, {"title": "Adversarial preference optimization: Enhancing your alignment via rm-llm game", "link": "https://aclanthology.org/2024.findings-acl.221.pdf", "details": "P Cheng, Y Yang, J Li, Y Dai, T Hu, P Cao, N Du, X Li - Findings of the Association for \u2026, 2024", "abstract": "Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing alignment methods depend on manually annotated preference data to guide the LLM optimization directions. However \u2026"}]
