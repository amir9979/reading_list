'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Handling Class Imbalance and Overlap with a Hesitation-based'
[{"title": "Scaling Up Video Summarization Pretraining with Large Language Models", "link": "https://arxiv.org/pdf/2404.03398", "details": "DM Argaw, S Yoon, FC Heilbron, H Deilamsalehy\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Long-form video content constitutes a significant portion of internet traffic, making automated video summarization an essential research problem. However, existing video summarization datasets are notably limited in their size, constraining the \u2026"}, {"title": "Data Augmentation and Large Language Model for Legal Case Retrieval and Entailment", "link": "https://link.springer.com/article/10.1007/s12626-024-00158-2", "details": "MQ Bui, DT Do, NK Le, DH Nguyen, KVH Nguyen\u2026 - The Review of Socionetwork \u2026, 2024", "abstract": "Abstract The Competition on Legal Information Extraction and Entailment (COLIEE) is a well-known international competition organized each year with the goal of applying machine learning algorithms and techniques in the analysis and \u2026"}, {"title": "Best Practices and Lessons Learned on Synthetic Data for Language Models", "link": "https://arxiv.org/pdf/2404.07503", "details": "R Liu, J Wei, F Liu, C Si, Y Zhang, J Rao, S Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as a promising solution by generating \u2026"}, {"title": "Role-Guided Contrastive Learning for Event Argument Extraction", "link": "https://link.springer.com/chapter/10.1007/978-3-031-56027-9_21", "details": "C Yao, Y Guo, X Chen, Z Duan, J Fu - European Conference on Information Retrieval, 2024", "abstract": "Event argument extraction is a subtask of information extraction. Recent efforts have predominantly focused on mitigating the issue of error propagation associated with pipeline methods for extracting event arguments, such as machine reading \u2026"}, {"title": "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model", "link": "https://arxiv.org/pdf/2404.04167", "details": "X Du, Z Yu, S Gao, D Pan, Y Cheng, Z Ma, R Yuan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this study, we introduce CT-LLM, a 2B large language model (LLM) that illustrates a pivotal shift towards prioritizing the Chinese language in developing LLMs. Uniquely initiated from scratch, CT-LLM diverges from the conventional methodology \u2026"}]
