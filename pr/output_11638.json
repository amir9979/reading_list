[{"title": "Neural Probabilistic Circuits: Enabling Compositional and Interpretable Predictions through Logical Reasoning", "link": "https://arxiv.org/pdf/2501.07021", "details": "W Chen, S Yu, H Shao, L Sha, H Zhao - arXiv preprint arXiv:2501.07021, 2025", "abstract": "End-to-end deep neural networks have achieved remarkable success across various domains but are often criticized for their lack of interpretability. While post hoc explanation methods attempt to address this issue, they often fail to accurately \u2026"}, {"title": "BossNAS Family: Block-wisely Self-supervised Neural Architecture Search", "link": "https://www.computer.org/csdl/journal/tp/5555/01/10839629/23th14MPwZi", "details": "C Li, S Lin, T Tang, G Wang, M Li, Z Li, X Chang - IEEE Transactions on Pattern \u2026, 2025", "abstract": "Recent advances in hand-crafted neural architectures for visual recognition underscore the pressing need to explore architecture designs comprising diverse building blocks. Concurrently, neural architecture search (NAS) methods have \u2026"}, {"title": "Rethinking Evaluation of Sparse Autoencoders through the Representation of Polysemous Words", "link": "https://arxiv.org/pdf/2501.06254", "details": "G Minegishi, H Furuta, Y Iwasawa, Y Matsuo - arXiv preprint arXiv:2501.06254, 2025", "abstract": "Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool to improve the interpretability of large language models (LLMs) by mapping the complex superposition of polysemantic neurons into monosemantic features and \u2026"}, {"title": "LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2501.06986", "details": "MN Azadani, J Riddell, S Sedwards, K Czarnecki - arXiv preprint arXiv:2501.06986, 2025", "abstract": "Enhanced visual understanding serves as a cornerstone for multimodal large language models (MLLMs). Recent hybrid MLLMs incorporate a mixture of vision experts to address the limitations of using a single vision encoder and excessively \u2026"}, {"title": "Improving Self-Supervised Medical Image Pre-Training by Early Alignment with Human Eye Gaze Information", "link": "https://ieeexplore.ieee.org/abstract/document/10839445/", "details": "S Wang, Z Zhao, Z Shen, B Wang, Q Wang, D Shen - IEEE Transactions on Medical \u2026, 2025", "abstract": "Alignment between human knowledge and machine learning models is crucial for achieving efficient and interpretable AI systems. However, conventional self- supervised pre-training methods often suffer from low efficiency, as they do not \u2026"}]
