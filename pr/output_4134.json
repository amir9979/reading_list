[{"title": "Unlocking Continual Learning Abilities in Language Models", "link": "https://arxiv.org/pdf/2406.17245", "details": "W Du, S Cheng, T Luo, Z Qiu, Z Huang, KC Cheung\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) exhibit impressive performance and generalization capabilities. However, LMs struggle with the persistent challenge of catastrophic forgetting, which undermines their long-term sustainability in continual learning (CL) \u2026"}, {"title": "Scaling Laws for Linear Complexity Language Models", "link": "https://arxiv.org/pdf/2406.16690", "details": "X Shen, D Li, R Leng, Z Qin, W Sun, Y Zhong - arXiv preprint arXiv:2406.16690, 2024", "abstract": "The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their \u2026"}, {"title": "DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning", "link": "https://arxiv.org/pdf/2406.17659", "details": "X Zhang, Z Altaweel, Y Hayamizu, Y Ding, S Amiri\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language models (VLMs) have been applied to robot task planning problems, where the robot receives a task in natural language and generates plans based on visual inputs. While current VLMs have demonstrated strong vision-language \u2026"}, {"title": "Securing Multi-turn Conversational Language Models Against Distributed Backdoor Triggers", "link": "https://arxiv.org/pdf/2407.04151", "details": "T Tong, J Xu, Q Liu, M Chen - arXiv preprint arXiv:2407.04151, 2024", "abstract": "The security of multi-turn conversational large language models (LLMs) is understudied despite it being one of the most popular LLM utilization. Specifically, LLMs are vulnerable to data poisoning backdoor attacks, where an adversary \u2026"}, {"title": "Leveraging Language Models and Automatic Summarization in Online Programming Learning Environments", "link": "https://dl.acm.org/doi/full/10.1145/3653323", "details": "C Areces, L Benotti, F Bulgarelli, E Echeveste, N Finzi - Communications of the ACM", "abstract": "Objective A. Enhance the interaction between tutors, the Mumuki platform, and the group of trainee programmers. By utilizing the stochastic language models of learners' errors in each programming language, training errors in the exercise are \u2026"}, {"title": "Exploring Universal Intrinsic Task Subspace for Few-shot Learning via Prompt Tuning", "link": "https://ieeexplore.ieee.org/iel8/6570655/6633080/10603438.pdf", "details": "Y Qin, X Wang, Y Su, Y Lin, N Ding, J Yi, W Chen, Z Liu\u2026 - IEEE/ACM Transactions on \u2026, 2024", "abstract": "Why can pre-trained language models (PLMs) learn universal representations and effectively adapt to broad NLP tasks differing a lot superficially? In this work, we empirically find evidence indicating that the adaptations of PLMs to various fewshot \u2026"}, {"title": "Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models", "link": "https://openreview.net/pdf%3Fid%3Decmrk5PEjy", "details": "A Girish, A Nagle, AV Makkuva, M Bondaschi\u2026 - ICML 2024 Workshop on \u2026", "abstract": "We formalize the problem of token-level hard prompt compression for black-box large language models (LLMs). We derive the distortion-rate function for this setup as a linear program, and provide an efficient algorithm to compute this fundamental limit \u2026"}, {"title": "Automatic uncovering of patient primary concerns in portal messages using a fusion framework of pretrained language models", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae144/7700019", "details": "Y Ren, Y Wu, JW Fan, A Khurana, S Fu, D Wu, H Liu\u2026 - Journal of the American \u2026, 2024", "abstract": "Objectives The surge in patient portal messages (PPMs) with increasing needs and workloads for efficient PPM triage in healthcare settings has spurred the exploration of AI-driven solutions to streamline the healthcare workflow processes, ensuring \u2026"}, {"title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models", "link": "https://arxiv.org/pdf/2407.02408", "details": "S Wang, P Wang, T Zhou, Y Dong, Z Tan, J Li - arXiv preprint arXiv:2407.02408, 2024", "abstract": "As Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases \u2026"}]
