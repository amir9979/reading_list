[{"title": "Personalization Toolkit: Training Free Personalization of Large Vision Language Models", "link": "https://arxiv.org/pdf/2502.02452%3F", "details": "S Seifi, V Dorovatas, DO Reino, R Aljundi - arXiv preprint arXiv:2502.02452, 2025", "abstract": "Large Vision Language Models (LVLMs) have significant potential to deliver personalized assistance by adapting to individual users' unique needs and preferences. Personalization of LVLMs is an emerging area that involves \u2026"}, {"title": "\" See the World, Discover Knowledge\": A Chinese Factuality Evaluation for Large Vision Language Models", "link": "https://arxiv.org/pdf/2502.11718", "details": "J Gu, Y Wang, P Bu, C Wang, Z Wang, T Song, D Wei\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The evaluation of factual accuracy in large vision language models (LVLMs) has lagged behind their rapid development, making it challenging to fully reflect these models' knowledge capacity and reliability. In this paper, we introduce the first \u2026"}, {"title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models", "link": "https://arxiv.org/pdf/2502.10250%3F", "details": "GK Kumar, I Chaabane, K Wu - arXiv preprint arXiv:2502.10250, 2025", "abstract": "Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved \u2026"}, {"title": "Investigating the Robustness of Deductive Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2502.04352", "details": "F Hoppe, F Ilievski, JC Kalo - arXiv preprint arXiv:2502.04352, 2025", "abstract": "Large Language Models (LLMs) have been shown to achieve impressive results for many reasoning-based Natural Language Processing (NLP) tasks, suggesting a degree of deductive reasoning capability. However, it remains unclear to which \u2026"}, {"title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG", "link": "https://arxiv.org/pdf/2502.08356", "details": "K Bhushan, Y Nandwani, D Khandelwal, S Gupta\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the \u2026"}, {"title": "Franken-Adapter: Cross-Lingual Adaptation of LLMs by Embedding Surgery", "link": "https://arxiv.org/pdf/2502.08037", "details": "F Jiang, H Yu, G Chung, T Cohn - arXiv preprint arXiv:2502.08037, 2025", "abstract": "The capabilities of Large Language Models (LLMs) in low-resource languages lag far behind those in English, making their universal accessibility a significant challenge. To alleviate this, we present $\\textit {Franken-Adapter} $, a modular \u2026"}, {"title": "Can Large Language Models Be Query Optimizer for Relational Databases?", "link": "https://arxiv.org/pdf/2502.05562", "details": "J Tan, K Zhao, R Li, JX Yu, C Piao, H Cheng, H Meng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Query optimization, which finds the optimized execution plan for a given query, is a complex planning and decision-making problem within the exponentially growing plan space in database management systems (DBMS). Traditional optimizers heavily \u2026"}, {"title": "FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2502.11811", "details": "Q Zhang, H Zhang, L Pang, H Zheng, Y Tong, Z Zheng - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieved documents containing noise will hinder Retrieval-Augmented Generation (RAG) from detecting answer clues, necessitating noise filtering mechanisms to enhance accuracy. Existing methods use re-ranking or summarization to identify the \u2026"}, {"title": "SelfElicit: Your Language Model Secretly Knows Where is the Relevant Evidence", "link": "https://arxiv.org/pdf/2502.08767", "details": "Z Liu, RA Amjad, R Adkathimar, T Wei, H Tong - arXiv preprint arXiv:2502.08767, 2025", "abstract": "Providing Language Models (LMs) with relevant evidence in the context (either via retrieval or user-provided) can significantly improve their ability to provide factually correct grounded responses. However, recent studies have found that LMs often \u2026"}]
