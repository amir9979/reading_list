[{"title": "Training and Evaluating Language Models with Template-based Data Generation", "link": "https://arxiv.org/pdf/2411.18104", "details": "Y Zhang - arXiv preprint arXiv:2411.18104, 2024", "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, these \u2026"}, {"title": "Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks", "link": "https://arxiv.org/pdf/2411.16721", "details": "H Wang, G Wang, H Zhang - arXiv preprint arXiv:2411.16721, 2024", "abstract": "Vision Language Models (VLMs) can produce unintended and harmful content when exposed to adversarial attacks, particularly because their vision capabilities create new vulnerabilities. Existing defenses, such as input preprocessing, adversarial \u2026"}]
