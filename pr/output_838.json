'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [CORE-BEHRT: A Carefully Optimized and Rigorously Evalu'
[{"title": "EHRFL: Federated Learning Framework for Heterogeneous EHRs and Precision-guided Selection of Participating Clients", "link": "https://arxiv.org/pdf/2404.13318", "details": "J Kim, J Kim, K Hur, E Choi - arXiv preprint arXiv:2404.13318, 2024", "abstract": "In this study, we provide solutions to two practical yet overlooked scenarios in federated learning for electronic health records (EHRs): firstly, we introduce EHRFL, a framework that facilitates federated learning across healthcare institutions with \u2026"}, {"title": "CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and Radiology Reports for Full-Body Scenarios", "link": "https://arxiv.org/pdf/2404.15272", "details": "J Lin, Y Xia, J Zhang, K Yan, L Lu, J Luo, L Zhang - arXiv preprint arXiv:2404.15272, 2024", "abstract": "Medical Vision-Language Pretraining (Med-VLP) establishes a connection between visual content from medical images and the relevant textual descriptions. Existing Med-VLP methods primarily focus on 2D images depicting a single body part \u2026"}, {"title": "Grounded Knowledge-Enhanced Medical VLP for Chest X-Ray", "link": "https://arxiv.org/pdf/2404.14750", "details": "Q Deng, Z Huang, Y Wang, Z Wang, Z Wang, X Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical vision-language pre-training has emerged as a promising approach for learning domain-general representations of medical image and text. Current algorithms that exploit the global and local alignment between medical image and \u2026"}, {"title": "Memory-based Cross-modal Semantic Alignment Network for Radiology Report Generation", "link": "https://arxiv.org/pdf/2404.00588", "details": "Y Tao, L Ma, J Yu, H Zhang - arXiv preprint arXiv:2404.00588, 2024", "abstract": "Generating radiology reports automatically reduces the workload of radiologists and helps the diagnoses of specific diseases. Many existing methods take this task as modality transfer process. However, since the key information related to disease \u2026"}, {"title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies", "link": "https://arxiv.org/pdf/2404.06395", "details": "S Hu, Y Tu, X Han, C He, G Cui, X Long, Z Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This \u2026"}]
