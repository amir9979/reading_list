[{"title": "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "link": "https://aclanthology.org/2024.findings-emnlp.61.pdf", "details": "X Wei, H Chen, H Yu, H Fei, Q Liu - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have achieved notable success in commonsense reasoning tasks, benefiting from their extensive world knowledge acquired through extensive pretraining. While approaches like Chain-of-Thought \u2026"}, {"title": "Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality", "link": "https://arxiv.org/pdf/2411.11531", "details": "V Chekalina, A Razzigaev, E Goncharova, A Kuznetsov - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper we present an approach to reduce hallucinations in Large Language Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional modality. Our method involves transforming input text into a set of KG embeddings and using \u2026"}, {"title": "Text as Images: Can Multimodal Large Language Models Follow Printed Instructions in Pixels?", "link": "https://openreview.net/pdf%3Fid%3DdC9kEMBchM", "details": "X Li, Y Lu, WY Wang, Y Choi - Adaptive Foundation Models: Evolving AI for \u2026, 2024", "abstract": "Recent multimodal large language models (MLLMs) have shown promising instruction following capabilities on vision-language tasks. In this work, we introduce VISUAL MODALITY INSTRUCTION (VIM) 1, and investigate how well multimodal \u2026"}, {"title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices", "link": "https://arxiv.org/pdf/2411.10640", "details": "X Lu, Y Chen, C Chen, H Tan, B Chen, Y Xie, R Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile \u2026"}, {"title": "Group Robust Best-of-K Decoding of Language Models for Pluralistic Alignment", "link": "https://openreview.net/pdf%3Fid%3DJI6j4NUGHv", "details": "S Yoon, W Bankes, S Son, A Petrovic, SS Ramesh\u2026 - Pluralistic Alignment Workshop at \u2026", "abstract": "The desirable behaviour of a chat agent can be described with multiple criteria, such as harmlessness, helpfulness, and conciseness, each of which can be scored by a reward model. While each user, or a group of users, may perceive each criterion with \u2026"}, {"title": "Contextualized Evaluations: Taking the Guesswork Out of Language Model Evaluations", "link": "https://arxiv.org/pdf/2411.07237", "details": "C Malaviya, JC Chang, D Roth, M Iyyer, M Yatskar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language model users often issue queries that lack specification, where the context under which a query was issued--such as the user's identity, the query's intent, and the criteria for a response to be useful--is not explicit. For instance, a good response \u2026"}, {"title": "Continual Memorization of Factoids in Large Language Models", "link": "https://arxiv.org/pdf/2411.07175", "details": "H Chen, J Geng, A Bhaskar, D Friedman, D Chen - arXiv preprint arXiv:2411.07175, 2024", "abstract": "Large language models can absorb a massive amount of knowledge through pretraining, but pretraining is inefficient for acquiring long-tailed or specialized facts. Therefore, fine-tuning on specialized or new knowledge that reflects changes in the \u2026"}, {"title": "AssistRAG: Boosting the Potential of Large Language Models with an Intelligent Information Assistant", "link": "https://arxiv.org/pdf/2411.06805", "details": "Y Zhou, Z Liu, Z Dou - arXiv preprint arXiv:2411.06805, 2024", "abstract": "The emergence of Large Language Models (LLMs) has significantly advanced natural language processing, but these models often generate factually incorrect information, known as\" hallucination\". Initial retrieval-augmented generation (RAG) \u2026"}, {"title": "The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on Large Language Models", "link": "https://arxiv.org/pdf/2411.11407", "details": "X Yang, X Tang, J Han, S Hu - arXiv preprint arXiv:2411.11407, 2024", "abstract": "The widespread deployment of large language models (LLMs) across various domains has showcased their immense potential while exposing significant safety vulnerabilities. A major concern is ensuring that LLM-generated content aligns with \u2026"}]
