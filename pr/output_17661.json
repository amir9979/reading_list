[{"title": "Cross-Modal Clustering-Guided Negative Sampling for Self-Supervised Joint Learning from Medical Images and Reports", "link": "https://arxiv.org/pdf/2506.11674", "details": "L Lan, H Li, Z Xia, J Zhou, X Zhu, Y Li, Y Zhang, X Luo - arXiv preprint arXiv \u2026, 2025", "abstract": "Learning medical visual representations directly from paired images and reports through multimodal self-supervised learning has emerged as a novel and efficient approach to digital diagnosis in recent years. However, existing models suffer from \u2026", "entry_id": "http://arxiv.org/abs/2506.11674v1", "updated": "2025-06-13 11:08:16", "published": "2025-06-13 11:08:16", "authors": "Libin Lan;Hongxing Li;Zunhui Xia;Juan Zhou;Xiaofei Zhu;Yongmei Li;Yudong Zhang;Xin Luo", "summary": "Learning medical visual representations directly from paired images and\nreports through multimodal self-supervised learning has emerged as a novel and\nefficient approach to digital diagnosis in recent years. However, existing\nmodels suffer from several severe limitations. 1) neglecting the selection of\nnegative samples, resulting in the scarcity of hard negatives and the inclusion\nof false negatives; 2) focusing on global feature extraction, but overlooking\nthe fine-grained local details that are crucial for medical image recognition\ntasks; and 3) contrastive learning primarily targets high-level features but\nignoring low-level details which are essential for accurate medical analysis.\nMotivated by these critical issues, this paper presents a Cross-Modal\nCluster-Guided Negative Sampling (CM-CGNS) method with two-fold ideas. First,\nit extends the k-means clustering used for local text features in the\nsingle-modal domain to the multimodal domain through cross-modal attention.\nThis improvement increases the number of negative samples and boosts the model\nrepresentation capability. Second, it introduces a Cross-Modal Masked Image\nReconstruction (CM-MIR) module that leverages local text-to-image features\nobtained via cross-modal attention to reconstruct masked local image regions.\nThis module significantly strengthens the model's cross-modal information\ninteraction capabilities and retains low-level image features essential for\ndownstream tasks. By well handling the aforementioned limitations, the proposed\nCM-CGNS can learn effective and robust medical visual representations suitable\nfor various recognition tasks. Extensive experimental results on\nclassification, detection, and segmentation tasks across five downstream\ndatasets show that our method outperforms state-of-the-art approaches on\nmultiple metrics, verifying its superior performance.", "comment": "This work has been submitted to the IEEE TMI for possible\n  publication. Our code is available at https://github.com/violet-42/CM-CGNS", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.11674v1;http://arxiv.org/pdf/2506.11674v1", "pdf_url": "http://arxiv.org/pdf/2506.11674v1"}, {"title": "Dual-Masked Contrastive Learning Based Hypergraph Foundation Model for Whole Slide Images", "link": "https://www.sciencedirect.com/science/article/pii/S0031320325006557", "details": "X Zhou, S Ding, W Zhang, J Li, J Wang, J Chen, J Shi - Pattern Recognition, 2025", "abstract": "The spatial and contextual information in whole slide images (WSIs) is crucial for improving the diagnostic accuracy of computer-aided diagnosis (CAD). Although existing hypergraph-based methods have shown their effectiveness in capturing \u2026"}]
