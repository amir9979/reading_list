[{"title": "Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time Series", "link": "http://www.dm.sanken.osaka-u.ac.jp/~yasushi/publications/missnet.pdf", "details": "K Obata, K Kawabata, Y Matsubara, Y Sakurai - 2024", "abstract": "Multivariate time series data suffer from the problem of missing values, which hinders the application of many analytical methods. To achieve the accurate imputation of these missing values, exploiting inter-correlation by employing the relationships \u2026"}, {"title": "Guidelines for Augmentation Selection in Contrastive Learning for Time Series Classification", "link": "https://arxiv.org/pdf/2407.09336", "details": "Z Liu, A Alavi, M Li, X Zhang - arXiv preprint arXiv:2407.09336, 2024", "abstract": "Self-supervised contrastive learning has become a key technique in deep learning, particularly in time series analysis, due to its ability to learn meaningful representations without explicit supervision. Augmentation is a critical component in \u2026"}, {"title": "Towards Robustness Prompt Tuning with Fully Test-Time Adaptation for CLIP's Zero-Shot Generalization", "link": "https://openreview.net/pdf%3Fid%3DBVFAVis7ui", "details": "R Wang, H Zuo, Z Fang, J Lu - ACM Multimedia 2024", "abstract": "In the field of Vision-Language Models (VLM), the Contrastive Language-Image Pretraining (CLIP) model has yielded outstanding performance on many downstream tasks through prompt tuning. By integrating image and text representations, CLIP \u2026"}, {"title": "Multi-view Self-Supervised Contrastive Learning for Multivariate Time Series", "link": "https://openreview.net/pdf%3Fid%3DBgjLy3chju", "details": "Y Wu, X Meng, Y He, J Zhang, H Zhang, Y Dong, D Lu - ACM Multimedia 2024", "abstract": "Learning semantic-rich representations from unlabeled time series data with intricate dynamics is a notable challenge. Traditional contrastive learning techniques predominantly focus on segment-level augmentations through time slicing, a practice \u2026"}, {"title": "Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks", "link": "https://arxiv.org/pdf/2407.20657", "details": "H Yang, J Jeong, KJ Yoon - arXiv preprint arXiv:2407.20657, 2024", "abstract": "Recent vision-language foundation models, such as CLIP, have demonstrated superior capabilities in learning representations that can be transferable across diverse range of downstream tasks and domains. With the emergence of such \u2026"}, {"title": "Sparse transformer with local and seasonal adaptation for multivariate time series forecasting", "link": "https://www.nature.com/articles/s41598-024-66886-1", "details": "Y Zhang, R Wu, SM Dascalu, FC Harris Jr - Scientific Reports, 2024", "abstract": "Transformers have achieved remarkable performance in multivariate time series (MTS) forecasting due to their capability to capture long-term dependencies. However, the canonical attention mechanism has two key limitations:(1) its quadratic \u2026"}, {"title": "Learning by imitating the classics: Mitigating class imbalance in federated learning via simulated centralized learning", "link": "https://www.sciencedirect.com/science/article/pii/S0957417424016221", "details": "G Zhu, X Liu, J Niu, Y Wei, S Tang, J Zhang - Expert Systems with Applications, 2024", "abstract": "Federated learning (FL) is a distributed machine learning framework in which multiple clients update their local models in parallel and then aggregate them to generate a global model. However, when local data on different clients are class \u2026"}, {"title": "FMamba: Mamba based on Fast-attention for Multivariate Time-series Forecasting", "link": "https://arxiv.org/pdf/2407.14814", "details": "S Ma, Y Kang, P Bai, YB Zhao - arXiv preprint arXiv:2407.14814, 2024", "abstract": "In multivariate time-series forecasting (MTSF), extracting the temporal correlations of the input sequences is crucial. While popular Transformer-based predictive models can perform well, their quadratic computational complexity results in inefficiency and \u2026"}]
