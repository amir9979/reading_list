[{"title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information", "link": "https://arxiv.org/pdf/2412.00947", "details": "R Kamoi, Y Zhang, SSS Das, RH Zhang, R Zhang - arXiv preprint arXiv:2412.00947, 2024", "abstract": "Errors in understanding visual information in images (ie, visual perception errors) remain a major source of mistakes in Large Vision Language Models (LVLMs). While further analysis is essential, there is a deficiency in datasets for evaluating the visual \u2026"}, {"title": "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset", "link": "https://arxiv.org/pdf/2412.02595", "details": "D Su, K Kong, Y Lin, J Jennings, B Norick, M Kliegl\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved significant benchmark gains via aggressive model-based filtering, but at the cost of removing 90% of data. This limits their suitability for long token horizon training, such \u2026"}, {"title": "Lupus Alberto: A Transformer-Based Approach for SLE Information Extraction from Italian Clinical Reports", "link": "https://ceur-ws.org/Vol-3878/59_main_long.pdf", "details": "L Lilli, L Antenucci, A Ortolan, SL Bosello\u2026 - 2024", "abstract": "Abstract Natural Language Processing (NLP) is widely used across several fields, such as in medicine, where information often originates from unstructured data sources. This creates the need for automated systems, in order to classify text and \u2026"}, {"title": "Implicit Bias in ICU Electronic Health Record Data Measurement Frequencies and Missingness Rates of Clinical Variables", "link": "https://www.researchsquare.com/article/rs-5362869/latest.pdf", "details": "JS Shi, AE Hubbard, N Fong, R Pirracchio - 2024", "abstract": "Background: Disparities in data collection within electronic health records (EHRs), especially in Intensive Care Units (ICUs), can reveal underlying biases that may affect patient outcomes. Identifying and mitigating these biases is critical for ensuring \u2026"}, {"title": "Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models", "link": "https://arxiv.org/pdf/2412.16545", "details": "Z Zhang, Y Wang, X Huang, T Fang, H Zhang, C Deng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models have shown remarkable performance across a wide range of language tasks, owing to their exceptional capabilities in context modeling. The most commonly used method of context modeling is full self-attention, as seen in \u2026"}, {"title": "Mastering Board Games by External and Internal Planning with Language Models", "link": "https://arxiv.org/pdf/2412.12119%3F", "details": "J Schultz, J Adamek, M Jusup, M Lanctot, M Kaisers\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While large language models perform well on a range of complex tasks (eg, text generation, question answering, summarization), robust multi-step planning and reasoning remains a considerable challenge for them. In this paper we show that \u2026"}, {"title": "Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective", "link": "https://arxiv.org/pdf/2412.00554", "details": "Y Zhou, B Di Eugenio, L Cheng - arXiv preprint arXiv:2412.00554, 2024", "abstract": "This paper studies the performance of large language models (LLMs), particularly regarding demographic fairness, in solving real-world healthcare tasks. We evaluate state-of-the-art LLMs with three prevalent learning frameworks across six diverse \u2026"}, {"title": "Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training", "link": "https://arxiv.org/pdf/2412.00440", "details": "H Wang, C Ju, W Lin, S Xiao, M Chen, Y Huang, C Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In rapidly evolving field of vision-language models (VLMs), contrastive language- image pre-training (CLIP) has made significant strides, becoming foundation for various downstream tasks. However, relying on one-to-one (image, text) contrastive \u2026"}, {"title": "Improving intermediate reasoning in zero-shot chain-of-thought for large language models with filter supervisor-self correction", "link": "https://www.sciencedirect.com/science/article/pii/S0925231224019908", "details": "J Sun, Y Pan, X Yan - Neurocomputing, 2024", "abstract": "Abstract Chain of Thought (CoT) prompting enables Large Language Models (LLMs) to generate detailed intermediate reasoning steps to solve problems, demonstrating excellent performance across various fields. However, when LLMs encounter \u2026"}]
