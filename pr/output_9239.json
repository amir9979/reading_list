[{"title": "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.221.pdf", "details": "S Jiang, T Zheng, Y Zhang, Y Jin, L Yuan, Z Liu - Findings of the Association for \u2026, 2024", "abstract": "Recent advancements in general-purpose or domain-specific multimodal large language models (LLMs) have witnessed remarkable progress for medical decision- making. However, they are designated for specific classification or generative tasks \u2026"}, {"title": "LM2: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.920.pdf", "details": "G Juneja, S Dutta, T Chakraborty - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}, {"title": "Mixed Distillation Helps Smaller Language Models Reason Better", "link": "https://aclanthology.org/2024.findings-emnlp.91.pdf", "details": "L Chenglin, Q Chen, L Li, C Wang, F Tao, Y Li, Z Chen\u2026 - Findings of the Association \u2026, 2024", "abstract": "As large language models (LLMs) have demonstrated impressive multiple step-by- step reasoning capabilities in recent natural language processing (NLP) reasoning tasks, many studies are interested in distilling reasoning abilities into smaller \u2026"}, {"title": "SearchLVLMs: A Plug-and-Play Framework for Augmenting Large Vision-Language Models by Searching Up-to-Date Internet Knowledge", "link": "https://openreview.net/pdf%3Fid%3Dleeosk2RAM", "details": "C Li, Z Li, C Jing, S Liu, W Shao, Y Wu, P Luo, Y Qiao\u2026 - The Thirty-eighth Annual \u2026, 2024", "abstract": "Large vision-language models (LVLMs) are ignorant of the up-to-date knowledge, such as LLaVA series, because they cannot be updated frequently due to the large amount of resources required, and therefore fail in many cases. For example, if a \u2026"}, {"title": "Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality", "link": "https://arxiv.org/pdf/2411.11531", "details": "V Chekalina, A Razzigaev, E Goncharova, A Kuznetsov - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper we present an approach to reduce hallucinations in Large Language Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional modality. Our method involves transforming input text into a set of KG embeddings and using \u2026"}, {"title": "End-to-End Navigation with Vision Language Models: Transforming Spatial Reasoning into Question-Answering", "link": "https://arxiv.org/pdf/2411.05755", "details": "D Goetting, HG Singh, A Loquercio - arXiv preprint arXiv:2411.05755, 2024", "abstract": "We present VLMnav, an embodied framework to transform a Vision-Language Model (VLM) into an end-to-end navigation policy. In contrast to prior work, we do not rely on a separation between perception, planning, and control; instead, we use a VLM to \u2026"}, {"title": "Hidden in Plain Sight: Evaluating Abstract Shape Recognition in Vision-Language Models", "link": "https://arxiv.org/pdf/2411.06287", "details": "A Hemmat, A Davies, TA Lamb, J Yuan, P Torr\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the importance of shape perception in human vision, early neural image classifiers relied less on shape information for object recognition than other (often spurious) features. While recent research suggests that current large Vision \u2026"}, {"title": "Improving Adversarial Robustness in Vision-Language Models with Architecture and Prompt Design", "link": "https://aclanthology.org/2024.findings-emnlp.990.pdf", "details": "R Bhagwatkar, S Nayak, P Bashivan, I Rish - Findings of the Association for \u2026, 2024", "abstract": "Abstract Vision-Language Models (VLMs) have seen a significant increase in both research interest and real-world applications across various domains, including healthcare, autonomous systems, and security. However, their growing prevalence \u2026"}, {"title": "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "link": "https://aclanthology.org/2024.emnlp-main.124.pdf", "details": "Y Yang, J Ko, SY Yun - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Vision-language models (VLMs) like CLIP have demonstrated remarkable applicability across a variety of downstream tasks, including zero-shot image classification. Recently, the use of prompts or adapters for efficient transfer learning \u2026"}]
