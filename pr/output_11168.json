[{"title": "CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries", "link": "https://arxiv.org/pdf/2501.01282", "details": "S Liu, Y Jin, C Li, DF Wong, Q Wen, L Sun, H Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) have advanced human-AI interaction but struggle with cultural understanding, often misinterpreting symbols, gestures, and artifacts due to biases in predominantly Western-centric training data. In this paper, we \u2026"}, {"title": "Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking", "link": "https://arxiv.org/pdf/2501.01306", "details": "X Cheng, J Li, WX Zhao, JR Wen - arXiv preprint arXiv:2501.01306, 2025", "abstract": "Large language models (LLMs) demonstrate exceptional capabilities, yet still face the hallucination issue. Typical text generation approaches adopt an auto-regressive generation without deliberate reasoning, which often results in untrustworthy and \u2026"}, {"title": "2 OLMo 2 Furious", "link": "https://arxiv.org/pdf/2501.00656", "details": "T OLMo, P Walsh, L Soldaini, D Groeneveld, K Lo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model \u2026"}, {"title": "ToolComp: A Multi-Tool Reasoning & Process Supervision Benchmark", "link": "https://arxiv.org/pdf/2501.01290", "details": "V Nath, P Raja, C Yoon, S Hendryx - arXiv preprint arXiv:2501.01290, 2025", "abstract": "Despite recent advances in AI, the development of systems capable of executing complex, multi-step reasoning tasks involving multiple tools remains a significant challenge. Current benchmarks fall short in capturing the real-world complexity of \u2026"}]
