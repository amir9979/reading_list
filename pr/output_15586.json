[{"title": "Transformation of audio embeddings into interpretable, concept-based representations", "link": "https://arxiv.org/pdf/2504.14076", "details": "A Zhang, E Thomaz, L Lu - arXiv preprint arXiv:2504.14076, 2025", "abstract": "Advancements in audio neural networks have established state-of-the-art results on downstream audio tasks. However, the black-box structure of these models makes it difficult to interpret the information encoded in their internal audio representations. In \u2026"}, {"title": "Negate or Embrace: On How Misalignment Shapes Multimodal Representation Learning", "link": "https://arxiv.org/pdf/2504.10143%3F", "details": "Y Cai, Y Liu, E Gao, T Jiang, Z Zhang, A Hengel, JQ Shi - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal representation learning, exemplified by multimodal contrastive learning (MMCL) using image-text pairs, aims to learn powerful representations by aligning cues across modalities. This approach relies on the core assumption that the \u2026"}, {"title": "Masked Autoencoder Self Pre-Training for Defect Detection in Microelectronics", "link": "https://arxiv.org/pdf/2504.10021", "details": "N R\u00f6hrich, A Hoffmann, R Nordsieck, E Zarbali\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Whereas in general computer vision, transformer-based architectures have quickly become the gold standard, microelectronics defect detection still heavily relies on convolutional neural networks (CNNs). We hypothesize that this is due to the fact that \u2026"}, {"title": "PixCon: Pixel-Level Contrastive Learning Revisited", "link": "https://www.mdpi.com/2079-9292/14/8/1623", "details": "Z Pang, Y Nakashima, M Otani, H Nagahara - Electronics, 2025", "abstract": "Contrastive image representation learning has been essential for pre-training vision foundation models to deliver excellent transfer learning performance. It was originally developed based on instance discrimination, which focuses on instance-level \u2026"}, {"title": "Causal Disentanglement for Robust Long-tail Medical Image Generation", "link": "https://arxiv.org/pdf/2504.14450", "details": "W Nie, Z Zhang, W Wang, B Lepri, A Liu, N Seb - arXiv preprint arXiv:2504.14450, 2025", "abstract": "Counterfactual medical image generation effectively addresses data scarcity and enhances the interpretability of medical images. However, due to the complex and diverse pathological features of medical images and the imbalanced class \u2026"}, {"title": "D $^ 2$ iT: Dynamic Diffusion Transformer for Accurate Image Generation", "link": "https://arxiv.org/pdf/2504.09454", "details": "W Jia, M Huang, N Chen, L Zhang, Z Mao - arXiv preprint arXiv:2504.09454, 2025", "abstract": "Diffusion models are widely recognized for their ability to generate high-fidelity images. Despite the excellent performance and scalability of the Diffusion Transformer (DiT) architecture, it applies fixed compression across different image \u2026"}, {"title": "MirrorDiff: Prompt redescription for zero-shot grounded text-to-image generation with attention modulation", "link": "https://www.sciencedirect.com/science/article/pii/S0952197625007419", "details": "C Liu, M Shao, Z Gong, X Lv, L Meng - Engineering Applications of Artificial \u2026, 2025", "abstract": "Large-scale layout-conditioned text-to-image diffusion models have made significant progress and achieved remarkable results in generating diverse and high-quality images, realizing objects appearing in specific regions simultaneously. However \u2026"}]
