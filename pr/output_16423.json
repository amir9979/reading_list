[{"title": "AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection", "link": "https://arxiv.org/pdf/2505.07293", "details": "K Hua, S Wu, G Zhang, K Shen - arXiv preprint arXiv:2505.07293, 2025", "abstract": "Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling \u2026"}, {"title": "Consistency in Language Models: Current Landscape, Challenges, and Future Directions", "link": "https://arxiv.org/pdf/2505.00268%3F", "details": "J Novikova, C Anderson, B Blili-Hamelin, S Majumdar - arXiv preprint arXiv \u2026, 2025", "abstract": "The hallmark of effective language use lies in consistency--expressing similar meanings in similar contexts and avoiding contradictions. While human communication naturally demonstrates this principle, state-of-the-art language \u2026"}, {"title": "HMI: hierarchical knowledge management for efficient multi-tenant inference in pretrained language models", "link": "https://arxiv.org/pdf/2504.17449", "details": "J Zhang, J Wang, H Li, L Shou, K Chen, G Chen, Q Xie\u2026 - The VLDB Journal, 2025", "abstract": "The significant computational demands of pretrained language models (PLMs), which often require dedicated hardware, present a substantial challenge in serving them efficiently, especially in multi-tenant environments. To address this, we \u2026"}, {"title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation", "link": "https://arxiv.org/pdf/2504.20859", "details": "G Hadad, H Roitman, Y Eshel, B Shapira, L Rokach - arXiv preprint arXiv:2504.20859, 2025", "abstract": "As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents``X-Cross''--a novel cross-domain sequential-recommendation model \u2026"}, {"title": "Towards Zero-shot Question Answering in CPS-IoT: Large Language Models and Knowledge Graphs", "link": "https://dl.acm.org/doi/pdf/10.1145/3722565.3727197", "details": "OB Mulayim, G Fierro, M Berg\u00e9s, M Pritoni - \u2026 of the 2nd International Workshop on \u2026, 2025", "abstract": "Natural language provides an intuitive interface for querying data, yet its unstructured nature often makes precise retrieval of information challenging. Knowledge graphs (KGs), with their structured and relational representations, offer a powerful solution to \u2026"}, {"title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment", "link": "https://arxiv.org/pdf/2504.12663", "details": "X Zhang, R Chen, Y Feng, Z Liu - arXiv preprint arXiv:2504.12663, 2025", "abstract": "Aligning language models with human preferences presents significant challenges, particularly in achieving personalization without incurring excessive computational costs. Existing methods rely on reward signals and additional annotated data \u2026"}, {"title": "A Framework for Domain-Specific Dataset Creation and Adaptation of Large Language Models", "link": "https://www.mdpi.com/2073-431X/14/5/172", "details": "G Balaskas, H Papadopoulos, D Pappa, Q Loisel\u2026 - Computers, 2025", "abstract": "This paper introduces a novel framework for addressing domain adaptation challenges in large language models (LLMs), emphasising privacy-preserving synthetic data generation and efficient fine-tuning. The proposed framework employs \u2026"}]
