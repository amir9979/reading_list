[{"title": "Mixed Distillation Helps Smaller Language Models Reason Better", "link": "https://aclanthology.org/2024.findings-emnlp.91.pdf", "details": "L Chenglin, Q Chen, L Li, C Wang, F Tao, Y Li, Z Chen\u2026 - Findings of the Association \u2026, 2024", "abstract": "As large language models (LLMs) have demonstrated impressive multiple step-by- step reasoning capabilities in recent natural language processing (NLP) reasoning tasks, many studies are interested in distilling reasoning abilities into smaller \u2026"}, {"title": "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "link": "https://aclanthology.org/2024.emnlp-main.565.pdf", "details": "J Li, H Zhang, F Zhang, TW Chang, K Kuang, L Chen\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Reinforcement learning from human feedback (RLHF) and AI-generated feedback (RLAIF) have become prominent techniques that significantly enhance the functionality of pre-trained language models (LMs). These methods harness \u2026"}, {"title": "Improving Referring Ability for Biomedical Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.375.pdf", "details": "J Jiang, F Cheng, A Aizawa - Findings of the Association for Computational \u2026, 2024", "abstract": "Existing auto-regressive large language models (LLMs) are primarily trained using documents from general domains. In the biomedical domain, continual pre-training is a prevalent method for domain adaptation to inject professional knowledge into \u2026"}, {"title": "LM2: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.920.pdf", "details": "G Juneja, S Dutta, T Chakraborty - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}, {"title": "GPTKB: Building Very Large Knowledge Bases from Language Models", "link": "https://arxiv.org/pdf/2411.04920", "details": "Y Hu, S Ghosh, TP Nguyen, S Razniewski - arXiv preprint arXiv:2411.04920, 2024", "abstract": "General-domain knowledge bases (KB), in particular the\" big three\"--Wikidata, Yago and DBpedia--are the backbone of many intelligent applications. While these three have seen steady development, comprehensive KB construction at large has seen \u2026"}, {"title": "Efficient Large Multi-modal Models via Visual Context Compression", "link": "https://openreview.net/pdf%3Fid%3D5ujp72CiYB", "details": "J Chen, L Ye, J He, ZY Wang, D Khashabi, A Yuille - The Thirty-eighth Annual Conference \u2026", "abstract": "While significant advancements have been made in compressed representations for text embeddings in large language models (LLMs), the compression of visual tokens in multi-modal LLMs (MLLMs) has remained a largely overlooked area. In this work \u2026"}, {"title": "Rethinking the Role of Proxy Rewards in Language Model Alignment", "link": "https://aclanthology.org/2024.emnlp-main.1150.pdf", "details": "S Kim, M Seo - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Learning from human feedback via proxy reward modeling has been studied to align Large Language Models (LLMs) with human values. However, achieving reliable training through that proxy reward model (RM) is not a trivial problem, and its \u2026"}, {"title": "Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.803.pdf", "details": "Y Xu, X Qi, Z Qin, W Wang - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Multimodal Large Language Models (MLLMs) extend the capacity of LLMs to understand multimodal information comprehensively, achieving remarkable performance in many visioncentric tasks. Despite that, recent studies have shown \u2026"}, {"title": "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "link": "https://aclanthology.org/2024.emnlp-main.571.pdf", "details": "X Ren, B Wu, L Liu - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "This paper explores an intriguing observation: fine-tuning a large language model (LLM) with responses generated by a LLM often yields better results than using responses generated by humans, particularly in reasoning tasks. We conduct an in \u2026"}]
