[{"title": "How to Train Long-Context Language Models (Effectively)", "link": "https://arxiv.org/pdf/2410.02660%3F", "details": "T Gao, A Wettig, H Yen, D Chen - arXiv preprint arXiv:2410.02660, 2024", "abstract": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development--Instead of perplexity or simple \u2026"}, {"title": "MILE: Memory-Interactive Learning Engine for Neuro-Symbolic Solutions to Mathematical Problems", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10680497.pdf", "details": "Y Wu, H Nakayama - IEEE Access, 2024", "abstract": "Mathematical problem solving is a task that examines the capacity of machine learning systems to perform quantitative and logical reasoning. Existing work employed formulas as intermediate labels in this task to implement a neuro-symbolic \u2026"}, {"title": "Mutual Prompt Leaning for Vision Language Models", "link": "https://link.springer.com/article/10.1007/s11263-024-02243-z", "details": "S Long, Z Zhao, J Yuan, Z Tan, J Liu, J Feng, S Wang\u2026 - International Journal of \u2026, 2024", "abstract": "Large pre-trained vision language models (VLMs) have demonstrated impressive representation learning capabilities, but their transferability across various downstream tasks heavily relies on prompt learning. Since VLMs consist of text and \u2026"}, {"title": "TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles", "link": "https://arxiv.org/pdf/2410.05262", "details": "Q Yu, S Song, K Fang, Y Shi, Z Zheng, H Wang, S Niu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As the application of Large Language Models (LLMs) expands, the demand for reliable evaluations increases. Existing LLM evaluation benchmarks primarily rely on static datasets, making it challenging to assess model performance in dynamic \u2026"}, {"title": "Judgment of Thoughts: Courtroom of the Binary Logical Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2409.16635", "details": "S Park, D Choi - arXiv preprint arXiv:2409.16635, 2024", "abstract": "This paper proposes a novel prompt engineering technique called Judgment of Thought (JoT) that is specifically tailored for binary logical reasoning tasks. JoT employs three roles $\\unicode {x2014} $ lawyer, prosecutor, and judge $\\unicode \u2026"}, {"title": "GroupDebate: Enhancing the Efficiency of Multi-Agent Debate Using Group Discussion", "link": "https://arxiv.org/pdf/2409.14051", "details": "T Liu, X Wang, W Huang, W Xu, Y Zeng, L Jiang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse NLP tasks. Extensive research has explored how to enhance the logical reasoning abilities such as Chain-of-Thought, Chain-of-Thought \u2026"}, {"title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models", "link": "https://arxiv.org/pdf/2409.18943", "details": "J Li, L Zhang, Y Li, Z Liu, R Luo, L Chen, M Yang - arXiv preprint arXiv:2409.18943, 2024", "abstract": "The instruction-following ability of large language models enables humans to interact with AI agents in a natural way. However, when required to generate responses of a specific length, large language models often struggle to meet users' needs due to \u2026"}, {"title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "link": "https://arxiv.org/pdf/2409.14037", "details": "P Bajpai, N Chatterjee, S Dutta, T Chakraborty - arXiv preprint arXiv:2409.14037, 2024", "abstract": "Large Language Models (LLMs) and AI assistants driven by these models are experiencing exponential growth in usage among both expert and amateur users. In this work, we focus on evaluating the reliability of current LLMs as science \u2026"}, {"title": "Generative Chain-of-Thought for Zero-Shot Cognitive Reasoning", "link": "https://link.springer.com/chapter/10.1007/978-3-031-72344-5_22", "details": "L Liu, D Zhang, S Zhu, S Li - International Conference on Artificial Neural Networks, 2024", "abstract": "Cognitive reasoning holds a significant place within the field of Natural Language Processing (NLP). Yet, the exploration of zero-shot scenarios, which align more closely with real-life situations than supervised scenarios, has been relatively limited \u2026"}]
