'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Posterior Uncertainty Quantification in Neural Network'
[{"title": "Scalable Moment Propagation and Analysis of Variational Distributions for Practical Bayesian Deep Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10449445/", "details": "Y Hirayama, S Takamaeda-Yamazaki - IEEE Transactions on Neural Networks and \u2026, 2024", "abstract": "Bayesian deep learning is one of the key frameworks employed in handling predictive uncertainty. Variational inference (VI), an extensively used inference method, derives the predictive distributions by Monte Carlo (MC) sampling. The \u2026"}, {"title": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation", "link": "https://arxiv.org/pdf/2403.12015", "details": "A Sauer, F Boesel, T Dockhorn, A Blattmann, P Esser\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Diffusion models are the main driver of progress in image and video synthesis, but suffer from slow inference speed. Distillation methods, like the recently introduced adversarial diffusion distillation (ADD) aim to shift the model from many-shot to single \u2026"}, {"title": "Spatio-Temporal Consistency for Multivariate Time-Series Representation Learning", "link": "https://ieeexplore.ieee.org/iel7/6287639/6514899/10445124.pdf", "details": "S Lee, W Kim, Y Son - IEEE Access, 2024", "abstract": "Label sparsity in multivariate time series (MTS) makes using label information for practical applications challenging. Thus, unsupervised representation learning methods have gained attention to learn effective representations suitable for various \u2026"}, {"title": "Advancing Time Series Classification with Multimodal Language Modeling", "link": "https://arxiv.org/pdf/2403.12371", "details": "M Cheng, Y Chen, Q Liu, Z Liu, Y Luo - arXiv preprint arXiv:2403.12371, 2024", "abstract": "For the advancements of time series classification, scrutinizing previous studies, most existing methods adopt a common learning-to-classify paradigm-a time series classifier model tries to learn the relation between sequence inputs and target label \u2026"}, {"title": "PDETime: Rethinking Long-Term Multivariate Time Series Forecasting from the perspective of partial differential equations", "link": "https://arxiv.org/html/2402.16913v1", "details": "S Qi, Z Xu, Y Li, L Wen, Q Wen, Q Wang, Y Qi - arXiv preprint arXiv:2402.16913, 2024", "abstract": "Recent advancements in deep learning have led to the development of various models for long-term multivariate time-series forecasting (LMTF), many of which have shown promising results. Generally, the focus has been on historical-value-based \u2026"}, {"title": "Reward Guided Latent Consistency Distillation", "link": "https://arxiv.org/html/2403.11027v1", "details": "J Li, W Feng, W Chen, WY Wang - arXiv preprint arXiv:2403.11027, 2024", "abstract": "Latent Consistency Distillation (LCD) has emerged as a promising paradigm for efficient text-to-image synthesis. By distilling a latent consistency model (LCM) from a pre-trained teacher latent diffusion model (LDM), LCD facilitates the generation of \u2026"}, {"title": "Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling", "link": "https://arxiv.org/html/2402.12694v1", "details": "G Yu, J Zou, X Hu, AI Aviles-Rivero, J Qin, S Wang - arXiv preprint arXiv:2402.12694, 2024", "abstract": "Predicting multivariate time series is crucial, demanding precise modeling of intricate patterns, including inter-series dependencies and intra-series variations. Distinctive trend characteristics in each time series pose challenges, and existing methods \u2026"}]
