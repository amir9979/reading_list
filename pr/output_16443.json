[{"title": "VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models", "link": "https://arxiv.org/pdf/2505.08455", "details": "P Sarkar, A Etemad - arXiv preprint arXiv:2505.08455, 2025", "abstract": "Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for \u2026"}, {"title": "Exploring Grounding Abilities in Vision-Language Models through Contextual Perception", "link": "https://ieeexplore.ieee.org/abstract/document/10985830/", "details": "W Xu, T Zhou, T Zhang, J Li, P Chen, J Pan, X Liu - IEEE Transactions on Cognitive \u2026, 2025", "abstract": "Vision language models (VLMs) have demonstrated strong general capabilities and achieved great success in areas such as image understanding and reasoning. Visual prompts enhance the focus of VLMs on designated areas, but their fine \u2026"}, {"title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models", "link": "https://arxiv.org/pdf/2504.14395", "details": "B Jalaian, ND Bastian - arXiv preprint arXiv:2504.14395, 2025", "abstract": "To develop trustworthy Vision-Language Models (VLMs), it is essential to address adversarial robustness and hallucination mitigation, both of which impact factual accuracy in high-stakes applications such as defense and healthcare. Existing \u2026"}, {"title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "link": "https://arxiv.org/pdf/2504.14194", "details": "X Zhuang, J Peng, R Ma, Y Wang, T Bai, X Wei, J Qiu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural \u2026"}, {"title": "Exploring Multimodal Language Models for Sustainability Disclosure Extraction: A Comparative Study", "link": "https://aclanthology.org/2025.insights-1.13.pdf", "details": "T Gupta, T Goel, I Verma - The Sixth Workshop on Insights from Negative Results \u2026, 2025", "abstract": "Sustainability metrics have increasingly become a crucial non-financial criterion in investment decision-making. Organizations worldwide are recognizing the importance of sustainability and are proactively highlighting their efforts through \u2026"}, {"title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "link": "https://arxiv.org/pdf/2504.14366", "details": "P Haller, J Golde, A Akbik - arXiv preprint arXiv:2504.14366, 2025", "abstract": "Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures \u2026"}, {"title": "Investigating Zero-Shot Diagnostic Pathology in Vision-Language Models with Efficient Prompt Design", "link": "https://arxiv.org/pdf/2505.00134", "details": "V Sharma, A Alagha, A Khellaf, VQH Trinh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) have gained significant attention in computational pathology due to their multimodal learning capabilities that enhance big-data analytics of giga-pixel whole slide image (WSI). However, their sensitivity to large \u2026"}, {"title": "Knowledge-enhanced Parameter-efficient Transfer Learning with METER for medical vision-language tasks", "link": "https://www.sciencedirect.com/science/article/pii/S1532046425000693", "details": "X Liang, J Xie, J Wei, M Zhang, H Zhang - Journal of Biomedical Informatics, 2025", "abstract": "Objective: The full fine-tuning paradigm becomes impractical when applying pre- trained models to downstream tasks due to significant computational and storage costs. Parameter-efficient fine-tuning (PEFT) methods can alleviate the issue \u2026"}, {"title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example", "link": "https://arxiv.org/pdf/2504.20571", "details": "Y Wang, Q Yang, Z Zeng, L Ren, L Liu, B Peng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2. 5-Math \u2026"}]
