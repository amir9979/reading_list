[{"title": "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL", "link": "https://arxiv.org/pdf/2410.11371", "details": "Q Zhong, K Chen, L Ding, J Liu, B Du, D Tao - arXiv preprint arXiv:2410.11371, 2024", "abstract": "Large Language Models (LLMs) have shown promising performance in text-to-SQL, which involves translating natural language questions into SQL queries. However, current text-to-SQL LLMs are computationally expensive and challenging to deploy \u2026"}, {"title": "Large Language Model Evaluation via Matrix Nuclear-Norm", "link": "https://arxiv.org/pdf/2410.10672", "details": "Y Li, T Xia, Y Chang, Y Wu - arXiv preprint arXiv:2410.10672, 2024", "abstract": "As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are \u2026"}, {"title": "$\\textbf {Only-IF} $: Revealing the Decisive Effect of Instruction Diversity on Generalization", "link": "https://arxiv.org/pdf/2410.04717", "details": "D Zhang, J Wang, F Charton - arXiv preprint arXiv:2410.04717, 2024", "abstract": "Understanding and accurately following instructions is critical for large language models (LLMs) to be effective across diverse tasks. In this work, we rigorously examine the key factors that enable models to generalize to unseen instructions \u2026"}, {"title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "link": "https://arxiv.org/pdf/2410.11829", "details": "Y Cao, Y Liu, Z Chen, G Shi, W Wang, D Zhao, T Lu - arXiv preprint arXiv:2410.11829, 2024", "abstract": "Despite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating \u2026"}, {"title": "Enhancing Language Model Reasoning via Weighted Reasoning in Self-Consistency", "link": "https://arxiv.org/pdf/2410.07839", "details": "T Knappe, R Li, A Chauhan, K Chhua, K Zhu, S O'Brien - arXiv preprint arXiv \u2026, 2024", "abstract": "While large language models (LLMs) have rapidly improved their performance on a broad number of tasks, they still often fall short on reasoning tasks. As LLMs become more integrated in diverse real-world tasks, advancing their reasoning capabilities is \u2026"}, {"title": "$\\beta $-calibration of Language Model Confidence Scores for Generative QA", "link": "https://arxiv.org/pdf/2410.06615", "details": "P Manggala, A Mastakouri, E Kirschbaum\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim \u2026"}, {"title": "Applying Sparse Autoencoders to Unlearn Knowledge in Language Models", "link": "https://openreview.net/pdf%3Fid%3Di4z0HrBiIA", "details": "E Farrell, YT Lau, A Conmy - Neurips Safe Generative AI Workshop 2024", "abstract": "We investigate whether sparse autoencoders (SAEs) can be used to remove knowledge from language models. We use the biology subset of the Weapons of Mass Destruction Proxy dataset and test on the gemma-2b-it and gemma-2-2b-it \u2026"}, {"title": "TiC-LM: A Multi-Year Benchmark for Continual Pretraining of Language Models", "link": "https://openreview.net/pdf%3Fid%3DPpSDVE5rAy", "details": "J Li, M Armandpour, SI Mirzadeh, S Mehta, V Shankar\u2026 - NeurIPS 2024 Workshop on \u2026", "abstract": "Large language models (LLMs) are trained on data crawled over many years from the web. We investigate how quickly LLMs become outdated as the world evolves with time and how to best update them with newer data. Specifically, we simulate a \u2026"}, {"title": "TLRec: A Transfer Learning Framework to Enhance Large Language Models for Sequential Recommendation Tasks", "link": "https://dl.acm.org/doi/abs/10.1145/3640457.3691710", "details": "J Lin, S Peng, Z Zhang, P Zhao - Proceedings of the 18th ACM Conference on \u2026, 2024", "abstract": "Recently, Large Language Models (LLMs) have garnered significant attention in recommendation systems, improving recommendation performance through in- context learning or parameter-efficient fine-tuning. However, cross-domain \u2026"}]
