[{"title": "A vision\u2013language foundation model for precision oncology", "link": "https://www.nature.com/articles/s41586-024-08378-w", "details": "J Xiang, X Wang, X Zhang, Y Xi, F Eweje, Y Chen, Y Li\u2026 - Nature, 2025", "abstract": "Clinical decision-making is driven by multimodal data, including clinical notes and pathological characteristics. Artificial intelligence approaches that can effectively integrate multimodal data hold significant promise in advancing clinical care \u2026"}, {"title": "Weakly supervised multi-modal contrastive learning framework for predicting the HER2 scores in breast cancer", "link": "https://www.sciencedirect.com/science/article/pii/S0895611125000114", "details": "J Shi, D Sun, Z Jiang, J Du, W Wang, Y Zheng, H Wu - Computerized Medical Imaging \u2026, 2025", "abstract": "Human epidermal growth factor receptor 2 (HER2) is an important biomarker for prognosis and prediction of treatment response in breast cancer (BC). HER2 scoring is typically evaluated by pathologist microscopic observation on \u2026"}, {"title": "CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification", "link": "https://arxiv.org/pdf/2501.12266", "details": "C Patr\u00edcio, I Rio-Torto, JS Cardoso, LF Teixeira\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter \u2026"}, {"title": "Are Traditional Deep Learning Model Approaches as Effective as a Retinal-Specific Foundation Model for Ocular and Systemic Disease Detection?", "link": "https://arxiv.org/pdf/2501.12016", "details": "SME Yew, X Lei, JHL Goh, Y Chen, S Srinivasan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Background: RETFound, a self-supervised, retina-specific foundation model (FM), showed potential in downstream applications. However, its comparative performance with traditional deep learning (DL) models remains incompletely understood. This \u2026"}, {"title": "Focus Your Attention: Multiple Instance Learning with Attention Modification for Whole Slide Pathological Image Classification", "link": "https://ieeexplore.ieee.org/iel8/76/4358651/10838539.pdf", "details": "H Cheng, S Huang, L Cai, Y Xu, R Wang, Y Zhang - IEEE Transactions on Circuits \u2026, 2025", "abstract": "Computer-aided pathology diagnosis based on whole slide images, which is often formulated as a weakly supervised multiple instance learning (MIL) paradigm. Current approaches generally employ attention mechanisms to aggregate instance \u2026"}, {"title": "Assessing the Image Quality of Digitally Reconstructed Radiographs from Chest CT", "link": "https://link.springer.com/article/10.1007/s10278-025-01406-9", "details": "OT Paalvast, O Hertgers, M Sevenster, HJ Lamb - Journal of Imaging Informatics in \u2026, 2025", "abstract": "Rising computed tomography (CT) workloads require more efficient image interpretation methods. Digitally reconstructed radiographs (DRRs), generated from CT data, may enhance workflow efficiency by enabling faster radiological \u2026"}, {"title": "Supervision-free Vision-Language Alignment", "link": "https://arxiv.org/pdf/2501.04568%3F", "details": "G Giannone, R Li, Q Feng, E Perevodchikov, R Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) have demonstrated remarkable potential in integrating visual and linguistic information, but their performance is often constrained by the need for extensive, high-quality image-text training data. Curation \u2026"}, {"title": "Vision-Language Modeling in PET/CT for Visual Grounding of Positive Findings", "link": "https://arxiv.org/pdf/2502.00528", "details": "Z Huemann, S Church, JD Warner, D Tran, X Tie\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models can connect the text description of an object to its specific location in an image through visual grounding. This has potential applications in enhanced radiology reporting. However, these models require large annotated \u2026"}, {"title": "Valley2: Exploring Multimodal Models with Scalable Vision-Language Design", "link": "https://arxiv.org/pdf/2501.05901", "details": "Z Wu, Z Chen, R Luo, C Zhang, Y Gao, Z He, X Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, vision-language models have made remarkable progress, demonstrating outstanding capabilities in various tasks such as image captioning and video understanding. We introduce Valley2, a novel multimodal large language model \u2026"}]
