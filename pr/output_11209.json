[{"title": "Reusing routine electronic health record data for nationwide COVID-19 surveillance in nursing homes: barriers, facilitators, and lessons learned", "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11674179/", "details": "Y Wieland-Jorna, RA Verheij, AL Francke, R Coppen\u2026 - BMC Medical Informatics \u2026, 2024", "abstract": "Background At the beginning of the COVID-19 pandemic in 2020, little was known about the spread of COVID-19 in Dutch nursing homes while older people were particularly at risk of severe symptoms. Therefore, attempts were made to develop a \u2026"}, {"title": "Do Large Language Models have Shared Weaknesses in Medical Question Answering?", "link": "https://openreview.net/pdf%3Fid%3DZjQ04tsRQl", "details": "AM Bean, K Korgul, F Krones, R McCraith, A Mahdi - Advancements In Medical \u2026, 2024", "abstract": "Large language models (LLMs) have made rapid improvement on medical benchmarks, but their unreliability remains a persistent challenge for safe real-world uses. To design for the use LLMs as a category, rather than for specific models \u2026"}, {"title": "Training large language models to reason in a continuous latent space", "link": "https://arxiv.org/pdf/2412.06769%3F", "details": "S Hao, S Sukhbaatar, DJ Su, X Li, Z Hu, J Weston\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) are restricted to reason in the\" language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may \u2026"}, {"title": "POINTS1. 5: Building a Vision-Language Model towards Real World Applications", "link": "https://arxiv.org/pdf/2412.08443", "details": "Y Liu, L Tian, X Zhou, X Gao, K Yu, Y Yu, J Zhou - arXiv preprint arXiv:2412.08443, 2024", "abstract": "Vision-language models have made significant strides recently, demonstrating superior performance across a range of tasks, eg optical character recognition and complex diagram analysis. Building on this trend, we introduce a new vision \u2026"}, {"title": "iPrOp: Interactive Prompt Optimization for Large Language Models with a Human in the Loop", "link": "https://arxiv.org/pdf/2412.12644", "details": "J Li, R Klinger - arXiv preprint arXiv:2412.12644, 2024", "abstract": "Prompt engineering has made significant contributions to the era of large language models, yet its effectiveness depends on the skills of a prompt author. Automatic prompt optimization can support the prompt development process, but requires \u2026"}, {"title": "Adaptive Pruning for Large Language Models with Structural Importance Awareness", "link": "https://arxiv.org/pdf/2412.15127", "details": "H Zheng, J Ren, Y Sun, R Zhang, W Zhang, Z Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The recent advancements in large language models (LLMs) have significantly improved language understanding and generation capabilities. However, it is difficult to deploy LLMs on resource-constrained edge devices due to their high \u2026"}, {"title": "Eliciting Causal Abilities in Large Language Models for Reasoning Tasks", "link": "https://arxiv.org/pdf/2412.15314", "details": "Y Wang, Z Luo, J Wang, Z Zhou, Y Chen, B Han - arXiv preprint arXiv:2412.15314, 2024", "abstract": "Prompt optimization automatically refines prompting expressions, unlocking the full potential of LLMs in downstream tasks. However, current prompt optimization methods are costly to train and lack sufficient interpretability. This paper proposes \u2026"}, {"title": "Towards a Speech Foundation Model for Singapore and Beyond", "link": "https://arxiv.org/pdf/2412.11538", "details": "M Huzaifah, T Liu, HB Sailor, KM Tan, TK Vangani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This technical report describes the MERaLiON Speech Encoder, a foundation model designed to support a wide range of downstream speech applications. Developed as part of Singapore's National Multimodal Large Language Model Programme, the \u2026"}]
