[{"title": "Development of Numerical Error Detection Tasks to Analyze the Numerical Capabilities of Language Models", "link": "https://aclanthology.org/2025.coling-main.666.pdf", "details": "T Sakamoto, S Sugawara, A Aizawa - \u2026 of the 31st International Conference on \u2026, 2025", "abstract": "Numbers are used to describe quantities in various scenarios in daily life; therefore, numerical errors can significantly affect the meaning of the entire sentence, and even a single-letter error can be fatal. Detecting numerical errors often requires a high \u2026"}, {"title": "Enhancing Factual Consistency in Text Summarization via Counterfactual Debiasing", "link": "https://aclanthology.org/2025.coling-main.530.pdf", "details": "Z Ling, Y Xie, C Dong, Y Shen - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Despite significant progress in abstractive text summarization aimed at generating fluent and informative outputs, how to ensure the factual consistency of generated summaries remains a crucial and challenging issue. In this study, drawing inspiration \u2026"}, {"title": "Incorporating Review-missing Interactions for Generative Explainable Recommendation", "link": "https://aclanthology.org/2025.coling-main.527.pdf", "details": "X Li, X Bo, C Ma, X Chen - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Explainable recommendation has attracted much attention from the academic and industry communities. Traditional models usually leverage user reviews as ground truths for model training, and the interactions without reviews are totally ignored \u2026"}, {"title": "Conditional Semantic Textual Similarity via Conditional Contrastive Learning", "link": "https://aclanthology.org/2025.coling-main.306.pdf", "details": "X Liu, Z Qin, Z Wang, W Liang, L Zong, B Xu - \u2026 of the 31st International Conference on \u2026, 2025", "abstract": "Conditional semantic textual similarity (C-STS) assesses the similarity between pairs of sentence representations under different conditions. The current method encounters the over-estimation issue of positive and negative samples. Specifically \u2026"}, {"title": "META-LORA: Memory-Efficient Sample Reweighting for Fine-Tuning Large Language Models", "link": "https://aclanthology.org/2025.coling-main.568.pdf", "details": "W Li, L Zou, M Tang, Q Yu, W Li, C Li - \u2026 of the 31st International Conference on \u2026, 2025", "abstract": "Supervised fine-tuning (SFT) is widely adopted for tailoring large language models (LLMs) to specific downstream tasks. However, the substantial computational demands of LLMs hinder iterative exploration of fine-tuning datasets and accurate \u2026"}]
