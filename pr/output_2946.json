[{"title": "Knowledge Editing in Language Models via Adapted Direct Preference Optimization", "link": "https://arxiv.org/pdf/2406.09920", "details": "A Rozner, B Battash, L Wolf, O Lindenbaum - arXiv preprint arXiv:2406.09920, 2024", "abstract": "Large Language Models (LLMs) can become outdated over time as they may lack updated world knowledge, leading to factual knowledge errors and gaps. Knowledge Editing (KE) aims to overcome this challenge using weight updates that do not \u2026"}, {"title": "Proxy-based robust deep metric learning in the presence of label noise", "link": "https://iopscience.iop.org/article/10.1088/1402-4896/ad5255/meta", "details": "FM Neamah, HS Aghdasi, P Salehpour, AS Sorkhabi - Physica Scripta, 2024", "abstract": "Real-world datasets contain label noise data that can deteriorate the performance of a deep learning model. Cleaning annotations manually requires substantial efforts from experts and is not practical in large datasets. Therefore, many methods are \u2026"}, {"title": "Few-Shot Learning for Medical Image Segmentation Using 3D U-Net and Model-Agnostic Meta-Learning (MAML)", "link": "https://www.mdpi.com/2075-4418/14/12/1213", "details": "AM Alsaleh, E Albalawi, A Algosaibi, SS Albakheet\u2026 - Diagnostics, 2024", "abstract": "Deep learning has attained state-of-the-art results in general image segmentation problems; however, it requires a substantial number of annotated images to achieve the desired outcomes. In the medical field, the availability of annotated images is \u2026"}, {"title": "GAugLLM: Improving Graph Contrastive Learning for Text-Attributed Graphs with Large Language Models", "link": "https://arxiv.org/abs/2406.11945", "details": "Y Fang, D Fan, D Zha, Q Tan - arXiv preprint arXiv:2406.11945, 2024", "abstract": "This work studies self-supervised graph learning for text-attributed graphs (TAGs) where nodes are represented by textual attributes. Unlike traditional graph contrastive methods that perturb the numerical feature space and alter the graph's \u2026"}, {"title": "Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2406.10630", "details": "R Ye, J Chai, X Liu, Y Yang, Y Wang, S Chen - arXiv preprint arXiv:2406.10630, 2024", "abstract": "Federated learning (FL) enables multiple parties to collaboratively fine-tune an large language model (LLM) without the need of direct data sharing. Ideally, by training on decentralized data that is aligned with human preferences and safety principles \u2026"}, {"title": "Exploring Adversarial Robustness of Deep State Space Models", "link": "https://arxiv.org/pdf/2406.05532", "details": "B Qi, Y Luo, J Gao, P Li, K Tian, Z Ma, B Zhou - arXiv preprint arXiv:2406.05532, 2024", "abstract": "Deep State Space Models (SSMs) have proven effective in numerous task scenarios but face significant security challenges due to Adversarial Perturbations (APs) in real- world deployments. Adversarial Training (AT) is a mainstream approach to \u2026"}, {"title": "Probabilistic Perspectives on Error Minimization in Adversarial Reinforcement Learning", "link": "https://arxiv.org/pdf/2406.04724", "details": "R Belaire, A Sinha, P Varakantham - arXiv preprint arXiv:2406.04724, 2024", "abstract": "Deep Reinforcement Learning (DRL) policies are critically vulnerable to adversarial noise in observations, posing severe risks in safety-critical scenarios. For example, a self-driving car receiving manipulated sensory inputs about traffic signs could lead to \u2026"}, {"title": "Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning", "link": "https://openreview.net/pdf%3Fid%3DXwnABAdH5y", "details": "Z Zhang, Y Shi, J Zhu, W Zhou, X Qi, H Li - Forty-first International Conference on Machine \u2026", "abstract": "Trustworthiness is an essential prerequisite for the real-world application of large language models. In this paper, we focus on the trustworthiness of language models with respect to retrieval augmentation. Despite being supported with external \u2026"}, {"title": "Black Box Differential Privacy Auditing Using Total Variation Distance", "link": "https://arxiv.org/pdf/2406.04827", "details": "A Koskela, J Mohammadi - arXiv preprint arXiv:2406.04827, 2024", "abstract": "We present a practical method to audit the differential privacy (DP) guarantees of a machine learning model using a small hold-out dataset that is not exposed to the model during the training. Having a score function such as the loss function \u2026"}]
