[{"title": "Contrastive Learning for Clinical Outcome Prediction with Partial Data Sources", "link": "https://openreview.net/pdf%3Fid%3DelCOPIm4Xw", "details": "M Xia, J Wilson, B Goldstein, R Henao - Forty-first International Conference on Machine \u2026", "abstract": "The use of machine learning models to predict clinical outcomes from (longitudinal) electronic health record (EHR) data is becoming increasingly popular due to advances in deep architectures, representation learning, and the growing availability \u2026"}, {"title": "Assessing the Reliability of Artificial Intelligence Systems: Challenges, Metrics, and Future Directions", "link": "https://ijimes.ir/index.php/ijimes/article/view/133", "details": "STH Mortaji, ME Sadeghi - International Journal of Innovation in Management \u2026, 2024", "abstract": "Purpose: As artificial intelligence (AI) systems become integral to diverse applications, ensuring their reliability is of paramount importance. This paper explores the multifaceted landscape of AI reliability, encompassing challenges \u2026"}, {"title": "From Basic to Extra Features: Hypergraph Transformer Pretrain-then-Finetuning for Balanced Clinical Predictions on EHR", "link": "https://www.cs.emory.edu/~jyang71/files/hypehr-pf.pdf", "details": "R Xu, Y Lu, C Liu, Y Chen, Y Sun, X Hu, JC Ho, C Yang - Proceedings of Machine \u2026, 2024", "abstract": "Abstract Electronic Health Records (EHRs) contain rich patient information and are crucial for clinical research and practice. In recent years, deep learning models have been applied to EHRs, but they often rely on massive features, which may not be \u2026"}, {"title": "Revisiting Pre-training of Embedding Layers in Transformer-based Neural Machine Translation", "link": "https://www.jstage.jst.go.jp/article/jnlp/31/2/31_534/_pdf", "details": "M Neishi, N Yoshinaga - Journal of Natural Language Processing, 2024", "abstract": "Recent trends in the pre-training and fine-tuning paradigm have made significant advances in several natural language processing tasks, including machine translation (MT), particularly for low-resource situations. However, it is reported that \u2026"}, {"title": "Pre-training Cross-Modal Retrieval by Expansive Lexicon-Patch Alignment", "link": "https://aclanthology.org/2024.lrec-main.1136.pdf", "details": "Y Yiyuan, G Long, M Blumenstein, X Geng, C Tao\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Recent large-scale vision-language pre-training depends on image-text global alignment by contrastive learning and is further boosted by fine-grained alignment in a weakly contrastive manner for cross-modal retrieval. Nonetheless, besides \u2026"}, {"title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation", "link": "https://arxiv.org/pdf/2405.13769", "details": "C Chhun, FM Suchanek, C Clavel - arXiv preprint arXiv:2405.13769, 2024", "abstract": "Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high \u2026"}, {"title": "Merlin: A Vision Language Foundation Model for 3D Computed Tomography", "link": "https://arxiv.org/pdf/2406.06512", "details": "L Blankemeier, JP Cohen, A Kumar, D Van Veen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Over 85 million computed tomography (CT) scans are performed annually in the US, of which approximately one quarter focus on the abdomen. Given the current radiologist shortage, there is a large impetus to use artificial intelligence to alleviate \u2026"}, {"title": "Towards a Personal Health Large Language Model", "link": "https://arxiv.org/pdf/2406.06474", "details": "J Cosentino, A Belyaeva, X Liu, NA Furlotte, Z Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In health, most large language model (LLM) research has focused on clinical tasks. However, mobile and wearable devices, which are rarely integrated into such tasks, provide rich, longitudinal data for personal health monitoring. Here we present \u2026"}, {"title": "Teaching-Assistant-in-the-Loop: Improving Knowledge Distillation from Imperfect Teacher Models in Low-Budget Scenarios", "link": "https://arxiv.org/pdf/2406.05322", "details": "Y Zhou, W Ai - arXiv preprint arXiv:2406.05322, 2024", "abstract": "There is increasing interest in distilling task-specific knowledge from large language models (LLM) to smaller student models. Nonetheless, LLM distillation presents a dual challenge: 1) there is a high cost associated with querying the teacher LLM \u2026"}]
