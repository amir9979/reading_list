[{"title": "Context-DPO: Aligning Language Models for Context-Faithfulness", "link": "https://arxiv.org/pdf/2412.15280", "details": "B Bi, S Huang, Y Wang, T Yang, Z Zhang, H Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Reliable responses from large language models (LLMs) require adherence to user instructions and retrieved information. While alignment techniques help LLMs align with human intentions and values, improving context-faithfulness through alignment \u2026"}, {"title": "EscapeBench: Pushing Language Models to Think Outside the Box", "link": "https://arxiv.org/pdf/2412.13549", "details": "C Qian, P Han, Q Luo, B He, X Chen, Y Zhang, H Du\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language model agents excel in long-session planning and reasoning, but existing benchmarks primarily focus on goal-oriented tasks with explicit objectives, neglecting creative adaptation in unfamiliar environments. To address this, we introduce \u2026"}, {"title": "CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries", "link": "https://arxiv.org/pdf/2501.01282", "details": "S Liu, Y Jin, C Li, DF Wong, Q Wen, L Sun, H Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) have advanced human-AI interaction but struggle with cultural understanding, often misinterpreting symbols, gestures, and artifacts due to biases in predominantly Western-centric training data. In this paper, we \u2026"}, {"title": "UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models", "link": "https://arxiv.org/pdf/2412.11803", "details": "B Xue, F Mi, Q Zhu, H Wang, R Wang, S Wang, E Yu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite demonstrating impressive capabilities, Large Language Models (LLMs) still often struggle to accurately express the factual knowledge they possess, especially in cases where the LLMs' knowledge boundaries are ambiguous. To improve LLMs' \u2026"}, {"title": "Unveiling the power of language models in chemical research question answering", "link": "https://www.nature.com/articles/s42004-024-01394-x", "details": "X Chen, T Wang, T Guo, K Guo, J Zhou, H Li, Z Song\u2026 - Communications Chemistry, 2025", "abstract": "While the abilities of language models are thoroughly evaluated in areas like general domains and biomedicine, academic chemistry remains less explored. Chemical QA tools also play a crucial role in both education and research by effectively translating \u2026"}, {"title": "CoReQA: Uncovering Potentials of Language Models in Code Repository Question Answering", "link": "https://arxiv.org/pdf/2501.03447", "details": "J Chen, K Zhao, J Liu, C Peng, J Liu, H Zhu, P Gao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models that enhance software development tasks, such as code generation, code completion, and code question answering (QA), have been extensively studied in both academia and the industry. The models are integrated \u2026"}, {"title": "A MapReduce Approach to Effectively Utilize Long Context Information in Retrieval Augmented Language Models", "link": "https://arxiv.org/pdf/2412.15271", "details": "G Zhang, Z Xu, Q Jin, F Chen, Y Fang, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While holding great promise for improving and facilitating healthcare, large language models (LLMs) struggle to produce up-to-date responses on evolving topics due to outdated knowledge or hallucination. Retrieval-augmented generation (RAG) is a \u2026"}, {"title": "Incorporating structural knowledge into language models for open knowledge graph completion", "link": "https://link.springer.com/article/10.1007/s11280-024-01314-y", "details": "X Song, Y Wang, B Zhou, H Wang, Y Huang, L Gao - World Wide Web, 2025", "abstract": "Abstract Knowledge graphs have been proven to be valuable and practical in many AI-supported applications, such as e-commerce recommendations and legal consultation. Our collective knowledge keeps growing so that KGs are generally \u2026"}, {"title": "Benchmarking and Improving Large Vision-Language Models for Fundamental Visual Graph Understanding and Reasoning", "link": "https://arxiv.org/pdf/2412.13540", "details": "Y Zhu, X Bai, K Chen, Y Xiang, M Zhang - arXiv preprint arXiv:2412.13540, 2024", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance across diverse tasks. Despite great success, recent studies show that LVLMs encounter substantial limitations when engaging with visual graphs. To study \u2026"}]
