"*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Predicting Risk of Alzheimer's Diseases and Related De"
[{"title": "Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation", "link": "https://arxiv.org/pdf/2404.17489", "details": "W Cui, R Hosseinzadeh, J Ma, T Wu, Y Sui, K Golestan - arXiv preprint arXiv \u2026, 2024", "abstract": "Contrastive learning is a model pre-training technique by first creating similar views of the original data, and then encouraging the data and its corresponding views to be close in the embedding space. Contrastive learning has witnessed success in image \u2026"}, {"title": "Pre-training enhanced unsupervised contrastive domain adaptation for industrial equipment remaining useful life prediction", "link": "https://www.sciencedirect.com/science/article/pii/S1474034624001654", "details": "H Li, P Cao, X Wang, Y Li, B Yi, M Huang - Advanced Engineering Informatics, 2024", "abstract": "An essential task in industrial intelligence is to accurately predict the remaining useful life (RUL) of industrial equipment, and there has been tremendous progress in RUL prediction based on data-driven methods. However, these methods rely heavily \u2026"}, {"title": "Mammo-CLIP: Leveraging Contrastive Language-Image Pre-training (CLIP) for Enhanced Breast Cancer Diagnosis with Multi-view Mammography", "link": "https://arxiv.org/pdf/2404.15946", "details": "X Chen, Y Li, M Hu, E Salari, X Chen, RLJ Qiu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Although fusion of information from multiple views of mammograms plays an important role to increase accuracy of breast cancer detection, developing multi-view mammograms-based computer-aided diagnosis (CAD) schemes still faces \u2026"}, {"title": "FedEval-LLM: Federated Evaluation of Large Language Models on Downstream Tasks with Collective Wisdom", "link": "https://arxiv.org/pdf/2404.12273", "details": "Y He, Y Kang, L Fan, Q Yang - arXiv preprint arXiv:2404.12273, 2024", "abstract": "Federated Learning (FL) has emerged as a promising solution for collaborative training of large language models (LLMs). However, the integration of LLMs into FL introduces new challenges, particularly concerning the evaluation of LLMs \u2026"}, {"title": "FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS By admin No Comments", "link": "https://youraisales.com/finetuned-language-models-are-zero-shot-learners-2/", "details": "J Wei, M Bosma, VY Zhao, K Guu, AW Yu, B Lester\u2026", "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning\u2014\ufb01netuning language models on a collection of datasets described via instructions\u2014substantially improves zero-shot \u2026"}, {"title": "Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning", "link": "https://arxiv.org/pdf/2404.09491", "details": "S Han, J Yoon, SO Arik, T Pfister - arXiv preprint arXiv:2404.09491, 2024", "abstract": "Large Language Models (LLMs), with their remarkable ability to tackle challenging and unseen reasoning problems, hold immense potential for tabular learning, that is vital for many real-world applications. In this paper, we propose a novel in-context \u2026"}, {"title": "MetaMedSeg: Volumetric Meta-learning for Few-Shot Organ Segmentation", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-16852-9.pdf%23page%3D56", "details": "N Navab - Domain Adaptation and Representation Transfer", "abstract": "The lack of sufficient annotated image data is a common issue in medical image segmentation. For some organs and densities, the annotation may be scarce, leading to poor model training convergence, while other organs have plenty of \u2026"}, {"title": "Diluie: constructing diverse demonstrations of in-context learning with large language model for unified information extraction", "link": "https://link.springer.com/article/10.1007/s00521-024-09728-5", "details": "Q Guo, Y Guo, J Zhao - Neural Computing and Applications, 2024", "abstract": "Large language models (LLMs) have demonstrated promising in-context learning capabilities, especially with instructive prompts. However, recent studies have shown that existing large models still face challenges in specific information extraction (IE) \u2026"}]
