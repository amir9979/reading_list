[{"title": "Exploring the Impact of Backbone Architecture on Explainable CNNs' Interpretability", "link": "https://www.researchgate.net/profile/Zalan-Bodo/publication/384925487_Exploring_the_Impact_of_Backbone_Architecture_on_Explainable_CNNs%27_Interpretability/links/670e3ccf77bab74415a19534/Exploring-the-Impact-of-Backbone-Architecture-on-Explainable-CNNs-Interpretability.pdf", "details": "\u00c1 PORTIK, A BAJCSI, A SZENKOVITS, Z BOD\u00d3 - Acta Univ. Sapientiae, 2024", "abstract": "The growing demand for interpretable models in machine learning underscores the importance of transparency in decision-making processes for building trust and ensuring accountability in AI systems. Unlike complex black-box models \u2026"}, {"title": "Counterfactual Generative Modeling with Variational Causal Inference", "link": "https://arxiv.org/pdf/2410.12730", "details": "Y Wu, L McConnell, C Iriondo - arXiv preprint arXiv:2410.12730, 2024", "abstract": "Estimating an individual's potential outcomes under counterfactual treatments is a challenging task for traditional causal inference and supervised learning approaches when the outcome is high-dimensional (eg gene expressions, facial images) and \u2026"}, {"title": "Diffusion Models Need Visual Priors for Image Generation", "link": "https://arxiv.org/pdf/2410.08531", "details": "X Yue, Z Wang, Z Lu, S Sun, M Wei, W Ouyang, L Bai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Conventional class-guided diffusion models generally succeed in generating images with correct semantic content, but often struggle with texture details. This limitation stems from the usage of class priors, which only provide coarse and limited \u2026"}, {"title": "Variational Diffusion Unlearning: a variational inference framework for unlearning in diffusion models", "link": "https://openreview.net/pdf%3Fid%3DB2wDjiED9V", "details": "S Panda, MS Varun, S Jain, SK Maharana\u2026 - Neurips Safe Generative AI \u2026", "abstract": "For responsible and safe deployment of diffusion models in various domains, regulating the generated outputs from these models is desirable because such models could generate undesired violent and obscene outputs. To tackle this \u2026"}, {"title": "Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?", "link": "https://arxiv.org/pdf/2410.13523", "details": "C Liu, Z Wan, H Wang, Y Chen, T Qaiser, C Jin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical Vision-Language Pre-training (MedVLP) has made significant progress in enabling zero-shot tasks for medical image understanding. However, training MedVLP models typically requires large-scale datasets with paired, high-quality \u2026"}, {"title": "Intermediate Representations for Enhanced Text-To-Image Generation Using Diffusion Models", "link": "https://arxiv.org/pdf/2410.09792", "details": "R Galun, S Benaim - arXiv preprint arXiv:2410.09792, 2024", "abstract": "Text-to-image diffusion models have demonstrated an impressive ability to produce high-quality outputs. However, they often struggle to accurately follow fine-grained spatial information in an input text. To this end, we propose a compositional \u2026"}, {"title": "Self-supervised contrastive learning performs non-linear system identification", "link": "https://arxiv.org/pdf/2410.14673%3F", "details": "RG Laiz, T Schmidt, S Schneider - arXiv preprint arXiv:2410.14673, 2024", "abstract": "Self-supervised learning (SSL) approaches have brought tremendous success across many tasks and domains. It has been argued that these successes can be attributed to a link between SSL and identifiable representation learning: Temporal \u2026"}, {"title": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation", "link": "https://arxiv.org/pdf/2410.08895", "details": "K Ding, Q Yu, H Zhang, G Meng, S Xiang - arXiv preprint arXiv:2410.08895, 2024", "abstract": "Cache-based approaches stand out as both effective and efficient for adapting vision- language models (VLMs). Nonetheless, the existing cache model overlooks three crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text similarity \u2026"}, {"title": "Classification Done Right for Vision-Language Pre-Training", "link": "https://arxiv.org/pdf/2411.03313", "details": "H Zilong, Y Qinghao, K Bingyi, F Jiashi, F Haoqi - arXiv preprint arXiv:2411.03313, 2024", "abstract": "We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data. Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised \u2026"}]
