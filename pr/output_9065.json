[{"title": "InterPLM: Discovering Interpretable Features in Protein Language Models via Sparse Autoencoders", "link": "https://www.biorxiv.org/content/biorxiv/early/2024/11/15/2024.11.14.623630.full.pdf", "details": "E Simon, J Zou - bioRxiv, 2024", "abstract": "Protein language models (PLMs) have demonstrated remarkable success in protein modeling and design, yet their internal mechanisms for predicting structure and function remain poorly understood. Here we present a systematic approach to extract \u2026"}, {"title": "Why These Documents? Explainable Generative Retrieval with Hierarchical Category Paths", "link": "https://arxiv.org/pdf/2411.05572", "details": "S Lee, R Heo, SK Kang, S Yoon, J Yeo, D Lee - arXiv preprint arXiv:2411.05572, 2024", "abstract": "Generative retrieval has recently emerged as a new alternative of traditional information retrieval approaches. However, existing generative retrieval methods directly decode docid when a query is given, making it impossible to provide users \u2026"}, {"title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise Reinforcement Learning", "link": "https://arxiv.org/pdf/2411.03817%3F", "details": "Z Deng, Z Dou, Y Zhu, JR Wen, R Xiong, M Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The outstanding capabilities of large language models (LLMs) render them a crucial component in various autonomous agent systems. While traditional methods depend on the inherent knowledge of LLMs without fine-tuning, more recent approaches \u2026"}, {"title": "Analysing the Residual Stream of Language Models Under Knowledge Conflicts", "link": "https://arxiv.org/pdf/2410.16090", "details": "Y Zhao, X Du, G Hong, AP Gema, A Devoto, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model \u2026"}, {"title": "Reducing Distraction in Long-Context Language Models by Focused Learning", "link": "https://arxiv.org/pdf/2411.05928", "details": "Z Wu, B Liu, R Yan, L Chen, T Delteil - arXiv preprint arXiv:2411.05928, 2024", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced their capacity to process long contexts. However, effectively utilizing this long context remains a challenge due to the issue of distraction, where irrelevant \u2026"}, {"title": "Group Robust Best-of-K Decoding of Language Models for Pluralistic Alignment", "link": "https://openreview.net/pdf%3Fid%3DJI6j4NUGHv", "details": "S Yoon, W Bankes, S Son, A Petrovic, SS Ramesh\u2026 - Pluralistic Alignment Workshop at \u2026", "abstract": "The desirable behaviour of a chat agent can be described with multiple criteria, such as harmlessness, helpfulness, and conciseness, each of which can be scored by a reward model. While each user, or a group of users, may perceive each criterion with \u2026"}, {"title": "Trojan Activation Attack: Red-Teaming Large Language Models using Steering Vectors for Safety-Alignment", "link": "https://dl.acm.org/doi/pdf/10.1145/3627673.3679821", "details": "H Wang, K Shu - Proceedings of the 33rd ACM International Conference \u2026, 2024", "abstract": "To ensure AI safety, instruction-tuned Large Language Models (LLMs) are specifically trained to ensure alignment, which refers to making models behave in accordance with human intentions. While these models have demonstrated \u2026"}, {"title": "Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models", "link": "https://arxiv.org/pdf/2411.00492%3F", "details": "DX Long, DN Yen, AT Luu, K Kawaguchi, MY Kan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple \u2026"}]
