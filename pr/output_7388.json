[{"title": "Learning to discriminate by learning to generate: zero-shot generative models increase human object recognition alignment", "link": "https://jov.arvojournals.org/article.aspx%3Farticleid%3D2801488", "details": "R Geirhos, K Clark, P Jaini - Journal of Vision, 2024", "abstract": "How does the human visual system recognize objects---through discriminative inference (fast but potentially unreliable) or using a generative model of the world (slow but potentially more robust)? The question of how the brain combines the best \u2026"}, {"title": "E3M: Zero-Shot Spatio-Temporal Video Grounding with Expectation-Maximization Multimodal Modulation", "link": "https://baopj.github.io/files/ECCV24_E3M_ZeroSTVG.pdf", "details": "P Bao, Z Shao, W Yang, BP Ng, AC Kot - 2024", "abstract": "Spatio-temporal video grounding aims to localize the spatiotemporal tube in a video according to the given language query. To eliminate the annotation costs, we make a first exploration to tackle spatiotemporal video grounding in a zero-shot manner. Our \u2026"}, {"title": "MMPT: Multimodal Prompt Tuning for Zero-shot Instruction Learning", "link": "https://arxiv.org/pdf/2409.15657", "details": "T Wang, Y Liu, JC Liang, Y Cui, Y Mao, S Nie, J Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal Large Language Models (MLLMs) demonstrate remarkable performance across a wide range of domains, with increasing emphasis on enhancing their zero- shot generalization capabilities for unseen tasks across various modalities \u2026"}]
