[{"title": "Improving Generalization in Offline Reinforcement Learning via Adversarial Data Splitting", "link": "https://openreview.net/pdf%3Fid%3DCV9PiQGt0i", "details": "D Wang, L Li, W Wei, Q Yu, HAO Jianye, J Liang - Forty-first International Conference on \u2026", "abstract": "Offline Reinforcement Learning (RL) commonly suffers from the out-of-distribution (OOD) overestimation issue due to the distribution shift. Prior work gradually shifts their focus from suppressing OOD overestimation to avoiding overly conservative \u2026"}, {"title": "LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices", "link": "https://arxiv.org/pdf/2406.08765", "details": "R Jin, Q Xu, M Wu, Y Xu, D Li, X Li, Z Chen - arXiv preprint arXiv:2406.08765, 2024", "abstract": "Limited by the scale and diversity of time series data, the neural networks trained on time series data often overfit and show unsatisfacotry performances. In comparison, large language models (LLMs) recently exhibit impressive generalization in diverse \u2026"}, {"title": "Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation", "link": "https://arxiv.org/pdf/2406.06890", "details": "Y Zhai, K Lin, Z Yang, L Li, J Wang, CC Lin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Image diffusion distillation achieves high-fidelity generation with very few sampling steps. However, applying these techniques directly to video diffusion often results in unsatisfactory frame quality due to the limited visual quality in public video datasets \u2026"}]
