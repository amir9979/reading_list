[{"title": "Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models: Enhancing Performance and Reducing Inference Costs", "link": "https://arxiv.org/pdf/2407.00945", "details": "E Liu, J Zhu, Z Lin, X Ning, MB Blaschko, S Yan, G Dai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancement of large language models (LLMs) has led to architectures with billions to trillions of parameters, posing significant deployment challenges due to their substantial demands on memory, processing power, and energy \u2026"}, {"title": "Lifelong Robot Library Learning: Bootstrapping Composable and Generalizable Skills for Embodied Control with Language Models", "link": "https://arxiv.org/pdf/2406.18746", "details": "G Tziafas, H Kasaei - arXiv preprint arXiv:2406.18746, 2024", "abstract": "Large Language Models (LLMs) have emerged as a new paradigm for embodied reasoning and control, most recently by generating robot policy code that utilizes a custom library of vision and control primitive skills. However, prior arts fix their skills \u2026"}, {"title": "Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment", "link": "https://arxiv.org/pdf/2407.03051", "details": "J Lee, S Park, S Hong, M Kim, DS Chang, J Choi - arXiv preprint arXiv:2407.03051, 2024", "abstract": "The rapid advancement of large language models (LLMs) has facilitated their transformation into conversational chatbots that can grasp contextual nuances and generate pertinent sentences, closely mirroring human values through advanced \u2026"}, {"title": "Unlocking Continual Learning Abilities in Language Models", "link": "https://arxiv.org/pdf/2406.17245", "details": "W Du, S Cheng, T Luo, Z Qiu, Z Huang, KC Cheung\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) exhibit impressive performance and generalization capabilities. However, LMs struggle with the persistent challenge of catastrophic forgetting, which undermines their long-term sustainability in continual learning (CL) \u2026"}, {"title": "Advancing Cross-domain Discriminability in Continual Learning of Vison-Language Models", "link": "https://arxiv.org/pdf/2406.18868", "details": "Y Xu, Y Chen, J Nie, Y Wang, H Zhuang, M Okumura - arXiv preprint arXiv \u2026, 2024", "abstract": "Continual learning (CL) with Vision-Language Models (VLMs) has overcome the constraints of traditional CL, which only focuses on previously encountered classes. During the CL of VLMs, we need not only to prevent the catastrophic forgetting on \u2026"}, {"title": "Collaborative Performance Prediction for Large Language Models", "link": "https://arxiv.org/pdf/2407.01300", "details": "Q Zhang, F Lyu, X Liu, C Ma - arXiv preprint arXiv:2407.01300, 2024", "abstract": "Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works \u2026"}, {"title": "Universal Approximation Theory: The basic theory for large language models", "link": "https://arxiv.org/pdf/2407.00958", "details": "W Wang, Q Li - arXiv preprint arXiv:2407.00958, 2024", "abstract": "Language models have emerged as a critical area of focus in artificial intelligence, particularly with the introduction of groundbreaking innovations like ChatGPT. Large- scale Transformer networks have quickly become the leading approach for \u2026"}, {"title": "Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement", "link": "https://arxiv.org/pdf/2407.01461", "details": "Z Huang, X Wang, F Zhang, Z Xu, C Zhang, X Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The capacity of large language models (LLMs) to generate honest, harmless, and helpful responses heavily relies on the quality of user prompts. However, these prompts often tend to be brief and vague, thereby significantly limiting the full \u2026"}, {"title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs", "link": "https://arxiv.org/pdf/2407.00653", "details": "Y Zhang, X Wang, J Liang, S Xia, L Chen, Y Xiao - arXiv preprint arXiv:2407.00653, 2024", "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new \u2026"}]
