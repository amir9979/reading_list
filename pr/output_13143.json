[{"title": "Modular Prompt Learning Improves Vision-Language Models", "link": "https://arxiv.org/pdf/2502.14125", "details": "Z Huang, T Pedapati, PY Chen, J Gao - arXiv preprint arXiv:2502.14125, 2025", "abstract": "Pre-trained vision-language models are able to interpret visual concepts and language semantics. Prompt learning, a method of constructing prompts for text encoders or image encoders, elicits the potentials of pre-trained models and readily \u2026"}, {"title": "DCFormer: Efficient 3D Vision-Language Modeling with Decomposed Convolutions", "link": "https://arxiv.org/pdf/2502.05091%3F", "details": "GC Ates, K Gong, W Shao - arXiv preprint arXiv:2502.05091, 2025", "abstract": "Vision-language models (VLMs) align visual and textual representations, enabling high-performance zero-shot classification and image-text retrieval in 2D medical imaging. However, extending VLMs to 3D medical imaging remains computationally \u2026"}, {"title": "Swarm learning with weak supervision enables automatic breast cancer detection in magnetic resonance imaging", "link": "https://www.nature.com/articles/s43856-024-00722-5", "details": "OL Saldanha, J Zhu, G M\u00fcller-Franzes, ZI Carrero\u2026 - Communications Medicine, 2025", "abstract": "Background Over the next 5 years, new breast cancer screening guidelines recommending magnetic resonance imaging (MRI) for certain patients will significantly increase the volume of imaging data to be analyzed. While this increase \u2026"}, {"title": "EyeBench: A Call for More Rigorous Evaluation of Retinal Image Enhancement", "link": "https://arxiv.org/pdf/2502.14260", "details": "W Zhu, X Dong, X Li, Y Xiong, X Chen, P Qiu, VK Vasa\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Over the past decade, generative models have achieved significant success in enhancement fundus images. However, the evaluation of these models still presents a considerable challenge. A comprehensive evaluation benchmark for fundus image \u2026"}, {"title": "Bootstrapping Vision-Language Models for Frequency-Centric Self-Supervised Remote Physiological Measurement", "link": "https://link.springer.com/article/10.1007/s11263-025-02388-5", "details": "Z Yue, M Shi, H Wang, S Ding, Q Chen, S Yang - International Journal of Computer \u2026, 2025", "abstract": "Facial video-based remote physiological measurement is a promising research area for detecting human vital signs (eg, heart rate, respiration frequency) in a non-contact way. Conventional approaches are mostly supervised learning, requiring extensive \u2026"}, {"title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models", "link": "https://arxiv.org/pdf/2502.10250", "details": "GK Kumar, I Chaabane, K Wu - arXiv preprint arXiv:2502.10250, 2025", "abstract": "Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved \u2026"}, {"title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features", "link": "https://arxiv.org/pdf/2502.14786", "details": "M Tschannen, A Gritsenko, X Wang, MF Naeem\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed \u2026"}, {"title": "PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery", "link": "https://arxiv.org/pdf/2502.12594", "details": "B He, L Yin, HL Zhen, X Zhang, M Yuan, C Ma - arXiv preprint arXiv:2502.12594, 2025", "abstract": "Model pruning is an effective approach for compressing large language models. However, this process often leads to significant degradation of model capabilities. While post-training techniques such as instruction tuning are commonly employed to \u2026"}, {"title": "EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models", "link": "https://arxiv.org/pdf/2502.06663", "details": "X Xing, Z Liu, S Xiao, B Gao, Y Liang, W Zhang, H Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Modern large language models (LLMs) driven by scaling laws, achieve intelligence emergency in large model sizes. Recently, the increasing concerns about cloud costs, latency, and privacy make it an urgent requirement to develop compact edge \u2026"}]
