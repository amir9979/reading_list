[{"title": "How Well Can Vision Language Models See Image Details?", "link": "https://arxiv.org/pdf/2408.03940", "details": "C Gou, A Felemban, FF Khan, D Zhu, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Model-based Vision-Language Models (LLM-based VLMs) have demonstrated impressive results in various vision-language understanding tasks. However, how well these VLMs can see image detail beyond the semantic level \u2026"}, {"title": "XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models", "link": "https://aclanthology.org/2024.bionlp-1.35/", "details": "OC Thawakar, AM Shaker, SS Mullappilly, H Cholakkal\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "The latest breakthroughs in large language models (LLMs) and vision-language models (VLMs) have showcased promising capabilities toward performing a wide range of tasks. Such models are typically trained on massive datasets comprising \u2026"}, {"title": "MindLLM: Lightweight large language model pre-training, evaluation and domain application", "link": "https://www.sciencedirect.com/science/article/pii/S2666651024000111", "details": "Y Yang, H Sun, J Li, R Liu, Y Li, Y Liu, Y Gao, H Huang - AI Open, 2024", "abstract": "Abstract Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is \u2026"}]
