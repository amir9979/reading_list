[{"title": "**Evaluating** the Intelligence of **large language models** : A comparative study using verbal and visual IQ tests", "link": "https://www.sciencedirect.com/science/article/pii/S2949882125000544", "details": "S Abdelkarim, D Lu, DL Flores, S Jaeggi, P Baldi - Computers in Human Behavior \u2026, 2025", "abstract": "**Large** **language** **models** (LLMs) excel on many specialised benchmarks, yet their general-reasoning ability remains opaque. We therefore test 18 models\u2014including GPT-4, Claude 3 and Gemini Pro\u2014on a 14-section IQ suite spanning verbal \u2026"}, {"title": "Assessing the power of AI: a comparative **evaluation** of **large language models** in generating patient education materials in dentistry", "link": "https://www.nature.com/articles/s41405-025-00349-1", "details": "G Sivaramakrishnan, M Almuqahwi, S Ansari\u2026 - BDJ open, 2025", "abstract": "\u2026 This study evaluates the use of **large** **language** **models** (LLMs) in generating Patient Education Materials (PEMs) for dental scenarios, focusing on their reliability, readability, understandability, and actionability. The study aimed to assess the \u2026"}, {"title": "Red teaming **large language models** : A comprehensive review and critical analysis", "link": "https://www.sciencedirect.com/science/article/pii/S0306457325001803", "details": "MS Jabbar, S Al-Azani, A Alotaibi, M Ahmed - Information Processing & Management, 2025", "abstract": "Securing **large** **language** **models** (LLMs) remains a critical challenge as their adoption across various sectors rapidly grows. While advancements in LLM development have enhanced their capabilities, inherent vulnerabilities continue to \u2026"}, {"title": "Performance of ChatGPT-4o and Four Open-Source **Large Language Models** in Generating Diagnoses Based on China's Rare Disease Catalog: Comparative Study", "link": "https://www.jmir.org/2025/1/e69929/", "details": "W Zhong, YF Liu, Y Liu, K Yang, HM Gao, HH Yan\u2026 - Journal of Medical Internet \u2026, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) offer new potential to enhance diagnostic workflows. Objective: This study aimed to **evaluate** the \u2026 (LLMs) generated 8 response sets (4 models\u00d72 languages); chain-of-thought integration with **large** \u2026"}, {"title": "DeVisE: Behavioral Testing of Medical Large Language Models", "link": "https://arxiv.org/pdf/2506.15339", "details": "CZ Tagliabue, HO Boll, A Erdem, E Erdem, I Calixto - arXiv preprint arXiv:2506.15339, 2025", "abstract": "**Large** **language** **models** (LLMs) are increasingly used in clinical decision support, yet current **evaluation** methods often fail to distinguish genuine medical reasoning from superficial patterns. We introduce DeVisE (Demographics and Vital signs \u2026", "entry_id": "http://arxiv.org/abs/2506.15339v1", "updated": "2025-06-18 10:42:22", "published": "2025-06-18 10:42:22", "authors": "Camila Zurdo Tagliabue;Heloisa Oss Boll;Aykut Erdem;Erkut Erdem;Iacer Calixto", "summary": "Large language models (LLMs) are increasingly used in clinical decision\nsupport, yet current evaluation methods often fail to distinguish genuine\nmedical reasoning from superficial patterns. We introduce DeVisE (Demographics\nand Vital signs Evaluation), a behavioral testing framework for probing\nfine-grained clinical understanding. We construct a dataset of ICU discharge\nnotes from MIMIC-IV, generating both raw (real-world) and template-based\n(synthetic) versions with controlled single-variable counterfactuals targeting\ndemographic (age, gender, ethnicity) and vital sign attributes. We evaluate\nfive LLMs spanning general-purpose and medically fine-tuned variants, under\nboth zero-shot and fine-tuned settings. We assess model behavior via (1)\ninput-level sensitivity - how counterfactuals alter the likelihood of a note;\nand (2) downstream reasoning - how they affect predicted hospital\nlength-of-stay. Our results show that zero-shot models exhibit more coherent\ncounterfactual reasoning patterns, while fine-tuned models tend to be more\nstable yet less responsive to clinically meaningful changes. Notably,\ndemographic factors subtly but consistently influence outputs, emphasizing the\nimportance of fairness-aware evaluation. This work highlights the utility of\nbehavioral testing in exposing the reasoning strategies of clinical LLMs and\ninforming the design of safer, more transparent medical AI systems.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.15339v1;http://arxiv.org/pdf/2506.15339v1", "pdf_url": "http://arxiv.org/pdf/2506.15339v1"}, {"title": "scExtract: leveraging **large language models** for fully automated single-cell RNA-seq data annotation and prior-informed multi-dataset integration", "link": "https://link.springer.com/article/10.1186/s13059-025-03639-x", "details": "Y Wu, F Tang - Genome Biology, 2025", "abstract": "\u2026 We first **evaluated** the preservation of group structure in single-cell RNA-seq datasets across different automatic annotation methods. Regarding the discrepancy between annotated cell type numbers and those reported in the original articles \u2026"}, {"title": "Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models", "link": "https://arxiv.org/pdf/2506.15568", "details": "Z Shan, ER Diana, J Zhou - arXiv preprint arXiv:2506.15568, 2025", "abstract": "\u2026 We present a comprehensive **evaluation** of gender fairness in **large** **language** **models** (LLMs)\u2026 GIFI consists of a wide range of **evaluations** at different levels, from simply probing the model \u2026 We conduct extensive **evaluations** with GIFI on 22 \u2026", "entry_id": "http://arxiv.org/abs/2506.15568v1", "updated": "2025-06-18 15:43:16", "published": "2025-06-18 15:43:16", "authors": "Zhengyang Shan;Emily Ruth Diana;Jiawei Zhou", "summary": "We present a comprehensive evaluation of gender fairness in large language\nmodels (LLMs), focusing on their ability to handle both binary and non-binary\ngenders. While previous studies primarily focus on binary gender distinctions,\nwe introduce the Gender Inclusivity Fairness Index (GIFI), a novel and\ncomprehensive metric that quantifies the diverse gender inclusivity of LLMs.\nGIFI consists of a wide range of evaluations at different levels, from simply\nprobing the model with respect to provided gender pronouns to testing various\naspects of model generation and cognitive behaviors under different gender\nassumptions, revealing biases associated with varying gender identifiers. We\nconduct extensive evaluations with GIFI on 22 prominent open-source and\nproprietary LLMs of varying sizes and capabilities, discovering significant\nvariations in LLMs' gender inclusivity. Our study highlights the importance of\nimproving LLMs' inclusivity, providing a critical benchmark for future\nadvancements in gender fairness in generative models.", "comment": "Accepted by ACL 2025 Main", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.15568v1;http://arxiv.org/pdf/2506.15568v1", "pdf_url": "http://arxiv.org/pdf/2506.15568v1"}, {"title": "Large Language Models for Unit Testing: A Systematic Literature Review", "link": "https://arxiv.org/pdf/2506.15227", "details": "Q Zhang, C Fang, S Gu, Y Shang, Z Chen, L Xiao - arXiv preprint arXiv:2506.15227, 2025", "abstract": "\u2026 Very recently, with the advances in **Large** **Language** **Models** (LLMs), a rapidly growing body of research has leveraged LLMs to automate \u2026 ten quality assessment questions to **evaluate** the relevance and rigor of included papers. For \u2026", "entry_id": "http://arxiv.org/abs/2506.15227v1", "updated": "2025-06-18 08:11:10", "published": "2025-06-18 08:11:10", "authors": "Quanjun Zhang;Chunrong Fang;Siqi Gu;Ye Shang;Zhenyu Chen;Liang Xiao", "summary": "Unit testing is a fundamental practice in modern software engineering, with\nthe aim of ensuring the correctness, maintainability, and reliability of\nindividual software components. Very recently, with the advances in Large\nLanguage Models (LLMs), a rapidly growing body of research has leveraged LLMs\nto automate various unit testing tasks, demonstrating remarkable performance\nand significantly reducing manual effort. However, due to ongoing explorations\nin the LLM-based unit testing field, it is challenging for researchers to\nunderstand existing achievements, open challenges, and future opportunities.\nThis paper presents the first systematic literature review on the application\nof LLMs in unit testing until March 2025. We analyze \\numpaper{} relevant\npapers from the perspectives of both unit testing and LLMs. We first categorize\nexisting unit testing tasks that benefit from LLMs, e.g., test generation and\noracle generation. We then discuss several critical aspects of integrating LLMs\ninto unit testing research, including model usage, adaptation strategies, and\nhybrid approaches. We further summarize key challenges that remain unresolved\nand outline promising directions to guide future research in this area.\nOverall, our paper provides a systematic overview of the research landscape to\nthe unit testing community, helping researchers gain a comprehensive\nunderstanding of achievements and promote future research. Our artifacts are\npublicly available at the GitHub repository:\nhttps://github.com/iSEngLab/AwesomeLLM4UT.", "comment": null, "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE", "links": "http://arxiv.org/abs/2506.15227v1;http://arxiv.org/pdf/2506.15227v1", "pdf_url": "http://arxiv.org/pdf/2506.15227v1"}, {"title": "Demystifying the Visual Quality Paradox in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2506.15645", "details": "S Xing, L Guo, H Hua, S Lee, P Li, Y Wang, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 In this section, we **evaluate** the robustness and performance of MLLMs using Visual Question Answering (VQA) benchmarks under various visual degradations. We specifically examine three broad categories of image degradation: noise (Gaussian \u2026", "entry_id": "http://arxiv.org/abs/2506.15645v1", "updated": "2025-06-18 17:14:07", "published": "2025-06-18 17:14:07", "authors": "Shuo Xing;Lanqing Guo;Hongyuan Hua;Seoyoung Lee;Peiran Li;Yufei Wang;Zhangyang Wang;Zhengzhong Tu", "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.", "comment": "18 pages", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2506.15645v1;http://arxiv.org/pdf/2506.15645v1", "pdf_url": "http://arxiv.org/pdf/2506.15645v1"}]
