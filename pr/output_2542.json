[{"title": "FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models", "link": "https://arxiv.org/pdf/2405.10286", "details": "A Bulat, Y Ouali, G Tzimiropoulos - arXiv preprint arXiv:2405.10286, 2024", "abstract": "Despite noise and caption quality having been acknowledged as important factors impacting vision-language contrastive pre-training, in this paper, we show that the full potential of improving the training process by addressing such issues is yet to be \u2026"}, {"title": "Observational Scaling Laws and the Predictability of Language Model Performance", "link": "https://arxiv.org/pdf/2405.10938", "details": "Y Ruan, CJ Maddison, T Hashimoto - arXiv preprint arXiv:2405.10938, 2024", "abstract": "Understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different \u2026"}, {"title": "Compositional Text-to-Image Generation with Dense Blob Representations", "link": "https://arxiv.org/pdf/2405.08246", "details": "W Nie, S Liu, M Mardani, C Liu, B Eckart, A Vahdat - arXiv preprint arXiv:2405.08246, 2024", "abstract": "Existing text-to-image models struggle to follow complex text prompts, raising the need for extra grounding inputs for better controllability. In this work, we propose to decompose a scene into visual primitives-denoted as dense blob representations \u2026"}, {"title": "Backdoor Removal for Generative Large Language Models", "link": "https://arxiv.org/pdf/2405.07667", "details": "H Li, Y Chen, Z Zheng, Q Hu, C Chan, H Liu, Y Song - arXiv preprint arXiv \u2026, 2024", "abstract": "With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased \u2026"}, {"title": "A Survey on Vision-Language-Action Models for Embodied AI", "link": "https://arxiv.org/pdf/2405.14093", "details": "Y Ma, Z Song, Y Zhuang, J Hao, I King - arXiv preprint arXiv:2405.14093, 2024", "abstract": "Deep learning has demonstrated remarkable success across many domains, including computer vision, natural language processing, and reinforcement learning. Representative artificial neural networks in these fields span convolutional neural \u2026"}, {"title": "EHMMQA: English, Hindi, and Marathi multilingual question answering framework using deep learning", "link": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/9E3BCB05DA488578931A7633C2646FF4/S2977042424000128a.pdf/ehmmqa_english_hindi_and_marathi_multilingual_question_answering_framework_using_deep_learning.pdf", "details": "P Lahoti, N Mittal, G Singh - Natural Language Processing", "abstract": "Multilingual question answering (MQA) is an effective access to multilingual data to provide accurate and precise answers, irrespective of language. Although a wide range of datasets is available for monolingual QA systems in natural language \u2026"}, {"title": "SEP: Self-Enhanced Prompt Tuning for Visual-Language Model", "link": "https://arxiv.org/pdf/2405.15549", "details": "H Yao, R Zhang, L Yu, C Xu - arXiv preprint arXiv:2405.15549, 2024", "abstract": "Prompt tuning based on Context Optimization (CoOp) effectively adapts visual- language models (VLMs) to downstream tasks by inferring additional learnable prompt tokens. However, these tokens are less discriminative as they are \u2026"}, {"title": "Improving Transformers with Dynamically Composable Multi-Head Attention", "link": "https://arxiv.org/pdf/2405.08553", "details": "D Xiao, Q Meng, S Li, X Yuan - arXiv preprint arXiv:2405.08553, 2024", "abstract": "Multi-Head Attention (MHA) is a key component of Transformer. In MHA, attention heads work independently, causing problems such as low-rank bottleneck of attention score matrices and head redundancy. We propose Dynamically \u2026"}, {"title": "JEMHopQA: Dataset for Japanese Explainable Multi-Hop Question Answering", "link": "https://aclanthology.org/2024.lrec-main.831.pdf", "details": "A Ishii, N Inoue, H Suzuki, S Sekine - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "We present JEMHopQA, a multi-hop QA dataset for the development of explainable QA systems. The dataset consists not only of question-answer pairs, but also of supporting evidence in the form of derivation triples, which contributes to making the \u2026"}]
