[{"title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge", "link": "https://arxiv.org/pdf/2407.19594", "details": "T Wu, W Yuan, O Golovneva, J Xu, Y Tian, J Jiao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can \u2026"}, {"title": "Artificial intelligence-based pulmonary embolism classification: Development and validation using real-world data", "link": "https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0305839", "details": "LO Silva, MCB Silva, GAS Ribeiro, TFO Camargo\u2026 - PloS one, 2024", "abstract": "This paper presents an artificial intelligence-based classification model for the detection of pulmonary embolism in computed tomography angiography. The proposed model, developed from public data and validated on a large dataset from a \u2026"}, {"title": "MindLLM: Lightweight large language model pre-training, evaluation and domain application", "link": "https://www.sciencedirect.com/science/article/pii/S2666651024000111", "details": "Y Yang, H Sun, J Li, R Liu, Y Li, Y Liu, Y Gao, H Huang - AI Open, 2024", "abstract": "Abstract Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is \u2026"}, {"title": "DOSSIER: Fact checking in electronic health records while preserving patient privacy", "link": "https://www.amazon.science/publications/dossier-fact-checking-in-electronic-health-records-while-preserving-patient-privacy", "details": "H Zhang, S Nagesh, M Shyani, N Mishra - 2024", "abstract": "Given a particular claim about a specific document, the fact checking problem is to determine if the claim is true and, if so, provide corroborating evidence. The problem is motivated by contexts where a document is too lengthy to quickly read and find an \u2026"}, {"title": "Prompt Learning with Extended Kalman Filter for Pre-trained Language Models", "link": "https://www.ijcai.org/proceedings/2024/0492.pdf", "details": "Q Li, X Xie, C Wang, SK Zhou", "abstract": "Prompt learning has gained popularity as a means to leverage the knowledge embedded in pre-trained language models (PLMs) for NLP tasks while using a limited number of trainable parameters. While it has shown promise in tasks like \u2026"}, {"title": "Pre-training data selection for biomedical domain adaptation using journal impact metrics", "link": "https://aclanthology.org/2024.bionlp-1.27.pdf", "details": "M Lai-king, P Paroubek - Proceedings of the 23rd Workshop on Biomedical \u2026, 2024", "abstract": "Abstract Domain adaptation is a widely used method in natural language processing (NLP) to improve the performance of a language model within a specific domain. This method is particularly common in the biomedical domain, which sees regular \u2026"}, {"title": "XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models", "link": "https://aclanthology.org/2024.bionlp-1.35.pdf", "details": "OC Thawakar, AM Shaker, SS Mullappilly, H Cholakkal\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "The latest breakthroughs in large language models (LLMs) and vision-language models (VLMs) have showcased promising capabilities toward performing a wide range of tasks. Such models are typically trained on massive datasets comprising \u2026"}, {"title": "Cognitive Assessment of Language Models", "link": "https://openreview.net/pdf%3Fid%3DpxRh1meUvN", "details": "D McDuff, D Munday, X Liu, I Galatzer-Levy - ICML 2024 Workshop on LLMs and Cognition", "abstract": "Large language models (LLMs) are a subclass of generative artificial intelligence that can interpret language inputs to generate novel responses. These capabilities are conceptualized as a significant step forward in artificial intelligence because the \u2026"}, {"title": "XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training", "link": "https://arxiv.org/pdf/2407.19546", "details": "B Wu, Y Xie, Z Zhang, MH Phan, Q Chen, L Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-and-language pretraining (VLP) in the medical field utilizes contrastive learning on image-text pairs to achieve effective transfer across tasks. Yet, current VLP approaches with the masked modelling strategy face two challenges when \u2026"}]
