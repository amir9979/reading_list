'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Can 3D Vision-Language Models Truly Understand Natural'
[{"title": "Adaptive Prompt Routing for Arbitrary Text Style Transfer with Pre-trained Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29832/31446", "details": "Q Liu, J Qin, W Ye, H Mou, Y He, K Wang - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Recently, arbitrary text style transfer (TST) has made significant progress with the paradigm of prompt learning. In this paradigm, researchers often design or search for a fixed prompt for any input. However, existing evidence shows that large language \u2026"}, {"title": "Emergent Abilities in Reduced-Scale Generative Language Models", "link": "https://arxiv.org/pdf/2404.02204", "details": "S Muckatira, V Deshpande, V Lialin, A Rumshisky - arXiv preprint arXiv:2404.02204, 2024", "abstract": "Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study \u2026"}, {"title": "Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards", "link": "https://arxiv.org/pdf/2404.10346", "details": "H Hwang, D Kim, S Kim, S Ye, M Seo - arXiv preprint arXiv:2404.10346, 2024", "abstract": "Training on large amounts of rationales (ie, CoT Fine-tuning) is effective at improving the reasoning capabilities of large language models (LLMs). However, acquiring human-authored rationales or augmenting rationales from proprietary models is \u2026"}, {"title": "Question-answering system extracts information on injection drug use from clinical notes", "link": "https://www.nature.com/articles/s43856-024-00470-6", "details": "M Mahbub, I Goethert, I Danciu, K Knight, S Srinivasan\u2026 - Communications Medicine, 2024", "abstract": "Background Injection drug use (IDU) can increase mortality and morbidity. Therefore, identifying IDU early and initiating harm reduction interventions can benefit individuals at risk. However, extracting IDU behaviors from patients' electronic health \u2026"}, {"title": "Emergent Language Symbolic Autoencoder (ELSA) with Weak Supervision to Model Hierarchical Brain Networks", "link": "https://arxiv.org/pdf/2404.10031", "details": "AAP Latheef, A Santamaria-Pang, CK Jones, HI Sair - arXiv preprint arXiv \u2026, 2024", "abstract": "Brain networks display a hierarchical organization, a complexity that poses a challenge for existing deep learning models, often structured as flat classifiers, leading to difficulties in interpretability and the'black box'issue. To bridge this gap, we \u2026"}, {"title": "Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context", "link": "https://arxiv.org/pdf/2404.02000", "details": "A Caubri\u00e8re, E Gauthier - arXiv preprint arXiv:2404.02000, 2024", "abstract": "We present the first self-supervised multilingual speech model trained exclusively on African speech. The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa. On the SSA \u2026"}, {"title": "Genetic Auto-prompt Learning for Pre-trained Code Intelligence Language Models", "link": "https://arxiv.org/pdf/2403.13588", "details": "C Feng, Y Sun, K Li, P Zhou, J Lv, A Lu - arXiv preprint arXiv:2403.13588, 2024", "abstract": "As Pre-trained Language Models (PLMs), a popular approach for code intelligence, continue to grow in size, the computational cost of their usage has become prohibitively expensive. Prompt learning, a recent development in the field of natural \u2026"}, {"title": "DESTEIN: Navigating Detoxification of Language Models via Universal Steering Pairs and Head-wise Activation Fusion", "link": "https://arxiv.org/pdf/2404.10464", "details": "Y Li, Z Wei, H Jiang, C Gong - arXiv preprint arXiv:2404.10464, 2024", "abstract": "Despite the remarkable achievements of language models (LMs) across a broad spectrum of tasks, their propensity for generating toxic outputs remains a prevalent concern. Current solutions involving fine-tuning or auxiliary models usually require \u2026"}, {"title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models", "link": "https://arxiv.org/pdf/2404.02936", "details": "J Zhang, J Sun, E Yeats, Y Ouyang, M Kuo, J Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. The current state-of-the-art approach, Min-K \u2026"}]
