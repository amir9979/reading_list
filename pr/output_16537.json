[{"title": "Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models", "link": "https://arxiv.org/pdf/2505.10446", "details": "Z Huang, Z Chen, Z Wang, T Li, GJ Qi - arXiv preprint arXiv:2505.10446, 2025", "abstract": "We introduce the\\emph {Diffusion Chain of Lateral Thought (DCoLT)}, a reasoning framework for diffusion language models. DCoLT treats each intermediate step in the reverse diffusion process as a latent\" thinking\" action and optimizes the entire \u2026"}, {"title": "Platonic Grounding for Efficient Multimodal Language Models", "link": "https://arxiv.org/pdf/2504.19327", "details": "M Choraria, X Wu, A Bhimaraju, N Sekhar, Y Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The hyperscaling of data and parameter count in Transformer-based models is yielding diminishing performance improvement, especially when weighed against training costs. Such plateauing indicates the importance of methods for more efficient \u2026"}, {"title": "Small but Significant: On the Promise of Small Language Models for Accessible AIED", "link": "https://arxiv.org/pdf/2505.08588", "details": "Y Wei, P Carvalho, J Stamper - arXiv preprint arXiv:2505.08588, 2025", "abstract": "GPT has become nearly synonymous with large language models (LLMs), an increasingly popular term in AIED proceedings. A simple keyword-based search reveals that 61% of the 76 long and short papers presented at AIED 2024 describe \u2026"}, {"title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning", "link": "https://arxiv.org/pdf/2505.10320", "details": "C Whitehouse, T Wang, P Yu, X Li, J Weston, I Kulikov\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as- a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best \u2026"}, {"title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data", "link": "https://arxiv.org/pdf/2505.05427", "details": "Y Wang, Z Fu, J Cai, P Tang, H Lyu, Y Fang, Z Zheng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Data quality has become a key factor in enhancing model performance with the rapid development of large language models (LLMs). Model-driven data filtering has increasingly become a primary approach for acquiring high-quality data. However, it \u2026"}, {"title": "Identifying signs and symptoms of pancreatic cancer: a population-based study using electronic health records and natural language processing", "link": "https://www.sciencedirect.com/science/article/pii/S1424390325000900", "details": "W Chen, F Xie, TQ Luong, J Chang, E Lustigova\u2026 - Pancreatology, 2025", "abstract": "ABSTRACT OBJECTIVES Patient-reported symptoms are often the trigger for diagnosis of pancreatic cancer (PC). We aimed to report the prevalence and lead time of symptoms of PC from electronic health records in a population-based sample \u2026"}, {"title": "Developing safe and responsible large language model: can we balance bias reduction and language understanding?", "link": "https://link.springer.com/article/10.1007/s10994-025-06767-4", "details": "S Raza, O Bamgbose, S Ghuge, F Tavakoli, DJ Reji\u2026 - Machine Learning, 2025", "abstract": "Abstract Large Language Models (LLMs) have advanced various Natural Language Processing (NLP) tasks, such as text generation and translation, among others. However, these models often generate texts that can perpetuate biases. Existing \u2026"}, {"title": "GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data", "link": "https://arxiv.org/pdf/2505.03233", "details": "S Deng, M Yan, S Wei, H Ma, Y Yang, J Chen, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post- training. However, existing models rely heavily on real-world data, which is costly \u2026"}, {"title": "R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation", "link": "https://arxiv.org/pdf/2505.02018%3F", "details": "MH Guo, J Xu, Y Zhang, J Song, H Peng, YX Deng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. Despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning \u2026"}]
