[{"title": "A Trajectory Perspective on the Role of Data Sampling Techniques in Offline Reinforcement Learning", "link": "https://www.ifaamas.org/Proceedings/aamas2024/pdfs/p1229.pdf", "details": "J Liu, Y Ma, J Hao, Y Hu, Y Zheng, T Lv, C Fan - Proceedings of the 23rd International \u2026, 2024", "abstract": "In recent years, offline reinforcement learning (RL) algorithms have gained considerable attention. However, the role of data sampling techniques in offline RL has been somewhat overlooked, despite their potential to enhance online RL \u2026"}, {"title": "Theoretical Analysis of Meta Reinforcement Learning: Generalization Bounds and Convergence Guarantees", "link": "https://arxiv.org/pdf/2405.13290", "details": "C Wang, M Sui, D Sun, Z Zhang, Y Zhou - arXiv preprint arXiv:2405.13290, 2024", "abstract": "This research delves deeply into Meta Reinforcement Learning (Meta RL) through a exploration focusing on defining generalization limits and ensuring convergence. By employing a approach this article introduces an innovative theoretical framework to \u2026"}, {"title": "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "link": "https://arxiv.org/pdf/2404.19335", "details": "X Liu, C Liu, Z Zhang, C Li, L Wang, Y Lan, C Shen - arXiv preprint arXiv:2404.19335, 2024", "abstract": "Large language models have shown their ability to become effective few-shot learners with prompting, revoluting the paradigm of learning with data scarcity. However, this approach largely depends on the quality of prompt initialization, and \u2026"}, {"title": "HFT: Half Fine-Tuning for Large Language Models", "link": "https://arxiv.org/pdf/2404.18466", "details": "T Hui, Z Zhang, S Wang, W Xu, Y Sun, H Wu - arXiv preprint arXiv:2404.18466, 2024", "abstract": "Large language models (LLMs) with one or more fine-tuning phases have become a necessary step to unlock various capabilities, enabling LLMs to follow natural language instructions or align with human preferences. However, it carries the risk of \u2026"}, {"title": "QCRD: Quality-guided Contrastive Rationale Distillation for Large Language Models", "link": "https://arxiv.org/pdf/2405.13014", "details": "W Wang, Z Li, Q Xu, Y Cai, H Song, Q Qi, R Zhou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Deploying large language models (LLMs) poses challenges in terms of resource limitations and inference efficiency. To address these challenges, recent research has focused on using smaller task-specific language models, which are enhanced by \u2026"}, {"title": "Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2405.00175", "details": "A Salemi, H Zamani - arXiv preprint arXiv:2405.00175, 2024", "abstract": "This paper introduces uRAG--a framework with a unified retrieval engine that serves multiple downstream retrieval-augmented generation (RAG) systems. Each RAG system consumes the retrieval results for a unique purpose, such as open-domain \u2026"}, {"title": "Investigating Symbolic Capabilities of Large Language Models", "link": "https://arxiv.org/pdf/2405.13209", "details": "N Dave, D Kifer, CL Giles, A Mali - arXiv preprint arXiv:2405.13209, 2024", "abstract": "Prompting techniques have significantly enhanced the capabilities of Large Language Models (LLMs) across various complex tasks, including reasoning, planning, and solving math word problems. However, most research has \u2026"}]
