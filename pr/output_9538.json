[{"title": "Learning local discrete features in explainable-by-design convolutional neural networks", "link": "https://arxiv.org/pdf/2411.00139", "details": "PI Kaplanoglou, K Diamantaras - arXiv preprint arXiv:2411.00139, 2024", "abstract": "Our proposed framework attempts to break the trade-off between performance and explainability by introducing an explainable-by-design convolutional neural network (CNN) based on the lateral inhibition mechanism. The ExplaiNet model consists of \u2026"}, {"title": "SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction", "link": "https://arxiv.org/pdf/2411.16765", "details": "S Gueuwou, X Du, G Shakhnarovich, K Livescu, AH Liu - arXiv preprint arXiv \u2026, 2024", "abstract": "Sign language processing has traditionally relied on task-specific models, limiting the potential for transfer learning across tasks. We introduce SHuBERT (Sign Hidden- Unit BERT), a self-supervised transformer encoder that learns strong representations \u2026"}, {"title": "Fast multi-geometry calorimeter simulation with conditional self-attention variational autoencoders", "link": "https://arxiv.org/pdf/2411.05996", "details": "D Smith, A Ghosh, J Liu, P Baldi, D Whiteson - arXiv preprint arXiv:2411.05996, 2024", "abstract": "The simulation of detector response is a vital aspect of data analysis in particle physics, but current Monte Carlo methods are computationally expensive. Machine learning methods, which learn a mapping from incident particle to detector response \u2026"}, {"title": "CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense", "link": "https://arxiv.org/pdf/2410.23091", "details": "M Zhang, K Bi, W Chen, Q Chen, J Guo, X Cheng - arXiv preprint arXiv:2410.23091, 2024", "abstract": "Despite ongoing efforts to defend neural classifiers from adversarial attacks, they remain vulnerable, especially to unseen attacks. In contrast, humans are difficult to be cheated by subtle manipulations, since we make judgments only based on \u2026"}, {"title": "VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models", "link": "https://arxiv.org/pdf/2411.17451", "details": "L Li, Y Wei, Z Xie, X Yang, Y Song, P Wang, C An, T Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language generative reward models (VL-GenRMs) play a crucial role in aligning and evaluating multimodal AI systems, yet their own evaluation remains under-explored. Current assessment methods primarily rely on AI-annotated \u2026"}, {"title": "You Don't Need Domain-Specific Data Augmentations When Scaling Self-Supervised Learning", "link": "https://openreview.net/pdf%3Fid%3D7RwKMRMNrc", "details": "T Moutakanni, M Oquab, M Szafraniec\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Self-Supervised learning (SSL) with Joint-Embedding Architectures (JEA) has led to outstanding performances. All instantiations of this paradigm were trained using strong and well-established hand-crafted data augmentations, leading to the general \u2026"}, {"title": "Decoding Report Generators: A Cyclic Vision-Language Adapter for Counterfactual Explanations", "link": "https://arxiv.org/pdf/2411.05261", "details": "Y Fang, Z Jin, S Guo, J Liu, Y Gao, J Ning, Z Yue, Z Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite significant advancements in report generation methods, a critical limitation remains: the lack of interpretability in the generated text. This paper introduces an innovative approach to enhance the explainability of text generated by report \u2026"}, {"title": "Self-supervised Video Instance Segmentation Can Boost Geographic Entity Alignment in Historical Maps", "link": "https://arxiv.org/pdf/2411.17425", "details": "X Xia, R Balestriero, T Zhang, L Hurni - arXiv preprint arXiv:2411.17425, 2024", "abstract": "Tracking geographic entities from historical maps, such as buildings, offers valuable insights into cultural heritage, urbanization patterns, environmental changes, and various historical research endeavors. However, linking these entities across diverse \u2026"}, {"title": "An Empirical Study of Self-Supervised Learning with Wasserstein Distance", "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11592799/", "details": "M Yamada, Y Takezawa, G Houry, KM D\u00fcsterwald\u2026 - Entropy, 2024", "abstract": "In this study, we consider the problem of self-supervised learning (SSL) utilizing the 1- Wasserstein distance on a tree structure (aka, Tree-Wasserstein distance (TWD)), where TWD is defined as the L1 distance between two tree-embedded vectors. In \u2026"}]
