[{"title": "BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models", "link": "https://arxiv.org/pdf/2406.17092", "details": "Y Zeng, W Sun, TN Huynh, D Song, B Li, R Jia - arXiv preprint arXiv:2406.17092, 2024", "abstract": "Safety backdoor attacks in large language models (LLMs) enable the stealthy triggering of unsafe behaviors while evading detection during normal interactions. The high dimensionality of potential triggers in the token space and the diverse \u2026"}, {"title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models", "link": "https://arxiv.org/pdf/2406.19146", "details": "T Porian, M Wortsman, J Jitsev, L Schmidt, Y Carmon - arXiv preprint arXiv \u2026, 2024", "abstract": "Kaplan et al. and Hoffmann et al. developed influential scaling laws for the optimal model size as a function of the compute budget, but these laws yield substantially different predictions. We explain the discrepancy by reproducing the Kaplan scaling \u2026"}, {"title": "Evaluating Copyright Takedown Methods for Language Models", "link": "https://arxiv.org/pdf/2406.18664", "details": "B Wei, W Shi, Y Huang, NA Smith, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) derive their capabilities from extensive training on diverse data, including potentially copyrighted material. These models can memorize and generate content similar to their training data, posing potential concerns. Therefore \u2026"}, {"title": "Monitoring Latent World States in Language Models with Propositional Probes", "link": "https://arxiv.org/pdf/2406.19501", "details": "J Feng, S Russell, J Steinhardt - arXiv preprint arXiv:2406.19501, 2024", "abstract": "Language models are susceptible to bias, sycophancy, backdoors, and other tendencies that lead to unfaithful responses to the input context. Interpreting internal states of language models could help monitor and correct unfaithful behavior. We \u2026"}, {"title": "Just read twice: closing the recall gap for recurrent language models", "link": "https://arxiv.org/pdf/2407.05483", "details": "S Arora, A Timalsina, A Singhal, B Spector, S Eyuboglu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (eg, Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However \u2026"}, {"title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners", "link": "https://arxiv.org/pdf/2406.14491", "details": "D Cheng, Y Gu, S Huang, J Bi, M Huang, F Wei - arXiv preprint arXiv:2406.14491, 2024", "abstract": "Unsupervised multitask pre-training has been the critical method behind the recent success of language models (LMs). However, supervised multitask learning still holds significant promise, as scaling it in the post-training stage trends towards better \u2026"}, {"title": "Dialogue Action Tokens: Steering Language Models in Goal-Directed Dialogue with a Multi-Turn Planner", "link": "https://arxiv.org/pdf/2406.11978", "details": "K Li, Y Wang, F Vi\u00e9gas, M Wattenberg - arXiv preprint arXiv:2406.11978, 2024", "abstract": "We present an approach called Dialogue Action Tokens (DAT) that adapts language model agents to plan goal-directed dialogues. The core idea is to treat each utterance as an action, thereby converting dialogues into games where existing \u2026"}, {"title": "Timo: Towards Better Temporal Reasoning for Language Models", "link": "https://arxiv.org/pdf/2406.14192", "details": "Z Su, J Zhang, T Zhu, X Qu, J Li, M Zhang, Y Cheng - arXiv preprint arXiv:2406.14192, 2024", "abstract": "Reasoning about time is essential for Large Language Models (LLMs) to understand the world. Previous works focus on solving specific tasks, primarily on time-sensitive question answering. While these methods have proven effective, they cannot \u2026"}, {"title": "STAR: SocioTechnical Approach to Red Teaming Language Models", "link": "https://arxiv.org/pdf/2406.11757", "details": "L Weidinger, J Mellor, BG Pegueroles, N Marchal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This research introduces STAR, a sociotechnical framework that improves on current best practices for red teaming safety of large language models. STAR makes two key contributions: it enhances steerability by generating parameterised instructions for \u2026"}]
