[{"title": "Utilization and Uptake of the UpToDate Clinical Decision Support Tool in Five Medical Schools in Uganda (August 2022-August 2023): A Partnership with the Better \u2026", "link": "https://www.scirp.org/journal/paperinformation%3Fpaperid%3D140952", "details": "AA Kinengyere, G Asiimwe, A Nyamwiza, W Adriko\u2026 - International Journal of \u2026, 2025", "abstract": "Background: Clinical decision support tools provide suggestions to support healthcare providers and clinicians, as they attend to patients. Clinicians use these tools to rapidly consult the evidence at the point of care, a practice which has been \u2026"}, {"title": "OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction", "link": "https://arxiv.org/pdf/2503.03734", "details": "H Huang, F Liu, L Fu, T Wu, M Mukadam, J Malik\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-Language-Action (VLA) models aim to predict robotic actions based on visual observations and language instructions. Existing approaches require fine-tuning pre- trained visionlanguage models (VLMs) as visual and language features are \u2026"}, {"title": "Medical foundation large language models for comprehensive text analysis and beyond", "link": "https://www.nature.com/articles/s41746-025-01533-1", "details": "Q Xie, Q Chen, A Chen, C Peng, Y Hu, F Lin, X Peng\u2026 - npj Digital Medicine, 2025", "abstract": "Recent advancements in large language models (LLMs) show significant potential in medical applications but are hindered by limited specialized medical knowledge. We present Me-LLaMA, a family of open-source medical LLMs integrating extensive \u2026"}, {"title": "Position: Editing Large Language Models Poses Serious Safety Risks", "link": "https://arxiv.org/pdf/2502.02958", "details": "P Youssef, Z Zhao, D Braun, J Schl\u00f6tterer, C Seifert - arXiv preprint arXiv:2502.02958, 2025", "abstract": "Large Language Models (LLMs) contain large amounts of facts about the world. These facts can become outdated over time, which has led to the development of knowledge editing methods (KEs) that can change specific facts in LLMs with limited \u2026"}, {"title": "OPTISHEAR: Towards Efficient and Adaptive Pruning of Large Language Models via Evolutionary Optimization", "link": "https://arxiv.org/pdf/2502.10735", "details": "S Liu, B He, H Wu, L Song - arXiv preprint arXiv:2502.10735, 2025", "abstract": "Post-training pruning has emerged as a crucial optimization technique as large language models (LLMs) continue to grow rapidly. However, the significant variations in weight distributions across different LLMs make fixed pruning strategies \u2026"}, {"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models", "link": "https://arxiv.org/pdf/2502.03199%3F", "details": "J Wu, Y Shen, S Liu, Y Tang, S Song, X Wang, L Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the \u2026"}, {"title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models", "link": "https://arxiv.org/pdf/2502.10743", "details": "S Liu, H Wu, B He, Z Liu, X Han, M Yuan, L Song - arXiv preprint arXiv:2502.10743, 2025", "abstract": "Recent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they \u2026"}, {"title": "Detecting LLM Fact-conflicting Hallucinations Enhanced by Temporal-logic-based Reasoning", "link": "https://arxiv.org/pdf/2502.13416", "details": "N Li, Y Song, K Wang, Y Li, L Shi, Y Liu, H Wang - arXiv preprint arXiv:2502.13416, 2025", "abstract": "Large language models (LLMs) face the challenge of hallucinations--outputs that seem coherent but are actually incorrect. A particularly damaging type is fact- conflicting hallucination (FCH), where generated content contradicts established \u2026"}, {"title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems", "link": "https://arxiv.org/pdf/2503.03686", "details": "R Ye, S Tang, R Ge, Y Du, Z Yin, S Chen, J Shao - arXiv preprint arXiv:2503.03686, 2025", "abstract": "LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability \u2026"}]
