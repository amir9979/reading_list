[{"title": "Efficient and explainable sequential recommendation with language model", "link": "https://drive.google.com/file/d/11rlrXoCrYwH9mIJCDfw2gHwD8PCImjoK/view", "details": "Z Li, L Zou, C Ma, C Li - Information Processing & Management, 2025", "abstract": "Motivated by the outstanding success of large language models (LLMs) in a broad spectrum of NLP tasks, applying them for explainable recommendation become a cutting-edge recently. However, due to the inherent inconsistency in the information \u2026"}, {"title": "DF-MIA: A Distribution-Free Membership Inference Attack on Fine-Tuned Large Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/32012/34167", "details": "Z Huang, Y Liu, D He, Y Li - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Abstract Membership Inference Attack (MIA) aims to determine if a specific sample is present in the training dataset of a target machine learning model. Previous MIAs against fine-tuned Large Language Models (LLMs) either fail to address the unique \u2026"}, {"title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs", "link": "https://arxiv.org/pdf/2504.07866%3F", "details": "Y Yin, W Huang, K Song, Y Tang, X Wu, W Guo, P Guo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing \u2026"}, {"title": "Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection", "link": "https://arxiv.org/pdf/2504.09440", "details": "MS Liu, S Bo, J Fang - arXiv preprint arXiv:2504.09440, 2025", "abstract": "Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical \u2026"}, {"title": "CSPO: chain-structured prompt optimisation for large language models", "link": "https://www.inderscienceonline.com/doi/abs/10.1504/IJAHUC.2025.145202", "details": "J Wang, S Lin, X Xue, S Chen, Z Tang - International Journal of Ad Hoc and \u2026, 2025", "abstract": "Large language models (LLMs) show promise in improving content distribution in mobile communication networks, but their performance heavily depends on input prompts. Manually crafting effective prompts for complex tasks is time-consuming \u2026"}]
