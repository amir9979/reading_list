[{"title": "UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare", "link": "https://arxiv.org/pdf/2506.06977", "details": "P Hu, X Han, F Wang, Y Ning - arXiv preprint arXiv:2506.06977, 2025", "abstract": "Domain generalization has become a critical challenge in clinical prediction, where patient cohorts often exhibit shifting data distributions that degrade model performance. Typical domain generalization approaches struggle in real-world \u2026", "entry_id": "http://arxiv.org/abs/2506.06977v1", "updated": "2025-06-08 03:20:34", "published": "2025-06-08 03:20:34", "authors": "Pengfei Hu;Xiaoxue Han;Fei Wang;Yue Ning", "summary": "Domain generalization has become a critical challenge in clinical prediction,\nwhere patient cohorts often exhibit shifting data distributions that degrade\nmodel performance. Typical domain generalization approaches struggle in\nreal-world healthcare settings for two main reasons: (1) patient-specific\ndomain labels are typically unavailable, making domain discovery especially\ndifficult; (2) purely data-driven approaches overlook key clinical insights,\nleading to a gap in medical knowledge integration. To address these problems,\nwe leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to\ngroup diseases into higher-level categories and discover more flexible latent\ndomains. In this paper, we introduce UdonCare, a hierarchy-guided framework\nthat iteratively prunes fine-grained domains, encodes these refined domains,\nand applies a Siamese-type inference mechanism to separate domain-related\nsignals from patient-level features. Experimental results on clinical datasets\n(MIMIC-III and MIMIC-IV) show that the proposed model achieves higher\nperformance compared to other domain generalization baselines when substantial\ndomain gaps presents, highlighting the untapped potential of medical knowledge\nfor enhancing domain generalization in practical healthcare applications.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2506.06977v1;http://arxiv.org/pdf/2506.06977v1", "pdf_url": "http://arxiv.org/pdf/2506.06977v1"}, {"title": "GT-STAFG: Graph Transformer with Spatiotemporal Attention Fusion Gate for Epileptic Seizure Detection in Imbalanced EEG Data", "link": "https://www.mdpi.com/2673-2688/6/6/120", "details": "MS Nafea, ZH Ismail - AI, 2025", "abstract": "Background: Electroencephalography (EEG) assists clinicians in diagnosing epileptic seizures by recording brain electrical activity. Existing models process spatiotemporal features inefficiently either through cascaded spatiotemporal \u2026"}, {"title": "SDoH-GPT: using large language models to extract social determinants of health", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocaf094/8159915", "details": "B Consoli, H Wang, X Wu, S Wang, X Zhao, Y Wang\u2026 - Journal of the American \u2026, 2025", "abstract": "Objective Extracting social determinants of health (SDoHs) from medical notes depends heavily on labor-intensive annotations, which are typically task-specific, hampering reusability and limiting sharing. Here, we introduce SDoH-GPT, a novel \u2026"}, {"title": "Clinical Information Extraction with Large Language Models: A Case Study on Organ Procurement", "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12099322/", "details": "H Adam, J Lin, J Lin, H Keenan, A Wilson, M Ghassemi - AMIA Annual Symposium \u2026, 2025", "abstract": "Recent work has demonstrated that large language models (LLMs) are powerful tools for clinical information extraction from unstructured text. However, existing approaches have largely ignored the extraction of numeric information such as \u2026"}, {"title": "Beyond RAG: Reinforced Reasoning Augmented Generation for Clinical Notes", "link": "https://arxiv.org/pdf/2506.05386", "details": "LPY Ting, C Zhao, YH Zeng, YJ Lim, KT Chuang - arXiv preprint arXiv:2506.05386, 2025", "abstract": "Clinical note generation aims to automatically produce free-text summaries of a patient's condition and diagnostic process, with discharge instructions being a representative long-form example. While recent large language model (LLM)-based \u2026", "entry_id": "http://arxiv.org/abs/2506.05386v1", "updated": "2025-06-03 12:59:52", "published": "2025-06-03 12:59:52", "authors": "Lo Pang-Yun Ting;Chengshuai Zhao;Yu-Hua Zeng;Yuan Jee Lim;Kun-Ta Chuang", "summary": "Clinical note generation aims to automatically produce free-text summaries of\na patient's condition and diagnostic process, with discharge instructions being\na representative long-form example. While recent large language model\n(LLM)-based methods pre-trained on general clinical corpora show promise in\nclinical text generation, they fall short in producing long-form notes from\nlimited patient information. In this paper, we propose R2AG, the first\nreinforced retriever for long-form discharge instruction generation based on\npre-admission data. R2AG is trained with reinforcement learning to retrieve\nreasoning paths from a medical knowledge graph, providing explicit semantic\nguidance to the LLM. To bridge the information gap, we propose Group-Based\nRetriever Optimization (GRO) which improves retrieval quality with\ngroup-relative rewards, encouraging reasoning leaps for deeper inference by the\nLLM. Comprehensive experiments on the MIMIC-IV-Note dataset show that R2AG\noutperforms baselines in both clinical efficacy and natural language generation\nmetrics. Further analysis reveals that R2AG fills semantic gaps in sparse input\nscenarios, and retrieved reasoning paths help LLMs avoid clinical\nmisinterpretation by focusing on key evidence and following coherent reasoning.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.05386v1;http://arxiv.org/pdf/2506.05386v1", "pdf_url": "http://arxiv.org/pdf/2506.05386v1"}]
