[{"title": "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "link": "https://aclanthology.org/2024.findings-emnlp.61.pdf", "details": "X Wei, H Chen, H Yu, H Fei, Q Liu - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have achieved notable success in commonsense reasoning tasks, benefiting from their extensive world knowledge acquired through extensive pretraining. While approaches like Chain-of-Thought \u2026"}, {"title": "LM2: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.920.pdf", "details": "G Juneja, S Dutta, T Chakraborty - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}, {"title": "Evaluating Language Models as Synthetic Data Generators", "link": "https://arxiv.org/pdf/2412.03679", "details": "S Kim, J Suk, X Yue, V Viswanathan, S Lee, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data \u2026"}, {"title": "Using a Two-Parameter Sensitivity Analysis Framework to Efficiently Combine Randomized and Non-randomized Studies", "link": "https://arxiv.org/pdf/2412.03731", "details": "R Yu, B Karmakar, J Vandeleest, EB Schwarz - arXiv preprint arXiv:2412.03731, 2024", "abstract": "Causal inference is vital for informed decision-making across fields such as biomedical research and social sciences. Randomized controlled trials (RCTs) are considered the gold standard for the internal validity of inferences, whereas \u2026"}, {"title": "ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large Language Models for Code Generation", "link": "https://arxiv.org/pdf/2411.07112", "details": "X Jiang, Y Dong, Y Tao, H Liu, Z Jin, W Jiao, G Li - arXiv preprint arXiv:2411.07112, 2024", "abstract": "Large language models (LLMs) have achieved impressive performance in code generation recently, offering programmers revolutionary assistance in software development. However, due to the auto-regressive nature of LLMs, they are \u2026"}, {"title": "CoCoP: Enhancing Text Classification with LLM through Code Completion Prompt", "link": "https://arxiv.org/pdf/2411.08979", "details": "MM Mohajeri, MJ Dousti, MN Ahmadabadi - arXiv preprint arXiv:2411.08979, 2024", "abstract": "Text classification is a fundamental task in natural language processing (NLP), and large language models (LLMs) have demonstrated their capability to perform this task across various domains. However, the performance of LLMs heavily depends on \u2026"}, {"title": "An Extensive Evaluation of Factual Consistency in Large Language Models for Data-to-Text Generation", "link": "https://arxiv.org/pdf/2411.19203", "details": "J Mahapatra, U Garain - arXiv preprint arXiv:2411.19203, 2024", "abstract": "Large Language Models (LLMs) have shown exceptional performance across various Data-to-Text Generation (DTG) tasks. However, generating factually consistent text in DTG remains challenging for LLMs. Despite this, in-depth \u2026"}, {"title": "Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models", "link": "https://arxiv.org/pdf/2412.02980", "details": "A Havrilla, A Dai, L O'Mahony, K Oostermeijer, V Zisler\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it \u2026"}]
