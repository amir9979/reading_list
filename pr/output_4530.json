[{"title": "Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models: Enhancing Performance and Reducing Inference Costs", "link": "https://arxiv.org/pdf/2407.00945", "details": "E Liu, J Zhu, Z Lin, X Ning, MB Blaschko, S Yan, G Dai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancement of large language models (LLMs) has led to architectures with billions to trillions of parameters, posing significant deployment challenges due to their substantial demands on memory, processing power, and energy \u2026"}, {"title": "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "link": "https://arxiv.org/pdf/2407.02118", "details": "W Zheng, W Pan, X Xu, L Qin, L Yue, M Zhou - arXiv preprint arXiv:2407.02118, 2024", "abstract": "In recent years, Large Language Models (LLMs) have made significant strides towards Artificial General Intelligence. However, training these models from scratch requires substantial computational resources and vast amounts of text data. In this \u2026"}, {"title": "LLM-Select: Feature Selection with Large Language Models", "link": "https://arxiv.org/pdf/2407.02694", "details": "DP Jeong, ZC Lipton, P Ravikumar - arXiv preprint arXiv:2407.02694, 2024", "abstract": "In this paper, we demonstrate a surprising capability of large language models (LLMs): given only input feature names and a description of a prediction task, they are capable of selecting the most predictive features, with performance rivaling the \u2026"}, {"title": "Large Language Models in the Clinic: A Comprehensive Benchmark", "link": "https://www.researchgate.net/profile/Fenglin-Liu-2/publication/381732507_Large_Language_Models_in_the_Clinic_A_Comprehensive_Benchmark/links/667c54891dec0c3c6fa5bee9/Large-Language-Models-in-the-Clinic-A-Comprehensive-Benchmark.pdf", "details": "F Liu, H Zhou, Y Hua, O Rohanian, A Thakur, L Clifton\u2026", "abstract": "The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the closeended question- answering (QA) task with answer options for evaluation. However, many clinical \u2026"}, {"title": "Universal Approximation Theory: The basic theory for large language models", "link": "https://arxiv.org/pdf/2407.00958", "details": "W Wang, Q Li - arXiv preprint arXiv:2407.00958, 2024", "abstract": "Language models have emerged as a critical area of focus in artificial intelligence, particularly with the introduction of groundbreaking innovations like ChatGPT. Large- scale Transformer networks have quickly become the leading approach for \u2026"}, {"title": "$\\text {Memory}^ 3$: Language Modeling with Explicit Memory", "link": "https://arxiv.org/pdf/2407.01178", "details": "H Yang, Z Lin, W Wang, H Wu, Z Li, B Tang, W Wei\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The training and inference of large language models (LLMs) are together a costly process that transports knowledge from raw data to meaningful computation. Inspired by the memory hierarchy of the human brain, we reduce this cost by equipping LLMs \u2026"}, {"title": "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models", "link": "https://arxiv.org/pdf/2407.01009", "details": "J Pan, Y Zhang, C Zhang, Z Liu, H Wang, H Li - arXiv preprint arXiv:2407.01009, 2024", "abstract": "Large language models (LLMs) have demonstrated emergent capabilities across diverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However, such a simple and fast COT approach often encounters limitations in dealing with \u2026"}]
