[{"title": "Personalizing Low-Rank Bayesian Neural Networks Via Federated Learning", "link": "https://arxiv.org/pdf/2410.14390", "details": "B Zhang, D Liu, O Simeone, G Wang, D Pezaros, G Zhu - arXiv preprint arXiv \u2026, 2024", "abstract": "To support real-world decision-making, it is crucial for models to be well-calibrated, ie, to assign reliable confidence estimates to their predictions. Uncertainty quantification is particularly important in personalized federated learning (PFL), as \u2026"}, {"title": "Evolutionary Contrastive Distillation for Language Model Alignment", "link": "https://arxiv.org/pdf/2410.07513", "details": "J Katz-Samuels, Z Li, H Yun, P Nigam, Y Xu, V Petricek\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The ability of large language models (LLMs) to execute complex instructions is essential for their real-world applications. However, several recent studies indicate that LLMs struggle with challenging instructions. In this paper, we propose \u2026"}, {"title": "FaithEval: Can Your Language Model Stay Faithful to Context, Even If\" The Moon is Made of Marshmallows\"", "link": "https://arxiv.org/pdf/2410.03727", "details": "Y Ming, S Purushwalkam, S Pandit, Z Ke, XP Nguyen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Ensuring faithfulness to context in large language models (LLMs) and retrieval- augmented generation (RAG) systems is crucial for reliable deployment in real-world applications, as incorrect or unsupported information can erode user trust. Despite \u2026"}, {"title": "Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training", "link": "https://arxiv.org/pdf/2410.04439", "details": "W Li, G Li, Z Lan, X Xu, W Zhuang, J Liu, X Xiao, J Su - arXiv preprint arXiv \u2026, 2024", "abstract": "Diffusion-based text-to-image models have demonstrated impressive achievements in diversity and aesthetics but struggle to generate images with legible visual texts. Existing backbone models have limitations such as misspelling, failing to generate \u2026"}, {"title": "TAEGAN: Generating Synthetic Tabular Data For Data Augmentation", "link": "https://arxiv.org/pdf/2410.01933%3F", "details": "J Li, Z Zhao, K Yee, U Javaid, B Sikdar - arXiv preprint arXiv:2410.01933, 2024", "abstract": "Synthetic tabular data generation has gained significant attention for its potential in data augmentation, software testing and privacy-preserving data sharing. However, most research has primarily focused on larger datasets and evaluating their quality in \u2026"}, {"title": "ImageNet-RIB Benchmark: Large Pre-Training Datasets Don't Guarantee Robustness after Fine-Tuning", "link": "https://openreview.net/pdf%3Fid%3DwpCiNhn2sC", "details": "J Hwang, B Cheung, ZW Hong, A Boopathy, P Agrawal\u2026 - \u2026 2024 Workshop on Fine-Tuning in \u2026", "abstract": "Highly performant large-scale pre-trained models promise to also provide a valuable foundation for learning specialized tasks, by fine-tuning the model to the desired task. By starting from a good general-purpose model, the goal is to achieve both \u2026"}, {"title": "Falcon Mamba: The First Competitive Attention-free 7B Language Model", "link": "https://arxiv.org/pdf/2410.05355", "details": "J Zuo, M Velikanov, DE Rhaiem, I Chahed, Y Belkada\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this technical report, we present Falcon Mamba 7B, a new base large language model based on the novel Mamba architecture. Falcon Mamba 7B is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model \u2026"}, {"title": "Law of the Weakest Link: Cross Capabilities of Large Language Models", "link": "https://arxiv.org/pdf/2409.19951", "details": "M Zhong, A Zhang, X Wang, R Hou, W Xiong, C Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real \u2026"}, {"title": "Repeatability of Fine-tuning Large Language Models Illustrated Using QLoRA", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10700744.pdf", "details": "SS Alahmari, LO Hall, PR Mouton, DB Goldgof - IEEE Access, 2024", "abstract": "Large language models (LLMs) have shown progress and promise in diverse applications ranging from the medical field to chat bots. Developing LLMs requires a large corpus of data and significant computation resources to achieve efficient \u2026"}]
