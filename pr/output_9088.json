[{"title": "Evaluation and mitigation of cognitive biases in medical language models", "link": "https://www.nature.com/articles/s41746-024-01283-6", "details": "S Schmidgall, C Harris, I Essien, D Olshvang\u2026 - npj Digital Medicine, 2024", "abstract": "Increasing interest in applying large language models (LLMs) to medicine is due in part to their impressive performance on medical exam questions. However, these exams do not capture the complexity of real patient\u2013doctor interactions because of \u2026"}, {"title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models", "link": "https://arxiv.org/pdf/2410.18252", "details": "M Noukhovitch, S Huang, S Xhonneux, A Hosseini\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this \u2026"}, {"title": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning", "link": "https://arxiv.org/pdf/2411.05193", "details": "J Hong, A Dragan, S Levine - arXiv preprint arXiv:2411.05193, 2024", "abstract": "Value-based reinforcement learning (RL) can in principle learn effective policies for a wide range of multi-turn problems, from games to dialogue to robotic control, including via offline RL from static previously collected datasets. However, despite \u2026"}, {"title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models", "link": "https://arxiv.org/pdf/2410.18785%3F", "details": "Q Li, X Liu, Z Tang, P Dong, Z Li, X Pan, X Chu - arXiv preprint arXiv:2410.18785, 2024", "abstract": "Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality, with many methods excelling across these criteria. Some \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers", "link": "https://openreview.net/pdf%3Fid%3D67tRrjgzsh", "details": "X Lu, Y Zhao, B Qin, L Huo, Q Yang, D Xu - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Accelerating Blockwise Parallel Language Models with Draft Refinement", "link": "https://openreview.net/pdf%3Fid%3DKT6F5Sw0eg", "details": "T Kim, AT Suresh, KA Papineni, M Riley, S Kumar\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Autoregressive language models have achieved remarkable advancements, yet their potential is often limited by the slow inference speeds associated with sequential token generation. Blockwise parallel decoding (BPD) was proposed by Stern et \u2026"}, {"title": "Code-switching finetuning: Bridging multilingual pretrained language models for enhanced cross-lingual performance", "link": "https://www.sciencedirect.com/science/article/pii/S0952197624016907", "details": "C Zan, L Ding, L Shen, Y Cao, W Liu - Engineering Applications of Artificial \u2026, 2025", "abstract": "In recent years, the development of pre-trained models has significantly propelled advancements in natural language processing. However, multilingual sequence-to- sequence pretrained language models (Seq2Seq PLMs) are pretrained on a wide \u2026"}, {"title": "Mathematical Reasoning via Multi-step Self Questioning and Answering for Small Language Models", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9440-9_7", "details": "K Chen, J Wang, X Zhang - CCF International Conference on Natural Language \u2026, 2024", "abstract": "Mathematical reasoning is challenging for large language models (LLMs), while the scaling relationship concerning LLM capacity is under-explored. Existing works have tried to leverage the rationales of LLMs to train small language models (SLMs) for \u2026"}, {"title": "Optimizing Fine-Tuning in Quantized Language Models: An In-Depth Analysis of Key Variables", "link": "https://cdn.techscience.cn/files/cmc/2024/online/CMC1030/TSP_CMC_57491/TSP_CMC_57491.pdf", "details": "A Shen, Z Lai, D Li, X Hu - 2024", "abstract": "ABSTRACT Large-scale Language Models (LLMs) have achieved significant breakthroughs in Natural Language Processing (NLP), driven by the pre-training and fine-tuning paradigm. While this approach allows models to specialize in specific \u2026"}]
