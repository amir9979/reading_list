'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [MedDr: Diagnosis-Guided Bootstrapping for Large-Scale '
[{"title": "Photo-Realistic Image Restoration in the Wild with Controlled Vision-Language Models", "link": "https://arxiv.org/pdf/2404.09732", "details": "Z Luo, FK Gustafsson, Z Zhao, J Sj\u00f6lund, TB Sch\u00f6n - arXiv preprint arXiv:2404.09732, 2024", "abstract": "Though diffusion models have been successfully applied to various image restoration (IR) tasks, their performance is sensitive to the choice of training datasets. Typically, diffusion models trained in specific datasets fail to recover images that \u2026"}, {"title": "Backdoor Removal for Generative Large Language Models", "link": "https://arxiv.org/pdf/2405.07667", "details": "H Li, Y Chen, Z Zheng, Q Hu, C Chan, H Liu, Y Song - arXiv preprint arXiv \u2026, 2024", "abstract": "With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased \u2026"}, {"title": "Population-based deep image prior for dynamic PET denoising: A data-driven approach to improve parametric quantification", "link": "https://www.sciencedirect.com/science/article/pii/S1361841524001051", "details": "Q Liu, YJ Tsai, JD Gallezot, X Guo, MK Chen, D Pucar\u2026 - Medical Image Analysis, 2024", "abstract": "The high noise level of dynamic Positron Emission Tomography (PET) images degrades the quality of parametric images. In this study, we aim to improve the quality and quantitative accuracy of K i images by utilizing deep learning techniques \u2026"}, {"title": "Parameter-Efficient Instruction Tuning of Large Language Models For Extreme Financial Numeral Labelling", "link": "https://arxiv.org/pdf/2405.06671", "details": "S Khatuya, R Mukherjee, A Ghosh, M Hegde\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We study the problem of automatically annotating relevant numerals (GAAP metrics) occurring in the financial documents with their corresponding XBRL tags. Different from prior works, we investigate the feasibility of solving this extreme classification \u2026"}, {"title": "MoE-TinyMed: Mixture of Experts for Tiny Medical Large Vision-Language Models", "link": "https://arxiv.org/pdf/2404.10237", "details": "S Jiang, T Zheng, Y Zhang, Y Jin, Z Liu - arXiv preprint arXiv:2404.10237, 2024", "abstract": "Mixture of Expert Tuning (MoE-Tuning) has effectively enhanced the performance of general MLLMs with fewer parameters, yet its application in resource-limited medical settings has not been fully explored. To address this gap, we developed MoE \u2026"}, {"title": "Phi-3 technical report: A highly capable language model locally on your phone", "link": "https://arxiv.org/pdf/2404.14219%3Ftrk%3Dpublic_post_comment-text", "details": "M Abdin, SA Jacobs, AA Awan, J Aneja, A Awadallah\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT \u2026"}, {"title": "SigBart: Enhanced Pre-training via Salient Content Representation Learning for Social Media Summarization", "link": "https://dl.acm.org/doi/abs/10.1145/3589335.3652505", "details": "S Sotudeh, N Goharian - Companion Proceedings of the ACM on Web \u2026, 2024", "abstract": "Our approach to automatically summarizing online mental health posts could help counselors by reducing their reading time, enabling quicker and more effective support for individuals seeking mental health assistance. Neural text summarization \u2026"}, {"title": "VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with Lightweight Blocks", "link": "https://arxiv.org/pdf/2405.06196", "details": "M Dhakal, R Adhikari, S Thapaliya, B Khanal - arXiv preprint arXiv:2405.06196, 2024", "abstract": "Foundation Vision-Language Models (VLMs) trained using large-scale open-domain images and text pairs have recently been adapted to develop Vision-Language Segmentation Models (VLSMs) that allow providing text prompts during inference to \u2026"}]
