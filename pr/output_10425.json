[{"title": "Evaluating the role of pre-training dataset size and diversity on single-cell foundation model performance", "link": "https://www.biorxiv.org/content/10.1101/2024.12.13.628448.full.pdf", "details": "A DenAdel, M Hughes, A Thoutam, A Gupta, AW Navia\u2026 - bioRxiv, 2024", "abstract": "The success of transformer-based foundation models on natural language and images has motivated their use in single-cell biology. Single-cell foundation models have been trained on increasingly larger transcriptomic datasets, scaling from initial \u2026"}, {"title": "Interpretable LLM-based Table Question Answering", "link": "https://arxiv.org/pdf/2412.12386", "details": "I Brugere, S Sharma, S Kariyappa, AT Nguyen, F Lecue - arXiv preprint arXiv \u2026, 2024", "abstract": "Interpretability for Table Question Answering (Table QA) is critical, particularly in high- stakes industries like finance or healthcare. Although recent approaches using Large Language Models (LLMs) have significantly improved Table QA performance, their \u2026"}, {"title": "TimeXAI: Concept-Based Counterfactual Explanations for Time Series", "link": "https://openreview.net/pdf%3Fid%3D2hS9zLkQQ8", "details": "K Oublal, D BENHAIEM, E LE BORGNE - AAAI 2025 Workshop on Artificial Intelligence with \u2026", "abstract": "Explaining AI systems operating on time series data is crucial in many decision- making areas, such as healthcare, energy, and public policy-making, which requires interpretable and transparent explanations to overcome the black-box nature of \u2026"}, {"title": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over Aligned Large Language Models", "link": "https://arxiv.org/pdf/2412.12865", "details": "Y Fan, Y Hong, Q Wang, J Bao, H Jiang, Y Song - arXiv preprint arXiv:2412.12865, 2024", "abstract": "Alignment, endowing a pre-trained Large language model (LLM) with the ability to follow instructions, is crucial for its real-world applications. Conventional supervised fine-tuning (SFT) methods formalize it as causal language modeling typically with a \u2026"}, {"title": "Assessing the Limitations of Large Language Models in Clinical Fact Decomposition", "link": "https://arxiv.org/pdf/2412.12422", "details": "M Munnangi, A Swaminathan, JA Fries, J Jindal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Verifying factual claims is critical for using large language models (LLMs) in healthcare. Recent work has proposed fact decomposition, which uses LLMs to rewrite source text into concise sentences conveying a single piece of information, as \u2026"}, {"title": "XTransplant: A Probe into the Upper Bound Performance of Multilingual Capability and Culture Adaptability in LLMs via Mutual Cross-lingual Feed-forward \u2026", "link": "https://arxiv.org/pdf/2412.12686", "details": "Y Ye, X Feng, X Feng, L Qin, Y Huang, L Huang, W Ma\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Current large language models (LLMs) often exhibit imbalances in multilingual capabilities and cultural adaptability, largely due to their English-centric pretraining data. To address this imbalance, we propose a probing method named XTransplant \u2026"}, {"title": "Balrog: Benchmarking agentic llm and vlm reasoning on games", "link": "https://arxiv.org/pdf/2411.13543", "details": "D Paglieri, B Cupia\u0142, S Coward, U Piterbarg, M Wolczyk\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities; however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require \u2026"}]
