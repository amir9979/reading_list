[{"title": "In-Context Learning of Physical Properties: Few-Shot Adaptation to Out-of-Distribution Molecular Graphs", "link": "https://arxiv.org/pdf/2406.01808", "details": "G Kaszuba, AD Naghdi, D Massa, S Papanikolaou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models manifest the ability of few-shot adaptation to a sequence of provided examples. This behavior, known as in-context learning, allows for performing nontrivial machine learning tasks during inference only. In this work, we \u2026"}, {"title": "XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples", "link": "https://arxiv.org/pdf/2405.05116", "details": "P Lin, AFT Martins, H Sch\u00fctze - arXiv preprint arXiv:2405.05116, 2024", "abstract": "Recent studies have shown that leveraging off-the-shelf or fine-tuned retrievers, capable of retrieving high-quality in-context examples, significantly improves in- context learning of English. However, adapting these methods to other languages \u2026"}, {"title": "TAIA: Large Language Models are Out-of-Distribution Data Learners", "link": "https://arxiv.org/pdf/2405.20192", "details": "S Jiang, Y Liao, Y Zhang, Y Wang, Y Wang - arXiv preprint arXiv:2405.20192, 2024", "abstract": "Fine-tuning on task-specific question-answer pairs is a predominant method for enhancing the performance of instruction-tuned large language models (LLMs) on downstream tasks. However, in certain specialized domains, such as healthcare or \u2026"}, {"title": "Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction", "link": "https://arxiv.org/pdf/2406.00755", "details": "X Li, W Wang, M Li, J Guo, Y Zhang, F Feng - arXiv preprint arXiv:2406.00755, 2024", "abstract": "The rapid advancement of Large Language Models (LLMs) in the realm of mathematical reasoning necessitates comprehensive evaluations to gauge progress and inspire future directions. Existing assessments predominantly focus on problem \u2026"}, {"title": "Unraveling and Mitigating Retriever Inconsistencies in Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2405.20680", "details": "M Li, X Li, Y Chen, W Xuan, W Zhang - arXiv preprint arXiv:2405.20680, 2024", "abstract": "Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their superiority in terms of factuality, they do not consistently outperform the original retrieval-free Language Models (LMs). Our experiments reveal that this example \u2026"}]
