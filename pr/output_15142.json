[{"title": "Evaluating Transformers Learning by Representing Self-Attention Weights as a Graph", "link": "https://www.scitepress.org/Papers/2025/131114/131114.pdf", "details": "R Leygonie, S Lobry, L Wendling", "abstract": "Transformers architectures have established themselves as the state of the art for sequential data processing, with applications ranging from machine translation to the processing of Electronic Health Records (EHR). These complex data present a \u2026"}]
