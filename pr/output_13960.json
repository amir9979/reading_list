[{"title": "Mapping 1,000+ Language Models via the Log-Likelihood Vector", "link": "https://arxiv.org/pdf/2502.16173", "details": "M Oyama, H Yamagiwa, Y Takase, H Shimodaira - arXiv preprint arXiv:2502.16173, 2025", "abstract": "To compare autoregressive language models at scale, we propose using log- likelihood vectors computed on a predefined text set as model features. This approach has a solid theoretical basis: when treated as model coordinates, their \u2026"}, {"title": "Process-based Self-Rewarding Language Models", "link": "https://arxiv.org/pdf/2503.03746", "details": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' \u2026"}, {"title": "Chain-of-Thought Matters: Improving Long-Context Language Models with Reasoning Path Supervision", "link": "https://arxiv.org/pdf/2502.20790", "details": "D Zhu, X Wei, G Zhao, W Wu, H Zou, J Ran, X Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in Large Language Models (LLMs) have highlighted the challenge of handling long-context tasks, where models need to reason over extensive input contexts to aggregate target information. While Chain-of-Thought (CoT) prompting \u2026"}, {"title": "Language Models Can Predict Their Own Behavior", "link": "https://arxiv.org/pdf/2502.13329", "details": "D Ashok, J May - arXiv preprint arXiv:2502.13329, 2025", "abstract": "Autoregressive Language Models output text by sequentially predicting the next token to generate, with modern methods like Chain-of-Thought (CoT) prompting achieving state-of-the-art reasoning capabilities by scaling the number of generated \u2026"}, {"title": "Algorithmic Bias from the Perspectives of Healthcare Professionals", "link": "https://www.scitepress.org/Papers/2025/130765/130765.pdf", "details": "J Xu, T Babaian", "abstract": "This paper focuses on algorithmic bias of machine learning and artificial intelligence applications in healthcare information systems. Based on the quantitative data and qualitative comments from a survey and interviews with healthcare professionals \u2026"}, {"title": "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness", "link": "https://arxiv.org/pdf/2503.09487", "details": "B Zhu, J Cui, H Zhang, C Zhang - arXiv preprint arXiv:2503.09487, 2025", "abstract": "While image-text foundation models have succeeded across diverse downstream tasks, they still face challenges in the presence of spurious correlations between the input and label. To address this issue, we propose a simple three-step approach \u2026"}, {"title": "Adapting Generative Large Language Models for Information Extraction from Unstructured Electronic Health Records in Residential Aged Care: A Comparative \u2026", "link": "https://link.springer.com/article/10.1007/s41666-025-00190-z", "details": "D Vithanage, C Deng, L Wang, M Yin, M Alkhalaf\u2026 - Journal of Healthcare \u2026, 2025", "abstract": "Abstract Information extraction (IE) of unstructured electronic health records is challenging due to the semantic complexity of textual data. Generative large language models (LLMs) offer promising solutions to address this challenge \u2026"}, {"title": "Towards label-only membership inference attack against pre-trained large language models", "link": "https://arxiv.org/pdf/2502.18943", "details": "Y He, B Li, L Liu, Z Ba, W Dong, Y Li, Z Qin, K Ren\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Membership Inference Attacks (MIAs) aim to predict whether a data sample belongs to the model's training set or not. Although prior research has extensively explored MIAs in Large Language Models (LLMs), they typically require accessing to complete \u2026"}, {"title": "Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation", "link": "https://arxiv.org/pdf/2502.11306", "details": "H Nguyen, Z He, SA Gandre, U Pasupulety\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) often suffer from hallucination, generating factually incorrect or ungrounded content, which limits their reliability in high-stakes applications. A key factor contributing to hallucination is the use of hard labels during \u2026"}]
