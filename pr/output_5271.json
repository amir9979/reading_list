[{"title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge", "link": "https://arxiv.org/pdf/2407.19594", "details": "T Wu, W Yuan, O Golovneva, J Xu, Y Tian, J Jiao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can \u2026"}, {"title": "Language Models Don't Learn the Physical Manifestation of Language", "link": "https://aclanthology.org/2024.acl-long.195.pdf", "details": "B Lee, J Lim - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "We argue that language-only models don't learn the physical manifestation of language. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-Test. These tasks highlight a \u2026"}, {"title": "Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models", "link": "https://aclanthology.org/2024.acl-long.765.pdf", "details": "S Li, R Lin, S Pei - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities in production. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer \u2026"}, {"title": "DOSSIER: Fact checking in electronic health records while preserving patient privacy", "link": "https://www.amazon.science/publications/dossier-fact-checking-in-electronic-health-records-while-preserving-patient-privacy", "details": "H Zhang, S Nagesh, M Shyani, N Mishra - 2024", "abstract": "Given a particular claim about a specific document, the fact checking problem is to determine if the claim is true and, if so, provide corroborating evidence. The problem is motivated by contexts where a document is too lengthy to quickly read and find an \u2026"}, {"title": "RT-Surv: Improving Mortality Prediction After Radiotherapy with Large Language Model Structuring of Large-Scale Unstructured Electronic Health Records", "link": "https://arxiv.org/pdf/2408.05074", "details": "S Park, CW Wee, SH Choi, KH Kim, JS Chang, HI Yoon\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Accurate patient selection is critical in radiotherapy (RT) to prevent ineffective treatments. Traditional survival prediction models, relying on structured data, often lack precision. This study explores the potential of large language models (LLMs) to \u2026"}, {"title": "DocLens: Multi-aspect Fine-grained Medical Text Evaluation", "link": "https://aclanthology.org/2024.acl-long.39.pdf", "details": "Y Xie, S Zhang, H Cheng, P Liu, Z Gero, C Wong\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Medical text generation aims to assist with administrative work and highlight salient information to support decision-making. To reflect the specific requirements of medical text, in this paper, we propose a set of metrics to evaluate the completeness \u2026"}, {"title": "The overview of the BioRED (Biomedical Relation Extraction Dataset) track at BioCreative VIII", "link": "https://academic.oup.com/database/article/doi/10.1093/database/baae069/7729400", "details": "R Islamaj, PT Lai, CH Wei, L Luo, T Almeida\u2026 - Database, 2024", "abstract": "Abstract The BioRED track at BioCreative VIII calls for a community effort to identify, semantically categorize, and highlight the novelty factor of the relationships between biomedical entities in unstructured text. Relation extraction is crucial for many \u2026"}, {"title": "Boosting entity recognition by leveraging cross-task domain models for weak supervision", "link": "https://www.amazon.science/publications/boosting-entity-recognition-by-leveraging-cross-task-domain-models-for-weak-supervision", "details": "S Agrawal, S Merugu, V Sembium - 2024", "abstract": "Entity Recognition (ER) is a common natural language processing task encountered in a number of real-world applications. For common domains and named entities such as places and organisations, there exists sufficient high quality annotated data \u2026"}, {"title": "Self-Supervised Position Debiasing for Large Language Models", "link": "https://aclanthology.org/2024.findings-acl.170.pdf", "details": "Z Liu, Z Chen, M Zhang, Z Ren, P Ren, Z Chen - Findings of the Association for \u2026, 2024", "abstract": "Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Previous \u2026"}]
