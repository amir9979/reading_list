[{"title": "Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language Models", "link": "https://arxiv.org/pdf/2409.01584", "details": "S Ozaki, K Hayashi, Y Sakai, H Kamigaito, K Hayashi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As the performance of Large-scale Vision Language Models (LVLMs) improves, they are increasingly capable of responding in multiple languages, and there is an expectation that the demand for explanations generated by LVLMs will grow \u2026"}, {"title": "Effective prompt extraction from language models", "link": "https://openreview.net/pdf%3Fid%3D0o95CVdNuz", "details": "Y Zhang, N Carlini, D Ippolito - First Conference on Language Modeling, 2024", "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden \u2026"}, {"title": "Few-shot Adaptation of Medical Vision-Language Models", "link": "https://arxiv.org/pdf/2409.03868", "details": "F Shakeri, Y Huang, J Silva-Rodr\u00edguez, H Bahig\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Integrating image and text data through multi-modal learning has emerged as a new approach in medical imaging research, following its successful deployment in computer vision. While considerable efforts have been dedicated to establishing \u2026"}, {"title": "Making Large Vision Language Models to be Good Few-shot Learners", "link": "https://arxiv.org/pdf/2408.11297", "details": "F Liu, W Cai, J Huo, C Zhang, D Chen, J Zhou - arXiv preprint arXiv:2408.11297, 2024", "abstract": "Few-shot classification (FSC) is a fundamental yet challenging task in computer vision that involves recognizing novel classes from limited data. While previous methods have focused on enhancing visual features or incorporating additional \u2026"}, {"title": "Towards Analyzing and Mitigating Sycophancy in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2408.11261", "details": "Y Zhao, R Zhang, J Xiao, C Ke, R Hou, Y Hao, Q Guo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) have shown significant capability in vision- language understanding. However, one critical issue that persists in these models is sycophancy, which means models are unduly influenced by leading or deceptive \u2026"}, {"title": "SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for Large-scale Vision-Language Models", "link": "https://arxiv.org/pdf/2408.12114", "details": "Y Yu, S Chung, BK Lee, YM Ro - arXiv preprint arXiv:2408.12114, 2024", "abstract": "Large-scale Vision-Language Models (LVLMs) have significantly advanced with text- aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate \u2026"}, {"title": "Evaluating Attribute Comprehension in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2408.13898", "details": "H Zhang, Z Yang, Y Liu, X Wang, Z He, K Liang, Z Ma - arXiv preprint arXiv \u2026, 2024", "abstract": "Currently, large vision-language models have gained promising progress on many downstream tasks. However, they still suffer many challenges in fine-grained visual understanding tasks, such as object attribute comprehension. Besides, there have \u2026"}, {"title": "Understanding Defects in Generated Codes by Language Models", "link": "https://arxiv.org/pdf/2408.13372", "details": "AM Esfahani, N Kahani, SA Ajila - arXiv preprint arXiv:2408.13372, 2024", "abstract": "This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the \u2026"}, {"title": "Poly-Visual-Expert Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3D7QaEO9WYMa", "details": "X Fan, T Ji, S Li, S Jin, S Song, J Wang, B Hong\u2026 - First Conference on Language \u2026", "abstract": "Current large vision-language models (VLMs) frequently face challenges such as the limited capabilities of a single visual component and the excessive length of visual tokens. These issues can limit the model's ability to interpret complex visual \u2026"}]
