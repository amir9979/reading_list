'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [How Important is Domain Specificity in Language Models'
[{"title": "Smaller Language Models are Better Zero-shot Machine-Generated Text Detectors", "link": "https://aclanthology.org/2024.eacl-short.25.pdf", "details": "N Mireshghallah, J Mattern, S Gao, R Shokri\u2026 - Proceedings of the 18th \u2026, 2024", "abstract": "As large language models are becoming more embedded in different user-facing services, it is important to be able to distinguish between human-written and machine- generated text to verify the authenticity of news articles, product reviews, etc. Thus, in \u2026"}, {"title": "\" My Answer is C\": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models", "link": "https://arxiv.org/pdf/2402.14499", "details": "X Wang, B Ma, C Hu, L Weber-Genzel, P R\u00f6ttger\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The \u2026"}, {"title": "RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models", "link": "https://arxiv.org/html/2403.02271v1", "details": "S Najafi, A Fyshe - arXiv preprint arXiv:2403.02271, 2024", "abstract": "Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter- efficient fine-tuning methods that optimize input prompts or adjust a small number of \u2026"}, {"title": "Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models", "link": "https://arxiv.org/pdf/2402.13492", "details": "S Maekawa, H Iso, S Gurajada, N Bhutani - arXiv preprint arXiv:2402.13492, 2024", "abstract": "While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant \u2026"}, {"title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.07860", "details": "S Zhao, S Hao, B Zi, H Xu, KYK Wong - arXiv preprint arXiv:2403.07860, 2024", "abstract": "Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding \u2026"}, {"title": "Identifying prognostic factors for survival in intensive care unit patients with SIRS or sepsis by machine learning analysis on electronic health records", "link": "https://journals.plos.org/digitalhealth/article%3Fid%3D10.1371/journal.pdig.0000459", "details": "M Mollura, D Chicco, A Paglialonga, R Barbieri - PLOS Digital Health, 2024", "abstract": "Background Systemic inflammatory response syndrome (SIRS) and sepsis are the most common causes of in-hospital death. However, the characteristics associated with the improvement in the patient conditions during the ICU stay were not fully \u2026"}, {"title": "STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning", "link": "https://arxiv.org/html/2402.13468v1", "details": "N Beck, A Iyer, R Iyer - arXiv preprint arXiv:2402.13468, 2024", "abstract": "As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models. Active learning, which attempts to mine \u2026"}, {"title": "Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning", "link": "https://arxiv.org/pdf/2402.11537", "details": "Y Zhao, L Du, X Ding, K Xiong, Z Sun, J Shi, T Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of \u2026"}, {"title": "Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity", "link": "https://arxiv.org/pdf/2403.05257", "details": "S Hoshino, A Kato, S Murakami, P Zhang - arXiv preprint arXiv:2403.05257, 2024", "abstract": "Learning better sentence embeddings leads to improved performance for natural language understanding tasks including semantic textual similarity (STS) and natural language inference (NLI). As prior studies leverage large-scale labeled NLI datasets \u2026"}]
