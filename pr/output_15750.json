[{"title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability", "link": "https://arxiv.org/pdf/2504.07416%3F", "details": "J Park, S Kim, B Yoon, K Choi - arXiv preprint arXiv:2504.07416, 2025", "abstract": "Recent advancements in multi-modal models have significantly improved vision- language alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning, rely on low-resolution \u2026"}, {"title": "Style over Substance: Distilled Language Models Reason Via Stylistic Replication", "link": "https://arxiv.org/pdf/2504.01738", "details": "P Lippmann, J Yang - arXiv preprint arXiv:2504.01738, 2025", "abstract": "Specialized reasoning language models (RLMs) have demonstrated that scaling test- time computation through detailed reasoning traces significantly enhances performance. Although these traces effectively facilitate knowledge distillation into \u2026"}, {"title": "What, how, where, and how well? a survey on test-time scaling in large language models", "link": "https://arxiv.org/pdf/2503.24235%3F", "details": "Q Zhang, F Lyu, Z Sun, L Wang, W Zhang, Z Guo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as``test-time computing''has emerged as a prominent research focus. Recent studies demonstrate \u2026"}, {"title": "Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for Large Language Models", "link": "https://arxiv.org/pdf/2504.02273%3F", "details": "H Le, D Do, D Nguyen, S Venkatesh - arXiv preprint arXiv:2504.02273, 2025", "abstract": "Recent advances in fine-tuning large language models (LLMs) with reinforcement learning (RL) have shown promising improvements in complex reasoning tasks, particularly when paired with chain-of-thought (CoT) prompting. However, these \u2026"}, {"title": "Exploring Multimodal Language Models for Sustainability Disclosure Extraction: A Comparative Study", "link": "https://aclanthology.org/anthology-files/anthology-files/pdf/insights/2025.insights-1.pdf%23page%3D146", "details": "TK Gupta, T Goel, I Verma - The 5th Workshop on Insights from Negative Results in \u2026, 2025", "abstract": "Sustainability metrics have increasingly become a crucial non-financial criterion in investment decision-making. Organizations worldwide are recognizing the importance of sustainability and are proactively highlighting their efforts through \u2026"}, {"title": "Fast-Slow-Thinking: Complex Task Solving with Large Language Models", "link": "https://arxiv.org/pdf/2504.08690%3F", "details": "Y Sun, Y Zhang, Z Zhao, S Wan, D Tao, C Gong - arXiv preprint arXiv:2504.08690, 2025", "abstract": "Nowadays, Large Language Models (LLMs) have been gradually employed to solve complex tasks. To face the challenge, task decomposition has become an effective way, which proposes to divide a complex task into multiple simpler subtasks and \u2026"}, {"title": "Can large language models independently complete tasks? A dynamic evaluation framework for multi-turn task planning and completion", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225008070", "details": "J Gao, J Cui, H Wu, L Xiang, H Zhao, X Li, M Fang\u2026 - Neurocomputing, 2025", "abstract": "Large language models (LLMs) are increasingly relied upon for multi-turn dialogue to conduct complex tasks. However, existing benchmarks mainly evaluate LLMs as agents, overlooking their potential as independent systems to accomplish complex \u2026"}, {"title": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scale Test-Time Compute", "link": "https://arxiv.org/pdf/2504.00762%3F", "details": "J Chen, Z Xun, B Zhou, H Qi, Q Zhang, Y Chen, W Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper presents a simple, effective, and cost-efficient strategy to improve LLM performance by scaling test-time compute. Our strategy builds upon the repeated- sampling-then-voting framework, with a novel twist: incorporating multiple models \u2026"}, {"title": "Cognitive memory in large language models", "link": "https://arxiv.org/pdf/2504.02441", "details": "L Shan, S Luo, Z Zhu, Y Yuan, Y Wu - arXiv preprint arXiv:2504.02441, 2025", "abstract": "This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long \u2026"}]
