[{"title": "FSP-Laplace: Function-Space Priors for the Laplace Approximation in Bayesian Deep Learning", "link": "https://arxiv.org/pdf/2407.13711", "details": "T Cinquin, M Pf\u00f6rtner, V Fortuin, P Hennig, R Bamler - arXiv preprint arXiv \u2026, 2024", "abstract": "Laplace approximations are popular techniques for endowing deep networks with epistemic uncertainty estimates as they can be applied without altering the predictions of the neural network, and they scale to large models and datasets. While \u2026"}, {"title": "One Process Spatiotemporal Learning of Transformers via Vcls Token for Multivariate Time Series Forecasting", "link": "https://www.researchgate.net/profile/Jingzehua-Xu/publication/381739943_One_Process_Spatiotemporal_Learning_of_Transformers_via_Vcls_Token_for_Multivariate_Time_Series_Forecasting/links/667d11d5f3b61c4e2c8ebd08/One-Process-Spatiotemporal-Learning-of-Transformers-via-Vcls-Token-for-Multivariate-Time-Series-Forecasting.pdf", "details": "T Cai, H Wu, D Niu, X Xia, J Jiang, J Xu", "abstract": "Previous Transformer-based models for multivariate time series forecasting mainly focus on temporal dependence learning and neglect the association between variables. The recent method of adding Attention on spatial (variate) tokens before or \u2026"}]
