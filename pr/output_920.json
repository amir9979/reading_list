'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Investigating Regularization of Self-Play Language Mod'
[{"title": "Emergent Abilities in Reduced-Scale Generative Language Models", "link": "https://arxiv.org/pdf/2404.02204", "details": "S Muckatira, V Deshpande, V Lialin, A Rumshisky - arXiv preprint arXiv:2404.02204, 2024", "abstract": "Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study \u2026"}, {"title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2404.05567", "details": "B Pan, Y Shen, H Liu, M Mishra, G Zhang, A Oliva\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) language models can reduce computational costs by 2- 4$\\times $ compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios. However, MoE models generally \u2026"}, {"title": "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models", "link": "https://arxiv.org/pdf/2404.02575", "details": "H Chae, Y Kim, S Kim, KT Ong, B Kwak, M Kim, S Kim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution. Such nature of algorithmic reasoning makes it a challenge for large \u2026"}, {"title": "Enhancing Question Answering for Enterprise Knowledge Bases using Large Language Models", "link": "https://arxiv.org/pdf/2404.08695", "details": "F Jiang, C Qin, K Yao, C Fang, F Zhuang, H Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Efficient knowledge management plays a pivotal role in augmenting both the operational efficiency and the innovative capacity of businesses and organizations. By indexing knowledge through vectorization, a variety of knowledge retrieval \u2026"}, {"title": "Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning", "link": "https://arxiv.org/pdf/2404.00909", "details": "R Li, Y Wu, X He - arXiv preprint arXiv:2404.00909, 2024", "abstract": "Generative vision-language models (VLMs) have shown impressive performance in zero-shot vision-language tasks like image captioning and visual question answering. However, improving their zero-shot reasoning typically requires second \u2026"}, {"title": "EMPOWERING CONTINUAL ROBOT LEARNING THROUG GUIDED SKILL ACQUISITION WITH LANGUAGE MODELS", "link": "https://openreview.net/pdf%3Fid%3DSKjaT0bgYF", "details": "Z Li, K Yu, S Cheng, D Xu - First Workshop on Vision-Language Models for \u2026", "abstract": "To support daily human tasks, robots must handle complex, long-term tasks and continuously learn new skills. Deep reinforcement learning (DRL) offers potential for finegrained skill learning but faces challenges with long-horizon tasks and relies \u2026"}, {"title": "FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations", "link": "https://arxiv.org/pdf/2404.06619", "details": "J Dwivedi-Yu, R Dwivedi, T Schick - arXiv preprint arXiv:2404.06619, 2024", "abstract": "The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes \u2026"}, {"title": "Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning", "link": "https://arxiv.org/pdf/2404.03891", "details": "G Choi, H Ahn - arXiv preprint arXiv:2404.03891, 2024", "abstract": "In robotics, the use of Large Language Models (LLMs) is becoming prevalent, especially for understanding human commands. In particular, LLMs are utilized as domain-agnostic task planners for high-level human commands. LLMs are capable \u2026"}, {"title": "LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation", "link": "https://arxiv.org/pdf/2404.00998", "details": "Z Wang, X Luo, X Jiang, D Li, L Qiu - arXiv preprint arXiv:2404.00998, 2024", "abstract": "Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task's clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare \u2026"}]
