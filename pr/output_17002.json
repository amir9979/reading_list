[{"title": "Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster", "link": "https://arxiv.org/pdf/2505.18642", "details": "X Chen, S Zhou, K Liang, X Sun, X Liu - arXiv preprint arXiv:2505.18642, 2025", "abstract": "Chain-of-thought (CoT) distillation allows a large language model (LLM) to guide a small language model (SLM) in reasoning tasks. Existing methods train the SLM to learn the long rationale in one iteration, resulting in two issues: 1) Long rationales \u2026", "entry_id": "http://arxiv.org/abs/2505.18642v1", "updated": "2025-05-24 11:04:52", "published": "2025-05-24 11:04:52", "authors": "Xiao Chen;Sihang Zhou;Ke Liang;Xiaoyu Sun;Xinwang Liu", "summary": "Chain-of-thought (CoT) distillation allows a large language model (LLM) to\nguide a small language model (SLM) in reasoning tasks. Existing methods train\nthe SLM to learn the long rationale in one iteration, resulting in two issues:\n1) Long rationales lead to a large token-level batch size during training,\nmaking gradients of core reasoning tokens (i.e., the token will directly affect\nthe correctness of subsequent reasoning) over-smoothed as they contribute a\ntiny fraction of the rationale. As a result, the SLM converges to sharp minima\nwhere it fails to grasp the reasoning logic. 2) The response is slow, as the\nSLM must generate a long rationale before reaching the answer. Therefore, we\npropose chunk-wise training (CWT), which uses a heuristic search to divide the\nrationale into internal semantically coherent chunks and focuses SLM on\nlearning from only one chunk per iteration. In this way, CWT naturally isolates\nnon-reasoning chunks that do not involve the core reasoning token (e.g.,\nsummary and transitional chunks) from the SLM learning for reasoning chunks,\nmaking the fraction of the core reasoning token increase in the corresponding\niteration. Based on CWT, skip-thinking training (STT) is proposed. STT makes\nthe SLM automatically skip non-reasoning medium chunks to reach the answer,\nimproving reasoning speed while maintaining accuracy. We validate our approach\non a variety of SLMs and multiple reasoning tasks.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.18642v1;http://arxiv.org/pdf/2505.18642v1", "pdf_url": "http://arxiv.org/pdf/2505.18642v1"}, {"title": "Pretraining Language Models to Ponder in Continuous Space", "link": "https://arxiv.org/pdf/2505.20674", "details": "B Zeng, S Song, S Huang, Y Wang, H Li, Z He, X Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Humans ponder before articulating complex sentence elements, enabling deeper cognitive processing through focused effort. In this work, we introduce this pondering process into language models by repeatedly invoking the forward process within a \u2026", "entry_id": "http://arxiv.org/abs/2505.20674v1", "updated": "2025-05-27 03:47:33", "published": "2025-05-27 03:47:33", "authors": "Boyi Zeng;Shixiang Song;Siyuan Huang;Yixuan Wang;He Li;Ziwei He;Xinbing Wang;Zhiyu Li;Zhouhan Lin", "summary": "Humans ponder before articulating complex sentence elements, enabling deeper\ncognitive processing through focused effort. In this work, we introduce this\npondering process into language models by repeatedly invoking the forward\nprocess within a single token generation step. During pondering, instead of\ngenerating an actual token sampled from the prediction distribution, the model\nponders by yielding a weighted sum of all token embeddings according to the\npredicted token distribution. The generated embedding is then fed back as input\nfor another forward pass. We show that the model can learn to ponder in this\nway through self-supervised learning, without any human annotations. Our method\nis straightforward and can be seamlessly integrated with various existing\nlanguage models. Experiments across three widely used open-source\narchitectures-GPT-2, Pythia, and LLaMA-and extensive downstream task\nevaluations demonstrate the effectiveness and generality of our method. For\nlanguage modeling tasks, pondering language models achieve performance\ncomparable to vanilla models with twice the number of parameters. On 9\ndownstream benchmarks, our pondering-enhanced Pythia models significantly\noutperform the official Pythia models. Notably, pondering-enhanced Pythia-1B is\ncomparable to TinyLlama-1.1B, which is trained on 10 times more data. The code\nis available at https://github.com/LUMIA-Group/PonderingLM.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.20674v1;http://arxiv.org/pdf/2505.20674v1", "pdf_url": "http://arxiv.org/pdf/2505.20674v1"}, {"title": "Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning", "link": "https://arxiv.org/pdf/2505.18503", "details": "A Chang, L Huang, AJ Boyd, P Bhatia, T Kass-Hout\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal attention distribution on visual inputs, leading to hallucinated or inaccurate outputs. Existing mitigation methods primarily rely on inference-time interventions, which are \u2026", "entry_id": "http://arxiv.org/abs/2505.18503v1", "updated": "2025-05-24 04:45:45", "published": "2025-05-24 04:45:45", "authors": "Aofei Chang;Le Huang;Alex James Boyd;Parminder Bhatia;Taha Kass-Hout;Cao Xiao;Fenglong Ma", "summary": "Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal\nattention distribution on visual inputs, leading to hallucinated or inaccurate\noutputs. Existing mitigation methods primarily rely on inference-time\ninterventions, which are limited in attention adaptation or require additional\nsupervision. To address this, we propose A$^3$Tune, a novel fine-tuning\nframework for Automatic Attention Alignment Tuning. A$^3$Tune leverages\nzero-shot weak labels from SAM, refines them into prompt-aware labels using\nBioMedCLIP, and then selectively modifies visually-critical attention heads to\nimprove alignment while minimizing interference. Additionally, we introduce a\nA$^3$MoE module, enabling adaptive parameter selection for attention tuning\nacross diverse prompts and images. Extensive experiments on medical VQA and\nreport generation benchmarks show that A$^3$Tune outperforms state-of-the-art\nbaselines, achieving enhanced attention distributions and performance in\nMed-LVLMs.", "comment": "Accepted to ACL2025 (main)", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.18503v1;http://arxiv.org/pdf/2505.18503v1", "pdf_url": "http://arxiv.org/pdf/2505.18503v1"}, {"title": "FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2505.20225", "details": "H Kang, Z Yu, C Xiong - arXiv preprint arXiv:2505.20225, 2025", "abstract": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong efficiency-performance trade-offs by activating only a fraction of the model per token \u2026", "entry_id": "http://arxiv.org/abs/2505.20225v1", "updated": "2025-05-26 17:06:25", "published": "2025-05-26 17:06:25", "authors": "Hao Kang;Zichun Yu;Chenyan Xiong", "summary": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4\nincreasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong\nefficiency-performance trade-offs by activating only a fraction of the model\nper token. Yet academic researchers still lack a fully open, end-to-end MoE\nplatform for investigating scaling, routing, and expert behavior. We release\nFLAME-MoE, a completely open-source research suite composed of seven\ndecoder-only models, ranging from 38M to 1.7B active parameters, whose\narchitecture--64 experts with top-8 gating and 2 shared experts--closely\nreflects modern production LLMs. All training data pipelines, scripts, logs,\nand checkpoints are publicly available to enable reproducible experimentation.\nAcross six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4\npoints over dense baselines trained with identical FLOPs. Leveraging full\ntraining trace transparency, we present initial analyses showing that (i)\nexperts increasingly specialize on distinct token subsets, (ii) co-activation\nmatrices remain sparse, reflecting diverse expert usage, and (iii) routing\nbehavior stabilizes early in training. All code, training logs, and model\ncheckpoints are available at https://github.com/cmu-flame/FLAME-MoE.", "comment": "All code, training logs, and model checkpoints are available at\n  https://github.com/cmu-flame/FLAME-MoE", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.20225v1;http://arxiv.org/pdf/2505.20225v1", "pdf_url": "http://arxiv.org/pdf/2505.20225v1"}, {"title": "Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2505.16416", "details": "C Wang, J Guo, H Li, Y Tian, Y Nie, C Xu, K Han - arXiv preprint arXiv:2505.16416, 2025", "abstract": "Rotary Position Embedding (RoPE) is a widely adopted technique for encoding relative positional information in large language models (LLMs). However, when extended to large vision-language models (LVLMs), its variants introduce \u2026", "entry_id": "http://arxiv.org/abs/2505.16416v1", "updated": "2025-05-22 09:05:01", "published": "2025-05-22 09:05:01", "authors": "Chengcheng Wang;Jianyuan Guo;Hongguang Li;Yuchuan Tian;Ying Nie;Chang Xu;Kai Han", "summary": "Rotary Position Embedding (RoPE) is a widely adopted technique for encoding\nrelative positional information in large language models (LLMs). However, when\nextended to large vision-language models (LVLMs), its variants introduce\nunintended cross-modal positional biases. Specifically, they enforce relative\npositional dependencies between text token indices and image tokens, causing\nspurious alignments. This issue arises because image tokens representing the\nsame content but located at different spatial positions are assigned distinct\npositional biases, leading to inconsistent cross-modal associations. To address\nthis, we propose Per-Token Distance (PTD) - a simple yet effective metric for\nquantifying the independence of positional encodings across modalities.\nInformed by this analysis, we introduce Circle-RoPE, a novel encoding scheme\nthat maps image token indices onto a circular trajectory orthogonal to the\nlinear path of text token indices, forming a cone-like structure. This\nconfiguration ensures that each text token maintains an equal distance to all\nimage tokens, reducing artificial cross-modal biases while preserving\nintra-image spatial information. To further enhance performance, we propose a\nstaggered layer strategy that applies different RoPE variants across layers.\nThis design leverages the complementary strengths of each RoPE variant, thereby\nenhancing the model's overall performance. Our experimental results demonstrate\nthat our method effectively preserves spatial information from images while\nreducing relative positional bias, offering a more robust and flexible\npositional encoding framework for LVLMs. The code is available at\n[https://github.com/lose4578/CircleRoPE](https://github.com/lose4578/CircleRoPE).", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.16416v1;http://arxiv.org/pdf/2505.16416v1", "pdf_url": "http://arxiv.org/pdf/2505.16416v1"}, {"title": "Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2505.19498", "details": "N Hu, X Duan, J Zhang, G Kang - arXiv preprint arXiv:2505.19498, 2025", "abstract": "Large Vision-Language Models (LVLMs) usually generate texts which satisfy context coherence but don't match the visual input. Such a hallucination issue hinders LVLMs' applicability in the real world. The key to solving hallucination in LVLM is to \u2026", "entry_id": "http://arxiv.org/abs/2505.19498v1", "updated": "2025-05-26 04:26:30", "published": "2025-05-26 04:26:30", "authors": "Nanxing Hu;Xiaoyue Duan;Jinchao Zhang;Guoliang Kang", "summary": "Large Vision-Language Models (LVLMs) usually generate texts which satisfy\ncontext coherence but don't match the visual input. Such a hallucination issue\nhinders LVLMs' applicability in the real world. The key to solving\nhallucination in LVLM is to make the text generation rely more on the visual\ncontent. Most previous works choose to enhance/adjust the features/output of a\nspecific modality (i.e., visual or textual) to alleviate hallucinations in\nLVLM, which do not explicitly or systematically enhance the visual reliance. In\nthis paper, we comprehensively investigate the factors which may degenerate the\nvisual reliance in text generation of LVLM from a Bayesian perspective. Based\non our observations, we propose to mitigate hallucination in LVLM from three\naspects. Firstly, we observe that not all visual tokens are informative in\ngenerating meaningful texts. We propose to evaluate and remove redundant visual\ntokens to avoid their disturbance. Secondly, LVLM may encode inappropriate\nprior information, making it lean toward generating unexpected words. We\npropose a simple yet effective way to rectify the prior from a Bayesian\nperspective. Thirdly, we observe that starting from certain steps, the\nposterior of next-token prediction conditioned on visual tokens may collapse to\na prior distribution which does not depend on any informative visual tokens at\nall. Thus, we propose to stop further text generation to avoid hallucination.\nExtensive experiments on three benchmarks including POPE, CHAIR, and MME\ndemonstrate that our method can consistently mitigate the hallucination issue\nof LVLM and performs favorably against previous state-of-the-arts.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.19498v1;http://arxiv.org/pdf/2505.19498v1", "pdf_url": "http://arxiv.org/pdf/2505.19498v1"}, {"title": "Efficient Long CoT Reasoning in Small Language Models", "link": "https://arxiv.org/pdf/2505.18440", "details": "Z Wang, J Jiang, T Qiu, H Liu, X Tang, H Yao - arXiv preprint arXiv:2505.18440, 2025", "abstract": "Recent large reasoning models such as DeepSeek-R1 exhibit strong complex problems solving abilities by generating long chain-of-thought (CoT) reasoning steps. It is challenging to directly train small language models (SLMs) to emerge long \u2026", "entry_id": "http://arxiv.org/abs/2505.18440v1", "updated": "2025-05-24 00:22:52", "published": "2025-05-24 00:22:52", "authors": "Zhaoyang Wang;Jinqi Jiang;Tian Qiu;Hui Liu;Xianfeng Tang;Huaxiu Yao", "summary": "Recent large reasoning models such as DeepSeek-R1 exhibit strong complex\nproblems solving abilities by generating long chain-of-thought (CoT) reasoning\nsteps. It is challenging to directly train small language models (SLMs) to\nemerge long CoT. Thus, distillation becomes a practical method to enable SLMs\nfor such reasoning ability. However, the long CoT often contains a lot of\nredundant contents (e.g., overthinking steps) which may make SLMs hard to learn\nconsidering their relatively poor capacity and generalization. To address this\nissue, we propose a simple-yet-effective method to prune unnecessary steps in\nlong CoT, and then employ an on-policy method for the SLM itself to curate\nvalid and useful long CoT training data. In this way, SLMs can effectively\nlearn efficient long CoT reasoning and preserve competitive performance at the\nsame time. Experimental results across a series of mathematical reasoning\nbenchmarks demonstrate the effectiveness of the proposed method in distilling\nlong CoT reasoning ability into SLMs which maintains the competitive\nperformance but significantly reduces generating redundant reasoning steps.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.18440v1;http://arxiv.org/pdf/2505.18440v1", "pdf_url": "http://arxiv.org/pdf/2505.18440v1"}, {"title": "Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation", "link": "https://arxiv.org/pdf/2505.19529", "details": "TH Sakib, MT Hosain, MK Morol - arXiv preprint arXiv:2505.19529, 2025", "abstract": "Small Language Models (SLMs) have gained substantial attention due to their ability to execute diverse language tasks successfully while using fewer computer resources. These models are particularly ideal for deployment in limited \u2026", "entry_id": "http://arxiv.org/abs/2505.19529v2", "updated": "2025-05-29 16:57:36", "published": "2025-05-26 05:29:47", "authors": "Tanjil Hasan Sakib;Md. Tanzib Hosain;Md. Kishor Morol", "summary": "Small Language Models (SLMs) have gained substantial attention due to their\nability to execute diverse language tasks successfully while using fewer\ncomputer resources. These models are particularly ideal for deployment in\nlimited environments, such as mobile devices, on-device processing, and edge\nsystems. In this study, we present a complete assessment of SLMs, focussing on\ntheir design frameworks, training approaches, and techniques for lowering model\nsize and complexity. We offer a novel classification system to organize the\noptimization approaches applied for SLMs, encompassing strategies like pruning,\nquantization, and model compression. Furthermore, we assemble SLM's studies of\nevaluation suite with some existing datasets, establishing a rigorous platform\nfor measuring SLM capabilities. Alongside this, we discuss the important\ndifficulties that remain unresolved in this sector, including trade-offs\nbetween efficiency and performance, and we suggest directions for future study.\nWe anticipate this study to serve as a beneficial guide for researchers and\npractitioners who aim to construct compact, efficient, and high-performing\nlanguage models.", "comment": "9 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.19529v2;http://arxiv.org/pdf/2505.19529v2", "pdf_url": "http://arxiv.org/pdf/2505.19529v2"}, {"title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records", "link": "https://arxiv.org/pdf/2505.16941", "details": "C Pang, V Jeanselme, YS Choi, X Jiang, Z Jing\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Foundation models hold significant promise in healthcare, given their capacity to extract meaningful representations independent of downstream tasks. This property has enabled state-of-the-art performance across several clinical applications trained \u2026", "entry_id": "http://arxiv.org/abs/2505.16941v2", "updated": "2025-05-23 02:06:25", "published": "2025-05-22 17:29:52", "authors": "Chao Pang;Vincent Jeanselme;Young Sang Choi;Xinzhuo Jiang;Zilin Jing;Aparajita Kashyap;Yuta Kobayashi;Yanwei Li;Florent Pollet;Karthik Natarajan;Shalmali Joshi", "summary": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.16941v2;http://arxiv.org/pdf/2505.16941v2", "pdf_url": "http://arxiv.org/pdf/2505.16941v2"}]
