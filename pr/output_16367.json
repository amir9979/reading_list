[{"title": "Leveraging long context in retrieval augmented language models for medical question answering", "link": "https://www.nature.com/articles/s41746-025-01651-w", "details": "G Zhang, Z Xu, Q Jin, F Chen, Y Fang, Y Liu\u2026 - npj Digital Medicine, 2025", "abstract": "While holding great promise for improving and facilitating healthcare through applications of medical literature summarization, large language models (LLMs) struggle to produce up-to-date responses on evolving topics due to outdated \u2026"}, {"title": "MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks", "link": "https://arxiv.org/pdf/2505.06152", "details": "W Zeng, Y Sun, C Ma, W Tan, B Yan - arXiv preprint arXiv:2505.06152, 2025", "abstract": "Medical vision-language models (VLMs) have shown promise as clinical assistants across various medical fields. However, specialized dermatology VLM capable of delivering professional and detailed diagnostic analysis remains underdeveloped \u2026"}, {"title": "ALGOPUZZLEVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Algorithmic Multimodal Puzzles", "link": "https://aclanthology.org/2025.naacl-long.486.pdf", "details": "D Ghosal, V Toh, YK Chia, S Poria - Proceedings of the 2025 Conference of the \u2026, 2025", "abstract": "This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models \u2026"}, {"title": "Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2504.13945", "details": "Z Wu, T Song, N Xie, W Zhang, M Zhu, S Wu, S Sun\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The rapid advancement of large vision-language models (LVLMs) has significantly propelled applications in document understanding, particularly in optical character recognition (OCR) and multilingual translation. However, current evaluations of \u2026"}, {"title": "Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions", "link": "https://arxiv.org/pdf/2505.05755", "details": "D Patel, A Sahoo, A Amballa, T Naseem, TGJ Rudner\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Autoregressive models (ARMs), which predict subsequent tokens one-by-one``from left to right,''have achieved significant success across a wide range of sequence generation tasks. However, they struggle to accurately represent sequences that \u2026"}, {"title": "Localizing Before Answering: A Benchmark for Grounded Medical Visual Question Answering", "link": "https://arxiv.org/pdf/2505.00744", "details": "D Nguyen, MK Ho, H Ta, TT Nguyen, Q Chen, K Rav\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical Large Multi-modal Models (LMMs) have demonstrated remarkable capabilities in medical data interpretation. However, these models frequently generate hallucinations contradicting source evidence, particularly due to \u2026"}, {"title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning", "link": "https://arxiv.org/pdf/2504.19627%3F", "details": "R Luo, R Shan, L Chen, Z Liu, L Wang, M Yang, X Xia - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like embodied intelligence due to their strong vision-language reasoning abilities. However, current LVLMs process entire images at the token level, which is inefficient \u2026"}, {"title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "link": "https://arxiv.org/pdf/2504.19162", "details": "J Chen, B Zhang, R Ma, P Wang, X Liang, Z Tu, X Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play \u2026"}, {"title": "A Context-Aware Contrastive Learning Framework for Hateful Meme Detection and Segmentation", "link": "https://aclanthology.org/2025.findings-naacl.289.pdf", "details": "X Su, Y Li, D Inkpen, N Japkowicz - Findings of the Association for Computational \u2026, 2025", "abstract": "Amidst the rise of Large Multimodal Models (LMMs) and their widespread application in generating and interpreting complex content, the risk of propagating biased and harmful memes remains significant. Current safety measures often fail to detect \u2026"}]
