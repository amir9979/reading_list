[{"title": "BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models", "link": "https://arxiv.org/pdf/2406.17092", "details": "Y Zeng, W Sun, TN Huynh, D Song, B Li, R Jia - arXiv preprint arXiv:2406.17092, 2024", "abstract": "Safety backdoor attacks in large language models (LLMs) enable the stealthy triggering of unsafe behaviors while evading detection during normal interactions. The high dimensionality of potential triggers in the token space and the diverse \u2026"}, {"title": "A fine-grained self-adapting prompt learning approach for few-shot learning with pre-trained language models", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124006026", "details": "X Chen, T Liu, P Fournier-Viger, B Zhang, G Long\u2026 - Knowledge-Based Systems, 2024", "abstract": "Pre-trained language models have demonstrated remarkable performance in few- shot learning through the emergence of \u201cprompt-based learning\u201d methods, where the performance of these tasks highly rely on the quality of prompts. Existing prompt \u2026"}, {"title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models", "link": "https://arxiv.org/pdf/2406.02224", "details": "T Fan, G Ma, Y Kang, H Gu, L Fan, Q Yang - arXiv preprint arXiv:2406.02224, 2024", "abstract": "Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small \u2026"}, {"title": "Semantically Diverse Language Generation for Uncertainty Estimation in Language Models", "link": "https://arxiv.org/pdf/2406.04306", "details": "L Aichberger, K Schweighofer, M Ielanskyi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) can suffer from hallucinations when generating text. These hallucinations impede various applications in society and industry by making LLMs untrustworthy. Current LLMs generate text in an autoregressive fashion by \u2026"}, {"title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning", "link": "https://arxiv.org/pdf/2406.00645", "details": "Y Fu, H Zhang, D Wu, W Xu, B Boulet - arXiv preprint arXiv:2406.00645, 2024", "abstract": "In this work, we investigate how to leverage pre-trained visual-language models (VLM) for online Reinforcement Learning (RL). In particular, we focus on sparse reward tasks with pre-defined textual task descriptions. We first identify the problem \u2026"}, {"title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions", "link": "https://arxiv.org/pdf/2406.15877", "details": "TY Zhuo, MC Vu, J Chim, H Hu, W Yu, R Widyasari\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Automated software engineering has been greatly empowered by the recent advances in Large Language Models (LLMs) for programming. While current benchmarks have shown that LLMs can perform various software engineering tasks \u2026"}, {"title": "Exploring Mathematical Extrapolation of Large Language Models with Synthetic Data", "link": "https://arxiv.org/pdf/2406.02100", "details": "H Li, Y Ma, Y Zhang, C Ye, J Chen - arXiv preprint arXiv:2406.02100, 2024", "abstract": "Large Language Models (LLMs) have shown excellent performance in language understanding, text generation, code synthesis, and many other tasks, while they still struggle in complex multi-step reasoning problems, such as mathematical reasoning \u2026"}, {"title": "Large Language Models are Interpretable Learners", "link": "https://arxiv.org/pdf/2406.17224", "details": "R Wang, S Si, F Yu, D Wiesmann, CJ Hsieh, I Dhillon - arXiv preprint arXiv \u2026, 2024", "abstract": "The trade-off between expressiveness and interpretability remains a core challenge when building human-centric predictive models for classification and decision- making. While symbolic rules offer interpretability, they often lack expressiveness \u2026"}, {"title": "Factual Dialogue Summarization via Learning from Large Language Models", "link": "https://arxiv.org/pdf/2406.14709", "details": "R Zhu, JH Lau, J Qi - arXiv preprint arXiv:2406.14709, 2024", "abstract": "Factual consistency is an important quality in dialogue summarization. Large language model (LLM)-based automatic text summarization models generate more factually consistent summaries compared to those by smaller pretrained language \u2026"}]
