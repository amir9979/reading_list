[{"title": "Automated Insomnia Phenotyping from Electronic Health Records: Leveraging Large Language Models to Decode Clinical Narratives", "link": "https://www.medrxiv.org/content/10.1101/2025.06.02.25328701.full.pdf", "details": "G Lopez-Garcia, D Weissenbacher, M Stadler\u2026 - medRxiv, 2025", "abstract": "Insomnia is a highly prevalent but often underdiagnosed condition in clinical practice. Its inconsistent documentation in electronic health records (EHRs) limits population-level analyses and obstructs efforts to evaluate treatment patterns or \u2026"}, {"title": "Cross-Modal Causal Representation Learning for Radiology Report Generation", "link": "https://ieeexplore.ieee.org/abstract/document/11005686/", "details": "W Chen, Y Liu, C Wang, J Zhu, G Li, CL Liu, L Lin - IEEE Transactions on Image \u2026, 2025", "abstract": "Radiology Report Generation (RRG) is essential for computer-aided diagnosis and medication guidance, which can relieve the heavy burden of radiologists by automatically generating the corresponding radiology reports according to the given \u2026"}, {"title": "Domain Adaptation Strategies for Transformer-Based Disease Prediction using Electronic Health Records", "link": "https://www.medrxiv.org/content/medrxiv/early/2025/06/02/2025.06.02.25328621.full.pdf", "details": "J Driever, M Lentzen, S Madan, H Fr\u00f6hlich - medRxiv, 2025", "abstract": "Electronic Health Records (EHRs) offer rich data for machine learning, but model generalizability across institutions is hindered by statistical and coding biases. This study investigates domain adaptation (DA) techniques to improve model transfer \u2026"}, {"title": "Dual-Domain Masked Image Modeling: A Self-Supervised Pretraining Strategy Using Spatial and Frequency Domain Masking for Hyperspectral Data", "link": "https://arxiv.org/pdf/2505.03220", "details": "S Mohamed, T Fernando, S Sridharan, P Moghadam\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Hyperspectral images (HSIs) capture rich spectral signatures that reveal vital material properties, offering broad applicability across various domains. However, the scarcity of labeled HSI data limits the full potential of deep learning, especially for \u2026", "entry_id": "http://arxiv.org/abs/2505.03220v1", "updated": "2025-05-06 06:24:21", "published": "2025-05-06 06:24:21", "authors": "Shaheer Mohamed;Tharindu Fernando;Sridha Sridharan;Peyman Moghadam;Clinton Fookes", "summary": "Hyperspectral images (HSIs) capture rich spectral signatures that reveal\nvital material properties, offering broad applicability across various domains.\nHowever, the scarcity of labeled HSI data limits the full potential of deep\nlearning, especially for transformer-based architectures that require\nlarge-scale training. To address this constraint, we propose Spatial-Frequency\nMasked Image Modeling (SFMIM), a self-supervised pretraining strategy for\nhyperspectral data that utilizes the large portion of unlabeled data. Our\nmethod introduces a novel dual-domain masking mechanism that operates in both\nspatial and frequency domains. The input HSI cube is initially divided into\nnon-overlapping patches along the spatial dimension, with each patch comprising\nthe entire spectrum of its corresponding spatial location. In spatial masking,\nwe randomly mask selected patches and train the model to reconstruct the masked\ninputs using the visible patches. Concurrently, in frequency masking, we remove\nportions of the frequency components of the input spectra and predict the\nmissing frequencies. By learning to reconstruct these masked components, the\ntransformer-based encoder captures higher-order spectral-spatial correlations.\nWe evaluate our approach on three publicly available HSI classification\nbenchmarks and demonstrate that it achieves state-of-the-art performance.\nNotably, our model shows rapid convergence during fine-tuning, highlighting the\nefficiency of our pretraining strategy.", "comment": "Preprint to appear in IEEE IGARSS 2025", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.03220v1;http://arxiv.org/pdf/2505.03220v1", "pdf_url": "http://arxiv.org/pdf/2505.03220v1"}, {"title": "Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies", "link": "https://arxiv.org/pdf/2505.19337", "details": "K Li, M Zitnik - arXiv preprint arXiv:2505.19337, 2025", "abstract": "Offline goal-conditioned reinforcement learning methods have shown promise for reach-avoid tasks, where an agent must reach a target state while avoiding undesirable regions of the state space. Existing approaches typically encode avoid \u2026", "entry_id": "http://arxiv.org/abs/2505.19337v2", "updated": "2025-05-27 02:56:11", "published": "2025-05-25 22:00:38", "authors": "Kevin Li;Marinka Zitnik", "summary": "Offline goal-conditioned reinforcement learning methods have shown promise\nfor reach-avoid tasks, where an agent must reach a target state while avoiding\nundesirable regions of the state space. Existing approaches typically encode\navoid-region information into an augmented state space and cost function, which\nprevents flexible, dynamic specification of novel avoid-region information at\nevaluation time. They also rely heavily on well-designed reward and cost\nfunctions, limiting scalability to complex or poorly structured environments.\nWe introduce RADT, a decision transformer model for offline, reward-free,\ngoal-conditioned, avoid region-conditioned RL. RADT encodes goals and avoid\nregions directly as prompt tokens, allowing any number of avoid regions of\narbitrary size to be specified at evaluation time. Using only suboptimal\noffline trajectories from a random policy, RADT learns reach-avoid behavior\nthrough a novel combination of goal and avoid-region hindsight relabeling. We\nbenchmark RADT against 3 existing offline goal-conditioned RL models across 11\ntasks, environments, and experimental settings. RADT generalizes in a zero-shot\nmanner to out-of-distribution avoid region sizes and counts, outperforming\nbaselines that require retraining. In one such zero-shot setting, RADT achieves\n35.7% improvement in normalized cost over the best retrained baseline while\nmaintaining high goal-reaching success. We apply RADT to cell reprogramming in\nbiology, where it reduces visits to undesirable intermediate gene expression\nstates during trajectories to desired target states, despite stochastic\ntransitions and discrete, structured state dynamics.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;q-bio.QM", "links": "http://arxiv.org/abs/2505.19337v2;http://arxiv.org/pdf/2505.19337v2", "pdf_url": "http://arxiv.org/pdf/2505.19337v2"}, {"title": "Efficient Event-Based Object Detection: A Hybrid Neural Network with Spatial and Temporal Attention", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Ahmed_Efficient_Event-Based_Object_Detection_A_Hybrid_Neural_Network_with_Spatial_CVPR_2025_paper.pdf", "details": "SH Ahmed, J Finkbeiner, E Neftci - Proceedings of the Computer Vision and Pattern \u2026, 2025", "abstract": "Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for robust object detection. While Spiking Neural Networks (SNNs) on neuromorphic hardware are often considered for energy \u2026"}, {"title": "The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning", "link": "https://arxiv.org/pdf/2506.02139", "details": "EY Chang - arXiv preprint arXiv:2506.02139, 2025", "abstract": "Few-shot learning in large language models (LLMs) reveals a deep paradox: Some tasks generalize from minimal examples, while others require extensive supervision. We address this through the Unified Cognitive Consciousness Theory (UCCT), which \u2026", "entry_id": "http://arxiv.org/abs/2506.02139v2", "updated": "2025-06-04 02:44:46", "published": "2025-06-02 18:12:43", "authors": "Edward Y. Chang", "summary": "Few-shot learning in large language models (LLMs) reveals a core paradox:\ncertain tasks generalize from just a few examples, while others demand\nextensive supervision. To explain this, we introduce the Unified Cognitive\nConsciousness Theory (UCCT), which reconceptualizes LLMs not as deficient\nagents, but as unconscious substrates: dense, distributed repositories of\nlinguistic and conceptual patterns that operate without explicit semantics,\nintention, or goal-directed reasoning. Under this view, LLMs are not flawed\nsimulations of cognition but foundational substrates for general intelligence.\nUCCT posits that semantic anchoring, via prompts, role assignments, and\nstructured interaction, functions as a conscious control layer that modulates\nlatent representations toward task-relevant semantics and enables coherent,\nstructured reasoning. It unifies prompting, fine-tuning, retrieval-augmented\ngeneralization, and multi-agent collaboration within a single framework,\ngrounded in the probabilistic alignment between unconscious pattern space and\nexternally imposed semantic constraints (e.g., prompts, supervision, task\nobjectives). The core implication is not to replace LLMs, but to integrate and\nunify them through a structured cognitive layer that supports intentional\nreasoning. This enables collections of LLMs to operate within\ndomain-specialized verticals (e.g., legal reasoning, medical diagnosis) that\nreason, regulate, and adapt together. Such integration is characterized by\nphase-transition behavior, wherein anchored representations cross coherence\nthresholds as a function of semantic constraint strength and interaction\ncontext.", "comment": "12 pages, 2 figure, 1 table", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;I.2.7", "links": "http://arxiv.org/abs/2506.02139v2;http://arxiv.org/pdf/2506.02139v2", "pdf_url": "http://arxiv.org/pdf/2506.02139v2"}, {"title": "CXPMRG-Bench: Pre-training and Benchmarking for X-ray Medical Report Generation on CheXpert Plus Dataset\u2013Supplementary Material\u2013", "link": "https://openaccess.thecvf.com/content/CVPR2025/supplemental/Wang_CXPMRG-Bench_Pre-training_and_CVPR_2025_supplemental.pdf", "details": "X Wang, F Wang, Y Li, Q Ma, S Wang, B Jiang, J Tang", "abstract": "Since its introduction in 2017, Transformer [56] has quickly become the preferred model framework for researchers due to its strong performance. However, as the model scales and sequences become longer, its limitations have surfaced. One \u2026"}, {"title": "A Gene Set Foundation Model Pre-Trained on a Massive Collection of Diverse Gene Sets", "link": "https://www.biorxiv.org/content/biorxiv/early/2025/06/02/2025.05.30.657124.full.pdf", "details": "DJB Clarke, GB Marino, A Ma'ayan - bioRxiv, 2025", "abstract": "Trained with large datasets, foundation models can capture complex patterns within these datasets to create embeddings that can be used for a variety of useful applications. Here we created a gene set foundation model that was trained on a \u2026"}]
