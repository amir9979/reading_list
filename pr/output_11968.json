[{"title": "LASS: A Novel and Economical Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization", "link": "https://aclanthology.org/2025.coling-main.412.pdf", "details": "Y Zhang, P Li, Y Lai, Y He, D Zhou - Proceedings of the 31st International Conference \u2026, 2025", "abstract": "As more than 70% of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are hesitant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct \u2026"}, {"title": "Evaluating Generalization Capability of Language Models across Abductive, Deductive and Inductive Logical Reasoning", "link": "https://aclanthology.org/2025.coling-main.330.pdf", "details": "Y Sheng, W Wen, L Li, D Zeng - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Transformer-based language models (LMs) have demonstrated remarkable performance on many natural language tasks, yet to what extent LMs possess the capability of generalizing to unseen logical rules remains not explored sufficiently. In \u2026"}, {"title": "When Evolution Strategy Meets Language Models Tuning", "link": "https://aclanthology.org/2025.coling-main.357.pdf", "details": "B Huang, Y Jiang, M Chen, Y Wang, H Chen, W Wang - Proceedings of the 31st \u2026, 2025", "abstract": "Supervised Fine-tuning has been pivotal in training autoregressive language models, yet it introduces exposure bias. To mitigate this, Post Fine-tuning, including on-policy and off-policy methods, has emerged as a solution to enhance models \u2026"}, {"title": "Differentiable Prompt Learning for Vision Language Models", "link": "https://arxiv.org/pdf/2501.00457", "details": "Z Huang, T Pedapati, PY Chen, J Gao - arXiv preprint arXiv:2501.00457, 2024", "abstract": "Prompt learning is an effective way to exploit the potential of large-scale pre-trained foundational models. Continuous prompts parameterize context tokens in prompts by turning them into differentiable vectors. Deep continuous prompts insert prompts not \u2026"}, {"title": "Partial Order-centered Hyperbolic Representation Learning for Few-shot Relation Extraction", "link": "https://aclanthology.org/2025.coling-main.101.pdf", "details": "B Hu, Z Huang, M Hu, P Yang, P Qiao, Y Dou, Z Wang - Proceedings of the 31st \u2026, 2025", "abstract": "Prototype network-based methods have made substantial progress in few-shot relation extraction (FSRE) by enhancing relation prototypes with relation descriptions. However, the distribution of relations and instances in distinct \u2026"}, {"title": "Towards a Generative Paradigm for Large-scale Microbiome Analysis by Generative Language Model", "link": "https://www.biorxiv.org/content/10.1101/2025.01.15.633278.full.pdf", "details": "H Zhang, Z Kang, Y Zhang, R Yang, K Ning - bioRxiv, 2025", "abstract": "Microbiome analysis has traditionally relied on taxonomic abundance tables, which, while effective, often constrain the exploration of deeper contextual relationships. In this study, we present MGM 2.0, a novel framework that applies advanced natural \u2026"}, {"title": "Large Language Models are Good Annotators for Type-aware Data Augmentation in Grammatical Error Correction", "link": "https://aclanthology.org/2025.coling-main.14.pdf", "details": "X Li, Y Lan - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Abstract Large Language Models (LLMs) have achieved outstanding performance across various NLP tasks. Grammatical Error Correction (GEC) is a task aiming at automatically correcting grammatical errors in text, but it encounters a severe \u2026"}, {"title": "Cross-lingual Evaluation of Multilingual Text Generation", "link": "https://aclanthology.org/2025.coling-main.520.pdf", "details": "S Chollampatt, MQ Pham, SR Indurthi, M Turchi - Proceedings of the 31st \u2026, 2025", "abstract": "Scaling automatic evaluation of multilingual text generation of LLMs to new tasks, domains, and languages remains a challenge. Traditional evaluation on benchmark datasets carries the risk of reference data leakage in LLM training or involves \u2026"}, {"title": "META-LORA: Memory-Efficient Sample Reweighting for Fine-Tuning Large Language Models", "link": "https://aclanthology.org/2025.coling-main.568.pdf", "details": "W Li, L Zou, M Tang, Q Yu, W Li, C Li - \u2026 of the 31st International Conference on \u2026, 2025", "abstract": "Supervised fine-tuning (SFT) is widely adopted for tailoring large language models (LLMs) to specific downstream tasks. However, the substantial computational demands of LLMs hinder iterative exploration of fine-tuning datasets and accurate \u2026"}]
