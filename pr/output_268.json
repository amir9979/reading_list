'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Investigating Continual Pretraining in Large Language'
[{"title": "Enhancing Protein Predictive Models via Proteins Data Augmentation: A Benchmark and New Directions", "link": "https://arxiv.org/html/2403.00875v1", "details": "R Sun, L Wu, H Lin, Y Huang, SZ Li - arXiv preprint arXiv:2403.00875, 2024", "abstract": "Augmentation is an effective alternative to utilize the small amount of labeled protein data. However, most of the existing work focuses on design-ing new architectures or pre-training tasks, and relatively little work has studied data augmentation for \u2026"}, {"title": "An Integrated Data Processing Framework for Pretraining Foundation Models", "link": "https://arxiv.org/pdf/2402.16358", "details": "Y Sun, F Wang, Y Zhu, WX Zhao, J Mao - arXiv preprint arXiv:2402.16358, 2024", "abstract": "The ability of the foundation models heavily relies on large-scale, diverse, and high- quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and \u2026"}, {"title": "DEE: Dual-stage Explainable Evaluation Method for Text Generation", "link": "https://arxiv.org/pdf/2403.11509", "details": "S Zhang, Y Li, R Wu, X Huang, Y Chen, W Xu, G Qi - arXiv preprint arXiv:2403.11509, 2024", "abstract": "Automatic methods for evaluating machine-generated texts hold significant importance due to the expanding applications of generative systems. Conventional methods tend to grapple with a lack of explainability, issuing a solitary numerical \u2026"}]
