'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Computer-Aided Diagnosis of Thoracic Diseases in Chest'
[{"title": "GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation", "link": "https://arxiv.org/pdf/2403.19754", "details": "M Gholami, M Akbari, C Hu, V Masrani, ZJ Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Knowledge distillation from LLMs is essential for the efficient deployment of language models. Prior works have proposed data generation using LLMs for preparing distilled models. We argue that generating data with LLMs is prone to \u2026"}, {"title": "Offset Unlearning for Large Language Models", "link": "https://arxiv.org/pdf/2404.11045", "details": "JY Huang, W Zhou, F Wang, F Morstatter, S Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, harmful, and private content has led to ethical and \u2026"}]
