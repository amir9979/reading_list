'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Can Language Models Solve Olympiad Programming?](https'
[{"title": "Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering", "link": "https://arxiv.org/pdf/2404.10193", "details": "Z Khan, Y Fu - arXiv preprint arXiv:2404.10193, 2024", "abstract": "The goal of selective prediction is to allow an a model to abstain when it may not be able to deliver a reliable prediction, which is important in safety-critical contexts. Existing approaches to selective prediction typically require access to the internals of \u2026"}, {"title": "MedConceptsQA--Open Source Medical Concepts QA Benchmark", "link": "https://arxiv.org/pdf/2405.07348", "details": "OB Shoham, N Rappoport - arXiv preprint arXiv:2405.07348, 2024", "abstract": "We present MedConceptsQA, a dedicated open source benchmark for medical concepts question answering. The benchmark comprises of questions of various medical concepts across different vocabularies: diagnoses, procedures, and drugs \u2026"}, {"title": "Advanced Natural-based interaction for the ITAlian language: LLaMAntino-3-ANITA", "link": "https://arxiv.org/pdf/2405.07101", "details": "M Polignano, P Basile, G Semeraro - arXiv preprint arXiv:2405.07101, 2024", "abstract": "In the pursuit of advancing natural language processing for the Italian language, we introduce a state-of-the-art Large Language Model (LLM) based on the novel Meta LLaMA-3 model: LLaMAntino-3-ANITA-8B-Inst-DPO-ITA. We fine-tuned the original \u2026"}, {"title": "DP-DyLoRA: Fine-Tuning Transformer-Based Models On-Device under Differentially Private Federated Learning using Dynamic Low-Rank Adaptation", "link": "https://arxiv.org/pdf/2405.06368", "details": "J Xu, K Saravanan, R van Dalen, H Mehmood\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Federated learning (FL) allows clients in an Internet of Things (IoT) system to collaboratively train a global model without sharing their local data with a server. However, clients' contributions to the server can still leak sensitive information \u2026"}, {"title": "Distributed Rumor Source Detection Via Boosted Federated Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10504633/", "details": "R Wang, Y Zhang, W Wan, M Chen, M Guizani - IEEE Transactions on Knowledge \u2026, 2024", "abstract": "How to localize the rumor source is a common interest of all sectors of the society. Many researchers have tried to use deep-learning-based graph models to detect rumor sources, but they have neglected how to train their deep-learning-based graph \u2026"}, {"title": "Disentangled Anomaly Detection For Multivariate Time Series", "link": "https://dl.acm.org/doi/abs/10.1145/3589335.3651492", "details": "X Jie, X Zhou, C Su, Z Zhou, Y Yuan, J Bu, H Wang - \u2026 Proceedings of the ACM on Web \u2026, 2024", "abstract": "Anomaly detection in time series that aims to identify unusual patterns has attracted a lot of attention recently. However, the representation of abnormal and normal data is difffcult to be distinguished because they are usually entangled. Recently \u2026"}, {"title": "Self-playing Adversarial Language Game Enhances LLM Reasoning", "link": "https://arxiv.org/pdf/2404.10642", "details": "P Cheng, T Hu, H Xu, Z Zhang, Y Dai, L Han, N Du - arXiv preprint arXiv:2404.10642, 2024", "abstract": "We explore the self-play training procedure of large language models (LLMs) in a two-player adversarial language game called Adversarial Taboo. In this game, an attacker and a defender communicate with respect to a target word only visible to the \u2026"}, {"title": "Causal Diffusion Autoencoders: Toward Counterfactual Generation via Diffusion Probabilistic Models", "link": "https://arxiv.org/pdf/2404.17735", "details": "A Komanduri, C Zhao, F Chen, X Wu - arXiv preprint arXiv:2404.17735, 2024", "abstract": "Diffusion probabilistic models (DPMs) have become the state-of-the-art in high- quality image generation. However, DPMs have an arbitrary noisy latent space with no interpretable or controllable semantics. Although there has been significant \u2026"}, {"title": "Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots", "link": "https://arxiv.org/pdf/2405.07990", "details": "C Wu, Y Ge, Q Guo, J Wang, Z Liang, Z Lu, Y Shan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has attracted significant attention due to their superior performance in visual contexts. However, their capabilities in turning visual figure to executable code, have not been \u2026"}]
