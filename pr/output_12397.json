[{"title": "When Evolution Strategy Meets Language Models Tuning", "link": "https://aclanthology.org/2025.coling-main.357.pdf", "details": "B Huang, Y Jiang, M Chen, Y Wang, H Chen, W Wang - Proceedings of the 31st \u2026, 2025", "abstract": "Supervised Fine-tuning has been pivotal in training autoregressive language models, yet it introduces exposure bias. To mitigate this, Post Fine-tuning, including on-policy and off-policy methods, has emerged as a solution to enhance models \u2026"}, {"title": "OntoTune: Ontology-Driven Self-training for Aligning Large Language Models", "link": "https://openreview.net/pdf%3Fid%3Dd7spHwemKX", "details": "Z Liu, C Gan, J Wang, Y Zhang, Z Bo, M Sun, H Chen\u2026 - THE WEB CONFERENCE 2025", "abstract": "Existing domain-specific Large Language Models (LLMs) are typically developed by fine-tuning general-purposed LLMs with large-scale domain-specific corpora. However, training on large-scale corpora often fails to effectively organize domain \u2026"}, {"title": "Understanding Before Reasoning: Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting", "link": "https://arxiv.org/pdf/2501.04341%3F", "details": "DH Zhu, YJ Xiong, JC Zhang, XJ Xie, CM Xia - arXiv preprint arXiv:2501.04341, 2025", "abstract": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language Models (LLMs) to enhance complex reasoning. It guides LLMs to present multi-step reasoning, rather than generating the final answer directly. However, CoT \u2026"}, {"title": "Zero-Shot Verification-guided Chain of Thoughts", "link": "https://arxiv.org/pdf/2501.13122", "details": "JR Chowdhury, C Caragea - arXiv preprint arXiv:2501.13122, 2025", "abstract": "Previous works have demonstrated the effectiveness of Chain-of-Thought (COT) prompts and verifiers in guiding Large Language Models (LLMs) through the space of reasoning. However, most such studies either use a fine-tuned verifier or rely on \u2026"}]
