[{"title": "Long-Tail Crisis in Nearest Neighbor Language Models", "link": "https://arxiv.org/pdf/2503.22426", "details": "Y Nishida, M Morishita, H Deguchi, H Kamigaito\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The $ k $-nearest-neighbor language model ($ k $ NN-LM), one of the retrieval- augmented language models, improves the perplexity for given text by directly accessing a large datastore built from any text data during inference. A widely held \u2026"}, {"title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models", "link": "https://arxiv.org/pdf/2503.18931", "details": "Y Chen, L Meng, W Peng, Z Wu, YG Jiang - arXiv preprint arXiv:2503.18931, 2025", "abstract": "Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of \u2026"}, {"title": "ViLBench: A Suite for Vision-Language Process Reward Modeling", "link": "https://arxiv.org/pdf/2503.20271", "details": "H Tu, W Feng, H Chen, H Liu, X Tang, C Xie - arXiv preprint arXiv:2503.20271, 2025", "abstract": "Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on \u2026"}, {"title": "Can large language models independently complete tasks? A dynamic evaluation framework for multi-turn task planning and completion", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225008070", "details": "J Gao, J Cui, H Wu, L Xiang, H Zhao, X Li, M Fang\u2026 - Neurocomputing, 2025", "abstract": "Large language models (LLMs) are increasingly relied upon for multi-turn dialogue to conduct complex tasks. However, existing benchmarks mainly evaluate LLMs as agents, overlooking their potential as independent systems to accomplish complex \u2026"}, {"title": "CEFW: A Comprehensive Evaluation Framework for Watermark in Large Language Models", "link": "https://arxiv.org/pdf/2503.20802", "details": "S Zhang, B Cheng, J Han, Y Chen, Z Wu, C Li, P Gu - arXiv preprint arXiv:2503.20802, 2025", "abstract": "Text watermarking provides an effective solution for identifying synthetic text generated by large language models. However, existing techniques often focus on satisfying specific criteria while ignoring other key aspects, lacking a unified \u2026"}, {"title": "Knowledge-Centered Dual-Process Reasoning for Math Word Problems with Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10946242/", "details": "J Liu, Z Huang, Q Liu, Z Ma, C Zhai, E Chen - IEEE Transactions on Knowledge and \u2026, 2025", "abstract": "Math word problem (MWP) serves as a critical milestone for assessing the text mining ability and knowledge mastery level of models. Recent advancements have witnessed large language models (LLMs) showcasing remarkable performance on \u2026"}, {"title": "STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in Large Language Models", "link": "https://arxiv.org/pdf/2503.17932%3F", "details": "X Wang, W Wang, Z Ji, Z Li, P Ma, D Wu, S Wang - arXiv preprint arXiv:2503.17932, 2025", "abstract": "Large Language Models (LLMs) have become increasingly vulnerable to jailbreak attacks that circumvent their safety mechanisms. While existing defense methods either suffer from adaptive attacks or require computationally expensive auxiliary \u2026"}, {"title": "Investigating Large Language Models in Diagnosing Students' Cognitive Skills in Math Problem-solving", "link": "https://arxiv.org/pdf/2504.00843%3F", "details": "H Jin, Y Kim, D Jung, S Kim, K Choi, J Son, J Kim - arXiv preprint arXiv:2504.00843, 2025", "abstract": "Mathematics learning entails mastery of both content knowledge and cognitive processing of knowing, applying, and reasoning with it. Automated math assessment primarily has focused on grading students' exhibition of content knowledge by finding \u2026"}, {"title": "Cognitive Memory in Large Language Models", "link": "https://arxiv.org/pdf/2504.02441", "details": "L Shan, S Luo, Z Zhu, Y Yuan, Y Wu - arXiv preprint arXiv:2504.02441, 2025", "abstract": "This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long \u2026"}]
