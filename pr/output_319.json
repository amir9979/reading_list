'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [RIFF: Learning to Rephrase Inputs for Few-shot Fine-t'
[{"title": "Anatomical Structure-Guided Medical Vision-Language Pre-training", "link": "https://arxiv.org/html/2403.09294v1", "details": "Q Li, X Yan, J Xu, R Yuan, Y Zhang, R Feng, Q Shen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Learning medical visual representations through vision-language pre-training has reached remarkable progress. Despite the promising performance, it still faces challenges, ie, local alignment lacks interpretability and clinical relevance, and the \u2026"}, {"title": "Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods", "link": "https://arxiv.org/pdf/2403.00998", "details": "P Tsvilodub, H Wang, S Grosch, M Franke - arXiv preprint arXiv:2403.00998, 2024", "abstract": "This paper systematically compares different methods of deriving item-level predictions of language models for multiple-choice tasks. It compares scoring methods for answer options based on free generation of responses, various \u2026"}, {"title": "Language Models for Text Classification: Is In-Context Learning Enough?", "link": "https://arxiv.org/pdf/2403.17661", "details": "A Edwards, J Camacho-Collados - arXiv preprint arXiv:2403.17661, 2024", "abstract": "Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero-and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand \u2026"}, {"title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models", "link": "https://arxiv.org/pdf/2403.08281", "details": "N Ding, Y Chen, G Cui, X Lv, R Xie, B Zhou, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three \u2026"}, {"title": "$\\texttt {COSMIC} $: Mutual Information for Task-Agnostic Summarization Evaluation", "link": "https://arxiv.org/pdf/2402.19457", "details": "M Darrin, P Formont, JCK Cheung, P Piantanida - arXiv preprint arXiv:2402.19457, 2024", "abstract": "Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks \u2026"}, {"title": "LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation", "link": "https://arxiv.org/html/2403.12019v1", "details": "Y Lan, F Hong, S Yang, S Zhou, X Meng, B Dai, X Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The field of neural rendering has witnessed significant progress with advancements in generative models and differentiable rendering techniques. Though 2D diffusion has achieved success, a unified 3D diffusion pipeline remains unsettled. This paper \u2026"}, {"title": "MagicClay: Sculpting Meshes With Generative Neural Fields", "link": "https://arxiv.org/pdf/2403.02460", "details": "A Barda, VG Kim, N Aigerman, AH Bermano, T Groueix - arXiv preprint arXiv \u2026, 2024", "abstract": "The recent developments in neural fields have brought phenomenal capabilities to the field of shape generation, but they lack crucial properties, such as incremental control-a fundamental requirement for artistic work. Triangular meshes, on the other \u2026"}, {"title": "Diffusion Models are Geometry Critics: Single Image 3D Editing Using Pre-Trained Diffusion Priors", "link": "https://arxiv.org/pdf/2403.11503", "details": "R Wang, J Xiang, J Yang, X Tong - arXiv preprint arXiv:2403.11503, 2024", "abstract": "We propose a novel image editing technique that enables 3D manipulations on single images, such as object rotation and translation. Existing 3D-aware image editing approaches typically rely on synthetic multi-view datasets for training \u2026"}, {"title": "Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models", "link": "https://arxiv.org/pdf/2403.17589", "details": "Y Zhang, W Zhu, H Tang, Z Ma, K Zhou, L Zhang - arXiv preprint arXiv:2403.17589, 2024", "abstract": "With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research. The adaptation strategies can be typically categorized into three \u2026"}]
