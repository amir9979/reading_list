[{"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "link": "https://arxiv.org/pdf/2410.05639", "details": "R Zhao, ZL Thai, Y Zhang, S Hu, Y Ba, J Zhou, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the \u2026"}, {"title": "Aligning Language Models Using Follow-up Likelihood as Reward Signal", "link": "https://arxiv.org/pdf/2409.13948", "details": "C Zhang, D Chong, F Jiang, C Tang, A Gao, G Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In natural human-to-human conversations, participants often receive feedback signals from one another based on their follow-up reactions. These reactions can include verbal responses, facial expressions, changes in emotional state, and other \u2026"}, {"title": "Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination", "link": "https://arxiv.org/pdf/2409.12746", "details": "ES Salido, R Morante, J Gonzalo, G Marco\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this article we present UNED-ACCESS 2024, a bilingual dataset that consists of 1003 multiple-choice questions of university entrance level exams in Spanish and English. Questions are originally formulated in Spanish and translated manually into \u2026"}, {"title": "Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models", "link": "https://arxiv.org/pdf/2410.00131", "details": "J Liu, J Ren, R Jin, Z Zhang, Y Zhou, P Valduriez\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As a promising paradigm to collaboratively train models with decentralized data, Federated Learning (FL) can be exploited to fine-tune Large Language Models (LLMs). While LLMs correspond to huge size, the scale of the training data \u2026"}, {"title": "TAEGAN: Generating Synthetic Tabular Data For Data Augmentation", "link": "https://arxiv.org/pdf/2410.01933", "details": "J Li, Z Zhao, K Yee, U Javaid, B Sikdar - arXiv preprint arXiv:2410.01933, 2024", "abstract": "Synthetic tabular data generation has gained significant attention for its potential in data augmentation, software testing and privacy-preserving data sharing. However, most research has primarily focused on larger datasets and evaluating their quality in \u2026"}, {"title": "ImageNet-RIB Benchmark: Large Pre-Training Datasets Don't Guarantee Robustness after Fine-Tuning", "link": "https://openreview.net/pdf%3Fid%3DwpCiNhn2sC", "details": "J Hwang, B Cheung, ZW Hong, A Boopathy, P Agrawal\u2026 - \u2026 2024 Workshop on Fine-Tuning in \u2026", "abstract": "Highly performant large-scale pre-trained models promise to also provide a valuable foundation for learning specialized tasks, by fine-tuning the model to the desired task. By starting from a good general-purpose model, the goal is to achieve both \u2026"}, {"title": "PromptTA: Prompt-driven Text Adapter for Source-free Domain Generalization", "link": "https://arxiv.org/pdf/2409.14163", "details": "H Zhang, S Bai, W Zhou, J Fu, B Chen - arXiv preprint arXiv:2409.14163, 2024", "abstract": "Source-free domain generalization (SFDG) tackles the challenge of adapting models to unseen target domains without access to source domain data. To deal with this challenging task, recent advances in SFDG have primarily focused on leveraging the \u2026"}, {"title": "Towards understanding evolution of science through language model series", "link": "https://arxiv.org/pdf/2409.09636", "details": "J Dong, Z Lyu, Q Ke - arXiv preprint arXiv:2409.09636, 2024", "abstract": "We introduce AnnualBERT, a series of language models designed specifically to capture the temporal evolution of scientific text. Deviating from the prevailing paradigms of subword tokenizations and\" one model to rule them all\", AnnualBERT \u2026"}, {"title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data", "link": "https://arxiv.org/pdf/2409.12437", "details": "J Zhou, A Ghaddar, G Zhang, L Ma, Y Hu, S Pal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the \u2026"}]
