[{"title": "SurGen: 1020 H&E-stained Whole Slide Images With Survival and Genetic Markers", "link": "https://arxiv.org/pdf/2502.04946", "details": "C Myles, IH Um, C Marshall, D Harris-Birtill\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "$\\textbf {Background} $: Cancer remains one of the leading causes of morbidity and mortality worldwide. Comprehensive datasets that combine histopathological images with genetic and survival data across various tumour sites are essential for \u2026"}, {"title": "A multimodal multidomain multilingual medical foundation model for zero shot clinical diagnosis", "link": "https://www.nature.com/articles/s41746-024-01339-7", "details": "F Liu, Z Li, Q Yin, J Huang, J Luo, A Thakur, K Branson\u2026 - npj Digital Medicine, 2025", "abstract": "Radiology images are one of the most commonly used in daily clinical diagnosis. Typically, clinical diagnosis using radiology images involves disease reporting and classification, where the former is a multimodal task whereby textual reports are \u2026"}, {"title": "Swarm learning with weak supervision enables automatic breast cancer detection in magnetic resonance imaging", "link": "https://www.nature.com/articles/s43856-024-00722-5", "details": "OL Saldanha, J Zhu, G M\u00fcller-Franzes, ZI Carrero\u2026 - Communications Medicine, 2025", "abstract": "Background Over the next 5 years, new breast cancer screening guidelines recommending magnetic resonance imaging (MRI) for certain patients will significantly increase the volume of imaging data to be analyzed. While this increase \u2026"}, {"title": "Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement", "link": "https://arxiv.org/pdf/2502.02573%3F", "details": "S Abbasloo - arXiv preprint arXiv:2502.02573, 2025", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem- solving, a crucial, ubiquitous, and complex domain. This paper explores the \u2026"}, {"title": "Self-supervised analogical learning using language models", "link": "https://arxiv.org/pdf/2502.00996", "details": "B Zhou, S Jain, Y Zhang, Q Ning, S Wang, Y Benajiba\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can \u2026"}, {"title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models", "link": "https://arxiv.org/pdf/2502.17387", "details": "A Albalak, D Phung, N Lile, R Rafailov, K Gandhi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written \u2026"}, {"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}]
