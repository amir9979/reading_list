[{"title": "Too Big to Fool: Resisting Deception in Language Models", "link": "https://arxiv.org/pdf/2412.10558", "details": "MR Samsami, ML Richter, J Rodriguez, M Thakkar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models must balance their weight-encoded knowledge with in- context information from prompts to generate accurate responses. This paper investigates this interplay by analyzing how models of varying capacities within the \u2026"}, {"title": "Multimodal Preference Data Synthetic Alignment with Reward Model", "link": "https://arxiv.org/pdf/2412.17417", "details": "R Wijaya, NB Nguyen, NM Cheung - arXiv preprint arXiv:2412.17417, 2024", "abstract": "Multimodal large language models (MLLMs) have significantly advanced tasks like caption generation and visual question answering by integrating visual and textual data. However, they sometimes produce misleading or hallucinate content due to \u2026"}, {"title": "Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces", "link": "https://arxiv.org/pdf/2412.14171%3F", "details": "J Yang, S Yang, AW Gupta, R Han, L Fei-Fei, S Xie - arXiv preprint arXiv:2412.14171, 2024", "abstract": "Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also``think in space''from videos? We present \u2026"}, {"title": "Smoothed Embeddings for Robust Language Models", "link": "https://www.merl.com/publications/docs/TR2024-170.pdf", "details": "H Ryo, MRU Rashid, A Lewis, J Liu, T Koike-Akino\u2026", "abstract": "Improving the safety and reliability of large language models (LLMs) is a crucial aspect of realizing trustworthy AI systems. Although alignment methods aim to suppress harmful content generation, LLMs are often still vulnerable to jail-breaking \u2026"}, {"title": "Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models", "link": "https://arxiv.org/pdf/2412.15287", "details": "Y Chow, G Tennenholtz, I Gur, V Zhuang, B Dai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent studies have indicated that effectively utilizing inference-time compute is crucial for attaining better performance from large language models (LLMs). In this work, we propose a novel inference-aware fine-tuning paradigm, in which the model \u2026"}, {"title": "UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models", "link": "https://arxiv.org/pdf/2412.11803", "details": "B Xue, F Mi, Q Zhu, H Wang, R Wang, S Wang, E Yu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite demonstrating impressive capabilities, Large Language Models (LLMs) still often struggle to accurately express the factual knowledge they possess, especially in cases where the LLMs' knowledge boundaries are ambiguous. To improve LLMs' \u2026"}, {"title": "InfAlign: Inference-aware language model alignment", "link": "https://arxiv.org/pdf/2412.19792", "details": "A Balashankar, Z Sun, J Berant, J Eisenstein, M Collins\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language model alignment has become a critical step in training modern generative language models. The goal of alignment is to finetune a reference model such that the win rate of a sample from the aligned model over a sample from the reference \u2026"}, {"title": "Hint Marginalization for Improved Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2412.13292", "details": "S Pal, D Ch\u00e9telat, Y Zhang, M Coates - arXiv preprint arXiv:2412.13292, 2024", "abstract": "Large Language Models (LLMs) have exhibited an impressive capability to perform reasoning tasks, especially if they are encouraged to generate a sequence of intermediate steps. Reasoning performance can be improved by suitably combining \u2026"}, {"title": "Concept-ROT: Poisoning Concepts in Large Language Models with Model Editing", "link": "https://arxiv.org/pdf/2412.13341", "details": "K Grimes, M Christiani, D Shriver, M Connor - arXiv preprint arXiv:2412.13341, 2024", "abstract": "Model editing methods modify specific behaviors of Large Language Models by altering a small, targeted set of network weights and require very little data and compute. These methods can be used for malicious applications such as inserting \u2026"}]
