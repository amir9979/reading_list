[{"title": "Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "link": "https://arxiv.org/pdf/2405.10431", "details": "S Furniturewala, S Jandial, A Java, P Banerjee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing debiasing techniques are typically training-based or require access to the model's internals and output distributions, so they are inaccessible to end-users looking to adapt LLM outputs for their particular needs. In this study, we examine \u2026"}, {"title": "Large Language Models Can Self-Correct with Minimal Effort", "link": "https://arxiv.org/pdf/2405.14092", "details": "Z Wu, Q Zeng, Z Zhang, Z Tan, C Shen, M Jiang - arXiv preprint arXiv:2405.14092, 2024", "abstract": "Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple \u2026"}, {"title": "A Systematic Analysis on the Temporal Generalization of Language Models in Social Media", "link": "https://arxiv.org/pdf/2405.13017", "details": "A Ushio, J Camacho-Collados - arXiv preprint arXiv:2405.13017, 2024", "abstract": "In machine learning, temporal shifts occur when there are differences between training and test splits in terms of time. For streaming data such as news or social media, models are commonly trained on a fixed corpus from a certain period of time \u2026"}, {"title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation", "link": "https://arxiv.org/pdf/2405.13769", "details": "C Chhun, FM Suchanek, C Clavel - arXiv preprint arXiv:2405.13769, 2024", "abstract": "Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high \u2026"}, {"title": "Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs", "link": "https://arxiv.org/pdf/2405.15485", "details": "S Guo, A Didolkar, NR Ke, A Goyal, F Husz\u00e1r\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We are beginning to see progress in language model assisted scientific discovery. Motivated by the use of LLMs as a general scientific assistant, this paper assesses the domain knowledge of LLMs through its understanding of different mathematical \u2026"}, {"title": "Effective In-Context Example Selection through Data Compression", "link": "https://arxiv.org/pdf/2405.11465", "details": "Z Sun, K Zhang, H Wang, X Zhang, J Xu - arXiv preprint arXiv:2405.11465, 2024", "abstract": "In-context learning has been extensively validated in large language models. However, the mechanism and selection strategy for in-context example selection, which is a crucial ingredient in this approach, lacks systematic and in-depth \u2026"}, {"title": "DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction", "link": "https://arxiv.org/pdf/2405.12100", "details": "H Chen, B Zeng, X Lin, L He, A Zhou - arXiv preprint arXiv:2405.12100, 2024", "abstract": "Math world problems correction (MWPC) is a novel task dedicated to rectifying reasoning errors in the process of solving mathematical problems. In this paper, leveraging the advancements in large language models (LLMs), we address two key \u2026"}, {"title": "QCRD: Quality-guided Contrastive Rationale Distillation for Large Language Models", "link": "https://arxiv.org/pdf/2405.13014", "details": "W Wang, Z Li, Q Xu, Y Cai, H Song, Q Qi, R Zhou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Deploying large language models (LLMs) poses challenges in terms of resource limitations and inference efficiency. To address these challenges, recent research has focused on using smaller task-specific language models, which are enhanced by \u2026"}, {"title": "DaVinci at SemEval-2024 Task 9: Few-shot prompting GPT-3.5 for Unconventional Reasoning", "link": "https://arxiv.org/pdf/2405.11559", "details": "SV Mathur, AR Jindal, M Shrivastava - arXiv preprint arXiv:2405.11559, 2024", "abstract": "While significant work has been done in the field of NLP on vertical thinking, which involves primarily logical thinking, little work has been done towards lateral thinking, which involves looking at problems from an unconventional perspective and defying \u2026"}]
