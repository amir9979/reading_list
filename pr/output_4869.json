[{"title": "MAGNET: Improving the Multilingual Fairness of Language Models with Adaptive Gradient-Based Tokenization", "link": "https://arxiv.org/pdf/2407.08818", "details": "O Ahia, S Kumar, H Gonen, V Hoffman, T Limisiewicz\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In multilingual settings, non-Latin scripts and low-resource languages are usually disadvantaged in terms of language models' utility, efficiency, and cost. Specifically, previous studies have reported multiple modeling biases that the current tokenization \u2026"}, {"title": "Accuracy and transportability of machine learning models for adolescent suicide prediction with longitudinal clinical records", "link": "https://www.nature.com/articles/s41398-024-03034-3", "details": "C Zang, Y Hou, D Lyu, J Jin, S Sacco, K Chen\u2026 - Translational Psychiatry, 2024", "abstract": "Abstract Machine Learning models trained from real-world data have demonstrated promise in predicting suicide attempts in adolescents. However, their transportability, namely the performance of a model trained on one dataset and applied to different \u2026"}, {"title": "Test-Time Low Rank Adaptation via Confidence Maximization for Zero-Shot Generalization of Vision-Language Models", "link": "https://arxiv.org/pdf/2407.15913", "details": "R Imam, H Gani, M Huzaifa, K Nandakumar - arXiv preprint arXiv:2407.15913, 2024", "abstract": "The conventional modus operandi for adapting pre-trained vision-language models (VLMs) during test-time involves tuning learnable prompts, ie, test-time prompt tuning. This paper introduces Test-Time Low-rank adaptation (TTL) as an alternative \u2026"}, {"title": "LLMBox: A Comprehensive Library for Large Language Models", "link": "https://arxiv.org/pdf/2407.05563", "details": "T Tang, Y Hu, B Li, W Luo, Z Qin, H Sun, J Wang, S Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To facilitate the research on large language models (LLMs), this paper presents a comprehensive and unified library, LLMBox, to ease the development, use, and evaluation of LLMs. This library is featured with three main merits:(1) a unified data \u2026"}, {"title": "Self-Improving Teacher Cultivates Better Student: Distillation Calibration for Multimodal Large Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3626772.3657692", "details": "X Li, L Lin, S Wang, C Qian - Proceedings of the 47th International ACM SIGIR \u2026, 2024", "abstract": "Multimodal content generation, which leverages visual information to enhance the comprehension of cross-modal understanding, plays a critical role in Multimodal Information Retrieval. With the development of large language models (LLMs), recent \u2026"}]
