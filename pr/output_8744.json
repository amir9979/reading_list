[{"title": "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "link": "https://aclanthology.org/2024.emnlp-main.852.pdf", "details": "L Chen, R Zheng, B Wang, S Jin, C Huang, J Ye\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract Reinforcement Learning from Human Feedback (RLHF) is a crucial approach to aligning language models with human values and intentions. A fundamental challenge in this method lies in ensuring that the reward model \u2026"}, {"title": "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "link": "https://aclanthology.org/2024.emnlp-main.759.pdf", "details": "F Liu, Z Li, H Zhou, Q Yin, J Yang, X Tang, C Luo\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question- answering (QA) task with answer options for evaluation. However, many clinical \u2026"}, {"title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "link": "https://aclanthology.org/2024.emnlp-main.1220.pdf", "details": "S Singla, Z Wang, T Liu, A Ashfaq, Z Hu, E Xing - \u2026 of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Abstract Aligning Large Language Models (LLMs) traditionally relies on complex and costly training processes like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). To address the challenge of achieving \u2026"}, {"title": "Evaluation of a task specific self-supervised learning framework in digital pathology relative to transfer learning approaches and existing foundation models", "link": "https://www.modernpathology.org/article/S0893-3952\\(24\\)00216-3/fulltext", "details": "T Rahman, AS Baras, R Chellappa - Modern Pathology, 2024", "abstract": "An integral stage in typical digital pathology workflows involves deriving specific features from tiles extracted from a tessellated whole slide image. Notably, various computer vision neural network architectures, particularly the ImageNet pre-trained \u2026"}, {"title": "Self-Training Large Language and Vision Assistant for Medical Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.1119.pdf", "details": "G Sun, C Qin, H Fu, L Wang, Z Tao - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "Abstract Large Vision-Language Models (LVLMs) have shown significant potential in assisting medical diagnosis by leveraging extensive biomedical datasets. However, the advancement of medical image understanding and reasoning critically depends \u2026"}, {"title": "Gradient Localization Improves Lifelong Pretraining of Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.949.pdf", "details": "J Fernandez, Y Bisk, E Strubell - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) trained on web-scale text corpora have been shown to capture world knowledge in their parameters. However, the mechanism by which language models store different types of knowledge is poorly \u2026"}, {"title": "SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology Image Understanding", "link": "https://arxiv.org/pdf/2410.11761", "details": "Y Chen, G Wang, Y Ji, Y Li, J Ye, T Li, B Zhang, N Pei\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the progress made by multimodal large language models (MLLMs) in computational pathology, they remain limited by a predominant focus on patch-level analysis, missing essential contextual information at the whole-slide level. The lack \u2026"}, {"title": "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.884.pdf", "details": "P Gao, T Yamasaki, K Imoto - Findings of the Association for Computational \u2026, 2024", "abstract": "We propose VE-KD, a novel method that balances knowledge distillation and vocabulary expansion with the aim of training efficient domain-specific language models. Compared with traditional pre-training approaches, VE-KD exhibits \u2026"}, {"title": "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "link": "https://aclanthology.org/2024.findings-emnlp.61.pdf", "details": "X Wei, H Chen, H Yu, H Fei, Q Liu - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have achieved notable success in commonsense reasoning tasks, benefiting from their extensive world knowledge acquired through extensive pretraining. While approaches like Chain-of-Thought \u2026"}]
