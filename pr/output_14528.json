[{"title": "Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras", "link": "https://arxiv.org/pdf/2503.01743%3F", "details": "A Abouelenin, A Ashfaq, A Atkinson, H Awadalla\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent \u2026"}, {"title": "VLog: Video-Language Models by Generative Retrieval of Narration Vocabulary", "link": "https://arxiv.org/pdf/2503.09402", "details": "KQ Lin, MZ Shou - arXiv preprint arXiv:2503.09402, 2025", "abstract": "Human daily activities can be concisely narrated as sequences of routine events (eg, turning off an alarm) in video streams, forming an event vocabulary. Motivated by this, we introduce VLog, a novel video understanding framework that define video \u2026"}, {"title": "Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge", "link": "https://arxiv.org/pdf/2503.04036", "details": "X Cui, JTZ Wei, S Swayamdipta, R Jia - arXiv preprint arXiv:2503.04036, 2025", "abstract": "Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques \u2026"}, {"title": "Multidimensional Consistency Improves Reasoning in Language Models", "link": "https://arxiv.org/pdf/2503.02670", "details": "H Lai, X Zhang, M Nissim - arXiv preprint arXiv:2503.02670, 2025", "abstract": "While Large language models (LLMs) have proved able to address some complex reasoning tasks, we also know that they are highly sensitive to input variation, which can lead to different solution paths and final answers. Answer consistency across \u2026"}, {"title": "Rethinking Data: Towards Better Performing Domain-Specific Small Language Models", "link": "https://arxiv.org/pdf/2503.01464", "details": "B Nazarov, D Frolova, Y Lubarsky, A Gaissinski\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Fine-tuning of Large Language Models (LLMs) for downstream tasks, performed on domain-specific data has shown significant promise. However, commercial use of such LLMs is limited by the high computational cost required for their deployment at \u2026"}, {"title": "Process-based self-rewarding language models", "link": "https://arxiv.org/pdf/2503.03746", "details": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' \u2026"}, {"title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models", "link": "https://arxiv.org/pdf/2503.09573%3F", "details": "M Arriola, A Gokaslan, JT Chiu, Z Yang, Z Qi, J Han\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a \u2026"}, {"title": "Language Models Predict Empathy Gaps Between Social In-groups and Out-groups", "link": "https://arxiv.org/pdf/2503.01030", "details": "Y Hou, H Daum\u00e9 III, R Rudinger - arXiv preprint arXiv:2503.01030, 2025", "abstract": "Studies of human psychology have demonstrated that people are more motivated to extend empathy to in-group members than out-group members (Cikara et al., 2011). In this study, we investigate how this aspect of intergroup relations in humans is \u2026"}, {"title": "Autoregressive Language Model with Historical Context Re-encoding", "link": "https://ieeexplore.ieee.org/abstract/document/10890165/", "details": "Y Zhuang - ICASSP 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "The foundation of current large language model applications lies in the generative language model, which typically employs an autoregressive token generation approach. However, this model faces two key limitations: its unidirectional causal \u2026"}]
