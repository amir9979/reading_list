[{"title": "Information Extraction from Clinical Notes: Are We Ready to Switch to Large Language Models?", "link": "https://arxiv.org/pdf/2411.10020", "details": "Y Hu, X Zuo, Y Zhou, X Peng, J Huang, VK Keloth\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Backgrounds: Information extraction (IE) is critical in clinical natural language processing (NLP). While large language models (LLMs) excel on generative tasks, their performance on extractive tasks remains debated. Methods: We investigated \u2026"}, {"title": "LanFL: Differentially Private Federated Learning with Large Language Models using Synthetic Samples", "link": "https://arxiv.org/pdf/2410.19114", "details": "H Wu, D Klabjan - arXiv preprint arXiv:2410.19114, 2024", "abstract": "Federated Learning (FL) is a collaborative, privacy-preserving machine learning framework that enables multiple participants to train a single global model. However, the recent advent of powerful Large Language Models (LLMs) with tens to hundreds \u2026"}, {"title": "Towards Explainable Computerized Adaptive Testing with Large Language Model", "link": "https://aclanthology.org/2024.findings-emnlp.149.pdf", "details": "C Cheng, GH Zhao, Z Huang, Y Zhuang, Z Pan, Q Liu\u2026 - Findings of the Association \u2026, 2024", "abstract": "As intelligent education evolves, it will provide students with multiple personalized learning services based on their individual abilities. Computerized adaptive testing (CAT) is designed to accurately measure a student's ability using the least questions \u2026"}, {"title": "Measuring Non-Adversarial Reproduction of Training Data in Large Language Models", "link": "https://arxiv.org/pdf/2411.10242", "details": "M Aerni, J Rando, E Debenedetti, N Carlini, D Ippolito\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models memorize parts of their training data. Memorizing short snippets and facts is required to answer questions about the world and to be fluent in any language. But models have also been shown to reproduce long verbatim \u2026"}, {"title": "MCL: Multi-view Enhanced Contrastive Learning for Chest X-ray Report Generation", "link": "https://arxiv.org/pdf/2411.10224", "details": "K Liu, Z Ma, K Xie, Z Jiao, Q Miao - arXiv preprint arXiv:2411.10224, 2024", "abstract": "Radiology reports are crucial for planning treatment strategies and enhancing doctor- patient communication, yet manually writing these reports is burdensome for radiologists. While automatic report generation offers a solution, existing methods \u2026"}, {"title": "Category-guided multi-interest collaborative metric learning with representation uniformity constraints", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324002966", "details": "L Wang, T Lian - Information Processing & Management, 2025", "abstract": "Multi-interest collaborative metric learning has recently emerged as an effective approach to modeling the multifaceted interests of a user in recommender systems. However, two issues remain unexplored.(1) There is no explicit guidance for the \u2026"}, {"title": "Weakly-Supervised Multimodal Learning on MIMIC-CXR", "link": "https://arxiv.org/pdf/2411.10356", "details": "A Agostini, D Chopard, Y Meng, N Fortin, B Shahbaba\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal data integration and label scarcity pose significant challenges for machine learning in medical settings. To address these issues, we conduct an in- depth evaluation of the newly proposed Multimodal Variational Mixture-of-Experts \u2026"}, {"title": "Graph Counterfactual Explainable AI via Latent Space Traversal", "link": "https://openreview.net/forum%3Fid%3DPyqnc9eWhB", "details": "AA Hansen, P Pegios, A Calissano, A Feragen - Northern Lights Deep Learning Conference \u2026", "abstract": "Explaining the predictions of a deep neural network is a nontrivial task, yet high- quality explanations for predictions are often a prerequisite for practitioners to trust these models.\\textit {Counterfactual explanations} aim to explain predictions by \u2026"}, {"title": "Enhancing Answer Attribution for Faithful Text Generation with Large Language Models", "link": "https://arxiv.org/pdf/2410.17112%3F", "details": "J Vladika, L M\u00fclln, F Matthes - arXiv preprint arXiv:2410.17112, 2024", "abstract": "The increasing popularity of Large Language Models (LLMs) in recent years has changed the way users interact with and pose questions to AI-based conversational systems. An essential aspect for increasing the trustworthiness of generated LLM \u2026"}]
