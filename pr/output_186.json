'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [MeDSLIP: Medical Dual-Stream Language-Image Pre-train'
[{"title": "RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning", "link": "https://arxiv.org/html/2403.11082v1", "details": "JR Asl, P Panzade, E Blanco, D Takabi, Z Cai - arXiv preprint arXiv:2403.11082, 2024", "abstract": "Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless, despite their success with unseen data, current PLM-based \u2026"}, {"title": "Improving Medical Multi-modal Contrastive Learning with Expert Annotations", "link": "https://arxiv.org/html/2403.10153v1", "details": "Y Kumar, P Marttinen - arXiv preprint arXiv:2403.10153, 2024", "abstract": "We introduce eCLIP, an enhanced version of the CLIP model that integrates expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key challenges in contrastive multi-modal medical imaging analysis, notably data scarcity and the\" \u2026"}, {"title": "TRELM: Towards Robust and Efficient Pre-training for Knowledge-Enhanced Language Models", "link": "https://arxiv.org/html/2403.11203v1", "details": "J Yan, C Wang, T Zhang, X He, J Huang, L Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "KEPLMs are pre-trained models that utilize external knowledge to enhance language understanding. Previous language models facilitated knowledge acquisition by incorporating knowledge-related pre-training tasks learned from \u2026"}, {"title": "mPLM-Sim: Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models", "link": "https://aclanthology.org/2024.findings-eacl.20.pdf", "details": "P Lin, C Hu, Z Zhang, AFT Martins, H Sch\u00fctze - Findings of the Association for \u2026, 2024", "abstract": "Recent multilingual pretrained language models (mPLMs) have been shown to encode strong language-specific signals, which are not explicitly provided during pretraining. It remains an open question whether it is feasible to employ mPLMs to \u2026"}, {"title": "Llm comparative assessment: Zero-shot nlg evaluation through pairwise comparisons using large language models", "link": "https://aclanthology.org/2024.eacl-long.8.pdf", "details": "A Liusie, P Manakul, M Gales - Proceedings of the 18th Conference of the European \u2026, 2024", "abstract": "Current developments in large language models (LLMs) have enabled impressive zero-shot capabilities across various natural language tasks. An interesting application of these systems is in the automated assessment of natural language \u2026"}, {"title": "Role-Guided Contrastive Learning for Event Argument Extraction", "link": "https://link.springer.com/chapter/10.1007/978-3-031-56027-9_21", "details": "C Yao, Y Guo, X Chen, Z Duan, J Fu - European Conference on Information Retrieval, 2024", "abstract": "Event argument extraction is a subtask of information extraction. Recent efforts have predominantly focused on mitigating the issue of error propagation associated with pipeline methods for extracting event arguments, such as machine reading \u2026"}, {"title": "Me LLaMA: Foundation Large Language Models for Medical Applications", "link": "https://arxiv.org/pdf/2402.12749", "details": "Q Xie, Q Chen, A Chen, C Peng, Y Hu, F Lin, X Peng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific \u2026"}, {"title": "An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis", "link": "https://arxiv.org/pdf/2403.12402", "details": "Y Peng, I Kulikov, Y Yang, S Popuri, H Lu, C Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Speech language models (LMs) are promising for high-quality speech synthesis through in-context learning. A typical speech LM takes discrete semantic units as content and a short utterance as prompt, and synthesizes speech which preserves \u2026"}, {"title": "ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models", "link": "https://arxiv.org/pdf/2403.11103", "details": "Y Heng, C Deng, Y Li, Y Yu, Y Li, R Zhang, C Zhang - arXiv preprint arXiv:2403.11103, 2024", "abstract": "Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER). This paper explores an innovative, cost-efficient \u2026"}]
