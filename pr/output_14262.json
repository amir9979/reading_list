[{"title": "Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge", "link": "https://arxiv.org/pdf/2503.04036", "details": "X Cui, JTZ Wei, S Swayamdipta, R Jia - arXiv preprint arXiv:2503.04036, 2025", "abstract": "Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques \u2026"}, {"title": "Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model for Advanced Medical Decision Support", "link": "https://arxiv.org/pdf/2502.18274%3F", "details": "G Wang, M Gao, S Yang, Y Zhang, L He, L Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs), particularly those with reasoning capabilities, have rapidly advanced in recent years, demonstrating significant potential across a wide range of applications. However, their deployment in healthcare, especially in disease \u2026"}, {"title": "Multidimensional Consistency Improves Reasoning in Language Models", "link": "https://arxiv.org/pdf/2503.02670", "details": "H Lai, X Zhang, M Nissim - arXiv preprint arXiv:2503.02670, 2025", "abstract": "While Large language models (LLMs) have proved able to address some complex reasoning tasks, we also know that they are highly sensitive to input variation, which can lead to different solution paths and final answers. Answer consistency across \u2026"}, {"title": "Rethinking Data: Towards Better Performing Domain-Specific Small Language Models", "link": "https://arxiv.org/pdf/2503.01464", "details": "B Nazarov, D Frolova, Y Lubarsky, A Gaissinski\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Fine-tuning of Large Language Models (LLMs) for downstream tasks, performed on domain-specific data has shown significant promise. However, commercial use of such LLMs is limited by the high computational cost required for their deployment at \u2026"}, {"title": "Autoregressive Language Model with Historical Context Re-encoding", "link": "https://ieeexplore.ieee.org/abstract/document/10890165/", "details": "Y Zhuang - ICASSP 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "The foundation of current large language model applications lies in the generative language model, which typically employs an autoregressive token generation approach. However, this model faces two key limitations: its unidirectional causal \u2026"}, {"title": "RoMedFormer: A Rotary-Embedding Transformer Foundation Model for 3D Genito-Pelvic Structure Segmentation in MRI and CT", "link": "https://arxiv.org/pdf/2503.14304", "details": "Y Li, M Hu, RLJ Qiu, M Thor, A Williams, D Marshall\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Deep learning-based segmentation of genito-pelvic structures in MRI and CT is crucial for applications such as radiation therapy, surgical planning, and disease diagnosis. However, existing segmentation models often struggle with \u2026"}, {"title": "RetriEVAL: Evaluating Text Generation with Contextualized Lexical Match", "link": "https://dl.acm.org/doi/abs/10.1145/3701551.3703581", "details": "Z Li, X Li, C Tao, J Feng, T Shen, C Xu, H Wang\u2026 - Proceedings of the \u2026, 2025", "abstract": "Pre-trained language models have made significant advancements in text generation tasks. Nevertheless, evaluating the generated text with automatic metrics is still challenging. Compared with supervised metrics, unsupervised metrics which \u2026"}, {"title": "X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography from Chest Radiography via Tri-Modal Contrastive Learning", "link": "https://arxiv.org/pdf/2503.02162", "details": "J You, Y Gao, S Kim, C Mcintosh - arXiv preprint arXiv:2503.02162, 2025", "abstract": "Computed tomography (CT) is a key imaging modality for diagnosis, yet its clinical utility is marred by high radiation exposure and long turnaround times, restricting its use for larger-scale screening. Although chest radiography (CXR) is more accessible \u2026"}, {"title": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots", "link": "https://arxiv.org/pdf/2503.14734", "details": "J Bjorck, F Casta\u00f1eda, N Cherniadev, X Da, R Ding\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "General-purpose robots need a versatile body and an intelligent mind. Recent advancements in humanoid robots have shown great promise as a hardware platform for building generalist autonomy in the human world. A robot foundation \u2026"}]
