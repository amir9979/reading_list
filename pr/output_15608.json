[{"title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability", "link": "https://arxiv.org/pdf/2504.06260", "details": "N Mudur, H Cui, S Venugopalan, P Raccuglia\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science. We present FEABench, a benchmark to evaluate the ability of large \u2026"}, {"title": "Focus Directions Make Your Language Models Pay More Attention to Relevant Contexts", "link": "https://arxiv.org/pdf/2503.23306", "details": "Y Zhu, R Li, D Wang, D Haehn, X Liang - arXiv preprint arXiv:2503.23306, 2025", "abstract": "Long-context large language models (LLMs) are prone to be distracted by irrelevant contexts. The reason for distraction remains poorly understood. In this paper, we first identify the contextual heads, a special group of attention heads that control the \u2026"}, {"title": "Pre-trained Language Models and Few-shot Learning for Medical Entity Extraction", "link": "https://arxiv.org/pdf/2504.04385", "details": "X Wang, G Liu, B Zhu, J He, H Zheng, H Zhang - arXiv preprint arXiv:2504.04385, 2025", "abstract": "This study proposes a medical entity extraction method based on Transformer to enhance the information extraction capability of medical literature. Considering the professionalism and complexity of medical texts, we compare the performance of \u2026"}, {"title": "Exploration of Plan-Guided Summarization for Narrative Texts: the Case of Small Language Models", "link": "https://arxiv.org/pdf/2504.09071", "details": "M Grenander, S Varia, P Czarnowska, Y Vyas\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Plan-guided summarization attempts to reduce hallucinations in small language models (SLMs) by grounding generated summaries to the source text, typically by targeting fine-grained details such as dates or named entities. In this work, we \u2026"}, {"title": "Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability", "link": "https://arxiv.org/pdf/2504.16056", "details": "D Hendriks, P Spitzer, N K\u00fchl, G Satzger - arXiv preprint arXiv:2504.16056, 2025", "abstract": "Artificial Intelligence (AI) has increasingly influenced modern society, recently in particular through significant advancements in Large Language Models (LLMs). However, high computational and storage demands of LLMs still limit their \u2026"}, {"title": "Never Start from Scratch: Expediting On-Device LLM Personalization via Explainable Model Selection", "link": "https://arxiv.org/pdf/2504.13938", "details": "H Wang, B Yang, X Yin, W Gao - arXiv preprint arXiv:2504.13938, 2025", "abstract": "Personalization of Large Language Models (LLMs) is important in practical applications to accommodate the individual needs of different mobile users. Due to data privacy concerns, LLM personalization often needs to be locally done at the \u2026"}, {"title": "TTRL: Test-Time Reinforcement Learning", "link": "https://arxiv.org/pdf/2504.16084", "details": "Y Zuo, K Zhang, S Qu, L Sheng, X Zhu, B Qi, Y Sun\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground \u2026"}, {"title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering", "link": "https://arxiv.org/pdf/2504.13828%3F", "details": "S Xia, Y Qin, X Li, Y Ma, RZ Fan, S Chern, H Zou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The first generation of Large Language Models-what might be called\" Act I\" of generative AI (2020-2023)-achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations in knowledge \u2026"}, {"title": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection", "link": "https://arxiv.org/pdf/2504.00695", "details": "X Zhu, Z Gu, S Zheng, T Wang, T Li, H Feng, Y Xiao - arXiv preprint arXiv:2504.00695, 2025", "abstract": "Pre-training large language models (LLMs) necessitates enormous diverse textual corpora, making effective data selection a key challenge for balancing computational resources and model performance. Current methodologies primarily emphasize data \u2026"}]
