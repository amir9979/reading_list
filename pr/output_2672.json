[{"title": "Implicit meta-learning may lead language models to trust more reliable sources", "link": "https://openreview.net/pdf%3Fid%3DFzp1DRzCIN", "details": "D Krasheninnikov, E Krasheninnikov, BK Mlodozeniec\u2026 - Forty-first International Conference \u2026", "abstract": "We demonstrate that large language models (LLMs) may learn indicators of document usefulness and modulate their updates accordingly. We introduce random strings (\" tags\") as indicators of usefulness in a synthetic fine-tuning dataset. Fine \u2026"}, {"title": "MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering", "link": "https://arxiv.org/abs/2406.06573", "details": "RO Ness, K Matton, H Helm, S Zhang, J Bajwa\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLM) have achieved impressive performance on medical question-answering benchmarks. However, high benchmark accuracy does not imply that the performance generalizes to real-world clinical settings. Medical \u2026"}]
