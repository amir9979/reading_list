[{"title": "Simulating clinical features on chest radiographs for medical image exploration and CNN explainability using a style-based generative adversarial autoencoder", "link": "https://www.nature.com/articles/s41598-024-75886-0", "details": "KA Hasenstab, L Hahn, N Chao, A Hsiao - Scientific Reports, 2024", "abstract": "Explainability of convolutional neural networks (CNNs) is integral for their adoption into radiological practice. Commonly used attribution methods localize image areas important for CNN prediction but do not characterize relevant imaging features \u2026"}, {"title": "CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense", "link": "https://arxiv.org/pdf/2410.23091", "details": "M Zhang, K Bi, W Chen, Q Chen, J Guo, X Cheng - arXiv preprint arXiv:2410.23091, 2024", "abstract": "Despite ongoing efforts to defend neural classifiers from adversarial attacks, they remain vulnerable, especially to unseen attacks. In contrast, humans are difficult to be cheated by subtle manipulations, since we make judgments only based on \u2026"}]
