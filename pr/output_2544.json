[{"title": "Impact of high-quality, mixed-domain data on the performance of medical language models", "link": "https://academic.oup.com/jamia/advance-article-abstract/doi/10.1093/jamia/ocae120/7680487", "details": "M Griot, C Hemptinne, J Vanderdonckt, D Yuksel - Journal of the American Medical \u2026, 2024", "abstract": "Objective To optimize the training strategy of large language models for medical applications, focusing on creating clinically relevant systems that efficiently integrate into healthcare settings, while ensuring high standards of accuracy and reliability \u2026"}, {"title": "Bridging Operator Learning and Conditioned Neural Fields: A Unifying Perspective", "link": "https://arxiv.org/pdf/2405.13998", "details": "S Wang, JH Seidman, S Sankaran, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Operator learning is an emerging area of machine learning which aims to learn mappings between infinite dimensional function spaces. Here we uncover a connection between operator learning architectures and conditioned neural fields \u2026"}, {"title": "Compressed-Language Models for Understanding Compressed File Formats: a JPEG Exploration", "link": "https://arxiv.org/pdf/2405.17146", "details": "JC P\u00e9rez, A Pardo, M Soldan, H Itani, J Leon-Alcazar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study investigates whether Compressed-Language Models (CLMs), ie language models operating on raw byte streams from Compressed File Formats~(CFFs), can understand files compressed by CFFs. We focus on the JPEG format as a \u2026"}, {"title": "GECKO: Generative Language Model for English, Code and Korean", "link": "https://arxiv.org/pdf/2405.15640", "details": "S Oh, D Kim - arXiv preprint arXiv:2405.15640, 2024", "abstract": "We introduce GECKO, a bilingual large language model (LLM) optimized for Korean and English, along with programming languages. GECKO is pretrained on the balanced, high-quality corpus of Korean and English employing LLaMA architecture \u2026"}, {"title": "PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting", "link": "https://arxiv.org/pdf/2405.16829", "details": "Z Wang, D Xu - arXiv preprint arXiv:2405.16829, 2024", "abstract": "Neural Radiance Fields (NeRFs) have demonstrated remarkable proficiency in synthesizing photorealistic images of large-scale scenes. However, they are often plagued by a loss of fine details and long rendering durations. 3D Gaussian Splatting \u2026"}, {"title": "EndoDAC: Efficient Adapting Foundation Model for Self-Supervised Depth Estimation from Any Endoscopic Camera", "link": "https://arxiv.org/pdf/2405.08672", "details": "B Cui, M Islam, L Bai, A Wang, H Ren - arXiv preprint arXiv:2405.08672, 2024", "abstract": "Depth estimation plays a crucial role in various tasks within endoscopic surgery, including navigation, surface reconstruction, and augmented reality visualization. Despite the significant achievements of foundation models in vision tasks, including \u2026"}, {"title": "Backdoor Removal for Generative Large Language Models", "link": "https://arxiv.org/pdf/2405.07667", "details": "H Li, Y Chen, Z Zheng, Q Hu, C Chan, H Liu, Y Song - arXiv preprint arXiv \u2026, 2024", "abstract": "With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased \u2026"}, {"title": "Exploring and Mitigating Shortcut Learning for Generative Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.602.pdf", "details": "Z Sun, Y Xiao, J Li, Y Ji, W Chen, M Zhang - Proceedings of the 2024 Joint \u2026, 2024", "abstract": "Recent generative large language models (LLMs) have exhibited incredible instruction-following capabilities while keeping strong task completion ability, even without task-specific fine-tuning. Some works attribute this to the bonus of the new \u2026"}, {"title": "Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "link": "https://arxiv.org/pdf/2405.10431", "details": "S Furniturewala, S Jandial, A Java, P Banerjee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing debiasing techniques are typically training-based or require access to the model's internals and output distributions, so they are inaccessible to end-users looking to adapt LLM outputs for their particular needs. In this study, we examine \u2026"}]
