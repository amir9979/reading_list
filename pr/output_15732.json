[{"title": "Video Summarization with Large Language Models", "link": "https://arxiv.org/pdf/2504.11199", "details": "MJ Lee, D Gong, M Cho - arXiv preprint arXiv:2504.11199, 2025", "abstract": "The exponential increase in video content poses significant challenges in terms of efficient navigation, search, and retrieval, thus requiring advanced video summarization techniques. Existing video summarization methods, which heavily \u2026"}, {"title": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection", "link": "https://arxiv.org/pdf/2504.00695", "details": "X Zhu, Z Gu, S Zheng, T Wang, T Li, H Feng, Y Xiao - arXiv preprint arXiv:2504.00695, 2025", "abstract": "Pre-training large language models (LLMs) necessitates enormous diverse textual corpora, making effective data selection a key challenge for balancing computational resources and model performance. Current methodologies primarily emphasize data \u2026"}, {"title": "Modality-aware contrast and fusion for multi-modal summarization", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225007660", "details": "L Dai, T Han, Z Yu, J Yu, M Tan, Y Liu - Neurocomputing, 2025", "abstract": "Abstract Multimodal Summarization with Multi-modal Output (MSMO) is an emerging field focused on generating reliable and high-quality summaries by integrating various media types, such as text and video. Current methods primarily focus on \u2026"}, {"title": "Evaluating the Confidentiality of Synthetic Clinical Texts Generated by Language Models", "link": "https://hal.science/hal-05046326v1/file/AIME_Confidentiality_of_Synthetic_Clinical_Corpora.pdf", "details": "F Estignard, S Ghannay, J Girard-Satabin, N Hiebel\u2026 - \u2026 International Conference on \u2026, 2025", "abstract": "Large Language Models (LLMs) can be used to produce synthetic documents that mimic real documents when these are not available due to confidentiality or copyright restrictions. Herein, we investigate potential privacy breaches in \u2026"}, {"title": "Slow-fast architecture for video multi-modal large language models", "link": "https://arxiv.org/pdf/2504.01328%3F", "details": "M Shi, S Wang, CY Chen, J Jain, K Wang, J Xiong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using \u2026"}, {"title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual Tokens in Ineffective Layers", "link": "https://arxiv.org/pdf/2504.00502", "details": "Q Yuan, Q Zhang, Y Liu, J Chen, Y Lu, H Lin, J Zheng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal Large Language Models (MLLMs) suffer from high computational costs due to their massive size and the large number of visual tokens. In this paper, we investigate layer-wise redundancy in MLLMs by introducing a novel metric, Layer \u2026"}, {"title": "Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation", "link": "https://arxiv.org/pdf/2504.02411%3F", "details": "A Misrahi, N Chirkova, M Louis, V Nikoulina - arXiv preprint arXiv:2504.02411, 2025", "abstract": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, but multi-domain applications face challenges like lack of diverse benchmarks and poor out-of-domain generalization. The first contribution of this work is to introduce a diverse benchmark \u2026"}, {"title": "OrchMLLM: Orchestrate Multimodal Data with Batch Post-Balancing to Accelerate Multimodal Large Language Model Training", "link": "https://arxiv.org/pdf/2503.23830", "details": "Y Zheng, B Xiao, L Shi, X Li, F Wu, T Li, X Xiao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal large language models (MLLMs), such as GPT-4o, are garnering significant attention. During the exploration of MLLM training, we identified Modality Composition Incoherence, a phenomenon that the proportion of a certain modality \u2026"}]
