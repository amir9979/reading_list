[{"title": "Towards normalized clinical information extraction in Chinese radiology report with large language models", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425002076", "details": "Q Xu, X Xu, C Zhou, Z Liu, F Huang, S Li, L Zhu, Z Bai\u2026 - Expert Systems with \u2026, 2025", "abstract": "Radiology reports serve as a fundamental component within electronic medical records. Converting unstructured free-text reports into structured formats holds paramount importance for the management and utilization of radiology reports. In this \u2026"}, {"title": "KIA: Knowledge-Guided Implicit Vision-Language Alignment for Chest X-Ray Report Generation", "link": "https://aclanthology.org/2025.coling-main.276.pdf", "details": "H Yin, S Zhou, P Wang, Z Wu, Y Hao - \u2026 of the 31st International Conference on \u2026, 2025", "abstract": "Report generation (RG) faces challenges in understanding complex medical images and establishing cross-modal semantic alignment in radiology image-report pairs. Previous methods often overlook fine-grained cross-modal interaction, leading to \u2026"}, {"title": "Histoires Morales: A French Dataset for Assessing Moral Alignment", "link": "https://arxiv.org/pdf/2501.17117%3F", "details": "T Leteno, I Proskurina, A Gourru, J Velcin, C Laclau\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Aligning language models with human values is crucial, especially as they become more integrated into everyday life. While models are often adapted to user preferences, it is equally important to ensure they align with moral norms and \u2026"}, {"title": "Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval", "link": "https://arxiv.org/pdf/2501.09134", "details": "D Deanda, YP Masupalli, J Yang, Y Lee, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical images and reports offer invaluable insights into patient health. The heterogeneity and complexity of these data hinder effective analysis. To bridge this gap, we investigate contrastive learning models for cross-domain retrieval, which \u2026"}, {"title": "FreqSpace-NeRF: A fourier-enhanced Neural Radiance Fields method via dual-domain contrastive learning for novel view synthesis", "link": "https://www.sciencedirect.com/science/article/pii/S009784932500010X", "details": "X Yu, X Tian, J Chen, Y Wang - Computers & Graphics, 2025", "abstract": "Abstract Inspired by Neural Radiance Field's (NeRF) groundbreaking success in novel view synthesis, current methods mostly employ variants of various deep neural network architectures, and use the combination of multi-scale feature maps with the \u2026"}, {"title": "Leveraging Language Models for Summarizing Mental State Examinations: A Comprehensive Evaluation and Dataset Release", "link": "https://aclanthology.org/2025.coling-main.182.pdf", "details": "NK Sahu, M Yadav, M Chaturvedi, S Gupta, HR Lone - Proceedings of the 31st \u2026, 2025", "abstract": "Mental health disorders affect a significant portion of the global population, with diagnoses primarily conducted through Mental State Examinations (MSEs). MSEs serve as structured assessments to evaluate behavioral and cognitive functioning \u2026"}, {"title": "Advancing General Multimodal Capability of Vision-language Models with Pyramid-descent Visual Position Encoding", "link": "https://arxiv.org/pdf/2501.10967", "details": "Z Chen, M Li, Z Chen, N Du, X Li, Y Zou - arXiv preprint arXiv:2501.10967, 2025", "abstract": "Vision-language Models (VLMs) have shown remarkable capabilities in advancing general artificial intelligence, yet the irrational encoding of visual positions persists in inhibiting the models' comprehensive perception performance across different levels \u2026"}, {"title": "Ocean-OCR: Towards General OCR Application via a Vision-Language Model", "link": "https://arxiv.org/pdf/2501.15558", "details": "S Chen, X Guo, Y Li, T Zhang, M Lin, D Kuang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal large language models (MLLMs) have shown impressive capabilities across various domains, excelling in processing and understanding information from multiple modalities. Despite the rapid progress made previously, insufficient OCR \u2026"}, {"title": "LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding", "link": "https://arxiv.org/pdf/2501.08282", "details": "H Li, J Chen, Z Wei, S Huang, T Hui, J Gao, X Wei\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in multimodal large language models (MLLMs) have shown promising results, yet existing approaches struggle to effectively handle both temporal and spatial localization simultaneously. This challenge stems from two key \u2026"}]
