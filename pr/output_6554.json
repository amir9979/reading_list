[{"title": "Understanding Defects in Generated Codes by Language Models", "link": "https://arxiv.org/pdf/2408.13372", "details": "AM Esfahani, N Kahani, SA Ajila - arXiv preprint arXiv:2408.13372, 2024", "abstract": "This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the \u2026"}, {"title": "MobileQuant: Mobile-friendly Quantization for On-device Language Models", "link": "https://arxiv.org/pdf/2408.13933", "details": "F Tan, R Lee, \u0141 Dudziak, SX Hu, S Bhattacharya\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have revolutionized language processing, delivering outstanding results across multiple applications. However, deploying LLMs on edge devices poses several challenges with respect to memory, energy, and compute \u2026"}, {"title": "Importance Weighting Can Help Large Language Models Self-Improve", "link": "https://arxiv.org/pdf/2408.09849", "details": "C Jiang, C Chan, W Xue, Q Liu, Y Guo - arXiv preprint arXiv:2408.09849, 2024", "abstract": "Large language models (LLMs) have shown remarkable capability in numerous tasks and applications. However, fine-tuning LLMs using high-quality datasets under external supervision remains prohibitively expensive. In response, LLM self \u2026"}, {"title": "Calibration and correctness of language models for code", "link": "https://software-lab.org/publications/icse2025_calibration.pdf", "details": "C Spiess, D Gros, KS Pai, M Pradel, MRI Rabin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Machine learning models are widely used, but can also often be wrong. Users would benefit from a reliable indication of whether a given output from a given model should be trusted, so a rational decision can be made whether to use the output or \u2026"}, {"title": "CogLM: Tracking Cognitive Development of Large Language Models", "link": "https://arxiv.org/pdf/2408.09150", "details": "X Wang, P Yuan, S Feng, Y Li, B Pan, H Wang, Y Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Piaget's Theory of Cognitive Development (PTC) posits that the development of cognitive levels forms the foundation for human learning across various abilities. As Large Language Models (LLMs) have recently shown remarkable abilities across a \u2026"}, {"title": "Designing Retrieval-Augmented Language Models for Clinical Decision Support", "link": "https://link.springer.com/chapter/10.1007/978-3-031-63592-2_13", "details": "K Quigley, T Koker, J Taylor, V Mancuso, L Brattain - AI for Health Equity and Fairness \u2026, 2024", "abstract": "Ever-increasing demands for physician expertise drive the need for trustworthy point- of-care tools that can help aid decision-making in all clinical settings. Retrieval- augmented language models carry potential to relieve the information burden on \u2026"}, {"title": "Benchmarking Large Language Models for Math Reasoning Tasks", "link": "https://arxiv.org/pdf/2408.10839", "details": "K Se\u00dfler, Y Rong, E G\u00f6zl\u00fckl\u00fc, E Kasneci - arXiv preprint arXiv:2408.10839, 2024", "abstract": "The use of Large Language Models (LLMs) in mathematical reasoning has become a cornerstone of related research, demonstrating the intelligence of these models and enabling potential practical applications through their advanced performance \u2026"}, {"title": "Scaling Pre-training Data and Language Models for African Languages", "link": "https://uwspace.uwaterloo.ca/bitstreams/0eb3cf2b-5660-492e-8be6-6f4d4e82f5e1/download", "details": "A Oladipo - 2024", "abstract": "Recent advancements in language models, particularly for high-resource languages, have not been paralleled in low-resource languages spoken across Africa. This thesis addresses this gap by scaling pre-training data and developing improved \u2026"}, {"title": "Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language Models", "link": "https://arxiv.org/pdf/2408.14853", "details": "Y Du, Z Li, P Cheng, X Wan, A Gao - arXiv preprint arXiv:2408.14853, 2024", "abstract": "Large Language Models (LLMs) have become a focal point in the rapidly evolving field of artificial intelligence. However, a critical concern is the presence of toxic content within the pre-training corpus of these models, which can lead to the \u2026"}]
