[{"title": "MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models", "link": "https://arxiv.org/pdf/2408.02718", "details": "F Meng, J Wang, C Li, Q Lu, H Tian, J Liao, X Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The capability to process multiple images is crucial for Large Vision-Language Models (LVLMs) to develop a more thorough and nuanced understanding of a scene. Recent multi-image LVLMs have begun to address this need. However, their \u2026"}, {"title": "A Unified Understanding of Adversarial Vulnerability Regarding Unimodal Models and Vision-Language Pre-training Models", "link": "https://arxiv.org/pdf/2407.17797", "details": "H Zheng, X Deng, W Jiang, W Li - arXiv preprint arXiv:2407.17797, 2024", "abstract": "With Vision-Language Pre-training (VLP) models demonstrating powerful multimodal interaction capabilities, the application scenarios of neural networks are no longer confined to unimodal domains but have expanded to more complex multimodal V+ L \u2026"}, {"title": "LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization and Classification Task", "link": "https://arxiv.org/pdf/2407.12064", "details": "K Le-Duc, R Zhang, NS Nguyen, TH Pham, A Dao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language models have been extensively explored across a wide range of tasks, achieving satisfactory performance; however, their application in medical imaging remains underexplored. In this work, we propose a unified framework \u2026"}, {"title": "Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models", "link": "https://arxiv.org/pdf/2407.11717", "details": "C Ju, H Wang, H Cheng, X Chen, Z Zhai, W Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-Language Large Models (VLMs) recently become primary backbone of AI, due to the impressive performance. However, their expensive computation costs, ie, throughput and delay, impede potentials in the real-world scenarios. To achieve \u2026"}, {"title": "Cross-Class Domain Adaptive Semantic Segmentation with Visual Language Models", "link": "https://openreview.net/pdf%3Fid%3DTjFn6xktTm", "details": "W Ren, R Xia, M Zheng, Z Wu, Y Tang, N Sebe - ACM Multimedia 2024", "abstract": "This paper addresses the issue of cross-class domain adaptation (CCDA) in semantic segmentation, where the target domain contains both shared and novel classes that are either unlabeled or unseen in the source domain. This problem is \u2026"}, {"title": "Enhancing Clinical Relevance of Pretrained Language Models Through Integration of External Knowledge: Case Study on Cardiovascular Diagnosis From Electronic \u2026", "link": "https://ai.jmir.org/2024/1/e56932/", "details": "Q Lu, A Wen, T Nguyen, H Liu - JMIR AI, 2024", "abstract": "Background: Despite their growing use in health care, pretrained language models (PLMs) often lack clinical relevance due to insufficient domain expertise and poor interpretability. A key strategy to overcome these challenges is integrating external \u2026"}, {"title": "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "link": "https://arxiv.org/pdf/2408.02103", "details": "P Wang, X Wang, C Lou, S Mao, P Xie, Y Jiang - arXiv preprint arXiv:2408.02103, 2024", "abstract": "In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language \u2026"}, {"title": "2D Neural Fields with Learned Discontinuities", "link": "https://arxiv.org/abs/2408.00771", "details": "C Liu, S Wang, M Fisher, D Aneja, A Jacobson - arXiv preprint arXiv:2408.00771, 2024", "abstract": "Effective representation of 2D images is fundamental in digital image processing, where traditional methods like raster and vector graphics struggle with sharpness and textural complexity respectively. Current neural fields offer high-fidelity and \u2026"}]
