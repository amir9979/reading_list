[{"title": "Do we really have to filter out random noise in pre-training data for language models?", "link": "https://arxiv.org/pdf/2502.06604", "details": "J Ru, Y Xie, X Zhuang, Y Yin, Y Zou - arXiv preprint arXiv:2502.06604, 2025", "abstract": "Web-scale pre-training datasets are the cornerstone of LLMs' success. However, text data curated from the internet inevitably contains random noise caused by decoding errors or unregulated web content. In contrast to previous works that focus on low \u2026"}, {"title": "Scaling Embedding Layers in Language Models", "link": "https://arxiv.org/pdf/2502.01637%3F", "details": "D Yu, E Cohen, B Ghazi, Y Huang, P Kamath, R Kumar\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We propose SCONE ($\\textbf {S} $ calable, $\\textbf {C} $ ontextualized, $\\textbf {O} $ ffloaded, $\\textbf {N} $-gram $\\textbf {E} $ mbedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To \u2026"}, {"title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features", "link": "https://arxiv.org/pdf/2502.14786", "details": "M Tschannen, A Gritsenko, X Wang, MF Naeem\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed \u2026"}, {"title": "ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model", "link": "https://arxiv.org/pdf/2502.14420", "details": "Z Zhou, Y Zhu, M Zhu, J Wen, N Liu, Z Xu, W Meng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world. Why can't large language models replicate this holistic understanding? Through a systematic analysis of existing training paradigms in \u2026"}, {"title": "HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding", "link": "https://arxiv.org/pdf/2501.15111", "details": "J Zhao, Q Yang, Y Peng, D Bai, S Yao, B Sun, X Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In human-centric scenes, the ability to simultaneously understand visual and auditory information is crucial. While recent omni models can process multiple modalities, they generally lack effectiveness in human-centric scenes due to the absence of \u2026"}, {"title": "Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text Alignment", "link": "https://arxiv.org/pdf/2501.19060", "details": "SL Lv, YY Chen, Z Zhou, YF Li, LZ Guo - arXiv preprint arXiv:2501.19060, 2025", "abstract": "Vision-language models (VLMs), such as CLIP, have demonstrated exceptional generalization capabilities and can quickly adapt to downstream tasks through prompt fine-tuning. Unfortunately, in classification tasks involving non-training \u2026"}, {"title": "Reusing Embeddings: Reproducible Reward Model Research in Large Language Model Alignment without GPUs", "link": "https://arxiv.org/pdf/2502.04357", "details": "H Sun, Y Shen, JF Ton, M van der Schaar - arXiv preprint arXiv:2502.04357, 2025", "abstract": "Large Language Models (LLMs) have made substantial strides in structured tasks through Reinforcement Learning (RL), demonstrating proficiency in mathematical reasoning and code generation. However, applying RL in broader domains like \u2026"}, {"title": "Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective", "link": "https://arxiv.org/pdf/2502.01524%3F", "details": "X Ma, H Xie, SJ Qin - arXiv preprint arXiv:2502.01524, 2025", "abstract": "The integration of vision-language modalities has been a significant focus in multimodal learning, traditionally relying on Vision-Language Pretrained Models. However, with the advent of Large Language Models (LLMs), there has been a \u2026"}, {"title": "Calling a Spade a Heart: Gaslighting Multimodal Large Language Models via Negation", "link": "https://arxiv.org/pdf/2501.19017%3F", "details": "B Zhu, Y Gui, J Chen, CW Ngo, EP Lim - arXiv preprint arXiv:2501.19017, 2025", "abstract": "Multimodal Large Language Models (MLLMs) have exhibited remarkable advancements in integrating different modalities, excelling in complex understanding and generation tasks. Despite their success, MLLMs remain vulnerable to \u2026"}]
