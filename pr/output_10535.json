[{"title": "MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization", "link": "https://arxiv.org/pdf/2412.06141", "details": "K Zhu, P Xia, Y Li, H Zhu, S Wang, H Yao - arXiv preprint arXiv:2412.06141, 2024", "abstract": "The advancement of Large Vision-Language Models (LVLMs) has propelled their application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter factuality challenges due to modality misalignment, where the models prioritize \u2026"}, {"title": "A Multimodal Biomedical Foundation Model Trained from Fifteen Million Image\u2013Text Pairs", "link": "https://ai.nejm.org/doi/abs/10.1056/AIoa2400640", "details": "S Zhang, Y Xu, N Usuyama, H Xu, J Bagga, R Tinn\u2026 - NEJM AI, 2024", "abstract": "Background Biomedical data are inherently multimodal, comprising physical measurements and natural-language narratives. A generalist biomedical artificial intelligence (AI) model needs to simultaneously process different modalities of data \u2026"}, {"title": "Detection of Patient-Level Immunotherapy-Related Adverse Events (irAEs) from Clinical Narratives of Electronic Health Records: A High-Sensitivity Artificial \u2026", "link": "https://www.tandfonline.com/doi/pdf/10.2147/POR.S468253", "details": "MM Zitu, ME Gatti-Mays, KC Johnson, S Zhang\u2026 - Pragmatic and \u2026, 2024", "abstract": "Purpose We developed an artificial intelligence (AI) model to detect immunotherapy- related adverse events (irAEs) from clinical narratives of electronic health records (EHRs) at the patient level. Patients and Methods Training data, used for internal \u2026"}, {"title": "Language Models as Continuous Self-Evolving Data Engineers", "link": "https://arxiv.org/pdf/2412.15151", "details": "P Wang, M Wang, Z Ma, X Yang, S Feng, D Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert \u2026"}, {"title": "Multimodal Whole Slide Foundation Model for Pathology", "link": "https://arxiv.org/pdf/2411.19666", "details": "T Ding, SJ Wagner, AH Song, RJ Chen, MY Lu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The field of computational pathology has been transformed with recent advances in foundation models that encode histopathology region-of-interests (ROIs) into versatile and transferable feature representations via self-supervised learning (SSL) \u2026"}, {"title": "Training Agents with Weakly Supervised Feedback from Large Language Models", "link": "https://arxiv.org/pdf/2411.19547", "details": "D Gong, P Lu, Z Wang, M Zhou, X He - arXiv preprint arXiv:2411.19547, 2024", "abstract": "Large Language Models (LLMs) offer a promising basis for creating agents that can tackle complex tasks through iterative environmental interaction. Existing methods either require these agents to mimic expert-provided trajectories or rely on definitive \u2026"}, {"title": "DoubleCCA: Improving Foundation Model Group Robustness with Random Sentence Embeddings", "link": "https://arxiv.org/pdf/2411.16236", "details": "H Liu, Y Lu - arXiv preprint arXiv:2411.16236, 2024", "abstract": "This paper presents a novel method to improve the robustness of foundation models to group-based biases. We propose a simple yet effective method, called DoubleCCA, that leverages random sentences and Canonical Correlation Analysis \u2026"}, {"title": "Large language models: game-changers in the healthcare industry", "link": "https://pubmed.ncbi.nlm.nih.gov/39674769/", "details": "B Dong, L Zhang, J Yuan, Y Chen, Q Li, L Shen - Science bulletin, 2024", "abstract": "Large language models: game-changers in the healthcare industry Large language models: game-changers in the healthcare industry Sci Bull (Beijing). 2024 Nov 26:S2095-9273(24)00847-8. doi: 10.1016/j.scib.2024.11.031. Online ahead of print. Authors Bin Dong 1 , Li Zhang \u2026"}, {"title": "Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model Fine-tuning", "link": "https://arxiv.org/pdf/2412.14780", "details": "Z Ye, Z Zhang, Y Zhang, J Ma, J Lin, F Feng - arXiv preprint arXiv:2412.14780, 2024", "abstract": "When using agent-task datasets to enhance agent capabilities for Large Language Models (LLMs), current methodologies often treat all tokens within a sample equally. However, we argue that tokens serving different roles-specifically, reasoning tokens \u2026"}]
