[{"title": "COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3717867.3717923", "details": "P Govil, H Jain, V Bonagiri, A Chadha, P Kumaraguru\u2026 - Proceedings of the 17th \u2026, 2025", "abstract": "Large Language Models (LLMs) often inherit biases from the web data they are trained on, which contains stereotypes and prejudices. Current methods for evaluating and mitigating these biases rely on bias-benchmark datasets. These \u2026"}, {"title": "Confidence-calibrated covariate shift correction for few-shot classification in Vision-Language Models", "link": "https://openaccess.thecvf.com/content/CVPR2025W/DG-EBF/papers/Khan_Confidence-calibrated_covariate_shift_correction_for_few-shot_classification_in_Vision-Language_Models_CVPRW_2025_paper.pdf", "details": "B Khan, R Qureshi, NM Durrani, TQ Syed - Proceedings of the Computer Vision and \u2026, 2025", "abstract": "Since the establishment of vision-language foundation models as the new mainstay in low-shot vision classification tasks, the question of domain generalization arising from insufficient target data is assuming more importance. This scarcity challenge \u2026"}, {"title": "Can Large Language Models Predict Audio Effects Parameters from Natural Language?", "link": "https://arxiv.org/pdf/2505.20770", "details": "S Doh, J Koo, MA Mart\u00ednez-Ram\u00edrez, WH Liao, J Nam\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In music production, manipulating audio effects (Fx) parameters through natural language has the potential to reduce technical barriers for non-experts. We present LLM2Fx, a framework leveraging Large Language Models (LLMs) to predict Fx \u2026", "entry_id": "http://arxiv.org/abs/2505.20770v1", "updated": "2025-05-28 00:29:19", "published": "2025-05-27 06:21:56", "authors": "Seungheon Doh;Junghyun Koo;Marco A. Mart\u00ednez-Ram\u00edrez;Wei-Hsiang Liao;Juhan Nam;Yuki Mitsufuji", "summary": "In music production, manipulating audio effects (Fx) parameters through natural language has the potential to reduce technical barriers for non-experts. We present LLM2Fx, a framework leveraging Large Language Models (LLMs) to predict Fx parameters directly from textual descriptions without requiring task-specific training or fine-tuning. Our approach address the text-to-effect parameter prediction (Text2Fx) task by mapping natural language descriptions to the corresponding Fx parameters for equalization and reverberation. We demonstrate that LLMs can generate Fx parameters in a zero-shot manner that elucidates the relationship between timbre semantics and audio effects in music production. To enhance performance, we introduce three types of in-context examples: audio Digital Signal Processing (DSP) features, DSP function code, and few-shot examples. Our results demonstrate that LLM-based Fx parameter generation outperforms previous optimization approaches, offering competitive performance in translating natural language descriptions to appropriate Fx settings. Furthermore, LLMs can serve as text-driven interfaces for audio production, paving the way for more intuitive and accessible music production tools.", "comment": "Submitted to WASPAA 2025", "journal_ref": null, "primary_category": "cs.SD", "categories": "cs.SD;cs.MM;eess.AS", "links": "https://arxiv.org/abs/2505.20770v1;https://arxiv.org/pdf/2505.20770v1", "pdf_url": null}, {"title": "GainRAG: Preference Alignment in Retrieval-Augmented Generation through Gain Signal Synthesis", "link": "https://arxiv.org/pdf/2505.18710", "details": "Y Jiang, S Zhao, J Li, H Wang, B Qin - arXiv preprint arXiv:2505.18710, 2025", "abstract": "The Retrieval-Augmented Generation (RAG) framework introduces a retrieval module to dynamically inject retrieved information into the input context of large language models (LLMs), and has demonstrated significant success in various NLP \u2026", "entry_id": "http://arxiv.org/abs/2505.18710v1", "updated": "2025-05-27 00:35:35", "published": "2025-05-24 14:14:57", "authors": "Yi Jiang;Sendong Zhao;Jianbo Li;Haochun Wang;Bing Qin", "summary": "The Retrieval-Augmented Generation (RAG) framework introduces a retrieval module to dynamically inject retrieved information into the input context of large language models (LLMs), and has demonstrated significant success in various NLP tasks. However, the current study points out that there is a preference gap between retrievers and LLMs in the RAG framework, which limit the further improvement of system performance. Some highly relevant passages may interfere with LLM reasoning because they contain complex or contradictory information; while some indirectly related or even inaccurate content may help LLM generate more accurate answers by providing suggestive information or logical clues. To solve this, we propose GainRAG, a novel approach that aligns the retriever's and LLM's preferences by defining a new metric, \"gain\", which measure how well an input passage contributes to correct outputs. Specifically, we propose a method to estimate these gain signals and train a middleware that aligns the preferences of the retriever and the LLM using only limited data. In addition, we introduce a pseudo-passage strategy to mitigate degradation. The experimental results on 6 datasets verify the effectiveness of GainRAG.", "comment": "Accepted by ACL 2025", "journal_ref": null, "primary_category": "cs.IR", "categories": "cs.IR;cs.AI", "links": "https://arxiv.org/abs/2505.18710v1;https://arxiv.org/pdf/2505.18710v1", "pdf_url": null}, {"title": "Towards Large Language Models with Self-Consistent Natural Language Explanations", "link": "https://arxiv.org/pdf/2506.07523", "details": "S Admoni, O Amir, A Hallak, Y Ziser - arXiv preprint arXiv:2506.07523, 2025", "abstract": "Large language models (LLMs) seem to offer an easy path to interpretability: just ask them to explain their decisions. Yet, studies show that these post-hoc explanations often misrepresent the true decision process, as revealed by mismatches in feature \u2026", "entry_id": "http://arxiv.org/abs/2506.07523v2", "updated": "2025-06-13 00:31:23", "published": "2025-06-09 08:06:33", "authors": "Sahar Admoni;Ofra Amir;Assaf Hallak;Yftah Ziser", "summary": "Large language models (LLMs) seem to offer an easy path to interpretability: just ask them to explain their decisions. Yet, studies show that these post-hoc explanations often misrepresent the true decision process, as revealed by mismatches in feature importance. Despite growing evidence of this inconsistency, no systematic solutions have emerged, partly due to the high cost of estimating feature importance, which limits evaluations to small datasets. To address this, we introduce the Post-hoc Self-Consistency Bank (PSCB) - a large-scale benchmark of decisions spanning diverse tasks and models, each paired with LLM-generated explanations and corresponding feature importance scores. Analysis of PSCB reveals that self-consistency scores barely differ between correct and incorrect predictions. We also show that the standard metric fails to meaningfully distinguish between explanations. To overcome this limitation, we propose an alternative metric that more effectively captures variation in explanation quality. We use it to fine-tune LLMs via Direct Preference Optimization (DPO), leading to significantly better alignment between explanations and decision-relevant features, even under domain shift. Our findings point to a scalable path toward more trustworthy, self-consistent LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "https://arxiv.org/abs/2506.07523v2;https://arxiv.org/pdf/2506.07523v2", "pdf_url": null}, {"title": "Large Language Models in Medical Image Understanding", "link": "https://www.taylorfrancis.com/chapters/edit/10.1201/9781032632483-7/large-language-models-medical-image-understanding-ghada-khoriba-muhammad-nouman-essam-rashed", "details": "G Khoriba, M Nouman, EA Rashed - Cutting-Edge Artificial Intelligence Advances and \u2026, 2025", "abstract": "Large language models (LLMs) have emerged as a leading technology in medical image understanding, representing a transformative development in the healthcare field. Trained on extensive datasets, these models can generate human-like reports \u2026"}]
