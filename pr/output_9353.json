[{"title": "Membership Inference Attacks against Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.02902", "details": "Z Li, Y Wu, Y Chen, F Tonin, EA Rocamora, V Cevher - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of \u2026"}, {"title": "Unsupervised Foundation Model-Agnostic Slide-Level Representation Learning", "link": "https://arxiv.org/pdf/2411.13623", "details": "T Lenz, P Neidlinger, M Ligero, G W\u00f6lflein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Representation learning of pathology whole-slide images (WSIs) has primarily relied on weak supervision with Multiple Instance Learning (MIL). This approach leads to slide representations highly tailored to a specific clinical task. Self-supervised \u2026"}, {"title": "Multifaceted Natural Language Processing Task\u2013Based Evaluation of Bidirectional Encoder Representations From Transformers Models for Bilingual (Korean and \u2026", "link": "https://medinform.jmir.org/2024/1/e52897/", "details": "K Kim, S Park, J Min, S Park, JY Kim, J Eun, K Jung\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background: The bidirectional encoder representations from transformers (BERT) model has attracted considerable attention in clinical applications, such as patient classification and disease prediction. However, current studies have typically \u2026"}, {"title": "Learning predictable and robust neural representations by straightening image sequences", "link": "https://arxiv.org/pdf/2411.01777", "details": "X Niu, C Savin, EP Simoncelli - arXiv preprint arXiv:2411.01777, 2024", "abstract": "Prediction is a fundamental capability of all living organisms, and has been proposed as an objective for learning sensory representations. Recent work demonstrates that in primate visual systems, prediction is facilitated by neural representations that \u2026"}, {"title": "AutoBench-V: Can Large Vision-Language Models Benchmark Themselves?", "link": "https://arxiv.org/pdf/2410.21259%3F", "details": "H Bao, Y Huang, Y Wang, J Ye, X Wang, X Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) have become essential for advancing the integration of visual and linguistic information, facilitating a wide range of complex applications and tasks. However, the evaluation of LVLMs presents significant \u2026"}, {"title": "Looking Beyond Text: Reducing Language bias in Large Vision-Language Models via Multimodal Dual-Attention and Soft-Image Guidance", "link": "https://arxiv.org/pdf/2411.14279", "details": "H Zhao, S Si, L Chen, Y Zhang, M Sun, M Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision-language models (LVLMs) have achieved impressive results in various vision-language tasks. However, despite showing promising performance, LVLMs suffer from hallucinations caused by language bias, leading to diminished focus on \u2026"}, {"title": "Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset", "link": "https://arxiv.org/pdf/2411.03554", "details": "Y Ma, J Wang, F Wang, S Ma, J Li, X Li, F Huang, L Sun\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Machine unlearning has emerged as an effective strategy for forgetting specific information in the training data. However, with the increasing integration of visual data, privacy concerns in Vision Language Models (VLMs) remain underexplored. To \u2026"}, {"title": "MWVOS: Mask-Free Weakly Supervised Video Object Segmentation via promptable foundation model", "link": "https://www.sciencedirect.com/science/article/pii/S0031320324008513", "details": "Z Zhang, S Zhang, Z Dai, Z Dong, S Zhu - Pattern Recognition, 2024", "abstract": "The current state-of-the-art techniques for video object segmentation necessitate extensive training on video datasets with mask annotations, thereby constraining their ability to transfer zero-shot learning to new image distributions and tasks \u2026"}, {"title": "Classification Done Right for Vision-Language Pre-Training", "link": "https://openreview.net/pdf%3Fid%3DHd2EOwKItm", "details": "Z Huang, Q Ye, B Kang, J Feng, H Fan - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data. Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised \u2026"}]
