[{"title": "Continually Self-Improving Language Models for Bariatric Surgery Question--Answering", "link": "https://arxiv.org/pdf/2505.16102", "details": "YK Atri, TH Shin, T Hartvigsen - arXiv preprint arXiv:2505.16102, 2025", "abstract": "\u2026 However, this process is often hindered by numerous **healthcare** disparities, such as logistical and access barriers, which impair easy \u2026 (RAG)-based model that autonomously integrates real-time **medical** evidence when response confidence \u2026", "entry_id": "http://arxiv.org/abs/2505.16102v1", "updated": "2025-05-22 01:02:51", "published": "2025-05-22 01:02:51", "authors": "Yash Kumar Atri;Thomas H Shin;Thomas Hartvigsen", "summary": "While bariatric and metabolic surgery (MBS) is considered the gold standard\ntreatment for severe and morbid obesity, its therapeutic efficacy hinges upon\nactive and longitudinal engagement with multidisciplinary providers, including\nsurgeons, dietitians/nutritionists, psychologists, and endocrinologists. This\nengagement spans the entire patient journey, from preoperative preparation to\nlong-term postoperative management. However, this process is often hindered by\nnumerous healthcare disparities, such as logistical and access barriers, which\nimpair easy patient access to timely, evidence-based, clinician-endorsed\ninformation. To address these gaps, we introduce bRAGgen, a novel adaptive\nretrieval-augmented generation (RAG)-based model that autonomously integrates\nreal-time medical evidence when response confidence dips below dynamic\nthresholds. This self-updating architecture ensures that responses remain\ncurrent and accurate, reducing the risk of misinformation. Additionally, we\npresent bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,\nvalidated by an expert bariatric surgeon. bRAGq constitutes the first\nlarge-scale, domain-specific benchmark for comprehensive MBS care. In a\ntwo-phase evaluation, bRAGgen is benchmarked against state-of-the-art models\nusing both large language model (LLM)--based metrics and expert surgeon review.\nAcross all evaluation dimensions, bRAGgen demonstrates substantially superior\nperformance in generating clinically accurate and relevant responses.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.16102v1;http://arxiv.org/pdf/2505.16102v1", "pdf_url": "http://arxiv.org/pdf/2505.16102v1"}, {"title": " **Large Language Models** in Portuguese for **Healthcare** : A Systematic Review", "link": "https://www.researchsquare.com/article/rs-6673690/latest", "details": "AM Shimaoka, AC da Silva Junior, JM Duarte\u2026 - 2025", "abstract": "\u2026 This study addresses **Large** **Language** **Models** (LLMs) pre-trained in Portuguese for **healthcare** \u2026 general **medicine** , COVID-19, oncology, and other related areas, accomplishing classification tasks, followed by named entity recognition (NER), topic \u2026"}, {"title": "Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization", "link": "https://arxiv.org/pdf/2505.15918", "details": "A Nafar, KB Venable, Z Cui, P Kordjamshidi - arXiv preprint arXiv:2505.15918, 2025", "abstract": "\u2026 There is no definitive correct **answer** to infer directly from the **question** itself; however, a **medical** expert familiar with the literature might approximate 20%. Similarly, we expect an adept language model to produce a similar estimate. Here \u2026", "entry_id": "http://arxiv.org/abs/2505.15918v1", "updated": "2025-05-21 18:15:05", "published": "2025-05-21 18:15:05", "authors": "Aliakbar Nafar;Kristen Brent Venable;Zijun Cui;Parisa Kordjamshidi", "summary": "Large Language Models (LLMs) have demonstrated potential as factual knowledge\nbases; however, their capability to generate probabilistic knowledge about\nreal-world events remains understudied. This paper investigates using\nprobabilistic knowledge inherent in LLMs to derive probability estimates for\nstatements concerning events and their interrelationships captured via a\nBayesian Network (BN). Using LLMs in this context allows for the\nparameterization of BNs, enabling probabilistic modeling within specific\ndomains. Experiments on eighty publicly available Bayesian Networks, from\nhealthcare to finance, demonstrate that querying LLMs about the conditional\nprobabilities of events provides meaningful results when compared to baselines,\nincluding random and uniform distributions, as well as approaches based on\nnext-token generation probabilities. We explore how these LLM-derived\ndistributions can serve as expert priors to refine distributions extracted from\nminimal data, significantly reducing systematic biases. Overall, this work\nintroduces a promising strategy for automatically constructing Bayesian\nNetworks by combining probabilistic knowledge extracted from LLMs with small\namounts of real-world data. Additionally, we evaluate several prompting\nstrategies for eliciting probabilistic knowledge from LLMs and establish the\nfirst comprehensive baseline for assessing LLM performance in extracting\nprobabilistic knowledge.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;I.2.7", "links": "http://arxiv.org/abs/2505.15918v1;http://arxiv.org/pdf/2505.15918v1", "pdf_url": "http://arxiv.org/pdf/2505.15918v1"}, {"title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models", "link": "https://arxiv.org/pdf/2505.16211", "details": "K Li, C Shen, Y Liu, J Han, K Zheng, X Zou, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 We further divide jailbreak attacks into three domains: enterprise systems, financial systems, and **healthcare** systems, each with 100 test items \u2026 To evaluate privacy leakage in ALLMs, we use an audio **question** **answering** (AQA) framework \u2026", "entry_id": "http://arxiv.org/abs/2505.16211v1", "updated": "2025-05-22 04:27:46", "published": "2025-05-22 04:27:46", "authors": "Kai Li;Can Shen;Yile Liu;Jirui Han;Kelong Zheng;Xuechao Zou;Zhe Wang;Xingjian Du;Shun Zhang;Hanjun Luo;Yingbin Jin;Xinxin Xing;Ziyang Ma;Yue Liu;Xiaojun Jia;Yifan Zhang;Junfeng Fang;Kun Wang;Yibo Yan;Haoyang Li;Yiming Li;Xiaobin Zhuang;Yang Liu;Haibo Hu;Zhuo Chen;Zhizheng Wu;Xiaolin Hu;Eng-Siong Chng;XiaoFeng Wang;Wenyuan Xu;Wei Dong;Xinfeng Li", "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.", "comment": "Technical Report", "journal_ref": null, "primary_category": "cs.SD", "categories": "cs.SD;cs.AI;cs.CL;eess.AS", "links": "http://arxiv.org/abs/2505.16211v1;http://arxiv.org/pdf/2505.16211v1", "pdf_url": "http://arxiv.org/pdf/2505.16211v1"}, {"title": "Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools", "link": "https://arxiv.org/pdf/2505.16113", "details": "P Lymperopoulos, V Sarathy - arXiv preprint arXiv:2505.16113, 2025", "abstract": "\u2026 As **large** **language** **models** (LLMs) have been increasingly deployed in practical applications, their **problem** -solving capacity has enabled \u2026 For example, in a **medical** application that suggests treatments to **medical** professionals, the external \u2026", "entry_id": "http://arxiv.org/abs/2505.16113v1", "updated": "2025-05-22 01:34:23", "published": "2025-05-22 01:34:23", "authors": "Panagiotis Lymperopoulos;Vasanth Sarathy", "summary": "Modern Large Language Models (LLMs) often require external tools, such as\nmachine learning classifiers or knowledge retrieval systems, to provide\naccurate answers in domains where their pre-trained knowledge is insufficient.\nThis integration of LLMs with external tools expands their utility but also\nintroduces a critical challenge: determining the trustworthiness of responses\ngenerated by the combined system. In high-stakes applications, such as medical\ndecision-making, it is essential to assess the uncertainty of both the LLM's\ngenerated text and the tool's output to ensure the reliability of the final\nresponse. However, existing uncertainty quantification methods do not account\nfor the tool-calling scenario, where both the LLM and external tool contribute\nto the overall system's uncertainty. In this work, we present a novel framework\nfor modeling tool-calling LLMs that quantifies uncertainty by jointly\nconsidering the predictive uncertainty of the LLM and the external tool. We\nextend previous methods for uncertainty quantification over token sequences to\nthis setting and propose efficient approximations that make uncertainty\ncomputation practical for real-world applications. We evaluate our framework on\ntwo new synthetic QA datasets, derived from well-known machine learning\ndatasets, which require tool-calling for accurate answers. Additionally, we\napply our method to retrieval-augmented generation (RAG) systems and conduct a\nproof-of-concept experiment demonstrating the effectiveness of our uncertainty\nmetrics in scenarios where external information retrieval is needed. Our\nresults show that the framework is effective in enhancing trust in LLM-based\nsystems, especially in cases where the LLM's internal knowledge is insufficient\nand external tools are required.", "comment": "10 pages 3 figures 3 tables", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CL", "links": "http://arxiv.org/abs/2505.16113v1;http://arxiv.org/pdf/2505.16113v1", "pdf_url": "http://arxiv.org/pdf/2505.16113v1"}, {"title": "SzegedAI at ArchEHR-QA 2025: Combining LLMs with traditional methods for grounded **question answering**", "link": "https://www.kispeterzsm.com/files/pdf/SzegedAI-ArchEHR-2025.pdf", "details": "SB Nagy, B Nyerges, ZM Kisp\u00e9ter, G T\u00f3th, AT Szl\u00faka\u2026", "abstract": "\u2026 Our approaches include multiple prompting techniques for **large** **language** **models** (LLMs), sentence similarity methods, and traditional \u2026 tasks using DPO and a curated **medical** instruction dataset. Llama3Med42-70B is optimized for **medical** \u2026"}, {"title": "CT-Agent: A Multimodal-LLM Agent for 3D CT Radiology Question Answering", "link": "https://arxiv.org/pdf/2505.16229", "details": "Y Mao, W Xu, Y Qin, Y Gao - arXiv preprint arXiv:2505.16229, 2025", "abstract": "\u2026 propose CT-Agent, a multimodal **large** **language** **models** (LLMs)-driven agentic framework \u2026 With the rise of **large** **language** **models** (LLMs) in the **medical** domain, approaches such as \u2026 ] integrate 3D vision encoders with **large** **language** **models** \u2026", "entry_id": "http://arxiv.org/abs/2505.16229v1", "updated": "2025-05-22 04:59:20", "published": "2025-05-22 04:59:20", "authors": "Yuren Mao;Wenyi Xu;Yuyang Qin;Yunjun Gao", "summary": "Computed Tomography (CT) scan, which produces 3D volumetric medical data that\ncan be viewed as hundreds of cross-sectional images (a.k.a. slices), provides\ndetailed anatomical information for diagnosis. For radiologists, creating CT\nradiology reports is time-consuming and error-prone. A visual question\nanswering (VQA) system that can answer radiologists' questions about some\nanatomical regions on the CT scan and even automatically generate a radiology\nreport is urgently needed. However, existing VQA systems cannot adequately\nhandle the CT radiology question answering (CTQA) task for: (1) anatomic\ncomplexity makes CT images difficult to understand; (2) spatial relationship\nacross hundreds slices is difficult to capture. To address these issues, this\npaper proposes CT-Agent, a multimodal agentic framework for CTQA. CT-Agent\nadopts anatomically independent tools to break down the anatomic complexity;\nfurthermore, it efficiently captures the across-slice spatial relationship with\na global-local token compression strategy. Experimental results on two 3D chest\nCT datasets, CT-RATE and RadGenome-ChestCT, verify the superior performance of\nCT-Agent.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.16229v1;http://arxiv.org/pdf/2505.16229v1", "pdf_url": "http://arxiv.org/pdf/2505.16229v1"}, {"title": "**LARGE LANGUAGE MODELS** IN (NOT ONLY) PSYCHOLOGY TRAINING AND RESEARCH: A BRIEF INTRODUCTION", "link": "https://www.researchgate.net/profile/Matus-Adamkovic-2/publication/391941171_Large_Language_Models_in_Not_Only_Psychology_Training_and_Research_A_Brief_Introduction/links/682dcf5f026fee1034f9deca/Large-Language-Models-in-Not-Only-Psychology-Training-and-Research-A-Brief-Introduction.pdf", "details": "M Adamkovi\u010d", "abstract": "\u2026 Its applications span across disciplines, from **healthcare** and finance to education and behavioral science, where it is reshaping \u2026 The book begins by laying a foundation in the basics of **large** **language** **models** (LLMs), covering their architecture \u2026"}, {"title": "ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts", "link": "https://arxiv.org/pdf/2505.16566", "details": "D Noh, D Koh, J Yuk, G Kim, J Lee, K Lim, C Park - arXiv preprint arXiv:2505.16566, 2025", "abstract": "\u2026 Prior benchmarks for evaluating the domainspecific knowledge of **large** **language** **models** (LLMs) lack the scalability to handle complex \u2026 MedMCQA covers 2,400 **medical** topics and 21 **medical** subjects, and contains a total of more than 194,000 \u2026", "entry_id": "http://arxiv.org/abs/2505.16566v1", "updated": "2025-05-22 11:59:06", "published": "2025-05-22 11:59:06", "authors": "Dongwon Noh;Donghyeok Koh;Junghun Yuk;Gyuwan Kim;Jaeyong Lee;Kyungtae Lim;Cheoneum Park", "summary": "Prior benchmarks for evaluating the domain-specific knowledge of large\nlanguage models (LLMs) lack the scalability to handle complex academic tasks.\nTo address this, we introduce \\texttt{ScholarBench}, a benchmark centered on\ndeep expert knowledge and complex academic problem-solving, which evaluates the\nacademic reasoning ability of LLMs and is constructed through a three-step\nprocess. \\texttt{ScholarBench} targets more specialized and logically complex\ncontexts derived from academic literature, encompassing five distinct problem\ntypes. Unlike prior benchmarks, \\texttt{ScholarBench} evaluates the\nabstraction, comprehension, and reasoning capabilities of LLMs across eight\ndistinct research domains. To ensure high-quality evaluation data, we define\ncategory-specific example attributes and design questions that are aligned with\nthe characteristic research methodologies and discourse structures of each\ndomain. Additionally, this benchmark operates as an English-Korean bilingual\ndataset, facilitating simultaneous evaluation for linguistic capabilities of\nLLMs in both languages. The benchmark comprises 5,031 examples in Korean and\n5,309 in English, with even state-of-the-art models like o3-mini achieving an\naverage evaluation score of only 0.543, demonstrating the challenging nature of\nthis benchmark.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.16566v1;http://arxiv.org/pdf/2505.16566v1", "pdf_url": "http://arxiv.org/pdf/2505.16566v1"}]
