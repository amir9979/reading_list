[{"title": "Foundations of Large Language Models", "link": "https://arxiv.org/pdf/2501.09223", "details": "T Xiao, J Zhu - arXiv preprint arXiv:2501.09223, 2025", "abstract": "This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting- edge technologies. The book is structured into four main chapters, each exploring a \u2026"}, {"title": "LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2501.06986", "details": "MN Azadani, J Riddell, S Sedwards, K Czarnecki - arXiv preprint arXiv:2501.06986, 2025", "abstract": "Enhanced visual understanding serves as a cornerstone for multimodal large language models (MLLMs). Recent hybrid MLLMs incorporate a mixture of vision experts to address the limitations of using a single vision encoder and excessively \u2026"}, {"title": "ArithmeticGPT: empowering small-size large language models with advanced arithmetic skills", "link": "https://link.springer.com/article/10.1007/s10994-024-06681-1", "details": "Z Liu, Y Zheng, Z Yin, J Chen, T Liu, M Tian, W Luo - Machine Learning, 2025", "abstract": "Large language models (LLMs) have shown remarkable capabilities in understanding and generating language across a wide range of domains. However, their performance in advanced arithmetic calculation remains a significant challenge \u2026"}, {"title": "Benchmarking Large Language Models via Random Variables", "link": "https://arxiv.org/pdf/2501.11790", "details": "Z Hong, H Wu, S Dong, J Dong, Y Xiao, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the continuous advancement of large language models (LLMs) in mathematical reasoning, evaluating their performance in this domain has become a prominent research focus. Recent studies have raised concerns about the reliability of current \u2026"}]
