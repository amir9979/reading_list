[{"title": "Differentially Private and Heterogeneity-Robust Federated Learning with Theoretical Guarantee", "link": "https://ieeexplore.ieee.org/abstract/document/10643038/", "details": "X Wang, S Wang, Y Li, F Fan, S Li, X Lin - IEEE Transactions on Artificial Intelligence, 2024", "abstract": "Federated learning (FL) is a popular distributed paradigm where enormous clients collaboratively train a machine learning (ML) model under the orchestration of a central server without knowing the clients' private raw data. The development of \u2026"}, {"title": "Do Vision Foundation Models Enhance Domain Generalization in Medical Image Segmentation?", "link": "https://arxiv.org/pdf/2409.07960", "details": "K Cekmeceli, M Himmetoglu, GI Tombak, A Susmelj\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Neural networks achieve state-of-the-art performance in many supervised learning tasks when the training data distribution matches the test data distribution. However, their performance drops significantly under domain (covariate) shift, a prevalent \u2026"}, {"title": "Natural Language Explainable Recommendation with Robustness Enhancement", "link": "https://dl.acm.org/doi/abs/10.1145/3637528.3671781", "details": "J Zhang, J Tang, X Chen, W Yu, L Hu, P Jiang, H Li - Proceedings of the 30th ACM \u2026, 2024", "abstract": "Natural language explainable recommendation has become a promising direction to facilitate more efficient and informed user decisions. Previous models mostly focus on how to enhance the explanation accuracy. However, the robustness problem has \u2026"}, {"title": "A Comprehensive Analysis of Memorization in Large Language Models", "link": "https://aclanthology.org/2024.inlg-main.45.pdf", "details": "H Kiyomaru, I Sugiura, D Kawahara, S Kurohashi - Proceedings of the 17th \u2026, 2024", "abstract": "This paper presents a comprehensive study that investigates memorization in large language models (LLMs) from multiple perspectives. Experiments are conducted with the Pythia and LLM-jp model suites, both of which offer LLMs with over 10B \u2026"}]
