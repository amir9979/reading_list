[{"title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models", "link": "https://arxiv.org/pdf/2407.03181", "details": "H Puerto, T Chubakov, X Zhu, HT Madabushi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model \u2026"}, {"title": "Multi-turn Response Selection with Commonsense-enhanced Language Models", "link": "https://arxiv.org/pdf/2407.18479", "details": "Y Wang, X Ren, T Chen, Y Dong, NQV Hung, J Tang - arXiv preprint arXiv \u2026, 2024", "abstract": "As a branch of advanced artificial intelligence, dialogue systems are prospering. Multi-turn response selection is a general research problem in dialogue systems. With the assistance of background information and pre-trained language models, the \u2026"}, {"title": "ViT-Based Multi-task Learning Method for Pulmonary Embolism Detection, Localization, and Type Classification", "link": "https://link.springer.com/chapter/10.1007/978-981-97-5692-6_41", "details": "AN Mohammed, H Kuang, J Wang - International Conference on Intelligent \u2026, 2024", "abstract": "Pulmonary Embolism (PE) is a life-threatening disease that causes a significant number of deaths annually. Accurate detection of PE from computed tomography pulmonary angiography (CTPA) scans assists healthcare professionals in making \u2026"}, {"title": "BadCLM: Backdoor Attack in Clinical Language Models for Electronic Health Records", "link": "https://arxiv.org/pdf/2407.05213", "details": "W Lyu, Z Bi, F Wang, C Chen - arXiv preprint arXiv:2407.05213, 2024", "abstract": "The advent of clinical language models integrated into electronic health records (EHR) for clinical decision support has marked a significant advancement, leveraging the depth of clinical notes for improved decision-making. Despite their \u2026"}, {"title": "AI Safety in Generative AI Large Language Models: A Survey", "link": "https://arxiv.org/pdf/2407.18369", "details": "J Chua, Y Li, S Yang, C Wang, L Yao - arXiv preprint arXiv:2407.18369, 2024", "abstract": "Large Language Model (LLMs) such as ChatGPT that exhibit generative AI capabilities are facing accelerated adoption and innovation. The increased presence of Generative AI (GAI) inevitably raises concerns about the risks and safety \u2026"}, {"title": "Speculative Speech Recognition by Audio-Prefixed Low-Rank Adaptation of Language Models", "link": "https://arxiv.org/pdf/2407.04641", "details": "B Yusuf, MK Baskar, A Rosenberg, B Ramabhadran - arXiv preprint arXiv:2407.04641, 2024", "abstract": "This paper explores speculative speech recognition (SSR), where we empower conventional automatic speech recognition (ASR) with speculation capabilities, allowing the recognizer to run ahead of audio. We introduce a metric for measuring \u2026"}, {"title": "Towards smaller language models via layer looping", "link": "https://openreview.net/pdf%3Fid%3D2N3CtUdoB0", "details": "S Eyuboglu, D Zinsley, J Saad-Falcon, S Arora\u2026 - Workshop on Efficient Systems for \u2026", "abstract": "Language models store a huge amount of knowledge in their parameters. This dominant architecture bears little resemblance to the implementations of optimized data stores (eg a database management system like PostgreSQL), which begs the \u2026"}, {"title": "Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment", "link": "https://arxiv.org/pdf/2407.03051", "details": "J Lee, S Park, S Hong, M Kim, DS Chang, J Choi - arXiv preprint arXiv:2407.03051, 2024", "abstract": "The rapid advancement of large language models (LLMs) has facilitated their transformation into conversational chatbots that can grasp contextual nuances and generate pertinent sentences, closely mirroring human values through advanced \u2026"}, {"title": "LLMBox: A Comprehensive Library for Large Language Models", "link": "https://arxiv.org/pdf/2407.05563", "details": "T Tang, Y Hu, B Li, W Luo, Z Qin, H Sun, J Wang, S Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To facilitate the research on large language models (LLMs), this paper presents a comprehensive and unified library, LLMBox, to ease the development, use, and evaluation of LLMs. This library is featured with three main merits:(1) a unified data \u2026"}]
