[{"title": "ExSPIN: Explicit Feedback-Based Self-Play Fine-Tuning for Text-to-SQL Parsing", "link": "https://www.mdpi.com/1099-4300/27/3/235", "details": "L Yan, J Su, C Liu, S Duan, Y Zhang, J Li, P Han, Y Liu - Entropy, 2025", "abstract": "Recently, self-play fine-tuning (SPIN) has garnered widespread attention as it enables large language models (LLMs) to iteratively enhance their capabilities through simulated interactions with themselves, transforming a weak LLM into a \u2026"}, {"title": "Mapping 1,000+ Language Models via the Log-Likelihood Vector", "link": "https://arxiv.org/pdf/2502.16173", "details": "M Oyama, H Yamagiwa, Y Takase, H Shimodaira - arXiv preprint arXiv:2502.16173, 2025", "abstract": "To compare autoregressive language models at scale, we propose using log- likelihood vectors computed on a predefined text set as model features. This approach has a solid theoretical basis: when treated as model coordinates, their \u2026"}, {"title": "CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport Alignment for Language Models with Different Tokenizers", "link": "https://arxiv.org/pdf/2502.16806", "details": "AD Le, T Vu, NL Hai, NTN Diep, LN Van, T Le\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) achieve state-of-the-art performance across various NLP tasks but face deployment challenges due to high computational costs and memory constraints. Knowledge distillation (KD) is a promising solution, transferring \u2026"}, {"title": "Every Expert Matters: Towards Effective Knowledge Distillation for Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2502.12947", "details": "G Kim, G Chu, E Yang - arXiv preprint arXiv:2502.12947, 2025", "abstract": "With the emergence of Mixture-of-Experts (MoE), the efficient scaling of model size has accelerated the development of large language models in recent years. However, their high memory requirements prevent their use in resource-constrained \u2026"}, {"title": "Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images", "link": "https://arxiv.org/pdf/2502.13928", "details": "S Wu, FY Sun, K Wen, N Haber - arXiv preprint arXiv:2502.13928, 2025", "abstract": "Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises \u2026"}, {"title": "Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models", "link": "https://arxiv.org/pdf/2502.15964", "details": "A Narayan, D Biderman, S Eyuboglu, A May\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We investigate an emerging setup in which a small, on-device language model (LM) with access to local data communicates with a frontier, cloud-hosted LM to solve real- world tasks involving financial, medical, and scientific reasoning over long \u2026"}, {"title": "Optimizing Temperature for Language Models with Multi-Sample Inference", "link": "https://arxiv.org/pdf/2502.05234", "details": "W Du, Y Yang, S Welleck - arXiv preprint arXiv:2502.05234, 2025", "abstract": "Multi-sample aggregation strategies, such as majority voting and best-of-N sampling, are widely used in contemporary large language models (LLMs) to enhance predictive accuracy across various tasks. A key challenge in this process is \u2026"}, {"title": "SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label Recognition in Vision-Language Models", "link": "https://arxiv.org/pdf/2502.16911", "details": "K Miller, S Mishra, A Gangrade, K Saenko, V Saligrama - arXiv preprint arXiv \u2026, 2025", "abstract": "Zero-shot multi-label recognition (MLR) with Vision-Language Models (VLMs) faces significant challenges without training data, model tuning, or architectural modifications. Existing approaches require prompt tuning or architectural \u2026"}, {"title": "EPERM: An Evidence Path Enhanced Reasoning Model for Knowledge Graph Question and Answering", "link": "https://arxiv.org/pdf/2502.16171", "details": "X Long, L Zhuang, A Li, M Yao, S Wang - arXiv preprint arXiv:2502.16171, 2025", "abstract": "Due to the remarkable reasoning ability, Large language models (LLMs) have demonstrated impressive performance in knowledge graph question answering (KGQA) tasks, which find answers to natural language questions over knowledge \u2026"}]
