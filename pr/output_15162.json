[{"title": "STAF-LLM: A scalable and task-adaptive fine-tuning framework for large language models in medical domain", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425012047", "details": "T Xu, L Chen, Z Hu, B Li - Expert Systems with Applications, 2025", "abstract": "Recent large language models (LLMs) have demonstrated remarkable performance across various NLP tasks. However, their application in the medical domain is often limited by a lack of specialized medical knowledge, which is crucial for practical \u2026"}, {"title": "HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/32171/34326", "details": "KHI Arif, JY Yoon, DS Nikolopoulos, H Vandierendonck\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "Abstract High-resolution Vision-Language Models (VLMs) are widely used in multimodal tasks to enhance accuracy by preserving detailed image information. However, these models often generate an excessive number of visual tokens due to \u2026"}, {"title": "ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models", "link": "https://arxiv.org/pdf/2503.19355%3F", "details": "D Ko, S Kim, Y Suh, M Yoon, M Chandraker, HJ Kim - arXiv preprint arXiv \u2026, 2025", "abstract": "Spatio-temporal reasoning is essential in understanding real-world environments in various fields, eg, autonomous driving and sports analytics. Recent advances have improved the spatial reasoning ability of Vision-Language Models (VLMs) by \u2026"}, {"title": "Unified Knowledge Maintenance Pruning and Progressive Recovery with Weight Recalling for Large Vision-Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/32923/35078", "details": "Z Wu, J Chen, Y Wang - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Abstract Large Vision-Language Model (LVLM), leveraging Large Language Model (LLM) as the cognitive core, has recently become one of the most representative multimodal model paradigms. However, with the expansion of unimodal \u2026"}, {"title": "Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving", "link": "https://arxiv.org/pdf/2503.21505%3F", "details": "Y Li, M Tian, Z Lin, J Zhu, D Zhu, H Liu, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing benchmarks for Vision-Language Model (VLM) on autonomous driving (AD) primarily assess interpretability through open-form visual question answering (QA) within coarse-grained tasks, which remain insufficient to assess capabilities in \u2026"}, {"title": "Towards Understanding How Knowledge Evolves in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2504.02862", "details": "S Wang, Y Zhang, Y Zhu, J Li, Z Wang, Y Liu, X Ji - arXiv preprint arXiv:2504.02862, 2025", "abstract": "Large Vision-Language Models (LVLMs) are gradually becoming the foundation for many artificial intelligence applications. However, understanding their internal working mechanisms has continued to puzzle researchers, which in turn limits the \u2026"}, {"title": "FATE: Feature-Adapted Parameter Tuning for Vision-Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/32975/35130", "details": "Z Xu, Z Peng, X Yang, W Shen - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Following the recent popularity of vision language models, several attempts, eg, parameter-efficient fine-tuning (PEFT), have been made to extend them to different downstream tasks. Previous PEFT works motivate their methods from the view of \u2026"}, {"title": "Can Vision-Language Models Answer Face to Face Questions in the Real-World?", "link": "https://arxiv.org/pdf/2503.19356", "details": "R Pourreza, R Dagli, A Bhattacharyya, S Panchal\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "AI models have made significant strides in recent years in their ability to describe and answer questions about real-world images. They have also made progress in the ability to converse with users in real-time using audio input. This raises the question \u2026"}, {"title": "Beyond Standard MoE: Mixture of Latent Experts for Resource-Efficient Language Models", "link": "https://arxiv.org/pdf/2503.23100", "details": "Z Liu, H Wu, R She, X Fu, X Han, T Zhong, M Yuan - arXiv preprint arXiv:2503.23100, 2025", "abstract": "Mixture of Experts (MoE) has emerged as a pivotal architectural paradigm for efficient scaling of Large Language Models (LLMs), operating through selective activation of parameter subsets for each input token. Nevertheless, conventional MoE \u2026"}]
