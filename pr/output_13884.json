[{"title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?", "link": "https://arxiv.org/pdf/2503.02199", "details": "A Deng, T Cao, Z Chen, B Hooi - arXiv preprint arXiv:2503.02199, 2025", "abstract": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual \u2026"}, {"title": "Evaluation of Safety Cognition Capability in Vision-Language Models for Autonomous Driving", "link": "https://arxiv.org/pdf/2503.06497", "details": "E Zhang, P Gong, X Dai, Y Lv, Q Miao - arXiv preprint arXiv:2503.06497, 2025", "abstract": "Assessing the safety of vision-language models (VLMs) in autonomous driving is particularly important; however, existing work mainly focuses on traditional benchmark evaluations. As interactive components within autonomous driving \u2026"}, {"title": "Boosting the Generalization and Reasoning of Vision Language Models with Curriculum Reinforcement Learning", "link": "https://arxiv.org/pdf/2503.07065", "details": "H Deng, D Zou, R Ma, H Luo, Y Cao, Y Kang - arXiv preprint arXiv:2503.07065, 2025", "abstract": "While state-of-the-art vision-language models (VLMs) have demonstrated remarkable capabilities in complex visual-text tasks, their success heavily relies on massive model scaling, limiting their practical deployment. Small-scale VLMs offer a \u2026"}, {"title": "UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces", "link": "https://arxiv.org/pdf/2503.06157", "details": "B Zhao, J Fang, Z Dai, Z Wang, J Zha, W Zhang, C Gao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large multimodal models exhibit remarkable intelligence, yet their embodied cognitive abilities during motion in open-ended urban 3D space remain to be explored. We introduce a benchmark to evaluate whether video-large language \u2026"}, {"title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models", "link": "https://arxiv.org/pdf/2502.17387", "details": "A Albalak, D Phung, N Lile, R Rafailov, K Gandhi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written \u2026"}, {"title": "LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle Zero-shot Radiology Recognition?", "link": "https://arxiv.org/pdf/2503.07487", "details": "B Li, W Huang, Y Shen, Y Wang, S Lin, J Lin, L You\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, multimodal large models (MLLMs) have demonstrated exceptional capabilities in visual understanding and reasoning across various vision-language tasks. However, MLLMs usually perform poorly in zero-shot medical disease \u2026"}, {"title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning", "link": "https://arxiv.org/pdf/2503.07459", "details": "X Tang, D Shao, J Sohn, J Chen, J Zhang, J Xiang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods \u2026"}, {"title": "Fine-grained knowledge fusion for retrieval-augmented medical visual question answering", "link": "https://www.sciencedirect.com/science/article/pii/S1566253525001320", "details": "X Liang, D Wang, B Jing, Z Jiao, R Li, R Liu, Q Miao\u2026 - Information Fusion, 2025", "abstract": "Given that medical image analysis often requires experts to recall typical symptoms from diagnostic archives or their own experience, implementing retrieval augmentation in multi-modal tasks like Medical Visual Question Answering \u2026"}, {"title": "Open-Source Large Language Models as Multilingual Crowdworkers: Synthesizing Open-Domain Dialogues in Several Languages With No Examples in Targets and \u2026", "link": "https://arxiv.org/pdf/2503.03462", "details": "A Njifenjou, V Sucal, B Jabaian, F Lef\u00e8vre - arXiv preprint arXiv:2503.03462, 2025", "abstract": "The prevailing paradigm in the domain of Open-Domain Dialogue agents predominantly focuses on the English language, encompassing both models and datasets. Furthermore, the financial and temporal investments required for \u2026"}]
