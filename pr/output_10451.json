[{"title": "Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models", "link": "https://arxiv.org/pdf/2412.12606", "details": "YF Zhang, S Lei, R Qiao, Z GongQue, X Song, G Dong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapidly developing field of large multimodal models (LMMs) has led to the emergence of diverse models with remarkable capabilities. However, existing benchmarks fail to comprehensively, objectively and accurately evaluate whether \u2026"}, {"title": "Exploring Multi-Grained Concept Annotations for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2412.05939", "details": "X Xu, T Niu, Y Xie, L Qin, W Che, MY Kan - arXiv preprint arXiv:2412.05939, 2024", "abstract": "Multimodal Large Language Models (MLLMs) excel in vision--language tasks by pre- training solely on coarse-grained concept annotations (eg, image captions). We hypothesize that integrating fine-grained concept annotations (eg, object labels and \u2026"}, {"title": "FlashSloth: Lightning Multimodal Large Language Models via Embedded Visual Compression", "link": "https://arxiv.org/pdf/2412.04317", "details": "B Tong, B Lai, Y Zhou, G Luo, Y Shen, K Li, X Sun, R Ji - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite a big leap forward in capability, multimodal large language models (MLLMs) tend to behave like a sloth in practical use, ie, slow response and large latency. Recent efforts are devoted to building tiny MLLMs for better efficiency, but the \u2026"}, {"title": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over Aligned Large Language Models", "link": "https://arxiv.org/pdf/2412.12865", "details": "Y Fan, Y Hong, Q Wang, J Bao, H Jiang, Y Song - arXiv preprint arXiv:2412.12865, 2024", "abstract": "Alignment, endowing a pre-trained Large language model (LLM) with the ability to follow instructions, is crucial for its real-world applications. Conventional supervised fine-tuning (SFT) methods formalize it as causal language modeling typically with a \u2026"}, {"title": "AD-LLM: Benchmarking Large Language Models for Anomaly Detection", "link": "https://arxiv.org/pdf/2412.11142", "details": "T Yang, Y Nian, S Li, R Xu, Y Li, J Li, Z Xiao, X Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Anomaly detection (AD) is an important machine learning task with many real-world uses, including fraud detection, medical diagnosis, and industrial monitoring. Within natural language processing (NLP), AD helps detect issues like spam \u2026"}, {"title": "GeoTool-GPT: a trainable method for facilitating Large Language Models to master GIS tools", "link": "https://www.tandfonline.com/doi/abs/10.1080/13658816.2024.2438937", "details": "C Wei, Y Zhang, X Zhao, Z Zeng, Z Wang, J Lin\u2026 - International Journal of \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) excel in natural language-relevant tasks like text generation and question answering Q&A. To further expand their application, efforts focus on enabling LLMs to utilize real-world tools. However, their tool-use \u2026"}, {"title": "Training Large Language Models to Reason in a Continuous Latent Space", "link": "https://arxiv.org/pdf/2412.06769%3F", "details": "S Hao, S Sukhbaatar, DJ Su, X Li, Z Hu, J Weston\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) are restricted to reason in the\" language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may \u2026"}, {"title": "XTransplant: A Probe into the Upper Bound Performance of Multilingual Capability and Culture Adaptability in LLMs via Mutual Cross-lingual Feed-forward \u2026", "link": "https://arxiv.org/pdf/2412.12686", "details": "Y Ye, X Feng, X Feng, L Qin, Y Huang, L Huang, W Ma\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Current large language models (LLMs) often exhibit imbalances in multilingual capabilities and cultural adaptability, largely due to their English-centric pretraining data. To address this imbalance, we propose a probing method named XTransplant \u2026"}, {"title": "Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models", "link": "https://arxiv.org/pdf/2412.08615", "details": "J Li, Y Hao, H Xu, X Wang, Y Hong - arXiv preprint arXiv:2412.08615, 2024", "abstract": "Despite the advancements in training Large Language Models (LLMs) with alignment techniques to enhance the safety of generated content, these models remain susceptible to jailbreak, an adversarial attack method that exposes security \u2026"}]
