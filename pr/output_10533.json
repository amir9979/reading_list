[{"title": "Memory-Tuning: A Unified Parameter-Efficient Tuning Method for Pre-trained Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10769026/", "details": "W Qi, R Liu, Y Zuo, F Li, Y Chen, J Wu - IEEE/ACM Transactions on Audio, Speech \u2026, 2024", "abstract": "Conventional fine-tuning encounters increasing difficulties given the size of current Pre-trained Language Models, which makes parameter-efficient tuning become the focal point of frontier research. Recent advances in this field is the unified tuning \u2026"}, {"title": "Mastering Board Games by External and Internal Planning with Language Models", "link": "https://arxiv.org/pdf/2412.12119%3F", "details": "J Schultz, J Adamek, M Jusup, M Lanctot, M Kaisers\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While large language models perform well on a range of complex tasks (eg, text generation, question answering, summarization), robust multi-step planning and reasoning remains a considerable challenge for them. In this paper we show that \u2026"}, {"title": "Self-Generated Critiques Boost Reward Modeling for Language Models", "link": "https://arxiv.org/pdf/2411.16646%3F", "details": "Y Yu, Z Chen, A Zhang, L Tan, C Zhu, RY Pang, Y Qian\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to \u2026"}, {"title": "Why language models collapse when trained on recursively generated text", "link": "https://arxiv.org/pdf/2412.14872", "details": "L Wang, X Shi, G Li, J Li, Y Dong, X Zhang, W Jiao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) have been widely used to generate text on the Internet. The generated text is often collected into the training corpus of the next generations of LMs. Previous work has experimentally found that LMs collapse when trained on \u2026"}, {"title": "A Multimodal Biomedical Foundation Model Trained from Fifteen Million Image\u2013Text Pairs", "link": "https://ai.nejm.org/doi/abs/10.1056/AIoa2400640", "details": "S Zhang, Y Xu, N Usuyama, H Xu, J Bagga, R Tinn\u2026 - NEJM AI, 2024", "abstract": "Background Biomedical data are inherently multimodal, comprising physical measurements and natural-language narratives. A generalist biomedical artificial intelligence (AI) model needs to simultaneously process different modalities of data \u2026"}, {"title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models", "link": "https://arxiv.org/pdf/2411.19477", "details": "Y Chen, X Pan, Y Li, B Ding, J Zhou - arXiv preprint arXiv:2411.19477, 2024", "abstract": "We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates $ N $ candidate solutions, and then chooses the \u2026"}, {"title": "Large language models: game-changers in the healthcare industry", "link": "https://pubmed.ncbi.nlm.nih.gov/39674769/", "details": "B Dong, L Zhang, J Yuan, Y Chen, Q Li, L Shen - Science bulletin, 2024", "abstract": "Large language models: game-changers in the healthcare industry Large language models: game-changers in the healthcare industry Sci Bull (Beijing). 2024 Nov 26:S2095-9273(24)00847-8. doi: 10.1016/j.scib.2024.11.031. Online ahead of print. Authors Bin Dong 1 , Li Zhang \u2026"}, {"title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents", "link": "https://arxiv.org/pdf/2412.00821", "details": "R Jaiswal, D Jain, HP Popat, A Anand\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in various reasoning tasks. However, they encounter significant challenges when it comes to scientific reasoning, particularly in physics, which requires not only mathematical \u2026"}, {"title": "Neural abstractive summarization: improvements at the sequence-level", "link": "https://dr.ntu.edu.sg/bitstream/10356/181414/2/Mathieu_Ravaut_s_PhD_Thesis_Edited_Final.pdf", "details": "M Ravaut - 2024", "abstract": "Automatic text summarization has made a fantastic leap forward in the last five to ten years, fueled by the rise of deep learning systems. Summarization at large consists in compressing an input text or series of texts (such as a scientific paper, news articles \u2026"}]
