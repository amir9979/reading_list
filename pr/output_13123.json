[{"title": "SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2502.13059", "details": "X Cheng, W Zhang, S Zhang, J Yang, X Guan, X Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The increasing application of multi-modal large language models (MLLMs) across various sectors have spotlighted the essence of their output reliability and accuracy, particularly their ability to produce content grounded in factual information (eg \u2026"}, {"title": "DeepThink: Aligning Language Models with Domain-Specific User Intents", "link": "https://arxiv.org/pdf/2502.05497", "details": "Y Li, M Luo, Y Gong, C Lin, J Jiao, Y Liu, K Huang - arXiv preprint arXiv:2502.05497, 2025", "abstract": "Supervised fine-tuning with synthesized instructions has been a common practice for adapting LLMs to domain-specific QA tasks. However, the synthesized instructions deviate from real user questions and expected answers. This study proposes a novel \u2026"}, {"title": "How Do LLMs Perform Two-Hop Reasoning in Context?", "link": "https://arxiv.org/pdf/2502.13913", "details": "T Guo, H Zhu, R Zhang, J Jiao, S Mei, MI Jordan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\" Socrates is human. All humans are mortal. Therefore, Socrates is mortal.\" This classical example demonstrates two-hop reasoning, where a conclusion logically follows from two connected premises. While transformer-based Large Language \u2026"}, {"title": "Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking", "link": "https://arxiv.org/pdf/2502.12470", "details": "AS Ziabari, N Ghazizadeh, Z Sourati\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) exhibit impressive reasoning abilities, yet their reliance on structured step-by-step processing reveals a critical limitation. While human cognition fluidly adapts between intuitive, heuristic (System 1) and analytical \u2026"}, {"title": "LESA: Learnable LLM Layer Scaling-Up", "link": "https://arxiv.org/pdf/2502.13794", "details": "Y Yang, Z Cao, X Ma, Y Yao, L Qin, Z Chen, H Zhao - arXiv preprint arXiv:2502.13794, 2025", "abstract": "Training Large Language Models (LLMs) from scratch requires immense computational resources, making it prohibitively expensive. Model scaling-up offers a promising solution by leveraging the parameters of smaller models to create larger \u2026"}, {"title": "Self-Enhanced Reasoning Training: Activating Latent Reasoning in Small Models for Enhanced Reasoning Distillation", "link": "https://arxiv.org/pdf/2502.12744", "details": "Y Zhang, B Zhang, Z Li, M Li, N Cheng, M Chen, T Wei\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The rapid advancement of large language models (LLMs) has significantly enhanced their reasoning abilities, enabling increasingly complex tasks. However, these capabilities often diminish in smaller, more computationally efficient models like GPT \u2026"}, {"title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models", "link": "https://arxiv.org/pdf/2502.12420", "details": "S Liu, H Wu, B He, X Han, M Yuan, L Song - arXiv preprint arXiv:2502.12420, 2025", "abstract": "Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task \u2026"}, {"title": "Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer Fine-Grained Experts", "link": "https://arxiv.org/pdf/2502.12928", "details": "L Pan, Z Su, M Lv, Y Xiong, X Zhang, Z Lin, H Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models have demonstrated exceptional performance across a wide range of tasks. However, dense models usually suffer from sparse activation, where many activation values tend towards zero (ie, being inactivated). We argue that this \u2026"}, {"title": "Multilingual Language Model Pretraining using Machine-translated Data", "link": "https://arxiv.org/pdf/2502.13252", "details": "J Wang, Y Lu, M Weber, M Ryabinin, D Adelani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-resource languages such as English, enables the pretraining of high-quality large language models (LLMs). The same can not be said for most other languages as LLMs still underperform for non-English languages, likely due to a gap in the \u2026"}]
