[{"title": "Metalic: Meta-Learning In-Context with Protein Language Models", "link": "https://arxiv.org/pdf/2410.08355", "details": "J Beck, S Surana, M McAuliffe, O Bent, TD Barrett\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Predicting the biophysical and functional properties of proteins is essential for in silico protein design. Machine learning has emerged as a promising technique for such prediction tasks. However, the relative scarcity of in vitro annotations means \u2026"}, {"title": "Learning to Explain is a Good Biomedical Few-Shot Learner", "link": "https://academic.oup.com/bioinformatics/advance-article-pdf/doi/10.1093/bioinformatics/btae589/59554854/btae589.pdf", "details": "P Chen, J Wang, L Luo, H Lin, Z Yang - Bioinformatics, 2024", "abstract": "Motivation Significant progress has been achieved in biomedical text mining using deep learning methods, which rely heavily on large amounts of high-quality data annotated by human experts. However, the reality is that obtaining high-quality \u2026"}, {"title": "Investigating Layer Importance in Large Language Models", "link": "https://arxiv.org/pdf/2409.14381", "details": "Y Zhang, Y Dong, K Kawaguchi - arXiv preprint arXiv:2409.14381, 2024", "abstract": "Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in \u2026"}, {"title": "AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models", "link": "https://arxiv.org/pdf/2410.10912", "details": "H Lu, Y Zhou, S Liu, Z Wang, MW Mahoney, Y Yang - arXiv preprint arXiv:2410.10912, 2024", "abstract": "Recent work on pruning large language models (LLMs) has shown that one can eliminate a large number of parameters without compromising performance, making pruning a promising strategy to reduce LLM model size. Existing LLM pruning \u2026"}, {"title": "Gender Bias in Decision-Making with Large Language Models: A Study of Relationship Conflicts", "link": "https://arxiv.org/pdf/2410.11084", "details": "S Levy, WD Adler, TS Karver, M Dredze, MR Kaufman - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) acquire beliefs about gender from training data and can therefore generate text with stereotypical gender attitudes. Prior studies have demonstrated model generations favor one gender or exhibit stereotypes about \u2026"}, {"title": "One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks", "link": "https://arxiv.org/pdf/2410.11005", "details": "F Lin, S Mao, E La Malfa, V Hofmann, A de Wynter\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language is not monolithic. While many benchmarks are used as proxies to systematically estimate Large Language Models'(LLM) performance in real-life tasks, they tend to ignore the nuances of within-language variation and thus fail to model \u2026"}, {"title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "link": "https://arxiv.org/pdf/2409.13853", "details": "Z Wang, R Bao, Y Wu, J Taylor, C Xiao, F Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pretrained large language models (LLMs) have revolutionized natural language processing (NLP) tasks such as summarization, question answering, and translation. However, LLMs pose significant security risks due to their tendency to memorize \u2026"}, {"title": "From Linguistic Giants to Sensory Maestros: A Survey on Cross-Modal Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2409.18996", "details": "S Qian, Z Zhou, D Xue, B Wang, C Xu - arXiv preprint arXiv:2409.18996, 2024", "abstract": "Cross-modal reasoning (CMR), the intricate process of synthesizing and drawing inferences across divergent sensory modalities, is increasingly recognized as a crucial capability in the progression toward more sophisticated and anthropomorphic \u2026"}, {"title": "Beyond Graphs: Can Large Language Models Comprehend Hypergraphs?", "link": "https://arxiv.org/pdf/2410.10083", "details": "Y Feng, C Yang, X Hou, S Du, S Ying, Z Wu, Y Gao - arXiv preprint arXiv:2410.10083, 2024", "abstract": "Existing benchmarks like NLGraph and GraphQA evaluate LLMs on graphs by focusing mainly on pairwise relationships, overlooking the high-order correlations found in real-world data. Hypergraphs, which can model complex beyond-pairwise \u2026"}]
