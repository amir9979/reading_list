[{"title": "Revisiting Attention for Multivariate Time Series Forecasting", "link": "https://arxiv.org/pdf/2407.13806", "details": "H Wu - arXiv preprint arXiv:2407.13806, 2024", "abstract": "Current Transformer methods for Multivariate Time-Series Forecasting (MTSF) are all based on the conventional attention mechanism. They involve sequence embedding and performing a linear projection of Q, K, and V, and then computing attention within \u2026"}, {"title": "FedGK: Communication-Efficient Federated Learning through Group-Guided Knowledge Distillation", "link": "https://dl.acm.org/doi/pdf/10.1145/3674973", "details": "W Zhang, XL Liu, S Tarkoma - ACM Transactions on Internet Technology, 2024", "abstract": "Federated learning (FL) empowers a cohort of participating devices to contribute collaboratively to a global neural network model, ensuring that their training data remains private and stored locally. Despite its advantages in computational efficiency \u2026"}]
