[{"title": "LEVERAGING VISUAL FOUNDATION MODEL FOR ECHOCARDIOGRAPHY WORKFLOW ENHANCEMENT", "link": "https://www.jacc.org/doi/full/10.1016/S0735-1097%252825%252903068-2", "details": "CJ Chao, Y Gu, W Kumar, T Xiang, L Appari, J Farina\u2026 - Journal of the American \u2026, 2025", "abstract": "Background The vision foundation model,\u201cSegment Anything (SAM),\u201d promises to segment any objects in images. However, the performance of SAM on clinical echocardiography images has yet to be investigated and compared against the state \u2026"}, {"title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement", "link": "https://arxiv.org/pdf/2503.17352", "details": "Y Deng, H Bansal, F Yin, N Peng, W Wang, KW Chang - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex reasoning abilities in large language models (LLMs), including sophisticated behaviors such as self-verification and self-correction, can be achieved by RL with \u2026"}]
