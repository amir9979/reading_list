[{"title": "Generative LLM Powered Conversational AI Application for Personalized Risk Assessment: A Case Study in COVID-19", "link": "https://arxiv.org/pdf/2409.15027", "details": "MA Roshani, X Zhou, Y Qiang, S Suresh, S Hicks\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have shown remarkable capabilities in various natural language tasks and are increasingly being applied in healthcare domains. This work demonstrates a new LLM-powered disease risk assessment approach via \u2026"}, {"title": "The Role of Deductive and Inductive Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2410.02892", "details": "C Cai, X Zhao, H Liu, Z Jiang, T Zhang, Z Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have achieved substantial progress in artificial intelligence, particularly in reasoning tasks. However, their reliance on static prompt structures, coupled with limited dynamic reasoning capabilities, often constrains their \u2026"}, {"title": "An Inference Method for Professional Texts with Computational Expressions under Few-shot Scenarios", "link": "https://splab.sdu.edu.cn/calc_infer.pdf", "details": "L Yang, W Zheng, F Yuan, Y Sun", "abstract": "We propose an inference method for complex professional texts with computational expressions. We use the expert rules to locate and rewrite the expressions. We adopt the pre-trained language model as the initial model and select the high quality \u2026"}, {"title": "Evaluation of Large Language Model Performance on the Biomedical Language Understanding and Reasoning Benchmark: Comparative Study", "link": "https://www.medrxiv.org/content/10.1101/2024.05.17.24307411.pdf", "details": "H Feng, F Ronzano, J LaFleur, M Garber, R de Oliveira\u2026", "abstract": "Background: The availability of increasingly powerful large language models (LLMs) has attracted substantial interest in their potential for interpreting and generating human-like text for biomedical and clinical applications. However, there are often \u2026"}, {"title": "POSIX: A Prompt Sensitivity Index For Large Language Models", "link": "https://arxiv.org/pdf/2410.02185", "details": "A Chatterjee, HK Renduchintala, S Bhatia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are found to be surprisingly sensitive to minor variations in prompts, often generating significantly divergent outputs in response to minor variations in the prompts, such as spelling \u2026"}, {"title": "Zero-Shot Multi-Hop Question Answering via Monte-Carlo Tree Search with Large Language Models", "link": "https://arxiv.org/pdf/2409.19382", "details": "S Lee, J Shin, Y Ahn, S Seo, O Kwon, KE Kim - arXiv preprint arXiv:2409.19382, 2024", "abstract": "Recent advances in large language models (LLMs) have significantly impacted the domain of multi-hop question answering (MHQA), where systems are required to aggregate information and infer answers from disparate pieces of text. However, the \u2026"}, {"title": "CITI: Enhancing Tool Utilizing Ability in Large Language Models without Sacrificing General Performance", "link": "https://arxiv.org/pdf/2409.13202", "details": "Y Hao, P Cao, Z Jin, H Liao, K Liu, J Zhao - arXiv preprint arXiv:2409.13202, 2024", "abstract": "Tool learning enables the Large Language Models (LLMs) to interact with the external environment by invoking tools, enriching the accuracy and capability scope of LLMs. However, previous works predominantly focus on improving model's tool \u2026"}, {"title": "Investigating Layer Importance in Large Language Models", "link": "https://arxiv.org/pdf/2409.14381", "details": "Y Zhang, Y Dong, K Kawaguchi - arXiv preprint arXiv:2409.14381, 2024", "abstract": "Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in \u2026"}, {"title": "Self-Evolutionary Large Language Models through Uncertainty-Enhanced Preference Optimization", "link": "https://arxiv.org/pdf/2409.11212", "details": "J Wang, Y Zhou, X Zhang, M Bao, P Yan - arXiv preprint arXiv:2409.11212, 2024", "abstract": "Iterative preference optimization has recently become one of the de-facto training paradigms for large language models (LLMs), but the performance is still underwhelming due to too much noisy preference data yielded in the loop. To \u2026"}]
