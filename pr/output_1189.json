'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Text summarization with ChatGPT for drug labeling doc'
[{"title": "GREEN: Generative Radiology Report Evaluation and Error Notation", "link": "https://arxiv.org/pdf/2405.03595", "details": "S Ostmeier, J Xu, Z Chen, M Varma, L Blankemeier\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Evaluating radiology reports is a challenging problem as factual correctness is extremely important due to the need for accurate medical communication about medical images. Existing automatic evaluation metrics either suffer from failing to \u2026"}, {"title": "Advancing Multimodal Medical Capabilities of Gemini", "link": "https://arxiv.org/pdf/2405.03162", "details": "L Yang, S Xu, A Sellergren, T Kohlberger, Y Zhou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Many clinical tasks require an understanding of specialized data, such as medical images and genomics, which is not typically found in general-purpose large multimodal models. Building upon Gemini's multimodal models, we develop several \u2026"}, {"title": "Adapting Dual-encoder Vision-language Models for Paraphrased Retrieval", "link": "https://arxiv.org/pdf/2405.03190", "details": "J Cheng, HV Shin, N Vasconcelos, B Russell\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the recent years, the dual-encoder vision-language models (\\eg CLIP) have achieved remarkable text-to-image retrieval performance. However, we discover that these models usually results in very different retrievals for a pair of paraphrased \u2026"}, {"title": "Pose Priors from Language Models", "link": "https://arxiv.org/pdf/2405.03689", "details": "S Subramanian, E Ng, L M\u00fcller, D Klein, S Ginosar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present a zero-shot pose optimization method that enforces accurate physical contact constraints when estimating the 3D pose of humans. Our central insight is that since language is often used to describe physical interaction, large pretrained \u2026"}, {"title": "MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning", "link": "https://arxiv.org/pdf/2405.03000", "details": "W Shi, R Xu, Y Zhuang, Y Yu, H Wu, C Yang, MD Wang - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their improved capabilities in generation and reasoning, adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and corporate privacy. In this work, we propose MedAdapter, a unified \u2026"}, {"title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training", "link": "https://arxiv.org/pdf/2405.03133", "details": "Z Zhong, M Xia, D Chen, M Lewis - arXiv preprint arXiv:2405.03133, 2024", "abstract": "Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture, SMEAR, was proposed \u2026"}, {"title": "NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli", "link": "https://arxiv.org/pdf/2405.02814", "details": "X Wang, C Li, Y Chang, J Wang, Y Wu - arXiv preprint arXiv:2405.02814, 2024", "abstract": "Large Language Models (LLMs) have become integral to a wide spectrum of applications, ranging from traditional computing tasks to advanced artificial intelligence (AI) applications. This widespread adoption has spurred extensive \u2026"}]
