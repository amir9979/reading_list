[{"title": "FedHPL: Efficient Heterogeneous Federated Learning with Prompt Tuning and Logit Distillation", "link": "https://arxiv.org/pdf/2405.17267", "details": "Y Ma, L Cheng, Y Wang, Z Zhong, X Xu, M Wang - arXiv preprint arXiv:2405.17267, 2024", "abstract": "Federated learning (FL) is a popular privacy-preserving paradigm that enables distributed clients to collaboratively train models with a central server while keeping raw data locally. In practice, distinct model architectures, varying data distributions \u2026"}, {"title": "Increasing the LLM Accuracy for Question Answering: Ontologies to the Rescue!", "link": "https://arxiv.org/pdf/2405.11706", "details": "D Allemang, J Sequeda - arXiv preprint arXiv:2405.11706, 2024", "abstract": "There is increasing evidence that question-answering (QA) systems with Large Language Models (LLMs), which employ a knowledge graph/semantic representation of an enterprise SQL database (ie Text-to-SPARQL), achieve higher \u2026"}, {"title": "Question Answering", "link": "https://link.springer.com/chapter/10.1007/978-3-031-55865-8_9", "details": "D Demner Fushman - Natural Language Processing in Biomedicine: A \u2026, 2024", "abstract": "Question answering refers to the process of providing direct and precise answers to natural language questions. Biomedical question answering is a task directed towards aiding researchers, healthcare professionals and the public in managing the \u2026"}, {"title": "TAeKD: Teacher Assistant Enhanced Knowledge Distillation for Closed-Source Multilingual Neural Machine Translation", "link": "https://aclanthology.org/2024.lrec-main.1350.pdf", "details": "B Lv, X Liu, K Wei, P Luo, Y Yu - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Knowledge Distillation (KD) serves as an efficient method for transferring language knowledge from open-source large language models (LLMs) to more computationally efficient models. However, challenges arise when attempting to \u2026"}, {"title": "Exploring and Mitigating Shortcut Learning for Generative Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.602.pdf", "details": "Z Sun, Y Xiao, J Li, Y Ji, W Chen, M Zhang - Proceedings of the 2024 Joint \u2026, 2024", "abstract": "Recent generative large language models (LLMs) have exhibited incredible instruction-following capabilities while keeping strong task completion ability, even without task-specific fine-tuning. Some works attribute this to the bonus of the new \u2026"}, {"title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective", "link": "https://arxiv.org/pdf/2405.16747", "details": "A Tomihari, I Sato - arXiv preprint arXiv:2405.16747, 2024", "abstract": "The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data. This success is largely \u2026"}, {"title": "TIQ: A Benchmark for Temporal Question Answering with Implicit Time Constraints", "link": "https://dl.acm.org/doi/pdf/10.1145/3589335.3651895", "details": "Z Jia, P Christmann, G Weikum - Companion Proceedings of the ACM on Web \u2026, 2024", "abstract": "Temporal question answering (QA) involves explicit (eg,\"... before 2024\") or implicit (eg,\"... during the Cold War period\") time constraints. Implicit constraints are more challenging; yet benchmarks for temporal QA largely disregard such questions. This \u2026"}, {"title": "Probe Then Retrieve and Reason: Distilling Probing and Reasoning Capabilities into Smaller Language Models", "link": "https://aclanthology.org/2024.lrec-main.1140.pdf", "details": "Y Zhao, S Zhou, H Zhu - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Step-by-step reasoning methods, such as the Chain-of-Thought (CoT), have been demonstrated to be highly effective in harnessing the reasoning capabilities of Large Language Models (LLMs). Recent research efforts have sought to distill LLMs into \u2026"}, {"title": "What Variables Affect Out-Of-Distribution Generalization in Pretrained Models?", "link": "https://arxiv.org/pdf/2405.15018", "details": "MY Harun, K Lee, J Gallardo, G Krishnan, C Kanan - arXiv preprint arXiv:2405.15018, 2024", "abstract": "Embeddings produced by pre-trained deep neural networks (DNNs) are widely used; however, their efficacy for downstream tasks can vary widely. We study the factors influencing out-of-distribution (OOD) generalization of pre-trained DNN embeddings \u2026"}]
