'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Envisioning MedCLIP: A Deep Dive into Explainability f'
[{"title": "Adaptive Prompt Routing for Arbitrary Text Style Transfer with Pre-trained Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29832/31446", "details": "Q Liu, J Qin, W Ye, H Mou, Y He, K Wang - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Recently, arbitrary text style transfer (TST) has made significant progress with the paradigm of prompt learning. In this paradigm, researchers often design or search for a fixed prompt for any input. However, existing evidence shows that large language \u2026"}, {"title": "Fine-Tuning Language Models with Reward Learning on Policy", "link": "https://arxiv.org/pdf/2403.19279", "details": "H Lang, F Huang, Y Li - arXiv preprint arXiv:2403.19279, 2024", "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences. RLHF contains three steps, ie, human preference collecting, reward learning, and policy \u2026"}, {"title": "ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2403.20194", "details": "S Liu, K Ying, H Zhang, Y Yang, Y Lin, T Zhang, C Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper presents ConvBench, a novel multi-turn conversation evaluation benchmark tailored for Large Vision-Language Models (LVLMs). Unlike existing benchmarks that assess individual capabilities in single-turn dialogues, ConvBench \u2026"}, {"title": "Multi-Level Explanations for Generative Language Models", "link": "https://arxiv.org/pdf/2403.14459", "details": "LM Paes, D Wei, HJ Do, H Strobelt, R Luss\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Perturbation-based explanation methods such as LIME and SHAP are commonly applied to text classification. This work focuses on their extension to generative language models. To address the challenges of text as output and long text inputs \u2026"}, {"title": "LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation", "link": "https://arxiv.org/pdf/2403.12019", "details": "Y Lan, F Hong, S Yang, S Zhou, X Meng, B Dai, X Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The field of neural rendering has witnessed significant progress with advancements in generative models and differentiable rendering techniques. Though 2D diffusion has achieved success, a unified 3D diffusion pipeline remains unsettled. This paper \u2026"}, {"title": "CRS-Diff: Controllable Generative Remote Sensing Foundation Model", "link": "https://arxiv.org/pdf/2403.11614", "details": "D Tang, X Cao, X Hou, Z Jiang, D Meng - arXiv preprint arXiv:2403.11614, 2024", "abstract": "The emergence of diffusion models has revolutionized the field of image generation, providing new methods for creating high-quality, high-resolution images across various applications. However, the potential of these models for generating domain \u2026"}, {"title": "LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2404.03118", "details": "GBM Stan, RY Rohekar, Y Gurwicz, ML Olson\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the rapidly evolving landscape of artificial intelligence, multi-modal large language models are emerging as a significant area of interest. These models, which combine various forms of data input, are becoming increasingly popular. However \u2026"}, {"title": "Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding", "link": "https://arxiv.org/pdf/2403.18715", "details": "X Wang, J Pan, L Ding, C Biemann - arXiv preprint arXiv:2403.18715, 2024", "abstract": "Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in multimodal decision-making and open-ended generation is hindered \u2026"}, {"title": "LLM meets Vision-Language Models for Zero-Shot One-Class Classification", "link": "https://arxiv.org/pdf/2404.00675", "details": "Y Bendou, G Lioi, B Pasdeloup, L Mauch, GB Hacene\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We consider the problem of zero-shot one-class visual classification. In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from \u2026"}]
