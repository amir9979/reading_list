[{"title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs", "link": "https://arxiv.org/pdf/2504.07866%3F", "details": "Y Yin, W Huang, K Song, Y Tang, X Wu, W Guo, P Guo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing \u2026"}, {"title": "Semantic role extraction in law texts: a comparative analysis of language models for legal information extraction", "link": "https://link.springer.com/article/10.1007/s10506-025-09437-x", "details": "RM Bakker, AJ Schoevers, RAN van Drie\u2026 - Artificial Intelligence and \u2026, 2025", "abstract": "Norms are essential in our society: they dictate how individuals should behave and interact within a community. They can be written down in laws or other written sources. Interpretations often differ; this is where formalisations offer a solution. They \u2026"}, {"title": "Subkv: Quantizing Long Context KV Cache for Sub\u2010Billion Parameter Language Models on Edge Devices", "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3422", "details": "Z Zeng, T Zhang, Z Lu, W Li, H Zhuang, H Shao\u2026 - Software: Practice and \u2026, 2025", "abstract": "ABSTRACT Background Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their substantial computational and memory requirements present significant challenges for \u2026"}, {"title": "SWI: Speaking with Intent in Large Language Models", "link": "https://arxiv.org/pdf/2503.21544%3F", "details": "Y Yin, EJ Hwang, G Carenini - arXiv preprint arXiv:2503.21544, 2025", "abstract": "Intent, typically clearly formulated and planned, functions as a cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated \u2026"}, {"title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models", "link": "https://arxiv.org/pdf/2503.19540", "details": "D Jung, S Lee, H Moon, C Park, H Lim - arXiv preprint arXiv:2503.19540, 2025", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced interactions between users and models. These advancements concurrently underscore the need for rigorous safety evaluations due to the \u2026"}, {"title": "GPBench: A Comprehensive and Fine-Grained Benchmark for Evaluating Large Language Models as General Practitioners", "link": "https://arxiv.org/pdf/2503.17599", "details": "Z Li, Y Yang, J Lang, W Jiang, Y Zhao, S Li, D Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "General practitioners (GPs) serve as the cornerstone of primary healthcare systems by providing continuous and comprehensive medical services. However, due to community-oriented nature of their practice, uneven training and resource gaps, the \u2026"}, {"title": "A Simple yet Effective Layout Token in Large Language Models for Document Understanding", "link": "https://arxiv.org/pdf/2503.18434%3F", "details": "Z Zhu, C Luo, Z Shao, F Gao, H Xing, Q Zheng, J Zhang - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent methods that integrate spatial layouts with text for document understanding in large language models (LLMs) have shown promising results. A commonly used method is to represent layout information as text tokens and interleave them with text \u2026"}]
