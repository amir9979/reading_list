'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Small Language Models Need Strong Verifiers to Self-Co'
[{"title": "Path-Aware Cross-Attention Network for Question Answering", "link": "https://link.springer.com/chapter/10.1007/978-981-97-2253-2_9", "details": "Z Luo, Y Xiong, B Tang - Pacific-Asia Conference on Knowledge Discovery and \u2026, 2024", "abstract": "Abstract Reasoning is an essential ability in QA systems, and the integration of this ability into QA systems has been the subject of considerable research. A prevalent strategy involves incorporating domain knowledge graphs using Graph Neural \u2026"}, {"title": "Revisiting the Adversarial Robustness of Vision Language Models: a Multimodal Perspective", "link": "https://arxiv.org/pdf/2404.19287", "details": "W Zhou, S Bai, Q Zhao, B Chen - arXiv preprint arXiv:2404.19287, 2024", "abstract": "Pretrained vision-language models (VLMs) like CLIP have shown impressive generalization performance across various downstream tasks, yet they remain vulnerable to adversarial attacks. While prior research has primarily concentrated on \u2026"}, {"title": "Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation", "link": "https://arxiv.org/pdf/2404.17489", "details": "W Cui, R Hosseinzadeh, J Ma, T Wu, Y Sui, K Golestan - arXiv preprint arXiv \u2026, 2024", "abstract": "Contrastive learning is a model pre-training technique by first creating similar views of the original data, and then encouraging the data and its corresponding views to be close in the embedding space. Contrastive learning has witnessed success in image \u2026"}, {"title": "Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering", "link": "https://arxiv.org/pdf/2404.14741", "details": "Y Xu, S He, J Chen, Z Wang, Y Song, H Tong, K Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To address the issue of insufficient knowledge and the tendency to generate hallucination in Large Language Models (LLMs), numerous studies have endeavored to integrate LLMs with Knowledge Graphs (KGs). However, all these \u2026"}, {"title": "Causal Diffusion Autoencoders: Toward Counterfactual Generation via Diffusion Probabilistic Models", "link": "https://arxiv.org/pdf/2404.17735", "details": "A Komanduri, C Zhao, F Chen, X Wu - arXiv preprint arXiv:2404.17735, 2024", "abstract": "Diffusion probabilistic models (DPMs) have become the state-of-the-art in high- quality image generation. However, DPMs have an arbitrary noisy latent space with no interpretable or controllable semantics. Although there has been significant \u2026"}, {"title": "Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class", "link": "https://arxiv.org/pdf/2404.16717", "details": "M Moayeri, M Rabbat, M Ibrahim, D Bouchacourt - arXiv preprint arXiv:2404.16717, 2024", "abstract": "Vision-language models enable open-world classification of objects without the need for any retraining. While this zero-shot paradigm marks a significant advance, even today's best models exhibit skewed performance when objects are dissimilar from \u2026"}, {"title": "RevOnt: Reverse Engineering of Competency Questions from Knowledge Graphs via Language Models", "link": "https://kclpure.kcl.ac.uk/portal/files/257691951/RevOnt_Revisions_Jongmo_3_.pdf", "details": "F Ciroku, J de Berardinis, J Kim, AM Penuela\u2026 - Journal of Web Semantics, 2024", "abstract": "RevOnt: Reverse Engineering of Competency Questions from Knowledge Graphs via Language Models Page 1 King\u2019s Research Portal Link to publication record in King's Research Portal Citation for published version (APA): Ciroku, F., de Berardinis, J., Kim \u2026"}, {"title": "Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in Radiology with General-Domain Large Language Model", "link": "https://arxiv.org/pdf/2405.01591", "details": "S Cho, C Kim, J Lee, C Chilkunda, S Choi, JH Yoon - arXiv preprint arXiv:2405.01591, 2024", "abstract": "Recent advancements in Large Multimodal Models (LMMs) have attracted interest in their generalization capability with only a few samples in the prompt. This progress is particularly relevant to the medical domain, where the quality and sensitivity of data \u2026"}, {"title": "Meta In-Context Learning Makes Large Language Models Better Zero and Few-Shot Relation Extractors", "link": "https://arxiv.org/pdf/2404.17807", "details": "G Li, P Wang, J Liu, Y Guo, K Ji, Z Shang, Z Xu - arXiv preprint arXiv:2404.17807, 2024", "abstract": "Relation extraction (RE) is an important task that aims to identify the relationships between entities in texts. While large language models (LLMs) have revealed remarkable in-context learning (ICL) capability for general zero and few-shot \u2026"}]
