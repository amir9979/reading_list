[{"title": "Looking at Model Debiasing through the Lens of Anomaly Detection", "link": "https://arxiv.org/pdf/2407.17449", "details": "VP Pastore, M Ciranni, D Marinelli, F Odone, V Murino - arXiv preprint arXiv \u2026, 2024", "abstract": "It is widely recognized that deep neural networks are sensitive to bias in the data. This means that during training these models are likely to learn spurious correlations between data and labels, resulting in limited generalization abilities and low \u2026"}, {"title": "Meta-GPS++: Enhancing Graph Meta-Learning with Contrastive Learning and Self-Training", "link": "https://arxiv.org/pdf/2407.14732", "details": "Y Liu, M Li, X Li, L Huang, F Giunchiglia, Y Liang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Node classification is an essential problem in graph learning. However, many models typically obtain unsatisfactory performance when applied to few-shot scenarios. Some studies have attempted to combine meta-learning with graph neural \u2026"}, {"title": "Modelling Patient Longitudinal Data for Clinical Decision Support: A Case Study on Emerging AI Healthcare Technologies", "link": "https://link.springer.com/article/10.1007/s10796-024-10513-x", "details": "S Niu, J Ma, Q Yin, Z Wang, L Bai, X Yang - Information Systems Frontiers, 2024", "abstract": "The COVID-19 pandemic has highlighted the critical need for advanced technology in healthcare. Clinical Decision Support Systems (CDSS) utilizing Artificial Intelligence (AI) have emerged as one of the most promising technologies for \u2026"}, {"title": "Semantic Compositions Enhance Vision-Language Contrastive Learning", "link": "https://arxiv.org/pdf/2407.01408", "details": "M Aladago, L Torresani, S Vosoughi - arXiv preprint arXiv:2407.01408, 2024", "abstract": "In the field of vision-language contrastive learning, models such as CLIP capitalize on matched image-caption pairs as positive examples and leverage within-batch non- matching pairs as negatives. This approach has led to remarkable outcomes in zero \u2026"}, {"title": "Open Challenges on Fairness of Artificial Intelligence in Medical Imaging Applications", "link": "https://arxiv.org/pdf/2407.16953", "details": "E Ferrante, R Echeveste - arXiv preprint arXiv:2407.16953, 2024", "abstract": "Recently, the research community of computerized medical imaging has started to discuss and address potential fairness issues that may emerge when developing and deploying AI systems for medical image analysis. This chapter covers some of \u2026"}, {"title": "Attribute or Abstain: Large Language Models as Long Document Assistants", "link": "https://arxiv.org/pdf/2407.07799", "details": "J Buchmann, X Liu, I Gurevych - arXiv preprint arXiv:2407.07799, 2024", "abstract": "LLMs can help humans working with long documents, but are known to hallucinate. Attribution can increase trust in LLM responses: The LLM provides evidence that supports its response, which enhances verifiability. Existing approaches to attribution \u2026"}, {"title": "CFinBench: A Comprehensive Chinese Financial Benchmark for Large Language Models", "link": "https://arxiv.org/pdf/2407.02301", "details": "Y Nie, B Yan, T Guo, H Liu, H Wang, W He, B Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have achieved remarkable performance on various NLP tasks, yet their potential in more challenging and domain-specific task, such as finance, has not been fully explored. In this paper, we present CFinBench: a \u2026"}, {"title": "Refusing Safe Prompts for Multi-modal Large Language Models", "link": "https://arxiv.org/pdf/2407.09050", "details": "Z Shao, H Liu, Y Hu, NZ Gong - arXiv preprint arXiv:2407.09050, 2024", "abstract": "Multimodal large language models (MLLMs) have become the cornerstone of today's generative AI ecosystem, sparking intense competition among tech giants and startups. In particular, an MLLM generates a text response given a prompt consisting \u2026"}, {"title": "Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning", "link": "https://arxiv.org/pdf/2406.19502", "details": "M Ko, SH Park, J Park, M Seo - arXiv preprint arXiv:2406.19502, 2024", "abstract": "Despite significant advancements, there is a limited understanding of how large language models (LLMs) utilize knowledge for reasoning. To address this, we propose a method that deconstructs complex real-world questions into a graph \u2026"}]
