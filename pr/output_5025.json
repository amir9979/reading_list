[{"title": "Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models", "link": "https://arxiv.org/pdf/2407.20271", "details": "H Tang, Y Liu, X Liu, K Zhang, Y Zhang, Q Liu, E Chen - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in machine learning, especially in Natural Language Processing (NLP), have led to the development of sophisticated models trained on vast datasets, but this progress has raised concerns about potential sensitive \u2026"}, {"title": "A Training Data Recipe to Accelerate A* Search with Language Models", "link": "https://arxiv.org/pdf/2407.09985", "details": "D Gupta, B Li - arXiv preprint arXiv:2407.09985, 2024", "abstract": "Recent works in AI planning have proposed to combine LLMs with iterative tree- search algorithms like A* and MCTS, where LLMs are typically used to calculate the heuristic, guiding the planner towards the goal. However, combining these \u2026"}, {"title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models", "link": "https://arxiv.org/pdf/2408.03907", "details": "SH Kumar, S Sahay, S Mazumder, E Okur\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have excelled at language understanding and generating human-level text. However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users \u2026"}, {"title": "Steering Language Models with Game-Theoretic Solvers", "link": "https://openreview.net/pdf%3Fid%3D5QLtIodDmu", "details": "I Gemp, R Patel, Y Bachrach, M Lanctot, V Dasagi\u2026 - Agentic Markets Workshop at ICML \u2026", "abstract": "Mathematical models of strategic interactions among rational agents have long been studied in game theory. However the interactions studied are often over a small set of discrete actions which is very different from how humans communicate in natural \u2026"}, {"title": "Generative Retrieval with Few-shot Indexing", "link": "https://arxiv.org/pdf/2408.02152", "details": "A Askari, C Meng, M Aliannejadi, Z Ren, E Kanoulas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing generative retrieval (GR) approaches rely on training-based indexing, ie, fine-tuning a model to memorise the associations between a query and the document identifier (docid) of a relevant document. Training-based indexing has \u2026"}, {"title": "Leveraging Variation Theory in Counterfactual Data Augmentation for Optimized Active Learning", "link": "https://arxiv.org/pdf/2408.03819", "details": "SA Gebreegziabher, K Ai, Z Zhang, EL Glassman\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Active Learning (AL) allows models to learn interactively from user feedback. This paper introduces a counterfactual data augmentation approach to AL, particularly addressing the selection of datapoints for user querying, a pivotal concern in \u2026"}, {"title": "GRAD-SUM: Leveraging Gradient Summarization for Optimal Prompt Engineering", "link": "https://arxiv.org/pdf/2407.12865", "details": "D Austin, E Chartock - arXiv preprint arXiv:2407.12865, 2024", "abstract": "Prompt engineering for large language models (LLMs) is often a manual time- intensive process that involves generating, evaluating, and refining prompts iteratively to ensure high-quality outputs. While there has been work on automating \u2026"}, {"title": "LAMPO: Large Language Models as Preference Machines for Few-shot Ordinal Classification", "link": "https://arxiv.org/pdf/2408.03359", "details": "Z Qin, J Wu, J Shen, T Liu, X Wang - arXiv preprint arXiv:2408.03359, 2024", "abstract": "We introduce LAMPO, a novel paradigm that leverages Large Language Models (LLMs) for solving few-shot multi-class ordinal classification tasks. Unlike conventional methods, which concatenate all demonstration examples with the test \u2026"}, {"title": "Cognitive Assessment of Language Models", "link": "https://openreview.net/pdf%3Fid%3DpxRh1meUvN", "details": "D McDuff, D Munday, X Liu, I Galatzer-Levy - ICML 2024 Workshop on LLMs and Cognition", "abstract": "Large language models (LLMs) are a subclass of generative artificial intelligence that can interpret language inputs to generate novel responses. These capabilities are conceptualized as a significant step forward in artificial intelligence because the \u2026"}]
