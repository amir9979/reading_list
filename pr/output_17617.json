[{"title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models", "link": "https://arxiv.org/pdf/2506.07468", "details": "M Liu, L Jiang, Y Liang, SS Du, Y Choi, T Althoff\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch--attackers \u2026", "entry_id": "http://arxiv.org/abs/2506.07468v1", "updated": "2025-06-10 01:17:47", "published": "2025-06-09 06:35:12", "authors": "Mickel Liu;Liwei Jiang;Yancheng Liang;Simon Shaolei Du;Yejin Choi;Tim Althoff;Natasha Jaques", "summary": "Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch -- attackers overfit to obsolete defenses, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction. We cast safety alignment as a two-player zero-sum game, where a single model alternates between attacker and defender roles -- generating adversarial prompts and safeguarding against them -- while a reward LM adjudicates outcomes. This enables dynamic co-adaptation. Grounded in the game-theoretic framework of zero-sum games, we establish a theoretical safety guarantee which motivates the design of our method: if self-play converges to a Nash Equilibrium, the defender will reliably produce safe responses to any adversarial input. Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared to attackers trained against static defenders and achieves higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained against static attackers. We further propose hidden Chain-of-Thought, allowing agents to plan privately, which boosts adversarial diversity and reduces over-refusals. Our results motivate a shift from reactive patching to proactive co-evolution in LM safety training, enabling scalable, autonomous, and robust self-improvement of LMs via multi-agent reinforcement learning (MARL).", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CL;cs.MA", "links": "https://arxiv.org/abs/2506.07468v1;https://arxiv.org/pdf/2506.07468v1", "pdf_url": null}, {"title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models", "link": "https://arxiv.org/pdf/2506.06395", "details": "P Li, M Skripkin, A Zubrey, A Kuznetsov, I Oseledets - arXiv preprint arXiv:2506.06395, 2025", "abstract": "Large language models (LLMs) excel at reasoning, yet post-training remains critical for aligning their behavior with task goals. Existing reinforcement learning (RL) methods often depend on costly human annotations or external reward models. We \u2026", "entry_id": "http://arxiv.org/abs/2506.06395v3", "updated": "2025-06-12 00:27:20", "published": "2025-06-05 19:55:15", "authors": "Pengyi Li;Matvey Skripkin;Alexander Zubrey;Andrey Kuznetsov;Ivan Oseledets", "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains critical for aligning their behavior with task goals. Existing reinforcement learning (RL) methods often depend on costly human annotations or external reward models. We propose Reinforcement Learning via Self-Confidence (RLSC), which uses the model's own confidence as reward signals-eliminating the need for labels, preference models, or reward engineering. Applied to Qwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps, RLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on Minerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a simple, scalable post-training method for inference models, requiring only a small number of samples and unlabelled supervision.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "https://arxiv.org/abs/2506.06395v3;https://arxiv.org/pdf/2506.06395v3", "pdf_url": null}, {"title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "link": "https://arxiv.org/pdf/2506.09965", "details": "J Wu, J Guan, K Feng, Q Liu, S Wu, L Wang, W Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As textual reasoning with large language models (LLMs) has advanced significantly, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs). However, existing methods primarily \u2026", "entry_id": "http://arxiv.org/abs/2506.09965v1", "updated": "2025-06-12 01:02:15", "published": "2025-06-11 17:41:50", "authors": "Junfei Wu;Jian Guan;Kaituo Feng;Qiang Liu;Shu Wu;Liang Wang;Wei Wu;Tieniu Tan", "summary": "As textual reasoning with large language models (LLMs) has advanced significantly, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs). However, existing methods primarily approach multimodal reasoning in a straightforward, text-centric manner, where both reasoning and answer derivation are conducted purely through text, with the only difference being the presence of multimodal input. As a result, these methods often encounter fundamental limitations in spatial reasoning tasks that demand precise geometric understanding and continuous spatial tracking-capabilities that humans achieve through mental visualization and manipulation. To address the limitations, we propose drawing to reason in space, a novel paradigm that enables LVLMs to reason through elementary drawing operations in the visual space. By equipping models with basic drawing operations, including annotating bounding boxes and drawing auxiliary lines, we empower them to express and analyze spatial relationships through direct visual manipulation, meanwhile avoiding the performance ceiling imposed by specialized perception tools in previous tool-integrated reasoning approaches. To cultivate this capability, we develop a three-stage training framework: cold-start training with synthetic data to establish basic drawing abilities, reflective rejection sampling to enhance self-reflection behaviors, and reinforcement learning to directly optimize for target rewards. Extensive experiments demonstrate that our model, named VILASR, consistently outperforms existing methods across diverse spatial reasoning benchmarks, involving maze navigation, static spatial reasoning, video-based reasoning, and multi-view-based reasoning tasks, with an average improvement of 18.4%.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "https://arxiv.org/abs/2506.09965v1;https://arxiv.org/pdf/2506.09965v1", "pdf_url": null}, {"title": "Revisiting Backdoor Attacks against Large Vision-Language Models from Domain Shift", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liang_Revisiting_Backdoor_Attacks_against_Large_Vision-Language_Models_from_Domain_Shift_CVPR_2025_paper.pdf", "details": "S Liang, J Liang, T Pang, C Du, A Liu, M Zhu, X Cao\u2026 - Proceedings of the \u2026, 2025", "abstract": "Instruction tuning enhances large vision-language models (LVLMs) but increases their vulnerability to backdoor attacks due to their open design. Unlike prior studies in static settings, this paper explores backdoor attacks in LVLM instruction tuning \u2026"}, {"title": "SLADE: Shielding against Dual Exploits in Large Vision-Language Models", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Hossain_SLADE_Shielding_against_Dual_Exploits_in_Large_Vision-Language_Models_CVPR_2025_paper.pdf", "details": "MZ Hossain, A Imteaj - Proceedings of the Computer Vision and Pattern \u2026, 2025", "abstract": "Abstract Large Vision-Language Models (LVLMs) have emerged as transformative tools in multimodal tasks, seamlessly integrating pretrained vision encoders to align visual and textual modalities. Prior works have highlighted the susceptibility of \u2026"}, {"title": "RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics", "link": "https://arxiv.org/pdf/2506.04308", "details": "E Zhou, J An, C Chi, Y Han, S Rong, C Zhang, P Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the \u2026", "entry_id": "http://arxiv.org/abs/2506.04308v1", "updated": "2025-06-06 00:02:00", "published": "2025-06-04 17:59:27", "authors": "Enshen Zhou;Jingkun An;Cheng Chi;Yi Han;Shanyu Rong;Chi Zhang;Pengwei Wang;Zhongyuan Wang;Tiejun Huang;Lu Sheng;Shanghang Zhang", "summary": "Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the complex 3D scenes and dynamically reason about the instruction-indicated locations for interaction. To this end, we propose RoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding by integrating a disentangled but dedicated depth encoder via supervised fine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial reasoning via reinforcement fine-tuning (RFT), with metric-sensitive process reward functions tailored for spatial referring tasks. To support SFT and RFT training, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x prior), covering 31 spatial relations (vs. 15 prior) and supporting complex reasoning processes (up to 5 steps). In addition, we introduce RefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial referring with multi-step reasoning. Experiments show that SFT-trained RoboRefer achieves state-of-the-art spatial understanding, with an average success rate of 89.6%. RFT-trained RoboRefer further outperforms all other baselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average accuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (e,g., UR5, G1 humanoid) in cluttered real-world scenes.", "comment": "Project page: https://zhoues.github.io/RoboRefer/", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.AI;cs.CV", "links": "https://arxiv.org/abs/2506.04308v1;https://arxiv.org/pdf/2506.04308v1", "pdf_url": null}, {"title": "Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models", "link": "https://arxiv.org/pdf/2506.02557", "details": "S Gong, Y Jiang, Q Dou, F Farnia - arXiv preprint arXiv:2506.02557, 2025", "abstract": "Vision-language models, such as CLIP, have achieved significant success in aligning visual and textual representations, becoming essential components of many multi-modal large language models (MLLMs) like LLaVA and OpenFlamingo \u2026", "entry_id": "http://arxiv.org/abs/2506.02557v1", "updated": "2025-06-04 00:37:44", "published": "2025-06-03 07:44:43", "authors": "Shizhan Gong;Yankai Jiang;Qi Dou;Farzan Farnia", "summary": "Vision-language models, such as CLIP, have achieved significant success in aligning visual and textual representations, becoming essential components of many multi-modal large language models (MLLMs) like LLaVA and OpenFlamingo. However, numerous studies have identified CLIP's limited fine-grained perception as a critical drawback, leading to substantial failures in downstream MLLMs. In contrast, vision-centric foundation models like DINOv2 demonstrate remarkable capabilities in capturing fine details from images. In this work, we propose a novel kernel-based method to align CLIP's visual representation with that of DINOv2, ensuring that the resulting embeddings maintain compatibility with text embeddings while enhancing perceptual capabilities. Our alignment objective is designed for efficient stochastic optimization. Following this image-only alignment fine-tuning, the visual encoder retains compatibility with the frozen text encoder and exhibits significant improvements in zero-shot object recognition, fine-grained spatial reasoning, and localization. By integrating the aligned visual encoder, downstream MLLMs also demonstrate enhanced performance.", "comment": "ICML 2025", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "https://arxiv.org/abs/2506.02557v1;https://arxiv.org/pdf/2506.02557v1", "pdf_url": null}, {"title": "PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier", "link": "https://arxiv.org/pdf/2506.10406", "details": "Y Jiang, Y Xiong, Y Yuan, C Xin, W Xu, Y Yue, Q Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks, yet they still struggle to reliably verify the correctness of their own outputs. Existing solutions to this verification challenge often depend on \u2026", "entry_id": "http://arxiv.org/abs/2506.10406v1", "updated": "2025-06-13 00:26:23", "published": "2025-06-12 06:59:35", "authors": "Yuhua Jiang;Yuwen Xiong;Yufeng Yuan;Chao Xin;Wenyuan Xu;Yu Yue;Qianchuan Zhao;Lin Yan", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks, yet they still struggle to reliably verify the correctness of their own outputs. Existing solutions to this verification challenge often depend on separate verifier models or require multi-stage self-correction training pipelines, which limit scalability. In this paper, we propose Policy as Generative Verifier (PAG), a simple and effective framework that empowers LLMs to self-correct by alternating between policy and verifier roles within a unified multi-turn reinforcement learning (RL) paradigm. Distinct from prior approaches that always generate a second attempt regardless of model confidence, PAG introduces a selective revision mechanism: the model revises its answer only when its own generative verification step detects an error. This verify-then-revise workflow not only alleviates model collapse but also jointly enhances both reasoning and verification abilities. Extensive experiments across diverse reasoning benchmarks highlight PAG's dual advancements: as a policy, it enhances direct generation and self-correction accuracy; as a verifier, its self-verification outperforms self-consistency.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "https://arxiv.org/abs/2506.10406v1;https://arxiv.org/pdf/2506.10406v1", "pdf_url": null}, {"title": "Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward", "link": "https://arxiv.org/pdf/2506.07218", "details": "T Xiao, X Xu, Z Huang, H Gao, Q Liu, Q Liu, E Chen - arXiv preprint arXiv:2506.07218, 2025", "abstract": "Enhancing the multimodal reasoning capabilities of Multimodal Large Language Models (MLLMs) is a challenging task that has attracted increasing attention in the community. Recently, several studies have applied Reinforcement Learning with \u2026", "entry_id": "http://arxiv.org/abs/2506.07218v1", "updated": "2025-06-10 01:01:39", "published": "2025-06-08 16:48:42", "authors": "Tong Xiao;Xin Xu;Zhenya Huang;Hongyu Gao;Quan Liu;Qi Liu;Enhong Chen", "summary": "Enhancing the multimodal reasoning capabilities of Multimodal Large Language Models (MLLMs) is a challenging task that has attracted increasing attention in the community. Recently, several studies have applied Reinforcement Learning with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the reasoning abilities of MLLMs. However, these works largely overlook the enhancement of multimodal perception capabilities in MLLMs, which serve as a core prerequisite and foundational component of complex multimodal reasoning. Through McNemar's test, we find that existing RLVR method fails to effectively enhance the multimodal perception capabilities of MLLMs, thereby limiting their further improvement in multimodal reasoning. To address this limitation, we propose Perception-R1, which introduces a novel visual perception reward that explicitly encourages MLLMs to perceive the visual content accurately, thereby can effectively incentivizing both their multimodal perception and reasoning capabilities. Specifically, we first collect textual visual annotations from the CoT trajectories of multimodal problems, which will serve as visual references for reward assignment. During RLVR training, we employ a judging LLM to assess the consistency between the visual annotations and the responses generated by MLLM, and assign the visual perception reward based on these consistency judgments. Extensive experiments on several multimodal reasoning benchmarks demonstrate the effectiveness of our Perception-R1, which achieves state-of-the-art performance on most benchmarks using only 1,442 training data.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CV", "links": "https://arxiv.org/abs/2506.07218v1;https://arxiv.org/pdf/2506.07218v1", "pdf_url": null}]
