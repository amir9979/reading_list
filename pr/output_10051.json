[{"title": "Are Clinical T5 Models Better for Clinical Text?", "link": "https://arxiv.org/pdf/2412.05845", "details": "Y Li, K Harrigian, A Zirikly, M Dredze - arXiv preprint arXiv:2412.05845, 2024", "abstract": "Large language models with a transformer-based encoder/decoder architecture, such as T5, have become standard platforms for supervised tasks. To bring these technologies to the clinical domain, recent work has trained new or adapted existing \u2026"}, {"title": "The Vulnerability of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?", "link": "https://arxiv.org/pdf/2412.03597", "details": "S Banerjee, A Agarwal, E Singh - arXiv preprint arXiv:2412.03597, 2024", "abstract": "The pursuit of leaderboard rankings in Large Language Models (LLMs) has created a fundamental paradox: models excel at standardized tests while failing to demonstrate genuine language understanding and adaptability. Our systematic \u2026"}, {"title": "One-Shot Classification Is Enough for Automatic Label Mapping", "link": "https://link.springer.com/chapter/10.1007/978-3-031-78169-8_25", "details": "X Lin, A Aysa, K Ubul - International Conference on Pattern Recognition, 2024", "abstract": "In recent years, prompt-based learning has achieved some success in various natural language tasks. In text classification tasks, the construction of prompt templates and label mapping have a significant impact on the results. Therefore \u2026"}]
