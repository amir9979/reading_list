[{"title": "A Critical Look At Tokenwise Reward-Guided Text Generation", "link": "https://arxiv.org/pdf/2406.07780", "details": "A Rashid, R Wu, J Grosse, A Kristiadi, P Poupart - arXiv preprint arXiv:2406.07780, 2024", "abstract": "Large language models (LLMs) can significantly be improved by aligning to human preferences--the so-called reinforcement learning from human feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive for many users. Due to their \u2026"}, {"title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs", "link": "https://arxiv.org/pdf/2407.00653", "details": "Y Zhang, X Wang, J Liang, S Xia, L Chen, Y Xiao - arXiv preprint arXiv:2407.00653, 2024", "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new \u2026"}, {"title": "Training Compute-Optimal Protein Language Models", "link": "https://www.biorxiv.org/content/10.1101/2024.06.06.597716.full.pdf", "details": "X Cheng, B Chen, P Li, J Gong, J Tang, L Song - bioRxiv, 2024", "abstract": "We explore optimally training protein language models, an area of significant interest in biological research where guidance on best practices is limited. Most models are trained with extensive compute resources until performance gains plateau, focusing \u2026"}, {"title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback", "link": "https://openreview.net/pdf%3Fid%3DBOorDpKHiJ", "details": "G Cui, L Yuan, N Ding, G Yao, B He, W Zhu, Y Ni, G Xie\u2026 - Forty-first International \u2026, 2024", "abstract": "Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability \u2026"}, {"title": "Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks", "link": "https://arxiv.org/pdf/2406.12066", "details": "J Gallifant, S Chen, P Moreira, N Munch, M Gao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical knowledge is context-dependent and requires consistent reasoning across various natural language expressions of semantically equivalent phrases. This is particularly crucial for drug names, where patients often use brand names like Advil \u2026"}, {"title": "Symmetric Dot-Product Attention for Efficient Training of BERT Language Models", "link": "https://arxiv.org/pdf/2406.06366", "details": "M Courtois, M Ostendorff, L Hennig, G Rehm - arXiv preprint arXiv:2406.06366, 2024", "abstract": "Initially introduced as a machine translation model, the Transformer architecture has now become the foundation for modern deep learning architecture, with applications in a wide range of fields, from computer vision to natural language processing \u2026"}, {"title": "Steering Without Side Effects: Improving Post-Deployment Control of Language Models", "link": "https://arxiv.org/pdf/2406.15518", "details": "AC Stickland, A Lyzhov, J Pfau, S Mahdi, SR Bowman - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) have been shown to behave unexpectedly post- deployment. For example, new jailbreaks continually arise, allowing model misuse, despite extensive red-teaming and adversarial training from developers. Given most \u2026"}, {"title": "Is On-Device AI Broken and Exploitable? Assessing the Trust and Ethics in Small Language Models", "link": "https://arxiv.org/pdf/2406.05364", "details": "K Nakka, J Dani, N Saxena - arXiv preprint arXiv:2406.05364, 2024", "abstract": "In this paper, we present a very first study to investigate trust and ethical implications of on-device artificial intelligence (AI), focusing on''small''language models (SLMs) amenable for personal devices like smartphones. While on-device SLMs promise \u2026"}, {"title": "On Trojans in Refined Language Models", "link": "https://arxiv.org/pdf/2406.07778", "details": "J Raghuram, G Kesidis, DJ Miller - arXiv preprint arXiv:2406.07778, 2024", "abstract": "A Trojan in a language model can be inserted when the model is refined for a particular application such as determining the sentiment of product reviews. In this paper, we clarify and empirically explore variations of the data-poisoning threat \u2026"}]
