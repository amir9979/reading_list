[{"title": "Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits on Large Audio Language Models in Jailbreak", "link": "https://arxiv.org/pdf/2501.13772", "details": "E Xiao, H Cheng, J Shao, J Duan, K Xu, L Yang, J Gu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) demonstrate remarkable zero-shot performance across various natural language processing tasks. The integration of multimodal encoders extends their capabilities, enabling the development of Multimodal Large \u2026"}, {"title": "Spurious Forgetting in Continual Learning of Language Models", "link": "https://arxiv.org/pdf/2501.13453", "details": "J Zheng, X Cai, S Qiu, Q Ma - arXiv preprint arXiv:2501.13453, 2025", "abstract": "Recent advancements in large language models (LLMs) reveal a perplexing phenomenon in continual learning: despite extensive training, models experience significant performance declines, raising questions about task alignment and \u2026"}, {"title": "Foundations of Large Language Models", "link": "https://arxiv.org/pdf/2501.09223", "details": "T Xiao, J Zhu - arXiv preprint arXiv:2501.09223, 2025", "abstract": "This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting- edge technologies. The book is structured into four main chapters, each exploring a \u2026"}, {"title": "FinMoE: A MoE-based Large Chinese Financial Language Model", "link": "https://aclanthology.org/2025.finnlp-1.4.pdf", "details": "X Zhang, Q Yang - Proceedings of the Joint Workshop of the 9th Financial \u2026, 2025", "abstract": "Large-scale language models have demonstrated remarkable success, achieving strong performance across a variety of general tasks. However, when applied to domain-specific fields, such as finance, these models face challenges due to the \u2026"}, {"title": "LLM-BS: Enhancing Large Language Models for Recommendation through Exogenous Behavior-Semantics Integration", "link": "https://openreview.net/pdf%3Fid%3Drm07DoACiF", "details": "M Hong, Y Xia, Z Wang, J Zhu, Y Wang, S Cai, X Yang\u2026 - THE WEB CONFERENCE 2025", "abstract": "Large language models (LLMs) are increasingly leveraged as foundational backbones in the development of advanced recommender systems, offering enhanced capabilities through their extensive knowledge and reasoning. Existing \u2026"}, {"title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models", "link": "https://arxiv.org/pdf/2501.17905", "details": "M Feng, J Wu, S Zhang, P Shao, R Jin, Z Wen, J Tao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have achieved significant progress across various domains, but their increasing scale results in high computational and memory costs. Recent studies have revealed that LLMs exhibit sparsity, providing the potential to \u2026"}, {"title": "Benchmarking Large Language Models via Random Variables", "link": "https://arxiv.org/pdf/2501.11790", "details": "Z Hong, H Wu, S Dong, J Dong, Y Xiao, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the continuous advancement of large language models (LLMs) in mathematical reasoning, evaluating their performance in this domain has become a prominent research focus. Recent studies have raised concerns about the reliability of current \u2026"}, {"title": "Explainable and Efficient Editing for Large Language Models", "link": "https://openreview.net/pdf%3Fid%3DiAn7rlIfgc", "details": "T Zhang, J Fang, H Jiang, B Bi, X Wang, X He - THE WEB CONFERENCE 2025", "abstract": "Large Language Models (LLMs) possess remarkable capabilities in storing and retrieving vast factual knowledge but often retain outdated or incorrect information from web corpora. While full retraining is costly, locate-and-edit model editing \u2026"}, {"title": "LLMCDSR: Enhancing Cross-Domain Sequential Recommendation with Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3715099", "details": "H Xin, Y Sun, C Wang, H Xiong - ACM Transactions on Information Systems, 2025", "abstract": "Cross-Domain Sequential Recommendation (CDSR) aims to predict users' preferences based on historical sequential interactions across multiple domains. Existing works focus on the overlapped users who interact in multiple domains to \u2026"}]
