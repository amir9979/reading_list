[{"title": "Enhancing LLM Reasoning Capabilities Through Brokered Multi-Expert Reflection", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10966887.pdf", "details": "T Sheokand, G Jain, A Bahga, VK Madisetti - IEEE Access, 2025", "abstract": "Large Language Models (LLMs) have found increasing application in tasks requiring multi-step reasoning, yet challenges such as hallucinations and inconsistencies in the generated responses persist. This study presents an innovative methodology to \u2026"}, {"title": "Benchmarking Vision Language Models on German Factual Data", "link": "https://arxiv.org/pdf/2504.11108%3F", "details": "R Peinl, V Tischler - arXiv preprint arXiv:2504.11108, 2025", "abstract": "Similar to LLMs, the development of vision language models is mainly driven by English datasets and models trained in English and Chinese language, whereas support for other languages, even those considered high-resource languages such \u2026"}, {"title": "Consistency in Language Models: Current Landscape, Challenges, and Future Directions", "link": "https://arxiv.org/pdf/2505.00268%3F", "details": "J Novikova, C Anderson, B Blili-Hamelin, S Majumdar - arXiv preprint arXiv \u2026, 2025", "abstract": "The hallmark of effective language use lies in consistency--expressing similar meanings in similar contexts and avoiding contradictions. While human communication naturally demonstrates this principle, state-of-the-art language \u2026"}, {"title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models", "link": "https://arxiv.org/pdf/2504.17449", "details": "J Zhang, J Wang, H Li, L Shou, K Chen, G Chen, Q Xie\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The significant computational demands of pretrained language models (PLMs), which often require dedicated hardware, present a substantial challenge in serving them efficiently, especially in multi-tenant environments. To address this, we \u2026"}, {"title": "Multi-Agent Simulator Drives Language Models for Legal Intensive Interaction", "link": "https://aclanthology.org/2025.findings-naacl.365.pdf", "details": "SY ShengbinYue, T Huang, Z Jia, S Wang, S Liu\u2026 - Findings of the Association \u2026, 2025", "abstract": "Abstract Large Language Models (LLMs) have significantly advanced legal intelligence, but the scarcity of scenario data impedes the progress toward interactive legal scenarios. This paper introduces a Multi-agent Legal Simulation Driver \u2026"}, {"title": "Refining Zero-Shot Text-to-SQL Benchmarks via Prompt Strategies with Large Language Models", "link": "https://www.mdpi.com/2076-3417/15/10/5306", "details": "R Zhou, F Zhang - Applied Sciences, 2025", "abstract": "Text-to-SQL leverages large language models (LLMs) for natural language database queries, yet existing benchmarks like BIRD (12,751 question\u2013SQL pairs, 95 databases) suffer from inconsistencies\u2014eg, 30% of queries misalign with SQL \u2026"}, {"title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment", "link": "https://arxiv.org/pdf/2504.12663", "details": "X Zhang, R Chen, Y Feng, Z Liu - arXiv preprint arXiv:2504.12663, 2025", "abstract": "Aligning language models with human preferences presents significant challenges, particularly in achieving personalization without incurring excessive computational costs. Existing methods rely on reward signals and additional annotated data \u2026"}]
