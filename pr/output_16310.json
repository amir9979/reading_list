[{"title": "Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization", "link": "https://arxiv.org/pdf/2504.12083", "details": "P Sarkar, A Etemad - arXiv preprint arXiv:2504.12083, 2025", "abstract": "Despite recent advances in Large Video Language Models (LVLMs), they still struggle with fine-grained temporal understanding, hallucinate, and often make simple mistakes on even simple video question-answering tasks, all of which pose \u2026"}, {"title": "Bigger But Not Better: Small Neural Language Models Outperform LLMs in Detection of Thought Disorder", "link": "https://aclanthology.org/2025.clpsych-1.8.pdf", "details": "C Li, W Xu, S Pakhomov, E Bradley, D Ben-Zeev\u2026 - Proceedings of the 10th \u2026, 2025", "abstract": "Disorganized thinking is a key diagnostic indicator of schizophrenia-spectrum disorders. Recently, clinical estimates of the severity of disorganized thinking have been shown to correlate with measures of how difficult speech transcripts would be \u2026"}, {"title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models", "link": "https://arxiv.org/pdf/2504.12526%3F", "details": "J Zhang, T Zhu, C Luo, A Anandkumar - arXiv preprint arXiv:2504.12526, 2025", "abstract": "Long-context language models exhibit impressive performance but remain challenging to deploy due to high GPU memory demands during inference. We propose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that \u2026"}, {"title": "NNTile: a machine learning framework capable of training extremely large GPT language models on a single node", "link": "https://arxiv.org/pdf/2504.13236", "details": "A Mikhalev, A Katrutsa, K Sozykin, I Oseledets - arXiv preprint arXiv:2504.13236, 2025", "abstract": "This study presents an NNTile framework for training large deep neural networks in heterogeneous clusters. The NNTile is based on a StarPU library, which implements task-based parallelism and schedules all provided tasks onto all available \u2026"}, {"title": "Enhancing Multi-task Learning Capability of Medical Generalist Foundation Model via Image-centric Multi-annotation Data", "link": "https://arxiv.org/pdf/2504.09967", "details": "X Zhu, F Mo, Z Zhang, J Wang, Y Shi, M Wu, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The emergence of medical generalist foundation models has revolutionized conventional task-specific model development paradigms, aiming to better handle multiple tasks through joint training on large-scale medical datasets. However, recent \u2026"}, {"title": "Developing safe and responsible large language model: can we balance bias reduction and language understanding?", "link": "https://link.springer.com/article/10.1007/s10994-025-06767-4", "details": "S Raza, O Bamgbose, S Ghuge, F Tavakoli, DJ Reji\u2026 - Machine Learning, 2025", "abstract": "Abstract Large Language Models (LLMs) have advanced various Natural Language Processing (NLP) tasks, such as text generation and translation, among others. However, these models often generate texts that can perpetuate biases. Existing \u2026"}, {"title": "Overview of the peranssumm 2025 shared task on perspective-aware healthcare answer summarization", "link": "https://aclanthology.org/2025.cl4health-1.41.pdf", "details": "S Agarwal, MS Akhtar, S Yadav - Proceedings of the Second Workshop on Patient \u2026, 2025", "abstract": "This paper presents an overview of the Perspective-aware Answer Summarization (PerAnsSumm) Shared Task on summarizing healthcare answers in Community Question Answering forums hosted at the CL4Health Workshop at NAACL 2025. In \u2026"}, {"title": "Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization", "link": "https://arxiv.org/pdf/2505.05070", "details": "A Abrar, F Tabassum, S Ahmed - arXiv preprint arXiv:2505.05070, 2025", "abstract": "Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language, often contain extraneous details, complicating efficient medical responses. This study investigates the zero-shot performance of nine advanced large language models \u2026"}, {"title": "Development, Evaluation, and Assessment of Large Language Models (DEAL) Checklist: A Technical Report", "link": "https://ai.nejm.org/doi/abs/10.1056/AIp2401106", "details": "S Tripathi, D Alkhulaifat, FX Doo, P Rajpurkar\u2026 - NEJM AI, 2025", "abstract": "Large language models (LLMs) have advanced artificial intelligence research in medicine, especially in natural language processing tasks. However, the nascent evolution of LLM practices presents challenges to the transparency, reproducibility \u2026"}]
