[{"title": "Self-Generated Critiques Boost Reward Modeling for Language Models", "link": "https://arxiv.org/pdf/2411.16646%3F", "details": "Y Yu, Z Chen, A Zhang, L Tan, C Zhu, RY Pang, Y Qian\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to \u2026"}, {"title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent", "link": "https://arxiv.org/pdf/2411.02265%3F", "details": "X Sun, Y Chen, Y Huang, R Xie, J Zhu, K Zhang, S Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we introduce Hunyuan-Large, which is currently the largest open- source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K \u2026"}, {"title": "ModSCAN: Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities", "link": "https://aclanthology.org/2024.emnlp-main.713.pdf", "details": "Y Jiang, Z Li, X Shen, Y Liu, M Backes, Y Zhang - Proceedings of the 2024 \u2026, 2024", "abstract": "Large vision-language models (LVLMs) have been rapidly developed and widely used in various fields, but the (potential) stereotypical bias in the model is largely unexplored. In this study, we present a pioneering measurement framework \u2026"}, {"title": "Sparsing Law: Towards Large Language Models with Greater Activation Sparsity", "link": "https://arxiv.org/pdf/2411.02335%3F", "details": "Y Luo, C Song, X Han, Y Chen, C Xiao, Z Liu, M Sun - arXiv preprint arXiv \u2026, 2024", "abstract": "Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting \u2026"}, {"title": "ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.15268", "details": "J Chen, T Zhang, S Huang, Y Niu, L Zhang, L Wen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the recent breakthroughs achieved by Large Vision Language Models (LVLMs) in understanding and responding to complex visual-textual contexts, their inherent hallucination tendencies limit their practical application in real-world \u2026"}, {"title": "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.110.pdf", "details": "D Hao, Q Wang, L Guo, J Jiang, J Liu - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "While large pre-trained visual-language models have shown promising results on traditional visual question answering benchmarks, it is still challenging for them to answer complex VQA problems which requires diverse world knowledge. Motivated \u2026"}, {"title": "Lifelong Knowledge Editing for Vision Language Models with Low-Rank Mixture-of-Experts", "link": "https://arxiv.org/pdf/2411.15432", "details": "Q Chen, C Wang, D Wang, T Zhang, W Li, X He - arXiv preprint arXiv:2411.15432, 2024", "abstract": "Model editing aims to correct inaccurate knowledge, update outdated information, and incorporate new data into Large Language Models (LLMs) without the need for retraining. This task poses challenges in lifelong scenarios where edits must be \u2026"}, {"title": "Defining and Evaluating Physical Safety for Large Language Models", "link": "https://arxiv.org/pdf/2411.02317%3F", "details": "YC Tang, PY Chen, TY Ho - arXiv preprint arXiv:2411.02317, 2024", "abstract": "Large Language Models (LLMs) are increasingly used to control robotic systems such as drones, but their risks of causing physical threats and harm in real-world applications remain unexplored. Our study addresses the critical gap in evaluating \u2026"}, {"title": "Can Machine Unlearning Reduce Social Bias in Language Models?", "link": "https://aclanthology.org/2024.emnlp-industry.71.pdf", "details": "O Dige, D Arneja, TF Yau, Q Zhang, M Bolandraftar\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Mitigating bias in language models (LMs) has become a critical problem due to the widespread deployment of LMs in the industry and customer-facing applications. Numerous approaches revolve around data pre-processing and subsequent fine \u2026"}]
