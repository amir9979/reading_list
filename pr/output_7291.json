[{"title": "Few-shot Adaptation of Medical Vision-Language Models", "link": "https://arxiv.org/pdf/2409.03868", "details": "F Shakeri, Y Huang, J Silva-Rodr\u00edguez, H Bahig\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Integrating image and text data through multi-modal learning has emerged as a new approach in medical imaging research, following its successful deployment in computer vision. While considerable efforts have been dedicated to establishing \u2026"}, {"title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks", "link": "https://arxiv.org/pdf/2409.07353", "details": "MZ Hossain, A Imteaj - arXiv preprint arXiv:2409.07353, 2024", "abstract": "Large Vision-Language Models (LVLMs), trained on multimodal big datasets, have significantly advanced AI by excelling in vision-language tasks. However, these models remain vulnerable to adversarial attacks, particularly jailbreak attacks, which \u2026"}, {"title": "PIP: Detecting Adversarial Examples in Large Vision-Language Models via Attention Patterns of Irrelevant Probe Questions", "link": "https://arxiv.org/pdf/2409.05076", "details": "Y Zhang, R Xie, J Chen, X Sun, Y Wang - arXiv preprint arXiv:2409.05076, 2024", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated their powerful multimodal capabilities. However, they also face serious safety problems, as adversaries can induce robustness issues in LVLMs through the use of well \u2026"}, {"title": "Top-down Activity Representation Learning for Video Question Answering", "link": "https://arxiv.org/pdf/2409.07748", "details": "Y Wang, S Haruta, D Zeng, J Vizcarra, M Kurokawa - arXiv preprint arXiv:2409.07748, 2024", "abstract": "Capturing complex hierarchical human activities, from atomic actions (eg, picking up one present, moving to the sofa, unwrapping the present) to contextual events (eg, celebrating Christmas) is crucial for achieving high-performance video question \u2026"}, {"title": "Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types", "link": "https://arxiv.org/pdf/2409.09269", "details": "N Sinha, V Jain, A Chadha - arXiv preprint arXiv:2409.09269, 2024", "abstract": "Visual Question-Answering (VQA) has become a key use-case in several applications to aid user experience, particularly after Vision-Language Models (VLMs) achieving good results in zero-shot inference. But evaluating different VLMs \u2026"}, {"title": "Mitigating Hallucination in Visual-Language Models via Re-Balancing Contrastive Decoding", "link": "https://arxiv.org/pdf/2409.06485", "details": "X Liang, J Yu, L Mu, J Zhuang, J Hu, Y Yang, J Ye, L Lu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Although Visual-Language Models (VLMs) have shown impressive capabilities in tasks like visual question answering and image captioning, they still struggle with hallucinations. Analysis of attention distribution in these models shows that VLMs \u2026"}, {"title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks", "link": "https://arxiv.org/pdf/2409.06173", "details": "G Chochlakis, NM Pandiyan, K Lerman, S Narayanan - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the dominant technique for performing natural language tasks, as it does not require updating the model parameters with gradient-based methods. ICL promises to\" \u2026"}, {"title": "Selective Self-Rehearsal: A Fine-Tuning Approach to Improve Generalization in Large Language Models", "link": "https://arxiv.org/pdf/2409.04787", "details": "S Gupta, Y Nandwani, A Yehudai, M Mishra, G Pandey\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task \u2026"}, {"title": "Targeted training for numerical reasoning with large language models", "link": "https://link.springer.com/article/10.1007/s10115-024-02216-1", "details": "X Li, S Liu, Y Zhu, G Cheng - Knowledge and Information Systems, 2024", "abstract": "After recent gains achieved by large language models (LLMs) on numerical reasoning tasks, it has become of interest to have LLMs teach small models to improve on numerical reasoning. Instructing LLMs to generate Chains of Thought to \u2026"}]
