[{"title": "When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?", "link": "https://arxiv.org/pdf/2407.15211", "details": "R Schaeffer, D Valentine, L Bailey, J Chua\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The integration of new modalities into frontier AI systems offers exciting capabilities, but also increases the possibility such systems can be adversarially manipulated in undesirable ways. In this work, we focus on a popular class of vision-language \u2026"}, {"title": "Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation", "link": "https://arxiv.org/pdf/2407.15268", "details": "L Sun, J Zhao, M Han, C Xiong - arXiv preprint arXiv:2407.15268, 2024", "abstract": "Multimodal foundation models hold significant potential for automating radiology report generation, thereby assisting clinicians in diagnosing cardiac diseases. However, generated reports often suffer from serious factual inaccuracy. In this \u2026"}, {"title": "In-Context Learning Improves Compositional Understanding of Vision-Language Models", "link": "https://arxiv.org/pdf/2407.15487", "details": "M Nulli, A Ibrahimi, A Pal, H Lee, I Najdenkoska - arXiv preprint arXiv:2407.15487, 2024", "abstract": "Vision-Language Models (VLMs) have shown remarkable capabilities in a large number of downstream tasks. Nonetheless, compositional image understanding remains a rather difficult task due to the object bias present in training data. In this \u2026"}, {"title": "Imposter. AI: Adversarial Attacks with Hidden Intentions towards Aligned Large Language Models", "link": "https://arxiv.org/pdf/2407.15399", "details": "X Liu, L Li, T Xiang, F Ye, L Wei, W Li, N Garcia - arXiv preprint arXiv:2407.15399, 2024", "abstract": "With the development of large language models (LLMs) like ChatGPT, both their vast applications and potential vulnerabilities have come to the forefront. While developers have integrated multiple safety mechanisms to mitigate their misuse, a \u2026"}, {"title": "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference", "link": "https://arxiv.org/pdf/2406.17626", "details": "E Yu, J Li, M Liao, S Wang, Z Gao, F Mi, L Hong - arXiv preprint arXiv:2406.17626, 2024", "abstract": "As large language models (LLMs) constantly evolve, ensuring their safety remains a critical research problem. Previous red-teaming approaches for LLM safety have primarily focused on single prompt attacks or goal hijacking. To the best of our \u2026"}, {"title": "CogMG: Collaborative Augmentation Between Large Language Model and Knowledge Graph", "link": "https://arxiv.org/pdf/2406.17231", "details": "T Zhou, Y Chen, K Liu, J Zhao - arXiv preprint arXiv:2406.17231, 2024", "abstract": "Large language models have become integral to question-answering applications despite their propensity for generating hallucinations and factually inaccurate content. Querying knowledge graphs to reduce hallucinations in LLM meets the \u2026"}, {"title": "Aligning Model Evaluations with Human Preferences: Mitigating Token Count Bias in Language Model Assessments", "link": "https://arxiv.org/pdf/2407.12847", "details": "R Daynauth, J Mars - arXiv preprint arXiv:2407.12847, 2024", "abstract": "The SLAM paper demonstrated that on-device Small Language Models (SLMs) are a viable and cost-effective alternative to API-based Large Language Models (LLMs), such as OpenAI's GPT-4, offering comparable performance and stability. However \u2026"}]
