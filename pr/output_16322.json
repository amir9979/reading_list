[{"title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "link": "https://arxiv.org/pdf/2504.14194", "details": "X Zhuang, J Peng, R Ma, Y Wang, T Bai, X Wei, J Qiu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural \u2026"}, {"title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "link": "https://arxiv.org/pdf/2504.14366", "details": "P Haller, J Golde, A Akbik - arXiv preprint arXiv:2504.14366, 2025", "abstract": "Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures \u2026"}, {"title": "Platonic Grounding for Efficient Multimodal Language Models", "link": "https://arxiv.org/pdf/2504.19327", "details": "M Choraria, X Wu, A Bhimaraju, N Sekhar, Y Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The hyperscaling of data and parameter count in Transformer-based models is yielding diminishing performance improvement, especially when weighed against training costs. Such plateauing indicates the importance of methods for more efficient \u2026"}, {"title": "Investigating Zero-Shot Diagnostic Pathology in Vision-Language Models with Efficient Prompt Design", "link": "https://arxiv.org/pdf/2505.00134", "details": "V Sharma, A Alagha, A Khellaf, VQH Trinh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) have gained significant attention in computational pathology due to their multimodal learning capabilities that enhance big-data analytics of giga-pixel whole slide image (WSI). However, their sensitivity to large \u2026"}, {"title": "Can Long-Context Language Models Solve Repository-Level Code Generation?", "link": "https://openreview.net/pdf%3Fid%3DpmcWo9DtDw", "details": "Y PENG, ZZ Wang, D Fried - LTI Student Research Symposium 2025", "abstract": "With the advance of real-world tasks that necessitate increasingly long contexts, recent language models (LMs) have begun to support longer context windows. One particularly complex task is repository-level code generation, where retrieval \u2026"}, {"title": "Towards trustworthy and reliable language models", "link": "https://dr.ntu.edu.sg/bitstream/10356/184392/2/Amended%2520Thesis.pdf", "details": "R Zhao - 2025", "abstract": "This thesis addresses the critical challenge of developing trustworthy and reliable Natural Language Processing (NLP) systems, specifically the newly emerged Large Language Models (LLMs). As LLMs become increasingly prevalent in various \u2026"}, {"title": "Creation of a gold standard Dutch corpus of clinical notes for adverse drug event detection: the Dutch ADE corpus", "link": "https://link.springer.com/article/10.1007/s10579-025-09832-5", "details": "RM Murphy, DA Dongelmans, NF de Keizer\u2026 - Language Resources and \u2026, 2025", "abstract": "Our objective was to create a gold standard Dutch language annotated corpus of clinical notes with adverse drug event (ADE) mentions, specifically for Intensive Care patients with drug-related acute kidney injury. We used anonymized clinical notes \u2026"}, {"title": "HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models", "link": "https://arxiv.org/pdf/2504.14594", "details": "F Gao, X Zhao, D Xia, Z Zhou, R Yang, J Lu, H Jiang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Seeking dietary guidance often requires navigating complex professional knowledge while accommodating individual health conditions. Knowledge Graphs (KGs) offer structured and interpretable nutritional information, whereas Large Language \u2026"}, {"title": "Data-efficient LLM Fine-tuning for Code Generation", "link": "https://arxiv.org/pdf/2504.12687%3F", "details": "W Lv, X Xia, SJ Huang - arXiv preprint arXiv:2504.12687, 2025", "abstract": "Large language models (LLMs) have demonstrated significant potential in code generation tasks. However, there remains a performance gap between open-source and closed-source models. To address this gap, existing approaches typically \u2026"}]
