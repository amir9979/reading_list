[{"title": "SimAug: Enhancing Recommendation with Pretrained Language Models for Dense and Balanced Data Augmentation", "link": "https://arxiv.org/pdf/2505.01695", "details": "Y Zhao, X Yang, H Chen, X Fan, Y Wang, Y Cai, T Derr - arXiv preprint arXiv \u2026, 2025", "abstract": "Deep Neural Networks (DNNs) are extensively used in collaborative filtering due to their impressive effectiveness. These systems depend on interaction data to learn user and item embeddings that are crucial for recommendations. However, the data \u2026"}, {"title": "Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models", "link": "https://arxiv.org/pdf/2505.00150", "details": "MH Van, X Wu - arXiv preprint arXiv:2505.00150, 2025", "abstract": "The rapid evolution of social media has provided enhanced communication channels for individuals to create online content, enabling them to express their thoughts and opinions. Multimodal memes, often utilized for playful or humorous \u2026"}, {"title": "RAISE: Reinforenced Adaptive Instruction Selection For Large Language Models", "link": "https://arxiv.org/pdf/2504.07282", "details": "L Qingsong, Y Li, Z Lan, Z Xu, J Tang, Y Li, W Jiang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In the instruction fine-tuning of large language models (LLMs), it has become a consensus that a few high-quality instructions are superior to a large number of low- quality instructions. At present, many instruction selection methods have been \u2026"}, {"title": "When and How to Augment Your Input: Question Routing Helps Balance the Accuracy and Efficiency of Large Language Models", "link": "https://aclanthology.org/2025.findings-naacl.200.pdf", "details": "S Chen, H Zheng, L Cui - Findings of the Association for Computational \u2026, 2025", "abstract": "Although large language models rely on parametric knowledge to achieve exceptional performance across various question-answering tasks, they still face challenges when addressing knowledge-based long-tail questions. Augmented \u2026"}, {"title": "Reasoning Capabilities and Invariability of Large Language Models", "link": "https://arxiv.org/pdf/2505.00776", "details": "A Raganato, R Pe\u00f1aloza, M Viviani, G Pasi - arXiv preprint arXiv:2505.00776, 2025", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in manipulating natural language across multiple applications, but their ability to handle simple reasoning tasks is often questioned. In this work, we aim to provide a \u2026"}]
