[{"title": "Alphaedit: Null-space constrained knowledge editing for language models", "link": "https://arxiv.org/pdf/2410.02355%3F", "details": "J Fang, H Jiang, K Wang, Y Ma, X Wang, X He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) often exhibit hallucinations due to incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating \u2026"}, {"title": "BIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models", "link": "https://arxiv.org/pdf/2410.09079", "details": "A Chang, J Wang, H Liu, P Bhatia, C Xiao, T Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Parameter Efficient Fine-Tuning (PEFT) offers an efficient solution for fine-tuning large pretrained language models for downstream tasks. However, most PEFT strategies are manually designed, often resulting in suboptimal performance. Recent \u2026"}, {"title": "How to Train Long-Context Language Models (Effectively)", "link": "https://arxiv.org/pdf/2410.02660%3F", "details": "T Gao, A Wettig, H Yen, D Chen - arXiv preprint arXiv:2410.02660, 2024", "abstract": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development--Instead of perplexity or simple \u2026"}, {"title": "3D-CT-GPT: Generating 3D Radiology Reports through Integration of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.19330", "details": "H Chen, W Zhao, Y Li, T Zhong, Y Wang, Y Shang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical image analysis is crucial in modern radiological diagnostics, especially given the exponential growth in medical imaging data. The demand for automated report generation systems has become increasingly urgent. While prior research has \u2026"}, {"title": "No Need to Talk: Asynchronous Mixture of Language Models", "link": "https://arxiv.org/pdf/2410.03529%3F", "details": "A Filippova, A Katharopoulos, D Grangier, R Collobert - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce SmallTalk LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each model of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth \u2026"}, {"title": "Evaluation of pretrained language models on music understanding", "link": "https://arxiv.org/pdf/2409.11449", "details": "Y Vasilakis, R Bittner, J Pauwels - arXiv preprint arXiv:2409.11449, 2024", "abstract": "Music-text multimodal systems have enabled new approaches to Music Information Research (MIR) applications such as audio-to-text and text-to-audio retrieval, text- based song generation, and music captioning. Despite the reported success, little \u2026"}, {"title": "On Unsupervised Prompt Learning for Classification with Black-box Language Models", "link": "https://arxiv.org/pdf/2410.03124", "details": "ZY Zhang, J Zhang, H Yao, G Niu, M Sugiyama - arXiv preprint arXiv:2410.03124, 2024", "abstract": "Large language models (LLMs) have achieved impressive success in text-formatted learning problems, and most popular LLMs have been deployed in a black-box fashion. Meanwhile, fine-tuning is usually necessary for a specific downstream task \u2026"}, {"title": "The impact of limited access to digital health records on doctors and their willingness to adopt electronic health record systems", "link": "https://journals.sagepub.com/doi/pdf/10.1177/20552076241281626", "details": "MM Bouh, F Hossain, P Paul, MM Rahman, R Islam\u2026 - Digital Health, 2024", "abstract": "Objective Research over the past decade has extensively covered the benefits of electronic health records in developing countries. Yet, the specific impact of their limited access on doctors' workload and clinical decision-making, particularly in \u2026"}, {"title": "Comparison of deep learning and conventional methods for disease onset prediction", "link": "https://arxiv.org/pdf/2410.10505", "details": "LH John, C Kim, JA Kors, J Chang, H Morgan-Cooper\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Background: Conventional prediction methods such as logistic regression and gradient boosting have been widely utilized for disease onset prediction for their reliability and interpretability. Deep learning methods promise enhanced prediction \u2026"}]
