[{"title": "Scaling Embedding Layers in Language Models", "link": "https://arxiv.org/pdf/2502.01637", "details": "D Yu, E Cohen, B Ghazi, Y Huang, P Kamath, R Kumar\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We propose SCONE ($\\textbf {S} $ calable, $\\textbf {C} $ ontextualized, $\\textbf {O} $ ffloaded, $\\textbf {N} $-gram $\\textbf {E} $ mbedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To \u2026"}, {"title": "Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration", "link": "https://arxiv.org/pdf/2502.01969", "details": "Y Zhu, L Tao, M Dong, C Xu - arXiv preprint arXiv:2502.01969, 2025", "abstract": "Large Vision-Language Models (LVLMs) exhibit impressive multimodal reasoning capabilities but remain highly susceptible to object hallucination, where models generate responses that are not factually aligned with the visual content. Recent \u2026"}, {"title": "Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement", "link": "https://arxiv.org/pdf/2502.02573", "details": "S Abbasloo - arXiv preprint arXiv:2502.02573, 2025", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem- solving, a crucial, ubiquitous, and complex domain. This paper explores the \u2026"}, {"title": "MedAssist: LLM-Empowered Medical Assistant for Assisting the Scrutinization and Comprehension of Electronic Health Records", "link": "https://www.cs.emory.edu/~jyang71/files/medassist.pdf", "details": "R Xu, W Shi, J Wang, J Zhou, C Yang - 2025", "abstract": "Efficiently comprehending diagnosis and treatment plans remains a significant challenge for both medical professionals and patients, particularly when dealing with rare or newly emerging diseases and specific combinations of comorbidities. We \u2026"}, {"title": "Digital Twin: A Promising New Era for Elderly Healthcare", "link": "https://www.igi-global.com/chapter/digital-twin/369239", "details": "M Chugh, S Vyas - Digital Twins for Sustainable Healthcare in the \u2026, 2025", "abstract": "Abstract The Digital Twin is an emerging technology in which digital imitations of people, tools, processes, and systems are created that businesses use. In healthcare systems, digital twins are used for constructing digital illustrations of healthcare data \u2026"}, {"title": "Scalable In-Context Learning on Tabular Data via Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2502.03147", "details": "X Wen, S Zheng, Z Xu, Y Sun, J Bian - arXiv preprint arXiv:2502.03147, 2025", "abstract": "Recent studies have shown that large language models (LLMs), when customized with post-training on tabular data, can acquire general tabular in-context learning (TabICL) capabilities. These models are able to transfer effectively across diverse \u2026"}, {"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models", "link": "https://arxiv.org/pdf/2502.03199", "details": "J Wu, Y Shen, S Liu, Y Tang, S Song, X Wang, L Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the \u2026"}, {"title": "LLM-based Affective Text Generation Quality Based on Different Quantization Values", "link": "https://arxiv.org/pdf/2501.19317%3F", "details": "YM Resendiz, R Klinger - arXiv preprint arXiv:2501.19317, 2025", "abstract": "Large language models exhibit a remarkable capacity in language generation and comprehension. These advances enable AI systems to produce more human-like and emotionally engaging text. However, these models rely on a large number of \u2026"}]
