[{"title": "ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding", "link": "https://arxiv.org/pdf/2506.04353", "details": "A Pal, JO Lee, X Zhang, M Sankarasubbu, S Roh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Technical and **Clinical** Implications. ReXVQA advances our understanding of how multimodal **large** **language** **models** (LLMs) reason about **medical** images by introducing **clinically** aligned evaluation dimensions such as presence detection \u2026", "entry_id": "http://arxiv.org/abs/2506.04353v1", "updated": "2025-06-04 18:11:59", "published": "2025-06-04 18:11:59", "authors": "Ankit Pal;Jung-Oh Lee;Xiaoman Zhang;Malaikannan Sankarasubbu;Seunghyeon Roh;Won Jung Kim;Meesun Lee;Pranav Rajpurkar", "summary": "We present ReXVQA, the largest and most comprehensive benchmark for visual\nquestion answering (VQA) in chest radiology, comprising approximately 696,000\nquestions paired with 160,000 chest X-rays studies across training, validation,\nand test sets. Unlike prior efforts that rely heavily on template based\nqueries, ReXVQA introduces a diverse and clinically authentic task suite\nreflecting five core radiological reasoning skills: presence assessment,\nlocation analysis, negation detection, differential diagnosis, and geometric\nreasoning. We evaluate eight state-of-the-art multimodal large language models,\nincluding MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The\nbest-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge\nthe gap between AI performance and clinical expertise, we conducted a\ncomprehensive human reader study involving 3 radiology residents on 200\nrandomly sampled cases. Our evaluation demonstrates that MedGemma achieved\nsuperior performance (83.84% accuracy) compared to human readers (best\nradiology resident: 77.27%), representing a significant milestone where AI\nperformance exceeds expert human evaluation on chest X-ray interpretation. The\nreader study reveals distinct performance patterns between AI models and human\nexperts, with strong inter-reader agreement among radiologists while showing\nmore variable agreement patterns between human readers and AI models. ReXVQA\nestablishes a new standard for evaluating generalist radiological AI systems,\noffering public leaderboards, fine-grained evaluation splits, structured\nexplanations, and category-level breakdowns. This benchmark lays the foundation\nfor next-generation AI systems capable of mimicking expert-level clinical\nreasoning beyond narrow pathology classification. Our dataset will be\nopen-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI;cs.CE;cs.CL;cs.LG", "links": "http://arxiv.org/abs/2506.04353v1;http://arxiv.org/pdf/2506.04353v1", "pdf_url": "http://arxiv.org/pdf/2506.04353v1"}, {"title": "A review on synergizing knowledge graphs and **large language models**", "link": "https://link.springer.com/article/10.1007/s00607-025-01499-8", "details": "Z Yang, S Yuan, Z Shao, W Li, R Liu - Computing, 2025", "abstract": "\u2026 Models such as GPT-3 [1] and ChatGPT Footnote 1 showcase cross-domain knowledge integration, adaptability, and versatility in applications ranging from text summarization to complex **question** **answering** [2]. However, these models face \u2026"}, {"title": "PathVLM-Eval: Evaluation of open vision language models in histopathology", "link": "https://www.sciencedirect.com/science/article/pii/S2153353925000409", "details": "NU Gilal, R Zegour, K Al-Thelaya, E \u00d6zer, M Agus\u2026 - Journal of Pathology \u2026, 2025", "abstract": "The emerging trend of vision language models (VLMs) has introduced a new paradigm in artificial intelligence (AI). However, their evaluation has predominantly focused on general-purpose datasets, providing a limited understanding of their \u2026"}, {"title": "Instruction tuning and cot prompting for contextual **medical** qa with llms", "link": "https://www.preprints.org/frontend/manuscript/3040bf0fe0df8ad5b9f2af5867cd840d/download_pub", "details": "C Le, Z Gong, C Wang, H Ni, P Li, X Chen - 2025 International Conference on Artificial \u2026, 2025", "abstract": "\u2026 In this study, we explored the impact of prompt design and instruction fine-tuning on the performance of **large** **language** **models** for biomedical **question** **answering**. We find that Chain-ofThought prompting improves zero-shot performance, while \u2026"}, {"title": "MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale", "link": "https://arxiv.org/pdf/2506.04405", "details": "R Xu, Y Zhuang, Y Zhong, Y Yu, X Tang, H Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Recent advances in **large** **language** **models** (LLMs) have significantly enhanced complex reasoning [49, 13], including code generation [7] \u2026 TREQS [79] is a text-to-SQL benchmark tailored specifically to **clinical** **question** **answering** using the MIMIC-III \u2026", "entry_id": "http://arxiv.org/abs/2506.04405v1", "updated": "2025-06-04 19:38:55", "published": "2025-06-04 19:38:55", "authors": "Ran Xu;Yuchen Zhuang;Yishan Zhong;Yue Yu;Xiangru Tang;Hang Wu;May D. Wang;Peifeng Ruan;Donghan Yang;Tao Wang;Guanghua Xiao;Carl Yang;Yang Xie;Wenqi Shi", "summary": "We introduce MedAgentGYM, the first publicly available training environment\ndesigned to enhance coding-based medical reasoning capabilities in large\nlanguage model (LLM) agents. MedAgentGYM comprises 72,413 task instances across\n129 categories derived from authentic real-world biomedical scenarios. Tasks\nare encapsulated within executable coding environments, each featuring detailed\ntask descriptions, interactive feedback mechanisms, verifiable ground-truth\nannotations, and scalable training trajectory generation. Extensive\nbenchmarking of over 30 LLMs reveals a notable performance disparity between\ncommercial API-based models and open-source counterparts. Leveraging\nMedAgentGYM, Med-Copilot-7B achieves substantial performance gains through\nsupervised fine-tuning (+36.44%) and continued reinforcement learning\n(+42.47%), emerging as an affordable and privacy-preserving alternative\ncompetitive with gpt-4o. By offering both a comprehensive benchmark and\naccessible, expandable training resources within unified execution\nenvironments, MedAgentGYM delivers an integrated platform to develop LLM-based\ncoding assistants for advanced biomedical research and practice.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2506.04405v1;http://arxiv.org/pdf/2506.04405v1", "pdf_url": "http://arxiv.org/pdf/2506.04405v1"}]
