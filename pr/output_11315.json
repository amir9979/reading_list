[{"title": "Rethinking IDE Customization for Enhanced HAX: A Hyperdimensional Perspective", "link": "https://arxiv.org/pdf/2501.02491", "details": "R Koohestani, M Izadi - arXiv preprint arXiv:2501.02491, 2025", "abstract": "As Integrated Development Environments (IDEs) increasingly integrate Artificial Intelligence, Software Engineering faces both benefits like productivity gains and challenges like mismatched user preferences. We propose Hyper-Dimensional (HD) \u2026"}, {"title": "Open-source small language models for personal medical assistant chatbots", "link": "https://www.sciencedirect.com/science/article/pii/S2666521224000644", "details": "M Magnini, G Aguzzi, S Montagna - Intelligence-Based Medicine, 2025", "abstract": "Medical chatbots are becoming essential components of telemedicine applications as tools to assist patients in the self-management of their conditions. This trend is particularly driven by advancements in natural language processing techniques with \u2026"}, {"title": "Early detection of heart failure using in-patient longitudinal electronic health records", "link": "https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0314145", "details": "I Drozdov, B Szubert, C Murphy, K Brooksbank\u2026 - PloS one, 2024", "abstract": "Heart Failure (HF) is common, with worldwide prevalence of 1%-3% and a lifetime risk of 20% for individuals 40 years or older. Despite its considerable health economic burden, techniques for early detection of HF in the general population are \u2026"}, {"title": "Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation", "link": "https://arxiv.org/pdf/2501.03225", "details": "Y Zhang, Y Su, Y Liu, X Wang, J Burgess, E Sui\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to \u2026"}, {"title": "Instruction-Following Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2501.02086", "details": "B Hou, Q Chen, J Wang, G Yin, C Wang, N Du, R Pang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from \u2026"}, {"title": "Distilling Large Language Models for Efficient Clinical Information Extraction", "link": "https://arxiv.org/pdf/2501.00031", "details": "KS Vedula, A Gupta, A Swaminathan, I Lopez, S Bedi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) excel at clinical information extraction but their computational demands limit practical deployment. Knowledge distillation--the process of transferring knowledge from larger to smaller models--offers a potential \u2026"}]
