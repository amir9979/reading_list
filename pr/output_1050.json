'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [LM Transparency Tool: Interactive Tool for Analyzing T'
[{"title": "Causal Evaluation of Language Models", "link": "https://arxiv.org/pdf/2405.00622", "details": "S Chen, B Peng, M Chen, R Wang, M Xu, X Zeng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for \u2026"}, {"title": "A Primer on the Inner Workings of Transformer-based Language Models", "link": "https://arxiv.org/pdf/2405.00208", "details": "J Ferrando, G Sarti, A Bisazza, MR Costa-juss\u00e0 - arXiv preprint arXiv:2405.00208, 2024", "abstract": "The rapid progress of research aimed at interpreting the inner workings of advanced language models has highlighted a need for contextualizing the insights gained from years of work in this area. This primer provides a concise technical introduction to the \u2026"}, {"title": "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "link": "https://arxiv.org/pdf/2404.19335", "details": "X Liu, C Liu, Z Zhang, C Li, L Wang, Y Lan, C Shen - arXiv preprint arXiv:2404.19335, 2024", "abstract": "Large language models have shown their ability to become effective few-shot learners with prompting, revoluting the paradigm of learning with data scarcity. However, this approach largely depends on the quality of prompt initialization, and \u2026"}]
