[{"title": "Imperfect Vision Encoders: Efficient and Robust Tuning for Vision-Language Models", "link": "https://arxiv.org/pdf/2407.16526", "details": "A Panos, R Aljundi, DO Reino, RE Turner - arXiv preprint arXiv:2407.16526, 2024", "abstract": "Vision language models (VLMs) demonstrate impressive capabilities in visual question answering and image captioning, acting as a crucial link between visual and language models. However, existing open-source VLMs heavily rely on \u2026"}, {"title": "Lifelong Robot Library Learning: Bootstrapping Composable and Generalizable Skills for Embodied Control with Language Models", "link": "https://arxiv.org/pdf/2406.18746", "details": "G Tziafas, H Kasaei - arXiv preprint arXiv:2406.18746, 2024", "abstract": "Large Language Models (LLMs) have emerged as a new paradigm for embodied reasoning and control, most recently by generating robot policy code that utilizes a custom library of vision and control primitive skills. However, prior arts fix their skills \u2026"}, {"title": "Test-Time Low Rank Adaptation via Confidence Maximization for Zero-Shot Generalization of Vision-Language Models", "link": "https://arxiv.org/pdf/2407.15913", "details": "R Imam, H Gani, M Huzaifa, K Nandakumar - arXiv preprint arXiv:2407.15913, 2024", "abstract": "The conventional modus operandi for adapting pre-trained vision-language models (VLMs) during test-time involves tuning learnable prompts, ie, test-time prompt tuning. This paper introduces Test-Time Low-rank adaptation (TTL) as an alternative \u2026"}, {"title": "Evaluating machine learning approaches for multi-label classification of unstructured electronic health records with a generative large language model", "link": "https://www.medrxiv.org/content/10.1101/2024.06.24.24309441.full.pdf", "details": "D Vithanage, C Deng, L Wang, M Yin, M Alkhalaf\u2026 - medRxiv, 2024", "abstract": "Multi-label classification of unstructured electronic health records (EHR) poses challenges due to the inherent semantic complexity in textual data. Advances in natural language processing (NLP) using large language models (LLMs) show \u2026"}, {"title": "Check for updates Extending the Tractability of the Clique Problem via Graph Classes Generalizing Treewidth", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DFYYVEQAAQBAJ%26oi%3Dfnd%26pg%3DPA94%26ots%3D8io2MbS56n%26sig%3DAtU4o5B_3Kl7cBjDTVtubwnlSBA", "details": "P J\u00e9gou - Artificial Intelligence and Image Analysis: 18th \u2026", "abstract": "The study of the Clique problem in algorithmic graph theory is important both because it is a central problem in complexity theory, almost at the same level as SAT [8], but also because its practical resolution has many applications, notably in \u2026"}, {"title": "PharmGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry", "link": "https://arxiv.org/pdf/2406.18045", "details": "L Chen, W Wang, Z Bai, P Xu, Y Fang, J Fang, W Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have revolutionized Natural Language Processing (NLP) by by minimizing the need for complex feature engineering. However, the application of LLMs in specialized domains like biopharmaceuticals and chemistry \u2026"}, {"title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs", "link": "https://arxiv.org/pdf/2407.00653", "details": "Y Zhang, X Wang, J Liang, S Xia, L Chen, Y Xiao - arXiv preprint arXiv:2407.00653, 2024", "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new \u2026"}, {"title": "Universal Approximation Theory: The basic theory for large language models", "link": "https://arxiv.org/pdf/2407.00958", "details": "W Wang, Q Li - arXiv preprint arXiv:2407.00958, 2024", "abstract": "Language models have emerged as a critical area of focus in artificial intelligence, particularly with the introduction of groundbreaking innovations like ChatGPT. Large- scale Transformer networks have quickly become the leading approach for \u2026"}, {"title": "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models", "link": "https://arxiv.org/pdf/2407.01906", "details": "Z Wang, D Chen, D Dai, R Xu, Z Li, Y Wu - arXiv preprint arXiv:2407.01906, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) is crucial for customizing Large Language Models (LLMs) with constrained resources. Although there have been various PEFT methods for dense-architecture LLMs, PEFT for sparse-architecture LLMs is still \u2026"}]
