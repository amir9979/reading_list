[{"title": "Leveraging long context in retrieval augmented language models for medical question answering", "link": "https://www.nature.com/articles/s41746-025-01651-w", "details": "G Zhang, Z Xu, Q Jin, F Chen, Y Fang, Y Liu\u2026 - npj Digital Medicine, 2025", "abstract": "While holding great promise for improving and facilitating healthcare through applications of medical literature summarization, large language models (LLMs) struggle to produce up-to-date responses on evolving topics due to outdated \u2026"}, {"title": "MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks", "link": "https://arxiv.org/pdf/2505.06152%3F", "details": "W Zeng, Y Sun, C Ma, W Tan, B Yan - arXiv preprint arXiv:2505.06152, 2025", "abstract": "Medical vision-language models (VLMs) have shown promise as clinical assistants across various medical fields. However, specialized dermatology VLM capable of delivering professional and detailed diagnostic analysis remains underdeveloped \u2026"}, {"title": "ALGOPUZZLEVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Algorithmic Multimodal Puzzles", "link": "https://aclanthology.org/2025.naacl-long.486.pdf", "details": "D Ghosal, V Toh, YK Chia, S Poria - Proceedings of the 2025 Conference of the \u2026, 2025", "abstract": "This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models \u2026"}, {"title": "Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning", "link": "https://arxiv.org/pdf/2505.09118", "details": "D Liang, C Zheng, Z Wen, Y Cai, XY Wei, Q Li - arXiv preprint arXiv:2505.09118, 2025", "abstract": "Traditional scene graphs primarily focus on spatial relationships, limiting vision- language models'(VLMs) ability to reason about complex interactions in visual scenes. This paper addresses two key challenges:(1) conventional detection-to \u2026"}, {"title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning", "link": "https://arxiv.org/pdf/2504.19627%3F", "details": "R Luo, R Shan, L Chen, Z Liu, L Wang, M Yang, X Xia - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like embodied intelligence due to their strong vision-language reasoning abilities. However, current LVLMs process entire images at the token level, which is inefficient \u2026"}, {"title": "Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput", "link": "https://arxiv.org/pdf/2505.09498", "details": "B Zhang, S Li, R Tian, Y Yang, J Tang, J Zhou, L Ma - arXiv preprint arXiv:2505.09498, 2025", "abstract": "In this paper, we introduce Flash-VL 2B, a novel approach to optimizing Vision- Language Models (VLMs) for real-time applications, targeting ultra-low latency and high throughput without sacrificing accuracy. Leveraging advanced architectural \u2026"}, {"title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "link": "https://arxiv.org/pdf/2504.19162", "details": "J Chen, B Zhang, R Ma, P Wang, X Liang, Z Tu, X Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play \u2026"}, {"title": "A Context-Aware Contrastive Learning Framework for Hateful Meme Detection and Segmentation", "link": "https://aclanthology.org/2025.findings-naacl.289.pdf", "details": "X Su, Y Li, D Inkpen, N Japkowicz - Findings of the Association for Computational \u2026, 2025", "abstract": "Amidst the rise of Large Multimodal Models (LMMs) and their widespread application in generating and interpreting complex content, the risk of propagating biased and harmful memes remains significant. Current safety measures often fail to detect \u2026"}, {"title": "Learn to Think: Bootstrapping LLM Reasoning Capability Through Graph Learning", "link": "https://arxiv.org/pdf/2505.06321", "details": "H Gao, C Zhang, T Wang, J Zhao, F Wu, C Zheng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have achieved remarkable success across various domains. However, they still face significant challenges, including high computational costs for training and limitations in solving complex reasoning \u2026"}]
