'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Adapting transformer-based language models for heart '
[{"title": "Emergent Abilities in Reduced-Scale Generative Language Models", "link": "https://arxiv.org/pdf/2404.02204", "details": "S Muckatira, V Deshpande, V Lialin, A Rumshisky - arXiv preprint arXiv:2404.02204, 2024", "abstract": "Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study \u2026"}, {"title": "Context versus Prior Knowledge in Language Models", "link": "https://arxiv.org/pdf/2404.04633", "details": "K Du, V Sn\u00e6bjarnarson, N Stoehr, JC White, A Schein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To answer a question, language models often need to integrate prior knowledge learned during pretraining and new information presented in context. We hypothesize that models perform this integration in a predictable way across different \u2026"}, {"title": "Mining Clinical Notes for Physical Rehabilitation Exercise Information: Natural Language Processing Algorithm Development and Validation Study", "link": "https://medinform.jmir.org/2024/1/e52289/", "details": "S Sivarajkumar, F Gao, P Denny, B Aldhahwani\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background: The rehabilitation of a patient who had a stroke requires precise, personalized treatment plans. Natural language processing (NLP) offers the potential to extract valuable exercise information from clinical notes, aiding in the development \u2026"}, {"title": "Question-answering system extracts information on injection drug use from clinical notes", "link": "https://www.nature.com/articles/s43856-024-00470-6", "details": "M Mahbub, I Goethert, I Danciu, K Knight, S Srinivasan\u2026 - Communications Medicine, 2024", "abstract": "Background Injection drug use (IDU) can increase mortality and morbidity. Therefore, identifying IDU early and initiating harm reduction interventions can benefit individuals at risk. However, extracting IDU behaviors from patients' electronic health \u2026"}, {"title": "Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context", "link": "https://arxiv.org/pdf/2404.02000", "details": "A Caubri\u00e8re, E Gauthier - arXiv preprint arXiv:2404.02000, 2024", "abstract": "We present the first self-supervised multilingual speech model trained exclusively on African speech. The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa. On the SSA \u2026"}, {"title": "Investigating Regularization of Self-Play Language Models", "link": "https://arxiv.org/pdf/2404.04291", "details": "R Alami, A Abubaker, M Achab, MEA Seddik, S Lahlou - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper explores the effects of various forms of regularization in the context of language model alignment via self-play. While both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) require to collect \u2026"}, {"title": "$\\texttt {LM}^\\texttt {2} $: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://arxiv.org/pdf/2404.02255", "details": "G Juneja, S Dutta, T Chakraborty - arXiv preprint arXiv:2404.02255, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}, {"title": "Meta In-Context Learning Makes Large Language Models Better Zero and Few-Shot Relation Extractors", "link": "https://arxiv.org/pdf/2404.17807", "details": "G Li, P Wang, J Liu, Y Guo, K Ji, Z Shang, Z Xu - arXiv preprint arXiv:2404.17807, 2024", "abstract": "Relation extraction (RE) is an important task that aims to identify the relationships between entities in texts. While large language models (LLMs) have revealed remarkable in-context learning (ICL) capability for general zero and few-shot \u2026"}, {"title": "IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe Biomedical Natural Language Inference for Clinical Trials", "link": "https://arxiv.org/pdf/2404.04510", "details": "S Mandal, A Modi - arXiv preprint arXiv:2404.04510, 2024", "abstract": "Large Language models (LLMs) have demonstrated state-of-the-art performance in various natural language processing (NLP) tasks across multiple domains, yet they are prone to shortcut learning and factual inconsistencies. This research investigates \u2026"}]
