[{"title": "Improving Context-Aware Preference Modeling for Language Models", "link": "https://arxiv.org/pdf/2407.14916", "details": "S Pitis, Z Xiao, NL Roux, A Sordoni - arXiv preprint arXiv:2407.14916, 2024", "abstract": "While finetuning language models from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where \u2026"}, {"title": "Out of spuriousity: Improving robustness to spurious correlations without group annotations", "link": "https://arxiv.org/pdf/2407.14974", "details": "PQ Le, J Schl\u00f6tterer, C Seifert - arXiv preprint arXiv:2407.14974, 2024", "abstract": "Machine learning models are known to learn spurious correlations, ie, features having strong relations with class labels but no causal relation. Relying on those correlations leads to poor performance in the data groups without these correlations \u2026"}, {"title": "Compact Language Models via Pruning and Knowledge Distillation", "link": "https://arxiv.org/pdf/2407.14679", "details": "S Muralidharan, ST Sreenivas, R Joshi, M Chochowski\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute- intensive. In this paper, we investigate if pruning an existing LLM and then re-training \u2026"}, {"title": "Universally Harmonizing Differential Privacy Mechanisms for Federated Learning: Boosting Accuracy and Convergence", "link": "https://arxiv.org/pdf/2407.14710", "details": "S Feng, M Mohammady, H Hong, S Yan, A Kundu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Differentially private federated learning (DP-FL) is a promising technique for collaborative model training while ensuring provable privacy for clients. However, optimizing the tradeoff between privacy and accuracy remains a critical challenge. To \u2026"}, {"title": "Learn to Preserve and Diversify: Parameter-Efficient Group with Orthogonal Regularization for Domain Generalization", "link": "https://arxiv.org/pdf/2407.15085", "details": "J Hu, J Zhang, L Qi, Y Shi, Y Gao - arXiv preprint arXiv:2407.15085, 2024", "abstract": "Domain generalization (DG) aims to avoid the performance degradation of the model when the distribution shift between the limited training data and unseen test data occurs. Recently, foundation models with enormous parameters have been pre \u2026"}, {"title": "DANSK: Domain Generalization of Danish Named Entity Recognition", "link": "https://nejlt.ep.liu.se/article/view/5249/4332", "details": "K Enevoldsen, ET Jessen, R Baglini - Northern European Journal of Language \u2026, 2024", "abstract": "Named entity recognition is an important application within Danish NLP, essential within both industry and research. However, Danish NER is inhibited by a lack coverage across domains and entity types. As a consequence, no current models are \u2026"}, {"title": "Lifelong Robot Library Learning: Bootstrapping Composable and Generalizable Skills for Embodied Control with Language Models", "link": "https://arxiv.org/pdf/2406.18746", "details": "G Tziafas, H Kasaei - arXiv preprint arXiv:2406.18746, 2024", "abstract": "Large Language Models (LLMs) have emerged as a new paradigm for embodied reasoning and control, most recently by generating robot policy code that utilizes a custom library of vision and control primitive skills. However, prior arts fix their skills \u2026"}, {"title": "Generalization vs Memorization: Tracing Language Models' Capabilities Back to Pretraining Data", "link": "https://arxiv.org/pdf/2407.14985", "details": "A Antoniades, X Wang, Y Elazar, A Amayuelas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the proven utility of large language models (LLMs) in real-world applications, there remains a lack of understanding regarding how they leverage their large-scale pretraining text corpora to achieve such capabilities. In this work, we investigate the \u2026"}, {"title": "Entropy-Based Decoding for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2406.17519", "details": "Z Qiu, Z Ou, B Wu, J Li, A Liu, I King - arXiv preprint arXiv:2406.17519, 2024", "abstract": "Augmenting Large Language Models (LLMs) with retrieved external knowledge has proven effective for improving the factual accuracy of generated responses. Despite their success, retrieval-augmented LLMs still face the distractibility issue, where the \u2026"}]
