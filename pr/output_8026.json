[{"title": "Training Language Models on Synthetic Edit Sequences Improves Code Synthesis", "link": "https://arxiv.org/pdf/2410.02749%3F", "details": "U Piterbarg, L Pinto, R Fergus - arXiv preprint arXiv:2410.02749, 2024", "abstract": "Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality \u2026"}, {"title": "No Need to Talk: Asynchronous Mixture of Language Models", "link": "https://arxiv.org/pdf/2410.03529%3F", "details": "A Filippova, A Katharopoulos, D Grangier, R Collobert - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce SmallTalk LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each model of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth \u2026"}, {"title": "Scaling Parameter-Constrained Language Models with Quality Data", "link": "https://arxiv.org/pdf/2410.03083", "details": "E Chang, M Paltenghi, Y Li, PJ Lin, C Zhao, P Huber\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws in language modeling traditionally quantify training loss as a function of dataset size and model parameters, providing compute-optimal estimates but often neglecting the impact of data quality on model generalization. In this paper, we \u2026"}, {"title": "Unsupervised SapBERT-based bi-encoders for medical concept annotation of clinical narratives with SNOMED CT", "link": "https://journals.sagepub.com/doi/pdf/10.1177/20552076241288681", "details": "A Abdulnazar, R Roller, S Schulz, M Kreuzthaler - DIGITAL HEALTH, 2024", "abstract": "Objective Clinical narratives provide comprehensive patient information. Achieving interoperability involves mapping relevant details to standardized medical vocabularies. Typically, natural language processing divides this task into named \u2026"}, {"title": "Ascle\u2014A Python Natural Language Processing Toolkit for Medical Text Generation: Development and Evaluation Study", "link": "https://www.jmir.org/2024/1/e60601/", "details": "R Yang, Q Zeng, K You, Y Qiao, L Huang, CC Hsieh\u2026 - Journal of Medical Internet \u2026, 2024", "abstract": "Background Medical texts present significant domain-specific challenges, and manually curating these texts is a time-consuming and labor-intensive process. To address this, natural language processing (NLP) algorithms have been developed to \u2026"}, {"title": "Dynamic Contrastive Learning for Time Series Representation", "link": "https://arxiv.org/pdf/2410.15416", "details": "AK Shamba, K Bach, G Taylor - arXiv preprint arXiv:2410.15416, 2024", "abstract": "Understanding events in time series is an important task in a variety of contexts. However, human analysis and labeling are expensive and time-consuming. Therefore, it is advantageous to learn embeddings for moments in time series in an \u2026"}, {"title": "Reflections on interactive visualization of electronic health records: past, present, future", "link": "https://academic.oup.com/jamia/article-abstract/31/11/2423/7824391", "details": "A Arleo, AT Chen, D Gotz, S Kandaswamy, J Bernard - Journal of the American \u2026, 2024", "abstract": "In the early 2000s, the transition to paperless documentation of patients' health data begun at large scale, with the introduction of Electronic Health and Medical Records (EHR and EMR, respectively). This constituted a paradigm shift in how patient data \u2026"}, {"title": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt Optimization for Text Generation", "link": "https://arxiv.org/pdf/2410.02748%3F", "details": "H He, Q Liu, L Xu, C Shivade, Y Zhang, S Srinivasan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) can generate fluent summaries across domains using prompting techniques, reducing the need to train models for summarization applications. However, crafting effective prompts that guide LLMs to generate \u2026"}, {"title": "Small Language Models: Survey, Measurements, and Insights", "link": "https://arxiv.org/pdf/2409.15790%3F", "details": "Z Lu, X Li, D Cai, R Yi, F Liu, X Zhang, ND Lane, M Xu - arXiv preprint arXiv \u2026, 2024", "abstract": "Small language models (SLMs), despite their widespread adoption in modern smart devices, have received significantly less academic attention compared to their large language model (LLM) counterparts, which are predominantly deployed in data \u2026"}]
