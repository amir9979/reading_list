[{"title": "194 Machine learning for early prediction of sepsis from electronic health records: a preliminary validation study at GOSH", "link": "https://bmjpaedsopen.bmj.com/content/9/Suppl_1/A62.2", "details": "S Champeaux, SA Bowyer, J Booth, D Key, NJ Sebire - 2025", "abstract": "Background Electronic Health Records (EHRs) are being leveraged to build Machine Learning (ML) models to help tackle the growing burden of neonatal sepsis. Regrettably, obstacles in external validation hinder their seamless integration into \u2026"}, {"title": "Efficient GPT-4V Level Multimodal Large Language Model for Deployment on Edge Devices", "link": "https://www.researchsquare.com/article/rs-5830327/latest.pdf", "details": "Y Yao, T Yu, A Zhang, C Wang, J Cui, H Zhu, T Cai\u2026 - 2025", "abstract": "The recent surge of Multimodal Large Language Models (MLLMs) has fundamentally reshaped the landscape of AI research and industry, shedding light on a promising path toward the next AI milestone. However, significant challenges remain \u2026"}, {"title": "ChartAdapter: Large Vision-Language Model for Chart Summarization", "link": "https://arxiv.org/pdf/2412.20715", "details": "P Xu, Y Ding, W Fan - arXiv preprint arXiv:2412.20715, 2024", "abstract": "Chart summarization, which focuses on extracting key information from charts and interpreting it in natural language, is crucial for generating and delivering insights through effective and accessible data analysis. Traditional methods for chart \u2026"}, {"title": "An Empirical Study of Autoregressive Pre-training from Videos", "link": "https://arxiv.org/pdf/2501.05453", "details": "J Rajasegaran, I Radosavovic, R Ravishankar\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We empirically study autoregressive pre-training from videos. To perform our study, we construct a series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict \u2026"}, {"title": "Foundations of Large Language Models", "link": "https://arxiv.org/pdf/2501.09223", "details": "T Xiao, J Zhu - arXiv preprint arXiv:2501.09223, 2025", "abstract": "This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting- edge technologies. The book is structured into four main chapters, each exploring a \u2026"}, {"title": "Large Language Models and Large Multimodal Models in Medical Imaging: A Primer for Physicians", "link": "https://jnm.snmjournals.org/content/early/2025/01/16/jnumed.124.268072.abstract", "details": "TJ Bradshaw, X Tie, J Warner, J Hu, Q Li, X Li - Journal of Nuclear Medicine, 2025", "abstract": "Large language models (LLMs) are poised to have a disruptive impact on health care. Numerous studies have demonstrated promising applications of LLMs in medical imaging, and this number will grow as LLMs further evolve into large \u2026"}, {"title": "Knowledge Graph as Pre-training Corpus for Structural Reasoning via Multi-hop Linearization", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10817607.pdf", "details": "W Kim, H Jung, W Kim - IEEE Access, 2024", "abstract": "Large language models have demonstrated exceptional performance across various natural language processing tasks. However, their reliance on unstructured text corpora for pre-training limits their effectiveness in tasks requiring structured \u2026"}]
