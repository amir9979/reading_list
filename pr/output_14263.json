[{"title": "OT-DETECTOR: Delving into Optimal Transport for Zero-shot Out-of-Distribution Detection", "link": "https://arxiv.org/pdf/2503.06442", "details": "Y Liu, H Tang, H Zhang, J Qin, Z Li - arXiv preprint arXiv:2503.06442, 2025", "abstract": "Out-of-distribution (OOD) detection is crucial for ensuring the reliability and safety of machine learning models in real-world applications. While zero-shot OOD detection, which requires no training on in-distribution (ID) data, has become feasible with the \u2026"}, {"title": "Continual Multimodal Contrastive Learning", "link": "https://arxiv.org/pdf/2503.14963", "details": "X Liu, X Xia, SK Ng, TS Chua - arXiv preprint arXiv:2503.14963, 2025", "abstract": "Multimodal contrastive learning (MCL) advances in aligning different modalities and generating multimodal representations in a joint space. By leveraging contrastive learning across diverse modalities, large-scale multimodal data enhances \u2026"}, {"title": "A Time Series Self-Supervised Contrastive Pre-Training Method with Data Augmentation using Discrepancy of Reconstruction Information Loss", "link": "https://ieeexplore.ieee.org/abstract/document/10912708/", "details": "Z Zhang, Y Han, B Ma, Z Geng - IEEE Transactions on Instrumentation and \u2026, 2025", "abstract": "Self-supervised pre-training has shown considerable advantages in multiple time series tasks. A widely used class of methods is based on contrastive learning and a key problem is how to construct augmented samples. At present, explicit \u2026"}, {"title": "One-Class Classification Constraint in Reconstruction Networks for Multivariate Time Series Anomaly Detection", "link": "https://ieeexplore.ieee.org/abstract/document/10912658/", "details": "J Li, Z Yu, Q Jiang, Z Cao - IEEE Transactions on Instrumentation and \u2026, 2025", "abstract": "Detecting anomalies in multivariate time series (MTS) data is crucial for maintaining the stability of industrial manufacturing processes and biochemical operations. However, current methods often focus on capturing the normal patterns of training \u2026"}, {"title": "AD2T: Multivariate Time Series Anomaly Detection with Association Discrepancy Dual-Decoder Transformer", "link": "https://www.researchgate.net/profile/Zezhong-Li-12/publication/389356933_AD2T_Multivariate_Time_Series_Anomaly_Detection_with_Association_Discrepancy_Dual-Decoder_Transformer/links/67bfbe528311ce680c760f3f/AD2T-Multivariate-Time-Series-Anomaly-Detection-with-Association-Discrepancy-Dual-Decoder-Transformer.pdf", "details": "Z Li, W Guo, J An, Q Wang, Y Mei, R Juan, T Wang, Y Li\u2026 - IEEE Sensors Journal, 2025", "abstract": "Multivariate time series (MTS) anomaly detection is of great importance in both condition monitoring and malfunction identification within multi-sensor systems. Current MTS anomaly detection approaches are typically based on reconstruction \u2026"}, {"title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation", "link": "https://arxiv.org/pdf/2503.04872", "details": "L Sun, G Zhao, X Jian, Y Wu, W Lin, Y Zhu, L Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The challenge of reducing the size of Large Language Models (LLMs) while maintaining their performance has gained significant attention. However, existing methods, such as model distillation and transfer learning, often fail to achieve high \u2026"}, {"title": "Contrastive Learning via Randomly Generated Deep Supervision", "link": "https://ieeexplore.ieee.org/abstract/document/10890867/", "details": "S Wang, Z Ma, KH Chan, Y Liu, T Tong, Q Gao, G Zhai\u2026 - ICASSP 2025-2025 IEEE \u2026, 2025", "abstract": "Unsupervised visual representation learning has gained significant attention in the computer vision community, driven by recent advancements in contrastive learning. Most existing contrastive learning frameworks rely on instance discrimination as a \u2026"}, {"title": "TraFlow: Trajectory Distillation on Pre-Trained Rectified Flow", "link": "https://arxiv.org/pdf/2502.16972", "details": "Z Wu, X Fan, H Wu, L Cao - arXiv preprint arXiv:2502.16972, 2025", "abstract": "Majorities of distillation methods on pre-trained diffusion models or on pre-trained rectified flow, focus on either the distillation outputs or the trajectories between random noises and clean images to speed up sample generations from pre-trained \u2026"}, {"title": "[TINY] Vision language models can implicitly quantify aleatoric uncertainty", "link": "https://openreview.net/pdf%3Fid%3DBkWVcXevTs", "details": "X Wang, E Nalisnick - \u2026 Workshop: Quantify Uncertainty and Hallucination in \u2026", "abstract": "Recent advances in vision language models (VLMs), such as GPT-4o, have revolutionized visual reasoning by enabling zero-shot task completion through natural language instructions. In this paper, we study VLMs' ability to detect input \u2026"}]
