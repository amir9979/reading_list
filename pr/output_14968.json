[{"title": "Lost in Multilinguality: Dissecting Cross-lingual Factual Inconsistency in Transformer Language Models", "link": "https://arxiv.org/pdf/2504.04264", "details": "M Wang, H Adel, L Lange, Y Liu, E Nie, J Str\u00f6tgen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multilingual language models (MLMs) store factual knowledge across languages but often struggle to provide consistent responses to semantically equivalent prompts in different languages. While previous studies point out this cross-lingual inconsistency \u2026"}, {"title": "JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model", "link": "https://arxiv.org/pdf/2504.03770", "details": "Y Nian, S Zhu, Y Qin, L Li, Z Wang, C Xiao, Y Zhao - arXiv preprint arXiv:2504.03770, 2025", "abstract": "Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety \u2026"}, {"title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models", "link": "https://arxiv.org/pdf/2503.18931", "details": "Y Chen, L Meng, W Peng, Z Wu, YG Jiang - arXiv preprint arXiv:2503.18931, 2025", "abstract": "Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of \u2026"}, {"title": "Boundary-guided Contrastive Learning for Semi-supervised Medical Image Segmentation", "link": "https://ieeexplore.ieee.org/abstract/document/10946212/", "details": "Y Yang, J Zhuang, G Sun, R Wang, J Su - IEEE Transactions on Medical Imaging, 2025", "abstract": "Semi-supervised learning methods, compared to fully supervised learning, offer significant potential to alleviate the burden of manual annotations on clinicians. By leveraging unlabeled data, these methods can aid in the development of medical \u2026"}, {"title": "Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregressive Representation Alignment", "link": "https://arxiv.org/pdf/2503.07334%3F", "details": "X Xie, J Liu, Z Lin, H Fan, Z Han, Y Tang, L Qu - arXiv preprint arXiv:2503.07334, 2025", "abstract": "We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural changes. Unlike prior work that requires complex \u2026"}, {"title": "ViLBench: A Suite for Vision-Language Process Reward Modeling", "link": "https://arxiv.org/pdf/2503.20271", "details": "H Tu, W Feng, H Chen, H Liu, X Tang, C Xie - arXiv preprint arXiv:2503.20271, 2025", "abstract": "Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on \u2026"}, {"title": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning", "link": "https://arxiv.org/pdf/2504.04348", "details": "S Wang, Z Yu, X Jiang, S Lan, M Shi, N Chang, J Kautz\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The advances in vision-language models (VLMs) have led to a growing interest in autonomous driving to leverage their strong reasoning capabilities. However, extending these capabilities from 2D to full 3D understanding is crucial for real-world \u2026"}, {"title": "Cognitive Debiasing Large Language Models for Decision-Making", "link": "https://arxiv.org/pdf/2504.04141", "details": "Y Lyu, S Ren, Y Feng, Z Wang, Z Chen, Z Ren\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have shown potential in supporting decision-making applications, particularly as personal conversational assistants in the financial, healthcare, and legal domains. While prompt engineering strategies have enhanced \u2026"}, {"title": "SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models", "link": "https://arxiv.org/pdf/2504.02883", "details": "A Ramakrishna, Y Wan, X Jin, KW Chang, Z Bu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce SemEval-2025 Task 4: unlearning sensitive content from Large Language Models (LLMs). The task features 3 subtasks for LLM unlearning spanning different use cases:(1) unlearn long form synthetic creative documents spanning \u2026"}]
