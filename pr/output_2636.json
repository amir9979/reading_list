[{"title": "Building Vision-Language Models on Solid Foundations with Masked Distillation", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Sameni_Building_Vision-Language_Models_on_Solid_Foundations_with_Masked_Distillation_CVPR_2024_paper.pdf", "details": "S Sameni, K Kafle, H Tan, S Jenni - Proceedings of the IEEE/CVF Conference on \u2026, 2024", "abstract": "Abstract Recent advancements in Vision-Language Models (VLMs) have marked a significant leap in bridging the gap between computer vision and natural language processing. However traditional VLMs trained through contrastive learning on limited \u2026"}, {"title": "Symmetric Dot-Product Attention for Efficient Training of BERT Language Models", "link": "https://arxiv.org/pdf/2406.06366", "details": "M Courtois, M Ostendorff, L Hennig, G Rehm - arXiv preprint arXiv:2406.06366, 2024", "abstract": "Initially introduced as a machine translation model, the Transformer architecture has now become the foundation for modern deep learning architecture, with applications in a wide range of fields, from computer vision to natural language processing \u2026"}, {"title": "MMA: Multi-Modal Adapter for Vision-Language Models", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_MMA_Multi-Modal_Adapter_for_Vision-Language_Models_CVPR_2024_paper.pdf", "details": "L Yang, RY Zhang, Y Wang, X Xie - Proceedings of the IEEE/CVF Conference on \u2026, 2024", "abstract": "Abstract Pre-trained Vision-Language Models (VLMs) have served as excellent foundation models for transfer learning in diverse downstream tasks. However tuning VLMs for few-shot generalization tasks faces a discrimination--generalization \u2026"}, {"title": "FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Bulat_FFF_Fixing_Flawed_Foundations_in_Contrastive_Pre-Training_Results_in_Very_CVPR_2024_paper.pdf", "details": "A Bulat, Y Ouali, G Tzimiropoulos - Proceedings of the IEEE/CVF Conference on \u2026, 2024", "abstract": "Despite noise and caption quality having been acknowledged as important factors impacting vision-language contrastive pre-training in this paper we show that the full potential of improving the training process by addressing such issues is yet to be \u2026"}, {"title": "Why are Visually-Grounded Language Models Bad at Image Classification?", "link": "https://arxiv.org/pdf/2405.18415", "details": "Y Zhang, A Unell, X Wang, D Ghosh, Y Su, L Schmidt\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Image classification is one of the most fundamental capabilities of machine vision intelligence. In this work, we revisit the image classification task using visually- grounded language models (VLMs) such as GPT-4V and LLaVA. We find that \u2026"}, {"title": "Observational Scaling Laws and the Predictability of Language Model Performance", "link": "https://arxiv.org/pdf/2405.10938", "details": "Y Ruan, CJ Maddison, T Hashimoto - arXiv preprint arXiv:2405.10938, 2024", "abstract": "Understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different \u2026"}, {"title": "CoG-DQA: Chain-of-Guiding Learning with Large Language Models for Diagram Question Answering", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_CoG-DQA_Chain-of-Guiding_Learning_with_Large_Language_Models_for_Diagram_Question_CVPR_2024_paper.pdf", "details": "S Wang, L Zhang, L Zhu, T Qin, KH Yap, X Zhang, J Liu - Proceedings of the IEEE \u2026, 2024", "abstract": "Abstract Diagram Question Answering (DQA) is a challenging task requiring models to answer natural language questions based on visual diagram contexts. It serves as a crucial basis for academic tutoring technical support and more practical \u2026"}, {"title": "Concept Formation and Alignment in Language Models: Bridging Statistical Patterns in Latent Space to Concept Taxonomy", "link": "https://arxiv.org/abs/2406.05315", "details": "M Khatir, CK Reddy - arXiv preprint arXiv:2406.05315, 2024", "abstract": "This paper explores the concept formation and alignment within the realm of language models (LMs). We propose a mechanism for identifying concepts and their hierarchical organization within the semantic representations learned by various \u2026"}, {"title": "MarkovGen: Structured Prediction for Efficient Text-to-Image Generation", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Jayasumana_MarkovGen_Structured_Prediction_for_Efficient_Text-to-Image_Generation_CVPR_2024_paper.pdf", "details": "S Jayasumana, D Glasner, S Ramalingam, A Veit\u2026 - Proceedings of the IEEE \u2026, 2024", "abstract": "Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However this quality comes at significant computational cost: nearly all of these models are iterative and require \u2026"}]
