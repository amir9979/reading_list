[{"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers", "link": "https://openreview.net/pdf%3Fid%3D67tRrjgzsh", "details": "X Lu, Y Zhao, B Qin, L Huo, Q Yang, D Xu - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Multisource representation learning for pediatric knowledge extraction from electronic health records", "link": "https://www.nature.com/articles/s41746-024-01320-4", "details": "M Li, X Li, K Pan, A Geva, D Yang, SM Sweet\u2026 - npj Digital Medicine, 2024", "abstract": "Abstract Electronic Health Record (EHR) systems are particularly valuable in pediatrics due to high barriers in clinical studies, but pediatric EHR data often suffer from low content density. Existing EHR code embeddings tailored for the general \u2026"}, {"title": "Accelerating Blockwise Parallel Language Models with Draft Refinement", "link": "https://openreview.net/pdf%3Fid%3DKT6F5Sw0eg", "details": "T Kim, AT Suresh, KA Papineni, M Riley, S Kumar\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Autoregressive language models have achieved remarkable advancements, yet their potential is often limited by the slow inference speeds associated with sequential token generation. Blockwise parallel decoding (BPD) was proposed by Stern et \u2026"}, {"title": "Mathematical Reasoning via Multi-step Self Questioning and Answering for Small Language Models", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9440-9_7", "details": "K Chen, J Wang, X Zhang - CCF International Conference on Natural Language \u2026, 2024", "abstract": "Mathematical reasoning is challenging for large language models (LLMs), while the scaling relationship concerning LLM capacity is under-explored. Existing works have tried to leverage the rationales of LLMs to train small language models (SLMs) for \u2026"}, {"title": "Optimizing Fine-Tuning in Quantized Language Models: An In-Depth Analysis of Key Variables", "link": "https://cdn.techscience.cn/files/cmc/2024/online/CMC1030/TSP_CMC_57491/TSP_CMC_57491.pdf", "details": "A Shen, Z Lai, D Li, X Hu - 2024", "abstract": "ABSTRACT Large-scale Language Models (LLMs) have achieved significant breakthroughs in Natural Language Processing (NLP), driven by the pre-training and fine-tuning paradigm. While this approach allows models to specialize in specific \u2026"}, {"title": "Probing Social Bias in Labor Market Text Generation by ChatGPT: A Masked Language Model Approach", "link": "https://openreview.net/pdf%3Fid%3DMP7j58lbWO", "details": "L Ding, Y Hu, N Denier, E Shi, J Zhang, Q Hu\u2026 - The Thirty-eighth Annual \u2026", "abstract": "As generative large language models (LLMs) such as ChatGPT gain widespread adoption in various domains, their potential to propagate and amplify social biases, particularly in high-stakes areas such as the labor market, has become a pressing \u2026"}, {"title": "Language-Emphasized Cross-Lingual In-Context Learning for Multilingual LLM", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9437-9_26", "details": "J Li, X Wei, X Wang, N Zhuang, L Wang, J Dang - CCF International Conference on \u2026, 2024", "abstract": "With the recent rise of large language models (LLMs), in-context learning (ICL) has shown remarkable performance, eliminating the need for fine-tuning parameters and reducing the reliance on extensive labeled data. However, the intricacies of cross \u2026"}, {"title": "PqE: Zero-Shot Document Expansion for Dense Retrieval with Large Language Models", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9431-7_8", "details": "J Liu, D Zou, N Chai, Y Yang, H Wang, X Song - CCF International Conference on \u2026, 2024", "abstract": "The dense retrieval model offers remarkable capabilities, yet it exhibits inconsistencies in the embedding space of queries and documents due to its dual- encoder structure. Addressing this limitation, we introduce Pseudo-query Embedding \u2026"}, {"title": "HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2410.22832", "details": "Y Zhang, Q Li, T Du, X Zhang, X Zhao, Z Feng, J Yin - arXiv preprint arXiv:2410.22832, 2024", "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge, making them adaptable and cost-effective for various applications. However, the growing reliance on these systems also \u2026"}]
