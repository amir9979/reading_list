[{"title": "LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection", "link": "https://arxiv.org/pdf/2505.12507", "details": "X Zheng, Z Chen, E Schafir, S Chen, HA Salehi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The impressive ability of large language models to generate natural text across various tasks has led to critical challenges in authorship authentication. Although numerous detection methods have been developed to differentiate between machine \u2026", "entry_id": "http://arxiv.org/abs/2505.12507v1", "updated": "2025-05-18 17:55:45", "published": "2025-05-18 17:55:45", "authors": "Xu Zheng;Zhuomin Chen;Esteban Schafir;Sipeng Chen;Hojat Allah Salehi;Haifeng Chen;Farhad Shirani;Wei Cheng;Dongsheng Luo", "summary": "The impressive ability of large language models to generate natural text\nacross various tasks has led to critical challenges in authorship\nauthentication. Although numerous detection methods have been developed to\ndifferentiate between machine-generated texts (MGT) and human-generated texts\n(HGT), the explainability of these methods remains a significant gap.\nTraditional explainability techniques often fall short in capturing the complex\nword relationships that distinguish HGT from MGT. To address this limitation,\nwe present LM$^2$otifs, a novel explainable framework for MGT detection.\nInspired by probabilistic graphical models, we provide a theoretical rationale\nfor the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks\nto achieve both accurate detection and interpretability. The LM$^2$otifs\npipeline operates in three key stages: first, it transforms text into graphs\nbased on word co-occurrence to represent lexical dependencies; second, graph\nneural networks are used for prediction; and third, a post-hoc explainability\nmethod extracts interpretable motifs, offering multi-level explanations from\nindividual words to sentence structures. Extensive experiments on multiple\nbenchmark datasets demonstrate the comparable performance of LM$^2$otifs. The\nempirical evaluation of the extracted explainable motifs confirms their\neffectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis\nreveals distinct and visible linguistic fingerprints characteristic of MGT.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.CY", "links": "http://arxiv.org/abs/2505.12507v1;http://arxiv.org/pdf/2505.12507v1", "pdf_url": "http://arxiv.org/pdf/2505.12507v1"}, {"title": "Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning", "link": "https://arxiv.org/pdf/2505.14125", "details": "VAK Tran, E Neftci, W Wybo - arXiv preprint arXiv:2505.14125, 2025", "abstract": "Biological brains learn continually from a stream of unlabeled data, while integrating specialized information from sparsely labeled examples without compromising their ability to generalize. Meanwhile, machine learning methods are susceptible to \u2026", "entry_id": "http://arxiv.org/abs/2505.14125v1", "updated": "2025-05-20 09:31:57", "published": "2025-05-20 09:31:57", "authors": "Viet Anh Khoa Tran;Emre Neftci;Willem. A. M. Wybo", "summary": "Biological brains learn continually from a stream of unlabeled data, while\nintegrating specialized information from sparsely labeled examples without\ncompromising their ability to generalize. Meanwhile, machine learning methods\nare susceptible to catastrophic forgetting in this natural learning setting, as\nsupervised specialist fine-tuning degrades performance on the original task. We\nintroduce task-modulated contrastive learning (TMCL), which takes inspiration\nfrom the biophysical machinery in the neocortex, using predictive coding\nprinciples to integrate top-down information continually and without\nsupervision. We follow the idea that these principles build a view-invariant\nrepresentation space, and that this can be implemented using a contrastive\nloss. Then, whenever labeled samples of a new class occur, new affine\nmodulations are learned that improve separation of the new class from all\nothers, without affecting feedforward weights. By co-opting the view-invariance\nlearning mechanism, we then train feedforward weights to match the unmodulated\nrepresentation of a data sample to its modulated counterparts. This introduces\nmodulation invariance into the representation space, and, by also using past\nmodulations, stabilizes it. Our experiments show improvements in both\nclass-incremental and transfer learning over state-of-the-art unsupervised\napproaches, as well as over comparable supervised approaches, using as few as\n1% of available labels. Taken together, our work suggests that top-down\nmodulations play a crucial role in balancing stability and plasticity.", "comment": "33 pages, 5 figures", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;q-bio.NC;68T05 (primary), 68T07, 68T45 (secondary);I.2.6; I.2.10", "links": "http://arxiv.org/abs/2505.14125v1;http://arxiv.org/pdf/2505.14125v1", "pdf_url": "http://arxiv.org/pdf/2505.14125v1"}, {"title": "GenZSL: Generative Zero-Shot Learning Via Inductive Variational Autoencoder", "link": "https://arxiv.org/pdf/2505.11882", "details": "S Chen, D Fu, S Khan, FS Khan - arXiv preprint arXiv:2505.11882, 2025", "abstract": "Remarkable progress in zero-shot learning (ZSL) has been achieved using generative models. However, existing generative ZSL methods merely generate (imagine) the visual features from scratch guided by the strong class semantic \u2026", "entry_id": "http://arxiv.org/abs/2505.11882v1", "updated": "2025-05-17 07:24:13", "published": "2025-05-17 07:24:13", "authors": "Shiming Chen;Dingjie Fu;Salman Khan;Fahad Shahbaz Khan", "summary": "Remarkable progress in zero-shot learning (ZSL) has been achieved using\ngenerative models. However, existing generative ZSL methods merely generate\n(imagine) the visual features from scratch guided by the strong class semantic\nvectors annotated by experts, resulting in suboptimal generative performance\nand limited scene generalization. To address these and advance ZSL, we propose\nan inductive variational autoencoder for generative zero-shot learning, dubbed\nGenZSL. Mimicking human-level concept learning, GenZSL operates by inducting\nnew class samples from similar seen classes using weak class semantic vectors\nderived from target class names (i.e., CLIP text embedding). To ensure the\ngeneration of informative samples for training an effective ZSL classifier, our\nGenZSL incorporates two key strategies. Firstly, it employs class diversity\npromotion to enhance the diversity of class semantic vectors. Secondly, it\nutilizes target class-guided information boosting criteria to optimize the\nmodel. Extensive experiments conducted on three popular benchmark datasets\nshowcase the superiority and potential of our GenZSL with significant efficacy\nand efficiency over f-VAEGAN, e.g., 24.7% performance gains and more than\n$60\\times$ faster training speed on AWA2. Codes are available at\nhttps://github.com/shiming-chen/GenZSL.", "comment": "Accepted to ICML'25", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.LG", "links": "http://arxiv.org/abs/2505.11882v1;http://arxiv.org/pdf/2505.11882v1", "pdf_url": "http://arxiv.org/pdf/2505.11882v1"}, {"title": "Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability", "link": "https://arxiv.org/pdf/2505.13963", "details": "Q Wang, M Wang, N Feldhus, S Ostermann, Y Cao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Quantization methods are widely used to accelerate inference and streamline the deployment of large language models (LLMs). While prior research has extensively investigated the degradation of various LLM capabilities due to quantization, its \u2026", "entry_id": "http://arxiv.org/abs/2505.13963v1", "updated": "2025-05-20 06:01:09", "published": "2025-05-20 06:01:09", "authors": "Qianli Wang;Mingyang Wang;Nils Feldhus;Simon Ostermann;Yuan Cao;Hinrich Sch\u00fctze;Sebastian M\u00f6ller;Vera Schmitt", "summary": "Quantization methods are widely used to accelerate inference and streamline\nthe deployment of large language models (LLMs). While prior research has\nextensively investigated the degradation of various LLM capabilities due to\nquantization, its effects on model explainability and interpretability, which\nare crucial for understanding decision-making processes, remain unexplored. To\naddress this gap, we conduct comprehensive experiments using three common\nquantization techniques at distinct bit widths, in conjunction with two\nexplainability methods, counterfactual examples and natural language\nexplanations, as well as two interpretability approaches, knowledge\nmemorization analysis and latent multi-hop reasoning analysis. We complement\nour analysis with a thorough user study, evaluating selected explainability\nmethods. Our findings reveal that, depending on the configuration, quantization\ncan significantly impact model explainability and interpretability. Notably,\nthe direction of this effect is not consistent, as it strongly depends on (1)\nthe quantization method, (2) the explainability or interpretability approach,\nand (3) the evaluation protocol. In some settings, human evaluation shows that\nquantization degrades explainability, while in others, it even leads to\nimprovements. Our work serves as a cautionary tale, demonstrating that\nquantization can unpredictably affect model transparency. This insight has\nimportant implications for deploying LLMs in applications where transparency is\na critical requirement.", "comment": "In submission", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.13963v1;http://arxiv.org/pdf/2505.13963v1", "pdf_url": "http://arxiv.org/pdf/2505.13963v1"}, {"title": "Contrastive Learning via Variational Information Bottleneck", "link": "https://pubmed.ncbi.nlm.nih.gov/40392641/", "details": "J Li, Y Wang, X Zhang, D Jiang, W Dai, C Li, H Xiong\u2026 - IEEE transactions on pattern \u2026, 2025", "abstract": "Recent advances in self-supervised learning have witnessed great achievements, especially with the introduction of contrastive learning, where the goal is to maximize the mutual information between different augmentations of the same image, ie \u2026"}, {"title": "Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders", "link": "https://arxiv.org/pdf/2505.11756", "details": "D Chanin, T Dulka, A Garriga-Alonso - arXiv preprint arXiv:2505.11756, 2025", "abstract": "It is assumed that sparse autoencoders (SAEs) decompose polysemantic activations into interpretable linear directions, as long as the activations are composed of sparse linear combinations of underlying features. However, we find that if an SAE is more \u2026", "entry_id": "http://arxiv.org/abs/2505.11756v1", "updated": "2025-05-16 23:30:17", "published": "2025-05-16 23:30:17", "authors": "David Chanin;Tom\u00e1\u0161 Dulka;Adri\u00e0 Garriga-Alonso", "summary": "It is assumed that sparse autoencoders (SAEs) decompose polysemantic\nactivations into interpretable linear directions, as long as the activations\nare composed of sparse linear combinations of underlying features. However, we\nfind that if an SAE is more narrow than the number of underlying \"true\nfeatures\" on which it is trained, and there is correlation between features,\nthe SAE will merge components of correlated features together, thus destroying\nmonosemanticity. In LLM SAEs, these two conditions are almost certainly true.\nThis phenomenon, which we call feature hedging, is caused by SAE reconstruction\nloss, and is more severe the narrower the SAE. In this work, we introduce the\nproblem of feature hedging and study it both theoretically in toy models and\nempirically in SAEs trained on LLMs. We suspect that feature hedging may be one\nof the core reasons that SAEs consistently underperform supervised baselines.\nFinally, we use our understanding of feature hedging to propose an improved\nvariant of matryoshka SAEs. Our work shows there remain fundamental issues with\nSAEs, but we are hopeful that that highlighting feature hedging will catalyze\nfuture advances that allow SAEs to achieve their full potential of interpreting\nLLMs at scale.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CL", "links": "http://arxiv.org/abs/2505.11756v1;http://arxiv.org/pdf/2505.11756v1", "pdf_url": "http://arxiv.org/pdf/2505.11756v1"}, {"title": "Learning from Less: Guiding Deep Reinforcement Learning with Differentiable Symbolic Planning", "link": "https://arxiv.org/pdf/2505.11661", "details": "Z Ye, O Arenz, K Kersting - arXiv preprint arXiv:2505.11661, 2025", "abstract": "When tackling complex problems, humans naturally break them down into smaller, manageable subtasks and adjust their initial plans based on observations. For instance, if you want to make coffee at a friend's place, you might initially plan to grab \u2026", "entry_id": "http://arxiv.org/abs/2505.11661v1", "updated": "2025-05-16 19:52:36", "published": "2025-05-16 19:52:36", "authors": "Zihan Ye;Oleg Arenz;Kristian Kersting", "summary": "When tackling complex problems, humans naturally break them down into\nsmaller, manageable subtasks and adjust their initial plans based on\nobservations. For instance, if you want to make coffee at a friend's place, you\nmight initially plan to grab coffee beans, go to the coffee machine, and pour\nthem into the machine. Upon noticing that the machine is full, you would skip\nthe initial steps and proceed directly to brewing. In stark contrast, state of\nthe art reinforcement learners, such as Proximal Policy Optimization (PPO),\nlack such prior knowledge and therefore require significantly more training\nsteps to exhibit comparable adaptive behavior. Thus, a central research\nquestion arises: \\textit{How can we enable reinforcement learning (RL) agents\nto have similar ``human priors'', allowing the agent to learn with fewer\ntraining interactions?} To address this challenge, we propose differentiable\nsymbolic planner (Dylan), a novel framework that integrates symbolic planning\ninto Reinforcement Learning. Dylan serves as a reward model that dynamically\nshapes rewards by leveraging human priors, guiding agents through intermediate\nsubtasks, thus enabling more efficient exploration. Beyond reward shaping,\nDylan can work as a high level planner that composes primitive policies to\ngenerate new behaviors while avoiding common symbolic planner pitfalls such as\ninfinite execution loops. Our experimental evaluations demonstrate that Dylan\nsignificantly improves RL agents' performance and facilitates generalization to\nunseen tasks.", "comment": "conference paper, 9 pages", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.11661v1;http://arxiv.org/pdf/2505.11661v1", "pdf_url": "http://arxiv.org/pdf/2505.11661v1"}, {"title": "SGD-Mix: Enhancing Domain-Specific Image Classification with Label-Preserving Data Augmentation", "link": "https://arxiv.org/pdf/2505.11813", "details": "Y Dong, FY Su, JH Chiang - arXiv preprint arXiv:2505.11813, 2025", "abstract": "Data augmentation for domain-specific image classification tasks often struggles to simultaneously address diversity, faithfulness, and label clarity of generated data, leading to suboptimal performance in downstream tasks. While existing generative \u2026", "entry_id": "http://arxiv.org/abs/2505.11813v1", "updated": "2025-05-17 03:51:18", "published": "2025-05-17 03:51:18", "authors": "Yixuan Dong;Fang-Yi Su;Jung-Hsien Chiang", "summary": "Data augmentation for domain-specific image classification tasks often\nstruggles to simultaneously address diversity, faithfulness, and label clarity\nof generated data, leading to suboptimal performance in downstream tasks. While\nexisting generative diffusion model-based methods aim to enhance augmentation,\nthey fail to cohesively tackle these three critical aspects and often overlook\nintrinsic challenges of diffusion models, such as sensitivity to model\ncharacteristics and stochasticity under strong transformations. In this paper,\nwe propose a novel framework that explicitly integrates diversity,\nfaithfulness, and label clarity into the augmentation process. Our approach\nemploys saliency-guided mixing and a fine-tuned diffusion model to preserve\nforeground semantics, enrich background diversity, and ensure label\nconsistency, while mitigating diffusion model limitations. Extensive\nexperiments across fine-grained, long-tail, few-shot, and background robustness\ntasks demonstrate our method's superior performance over state-of-the-art\napproaches.", "comment": "11 pages, 6 figures, 6 tables", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.11813v1;http://arxiv.org/pdf/2505.11813v1", "pdf_url": "http://arxiv.org/pdf/2505.11813v1"}, {"title": "ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling", "link": "https://arxiv.org/pdf/2505.12890", "details": "E \u00d6zsoy, C Pellegrini, D Bani-Harouni, K Yuan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The real-world complexity of surgeries necessitates surgeons to have deep and holistic comprehension to ensure precision, safety, and effective interventions. Computational systems are required to have a similar level of comprehension within \u2026", "entry_id": "http://arxiv.org/abs/2505.12890v1", "updated": "2025-05-19 09:20:29", "published": "2025-05-19 09:20:29", "authors": "Ege \u00d6zsoy;Chantal Pellegrini;David Bani-Harouni;Kun Yuan;Matthias Keicher;Nassir Navab", "summary": "The real-world complexity of surgeries necessitates surgeons to have deep and\nholistic comprehension to ensure precision, safety, and effective\ninterventions. Computational systems are required to have a similar level of\ncomprehension within the operating room. Prior works, limited to single-task\nefforts like phase recognition or scene graph generation, lack scope and\ngeneralizability. In this work, we introduce ORQA, a novel OR question\nanswering benchmark and foundational multimodal model to advance OR\nintelligence. By unifying all four public OR datasets into a comprehensive\nbenchmark, we enable our approach to concurrently address a diverse range of OR\nchallenges. The proposed multimodal large language model fuses diverse OR\nsignals such as visual, auditory, and structured data, for a holistic modeling\nof the OR. Finally, we propose a novel, progressive knowledge distillation\nparadigm, to generate a family of models optimized for different speed and\nmemory requirements. We show the strong performance of ORQA on our proposed\nbenchmark, and its zero-shot generalization, paving the way for scalable,\nunified OR modeling and significantly advancing multimodal surgical\nintelligence. We will release our code and data upon acceptance.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.12890v1;http://arxiv.org/pdf/2505.12890v1", "pdf_url": "http://arxiv.org/pdf/2505.12890v1"}]
