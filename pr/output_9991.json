[{"title": "LHRS-Bot-Nova: Improved Multimodal Large Language Model for Remote Sensing Vision-Language Interpretation", "link": "https://arxiv.org/pdf/2411.09301", "details": "Z Li, D Muhtar, F Gu, X Zhang, P Xiao, G He, X Zhu - arXiv preprint arXiv:2411.09301, 2024", "abstract": "Automatically and rapidly understanding Earth's surface is fundamental to our grasp of the living environment and informed decision-making. This underscores the need for a unified system with comprehensive capabilities in analyzing Earth's surface to \u2026"}, {"title": "Dolphins: Multimodal Language Model for Driving", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-72995-9_23.pdf", "details": "C Xiao", "abstract": "The quest for fully autonomous vehicles (AVs) capable of navigating complex real- world scenarios with human-like understanding and responsiveness. In this paper, we introduce Dolphins, a novel visionlanguage model architected to imbibe human \u2026"}, {"title": "Efficient Transfer Learning for Video-language Foundation Models", "link": "https://arxiv.org/pdf/2411.11223", "details": "H Chen, Z Huang, Y Hong, Y Wang, Z Lyu, Z Xu, J Lan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained vision-language models provide a robust foundation for efficient transfer learning across various downstream tasks. In the field of video action recognition, mainstream approaches often introduce additional parameter modules to capture \u2026"}, {"title": "MvKeTR: Chest CT Report Generation with Multi-View Perception and Knowledge Enhancement", "link": "https://arxiv.org/pdf/2411.18309", "details": "X Deng, X He, Y Zhou, S Cai, C Cai, Z Chen - arXiv preprint arXiv:2411.18309, 2024", "abstract": "CT report generation (CTRG) aims to automatically generate diagnostic reports for 3D volumes, relieving clinicians' workload and improving patient care. Despite clinical value, existing works fail to effectively incorporate diagnostic information from \u2026"}, {"title": "Group Robust Best-of-K Decoding of Language Models for Pluralistic Alignment", "link": "https://openreview.net/pdf%3Fid%3DJI6j4NUGHv", "details": "S Yoon, W Bankes, S Son, A Petrovic, SS Ramesh\u2026 - Pluralistic Alignment Workshop at \u2026", "abstract": "The desirable behaviour of a chat agent can be described with multiple criteria, such as harmlessness, helpfulness, and conciseness, each of which can be scored by a reward model. While each user, or a group of users, may perceive each criterion with \u2026"}, {"title": "LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language Models", "link": "https://arxiv.org/pdf/2411.06839", "details": "R Yang, T Wu, J Wang, P Hu, N Wong, Y Yang - arXiv preprint arXiv:2411.06839, 2024", "abstract": "In this paper, we propose a novel LLM-Neo framework that efficiently transfers knowledge from a large language model (LLM) teacher to a compact student. Initially, we revisit the knowledge distillation (KD) and low-rank adaption (LoRA), and \u2026"}, {"title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization", "link": "https://arxiv.org/pdf/2411.10442", "details": "W Wang, Z Chen, W Wang, Y Cao, Y Liu, Z Gao, J Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing open-source multimodal large language models (MLLMs) generally follow a training process involving pre-training and supervised fine-tuning. However, these models suffer from distribution shifts, which limit their multimodal reasoning \u2026"}, {"title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices", "link": "https://arxiv.org/pdf/2411.10640", "details": "X Lu, Y Chen, C Chen, H Tan, B Chen, Y Xie, R Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile \u2026"}, {"title": "Text as Images: Can Multimodal Large Language Models Follow Printed Instructions in Pixels?", "link": "https://openreview.net/pdf%3Fid%3DdC9kEMBchM", "details": "X Li, Y Lu, WY Wang, Y Choi - Adaptive Foundation Models: Evolving AI for \u2026, 2024", "abstract": "Recent multimodal large language models (MLLMs) have shown promising instruction following capabilities on vision-language tasks. In this work, we introduce VISUAL MODALITY INSTRUCTION (VIM) 1, and investigate how well multimodal \u2026"}]
