[{"title": "TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification", "link": "https://arxiv.org/pdf/2505.18283", "details": "J Wu, F Tang, Y Li, M Hu, H Xue, S Jameel, Y Xie\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances such as Chain-of-Thought prompting have significantly improved large language models (LLMs) in zero-shot medical reasoning. However, prompting- based methods often remain shallow and unstable, while fine-tuned medical LLMs \u2026", "entry_id": "http://arxiv.org/abs/2505.18283v1", "updated": "2025-05-23 18:28:59", "published": "2025-05-23 18:28:59", "authors": "Jianghao Wu;Feilong Tang;Yulong Li;Ming Hu;Haochen Xue;Shoaib Jameel;Yutong Xie;Imran Razzak", "summary": "Recent advances such as Chain-of-Thought prompting have significantly\nimproved large language models (LLMs) in zero-shot medical reasoning. However,\nprompting-based methods often remain shallow and unstable, while fine-tuned\nmedical LLMs suffer from poor generalization under distribution shifts and\nlimited adaptability to unseen clinical scenarios. To address these\nlimitations, we present TAGS, a test-time framework that combines a broadly\ncapable generalist with a domain-specific specialist to offer complementary\nperspectives without any model fine-tuning or parameter updates. To support\nthis generalist-specialist reasoning process, we introduce two auxiliary\nmodules: a hierarchical retrieval mechanism that provides multi-scale exemplars\nby selecting examples based on both semantic and rationale-level similarity,\nand a reliability scorer that evaluates reasoning consistency to guide final\nanswer aggregation. TAGS achieves strong performance across nine MedQA\nbenchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and\nimproving a vanilla 7B model from 14.1% to 23.9%. These results surpass several\nfine-tuned medical LLMs, without any parameter updates. The code will be\navailable at https://github.com/JianghaoWu/TAGS.", "comment": "16 pages including references, 2 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.MA;I.2.7", "links": "http://arxiv.org/abs/2505.18283v1;http://arxiv.org/pdf/2505.18283v1", "pdf_url": "http://arxiv.org/pdf/2505.18283v1"}, {"title": "Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models", "link": "https://arxiv.org/pdf/2505.19037", "details": "KH Lu, CY Kuan, H Lee - arXiv preprint arXiv:2505.19037, 2025", "abstract": "We introduce Speech-IFeval, an evaluation framework designed to assess instruction-following capabilities and quantify catastrophic forgetting in speech- aware language models (SLMs). Recent SLMs integrate speech perception with \u2026", "entry_id": "http://arxiv.org/abs/2505.19037v1", "updated": "2025-05-25 08:37:55", "published": "2025-05-25 08:37:55", "authors": "Ke-Han Lu;Chun-Yi Kuan;Hung-yi Lee", "summary": "We introduce Speech-IFeval, an evaluation framework designed to assess\ninstruction-following capabilities and quantify catastrophic forgetting in\nspeech-aware language models (SLMs). Recent SLMs integrate speech perception\nwith large language models (LLMs), often degrading textual capabilities due to\nspeech-centric training. Existing benchmarks conflate speech perception with\ninstruction-following, hindering evaluation of these distinct skills. To\naddress this gap, we provide a benchmark for diagnosing the\ninstruction-following abilities of SLMs. Our findings show that most SLMs\nstruggle with even basic instructions, performing far worse than text-based\nLLMs. Additionally, these models are highly sensitive to prompt variations,\noften yielding inconsistent and unreliable outputs. We highlight core\nchallenges and provide insights to guide future research, emphasizing the need\nfor evaluation beyond task-level metrics.", "comment": "Accecpted by Interspeech 2025;\n  https://github.com/kehanlu/Speech-IFEval", "journal_ref": null, "primary_category": "eess.AS", "categories": "eess.AS;cs.CL", "links": "http://arxiv.org/abs/2505.19037v1;http://arxiv.org/pdf/2505.19037v1", "pdf_url": "http://arxiv.org/pdf/2505.19037v1"}, {"title": "Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster", "link": "https://arxiv.org/pdf/2505.18642", "details": "X Chen, S Zhou, K Liang, X Sun, X Liu - arXiv preprint arXiv:2505.18642, 2025", "abstract": "Chain-of-thought (CoT) distillation allows a large language model (LLM) to guide a small language model (SLM) in reasoning tasks. Existing methods train the SLM to learn the long rationale in one iteration, resulting in two issues: 1) Long rationales \u2026", "entry_id": "http://arxiv.org/abs/2505.18642v1", "updated": "2025-05-24 11:04:52", "published": "2025-05-24 11:04:52", "authors": "Xiao Chen;Sihang Zhou;Ke Liang;Xiaoyu Sun;Xinwang Liu", "summary": "Chain-of-thought (CoT) distillation allows a large language model (LLM) to\nguide a small language model (SLM) in reasoning tasks. Existing methods train\nthe SLM to learn the long rationale in one iteration, resulting in two issues:\n1) Long rationales lead to a large token-level batch size during training,\nmaking gradients of core reasoning tokens (i.e., the token will directly affect\nthe correctness of subsequent reasoning) over-smoothed as they contribute a\ntiny fraction of the rationale. As a result, the SLM converges to sharp minima\nwhere it fails to grasp the reasoning logic. 2) The response is slow, as the\nSLM must generate a long rationale before reaching the answer. Therefore, we\npropose chunk-wise training (CWT), which uses a heuristic search to divide the\nrationale into internal semantically coherent chunks and focuses SLM on\nlearning from only one chunk per iteration. In this way, CWT naturally isolates\nnon-reasoning chunks that do not involve the core reasoning token (e.g.,\nsummary and transitional chunks) from the SLM learning for reasoning chunks,\nmaking the fraction of the core reasoning token increase in the corresponding\niteration. Based on CWT, skip-thinking training (STT) is proposed. STT makes\nthe SLM automatically skip non-reasoning medium chunks to reach the answer,\nimproving reasoning speed while maintaining accuracy. We validate our approach\non a variety of SLMs and multiple reasoning tasks.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.18642v1;http://arxiv.org/pdf/2505.18642v1", "pdf_url": "http://arxiv.org/pdf/2505.18642v1"}, {"title": "Multilingual Pretraining for Pixel Language Models", "link": "https://arxiv.org/pdf/2505.21265", "details": "I Kesen, JF Lotz, I Ziegler, P Rust, D Elliott - arXiv preprint arXiv:2505.21265, 2025", "abstract": "Pixel language models operate directly on images of rendered text, eliminating the need for a fixed vocabulary. While these models have demonstrated strong capabilities for downstream cross-lingual transfer, multilingual pretraining remains \u2026", "entry_id": "http://arxiv.org/abs/2505.21265v1", "updated": "2025-05-27 14:40:47", "published": "2025-05-27 14:40:47", "authors": "Ilker Kesen;Jonas F. Lotz;Ingo Ziegler;Phillip Rust;Desmond Elliott", "summary": "Pixel language models operate directly on images of rendered text,\neliminating the need for a fixed vocabulary. While these models have\ndemonstrated strong capabilities for downstream cross-lingual transfer,\nmultilingual pretraining remains underexplored. We introduce PIXEL-M4, a model\npretrained on four visually and linguistically diverse languages: English,\nHindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic\nand syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart\non non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4\ncaptures rich linguistic features, even in languages not seen during\npretraining. Furthermore, an analysis of its hidden representations shows that\nmultilingual pretraining yields a semantic embedding space closely aligned\nacross the languages used for pretraining. This work demonstrates that\nmultilingual pretraining substantially enhances the capability of pixel\nlanguage models to effectively support a diverse set of languages.", "comment": "17 pages, 19 figures, 7 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.21265v1;http://arxiv.org/pdf/2505.21265v1", "pdf_url": "http://arxiv.org/pdf/2505.21265v1"}, {"title": "Genome-Bench: A Scientific Reasoning Benchmark from Real-World Expert Discussions", "link": "https://arxiv.org/pdf/2505.19501", "details": "M Yin, Y Qu, D Liu, L Yang, L Cong, M Wang - arXiv preprint arXiv:2505.19501, 2025", "abstract": "In this short report, we present an automated pipeline tailored for the genomics domain and introduce\\textit {Genome-Bench}, a new benchmark constructed from over a decade of scientific forum discussions on genome engineering. Our pipeline \u2026", "entry_id": "http://arxiv.org/abs/2505.19501v1", "updated": "2025-05-26 04:28:46", "published": "2025-05-26 04:28:46", "authors": "Ming Yin;Yuanhao Qu;Dyllan Liu;Ling Yang;Le Cong;Mengdi Wang", "summary": "In this short report, we present an automated pipeline tailored for the\ngenomics domain and introduce \\textit{Genome-Bench}, a new benchmark\nconstructed from over a decade of scientific forum discussions on genome\nengineering. Our pipeline transforms raw interactions into a reinforcement\nlearning friendly multiple-choice questions format, supported by 3000+ high\nquality question answer pairs spanning foundational biology, experimental\ntroubleshooting, tool usage, and beyond. To our knowledge, this is the first\nend-to-end pipeline for teaching LLMs to reason from scientific discussions,\nwith promising potential for generalization across scientific domains beyond\nbiology.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.19501v1;http://arxiv.org/pdf/2505.19501v1", "pdf_url": "http://arxiv.org/pdf/2505.19501v1"}, {"title": "Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation", "link": "https://arxiv.org/pdf/2505.19529", "details": "TH Sakib, MT Hosain, MK Morol - arXiv preprint arXiv:2505.19529, 2025", "abstract": "Small Language Models (SLMs) have gained substantial attention due to their ability to execute diverse language tasks successfully while using fewer computer resources. These models are particularly ideal for deployment in limited \u2026", "entry_id": "http://arxiv.org/abs/2505.19529v2", "updated": "2025-05-29 16:57:36", "published": "2025-05-26 05:29:47", "authors": "Tanjil Hasan Sakib;Md. Tanzib Hosain;Md. Kishor Morol", "summary": "Small Language Models (SLMs) have gained substantial attention due to their\nability to execute diverse language tasks successfully while using fewer\ncomputer resources. These models are particularly ideal for deployment in\nlimited environments, such as mobile devices, on-device processing, and edge\nsystems. In this study, we present a complete assessment of SLMs, focussing on\ntheir design frameworks, training approaches, and techniques for lowering model\nsize and complexity. We offer a novel classification system to organize the\noptimization approaches applied for SLMs, encompassing strategies like pruning,\nquantization, and model compression. Furthermore, we assemble SLM's studies of\nevaluation suite with some existing datasets, establishing a rigorous platform\nfor measuring SLM capabilities. Alongside this, we discuss the important\ndifficulties that remain unresolved in this sector, including trade-offs\nbetween efficiency and performance, and we suggest directions for future study.\nWe anticipate this study to serve as a beneficial guide for researchers and\npractitioners who aim to construct compact, efficient, and high-performing\nlanguage models.", "comment": "9 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.19529v2;http://arxiv.org/pdf/2505.19529v2", "pdf_url": "http://arxiv.org/pdf/2505.19529v2"}, {"title": "Identifying signs and symptoms of pancreatic cancer: a population-based study using electronic health records and natural language processing", "link": "https://www.sciencedirect.com/science/article/pii/S1424390325000900", "details": "W Chen, F Xie, TQ Luong, J Chang, E Lustigova\u2026 - Pancreatology, 2025", "abstract": "ABSTRACT OBJECTIVES Patient-reported symptoms are often the trigger for diagnosis of pancreatic cancer (PC). We aimed to report the prevalence and lead time of symptoms of PC from electronic health records in a population-based sample \u2026"}, {"title": "Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models", "link": "https://arxiv.org/pdf/2505.18673", "details": "Z Xu, Y Wang, Y Huang, X Chen, J Zhao, M Jiang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have achieved remarkable success in Natural Language Processing (NLP), yet their cross-lingual performance consistency remains a significant challenge. This paper introduces a novel methodology for \u2026", "entry_id": "http://arxiv.org/abs/2505.18673v1", "updated": "2025-05-24 12:31:27", "published": "2025-05-24 12:31:27", "authors": "Zixiang Xu;Yanbo Wang;Yue Huang;Xiuying Chen;Jieyu Zhao;Meng Jiang;Xiangliang Zhang", "summary": "Large Language Models (LLMs) have achieved remarkable success in Natural\nLanguage Processing (NLP), yet their cross-lingual performance consistency\nremains a significant challenge. This paper introduces a novel methodology for\nefficiently identifying inherent cross-lingual weaknesses in LLMs. Our approach\nleverages beam search and LLM-based simulation to generate bilingual question\npairs that expose performance discrepancies between English and target\nlanguages. We construct a new dataset of over 6,000 bilingual pairs across 16\nlanguages using this methodology, demonstrating its effectiveness in revealing\nweaknesses even in state-of-the-art models. The extensive experiments\ndemonstrate that our method precisely and cost-effectively pinpoints\ncross-lingual weaknesses, consistently revealing over 50\\% accuracy drops in\ntarget languages across a wide range of models. Moreover, further experiments\ninvestigate the relationship between linguistic similarity and cross-lingual\nweaknesses, revealing that linguistically related languages share similar\nperformance patterns and benefit from targeted post-training. Code is available\nat https://github.com/xzx34/Cross-Lingual-Pitfalls.", "comment": "ACL 2025. Code available at\n  https://github.com/xzx34/Cross-Lingual-Pitfalls", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.18673v1;http://arxiv.org/pdf/2505.18673v1", "pdf_url": "http://arxiv.org/pdf/2505.18673v1"}, {"title": "Using a Healthcare Process Modeling Approach to Understand Electronic Health Records-based Pressure Injury Data and to Support Development of a Standardized \u2026", "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12099414/", "details": "L Liu, MJ Kang, M Sainlaire, G Lowenthal, T Martel\u2026 - AMIA Annual Symposium \u2026, 2025", "abstract": "The complexity of health care processes present significant challenges for using Electronic Health Records (EHR) data to build high fidelity phenotypes. This study leverages a healthcare process modeling (HPM) approach to enable understanding \u2026"}]
