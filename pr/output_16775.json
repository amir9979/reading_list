[{"title": "An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents", "link": "https://arxiv.org/pdf/2505.15117", "details": "B Jin, J Yoon, P Kargupta, SO Arik, J Han - arXiv preprint arXiv:2505.15117, 2025", "abstract": "Reinforcement learning (RL) has demonstrated strong potential in training large language models (LLMs) capable of complex reasoning for real-world problem solving. More recently, RL has been leveraged to create sophisticated LLM-based \u2026", "entry_id": "http://arxiv.org/abs/2505.15117v1", "updated": "2025-05-21 05:09:43", "published": "2025-05-21 05:09:43", "authors": "Bowen Jin;Jinsung Yoon;Priyanka Kargupta;Sercan O. Arik;Jiawei Han", "summary": "Reinforcement learning (RL) has demonstrated strong potential in training\nlarge language models (LLMs) capable of complex reasoning for real-world\nproblem solving. More recently, RL has been leveraged to create sophisticated\nLLM-based search agents that adeptly combine reasoning with search engine use.\nWhile the use of RL for training search agents is promising, the optimal design\nof such agents remains not fully understood. In particular, key factors -- such\nas (1) reward formulation, (2) the choice and characteristics of the underlying\nLLM, and (3) the role of the search engine in the RL process -- require further\ninvestigation. In this work, we conduct comprehensive empirical studies to\nsystematically investigate these and offer actionable insights. We highlight\nseveral key findings: format rewards are effective in improving final\nperformance, whereas intermediate retrieval rewards have limited impact; the\nscale and initialization of the LLM (general-purpose vs. reasoning-specialized)\nsignificantly influence RL outcomes; and the choice of search engine plays a\ncritical role in shaping RL training dynamics and the robustness of the trained\nagent during inference. These establish important guidelines for successfully\nbuilding and deploying LLM-based search agents in real-world applications. Code\nis available at https://github.com/PeterGriffinJin/Search-R1.", "comment": "22 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.IR", "links": "http://arxiv.org/abs/2505.15117v1;http://arxiv.org/pdf/2505.15117v1", "pdf_url": "http://arxiv.org/pdf/2505.15117v1"}, {"title": "Social Bias in Popular Question-Answering Benchmarks", "link": "https://arxiv.org/pdf/2505.15553", "details": "A Kraft, J Simon, S Schimmler - arXiv preprint arXiv:2505.15553, 2025", "abstract": "Question-answering (QA) and reading comprehension (RC) benchmarks are essential for assessing the capabilities of large language models (LLMs) in retrieving and reproducing knowledge. However, we demonstrate that popular QA and RC \u2026", "entry_id": "http://arxiv.org/abs/2505.15553v2", "updated": "2025-05-22 09:39:49", "published": "2025-05-21 14:14:47", "authors": "Angelie Kraft;Judith Simon;Sonja Schimmler", "summary": "Question-answering (QA) and reading comprehension (RC) benchmarks are\nessential for assessing the capabilities of large language models (LLMs) in\nretrieving and reproducing knowledge. However, we demonstrate that popular QA\nand RC benchmarks are biased and do not cover questions about different\ndemographics or regions in a representative way, potentially due to a lack of\ndiversity of those involved in their creation. We perform a qualitative content\nanalysis of 30 benchmark papers and a quantitative analysis of 20 respective\nbenchmark datasets to learn (1) who is involved in the benchmark creation, (2)\nhow social bias is addressed or prevented, and (3) whether the demographics of\nthe creators and annotators correspond to particular biases in the content.\nMost analyzed benchmark papers provided insufficient information regarding the\nstakeholders involved in benchmark creation, particularly the annotators.\nNotably, just one of the benchmark papers explicitly reported measures taken to\naddress social representation issues. Moreover, the data analysis revealed\ngender, religion, and geographic biases across a wide range of encyclopedic,\ncommonsense, and scholarly benchmarks. More transparent and bias-aware QA and\nRC benchmark creation practices are needed to facilitate better scrutiny and\nincentivize the development of fairer LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.CY", "links": "http://arxiv.org/abs/2505.15553v2;http://arxiv.org/pdf/2505.15553v2", "pdf_url": "http://arxiv.org/pdf/2505.15553v2"}, {"title": "Ranked Voting based Self-Consistency of Large Language Models", "link": "https://arxiv.org/pdf/2505.10772", "details": "W Wang, Y Wang, H Huang - arXiv preprint arXiv:2505.10772, 2025", "abstract": "Majority voting is considered an effective method to enhance chain-of-thought reasoning, as it selects the answer with the highest\" self-consistency\" among different reasoning paths (Wang et al., 2023). However, previous chain-of-thought \u2026", "entry_id": "http://arxiv.org/abs/2505.10772v1", "updated": "2025-05-16 01:09:43", "published": "2025-05-16 01:09:43", "authors": "Weiqin Wang;Yile Wang;Hui Huang", "summary": "Majority voting is considered an effective method to enhance chain-of-thought\nreasoning, as it selects the answer with the highest \"self-consistency\" among\ndifferent reasoning paths (Wang et al., 2023). However, previous\nchain-of-thought reasoning methods typically generate only a single answer in\neach trial, thereby ignoring the possibility of other potential answers. As a\nresult, these alternative answers are often overlooked in subsequent voting\nprocesses. In this work, we propose to generate ranked answers in each\nreasoning process and conduct ranked voting among multiple ranked answers from\ndifferent responses, thereby making the overall self-consistency more reliable.\nSpecifically, we use three ranked voting methods: Instant-runoff voting, Borda\ncount voting, and mean reciprocal rank voting. We validate our methods on six\ndatasets, including three multiple-choice and three open-ended\nquestion-answering tasks, using both advanced open-source and closed-source\nlarge language models. Extensive experimental results indicate that our\nproposed method outperforms the baselines, showcasing the potential of\nleveraging the information of ranked answers and using ranked voting to improve\nreasoning performance. The code is available at\nhttps://github.com/szu-tera/RankedVotingSC.", "comment": "ACL 2025 Findings", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.10772v1;http://arxiv.org/pdf/2505.10772v1", "pdf_url": "http://arxiv.org/pdf/2505.10772v1"}, {"title": "Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective", "link": "https://arxiv.org/pdf/2505.15045", "details": "S Zhang, Y Zhao, L Geng, A Cohan, AT Luu, C Zhao - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language model (LLM)-based embedding models, benefiting from large scale pre-training and post-training, have begun to surpass BERT and T5-based models on general-purpose text embedding tasks such as document retrieval. However, a \u2026", "entry_id": "http://arxiv.org/abs/2505.15045v1", "updated": "2025-05-21 02:59:14", "published": "2025-05-21 02:59:14", "authors": "Siyue Zhang;Yilun Zhao;Liyuan Geng;Arman Cohan;Anh Tuan Luu;Chen Zhao", "summary": "Large language model (LLM)-based embedding models, benefiting from large\nscale pre-training and post-training, have begun to surpass BERT and T5-based\nmodels on general-purpose text embedding tasks such as document retrieval.\nHowever, a fundamental limitation of LLM embeddings lies in the unidirectional\nattention used during autoregressive pre-training, which misaligns with the\nbidirectional nature of text embedding tasks. To this end, We propose adopting\ndiffusion language models for text embeddings, motivated by their inherent\nbidirectional architecture and recent success in matching or surpassing LLMs\nespecially on reasoning tasks. We present the first systematic study of the\ndiffusion language embedding model, which outperforms the LLM-based embedding\nmodel by 20% on long-document retrieval, 8% on reasoning-intensive retrieval,\n2% on instruction-following retrieval, and achieve competitive performance on\ntraditional text embedding benchmarks. Our analysis verifies that bidirectional\nattention is crucial for encoding global context in long and complex text.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15045v1;http://arxiv.org/pdf/2505.15045v1", "pdf_url": "http://arxiv.org/pdf/2505.15045v1"}, {"title": "dKV-Cache: The Cache for Diffusion Language Models", "link": "https://arxiv.org/pdf/2505.15781", "details": "X Ma, R Yu, G Fang, X Wang - arXiv preprint arXiv:2505.15781, 2025", "abstract": "Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non \u2026", "entry_id": "http://arxiv.org/abs/2505.15781v1", "updated": "2025-05-21 17:32:10", "published": "2025-05-21 17:32:10", "authors": "Xinyin Ma;Runpeng Yu;Gongfan Fang;Xinchao Wang", "summary": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs.", "comment": "The code is available at https://github.com/horseee/dKV-Cache", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15781v1;http://arxiv.org/pdf/2505.15781v1", "pdf_url": "http://arxiv.org/pdf/2505.15781v1"}, {"title": "Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models", "link": "https://arxiv.org/pdf/2505.09062", "details": "J Zhao, Y Song, E Cohen - Journal of Systems and Software, 2025", "abstract": "Recent advancements in source code summarization have leveraged transformer- based pre-trained models, including Large Language Models of Code (LLMCs), to automate and improve the generation of code summaries. However, existing \u2026", "entry_id": "http://arxiv.org/abs/2505.09062v1", "updated": "2025-05-14 01:46:56", "published": "2025-05-14 01:46:56", "authors": "Junda Zhao;Yuliang Song;Eldan Cohen", "summary": "Recent advancements in source code summarization have leveraged\ntransformer-based pre-trained models, including Large Language Models of Code\n(LLMCs), to automate and improve the generation of code summaries. However,\nexisting methods often focus on generating a single high-quality summary for a\ngiven source code, neglecting scenarios where the generated summary might be\ninadequate and alternative options are needed. In this paper, we introduce\nVariational Prefix Tuning (VPT), a novel approach that enhances pre-trained\nmodels' ability to generate diverse yet accurate sets of summaries, allowing\nthe user to choose the most suitable one for the given source code. Our method\nintegrates a Conditional Variational Autoencoder (CVAE) framework as a modular\ncomponent into pre-trained models, enabling us to model the distribution of\nobserved target summaries and sample continuous embeddings to be used as\nprefixes to steer the generation of diverse outputs during decoding.\nImportantly, we construct our method in a parameter-efficient manner,\neliminating the need for expensive model retraining, especially when using\nLLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset\nof generated summaries, optimizing both the diversity and the accuracy of the\noptions presented to users. We present extensive experimental evaluations using\nwidely used datasets and current state-of-the-art pre-trained code\nsummarization models to demonstrate the effectiveness of our approach and its\nadaptability across models.", "comment": "Accepted by the Journal of Systems and Software", "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE;cs.AI;cs.LG;D.2.7", "links": "http://dx.doi.org/10.1016/j.jss.2025.112493;http://arxiv.org/abs/2505.09062v1;http://arxiv.org/pdf/2505.09062v1", "pdf_url": "http://arxiv.org/pdf/2505.09062v1"}, {"title": "Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment", "link": "https://arxiv.org/pdf/2505.15456", "details": "W Zhao, X Sui, Y Hu, J Guo, H Liu, B Li, Y Zhao, B Qin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Personalized alignment is essential for enabling large language models (LLMs) to engage effectively in user-centric dialogue. While recent prompt-based and offline optimization methods offer preliminary solutions, they fall short in cold-start scenarios \u2026", "entry_id": "http://arxiv.org/abs/2505.15456v1", "updated": "2025-05-21 12:38:36", "published": "2025-05-21 12:38:36", "authors": "Weixiang Zhao;Xingyu Sui;Yulin Hu;Jiahe Guo;Haixiao Liu;Biye Li;Yanyan Zhao;Bing Qin;Ting Liu", "summary": "Personalized alignment is essential for enabling large language models (LLMs)\nto engage effectively in user-centric dialogue. While recent prompt-based and\noffline optimization methods offer preliminary solutions, they fall short in\ncold-start scenarios and long-term personalization due to their inherently\nstatic and shallow designs. In this work, we introduce the Reinforcement\nLearning for Personalized Alignment (RLPA) framework, in which an LLM interacts\nwith a simulated user model to iteratively infer and refine user profiles\nthrough dialogue. The training process is guided by a dual-level reward\nstructure: the Profile Reward encourages accurate construction of user\nrepresentations, while the Response Reward incentivizes generation of responses\nconsistent with the inferred profile. We instantiate RLPA by fine-tuning\nQwen-2.5-3B-Instruct, resulting in Qwen-RLPA, which achieves state-of-the-art\nperformance in personalized dialogue. Empirical evaluations demonstrate that\nQwen-RLPA consistently outperforms prompting and offline fine-tuning baselines,\nand even surpasses advanced commercial models such as Claude-3.5 and GPT-4o.\nFurther analysis highlights Qwen-RLPA's robustness in reconciling conflicting\nuser preferences, sustaining long-term personalization and delivering more\nefficient inference compared to recent reasoning-focused LLMs. These results\nemphasize the potential of dynamic profile inference as a more effective\nparadigm for building personalized dialogue systems.", "comment": "30 pages, 18 figures, 10 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15456v1;http://arxiv.org/pdf/2505.15456v1", "pdf_url": "http://arxiv.org/pdf/2505.15456v1"}, {"title": "When and How to Augment Your Input: Question Routing Helps Balance the Accuracy and Efficiency of Large Language Models", "link": "https://aclanthology.org/2025.findings-naacl.200.pdf", "details": "S Chen, H Zheng, L Cui - Findings of the Association for Computational \u2026, 2025", "abstract": "Although large language models rely on parametric knowledge to achieve exceptional performance across various question-answering tasks, they still face challenges when addressing knowledge-based long-tail questions. Augmented \u2026"}, {"title": "Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack", "link": "https://arxiv.org/pdf/2505.15323", "details": "S Cappelletti, T Poppi, S Poppi, ZX Yong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) are increasingly evaluated on multiple-choice question answering (MCQA) tasks using* first-token probability*(FTP), which selects the answer option whose initial token has the highest likelihood. While efficient, FTP \u2026", "entry_id": "http://arxiv.org/abs/2505.15323v1", "updated": "2025-05-21 09:58:38", "published": "2025-05-21 09:58:38", "authors": "Silvia Cappelletti;Tobia Poppi;Samuele Poppi;Zheng-Xin Yong;Diego Garcia-Olano;Marcella Cornia;Lorenzo Baraldi;Rita Cucchiara", "summary": "Large Language Models (LLMs) are increasingly evaluated on multiple-choice\nquestion answering (MCQA) tasks using *first-token probability* (FTP), which\nselects the answer option whose initial token has the highest likelihood. While\nefficient, FTP can be fragile: models may assign high probability to unrelated\ntokens (*misalignment*) or use a valid token merely as part of a generic\npreamble rather than as a clear answer choice (*misinterpretation*),\nundermining the reliability of symbolic evaluation. We propose a simple\nsolution: the *prefilling attack*, a structured natural-language prefix (e.g.,\n\"*The correct option is:*\") prepended to the model output. Originally explored\nin AI safety, we repurpose prefilling to steer the model to respond with a\nclean, valid option, without modifying its parameters. Empirically, the FTP\nwith prefilling strategy substantially improves accuracy, calibration, and\noutput consistency across a broad set of LLMs and MCQA benchmarks. It\noutperforms standard FTP and often matches the performance of open-ended\ngeneration approaches that require full decoding and external classifiers,\nwhile being significantly more efficient. Our findings suggest that prefilling\nis a simple, robust, and low-cost method to enhance the reliability of\nFTP-based evaluation in multiple-choice settings.", "comment": "13 pages, 5 figures, 7 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15323v1;http://arxiv.org/pdf/2505.15323v1", "pdf_url": "http://arxiv.org/pdf/2505.15323v1"}]
