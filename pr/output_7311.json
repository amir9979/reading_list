[{"title": "Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language Models", "link": "https://arxiv.org/pdf/2409.01584", "details": "S Ozaki, K Hayashi, Y Sakai, H Kamigaito, K Hayashi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As the performance of Large-scale Vision Language Models (LVLMs) improves, they are increasingly capable of responding in multiple languages, and there is an expectation that the demand for explanations generated by LVLMs will grow \u2026"}, {"title": "A Capture-Recapture Approach to Facilitate Causal Inference for a Trial-eligible Observational Cohort", "link": "https://arxiv.org/pdf/2409.18358", "details": "L Ge, Y Zhang, LA Waller, RH Lyles - arXiv preprint arXiv:2409.18358, 2024", "abstract": "Background: We extend recently proposed design-based capture-recapture methods for prevalence estimation among registry participants, in order to support causal inference among a trial-eligible target population. The proposed design for CRC \u2026"}, {"title": "Towards a Unified View of Preference Learning for Large Language Models: A Survey", "link": "https://arxiv.org/pdf/2409.02795", "details": "B Gao, F Song, Y Miao, Z Cai, Z Yang, L Chen, H Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of the crucial factors to achieve success is aligning the LLM's output with human preferences. This alignment process often requires only a small amount of data to \u2026"}, {"title": "CRADLE-VAE: Enhancing Single-Cell Gene Perturbation Modeling with Counterfactual Reasoning-based Artifact Disentanglement", "link": "https://arxiv.org/pdf/2409.05484", "details": "S Baek, S Park, YT Chok, J Lee, J Park, M Gim, J Kang - arXiv preprint arXiv \u2026, 2024", "abstract": "Predicting cellular responses to various perturbations is a critical focus in drug discovery and personalized therapeutics, with deep learning models playing a significant role in this endeavor. Single-cell datasets contain technical artifacts that \u2026"}, {"title": "\" Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree\": Zero-Shot Decision Tree Induction and Embedding with Large Language Models", "link": "https://arxiv.org/pdf/2409.18594", "details": "R Knauer, M Koddenbrock, R Wallsberger, NM Brisson\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) provide powerful means to leverage prior knowledge for predictive modeling when data is limited. In this work, we demonstrate how LLMs can use their compressed world knowledge to generate intrinsically interpretable \u2026"}, {"title": "A Survey on the Honesty of Large Language Models", "link": "https://arxiv.org/pdf/2409.18786", "details": "S Li, C Yang, T Wu, C Shi, Y Zhang, X Zhu, Z Cheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Honesty is a fundamental principle for aligning large language models (LLMs) with human values, requiring these models to recognize what they know and don't know and be able to faithfully express their knowledge. Despite promising, current LLMs \u2026"}, {"title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization", "link": "https://dl.acm.org/doi/pdf/10.1145/3670474.3685966", "details": "P Vijayaraghavan, A Nitsure, C Mackin, L Shi\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Large Language Models (LLMs) have become widely used across diverse NLP tasks and domains, demonstrating their adaptability and effectiveness. In the realm of Electronic Design Automation (EDA), LLMs show promise for tasks like Register \u2026"}]
