'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Simple linear attention language models balance the r'
[{"title": "Grounding Language Models for Visual Entity Recognition", "link": "https://arxiv.org/pdf/2402.18695", "details": "Z Xiao, M Gong, P Cascante-Bonilla, X Zhang, J Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of \u2026"}, {"title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.07860", "details": "S Zhao, S Hao, B Zi, H Xu, KYK Wong - arXiv preprint arXiv:2403.07860, 2024", "abstract": "Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding \u2026"}, {"title": "Synth $^ 2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings", "link": "https://arxiv.org/pdf/2403.07750", "details": "S Sharifzadeh, C Kaplanis, S Pathak, D Kumaran, A Ilic\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models \u2026"}, {"title": "$\\mathbf {(N, K)} $-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model", "link": "https://arxiv.org/html/2403.07191v1", "details": "Y Zhang, L Chen, B Liu, Y Yang, Q Cui, Y Tao, H Yang - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these \u2026"}, {"title": "Multi-modal Attribute Prompting for Vision-Language Models", "link": "https://arxiv.org/pdf/2403.00219", "details": "X Liu, J Wu, T Zhang - arXiv preprint arXiv:2403.00219, 2024", "abstract": "Large pre-trained Vision-Language Models (VLMs), like CLIP, exhibit strong generalization ability to downstream tasks but struggle in few-shot scenarios. Existing prompting techniques primarily focus on global text and image \u2026"}, {"title": "ZVQAF: Zero-shot visual question answering with feedback from large language models", "link": "https://www.sciencedirect.com/science/article/pii/S0925231224002765", "details": "C Liu, C Wang, Y Peng, Z Li - Neurocomputing, 2024", "abstract": "Due to the prominent zero-shot generalization in new language tasks shown by large language models (LLMs), applying LLMs for zero-shot visual question answering (VQA) has been a new trend. However, most prior approaches directly use off-the \u2026"}, {"title": "Dynamic Task-Oriented Dialogue: A Comparative Study of Llama-2 and BERT in Slot Value Generation", "link": "https://aclanthology.org/2024.eacl-srw.pdf%23page%3D368", "details": "B Magnini - The 18th Conference of the European Chapter of the \u2026, 2024", "abstract": "Recent advancements in instruction-based language models have demonstrated exceptional performance across various natural language processing tasks. We present a comprehensive analysis of the performance of two open-source language \u2026"}, {"title": "Caformer: Rethinking Time Series Analysis from Causal Perspective", "link": "https://arxiv.org/html/2403.08572v1", "details": "K Zhang, X Zou, Y Tang - arXiv preprint arXiv:2403.08572, 2024", "abstract": "Time series analysis is a vital task with broad applications in various domains. However, effectively capturing cross-dimension and cross-time dependencies in non- stationary time series poses significant challenges, particularly in the context of \u2026"}, {"title": "ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic", "link": "https://arxiv.org/pdf/2402.12840", "details": "F Koto, H Li, S Shatnawi, J Doughman, AB Sadallah\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The focus of language model evaluation has transitioned towards reasoning and knowledge-intensive tasks, driven by advancements in pretraining large models. While state-of-the-art models are partially trained on large Arabic texts, evaluating \u2026"}]
