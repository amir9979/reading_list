[{"title": "Large-vocabulary forensic pathological analyses via prototypical cross-modal contrastive learning", "link": "https://arxiv.org/pdf/2407.14904", "details": "C Shen, C Lian, W Zhang, F Wang, J Zhang, S Fan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Forensic pathology is critical in determining the cause and manner of death through post-mortem examinations, both macroscopic and microscopic. The field, however, grapples with issues such as outcome variability, laborious processes, and a scarcity \u2026"}, {"title": "Semantic Compositions Enhance Vision-Language Contrastive Learning", "link": "https://arxiv.org/pdf/2407.01408", "details": "M Aladago, L Torresani, S Vosoughi - arXiv preprint arXiv:2407.01408, 2024", "abstract": "In the field of vision-language contrastive learning, models such as CLIP capitalize on matched image-caption pairs as positive examples and leverage within-batch non- matching pairs as negatives. This approach has led to remarkable outcomes in zero \u2026"}, {"title": "Meta-GPS++: Enhancing Graph Meta-Learning with Contrastive Learning and Self-Training", "link": "https://arxiv.org/pdf/2407.14732", "details": "Y Liu, M Li, X Li, L Huang, F Giunchiglia, Y Liang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Node classification is an essential problem in graph learning. However, many models typically obtain unsatisfactory performance when applied to few-shot scenarios. Some studies have attempted to combine meta-learning with graph neural \u2026"}, {"title": "Open-world electrocardiogram classification via domain knowledge-driven contrastive learning", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004751", "details": "S Zhou, X Huang, N Liu, W Zhang, YT Zhang\u2026 - Neural Networks, 2024", "abstract": "Automatic electrocardiogram (ECG) classification provides valuable auxiliary information for assisting disease diagnosis and has received much attention in research. The success of existing classification models relies on fitting the labeled \u2026"}, {"title": "Report-Concept Textual-Prompt Learning for Enhancing X-ray Diagnosis", "link": "https://openreview.net/pdf%3Fid%3Dfgy59cM8X6", "details": "X Zhao, ZY Liu, F Liu, G Li, Y Dou, S Peng - ACM Multimedia 2024", "abstract": "Despite significant advances in image-text medical visual language modeling, the high cost of fine-grained annotation of images to align radiology reports has led current approaches to focus primarily on semantic alignment between the image and \u2026"}, {"title": "Debiased Contrastive Learning With Supervision Guidance for Industrial Fault Detection", "link": "https://ieeexplore.ieee.org/abstract/document/10600455/", "details": "R Cai, W Gao, L Peng, Z Lu, K Zhang, Y Liu - IEEE Transactions on Industrial \u2026, 2024", "abstract": "The time series self-supervised contrastive learning framework has succeeded significantly in industrial fault detection scenarios. It typically consists of pretraining on abundant unlabeled data and fine-tuning on limited annotated data. However, the \u2026"}, {"title": "Multi-Label Chest X-Ray Image Classification with Single Positive Labels", "link": "https://ieeexplore.ieee.org/abstract/document/10579876/", "details": "J Xiao, S Li, T Lin, J Zhu, X Yuan, DD Feng, B Sheng - IEEE Transactions on Medical \u2026, 2024", "abstract": "Deep learning approaches for multi-label Chest X-ray (CXR) images classification usually require large-scale datasets. However, acquiring such datasets with full annotations is costly, time-consuming, and prone to noisy labels. Therefore, we \u2026"}, {"title": "Multi-Label Generalized Zero Shot Chest Xray Classification By Combining Image-Text Information With Feature Disentanglement", "link": "https://ieeexplore.ieee.org/iel8/42/4359023/10601163.pdf", "details": "D Mahapatra, AJ Yepes, B Bozorgtabar, S Roy, Z Ge\u2026 - IEEE Transactions on \u2026, 2024", "abstract": "In fully supervised learning-based medical image classification, the robustness of a trained model is influenced by its exposure to the range of candidate disease classes. Generalized Zero Shot Learning (GZSL) aims to correctly predict seen and \u2026"}, {"title": "Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language Models", "link": "https://arxiv.org/pdf/2407.07263", "details": "J Parmar, S Satheesh, M Patwary, M Shoeybi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As language models have scaled both their number of parameters and pretraining dataset sizes, the computational cost for pretraining has become intractable except for the most well-resourced teams. This increasing cost makes it ever more important \u2026"}]
