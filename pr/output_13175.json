[{"title": "Chest X-ray Foundation Model with Global and Local Representations Integration", "link": "https://arxiv.org/pdf/2502.05142", "details": "Z Yang, X Xu, J Zhang, G Wang, MK Kalra, P Yan - arXiv preprint arXiv:2502.05142, 2025", "abstract": "Chest X-ray (CXR) is the most frequently ordered imaging test, supporting diverse clinical tasks from thoracic disease detection to postoperative monitoring. However, task-specific classification models are limited in scope, require costly labeled data \u2026"}, {"title": "Do we really have to filter out random noise in pre-training data for language models?", "link": "https://arxiv.org/pdf/2502.06604", "details": "J Ru, Y Xie, X Zhuang, Y Yin, Y Zou - arXiv preprint arXiv:2502.06604, 2025", "abstract": "Web-scale pre-training datasets are the cornerstone of LLMs' success. However, text data curated from the internet inevitably contains random noise caused by decoding errors or unregulated web content. In contrast to previous works that focus on low \u2026"}, {"title": "Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement", "link": "https://arxiv.org/pdf/2502.02573%3F", "details": "S Abbasloo - arXiv preprint arXiv:2502.02573, 2025", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem- solving, a crucial, ubiquitous, and complex domain. This paper explores the \u2026"}, {"title": "MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders", "link": "https://arxiv.org/pdf/2502.14753", "details": "M Varma, A Kumar, R van der Sluijs, S Ostmeier\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. Consequently, training deep learning models on medical images can incur large computational \u2026"}, {"title": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment", "link": "https://arxiv.org/pdf/2502.04328%3F", "details": "Z Liu, Y Dong, J Wang, Z Liu, W Hu, J Lu, Y Rao - arXiv preprint arXiv:2502.04328, 2025", "abstract": "Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have \u2026"}, {"title": "Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective", "link": "https://arxiv.org/pdf/2502.01524%3F", "details": "X Ma, H Xie, SJ Qin - arXiv preprint arXiv:2502.01524, 2025", "abstract": "The integration of vision-language modalities has been a significant focus in multimodal learning, traditionally relying on Vision-Language Pretrained Models. However, with the advent of Large Language Models (LLMs), there has been a \u2026"}, {"title": "Aligning, Autoencoding and Prompting Large Language Models for Novel Disease Reporting", "link": "https://drive.google.com/file/d/1Q8y9iQA3aw_4u98YkgaTas7I9IGfkEJM/view", "details": "F Liu, X Wu, J Huang, B Yang, K Branson, P Schwab\u2026 - IEEE Transactions on \u2026, 2025", "abstract": "Given radiology images, automatic radiology report generation aims to produce informative text that reports diseases. It can benefit current clinical practice in diagnostic radiology. Existing methods typically rely on large-scale medical datasets \u2026"}, {"title": "Large language models for error detection in radiology reports: a comparative analysis between closed-source and privacy-compliant open-source models", "link": "https://link.springer.com/article/10.1007/s00330-025-11438-y", "details": "B Salam, C St\u00fcwe, S Nowak, AM Sprinkart, M Theis\u2026 - European Radiology, 2025", "abstract": "Abstract Purpose Large language models (LLMs) like Generative Pre-trained Transformer 4 (GPT-4) can assist in detecting errors in radiology reports, but privacy concerns limit their clinical applicability. This study compares closed-source and \u2026"}, {"title": "Leveraging large language models for structured information extraction from pathology reports", "link": "https://arxiv.org/pdf/2502.12183", "details": "JB Balasubramanian, D Adams, I Roxanis\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Background: Structured information extraction from unstructured histopathology reports facilitates data accessibility for clinical research. Manual extraction by experts is time-consuming and expensive, limiting scalability. Large language models \u2026"}]
