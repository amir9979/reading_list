[{"title": "Inference Calibration of Vision-Language Foundation Models for Zero-Shot and Few-Shot Learning", "link": "https://www.sciencedirect.com/science/article/pii/S0167865525000959", "details": "M Hu, H Chang, S Shan, X Chen - Pattern Recognition Letters, 2025", "abstract": "Abstract Contrastive Language-Image Pre-training (CLIP) models exhibit impressive zero-shot performance across various downstream cross-modal tasks by simply computing the dot product between image and text features. CLIP is pre-trained on \u2026"}, {"title": "Towards Inpatient Discharge Summary Automation via Large Language Models: A Multidimensional Evaluation with a HIPAA-Compliant Instance of GPT-4o and \u2026", "link": "https://www.medrxiv.org/content/10.1101/2025.04.03.25325204.full.pdf", "details": "TG Osborne, SM Abbasi, SM Hong, RT Sexton\u2026 - medRxiv, 2025", "abstract": "Large language models (LLMs) have demonstrated potential to automate clinical documentation tasks that may reduce clinician burden, such as generation of hospital discharge summaries. Prior research used older LLMs and limited data \u2026"}]
