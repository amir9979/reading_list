[{"title": "CREAM: Consistency Regularized Self-Rewarding Language Models", "link": "https://arxiv.org/pdf/2410.12735%3F", "details": "Z Wang, W He, Z Liang, X Zhang, C Bansal, Y Wei\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent self-rewarding large language models (LLM) have successfully applied LLM- as-a-Judge to iteratively improve the alignment performance without the need of human annotations for preference data. These methods commonly utilize the same \u2026"}, {"title": "Fine-Tuning Pre-trained Language Models for Robust Causal Representation Learning", "link": "https://arxiv.org/pdf/2410.14375", "details": "J Yu, Y Zhou, Y He, NL Zhang, R Silva - arXiv preprint arXiv:2410.14375, 2024", "abstract": "The fine-tuning of pre-trained language models (PLMs) has been shown to be effective across various domains. By using domain-specific supervised data, the general-purpose representation derived from PLMs can be transformed into a \u2026"}, {"title": "Sleep apnea test prediction based on Electronic Health Records", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001552", "details": "LA Tahoun, AS Green, T Patalon, Y Dagan\u2026 - Journal of Biomedical \u2026, 2024", "abstract": "Abstract The identification of Obstructive Sleep Apnea (OSA) is done by a Polysomnography test which is often done in later ages. Being able to notify potential insured members at earlier ages is desirable. For that, we develop predictive models \u2026"}, {"title": "MiniPLM: Knowledge Distillation for Pre-Training Language Models", "link": "https://arxiv.org/pdf/2410.17215%3F", "details": "Y Gu, H Zhou, F Meng, J Zhou, M Huang - arXiv preprint arXiv:2410.17215, 2024", "abstract": "Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. While effective in fine-tuning, KD during pre-training faces challenges in efficiency, flexibility, and effectiveness \u2026"}, {"title": "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "link": "https://arxiv.org/pdf/2410.14897", "details": "M Roemmele, AS Gordon - arXiv preprint arXiv:2410.14897, 2024", "abstract": "LLMs can now perform a variety of complex writing tasks. They also excel in answering questions pertaining to natural language inference and commonsense reasoning. Composing these questions is itself a skilled writing task, so in this paper \u2026"}, {"title": "Enriching Tabular Data with Contextual LLM Embeddings: A Comprehensive Ablation Study for Ensemble Classifiers", "link": "https://arxiv.org/pdf/2411.01645", "details": "G Kasneci, E Kasneci - arXiv preprint arXiv:2411.01645, 2024", "abstract": "Feature engineering is crucial for optimizing machine learning model performance, particularly in tabular data classification tasks. Leveraging advancements in natural language processing, this study presents a systematic approach to enrich tabular \u2026"}, {"title": "Enhanced Prompt Learning for Few-shot Text Classification Method", "link": "http://crestapress.org/index.php/sidr/article/view/78", "details": "E Zio, M Rossi, E Garcia, YE Shuqin, Z Guangwei - Scientific Insights and Discoveries \u2026, 2024", "abstract": "An enhanced prompt learning method (EPL4FTC) for few-shot text classification task is proposed. This algorithm first converts the text classification task into the form of prompt learning based on natural language inference. Thus, the implicit data \u2026"}, {"title": "Safety-Aware Fine-Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2410.10014", "details": "HK Choi, X Du, Y Li - arXiv preprint arXiv:2410.10014, 2024", "abstract": "Fine-tuning Large Language Models (LLMs) has emerged as a common practice for tailoring models to individual needs and preferences. The choice of datasets for fine- tuning can be diverse, introducing safety concerns regarding the potential inclusion \u2026"}, {"title": "Sparsing Law: Towards Large Language Models with Greater Activation Sparsity", "link": "https://arxiv.org/pdf/2411.02335", "details": "Y Luo, C Song, X Han, Y Chen, C Xiao, Z Liu, M Sun - arXiv preprint arXiv \u2026, 2024", "abstract": "Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting \u2026"}]
