[{"title": "CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification", "link": "https://arxiv.org/pdf/2501.12266", "details": "C Patr\u00edcio, I Rio-Torto, JS Cardoso, LF Teixeira\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter \u2026"}, {"title": "Lightweight Weighted Average Ensemble Model for Pneumonia Detection in Chest X-Ray Images", "link": "https://arxiv.org/pdf/2501.16249", "details": "SB Nettur, S Karpurapu, U Nettur, LS Gajja, S Myneni\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Pneumonia is a leading cause of illness and death in children, underscoring the need for early and accurate detection. In this study, we propose a novel lightweight ensemble model for detecting pneumonia in children using chest X-ray images. This \u2026"}, {"title": "Few-Shot Adaptation of Training-Free Foundation Model for 3D Medical Image Segmentation", "link": "https://arxiv.org/pdf/2501.09138", "details": "X He, Y Hu, Z Zhou, M Jarraya, F Liu - arXiv preprint arXiv:2501.09138, 2025", "abstract": "Vision foundation models have achieved remarkable progress across various image analysis tasks. In the image segmentation task, foundation models like the Segment Anything Model (SAM) enable generalizable zero-shot segmentation through user \u2026"}, {"title": "Foreign object segmentation in chest x-rays through anatomy-guided shape insertion", "link": "https://arxiv.org/pdf/2501.12022", "details": "C Seibold, H Kalisch, L Heine, S Rei\u00df, J Kleesiek - arXiv preprint arXiv:2501.12022, 2025", "abstract": "In this paper, we tackle the challenge of instance segmentation for foreign objects in chest radiographs, commonly seen in postoperative follow-ups with stents, pacemakers, or ingested objects in children. The diversity of foreign objects \u2026"}, {"title": "Are Traditional Deep Learning Model Approaches as Effective as a Retinal-Specific Foundation Model for Ocular and Systemic Disease Detection?", "link": "https://arxiv.org/pdf/2501.12016", "details": "SME Yew, X Lei, JHL Goh, Y Chen, S Srinivasan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Background: RETFound, a self-supervised, retina-specific foundation model (FM), showed potential in downstream applications. However, its comparative performance with traditional deep learning (DL) models remains incompletely understood. This \u2026"}, {"title": "Deformable Image Registration of Dark-Field Chest Radiographs for Local Lung Signal Change Assessment", "link": "https://arxiv.org/abs/2501.10757", "details": "F Drexel, V Sideri-Lampretsa, H Bast, AW Marka\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Dark-field radiography of the human chest has been demonstrated to have promising potential for the analysis of the lung microstructure and the diagnosis of respiratory diseases. However, previous studies of dark-field chest radiographs evaluated the \u2026"}, {"title": "Focus Your Attention: Multiple Instance Learning with Attention Modification for Whole Slide Pathological Image Classification", "link": "https://ieeexplore.ieee.org/iel8/76/4358651/10838539.pdf", "details": "H Cheng, S Huang, L Cai, Y Xu, R Wang, Y Zhang - IEEE Transactions on Circuits \u2026, 2025", "abstract": "Computer-aided pathology diagnosis based on whole slide images, which is often formulated as a weakly supervised multiple instance learning (MIL) paradigm. Current approaches generally employ attention mechanisms to aggregate instance \u2026"}, {"title": "Supervision-free Vision-Language Alignment", "link": "https://arxiv.org/pdf/2501.04568%3F", "details": "G Giannone, R Li, Q Feng, E Perevodchikov, R Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) have demonstrated remarkable potential in integrating visual and linguistic information, but their performance is often constrained by the need for extensive, high-quality image-text training data. Curation \u2026"}, {"title": "Valley2: Exploring Multimodal Models with Scalable Vision-Language Design", "link": "https://arxiv.org/pdf/2501.05901", "details": "Z Wu, Z Chen, R Luo, C Zhang, Y Gao, Z He, X Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, vision-language models have made remarkable progress, demonstrating outstanding capabilities in various tasks such as image captioning and video understanding. We introduce Valley2, a novel multimodal large language model \u2026"}]
