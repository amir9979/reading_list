[{"title": "Multidimensional Consistency Improves Reasoning in Language Models", "link": "https://arxiv.org/pdf/2503.02670", "details": "H Lai, X Zhang, M Nissim - arXiv preprint arXiv:2503.02670, 2025", "abstract": "While Large language models (LLMs) have proved able to address some complex reasoning tasks, we also know that they are highly sensitive to input variation, which can lead to different solution paths and final answers. Answer consistency across \u2026"}, {"title": "Adding Alignment Control to Language Models", "link": "https://arxiv.org/pdf/2503.04346", "details": "W Zhu, W Zhang, R Wang - arXiv preprint arXiv:2503.04346, 2025", "abstract": "Post-training alignment has increasingly become a crucial factor in enhancing the usability of language models (LMs). However, the strength of alignment varies depending on individual preferences. This paper proposes a method to incorporate \u2026"}, {"title": "Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks", "link": "https://arxiv.org/pdf/2503.04833", "details": "L Lu, S Pang, S Liang, H Zhu, X Zeng, A Liu, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal large language models (MLLMs) have made remarkable strides in cross- modal comprehension and generation tasks. However, they remain vulnerable to jailbreak attacks, where crafted perturbations bypass security guardrails and elicit \u2026"}, {"title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems", "link": "https://arxiv.org/pdf/2503.03686", "details": "R Ye, S Tang, R Ge, Y Du, Z Yin, S Chen, J Shao - arXiv preprint arXiv:2503.03686, 2025", "abstract": "LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability \u2026"}, {"title": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs", "link": "https://arxiv.org/pdf/2503.05139", "details": "L Team, B Zeng, C Huang, C Zhang, C Tian, C Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this technical report, we tackle the challenges of training large-scale Mixture of Experts (MoE) models, focusing on overcoming cost inefficiency and resource limitations prevalent in such systems. To address these issues, we present two \u2026"}, {"title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation", "link": "https://arxiv.org/pdf/2503.02832", "details": "S Zhang, X Zhang, T Zhang, B Hu, Y Chen, J Xu - arXiv preprint arXiv:2503.02832, 2025", "abstract": "In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most \u2026"}, {"title": "Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety", "link": "https://arxiv.org/pdf/2503.05021", "details": "Y Zhang, M Li, W Han, Y Yao, Z Cen, D Zhao - arXiv preprint arXiv:2503.05021, 2025", "abstract": "Large Language Models (LLMs) are vulnerable to jailbreak attacks that exploit weaknesses in traditional safety alignment, which often relies on rigid refusal heuristics or representation engineering to block harmful outputs. While they are \u2026"}, {"title": "Benchmarking Reasoning Robustness in Large Language Models", "link": "https://arxiv.org/pdf/2503.04550", "details": "T Yu, Y Jing, X Zhang, W Jiang, W Wu, Y Wang, W Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite the recent success of large language models (LLMs) in reasoning such as DeepSeek, we for the first time identify a key dilemma in reasoning robustness and generalization: significant performance degradation on novel or incomplete data \u2026"}, {"title": "Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection via Backdoor Attacks", "link": "https://arxiv.org/pdf/2503.05445", "details": "M Lin, H Zhang, J Lao, R Li, Y Zhou, C Yang, Y Cao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have shown state-of-the-art results in translating natural language questions into SQL queries (Text-to-SQL), a long-standing challenge within the database community. However, security concerns remain \u2026"}]
