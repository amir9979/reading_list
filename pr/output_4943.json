[{"title": "Developing a natural language processing system using transformer-based models for adverse drug event detection in electronic health records", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/07/10/2024.07.09.24310100.full.pdf", "details": "J Wu, X Ruan, E McNeer, KM Rossow, L Choi - medRxiv, 2024", "abstract": "Objective: To develop a transformer-based natural language processing (NLP) system for detecting adverse drug events (ADEs) from clinical notes in electronic health records (EHRs). Materials and Methods: We fine-tuned BERT Short-Formers \u2026"}, {"title": "Stacked Reflective Reasoning in Large Neural Language Models", "link": "https://ceur-ws.org/Vol-3740/paper-121.pdf", "details": "K Villarreal-Haro, F S\u00e1nchez-Vega, A Rosales-P\u00e9rez\u2026 - Working Notes of CLEF, 2024", "abstract": "Sexism, far from being merely a conceptual issue, is a concerning and pervasive social health problem that negatively impacts individuals' well-being and perception. In today's digital era, as sexism permeates online platforms, the creation of systems \u2026"}, {"title": "MAGNET: Improving the Multilingual Fairness of Language Models with Adaptive Gradient-Based Tokenization", "link": "https://arxiv.org/pdf/2407.08818", "details": "O Ahia, S Kumar, H Gonen, V Hoffman, T Limisiewicz\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In multilingual settings, non-Latin scripts and low-resource languages are usually disadvantaged in terms of language models' utility, efficiency, and cost. Specifically, previous studies have reported multiple modeling biases that the current tokenization \u2026"}, {"title": "Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference", "link": "https://arxiv.org/pdf/2408.01935", "details": "K Shen, M Kejriwal - arXiv preprint arXiv:2408.01935, 2024", "abstract": "Despite their impressive performance, large language models (LLMs) such as ChatGPT are known to pose important risks. One such set of risks arises from misplaced confidence, whether over-confidence or under-confidence, that the \u2026"}, {"title": "Re-Envisioning Electronic Health Records to Optimize Patient-Centered Cancer Care, Quality, Surveillance, and Research", "link": "https://ascopubs.org/doi/pdfdirect/10.1200/OP.24.00260", "details": "AP Singh, EP Balogh, RW Carlson, MM Huizinga\u2026 - JCO Oncology Practice, 2024", "abstract": "Electronic health records (EHRs) are a significant advancement over paper records. However, the full potential of EHRs for improving care quality, patient outcomes, surveillance, and research in cancer care is yet to be realized. The organic evolution \u2026"}, {"title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Trustworthy Response Generation in Chinese", "link": "https://dl.acm.org/doi/pdf/10.1145/3686807", "details": "H Wang, S Zhao, Z Qiang, Z Li, C Liu, N Xi, Y Du, B Qin\u2026 - ACM Transactions on \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to \u2026"}, {"title": "MedSyn: LLM-based Synthetic Medical Text Generation Framework", "link": "https://arxiv.org/pdf/2408.02056", "details": "G Kumichev, P Blinov, Y Kuzkina, V Goncharov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating synthetic text addresses the challenge of data availability in privacy- sensitive domains such as healthcare. This study explores the applicability of synthetic data in real-world medical settings. We introduce MedSyn, a novel medical \u2026"}, {"title": "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "link": "https://arxiv.org/pdf/2408.02103", "details": "P Wang, X Wang, C Lou, S Mao, P Xie, Y Jiang - arXiv preprint arXiv:2408.02103, 2024", "abstract": "In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language \u2026"}, {"title": "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models", "link": "https://arxiv.org/pdf/2407.16470", "details": "K Benkirane, L Gongas, S Pelles, N Fuchs, J Darmon\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting \u2026"}]
