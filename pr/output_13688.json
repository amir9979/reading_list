[{"title": "Interpretable Few-Shot Retinal Disease Diagnosis with Concept-Guided Prompting of Vision-Language Models", "link": "https://arxiv.org/pdf/2503.02917", "details": "D Mehta, Y Jiang, CL Jan, M He, K Jadhav, Z Ge - arXiv preprint arXiv:2503.02917, 2025", "abstract": "Recent advancements in deep learning have shown significant potential for classifying retinal diseases using color fundus images. However, existing works predominantly rely exclusively on image data, lack interpretability in their diagnostic \u2026"}, {"title": "Generative adversarial networks vs large language models: a comparative study on synthetic tabular data generation", "link": "https://arxiv.org/pdf/2502.14523", "details": "AA Barr, R Rozman, E Guo - arXiv preprint arXiv:2502.14523, 2025", "abstract": "We propose a new framework for zero-shot generation of synthetic tabular data. Using the large language model (LLM) GPT-4o and plain-language prompting, we demonstrate the ability to generate high-fidelity tabular data without task-specific fine \u2026"}, {"title": "Parameter-Efficient Instruction Tuning Code Large Language Models: An Empirical Study", "link": "https://openreview.net/pdf%3Fid%3DdAiUf1MAbS", "details": "TY Zhuo, AR Zebaze, L Von Werra, H de Vries, Q Liu\u2026 - ICLR 2025 Third Workshop on \u2026", "abstract": "The high cost of full-parameter fine-tuning (FFT) of Large Language Models (LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods. However, it remains unclear which methods provide the best cost-performance trade-off at \u2026"}]
