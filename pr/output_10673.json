[{"title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding", "link": "https://arxiv.org/pdf/2412.16158", "details": "C Tao, S Su, X Zhu, C Zhang, Z Chen, J Liu, W Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advance of Large Language Models (LLMs) has catalyzed the development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid modality-specific encoders, offer a promising alternative to the compositional ones \u2026"}, {"title": "Domain knowledge boosted adaptation: Leveraging vision-language models for multi-source domain adaptation", "link": "https://www.sciencedirect.com/science/article/pii/S092523122401885X", "details": "Y He, J Feng, G Ding, Y Guo, T He - Neurocomputing, 2024", "abstract": "Multi-source domain adaptation (MSDA) aims to adapt a model trained on multiple labeled source domains to an unlabeled target domain. Existing MSDA methods primarily focus on reducing domain gaps by aligning the source domains with the \u2026"}, {"title": "Holmes\u2315 A Benchmark to Assess the Linguistic Competence of Language Models", "link": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00718/125534", "details": "A Waldis, Y Perlitz, L Choshen, Y Hou, I Gurevych - Transactions of the Association \u2026, 2024", "abstract": "We introduce Holmes, a new benchmark designed to assess language models'(LMs') linguistic competence\u2014their unconscious understanding of linguistic phenomena. Specifically, we use classifier-based probing to examine LMs' internal \u2026"}, {"title": "Context-DPO: Aligning Language Models for Context-Faithfulness", "link": "https://arxiv.org/abs/2412.15280", "details": "B Bi, S Huang, Y Wang, T Yang, Z Zhang, H Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Reliable responses from large language models (LLMs) require adherence to user instructions and retrieved information. While alignment techniques help LLMs align with human intentions and values, improving context-faithfulness through alignment \u2026"}, {"title": "CPLLM: Clinical prediction with large language models", "link": "https://journals.plos.org/digitalhealth/article%3Fid%3D10.1371/journal.pdig.0000680", "details": "O Ben Shoham, N Rappoport - PLOS Digital Health, 2024", "abstract": "We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for predicting clinical disease and readmission. We utilized quantization and fine-tuned the LLM using \u2026"}, {"title": "Insights from the Frontline: GenAI Utilization Among Software Engineering Students", "link": "https://arxiv.org/pdf/2412.15624", "details": "R Choudhuri, A Ramakrishnan, A Chatterjee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generative AI (genAI) tools (eg, ChatGPT, Copilot) have become ubiquitous in software engineering (SE). As SE educators, it behooves us to understand the consequences of genAI usage among SE students and to create a holistic view of \u2026"}, {"title": "Offline Reinforcement Learning for LLM Multi-Step Reasoning", "link": "https://arxiv.org/pdf/2412.16145", "details": "H Wang, S Hao, H Dong, S Zhang, Y Bao, Z Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Improving the multi-step reasoning ability of large language models (LLMs) with offline reinforcement learning (RL) is essential for quickly adapting them to complex tasks. While Direct Preference Optimization (DPO) has shown promise in aligning \u2026"}, {"title": "Frame Representation Hypothesis: Multi-Token LLM Interpretability and Concept-Guided Text Generation", "link": "https://arxiv.org/pdf/2412.07334", "details": "PHV Valois, LS Souza, EK Shimomoto, K Fukui - arXiv preprint arXiv:2412.07334, 2024", "abstract": "Interpretability is a key challenge in fostering trust for Large Language Models (LLMs), which stems from the complexity of extracting reasoning from model's parameters. We present the Frame Representation Hypothesis, a theoretically robust \u2026"}, {"title": "Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models", "link": "https://arxiv.org/pdf/2412.15287", "details": "Y Chow, G Tennenholtz, I Gur, V Zhuang, B Dai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent studies have indicated that effectively utilizing inference-time compute is crucial for attaining better performance from large language models (LLMs). In this work, we propose a novel inference-aware fine-tuning paradigm, in which the model \u2026"}]
