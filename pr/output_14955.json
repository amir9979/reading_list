[{"title": "Improving Multilingual Retrieval-Augmented Language Models through Dialectic Reasoning Argumentations", "link": "https://arxiv.org/pdf/2504.04771", "details": "L Ranaldi, F Ranaldi, FM Zanzotto, B Haddow, A Birch - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-augmented generation (RAG) is key to enhancing large language models (LLMs) to systematically access richer factual knowledge. Yet, using RAG brings intrinsic challenges, as LLMs must deal with potentially conflicting knowledge \u2026"}, {"title": "T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models", "link": "https://arxiv.org/pdf/2504.04718", "details": "M Kang, J Jeong, J Cho - arXiv preprint arXiv:2504.04718, 2025", "abstract": "Recent studies have demonstrated that test-time compute scaling effectively improves the performance of small language models (sLMs). However, prior research has mainly examined test-time compute scaling with an additional larger \u2026"}, {"title": "Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration", "link": "https://arxiv.org/pdf/2504.04915", "details": "R Xu, W Shi, Y Zhuang, Y Yu, JC Ho, H Wang, C Yang - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-Augmented Generation (RAG) systems often struggle to handle multi-hop question-answering tasks accurately due to irrelevant context retrieval and limited complex reasoning capabilities. We introduce Collab-RAG, a collaborative training \u2026"}]
