[{"title": "Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models", "link": "https://arxiv.org/pdf/2408.01308", "details": "Y Zhang, D Li, M Okumura - arXiv preprint arXiv:2408.01308, 2024", "abstract": "Learning token embeddings based on token co-occurrence statistics has proven effective for both pre-training and fine-tuning in natural language processing. However, recent studies have pointed out the distribution of learned embeddings \u2026"}, {"title": "Making Large Vision Language Models to be Good Few-shot Learners", "link": "https://arxiv.org/pdf/2408.11297", "details": "F Liu, W Cai, J Huo, C Zhang, D Chen, J Zhou - arXiv preprint arXiv:2408.11297, 2024", "abstract": "Few-shot classification (FSC) is a fundamental yet challenging task in computer vision that involves recognizing novel classes from limited data. While previous methods have focused on enhancing visual features or incorporating additional \u2026"}, {"title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "link": "https://arxiv.org/pdf/2407.21417", "details": "Z Wu, Y Zhang, P Qi, Y Xu, R Han, Y Zhang, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (ie, follow open-ended instructions) and \u2026"}, {"title": "Assessing the Ability of a Large Language Model to Score Free-Text Medical Student Clinical Notes: Quantitative Study", "link": "https://mededu.jmir.org/2024/1/e56342", "details": "HB Burke, A Hoang, JO Lopreiato, H King, P Hemmer\u2026 - JMIR Medical Education, 2024", "abstract": "Background Teaching medical students the skills required to acquire, interpret, apply, and communicate clinical information is an integral part of medical education. A crucial aspect of this process involves providing students with feedback regarding \u2026"}, {"title": "Do Language Models Have a Critical Period for Language Acquisition?", "link": "https://arxiv.org/pdf/2407.19325", "details": "I Constantinescu, T Pimentel, R Cotterell, A Warstadt - arXiv preprint arXiv \u2026, 2024", "abstract": "Humans appear to have a critical period (CP) for language acquisition: Second language (L2) acquisition becomes harder after early childhood, and ceasing exposure to a first language (L1) after this period (but not before) typically does not \u2026"}, {"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "link": "https://arxiv.org/pdf/2408.00724", "details": "Y Wu, Z Sun, S Li, S Welleck, Y Yang - arXiv preprint arXiv:2408.00724, 2024", "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth \u2026"}, {"title": "Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering", "link": "https://arxiv.org/pdf/2407.21368", "details": "D Guo, D Terzopoulos - arXiv preprint arXiv:2407.21368, 2024", "abstract": "Large Vision-Language Models (LVLMs) have achieved significant success in recent years, and they have been extended to the medical domain. Although demonstrating satisfactory performance on medical Visual Question Answering (VQA) tasks \u2026"}, {"title": "Graph and text multi-modal representation learning with momentum distillation on Electronic Health Records", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124010074", "details": "Y Cao, X Wang, Q Wang, Z Yuan, Y Shi, D Peng - Knowledge-Based Systems, 2024", "abstract": "The emergence and widespread adoption of electronic health records (EHR) in modern healthcare systems has generated a large amount of data with the potential to significantly improve patient outcomes. However, extracting meaningful insights \u2026"}, {"title": "Tabular Transfer Learning via Prompting LLMs", "link": "https://arxiv.org/pdf/2408.11063", "details": "J Nam, W Song, SH Park, J Tack, S Yun, J Kim, KH Oh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Learning with a limited number of labeled data is a central problem in real-world applications of machine learning, as it is often expensive to obtain annotations. To deal with the scarcity of labeled data, transfer learning is a conventional approach; it \u2026"}]
