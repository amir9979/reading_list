[{"title": "$\\texttt {MoE-RBench} $: Towards Building Reliable Language Models with Sparse Mixture-of-Experts", "link": "https://arxiv.org/pdf/2406.11353", "details": "G Chen, X Zhao, T Chen, Y Cheng - arXiv preprint arXiv:2406.11353, 2024", "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new \u2026"}, {"title": "Language Models can be Deductive Solvers", "link": "https://aclanthology.org/2024.findings-naacl.254.pdf", "details": "J Feng, R Xu, J Hao, H Sharma, Y Shen, D Zhao\u2026 - Findings of the Association \u2026, 2024", "abstract": "Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit \u2026"}, {"title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space", "link": "https://arxiv.org/pdf/2406.09325", "details": "T Ashuach, M Tutek, Y Belinkov - arXiv preprint arXiv:2406.09325, 2024", "abstract": "Large language models (LLMs) risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in training data, causing privacy concerns. Current approaches to address this issue involve costly dataset \u2026"}, {"title": "Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain", "link": "https://arxiv.org/pdf/2406.06435", "details": "B Hu, B Ray, A Leung, A Summerville, D Joy, C Funk\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In difficult decision-making scenarios, it is common to have conflicting opinions among expert human decision-makers as there may not be a single right answer. Such decisions may be guided by different attributes that can be used to characterize \u2026"}, {"title": "Advancing High Resolution Vision-Language Models in Biomedicine", "link": "https://arxiv.org/pdf/2406.09454", "details": "Z Chen, A Pekis, K Brown - arXiv preprint arXiv:2406.09454, 2024", "abstract": "Multi-modal learning has significantly advanced generative AI, especially in vision- language modeling. Innovations like GPT-4V and open-source projects such as LLaVA have enabled robust conversational agents capable of zero-shot task \u2026"}, {"title": "Language Models are Crossword Solvers", "link": "https://arxiv.org/pdf/2406.09043", "details": "S Saha, S Chakraborty, S Saha, U Garain - arXiv preprint arXiv:2406.09043, 2024", "abstract": "Crosswords are a form of word puzzle that require a solver to demonstrate a high degree of proficiency in natural language understanding, wordplay, reasoning, and world knowledge, along with adherence to character and length constraints. In this \u2026"}, {"title": "Aligning Language Models with the Human World", "link": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi%3Farticle%3D1241%26context%3Ddissertations", "details": "R LIU - 2024", "abstract": "Abstract The field of Natural Language Processing (NLP) has undergone a significant transformation with the emergence of large language models (LMs). These models have enabled the development of human-like conversational \u2026"}, {"title": "A comparative study of large language model-based zero-shot inference and task-specific supervised classification of breast cancer pathology reports", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae146/7696538", "details": "M Sushil, T Zack, D Mandair, Z Zheng, A Wali, YN Yu\u2026 - Journal of the American \u2026, 2024", "abstract": "Objective Although supervised machine learning is popular for information extraction from clinical notes, creating large annotated datasets requires extensive domain expertise and is time-consuming. Meanwhile, large language models (LLMs) have \u2026"}]
