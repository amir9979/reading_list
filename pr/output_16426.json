[{"title": "NNTile: a machine learning framework capable of training extremely large GPT language models on a single node", "link": "https://arxiv.org/pdf/2504.13236", "details": "A Mikhalev, A Katrutsa, K Sozykin, I Oseledets - arXiv preprint arXiv:2504.13236, 2025", "abstract": "This study presents an NNTile framework for training large deep neural networks in heterogeneous clusters. The NNTile is based on a StarPU library, which implements task-based parallelism and schedules all provided tasks onto all available \u2026"}, {"title": "HealthBench: Evaluating Large Language Models Towards Improved Human Health", "link": "https://arxiv.org/pdf/2505.08775", "details": "RK Arora, J Wei, RS Hicks, P Bowman\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present HealthBench, an open-source benchmark measuring the performance and safety of large language models in healthcare. HealthBench consists of 5,000 multi-turn conversations between a model and an individual user or healthcare \u2026"}, {"title": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models", "link": "https://arxiv.org/pdf/2504.18041", "details": "B An, S Zhang, M Dredze - arXiv preprint arXiv:2504.18041, 2025", "abstract": "Efforts to ensure the safety of large language models (LLMs) include safety fine- tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on \u2026"}, {"title": "Benchmarking Retrieval-Augmented Generation for Chemistry", "link": "https://arxiv.org/pdf/2505.07671", "details": "X Zhong, B Jin, S Ouyang, Y Shen, Q Jin, Y Fang, Z Lu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-augmented generation (RAG) has emerged as a powerful framework for enhancing large language models (LLMs) with external knowledge, particularly in scientific domains that demand specialized and dynamic information. Despite its \u2026"}, {"title": "Latte: Transfering LLMsLatent-level Knowledge for Few-shot Tabular Learning", "link": "https://arxiv.org/pdf/2505.05237", "details": "R Shi, H Gu, H Ye, Y Dai, X Shen, X Wang - arXiv preprint arXiv:2505.05237, 2025", "abstract": "Few-shot tabular learning, in which machine learning models are trained with a limited amount of labeled data, provides a cost-effective approach to addressing real- world challenges. The advent of Large Language Models (LLMs) has sparked \u2026"}, {"title": "A Note on Statistically Accurate Tabular Data Generation Using Large Language Models", "link": "https://arxiv.org/pdf/2505.02659%3F", "details": "A Sidorenko - arXiv preprint arXiv:2505.02659, 2025", "abstract": "Large language models (LLMs) have shown promise in synthetic tabular data generation, yet existing methods struggle to preserve complex feature dependencies, particularly among categorical variables. This work introduces a \u2026"}]
