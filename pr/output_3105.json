[{"title": "MedT5SQL: A Transformers-based Large Language Model for Text-to-SQL Conversion in the Healthcare Domain", "link": "https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2024.1371680/full", "details": "A Marshan, N Anwar, A Ioannou, D Bell, A Monaghan\u2026 - Frontiers in Big Data, 2024", "abstract": "Introduction In response to the increasing prevalence of electronic medical records (EMRs) stored in databases, healthcare staff are encountering difficulties retrieving these records due to their limited technical expertise in database operations. As \u2026"}, {"title": "MFC-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2406.11288", "details": "S Wang, H Lin, Z Luo, Z Ye, G Chen, J Ma - arXiv preprint arXiv:2406.11288, 2024", "abstract": "Large vision-language models (LVLMs) have significantly improved multimodal reasoning tasks, such as visual question answering and image captioning. These models embed multimodal facts within their parameters, rather than relying on \u2026"}, {"title": "Text-to-SQL with Large Language Models: Exploring the Promise and Pitfalls", "link": "https://sebd2024.unica.it/papers/paper65.pdf", "details": "L Sala, G Sullutrone, S Bergamaschi - 2024", "abstract": "Abstract The emergence of Large Language Models (LLMs) represents a fundamental change in the ever-evolving field of natural language processing (NLP). Over the past few years, the enhanced capabilities of these models have led to their \u2026"}, {"title": "Scaling Laws for Linear Complexity Language Models", "link": "https://arxiv.org/pdf/2406.16690", "details": "X Shen, D Li, R Leng, Z Qin, W Sun, Y Zhong - arXiv preprint arXiv:2406.16690, 2024", "abstract": "The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their \u2026"}, {"title": "AnnotatedTables: A Large Tabular Dataset with Language Model Annotations", "link": "https://arxiv.org/pdf/2406.16349", "details": "Y Hu, I Fountalis, J Tian, N Vasiloglou - arXiv preprint arXiv:2406.16349, 2024", "abstract": "Tabular data is ubiquitous in real-world applications and abundant on the web, yet its annotation has traditionally required human labor, posing a significant scalability bottleneck for tabular machine learning. Our methodology can successfully annotate \u2026"}, {"title": "DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning", "link": "https://arxiv.org/pdf/2406.17659", "details": "X Zhang, Z Altaweel, Y Hayamizu, Y Ding, S Amiri\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language models (VLMs) have been applied to robot task planning problems, where the robot receives a task in natural language and generates plans based on visual inputs. While current VLMs have demonstrated strong vision-language \u2026"}, {"title": "BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models", "link": "https://arxiv.org/pdf/2406.17092", "details": "Y Zeng, W Sun, TN Huynh, D Song, B Li, R Jia - arXiv preprint arXiv:2406.17092, 2024", "abstract": "Safety backdoor attacks in large language models (LLMs) enable the stealthy triggering of unsafe behaviors while evading detection during normal interactions. The high dimensionality of potential triggers in the token space and the diverse \u2026"}, {"title": "A fine-grained self-adapting prompt learning approach for few-shot learning with pre-trained language models", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124006026", "details": "X Chen, T Liu, P Fournier-Viger, B Zhang, G Long\u2026 - Knowledge-Based Systems, 2024", "abstract": "Pre-trained language models have demonstrated remarkable performance in few- shot learning through the emergence of \u201cprompt-based learning\u201d methods, where the performance of these tasks highly rely on the quality of prompts. Existing prompt \u2026"}, {"title": "Information Guided Regularization for Fine-tuning Language Models", "link": "https://arxiv.org/pdf/2406.14005", "details": "M Sharma, N Muralidhar, S Xu, RB Yosuf\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The pretraining-fine-tuning paradigm has been the de facto strategy for transfer learning in modern language modeling. With the understanding that task adaptation in LMs is often a function of parameters shared across tasks, we argue that a more \u2026"}]
