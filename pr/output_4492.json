[{"title": "Weakly Supervised Classification for Nasopharyngeal Carcinoma with Transformer in Whole Slide Images", "link": "https://ieeexplore.ieee.org/abstract/document/10584271/", "details": "Z Hu, J Wang, Q Gao, Z Wu, H Xu, Z Guo, J Quan\u2026 - IEEE Journal of Biomedical \u2026, 2024", "abstract": "Pathological examination of nasopharyngeal carcinoma (NPC) is an indispensable factor for diagnosis, guiding clinical treatment and judging prognosis. Traditional and fully supervised NPC diagnosis algorithms require manual delineation of regions of \u2026"}, {"title": "A deep-learning framework to predict cancer treatment response from histopathology images through imputed transcriptomics", "link": "https://repository.icr.ac.uk/bitstream/handle/internal/6313/s43018-024-00793-2.pdf%3Fsequence%3D2", "details": "DT Hoang, G Dinstag, ED Shulman, LC Hermida\u2026 - Nature Cancer, 2024", "abstract": "Advances in artificial intelligence have paved the way for leveraging hematoxylin and eosin-stained tumor slides for precision oncology. We present ENLIGHT\u2013 DeepPT, an indirect two-step approach consisting of (1) DeepPT, a deep-learning \u2026"}, {"title": "SCMIL: Sparse Context-aware Multiple Instance Learning for Predicting Cancer Survival Probability Distribution in Whole Slide Images", "link": "https://arxiv.org/pdf/2407.00664", "details": "Z Yang, H Liu, X Wang - arXiv preprint arXiv:2407.00664, 2024", "abstract": "Cancer survival prediction is a challenging task that involves analyzing of the tumor microenvironment within Whole Slide Image (WSI). Previous methods cannot effectively capture the intricate interaction features among instances within the local \u2026"}, {"title": "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "link": "https://arxiv.org/pdf/2407.02118", "details": "W Zheng, W Pan, X Xu, L Qin, L Yue, M Zhou - arXiv preprint arXiv:2407.02118, 2024", "abstract": "In recent years, Large Language Models (LLMs) have made significant strides towards Artificial General Intelligence. However, training these models from scratch requires substantial computational resources and vast amounts of text data. In this \u2026"}, {"title": "Financial Knowledge Large Language Model", "link": "https://arxiv.org/pdf/2407.00365", "details": "C Yang, C Xu, Y Qi - arXiv preprint arXiv:2407.00365, 2024", "abstract": "Artificial intelligence is making significant strides in the finance industry, revolutionizing how data is processed and interpreted. Among these technologies, large language models (LLMs) have demonstrated substantial potential to transform \u2026"}, {"title": "MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis", "link": "https://arxiv.org/pdf/2407.04106", "details": "A Alkhaldi, R Alnajim, L Alabdullatef, R Alyahya\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in artificial intelligence (AI) have precipitated significant breakthroughs in healthcare, particularly in refining diagnostic procedures. However, previous studies have often been constrained to limited functionalities. This study \u2026"}, {"title": "LLMGR: Large Language Model-based Generative Retrieval in Alipay Search", "link": "https://dl.acm.org/doi/abs/10.1145/3626772.3661364", "details": "C Wei, Y Ji, Z Chen, J Xu, Z Liu - Proceedings of the 47th International ACM SIGIR \u2026, 2024", "abstract": "The search system aims to help users quickly find items according to queries they enter, which includes the retrieval and ranking modules. Traditional retrieval is a multi-stage process, including indexing and sorting, which cannot be optimized end \u2026"}, {"title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models", "link": "https://arxiv.org/pdf/2407.03181", "details": "H Puerto, T Chubakov, X Zhu, HT Madabushi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model \u2026"}, {"title": "LIONs: An Empirically Optimized Approach to Align Language Models", "link": "https://arxiv.org/pdf/2407.06542", "details": "X Yu, Q Wu, Y Li, Z Yu - arXiv preprint arXiv:2407.06542, 2024", "abstract": "Alignment is a crucial step to enhance the instruction-following and conversational abilities of language models. Despite many recent work proposing new algorithms, datasets, and training pipelines, there is a lack of comprehensive studies measuring \u2026"}]
