[{"title": "Inter-structure and intra-semantics graph contrastive learning for disease prediction", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124006932", "details": "Y Kang, J Zheng, M Yang, N An - Knowledge-Based Systems, 2024", "abstract": "Ever-evolving healthcare applications have witnessed a surge in the utilization of electronic health records (EHR) for predicting future patient diagnoses. While Graph Neural Networks have demonstrated that promise in modeling disease-patient \u2026"}, {"title": "Timo: Towards Better Temporal Reasoning for Language Models", "link": "https://arxiv.org/pdf/2406.14192", "details": "Z Su, J Zhang, T Zhu, X Qu, J Li, M Zhang, Y Cheng - arXiv preprint arXiv:2406.14192, 2024", "abstract": "Reasoning about time is essential for Large Language Models (LLMs) to understand the world. Previous works focus on solving specific tasks, primarily on time-sensitive question answering. While these methods have proven effective, they cannot \u2026"}, {"title": "LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models", "link": "https://arxiv.org/pdf/2406.19486", "details": "S Guo, S Damani, K Chang - arXiv preprint arXiv:2406.19486, 2024", "abstract": "In prompt tuning, a prefix or suffix text is added to the prompt, and the embeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix are optimized to gain more control over language models for specific tasks. This approach eliminates the \u2026"}, {"title": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models", "link": "https://arxiv.org/pdf/2407.10380", "details": "P Pandya, AS Talwarr, V Gupta, T Kataria, V Gupta\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Cognitive textual and visual reasoning tasks, such as puzzles, series, and analogies, demand the ability to quickly reason, decipher, and evaluate patterns both textually and spatially. While LLMs and VLMs, through extensive training on large amounts of \u2026"}, {"title": "Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models", "link": "https://arxiv.org/pdf/2407.02716", "details": "X Han, L Jin, X Ma, X Liu - arXiv preprint arXiv:2407.02716, 2024", "abstract": "Fine-tuning pre-trained Vision-Language Models (VLMs) has shown remarkable capabilities in medical image and textual depiction synergy. Nevertheless, many pre- training datasets are restricted by patient privacy concerns, potentially containing \u2026"}, {"title": "UQE: A Query Engine for Unstructured Databases", "link": "https://arxiv.org/pdf/2407.09522", "details": "H Dai, BY Wang, X Wan, B Dai, S Yang, A Nova, P Yin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Analytics on structured data is a mature field with many successful methods. However, most real world data exists in unstructured form, such as images and conversations. We investigate the potential of Large Language Models (LLMs) to \u2026"}, {"title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment", "link": "https://arxiv.org/pdf/2407.10804", "details": "J Jiang, J Li, WX Zhao, Y Song, T Zhang, JR Wen - arXiv preprint arXiv:2407.10804, 2024", "abstract": "Adapting general large language models (LLMs) to specialized domains presents great challenges due to varied data distributions. This adaptation typically requires continual pre-training on massive domain-specific corpora to facilitate knowledge \u2026"}, {"title": "Robust Privacy-Preserving Recommendation Systems Driven by Multimodal Federated Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10562343/", "details": "C Feng, D Feng, G Huang, Z Liu, Z Wang, XG Xia - IEEE Transactions on Neural \u2026, 2024", "abstract": "Recommendation system (RS) is an important information filtering tool in nowadays digital era. With the growing concern on privacy, deploying RSs in a federated learning (FL) manner emerges as a promising solution, which can train a high-quality \u2026"}, {"title": "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation", "link": "https://arxiv.org/pdf/2407.02056", "details": "X Wang, Y Li, S Feng, P Yuan, B Pan, H Wang, Y Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-consistency (SC), leveraging multiple samples from LLMs, shows significant gains on various reasoning tasks but struggles with free-form generation due to the difficulty of aggregating answers. Its variants, UCS and USC, rely on sample \u2026"}]
