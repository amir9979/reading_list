[{"title": "Efficient Architectures for High Resolution Vision-Language Models", "link": "https://arxiv.org/pdf/2501.02584", "details": "M Carvalho, B Martins - arXiv preprint arXiv:2501.02584, 2025", "abstract": "Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This \u2026"}, {"title": "Advancements in Machine Learning and Artificial Intelligence in the Radiological Detection of Pulmonary Embolism", "link": "https://www.cureus.com/articles/324474-advancements-in-machine-learning-and-artificial-intelligence-in-the-radiological-detection-of-pulmonary-embolism.pdf", "details": "M Mohanarajan, PP Salunke, A Arif, PMI Gonzalez\u2026 - Cureus, 2025", "abstract": "Pulmonary embolism (PE) is a clinically challenging diagnosis that varies from silent to life-threatening symptoms. Timely diagnosis of the condition is subject to clinical assessment, D-dimer testing and radiological imaging. Computed tomography \u2026"}, {"title": "Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers", "link": "https://arxiv.org/pdf/2501.18237", "details": "M T\u00f6lle, M Scharaf, S Fischer, C Reich, S Zeid\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "A patient undergoes multiple examinations in each hospital stay, where each provides different facets of the health status. These assessments include temporal data with varying sampling rates, discrete single-point measurements, therapeutic \u2026"}, {"title": "EHealth: A Chinese Biomedical Language Model Built via Multi-Level Text Discrimination", "link": "https://ieeexplore.ieee.org/abstract/document/10857372/", "details": "Q Wang, S Dai, B Xu, Y Lyu, H Wu, H Wang - IEEE Transactions on Audio, Speech \u2026, 2025", "abstract": "Pre-trained language models (PLMs) have recently revolutionized the field of natural language processing, impacting not only the general domain but also the biomedical domain. Most previous studies on constructing biomedical PLMs relied simply on \u2026"}, {"title": "Instruction-Following Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2501.02086", "details": "B Hou, Q Chen, J Wang, G Yin, C Wang, N Du, R Pang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from \u2026"}, {"title": "The Power of Negative Zero: Datatype Customization for Quantized Large Language Models", "link": "https://arxiv.org/pdf/2501.04052", "details": "Y Chen, X Dai, C Chang, Y Akhauri, MS Abdelfattah - arXiv preprint arXiv:2501.04052, 2025", "abstract": "Large language models (LLMs) have demonstrated remarkable performance across various machine learning tasks, quickly becoming one of the most prevalent AI workloads. Yet the substantial memory requirement of LLMs significantly hinders \u2026"}, {"title": "ConceptCLIP: Towards Trustworthy Medical AI via Concept-Enhanced Contrastive Langauge-Image Pre-training", "link": "https://arxiv.org/pdf/2501.15579", "details": "Y Nie, S He, Y Bie, Y Wang, Z Chen, S Yang, H Chen - arXiv preprint arXiv \u2026, 2025", "abstract": "Trustworthiness is essential for the precise and interpretable application of artificial intelligence (AI) in medical imaging. Traditionally, precision and interpretability have been addressed as separate tasks, namely medical image analysis and explainable \u2026"}, {"title": "A foundation model for human-AI collaboration in medical literature mining", "link": "https://arxiv.org/pdf/2501.16255", "details": "Z Wang, L Cao, Q Jin, J Chan, N Wan, B Afzali, HJ Cho\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Systematic literature review is essential for evidence-based medicine, requiring comprehensive analysis of clinical trial publications. However, the application of artificial intelligence (AI) models for medical literature mining has been limited by \u2026"}, {"title": "A Sequential Optimal Learning Approach to Automated Prompt Engineering in Large Language Models", "link": "https://arxiv.org/pdf/2501.03508", "details": "S Wang, S Moazeni, D Klabjan - arXiv preprint arXiv:2501.03508, 2025", "abstract": "Designing effective prompts is essential to guiding large language models (LLMs) toward desired responses. Automated prompt engineering aims to reduce reliance on manual effort by streamlining the design, refinement, and optimization of natural \u2026"}]
