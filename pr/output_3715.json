[{"title": "Concise and Precise Context Compression for Tool-Using Language Models", "link": "https://arxiv.org/pdf/2407.02043", "details": "Y Xu, Y Feng, H Mu, Y Hou, Y Li, X Wang, W Zhong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Through reading the documentation in the context, tool-using language models can dynamically extend their capability using external tools. The cost is that we have to input lengthy documentation every time the model needs to use the tool, occupying \u2026"}, {"title": "Aligning Language Models with the Human World", "link": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi%3Farticle%3D1241%26context%3Ddissertations", "details": "R LIU - 2024", "abstract": "Abstract The field of Natural Language Processing (NLP) has undergone a significant transformation with the emergence of large language models (LMs). These models have enabled the development of human-like conversational \u2026"}, {"title": "Language Models Encode Collaborative Signals in Recommendation", "link": "https://arxiv.org/pdf/2407.05441", "details": "L Sheng, A Zhang, Y Zhang, Y Chen, X Wang, TS Chua - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent studies empirically indicate that language models (LMs) encode rich world knowledge beyond mere semantics, attracting significant attention across various fields. However, in the recommendation domain, it remains uncertain whether LMs \u2026"}, {"title": "Eliminating Position Bias of Language Models: A Mechanistic Approach", "link": "https://arxiv.org/pdf/2407.01100", "details": "Z Wang, H Zhang, X Li, KH Huang, C Han, S Ji\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Position bias has proven to be a prevalent issue of modern language models (LMs), where the models prioritize content based on its position within the given context. This bias often leads to unexpected model failures and hurts performance \u2026"}, {"title": "FamiCom: Further Demystifying Prompts for Language Models with Task-Agnostic Performance Estimation", "link": "https://arxiv.org/pdf/2406.11243", "details": "B Li, B Zhou, X Fu, F Wang, D Roth, M Chen - arXiv preprint arXiv:2406.11243, 2024", "abstract": "Language models have shown impressive in-context-learning capabilities, which allow them to benefit from input prompts and perform better on downstream end tasks. Existing works investigate the mechanisms behind this observation, and \u2026"}, {"title": "Securing Multi-turn Conversational Language Models Against Distributed Backdoor Triggers", "link": "https://arxiv.org/pdf/2407.04151", "details": "T Tong, J Xu, Q Liu, M Chen - arXiv preprint arXiv:2407.04151, 2024", "abstract": "The security of multi-turn conversational large language models (LLMs) is understudied despite it being one of the most popular LLM utilization. Specifically, LLMs are vulnerable to data poisoning backdoor attacks, where an adversary \u2026"}, {"title": "An Empirical Comparison of Vocabulary Expansion and Initialization Approaches for Language Models", "link": "https://arxiv.org/pdf/2407.05841", "details": "N Mundra, AN Kishore, R Dabre, R Puduppully\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language Models (LMs) excel in natural language processing tasks for English but show reduced performance in most other languages. This problem is commonly tackled by continually pre-training and fine-tuning these models for said languages \u2026"}, {"title": "Advancing DNA Language Models through Motif-Oriented Pre-Training with MoDNA", "link": "https://www.mdpi.com/2673-7426/4/2/85", "details": "W An, Y Guo, Y Bian, H Ma, J Yang, C Li, J Huang - BioMedInformatics, 2024", "abstract": "Acquiring meaningful representations of gene expression is essential for the accurate prediction of downstream regulatory tasks, such as identifying promoters and transcription factor binding sites. However, the current dependency on \u2026"}, {"title": "Associative Recurrent Memory Transformer", "link": "https://arxiv.org/pdf/2407.04841", "details": "I Rodkin, Y Kuratov, A Bulatov, M Burtsev - arXiv preprint arXiv:2407.04841, 2024", "abstract": "This paper addresses the challenge of creating a neural architecture for very long sequences that requires constant time for processing new information at each time step. Our approach, Associative Recurrent Memory Transformer (ARMT), is based on \u2026"}]
