[{"title": "Trained Models Tell Us How to Make Them Robust to Spurious Correlation without Group Annotation", "link": "https://arxiv.org/pdf/2410.05345", "details": "M Ghaznavi, H Asadollahzadeh, FH Noohdani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Classifiers trained with Empirical Risk Minimization (ERM) tend to rely on attributes that have high spurious correlation with the target. This can degrade the performance on underrepresented (or'minority') groups that lack these attributes, posing significant \u2026"}, {"title": "Data Selection via Optimal Control for Language Models", "link": "https://arxiv.org/pdf/2410.07064", "details": "Y Gu, L Dong, H Wang, Y Hao, Q Dong, F Wei\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved \u2026"}, {"title": "Probing Language Models on Their Knowledge Source", "link": "https://arxiv.org/pdf/2410.05817", "details": "Z Tighidet, A Mogini, J Mei, B Piwowarski, P Gallinari - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) often encounter conflicts between their learned, internal (parametric knowledge, PK) and external knowledge provided during inference (contextual knowledge, CK). Understanding how LLMs models prioritize \u2026"}, {"title": "Compositional Risk Minimization", "link": "https://arxiv.org/pdf/2410.06303", "details": "D Mahajan, M Pezeshki, I Mitliagkas, K Ahuja\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we tackle a challenging and extreme form of subpopulation shift, which is termed compositional shift. Under compositional shifts, some combinations of attributes are totally absent from the training distribution but present in the test \u2026"}, {"title": "Less is More: Selective reduction of CT data for self-supervised pre-training of deep learning models with contrastive learning improves downstream classification \u2026", "link": "https://www.sciencedirect.com/science/article/pii/S0010482524013271", "details": "D Wolf, T Payer, CS Lisson, CG Lisson, M Beer, M G\u00f6tz\u2026 - Computers in Biology and \u2026, 2024", "abstract": "Background: Self-supervised pre-training of deep learning models with contrastive learning is a widely used technique in image analysis. Current findings indicate a strong potential for contrastive pre-training on medical images. However, further \u2026"}, {"title": "ConML: A Universal Meta-Learning Framework with Task-Level Contrastive Learning", "link": "https://arxiv.org/pdf/2410.05975", "details": "S Wu, Y Wang, Y Bian, Q Yao - arXiv preprint arXiv:2410.05975, 2024", "abstract": "Meta-learning enables learning systems to adapt quickly to new tasks, similar to humans. To emulate this human-like rapid learning and enhance alignment and discrimination abilities, we propose ConML, a universal meta-learning framework \u2026"}, {"title": "Boosting Accuracy of Differentially Private Continuous Data Release for Federated Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10711967/", "details": "J Cai, Q Ye, H Hu, X Liu, Y Fu - IEEE Transactions on Information Forensics and \u2026, 2024", "abstract": "Incorporating differentially private continuous data release (DPCR) into private federated learning (FL) has recently emerged as a powerful technique for enhancing accuracy. Designing an effective DPCR model is the key to improving accuracy. Still \u2026"}, {"title": "LLM Embeddings Improve Test-time Adaptation to Tabular $ Y| X $-Shifts", "link": "https://arxiv.org/pdf/2410.07395", "details": "Y Zeng, J Liu, H Lam, H Namkoong - arXiv preprint arXiv:2410.07395, 2024", "abstract": "For tabular datasets, the change in the relationship between the label and covariates ($ Y| X $-shifts) is common due to missing variables (aka confounders). Since it is impossible to generalize to a completely new and unknown domain, we study \u2026"}, {"title": "Counterfactual Causal Inference in Natural Language with Large Language Models", "link": "https://arxiv.org/pdf/2410.06392", "details": "G Gendron, JM Ro\u017eanec, M Witbrock, G Dobbie - arXiv preprint arXiv:2410.06392, 2024", "abstract": "Causal structure discovery methods are commonly applied to structured data where the causal variables are known and where statistical testing can be used to assess the causal relationships. By contrast, recovering a causal structure from unstructured \u2026"}]
