[{"title": "LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation", "link": "https://arxiv.org/pdf/2501.05414", "details": "X Ye, F Yin, Y He, J Zhang, H Yen, T Gao, G Durrett\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing benchmarks for evaluating long-context language models (LCLMs) primarily focus on long-context recall, requiring models to produce short responses based on a few critical snippets while processing thousands of irrelevant tokens. We introduce \u2026"}, {"title": "Adaptive Concept Bottleneck for Foundation Models Under Distribution Shifts", "link": "https://arxiv.org/pdf/2412.14097%3F", "details": "J Choi, J Raghuram, Y Li, S Jha - arXiv preprint arXiv:2412.14097, 2024", "abstract": "Advancements in foundation models (FMs) have led to a paradigm shift in machine learning. The rich, expressive feature representations from these pre-trained, large- scale FMs are leveraged for multiple downstream tasks, usually via lightweight fine \u2026"}, {"title": "DnDScore: Decontextualization and Decomposition for Factuality Verification in Long-Form Text Generation", "link": "https://arxiv.org/pdf/2412.13175", "details": "M Wanner, B Van Durme, M Dredze - arXiv preprint arXiv:2412.13175, 2024", "abstract": "The decompose-then-verify strategy for verification of Large Language Model (LLM) generations decomposes claims that are then independently verified. Decontextualization augments text (claims) to ensure it can be verified outside of the \u2026"}, {"title": "CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology", "link": "https://arxiv.org/pdf/2412.12077", "details": "Y Sun, Y Si, C Zhu, X Gong, K Zhang, P Chen, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence of large multimodal models (LMMs) has brought significant advancements to pathology. Previous research has primarily focused on separately training patch-level and whole-slide image (WSI)-level models, limiting the \u2026"}, {"title": "Adaptive Pruning for Large Language Models with Structural Importance Awareness", "link": "https://arxiv.org/pdf/2412.15127", "details": "H Zheng, J Ren, Y Sun, R Zhang, W Zhang, Z Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The recent advancements in large language models (LLMs) have significantly improved language understanding and generation capabilities. However, it is difficult to deploy LLMs on resource-constrained edge devices due to their high \u2026"}]
