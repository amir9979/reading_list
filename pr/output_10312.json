[{"title": "A strategy for cost-effective large language model use at health system-scale", "link": "https://www.nature.com/articles/s41746-024-01315-1", "details": "E Klang, D Apakama, EE Abbott, A Vaid, J Lampert\u2026 - npj Digital Medicine, 2024", "abstract": "Large language models (LLMs) can optimize clinical workflows; however, the economic and computational challenges of their utilization at the health system scale are underexplored. We evaluated how concatenating queries with multiple clinical \u2026"}, {"title": "ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models", "link": "https://arxiv.org/pdf/2412.00447", "details": "X Ye, Y Gan, Y Ge, XP Zhang, Y Tang - arXiv preprint arXiv:2412.00447, 2024", "abstract": "Large Vision Language Models (LVLMs) have achieved significant success across multi-modal tasks. However, the computational cost of processing long visual tokens can be prohibitively expensive on resource-limited devices. Previous methods have \u2026"}, {"title": "Exploring Visual Multiple-Choice Question Answering with Pre-trained Vision-Language Models", "link": "https://openaccess.thecvf.com/content/ACCV2024W/LAVA/papers/Tran_Exploring_Visual_Multiple-Choice_Question_Answering_with_Pre-trained_Vision-Language_Models_ACCVW_2024_paper.pdf", "details": "GN Tran, DT Luu - Proceedings of the Asian Conference on Computer \u2026, 2024", "abstract": "Visual question answering is a challenging task in computer vision and natural language processing that involves answering questions about an image using both visual and textual information. This task is more challenging when it comes to the \u2026"}, {"title": "Domain Aware Multi-Task Pre-Training of 3D Swin Transformer for Brain MRI", "link": "https://openaccess.thecvf.com/content/ACCV2024/papers/Kim_Domain_Aware_Multi-Task_Pre-Training_of_3D_Swin_Transformer_for_Brain_ACCV_2024_paper.pdf", "details": "J Kim, M Kim, H Park - Proceedings of the Asian Conference on Computer \u2026, 2024", "abstract": "The scarcity of annotated medical images is a major bottleneck in developing learning models for medical image analysis. Hence, recent studies have focused on pretrained models with fewer annotation requirements that can be fine-tuned for \u2026"}, {"title": "Text-Guided Zero-Shot 3D Style Transfer of Neural Radiance Fields", "link": "https://link.springer.com/chapter/10.1007/978-3-031-78186-5_9", "details": "W Li, WS Zheng - International Conference on Pattern Recognition, 2024", "abstract": "Abstract 3D style transfer aims to generate novel, stylized views while maintaining multi-view consistency. However, current approaches primarily focus on uniformly stylizing entire 3D scenes, limiting the versatility of 3D style transfer. To address this \u2026"}, {"title": "Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training", "link": "https://arxiv.org/pdf/2411.15207", "details": "A Bawazir, K Wu, W Li - arXiv preprint arXiv:2411.15207, 2024", "abstract": "Recent advancements in vision-language pre-training via contrastive learning have significantly improved performance across computer vision tasks. However, in the medical domain, obtaining multimodal data is often costly and challenging due to \u2026"}, {"title": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos", "link": "https://arxiv.org/pdf/2411.14901", "details": "T Hannan, MM Islam, J Gu, T Seidl, G Bertasius - arXiv preprint arXiv:2411.14901, 2024", "abstract": "Large language models (LLMs) excel at retrieving information from lengthy text, but their vision-language counterparts (VLMs) face difficulties with hour-long videos, especially for temporal grounding. Specifically, these VLMs are constrained by frame \u2026"}, {"title": "Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers", "link": "https://arxiv.org/pdf/2412.00142", "details": "C Mitra, B Huang, T Chai, Z Lin, A Arbelle, R Feris\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a wide variety of vision-language (VL) tasks such as image captioning or visual question answering. Despite strong performance, LMMs are not directly suited for \u2026"}, {"title": "Med-2E3: A 2D-Enhanced 3D Medical Multimodal Large Language Model", "link": "https://arxiv.org/pdf/2411.12783", "details": "Y Shi, X Zhu, Y Hu, C Guo, M Li, J Wu - arXiv preprint arXiv:2411.12783, 2024", "abstract": "The analysis of 3D medical images is crucial for modern healthcare, yet traditional task-specific models are becoming increasingly inadequate due to limited generalizability across diverse clinical scenarios. Multimodal large language models \u2026"}]
