[{"title": "Reasoning Language Models: A Blueprint", "link": "https://arxiv.org/pdf/2501.11223", "details": "M Besta, J Barth, E Schreiber, A Kubicek, A Catarino\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have redefined AI's problem-solving capabilities by extending large language models \u2026"}, {"title": "Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits on Large Audio Language Models in Jailbreak", "link": "https://arxiv.org/pdf/2501.13772", "details": "E Xiao, H Cheng, J Shao, J Duan, K Xu, L Yang, J Gu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) demonstrate remarkable zero-shot performance across various natural language processing tasks. The integration of multimodal encoders extends their capabilities, enabling the development of Multimodal Large \u2026"}, {"title": "Spurious Forgetting in Continual Learning of Language Models", "link": "https://arxiv.org/pdf/2501.13453", "details": "J Zheng, X Cai, S Qiu, Q Ma - arXiv preprint arXiv:2501.13453, 2025", "abstract": "Recent advancements in large language models (LLMs) reveal a perplexing phenomenon in continual learning: despite extensive training, models experience significant performance declines, raising questions about task alignment and \u2026"}, {"title": "LLM360 K2: Scaling Up 360-Open-Source Large Language Models", "link": "https://arxiv.org/pdf/2501.07124", "details": "Z Liu, B Tan, H Wang, W Neiswanger, T Tao, H Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree OPEN SOURCE approach to the largest and most powerful models under project LLM360. While open-source LLMs continue to advance, the answer to\" How are the \u2026"}, {"title": "Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful Behaviors with Proximity Constraints", "link": "https://arxiv.org/pdf/2501.08246", "details": "J N\u00f6ther, A Singla, G Radanovi\u0107 - arXiv preprint arXiv:2501.08246, 2025", "abstract": "Recent work has proposed automated red-teaming methods for testing the vulnerabilities of a given target large language model (LLM). These methods use red- teaming LLMs to uncover inputs that induce harmful behavior in a target LLM. In this \u2026"}, {"title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models", "link": "https://arxiv.org/pdf/2501.04945", "details": "Q Ren, J Zeng, Q He, J Liang, Y Xiao, W Zhou, Z Sun\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "It is crucial for large language models (LLMs) to follow instructions that involve multiple constraints. However, soft constraints are semantically related and difficult to verify through automated methods. These constraints remain a significant challenge \u2026"}, {"title": "Disentangling Exploration of Large Language Models by Optimal Exploitation", "link": "https://arxiv.org/pdf/2501.08925", "details": "T Grams, P Betz, C Bartelt - arXiv preprint arXiv:2501.08925, 2025", "abstract": "Exploration is a crucial skill for self-improvement and open-ended problem-solving. However, it remains uncertain whether large language models can effectively explore the state-space. Existing evaluations predominantly focus on the trade-off \u2026"}, {"title": "Redundancy Principles for MLLMs Benchmarks", "link": "https://www.researchgate.net/profile/Zicheng-Zhang-9/publication/388231002_Redundancy_Principles_for_MLLMs_Benchmarks/links/67905d1075d4ab477e55efa4/Redundancy-Principles-for-MLLMs-Benchmarks.pdf", "details": "Z Zhang, X Zhao, X Fang, C Li, X Liu, X Min, H Duan\u2026", "abstract": "With the rapid iteration of Multi-modality Large Language Models (MLLMs) and the evolving demands of the field, the number of benchmarks produced annually has surged into the hundreds. The rapid growth has inevitably led to significant \u2026"}, {"title": "Fine-tuning Large Language Models for Improving Factuality in Legal Question Answering", "link": "https://arxiv.org/pdf/2501.06521", "details": "Y Hu, L Gan, W Xiao, K Kuang, F Wu - arXiv preprint arXiv:2501.06521, 2025", "abstract": "Hallucination, or the generation of incorrect or fabricated information, remains a critical challenge in large language models (LLMs), particularly in high-stake domains such as legal question answering (QA). In order to mitigate the hallucination \u2026"}]
