'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [al-BERT: a semi-supervised denoising technique for di'
[{"title": "NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli", "link": "https://arxiv.org/pdf/2405.02814", "details": "X Wang, C Li, Y Chang, J Wang, Y Wu - arXiv preprint arXiv:2405.02814, 2024", "abstract": "Large Language Models (LLMs) have become integral to a wide spectrum of applications, ranging from traditional computing tasks to advanced artificial intelligence (AI) applications. This widespread adoption has spurred extensive \u2026"}, {"title": "Pre-training Concept Frequency is predictive of CLIP Zero-shot Performance", "link": "https://openreview.net/pdf%3Fid%3D55iCzZ1TtD", "details": "V Udandarao, A Prabhu, P Torr, A Bibi, S Albanie\u2026 - ICLR 2024 Workshop on \u2026", "abstract": "Web-crawled pre-training datasets are speculated to be key drivers of zero-shot generalization abilities of Vision-Language Models (VLMs) like CLIP, across a range of downstream classification and retrieval tasks, spanning diverse visual concepts \u2026"}]
