[{"title": "Negative-Prompt-driven Alignment for Generative Language Model", "link": "https://arxiv.org/pdf/2410.12194", "details": "S Qiao, N Xv, B Liu, X Geng - arXiv preprint arXiv:2410.12194, 2024", "abstract": "Large language models have achieved remarkable capabilities, but aligning their outputs with human values and preferences remains a significant challenge. Existing alignment methods primarily focus on positive examples while overlooking the \u2026"}, {"title": "Prompt Learning for Few-Shot Question Answering via Self-Context Data Augmentation", "link": "https://ieeexplore.ieee.org/abstract/document/10723112/", "details": "JQ Qiu, CY Zhang, CLP Chen - IEEE Transactions on Artificial Intelligence, 2024", "abstract": "Pre-trained language models (PLMs) have shown remarkable performance on question answering (QA) tasks, but they usually require fine-tuning that depends on a substantial quantity of QA pairs. Therefore, improving the performance of PLMs in \u2026"}, {"title": "Probing-RAG: Self-Probing to Guide Language Models in Selective Document Retrieval", "link": "https://arxiv.org/pdf/2410.13339", "details": "I Baek, H Chang, B Kim, J Lee, H Lee - arXiv preprint arXiv:2410.13339, 2024", "abstract": "Retrieval-Augmented Generation (RAG) enhances language models by retrieving and incorporating relevant external knowledge. However, traditional retrieve-and- generate processes may not be optimized for real-world scenarios, where queries \u2026"}, {"title": "Tuning Language Models by Mixture-of-Depths Ensemble", "link": "https://arxiv.org/pdf/2410.13077", "details": "H Luo, L Specia - arXiv preprint arXiv:2410.13077, 2024", "abstract": "Transformer-based Large Language Models (LLMs) traditionally rely on final-layer loss for training and final-layer representations for predictions, potentially overlooking the predictive power embedded in intermediate layers. Surprisingly, we \u2026"}, {"title": "TabMedBERT: A Tabular Knowledge Enhanced Biomedical Pretrained Language Model", "link": "https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA240674", "details": "X Yan, L Geng, Z Cao, J Li, W Li, S Li, X Zhou, Y Yang\u2026 - ECAI 2024, 2024", "abstract": "Most existing biomedical language models are trained on plain text with general learning goals such as random word infilling, failing to capture the knowledge in the biomedical corpus sufficiently. Since biomedical articles usually contain many tables \u2026"}, {"title": "A General-Purpose Multimodal Foundation Model for Dermatology", "link": "https://arxiv.org/pdf/2410.15038", "details": "S Yan, Z Yu, C Primiero, C Vico-Alonso, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Diagnosing and treating skin diseases require advanced visual skills across multiple domains and the ability to synthesize information from various imaging modalities. Current deep learning models, while effective at specific tasks such as diagnosing \u2026"}, {"title": "Medical Information Extraction with Large Language Models", "link": "https://re.public.polimi.it/bitstream/11311/1275692/3/Medical_Event_Extraction_LLMs___ICNLSP_2024.pdf", "details": "R Fornasiere, N Brunello, V Scotti, MJ Carman - Proceedings of the 7th \u2026, 2024", "abstract": "The rapid increase in clinical text data due to the widespread adoption of electronic health records offers significant benefits for medical practice and introduces new challenges in automatic data extraction. Since manual extrac-tion is often inefficient \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers", "link": "https://openreview.net/pdf%3Fid%3D67tRrjgzsh", "details": "X Lu, Y Zhao, B Qin, L Huo, Q Yang, D Xu - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Demystifying Large Language Models for Medicine: A Primer", "link": "https://arxiv.org/pdf/2410.18856", "details": "Q Jin, N Wan, R Leaman, S Tian, Z Wang, Y Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) represent a transformative class of AI tools capable of revolutionizing various aspects of healthcare by generating human-like responses across diverse contexts and adapting to novel tasks following human instructions \u2026"}]
