'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Simple linear attention language models balance the r'
[{"title": "Grounding Language Models for Visual Entity Recognition", "link": "https://arxiv.org/pdf/2402.18695", "details": "Z Xiao, M Gong, P Cascante-Bonilla, X Zhang, J Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of \u2026"}, {"title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.07860", "details": "S Zhao, S Hao, B Zi, H Xu, KYK Wong - arXiv preprint arXiv:2403.07860, 2024", "abstract": "Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding \u2026"}, {"title": "Synth $^ 2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings", "link": "https://arxiv.org/pdf/2403.07750", "details": "S Sharifzadeh, C Kaplanis, S Pathak, D Kumaran, A Ilic\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models \u2026"}, {"title": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models", "link": "https://arxiv.org/html/2402.19427v1", "details": "S De, SL Smith, A Fernando, A Botev\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated \u2026"}, {"title": "Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models", "link": "https://arxiv.org/html/2403.12964v1", "details": "C Zhang, S Stepputtis, K Sycara, Y Xie - arXiv preprint arXiv:2403.12964, 2024", "abstract": "Recently, large-scale pre-trained Vision-Language Models (VLMs) have demonstrated great potential in learning open-world visual representations, and exhibit remarkable performance across a wide range of downstream tasks through \u2026"}, {"title": "BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English", "link": "https://arxiv.org/pdf/2403.10900", "details": "S Shafayat, HM Hasan, MRC Mahim, RA Putri\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this study, we introduce BEnQA, a dataset comprising parallel Bengali and English exam questions for middle and high school levels in Bangladesh. Our dataset consists of approximately 5K questions covering several subjects in science with \u2026"}, {"title": "$\\mathbf {(N, K)} $-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model", "link": "https://arxiv.org/html/2403.07191v1", "details": "Y Zhang, L Chen, B Liu, Y Yang, Q Cui, Y Tao, H Yang - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these \u2026"}, {"title": "An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models", "link": "https://arxiv.org/html/2403.09766v1", "details": "H Luo, J Gu, F Liu, P Torr - arXiv preprint arXiv:2403.09766, 2024", "abstract": "Different from traditional task-specific vision models, recent large VLMs can readily adapt to different vision tasks by simply using different textual instructions, ie, prompts. However, a well-known concern about traditional task-specific vision \u2026"}, {"title": "ZVQAF: Zero-shot visual question answering with feedback from large language models", "link": "https://www.sciencedirect.com/science/article/pii/S0925231224002765", "details": "C Liu, C Wang, Y Peng, Z Li - Neurocomputing, 2024", "abstract": "Due to the prominent zero-shot generalization in new language tasks shown by large language models (LLMs), applying LLMs for zero-shot visual question answering (VQA) has been a new trend. However, most prior approaches directly use off-the \u2026"}]
