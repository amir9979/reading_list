'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Can 3D Vision-Language Models Truly Understand Natural'
[{"title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models", "link": "https://arxiv.org/pdf/2403.08281", "details": "N Ding, Y Chen, G Cui, X Lv, R Xie, B Zhou, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three \u2026"}, {"title": "Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts", "link": "https://arxiv.org/html/2403.12918v1", "details": "SA Somayajula, Y Liang, A Singh, L Zhang, P Xie - arXiv preprint arXiv:2403.12918, 2024", "abstract": "Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle \u2026"}, {"title": "Do Not Worry if You Do Not Have Data: Building Pretrained Language Models Using Translationese", "link": "https://arxiv.org/pdf/2403.13638", "details": "M Doshi, R Dabre, P Bhattacharyya - arXiv preprint arXiv:2403.13638, 2024", "abstract": "In this paper, we explore the utility of\\textit {Translationese} as synthetic data created using machine translation for pre-training language models (LMs). Pre-training requires vast amounts of monolingual data, which is mostly unavailable for \u2026"}, {"title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.07860", "details": "S Zhao, S Hao, B Zi, H Xu, KYK Wong - arXiv preprint arXiv:2403.07860, 2024", "abstract": "Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding \u2026"}, {"title": "Quiet-star: Language models can teach themselves to think before speaking", "link": "https://arxiv.org/html/2403.09629v1", "details": "E Zelikman, G Harik, Y Shao, V Jayasiri, N Haber\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "When writing and talking, people sometimes pause to think. Although reasoning- focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models", "link": "https://arxiv.org/pdf/2403.02436", "details": "X Lu, Y Zhao, B Qin - arXiv preprint arXiv:2403.02436, 2024", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Language models scale reliably with over-training and on downstream tasks", "link": "https://arxiv.org/pdf/2403.08540", "details": "SY Gadre, G Smyrnis, V Shankar, S Gururangan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute \u2026"}, {"title": "CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following", "link": "https://arxiv.org/pdf/2403.03129", "details": "K Zhang, J Wang, E Hua, B Qi, N Ding, B Zhou - arXiv preprint arXiv:2403.03129, 2024", "abstract": "With the advancement of language models (LMs), their exposure to private data is increasingly inevitable, and their deployment (especially for smaller ones) on personal devices, such as PCs and smartphones, has become a prevailing trend. In \u2026"}, {"title": "Data-Efficient Sleep Staging with Synthetic Time Series Pretraining", "link": "https://arxiv.org/html/2403.08592v1", "details": "N Grieger, S Mehrkanoon, S Bialonski - arXiv preprint arXiv:2403.08592, 2024", "abstract": "Analyzing electroencephalographic (EEG) time series can be challenging, especially with deep neural networks, due to the large variability among human subjects and often small datasets. To address these challenges, various strategies, such as self \u2026"}]
