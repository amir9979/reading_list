[{"title": "The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models", "link": "https://arxiv.org/pdf/2411.08870%3F", "details": "DP Jeong, P Mani, S Garg, ZC Lipton, M Oberst - arXiv preprint arXiv:2411.08870, 2024", "abstract": "Several recent works seek to develop foundation models specifically for medical applications, adapting general-purpose large language models (LLMs) and vision- language models (VLMs) via continued pretraining on publicly available biomedical \u2026"}, {"title": "Stronger Than You Think: Benchmarking Weak Supervision on Realistic Tasks", "link": "https://openreview.net/pdf%3Fid%3Dc7SApXZz4b", "details": "T Zhang, L Cai, J Li, N Roberts, N Guha, F Sala - The Thirty-eight Conference on Neural \u2026", "abstract": "Weak supervision (WS) is a popular approach for label-efficient learning, leveraging diverse sources of noisy but inexpensive* weak labels* to automatically annotate training data. Despite heavy usage, the value of WS is challenging to benchmark due \u2026"}, {"title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models", "link": "https://arxiv.org/pdf/2410.18785%3F", "details": "Q Li, X Liu, Z Tang, P Dong, Z Li, X Pan, X Chu - arXiv preprint arXiv:2410.18785, 2024", "abstract": "Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality, with many methods excelling across these criteria. Some \u2026"}, {"title": "Automatic TNM staging of colorectal cancer radiology reports using pre-trained language models", "link": "https://www.sciencedirect.com/science/article/pii/S016926072400508X", "details": "M Chizhikova, P L\u00f3pez-\u00dabeda, T Mart\u00edn-Noguerol\u2026 - Computer Methods and \u2026, 2024", "abstract": "Abstract Background and Objective: Colorectal cancer is one of the major causes of cancer death worldwide. Essential for prognosis and treatment planning, TNM staging offers critical insights into the advancement of colorectal cancer. However \u2026"}, {"title": "CriteriaMapper: establishing the automatic identification of clinical trial cohorts from electronic health records by matching normalized eligibility criteria and patient \u2026", "link": "https://www.nature.com/articles/s41598-024-77447-x", "details": "K Lee, Y Mai, Z Liu, K Raja, T Jun, M Ma, T Wang, L Ai\u2026 - Scientific Reports, 2024", "abstract": "The use of electronic health records (EHRs) holds the potential to enhance clinical trial activities. However, the identification of eligible patients within EHRs presents considerable challenges. We aimed to develop a CriteriaMapper system for \u2026"}, {"title": "Comparing Commercial and Open-Source Large Language Models for Labeling Chest Radiograph Reports", "link": "https://pubs.rsna.org/doi/abs/10.1148/radiol.241139", "details": "FJ Dorfner, L J\u00fcrgensen, L Donle, F Al Mohamad\u2026 - Radiology, 2024", "abstract": "Background Rapid advances in large language models (LLMs) have led to the development of numerous commercial and open-source models. While recent publications have explored OpenAI's GPT-4 to extract information of interest from \u2026"}, {"title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "link": "https://openreview.net/pdf%3Fid%3DIRXyPm9IPW", "details": "X Wu, S Huang, G Wang, J Xiong, F Wei - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts. Despite these advances \u2026"}, {"title": "Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language Models", "link": "https://openreview.net/pdf%3Fid%3Dnvn80cscVm", "details": "L Wei, Z Tan, C Li, J Wang, W Huang - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Large Language Models (LLMs) have transformed natural language processing and extended their powerful capabilities to multi-modal domains. As LLMs continue to advance, it is crucial to develop diverse and appropriate metrics for their evaluation \u2026"}, {"title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models", "link": "https://arxiv.org/pdf/2410.18252", "details": "M Noukhovitch, S Huang, S Xhonneux, A Hosseini\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this \u2026"}]
