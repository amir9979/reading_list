'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [An Efficient Approach for Studying Cross-Lingual Trans'
[{"title": "Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data", "link": "https://arxiv.org/pdf/2404.03862", "details": "J Zhang, M Marone, T Li, B Van Durme, D Khashabi - arXiv preprint arXiv:2404.03862, 2024", "abstract": "For humans to trust the fluent generations of large language models (LLMs), they must be able to verify their correctness against trusted, external sources. Recent efforts aim to increase verifiability through citations of retrieved documents or post \u2026"}, {"title": "Gaze-infused BERT: Do human gaze signals help pre-trained language models?", "link": "https://link.springer.com/article/10.1007/s00521-024-09725-8", "details": "B Wang, B Liang, L Zhou, R Xu - Neural Computing and Applications, 2024", "abstract": "This research delves into the intricate connection between self-attention mechanisms in large-scale pre-trained language models, like BERT, and human gaze patterns, with the aim of harnessing gaze information to enhance the performance of natural \u2026"}, {"title": "A Comparison of Parameter-Efficient ASR Domain Adaptation Methods for Universal Speech and Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10445894/", "details": "KC Sim, Z Huo, T Munkhdalai, N Siddhartha, A Stooke\u2026 - ICASSP 2024-2024 IEEE \u2026, 2024", "abstract": "A recent paradigm shift in artificial intelligence has seen the rise of foundation models, such as the large language models and the universal speech models. With billions of model parameters and trained with a wide range of data, these foundation \u2026"}, {"title": "Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback", "link": "https://arxiv.org/pdf/2404.14233", "details": "W Xiao, Z Huang, L Gan, W He, H Li, Z Yu, H Jiang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapidly developing Large Vision Language Models (LVLMs) have shown notable capabilities on a range of multi-modal tasks, but still face the hallucination phenomena where the generated texts do not align with the given contexts \u2026"}, {"title": "Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models", "link": "https://arxiv.org/pdf/2404.02823", "details": "H Sun, L Liu, J Li, F Wang, B Dong, R Lin, R Huang - arXiv preprint arXiv:2404.02823, 2024", "abstract": "The ability of large language models (LLMs) to follow instructions is crucial to real- world applications. Despite recent advances, several studies have highlighted that LLMs struggle when faced with challenging instructions, especially those that include \u2026"}, {"title": "Generative Language Models for Personalized Information Understanding", "link": "https://scholarworks.umass.edu/cgi/viewcontent.cgi%3Farticle%3D4123%26context%3Ddissertations_2", "details": "P Cai - 2024", "abstract": "A major challenge in information understanding stems from the diverse nature of the audience, where individuals possess varying preferences, experiences, educational and cultural backgrounds. Consequently, adopting a one-size-fits-all approach to \u2026"}, {"title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models", "link": "https://arxiv.org/pdf/2403.19647", "details": "S Marks, C Rager, EJ Michaud, Y Belinkov, D Bau\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic \u2026"}, {"title": "Learning To Guide Human Decision Makers With Vision-Language Models", "link": "https://arxiv.org/pdf/2403.16501", "details": "D Banerjee, S Teso, BS Grunel, A Passerini - arXiv preprint arXiv:2403.16501, 2024", "abstract": "There is increasing interest in developing AIs for assisting human decision making in\\textit {high-stakes} tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain.% Mainstream approaches team up an \u2026"}, {"title": "EHRFL: Federated Learning Framework for Heterogeneous EHRs and Precision-guided Selection of Participating Clients", "link": "https://arxiv.org/pdf/2404.13318", "details": "J Kim, J Kim, K Hur, E Choi - arXiv preprint arXiv:2404.13318, 2024", "abstract": "In this study, we provide solutions to two practical yet overlooked scenarios in federated learning for electronic health records (EHRs): firstly, we introduce EHRFL, a framework that facilitates federated learning across healthcare institutions with \u2026"}]
