[{"title": "Mutual Prompt Leaning for Vision Language Models", "link": "https://link.springer.com/article/10.1007/s11263-024-02243-z", "details": "S Long, Z Zhao, J Yuan, Z Tan, J Liu, J Feng, S Wang\u2026 - International Journal of \u2026, 2024", "abstract": "Large pre-trained vision language models (VLMs) have demonstrated impressive representation learning capabilities, but their transferability across various downstream tasks heavily relies on prompt learning. Since VLMs consist of text and \u2026"}, {"title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks", "link": "https://arxiv.org/pdf/2409.06173", "details": "G Chochlakis, NM Pandiyan, K Lerman, S Narayanan - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the dominant technique for performing natural language tasks, as it does not require updating the model parameters with gradient-based methods. ICL promises to\" \u2026"}, {"title": "Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records", "link": "https://arxiv.org/pdf/2409.07012", "details": "D Kyung, J Kim, T Kim, E Choi - arXiv preprint arXiv:2409.07012, 2024", "abstract": "Chest X-ray imaging (CXR) is an important diagnostic tool used in hospitals to assess patient conditions and monitor changes over time. Generative models, specifically diffusion-based models, have shown promise in generating realistic \u2026"}, {"title": "Interpreting and Controlling Linguistic Features in Multilingual Language Models", "link": "https://dspace.cuni.cz/bitstream/handle/20.500.11956/192821/140123221.pdf%3Fsequence%3D1", "details": "T Limisiewicz - 2024", "abstract": "Language models based on neural networks have become the foundation for solving diverse tasks, yet their inner workings remain opaque. This dissertation investigates which components of language models are crucial for representing and processing \u2026"}, {"title": "Cross-prompt Pre-finetuning of Language Models for Short Answer Scoring", "link": "https://www.researchsquare.com/article/rs-4929687/latest.pdf", "details": "H Funayama, Y Matsubayashi, Y Asazuma, T Mizumoto\u2026 - 2024", "abstract": "Abstract Automated Short Answer Scoring (SAS) is the task of automatically scoring a given input to a prompt based on rubrics and reference answers. Although SAS is useful in real-world applications, both rubrics and reference answers differ between \u2026"}, {"title": "An open-source framework for end-to-end analysis of electronic health record data", "link": "https://www.nature.com/articles/s41591-024-03214-0", "details": "L Heumos, P Ehmele, T Treis, J Upmeier zu Belzen\u2026 - Nature Medicine, 2024", "abstract": "With progressive digitalization of healthcare systems worldwide, large-scale collection of electronic health records (EHRs) has become commonplace. However, an extensible framework for comprehensive exploratory analysis that accounts for \u2026"}, {"title": "Search for Efficient Large Language Models", "link": "https://arxiv.org/pdf/2409.17372", "details": "X Shen, P Zhao, Y Gong, Z Kong, Z Zhan, Y Wu, M Lin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have long held sway in the realms of artificial intelligence research. Numerous efficient techniques, including weight pruning, quantization, and distillation, have been embraced to compress LLMs, targeting \u2026"}, {"title": "StressPrompt: Does Stress Impact Large Language Models and Human Performance Similarly?", "link": "https://arxiv.org/pdf/2409.17167", "details": "G Shen, D Zhao, A Bao, X He, Y Dong, Y Zeng - arXiv preprint arXiv:2409.17167, 2024", "abstract": "Human beings often experience stress, which can significantly influence their performance. This study explores whether Large Language Models (LLMs) exhibit stress responses similar to those of humans and whether their performance \u2026"}, {"title": "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness", "link": "https://arxiv.org/pdf/2409.17791", "details": "J Li, H Huang, Y Zhang, P Xu, X Chen, R Song, L Shi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, there has been significant interest in replacing the reward model in Reinforcement Learning with Human Feedback (RLHF) methods for Large Language Models (LLMs), such as Direct Preference Optimization (DPO) and its \u2026"}]
