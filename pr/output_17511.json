[{"title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning", "link": "https://arxiv.org/pdf/2505.12477", "details": "H Van Assel, M Ibrahim, T Biancalani, A Regev\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reconstruction and joint embedding have emerged as two leading paradigms in Self Supervised Learning (SSL). Reconstruction methods focus on recovering the original sample from a different view in input space. On the other hand, joint embedding \u2026", "entry_id": "http://arxiv.org/abs/2505.12477v1", "updated": "2025-05-18 15:54:55", "published": "2025-05-18 15:54:55", "authors": "Hugues Van Assel;Mark Ibrahim;Tommaso Biancalani;Aviv Regev;Randall Balestriero", "summary": "Reconstruction and joint embedding have emerged as two leading paradigms in\nSelf Supervised Learning (SSL). Reconstruction methods focus on recovering the\noriginal sample from a different view in input space. On the other hand, joint\nembedding methods align the representations of different views in latent space.\nBoth approaches offer compelling advantages, yet practitioners lack clear\nguidelines for choosing between them. In this work, we unveil the core\nmechanisms that distinguish each paradigm. By leveraging closed form solutions\nfor both approaches, we precisely characterize how the view generation process,\ne.g. data augmentation, impacts the learned representations. We then\ndemonstrate that, unlike supervised learning, both SSL paradigms require a\nminimal alignment between augmentations and irrelevant features to achieve\nasymptotic optimality with increasing sample size. Our findings indicate that\nin scenarios where these irrelevant features have a large magnitude, joint\nembedding methods are preferable because they impose a strictly weaker\nalignment condition compared to reconstruction based methods. These results not\nonly clarify the trade offs between the two paradigms but also substantiate the\nempirical success of joint embedding approaches on real world challenging\ndatasets.", "comment": "33 pages, 9 figures", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2505.12477v1;http://arxiv.org/pdf/2505.12477v1", "pdf_url": "http://arxiv.org/pdf/2505.12477v1"}, {"title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models", "link": "https://arxiv.org/pdf/2506.09937", "details": "Q Gu, Y Ju, S Sun, I Gilitschenski, H Nishimura, M Itkina\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out-of-the-box. To allow these policies to safely \u2026", "entry_id": "http://arxiv.org/abs/2506.09937v1", "updated": "2025-06-11 16:59:13", "published": "2025-06-11 16:59:13", "authors": "Qiao Gu;Yuanliang Ju;Shengxiang Sun;Igor Gilitschenski;Haruki Nishimura;Masha Itkina;Florian Shkurti", "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, $\\pi_0$,\nand $\\pi_0$-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.", "comment": "Project Page: https://vla-safe.github.io/", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.AI", "links": "http://arxiv.org/abs/2506.09937v1;http://arxiv.org/pdf/2506.09937v1", "pdf_url": "http://arxiv.org/pdf/2506.09937v1"}, {"title": "Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization", "link": "https://arxiv.org/pdf/2506.10920", "details": "O Shafran, A Geiger, M Geva - arXiv preprint arXiv:2506.10920, 2025", "abstract": "A central goal for mechanistic interpretability has been to identify the right units of analysis in large language models (LLMs) that causally explain their outputs. While early work focused on individual neurons, evidence that neurons often encode \u2026", "entry_id": "http://arxiv.org/abs/2506.10920v1", "updated": "2025-06-12 17:33:29", "published": "2025-06-12 17:33:29", "authors": "Or Shafran;Atticus Geiger;Mor Geva", "summary": "A central goal for mechanistic interpretability has been to identify the\nright units of analysis in large language models (LLMs) that causally explain\ntheir outputs. While early work focused on individual neurons, evidence that\nneurons often encode multiple concepts has motivated a shift toward analyzing\ndirections in activation space. A key question is how to find directions that\ncapture interpretable features in an unsupervised manner. Current methods rely\non dictionary learning with sparse autoencoders (SAEs), commonly trained over\nresidual stream activations to learn directions from scratch. However, SAEs\noften struggle in causal evaluations and lack intrinsic interpretability, as\ntheir learning is not explicitly tied to the computations of the model. Here,\nwe tackle these limitations by directly decomposing MLP activations with\nsemi-nonnegative matrix factorization (SNMF), such that the learned features\nare (a) sparse linear combinations of co-activated neurons, and (b) mapped to\ntheir activating inputs, making them directly interpretable. Experiments on\nLlama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs\nand a strong supervised baseline (difference-in-means) on causal steering,\nwhile aligning with human-interpretable concepts. Further analysis reveals that\nspecific neuron combinations are reused across semantically-related features,\nexposing a hierarchical structure in the MLP's activation space. Together,\nthese results position SNMF as a simple and effective tool for identifying\ninterpretable features and dissecting concept representations in LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2506.10920v1;http://arxiv.org/pdf/2506.10920v1", "pdf_url": "http://arxiv.org/pdf/2506.10920v1"}, {"title": "Eye Guided Multimodal Fusion: Toward an Adaptive Learning Framework Using Explainable Artificial Intelligence", "link": "https://www.preprints.org/frontend/manuscript/8204af102f92a6cfd6901560075439e2/download_pub", "details": "S Moradizeyveh, A Hanif, S Liu, Y Qi, A Beheshti\u2026 - 2025", "abstract": "Interpreting diagnostic imaging and identifying relevant features in healthcare present significant challenges. For novices, the risk of misdiagnosis can be overwhelming, particularly in the absence of structured guidance and supervision \u2026"}, {"title": "Attention, Please! Revisiting Attentive Probing for Masked Image Modeling", "link": "https://arxiv.org/pdf/2506.10178", "details": "B Psomas, D Christopoulos, E Baltzi, I Kakogeorgiou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As fine-tuning (FT) becomes increasingly impractical at scale, probing is emerging as the preferred evaluation protocol for self-supervised learning (SSL). Yet, the standard linear probing (LP) fails to adequately reflect the potential of models trained with \u2026", "entry_id": "http://arxiv.org/abs/2506.10178v1", "updated": "2025-06-11 21:10:26", "published": "2025-06-11 21:10:26", "authors": "Bill Psomas;Dionysis Christopoulos;Eirini Baltzi;Ioannis Kakogeorgiou;Tilemachos Aravanis;Nikos Komodakis;Konstantinos Karantzalos;Yannis Avrithis;Giorgos Tolias", "summary": "As fine-tuning (FT) becomes increasingly impractical at scale, probing is\nemerging as the preferred evaluation protocol for self-supervised learning\n(SSL). Yet, the standard linear probing (LP) fails to adequately reflect the\npotential of models trained with Masked Image Modeling (MIM), due to the\ndistributed nature of patch tokens. This motivates the need for attentive\nprobing, an alternative that uses attention to selectively aggregate\npatch-level features. Despite its growing adoption, attentive probing remains\nunder-explored, with existing methods suffering from excessive parameterization\nand poor computational efficiency.\n  In this work, we revisit attentive probing through the lens of the\naccuracy-efficiency trade-off. We conduct a systematic study of existing\nmethods, analyzing their mechanisms and benchmarking their performance. We\nintroduce efficient probing (EP), a multi-query cross-attention mechanism that\neliminates redundant projections, reduces the number of trainable parameters,\nand achieves up to a 10$\\times$ speed-up over conventional multi-head\nattention. Despite its simplicity, EP outperforms LP and prior attentive\nprobing approaches across seven benchmarks, generalizes well beyond MIM to\ndiverse pre-training paradigms, produces interpretable attention maps, and\nachieves strong gains in low-shot and layer-wise settings. Code available at\nhttps://github.com/billpsomas/efficient-probing.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.10178v1;http://arxiv.org/pdf/2506.10178v1", "pdf_url": "http://arxiv.org/pdf/2506.10178v1"}, {"title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning", "link": "https://arxiv.org/pdf/2506.09985", "details": "M Assran, A Bardes, D Fan, Q Garrido, R Howes\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot \u2026", "entry_id": "http://arxiv.org/abs/2506.09985v1", "updated": "2025-06-11 17:57:09", "published": "2025-06-11 17:57:09", "authors": "Mido Assran;Adrien Bardes;David Fan;Quentin Garrido;Russell Howes;Mojtaba;Komeili;Matthew Muckley;Ammar Rizvi;Claire Roberts;Koustuv Sinha;Artem Zholus;Sergio Arnaud;Abha Gejji;Ada Martin;Francois Robert Hogan;Daniel Dugas;Piotr Bojanowski;Vasil Khalidov;Patrick Labatut;Francisco Massa;Marc Szafraniec;Kapil Krishnakumar;Yong Li;Xiaodong Ma;Sarath Chandar;Franziska Meier;Yann LeCun;Michael Rabbat;Nicolas Ballas", "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.", "comment": "48 pages, 19 figures", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CV;cs.LG;cs.RO", "links": "http://arxiv.org/abs/2506.09985v1;http://arxiv.org/pdf/2506.09985v1", "pdf_url": "http://arxiv.org/pdf/2506.09985v1"}, {"title": "From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models", "link": "https://arxiv.org/pdf/2506.09930", "details": "I Fang, J Zhang, S Tong, C Feng - arXiv preprint arXiv:2506.09930, 2025", "abstract": "One promise that Vision-Language-Action (VLA) models hold over traditional imitation learning for robotics is to leverage the broad generalization capabilities of large Vision-Language Models (VLMs) to produce versatile,\" generalist\" robot \u2026", "entry_id": "http://arxiv.org/abs/2506.09930v1", "updated": "2025-06-11 16:52:18", "published": "2025-06-11 16:52:18", "authors": "Irving Fang;Juexiao Zhang;Shengbang Tong;Chen Feng", "summary": "One promise that Vision-Language-Action (VLA) models hold over traditional\nimitation learning for robotics is to leverage the broad generalization\ncapabilities of large Vision-Language Models (VLMs) to produce versatile,\n\"generalist\" robot policies. However, current evaluations of VLAs remain\ninsufficient. Traditional imitation learning benchmarks are unsuitable due to\nthe lack of language instructions. Emerging benchmarks for VLAs that\nincorporate language often come with limited evaluation tasks and do not intend\nto investigate how much VLM pretraining truly contributes to the generalization\ncapabilities of the downstream robotic policy. Meanwhile, much research relies\non real-world robot setups designed in isolation by different institutions,\nwhich creates a barrier for reproducibility and accessibility. To address this\ngap, we introduce a unified probing suite of 50 simulation-based tasks across\n10 subcategories spanning language instruction, vision, and objects. We\nsystematically evaluate several state-of-the-art VLA architectures on this\nsuite to understand their generalization capability. Our results show that\nwhile VLM backbones endow VLAs with robust perceptual understanding and high\nlevel planning, which we refer to as good intentions, this does not reliably\ntranslate into precise motor execution: when faced with out-of-distribution\nobservations, policies often exhibit coherent intentions, but falter in action\nexecution. Moreover, finetuning on action data can erode the original VLM's\ngeneralist reasoning abilities. We release our task suite and evaluation code\nto serve as a standardized benchmark for future VLAs and to drive research on\nclosing the perception-to-action gap. More information, including the source\ncode, can be found at https://ai4ce.github.io/INT-ACT/", "comment": "Under review", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.CV", "links": "http://arxiv.org/abs/2506.09930v1;http://arxiv.org/pdf/2506.09930v1", "pdf_url": "http://arxiv.org/pdf/2506.09930v1"}, {"title": "Collapse-Proof NON-CONTRASTIVE SELF-SUPERVISED LEARNING", "link": "https://www.researchgate.net/profile/Emanuele-Sansone/publication/384698781_Collapse-Proof_Non-Contrastive_Self-Supervised_Learning/links/6844b34fd1054b0207fa8666/Collapse-Proof-Non-Contrastive-Self-Supervised-Learning.pdf", "details": "E Sansone, T Lebailly, T Tuytelaars", "abstract": "We present a principled and simplified design of the projector and loss function for non-contrastive self-supervised learning based on hyperdimensional computing. We theoretically demonstrate that this design introduces an inductive bias that \u2026"}]
