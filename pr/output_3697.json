[{"title": "$\\texttt {MoE-RBench} $: Towards Building Reliable Language Models with Sparse Mixture-of-Experts", "link": "https://arxiv.org/pdf/2406.11353", "details": "G Chen, X Zhao, T Chen, Y Cheng - arXiv preprint arXiv:2406.11353, 2024", "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new \u2026"}, {"title": "An Empirical Study of Mamba-based Language Models", "link": "https://arxiv.org/pdf/2406.07887", "details": "R Waleffe, W Byeon, D Riach, B Norick, V Korthikanti\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value \u2026"}, {"title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models", "link": "https://aclanthology.org/2024.findings-naacl.18.pdf", "details": "Z Su, Z Lin, B Baixue, H Chen, S Hu, W Zhou, G Ding\u2026 - Findings of the Association \u2026, 2024", "abstract": "Generative language models are usually pre-trained on large text corpus via predicting the next token (ie, sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language \u2026"}, {"title": "Word Embeddings Are Steers for Language Models", "link": "https://blender.cs.illinois.edu/paper/lmsteer2024.pdf", "details": "C Han, J Xu, M Li, Y Fung, C Sun, N Jiang\u2026", "abstract": "Abstract Language models (LMs) automatically learn word embeddings during pre- training on language corpora. Although word embeddings are usually interpreted as feature vectors for individual words, their roles in language model generation remain \u2026"}, {"title": "ACES: Automatic Cohort Extraction System for Event-Stream Datasets", "link": "https://arxiv.org/pdf/2406.19653", "details": "J Xu, J Gallifant, AEW Johnson, M McDermott - arXiv preprint arXiv:2406.19653, 2024", "abstract": "Reproducibility remains a significant challenge in machine learning (ML) for healthcare. In this field, datasets, model pipelines, and even task/cohort definitions are often private, leading to a significant barrier in sharing, iterating, and \u2026"}, {"title": "Igea: a Decoder-Only Language Model for Biomedical Text Generation in Italian", "link": "https://arxiv.org/pdf/2407.06011", "details": "TM Buonocore, S Rancati, E Parimbelli - arXiv preprint arXiv:2407.06011, 2024", "abstract": "The development of domain-specific language models has significantly advanced natural language processing applications in various specialized fields, particularly in biomedicine. However, the focus has largely been on English-language models \u2026"}, {"title": "Synthesizing Multimodal Electronic Health Records via Predictive Diffusion Models", "link": "https://arxiv.org/pdf/2406.13942", "details": "Y Zhong, X Wang, J Wang, X Zhang, Y Wang, M Huai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Synthesizing electronic health records (EHR) data has become a preferred strategy to address data scarcity, improve data quality, and model fairness in healthcare. However, existing approaches for EHR data generation predominantly rely on state \u2026"}, {"title": "Adaptive Rank Selections for Low-Rank Approximation of Language Models", "link": "https://aclanthology.org/2024.naacl-long.13.pdf", "details": "S Gao, T Hua, YC Hsu, Y Shen, H Jin - Proceedings of the 2024 Conference of the \u2026, 2024", "abstract": "Abstract Singular Value Decomposition (SVD) or its weighted variants has significantly progressed in compressing language models. Previous works assume the same importance for all operations and assign the same number of ranks for \u2026"}, {"title": "This Land is Your, My Land: Evaluating Geopolitical Bias in Language Models through Territorial Disputes", "link": "https://aclanthology.org/2024.naacl-long.213.pdf", "details": "B Li, S Haider, C Callison-Burch - Proceedings of the 2024 Conference of the North \u2026, 2024", "abstract": "Abstract Do the Spratly Islands belong to China, the Philippines, or Vietnam? A pretrained large language model (LLM) may answer differently if asked in the languages of each claimant country: Chinese, Tagalog, or Vietnamese. This \u2026"}]
