[{"title": "EFTNAS: Searching for Efficient Language Models in First-Order Weight-Reordered Super-Networks", "link": "https://aclanthology.org/2024.lrec-main.497.pdf", "details": "JP Munoz, Y Zheng, N Jain - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Transformer-based models have demonstrated outstanding performance in natural language processing (NLP) tasks and many other domains, eg, computer vision. Depending on the size of these models, which have grown exponentially in the past \u2026"}, {"title": "What Makes Good Few-shot Examples for Vision-Language Models?", "link": "https://arxiv.org/pdf/2405.13532", "details": "Z Guo, J Lu, X Liu, R Zhao, ZX Qian, F Tan - arXiv preprint arXiv:2405.13532, 2024", "abstract": "Despite the notable advancements achieved by leveraging pre-trained vision- language (VL) models through few-shot tuning for downstream tasks, our detailed empirical study highlights a significant dependence of few-shot learning outcomes \u2026"}, {"title": "A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks", "link": "https://arxiv.org/pdf/2405.10251", "details": "X Ni, P Li - arXiv preprint arXiv:2405.10251, 2024", "abstract": "Recent efforts have evaluated large language models (LLMs) in areas such as commonsense reasoning, mathematical reasoning, and code generation. However, to the best of our knowledge, no work has specifically investigated the performance \u2026"}, {"title": "EVA-X: A Foundation Model for General Chest X-ray Analysis with Self-supervised Learning", "link": "https://arxiv.org/pdf/2405.05237", "details": "J Yao, X Wang, Y Song, H Zhao, J Ma, Y Chen, W Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The diagnosis and treatment of chest diseases play a crucial role in maintaining human health. X-ray examination has become the most common clinical examination means due to its efficiency and cost-effectiveness. Artificial intelligence analysis \u2026"}, {"title": "Exploring and Mitigating Shortcut Learning for Generative Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.602.pdf", "details": "Z Sun, Y Xiao, J Li, Y Ji, W Chen, M Zhang - Proceedings of the 2024 Joint \u2026, 2024", "abstract": "Recent generative large language models (LLMs) have exhibited incredible instruction-following capabilities while keeping strong task completion ability, even without task-specific fine-tuning. Some works attribute this to the bonus of the new \u2026"}, {"title": "Theoretical Analysis of Meta Reinforcement Learning: Generalization Bounds and Convergence Guarantees", "link": "https://arxiv.org/pdf/2405.13290", "details": "C Wang, M Sui, D Sun, Z Zhang, Y Zhou - arXiv preprint arXiv:2405.13290, 2024", "abstract": "This research delves deeply into Meta Reinforcement Learning (Meta RL) through a exploration focusing on defining generalization limits and ensuring convergence. By employing a approach this article introduces an innovative theoretical framework to \u2026"}, {"title": "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "link": "https://arxiv.org/pdf/2405.20215", "details": "C Zhang, C Tang, D Chong, K Shi, G Tang, F Jiang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mainstream approaches to aligning large language models (LLMs) heavily rely on human preference data, particularly when models require periodic updates. The standard process for iterative alignment of LLMs involves collecting new human \u2026"}, {"title": "Large Language Models Synergize with Automated Machine Learning", "link": "https://arxiv.org/pdf/2405.03727", "details": "J Xu, Z Liu, NAV Suryanarayanan, H Iba - arXiv preprint arXiv:2405.03727, 2024", "abstract": "Recently, code generation driven by large language models (LLMs) has become increasingly popular. However, automatically generating code for machine learning (ML) tasks still poses significant challenges. This paper explores the limits of \u2026"}, {"title": "Correcting Language Model Bias for Text Classification in True Zero-Shot Learning", "link": "https://aclanthology.org/2024.lrec-main.359.pdf", "details": "F Zhao, W Xianlin, C Yan, CK Loo - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Combining pre-trained language models (PLMs) and manual templates is a common practice for text classification in zero-shot scenarios. However, the effect of this approach is highly volatile, ranging from random guesses to near state-of-the-art \u2026"}]
