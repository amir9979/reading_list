[{"title": "Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization", "link": "https://openreview.net/pdf%3Fid%3D1SiEfsCecd", "details": "X Zhao, D Xu, S Yuan - Forty-first International Conference on Machine \u2026", "abstract": "Pre-trained language models (PLMs) are commonly used for various downstream natural language processing tasks via fine-tuning. However, recent studies have demonstrated that PLMs are vulnerable to backdoor attacks, which can mislabel \u2026"}, {"title": "CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training", "link": "https://arxiv.org/pdf/2406.10670", "details": "D Brandfonbrener, H Zhang, A Kirsch, JR Schwarz\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Selecting high-quality data for pre-training is crucial in shaping the downstream task performance of language models. A major challenge lies in identifying this optimal subset, a problem generally considered intractable, thus necessitating scalable and \u2026"}]
