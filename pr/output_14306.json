[{"title": "Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge", "link": "https://arxiv.org/pdf/2503.04036", "details": "X Cui, JTZ Wei, S Swayamdipta, R Jia - arXiv preprint arXiv:2503.04036, 2025", "abstract": "Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques \u2026"}, {"title": "Stackelberg Game Preference Optimization for Data-Efficient Alignment of Language Models", "link": "https://arxiv.org/pdf/2502.18099", "details": "X Chu, Z Zhang, T Jia, Y Jin - arXiv preprint arXiv:2502.18099, 2025", "abstract": "Aligning language models with human preferences is critical for real-world deployment, but existing methods often require large amounts of high-quality human annotations. Aiming at a data-efficient alignment method, we propose Stackelberg \u2026"}, {"title": "Auditing language models for hidden objectives", "link": "https://arxiv.org/pdf/2503.10965%3F", "details": "S Marks, J Treutlein, T Bricken, J Lindsey, J Marcus\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We study the feasibility of conducting alignment audits: investigations into whether models have undesired objectives. As a testbed, we train a language model with a hidden objective. Our training pipeline first teaches the model about exploitable \u2026"}, {"title": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning", "link": "https://arxiv.org/pdf/2502.18439", "details": "C Park, S Han, X Guo, A Ozdaglar, K Zhang, JK Kim - arXiv preprint arXiv:2502.18439, 2025", "abstract": "Leveraging multiple large language models (LLMs) to build collaborative multi- agentic workflows has demonstrated significant potential. However, most previous studies focus on prompting the out-of-the-box LLMs, relying on their innate capability \u2026"}, {"title": "Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment", "link": "https://arxiv.org/pdf/2503.04647", "details": "W Yang, J Wu, C Wang, C Zong, J Zhang - arXiv preprint arXiv:2503.04647, 2025", "abstract": "Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences. While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is \u2026"}, {"title": "When Debate Fails: Bias Reinforcement in Large Language Models", "link": "https://openreview.net/pdf%3Fid%3Dc5bjw7hqix", "details": "J Oh, M Jeong, J Ko, SY Yun - Workshop on Reasoning and Planning for Large \u2026", "abstract": "Large Language Models (LLMs) solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self \u2026"}, {"title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers", "link": "https://arxiv.org/pdf/2503.00865", "details": "Y Zhao, C Liu, Y Deng, J Ying, M Aljunied, Z Li, L Bing\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced \u2026"}, {"title": "ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2502.19409", "details": "DS Villegas, I Ziegler, D Elliott - arXiv preprint arXiv:2502.19409, 2025", "abstract": "Reasoning over sequences of images remains a challenge for multimodal large language models (MLLMs). While recent models incorporate multi-image data during pre-training, they still struggle to recognize sequential structures, often treating \u2026"}, {"title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models", "link": "https://arxiv.org/pdf/2503.01763%3F", "details": "Z Shi, Y Wang, L Yan, P Ren, S Wang, D Yin, Z Ren - arXiv preprint arXiv:2503.01763, 2025", "abstract": "Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful \u2026"}]
