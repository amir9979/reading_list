'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Mastering Text, Code and Math Simultaneously via Fusin'
[{"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models", "link": "https://arxiv.org/pdf/2403.02436", "details": "X Lu, Y Zhao, B Qin - arXiv preprint arXiv:2403.02436, 2024", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records", "link": "https://arxiv.org/html/2403.04086v1", "details": "S Cui, P Mitra - arXiv preprint arXiv:2403.04086, 2024", "abstract": "In the realm of big data and digital healthcare, Electronic Health Records (EHR) have become a rich source of information with the potential to improve patient care and medical research. In recent years, machine learning models have proliferated for \u2026"}, {"title": "Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records", "link": "https://arxiv.org/pdf/2403.09226", "details": "A Ziletti, L D'Ambrosi - arXiv preprint arXiv:2403.09226, 2024", "abstract": "Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. Querying these databases to answer epidemiological questions is challenging due to the intricacy of medical \u2026"}, {"title": "Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging", "link": "https://arxiv.org/html/2403.08002v1", "details": "JMZ Chaves, SC Huang, Y Xu, H Xu, N Usuyama\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such large models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major \u2026"}, {"title": "Standing on FURM ground--A framework for evaluating Fair, Useful, and Reliable AI Models in healthcare systems", "link": "https://arxiv.org/pdf/2403.07911", "details": "A Callahan, D McElfresh, JM Banda, G Bunney, D Char\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The impact of using artificial intelligence (AI) to guide patient care or operational processes is an interplay of the AI model's output, the decision-making protocol based on that output, and the capacity of the stakeholders involved to take the \u2026"}, {"title": "Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM", "link": "https://arxiv.org/pdf/2403.08818", "details": "H Cui, X Fang, R Xu, X Kan, JC Ho, C Yang - arXiv preprint arXiv:2403.08818, 2024", "abstract": "Electronic Health Records (EHRs) have become increasingly popular to support clinical decision-making and healthcare in recent decades. EHRs usually contain heterogeneous information, such as structural data in tabular form and unstructured \u2026"}, {"title": "ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling", "link": "https://arxiv.org/html/2402.13542v1", "details": "L Zhang, Y Yu, K Wang, C Zhang - arXiv preprint arXiv:2402.13542, 2024", "abstract": "Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge \u2026"}, {"title": "Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models", "link": "https://arxiv.org/html/2402.18154v1", "details": "Z Jin, P Cao, H Yuan, Y Chen, J Xu, H Li, X Jiang, K Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, retrieval augmentation and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context. However, internal memory and external \u2026"}, {"title": "MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases", "link": "https://arxiv.org/pdf/2402.14905", "details": "Z Liu, C Zhao, F Iandola, C Lai, Y Tian, I Fedorov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical \u2026"}]
