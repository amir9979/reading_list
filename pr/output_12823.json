[{"title": "Automatic Evaluation of Healthcare LLMs Beyond Question-Answering", "link": "https://arxiv.org/pdf/2502.06666", "details": "A Arias-Duart, PA Martin-Torres, D Hinjos\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Current Large Language Models (LLMs) benchmarks are often based on open- ended or close-ended QA evaluations, avoiding the requirement of human labor. Close-ended measurements evaluate the factuality of responses but lack \u2026"}, {"title": "Self-supervised analogical learning using language models", "link": "https://arxiv.org/pdf/2502.00996", "details": "B Zhou, S Jain, Y Zhang, Q Ning, S Wang, Y Benajiba\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can \u2026"}, {"title": "Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models", "link": "https://arxiv.org/pdf/2501.14818", "details": "Z Li, G Chen, S Liu, S Wang, V VS, Y Ji, S Lan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, promising progress has been made by open-source vision-language models (VLMs) in bringing their capabilities closer to those of proprietary frontier models. However, most open-source models only publish their final model weights \u2026"}, {"title": "CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification", "link": "https://arxiv.org/pdf/2501.12266", "details": "C Patr\u00edcio, I Rio-Torto, JS Cardoso, LF Teixeira\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter \u2026"}, {"title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06788", "details": "H Diao, X Li, Y Cui, Y Wang, H Deng, T Pan, W Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient \u2026"}, {"title": "Learning with Enriched Inductive Biases for Vision-Language Models", "link": "https://ruyuanzhang.github.io/files/2501_indctbiasVisLangModel_IJCV.pdf", "details": "L Yang, RY Zhang, Q Chen, X Xie - International Journal of Computer Vision, 2025", "abstract": "Abstract Vision-Language Models, pre-trained on large-scale image-text pairs, serve as strong foundation models for transfer learning across a variety of downstream tasks. For few-shot generalization tasks, ie., when the model is trained on few-shot \u2026"}, {"title": "Noise is an Efficient Learner for Zero-Shot Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06019", "details": "R Imam, A Hanif, J Zhang, KW Dawoud\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, test-time adaptation has garnered attention as a method for tuning models without labeled data. The conventional modus operandi for adapting pre-trained vision-language models (VLMs) during test-time primarily focuses on tuning \u2026"}, {"title": "Test-time Loss Landscape Adaptation for Zero-Shot Generalization in Vision-Language Models", "link": "https://arxiv.org/pdf/2501.18864", "details": "A Li, L Zhuang, X Long, M Yao, S Wang - arXiv preprint arXiv:2501.18864, 2025", "abstract": "Test-time adaptation of pre-trained vision-language models has emerged as a technique for tackling distribution shifts during the test time. Although existing methods, especially those based on Test-time Prompt Tuning (TPT), have shown \u2026"}, {"title": "CSP-DCPE: Category-Specific Prompt with Deep Contextual Prompt Enhancement for Vision\u2013Language Models", "link": "https://www.mdpi.com/2079-9292/14/4/673", "details": "C Wu, Y Wu, Q Xu, X Zi - Electronics, 2025", "abstract": "Recently, prompt learning has emerged as a viable technique for fine-tuning pre- trained vision\u2013language models (VLMs). The use of prompts allows pre-trained VLMs to be quickly adapted to specific downstream tasks, bypassing the necessity to \u2026"}]
