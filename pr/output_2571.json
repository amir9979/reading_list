[{"title": "GCVR: Reconstruction from Cross-View Enable Sufficient and Robust Graph Contrastive Learning", "link": "https://openreview.net/pdf%3Fid%3DDA1zd1Qdon", "details": "Q Wen, Z Ouyang, C Zhang, Y Qian, C Zhang, Y Ye - The 40th Conference on Uncertainty in \u2026", "abstract": "Among the existing self-supervised learning (SSL) methods for graphs, graph contrastive learning (GCL) frameworks usually automatically generate supervision by transforming the same graph into different views through graph augmentation \u2026"}, {"title": "Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration", "link": "https://openreview.net/pdf%3Fid%3DDLTjFFiuUJ", "details": "Z Yu, Z Wang, Y Fu, H Shi, K Shaikh, YC Lin - Forty-first International Conference on Machine \u2026", "abstract": "Attention is a fundamental component behind the remarkable achievements of large language models (LLMs). However, our current understanding of the attention mechanism, especially regarding how attention distributions are established \u2026"}, {"title": "RefAI: a GPT-powered retrieval-augmented generative tool for biomedical literature recommendation and summarization", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae129/7690757", "details": "Y Li, J Zhao, M Li, Y Dang, E Yu, J Li, Z Sun, U Hussein\u2026 - Journal of the American Medical \u2026", "abstract": "Objectives Precise literature recommendation and summarization are crucial for biomedical professionals. While the latest iteration of generative pretrained transformer (GPT) incorporates 2 distinct modes\u2014real-time search and pretrained \u2026"}]
