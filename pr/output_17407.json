[{"title": "Leveraging Multimodal **Large Language Models** for Joint Discrete and Continuous **Evaluation** in Text-to-Image Alignment", "link": "https://openaccess.thecvf.com/content/CVPR2025W/NTIRE/papers/Zhang_Leveraging_Multimodal_Large_Language_Models_for_Joint_Discrete_and_Continuous_CVPRW_2025_paper.pdf", "details": "Z Zhang, X Li, W Sun, Z Zhang, Y Li, X Liu, G Zhai - Proceedings of the Computer \u2026, 2025", "abstract": "\u2026 advanced multimodal **large** **language** **models** [1, 28] (MLLMs) and introduce several innovative techniques to enhance **evaluation** accuracy \u2026 (eg, excellent, good, fair, poor, bad), thereby leveraging the robust natural language understanding \u2026"}, {"title": "SafeLawBench: Towards Safe Alignment of Large Language Models", "link": "https://arxiv.org/pdf/2506.06636", "details": "C Cao, H Zhu, J Ji, Q Sun, Z Zhu, Y Wu, J Dai, Y Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 of **large** **language** **models** (LLMs), the safety of LLMs has raised significant concerns. However, there is still a lack of definitive standards for **evaluating** \u2026 2024b) **evaluate** whether **large** **language** **models** can safely respond to risky queries from \u2026", "entry_id": "http://arxiv.org/abs/2506.06636v1", "updated": "2025-06-07 03:09:59", "published": "2025-06-07 03:09:59", "authors": "Chuxue Cao;Han Zhu;Jiaming Ji;Qichao Sun;Zhenghao Zhu;Yinyu Wu;Juntao Dai;Yaodong Yang;Sirui Han;Yike Guo", "summary": "With the growing prevalence of large language models (LLMs), the safety of\nLLMs has raised significant concerns. However, there is still a lack of\ndefinitive standards for evaluating their safety due to the subjective nature\nof current safety benchmarks. To address this gap, we conducted the first\nexploration of LLMs' safety evaluation from a legal perspective by proposing\nthe SafeLawBench benchmark. SafeLawBench categorizes safety risks into three\nlevels based on legal standards, providing a systematic and comprehensive\nframework for evaluation. It comprises 24,860 multi-choice questions and 1,106\nopen-domain question-answering (QA) tasks. Our evaluation included 2\nclosed-source LLMs and 18 open-source LLMs using zero-shot and few-shot\nprompting, highlighting the safety features of each model. We also evaluated\nthe LLMs' safety-related reasoning stability and refusal behavior.\nAdditionally, we found that a majority voting mechanism can enhance model\nperformance. Notably, even leading SOTA models like Claude-3.5-Sonnet and\nGPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench,\nwhile the average accuracy of 20 LLMs remains at 68.8\\%. We urge the community\nto prioritize research on the safety of LLMs.", "comment": "Accepted to ACL2025 Findings", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.06636v1;http://arxiv.org/pdf/2506.06636v1", "pdf_url": "http://arxiv.org/pdf/2506.06636v1"}, {"title": "**Large Language Models** in Healthcare and Medical Applications: A Review", "link": "https://www.mdpi.com/2306-5354/12/6/631", "details": "S Maity, MJ Saikia - Bioengineering, 2025", "abstract": "\u2026 This paper provides a systematic and in-depth examination of **large** **language** **models** (LLMs) in the healthcare domain, addressing their \u2026 comprehensive **evaluation** framework for healthcare LLMs [10,11]. For instance, GPT-4 has been \u2026"}, {"title": "Beyond Facts: Evaluating Intent Hallucination in Large Language Models", "link": "https://arxiv.org/pdf/2506.06539", "details": "Y Hao, H Yu, J You - arXiv preprint arXiv:2506.06539, 2025", "abstract": "\u2026 In this paper, we introduce the concept of Intent Hallucination, a specific form of hallucination that arises when **Large** **Language** **Models** (LLMs) omit or misinterpret crucial elements of complex queries, leading to outputs that diverge from users\u2019 intentions \u2026", "entry_id": "http://arxiv.org/abs/2506.06539v1", "updated": "2025-06-06 21:10:55", "published": "2025-06-06 21:10:55", "authors": "Yijie Hao;Haofei Yu;Jiaxuan You", "summary": "When exposed to complex queries containing multiple conditions, today's large\nlanguage models (LLMs) tend to produce responses that only partially satisfy\nthe query while neglecting certain conditions. We therefore introduce the\nconcept of Intent Hallucination. In this phenomenon, LLMs either omit\n(neglecting to address certain parts) or misinterpret (responding to invented\nquery parts) elements of the given query, leading to intent hallucinated\ngeneration. To systematically evaluate intent hallucination, we introduce\nFAITHQA, a novel benchmark for intent hallucination that contains 20,068\nproblems, covering both query-only and retrieval-augmented generation (RAG)\nsetups with varying topics and difficulty. FAITHQA is the first hallucination\nbenchmark that goes beyond factual verification, tailored to identify the\nfundamental cause of intent hallucination. By evaluating various LLMs on\nFAITHQA, we find that (1) intent hallucination is a common issue even for\nstate-of-the-art models, and (2) the phenomenon stems from omission or\nmisinterpretation of LLMs. To facilitate future research, we introduce an\nautomatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting\nintent hallucination. Human evaluation results demonstrate that CONSTRAINT\nSCORE is closer to human performance for intent hallucination compared to\nbaselines.", "comment": "Accepted to ACL 2025 main conference", "journal_ref": "Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.06539v1;http://arxiv.org/pdf/2506.06539v1", "pdf_url": "http://arxiv.org/pdf/2506.06539v1"}, {"title": " **Evaluating large language models** for information extraction from gastroscopy and colonoscopy reports through multi-strategy prompting", "link": "https://www.sciencedirect.com/science/article/pii/S1532046425000735", "details": "Z Yu, L Fang, Y Ding, Y Shen, L Xu, Y Cai, X Liu - Journal of Biomedical Informatics, 2025", "abstract": "Objective: To systematically **evaluate** **large** **language** **models** (LLMs) for automated information extraction from gastroscopy and colonoscopy reports through prompt engineering, addressing their ability to extract structured information, recognize \u2026"}, {"title": "A novel fine-tuning and **evaluation** methodology for **large language models** on IoT raw data summaries (LLM-RawDMeth): A joint perspective in diabetes care", "link": "https://www.sciencedirect.com/science/article/pii/S0169260725002950", "details": "JF Gait\u00e1n-Guerrero, C Mart\u00ednez-Cruz, M Espinilla\u2026 - Computer Methods and \u2026, 2025", "abstract": "\u2026 These results indicate the capability of our methods to align **Large** **Language** **Models** with the task of generating human-like text from raw data, highlighting their potential to manage diabetes by complex glucose patterns interpretation, alleviating \u2026"}, {"title": "Urban safety perception assessments via integrating multimodal **large language models** with street view images", "link": "https://www.sciencedirect.com/science/article/pii/S0264275125004226", "details": "J Zhang, Y Li, T Fukuda, B Wang - Cities, 2025", "abstract": "\u2026 Therefore, a fully automated safety **evaluation** method is essential. Recent multimodal **large** **language** **models** (MLLMs), such as GPT, \u2026 methods for large-scale urban safety **evaluations**. The proposed method provides an efficient, automated \u2026"}, {"title": "Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2506.06242", "details": "Z Babaiee, PM Kiasari, D Rus, R Grosu - arXiv preprint arXiv:2506.06242, 2025", "abstract": "Recent advancements in multimodal **large** **language** **models** have driven breakthroughs in visual question answering. Yet, a critical gap persists, `conceptualization'-the ability to recognize and reason about the same concept despite variations in visual \u2026", "entry_id": "http://arxiv.org/abs/2506.06242v1", "updated": "2025-06-06 17:06:25", "published": "2025-06-06 17:06:25", "authors": "Zahra Babaiee;Peyman M. Kiasari;Daniela Rus;Radu Grosu", "summary": "Recent advancements in multimodal large language models have driven\nbreakthroughs in visual question answering. Yet, a critical gap persists,\n`conceptualization'-the ability to recognize and reason about the same concept\ndespite variations in visual form, a basic ability of human reasoning. To\naddress this challenge, we introduce the Visual Graph Arena (VGA), a dataset\nfeaturing six graph-based tasks designed to evaluate and improve AI systems'\ncapacity for visual abstraction. VGA uses diverse graph layouts (e.g.,\nKamada-Kawai vs. planar) to test reasoning independent of visual form.\nExperiments with state-of-the-art vision models and multimodal LLMs reveal a\nstriking divide: humans achieved near-perfect accuracy across tasks, while\nmodels totally failed on isomorphism detection and showed limited success in\npath/cycle tasks. We further identify behavioral anomalies suggesting\npseudo-intelligent pattern matching rather than genuine understanding. These\nfindings underscore fundamental limitations in current AI models for visual\nunderstanding. By isolating the challenge of representation-invariant\nreasoning, the VGA provides a framework to drive progress toward human-like\nconceptualization in AI visual models. The Visual Graph Arena is available at:\n\\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2506.06242v1;http://arxiv.org/pdf/2506.06242v1", "pdf_url": "http://arxiv.org/pdf/2506.06242v1"}, {"title": "WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code", "link": "https://arxiv.org/pdf/2506.07818", "details": "Z Lin, Z Zhou, Z Zhao, T Wan, Y Ma, J Gao, X Li - arXiv preprint arXiv:2506.07818, 2025", "abstract": "\u2026 In this study, we introduce WebUIBench, a largescale and comprehensive benchmark designed to **evaluate** the WebUI-to-Code capabilities of Multimodal **Large** **Language** **Models** (MLLMs). WebUIBench comprises over 21K question-answer \u2026", "entry_id": "http://arxiv.org/abs/2506.07818v1", "updated": "2025-06-09 14:46:02", "published": "2025-06-09 14:46:02", "authors": "Zhiyu Lin;Zhengda Zhou;Zhiyuan Zhao;Tianrui Wan;Yilun Ma;Junyu Gao;Xuelong Li", "summary": "With the rapid advancement of Generative AI technology, Multimodal Large\nLanguage Models(MLLMs) have the potential to act as AI software engineers\ncapable of executing complex web application development. Considering that the\nmodel requires a confluence of multidimensional sub-capabilities to address the\nchallenges of various development phases, constructing a multi-view evaluation\nframework is crucial for accurately guiding the enhancement of development\nefficiency. However, existing benchmarks usually fail to provide an assessment\nof sub-capabilities and focus solely on webpage generation outcomes. In this\nwork, we draw inspiration from the principles of software engineering and\nfurther propose WebUIBench, a benchmark systematically designed to evaluate\nMLLMs in four key areas: WebUI Perception, HTML Programming,WebUI-HTML\nUnderstanding, and WebUI-to-Code. WebUIBench comprises 21K high-quality\nquestion-answer pairs derived from over 0.7K real-world websites. The extensive\nevaluation of 29 mainstream MLLMs uncovers the skill characteristics and\nvarious weakness that models encountered during the development process.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.07818v1;http://arxiv.org/pdf/2506.07818v1", "pdf_url": "http://arxiv.org/pdf/2506.07818v1"}]
