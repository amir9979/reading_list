[{"title": "Attention Prompting on Image for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.17143", "details": "R Yu, W Yu, X Wang - arXiv preprint arXiv:2409.17143, 2024", "abstract": "Compared with Large Language Models (LLMs), Large Vision-Language Models (LVLMs) can also accept images as input, thus showcasing more interesting emergent capabilities and demonstrating impressive performance on various vision \u2026"}, {"title": "VLM's Eye Examination: Instruct and Inspect Visual Competency of Vision Language Models", "link": "https://arxiv.org/pdf/2409.14759", "details": "N Hyeon-Woo, M Ye-Bin, W Choi, L Hyun, TH Oh - arXiv preprint arXiv:2409.14759, 2024", "abstract": "Vision language models (VLMs) have shown promising reasoning capabilities across various benchmarks; however, our understanding of their visual perception remains limited. In this work, we propose an eye examination process to investigate \u2026"}, {"title": "Judgment of Thoughts: Courtroom of the Binary Logical Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2409.16635", "details": "S Park, D Choi - arXiv preprint arXiv:2409.16635, 2024", "abstract": "This paper proposes a novel prompt engineering technique called Judgment of Thought (JoT) that is specifically tailored for binary logical reasoning tasks. JoT employs three roles $\\unicode {x2014} $ lawyer, prosecutor, and judge $\\unicode \u2026"}, {"title": "Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models", "link": "https://arxiv.org/pdf/2409.12512", "details": "J Rao, X Liu, Z Lin, L Ding, J Li, D Tao - arXiv preprint arXiv:2409.12512, 2024", "abstract": "Knowledge distillation (KD) is a technique that compresses large teacher models by training smaller student models to mimic them. The success of KD in auto-regressive language models mainly relies on Reverse KL for mode-seeking and student \u2026"}, {"title": "GroupDebate: Enhancing the Efficiency of Multi-Agent Debate Using Group Discussion", "link": "https://arxiv.org/pdf/2409.14051", "details": "T Liu, X Wang, W Huang, W Xu, Y Zeng, L Jiang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse NLP tasks. Extensive research has explored how to enhance the logical reasoning abilities such as Chain-of-Thought, Chain-of-Thought \u2026"}, {"title": "Task-oriented Prompt Enhancement via Script Generation", "link": "https://arxiv.org/pdf/2409.16418", "details": "CY Wang, A DaghighFarsoodeh, HV Pham - arXiv preprint arXiv:2409.16418, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities across various tasks, leveraging advanced reasoning. Yet, they struggle with task-oriented prompts due to a lack of specific prior knowledge of the task answers. The current \u2026"}, {"title": "Improving the Efficiency of Visually Augmented Language Models", "link": "https://arxiv.org/pdf/2409.11148", "details": "P Ontalvilla, A Ormazabal, G Azkune - arXiv preprint arXiv:2409.11148, 2024", "abstract": "Despite the impressive performance of autoregressive Language Models (LM) it has been shown that due to reporting bias, LMs lack visual knowledge, ie they do not know much about the visual world and its properties. To augment LMs with visual \u2026"}, {"title": "Language Models Learn to Mislead Humans via RLHF", "link": "https://arxiv.org/pdf/2409.12822", "details": "J Wen, R Zhong, A Khan, E Perez, J Steinhardt\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) can produce errors that are hard to detect for humans, especially when the task is complex. RLHF, the most popular post-training method, may exacerbate this problem: to achieve higher rewards, LMs might get better at \u2026"}, {"title": "A Gradient Analysis Framework for Rewarding Good and Penalizing Bad Examples in Language Models", "link": "https://arxiv.org/pdf/2408.16751", "details": "YL Tuan, WY Wang - arXiv preprint arXiv:2408.16751, 2024", "abstract": "Beyond maximum likelihood estimation (MLE), the standard objective of a language model (LM) that optimizes good examples probabilities, many studies have explored ways that also penalize bad examples for enhancing the quality of output distribution \u2026"}]
