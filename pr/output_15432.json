[{"title": "Cultural Learning-Based Culture Adaptation of Language Models", "link": "https://arxiv.org/pdf/2504.02953", "details": "CC Liu, A Korhonen, I Gurevych - arXiv preprint arXiv:2504.02953, 2025", "abstract": "Adapting large language models (LLMs) to diverse cultural values is a challenging task, as existing LLMs often reflect the values of specific groups by default, and potentially causing harm to others. In this paper, we present CLCA, a novel \u2026"}, {"title": "Mixture-of-Personas Language Models for Population Simulation", "link": "https://arxiv.org/pdf/2504.05019", "details": "N Bui, HT Nguyen, S Kumar, J Theodore, W Qiu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Advances in Large Language Models (LLMs) paved the way for their emerging applications in various domains, such as human behavior simulations, where LLMs could augment human-generated data in social science research and machine \u2026"}, {"title": "A Dual-Space Framework for General Knowledge Distillation of Large Language Models", "link": "https://arxiv.org/pdf/2504.11426", "details": "X Zhang, S Zhang, Y Liang, F Meng, Y Chen, J Xu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Knowledge distillation (KD) is a promising solution to compress large language models (LLMs) by transferring their knowledge to smaller models. During this process, white-box KD methods usually minimize the distance between the output \u2026"}, {"title": "FLUE: Streamlined Uncertainty Estimation for Large Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/33840/35995", "details": "S Gao, T Gong, Z Lin, R Xu, H Zhou, J Li - Proceedings of the AAAI Conference on \u2026, 2025", "abstract": "Uncertainty estimation is essential for practical applications such as decision- making, risk assessment, and human-AI collaboration. However, Uncertainty estimation in open-ended question-answering (QA) tasks presents unique \u2026"}, {"title": "Large language models could be rote learners", "link": "https://arxiv.org/pdf/2504.08300", "details": "Y Xu, R Hu, H Ying, J Wu, X Shi, W Lin - arXiv preprint arXiv:2504.08300, 2025", "abstract": "Multiple-choice question (MCQ) benchmarks are widely used for evaluating Large Language Models (LLMs), yet their reliability is undermined by benchmark contamination. In this study, we reframe contamination as an inherent aspect of \u2026"}, {"title": "Exploring Conversational Adaptability: Assessing the Proficiency of Large Language Models in Dynamic Alignment with Updated User Intent", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/34534/36689", "details": "YC Chen, HH Huang - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "This paper presents a practical problem in dialogue systems: the capability to adapt to changing user intentions and resolve inconsistencies in conversation histories. It is crucial in scenarios like train ticket booking, where travel plans often change \u2026"}, {"title": "Fast-Slow-Thinking: Complex Task Solving with Large Language Models", "link": "https://arxiv.org/pdf/2504.08690%3F", "details": "Y Sun, Y Zhang, Z Zhao, S Wan, D Tao, C Gong - arXiv preprint arXiv:2504.08690, 2025", "abstract": "Nowadays, Large Language Models (LLMs) have been gradually employed to solve complex tasks. To face the challenge, task decomposition has become an effective way, which proposes to divide a complex task into multiple simpler subtasks and \u2026"}, {"title": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning", "link": "https://arxiv.org/pdf/2504.13032", "details": "Z Wang, SX Teo, JJ Chew, W Shi - arXiv preprint arXiv:2504.13032, 2025", "abstract": "Recent advancements in large language models (LLMs) have enabled their use as agents for planning complex tasks. Existing methods typically rely on a thought- action-observation (TAO) process to enhance LLM performance, but these \u2026"}, {"title": "Teaching Large Language Models to Reason through Learning and Forgetting", "link": "https://arxiv.org/pdf/2504.11364", "details": "T Ni, A Nie, S Chaudhary, Y Liu, H Rangwala, R Fakoor - arXiv preprint arXiv \u2026, 2025", "abstract": "Leveraging inference-time search in large language models has proven effective in further enhancing a trained model's capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational \u2026"}]
