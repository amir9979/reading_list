[{"title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios", "link": "https://aclanthology.org/2024.findings-acl.230.pdf", "details": "L Lin, J Fu, P Liu, Q Li, Y Gong, J Wan, F Zhang\u2026 - Findings of the Association \u2026, 2024", "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local \u2026"}, {"title": "Selective Prefix Tuning for Pre-trained Language Models", "link": "https://aclanthology.org/2024.findings-acl.164.pdf", "details": "H Zhang, Z Li, P Wang, H Zhao - Findings of the Association for Computational \u2026, 2024", "abstract": "The prevalent approach for optimizing pre-trained language models in downstream tasks is fine-tuning. However, it is both time-consuming and memory-inefficient. In response, a more efficient method called Prefix Tuning, which insert learnable \u2026"}, {"title": "Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models", "link": "https://aclanthology.org/2024.acl-long.765.pdf", "details": "S Li, R Lin, S Pei - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities in production. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer \u2026"}, {"title": "DOSSIER: Fact checking in electronic health records while preserving patient privacy", "link": "https://www.amazon.science/publications/dossier-fact-checking-in-electronic-health-records-while-preserving-patient-privacy", "details": "H Zhang, S Nagesh, M Shyani, N Mishra - 2024", "abstract": "Given a particular claim about a specific document, the fact checking problem is to determine if the claim is true and, if so, provide corroborating evidence. The problem is motivated by contexts where a document is too lengthy to quickly read and find an \u2026"}, {"title": "NoteChat: A Dataset of Synthetic Patient-Physician Conversations Conditioned on Clinical Notes", "link": "https://aclanthology.org/2024.findings-acl.901.pdf", "details": "J Wang, Z Yao, Z Yang, H Zhou, R Li, X Wang, Y Xu\u2026 - Findings of the Association \u2026, 2024", "abstract": "We introduce NoteChat, a novel cooperative multi-agent framework leveraging Large Language Models (LLMs) to generate patient-physician dialogues. NoteChat embodies the principle that an ensemble of role-specific LLMs, through structured \u2026"}, {"title": "Teaching Small Language Models to Reason for Knowledge-Intensive Multi-Hop Question Answering", "link": "https://aclanthology.org/2024.findings-acl.464.pdf", "details": "X Li, S He, F Lei, JY JunYang, T Su, K Liu, J Zhao - Findings of the Association for \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) can teach small language models (SLMs) to solve complex reasoning tasks (eg, mathematical question answering) by Chain-of- thought Distillation (CoTD). Specifically, CoTD fine-tunes SLMs by utilizing rationales \u2026"}, {"title": "EchoSight: Advancing Visual-Language Models with Wiki Knowledge", "link": "https://arxiv.org/pdf/2407.12735", "details": "Y Yan, W Xie - arXiv preprint arXiv:2407.12735, 2024", "abstract": "Knowledge-based Visual Question Answering (KVQA) tasks require answering questions about images using extensive background knowledge. Despite significant advancements, generative models often struggle with these tasks due to the limited \u2026"}, {"title": "KoCommonGEN v2: A Benchmark for Navigating Korean Commonsense Reasoning Challenges in Large Language Models", "link": "https://aclanthology.org/2024.findings-acl.141.pdf", "details": "J Seo, J Lee, C Park, ST Hong, S Lee, HS Lim - Findings of the Association for \u2026, 2024", "abstract": "The evolution of large language models (LLMs) has culminated in a multitask model paradigm where prompts drive the generation of user-specific outputs. However, this advancement has revealed a critical challenge: LLMs frequently produce outputs \u2026"}, {"title": "MedSyn: LLM-based Synthetic Medical Text Generation Framework", "link": "https://arxiv.org/pdf/2408.02056", "details": "G Kumichev, P Blinov, Y Kuzkina, V Goncharov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating synthetic text addresses the challenge of data availability in privacy- sensitive domains such as healthcare. This study explores the applicability of synthetic data in real-world medical settings. We introduce MedSyn, a novel medical \u2026"}]
