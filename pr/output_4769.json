[{"title": "SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers", "link": "https://arxiv.org/pdf/2407.09413", "details": "S Pramanick, R Chellappa, S Venugopalan - arXiv preprint arXiv:2407.09413, 2024", "abstract": "Seeking answers to questions within long scientific research articles is a crucial area of study that aids readers in quickly addressing their inquiries. However, existing question-answering (QA) datasets based on scientific papers are limited in scale and \u2026"}, {"title": "Mitigating Multilingual Hallucination in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2408.00550", "details": "X Qu, M Song, W Wei, J Dong, Y Cheng - arXiv preprint arXiv:2408.00550, 2024", "abstract": "While Large Vision-Language Models (LVLMs) have exhibited remarkable capabilities across a wide range of tasks, they suffer from hallucination problems, where models generate plausible yet incorrect answers given the input image-query \u2026"}, {"title": "EUDA: An Efficient Unsupervised Domain Adaptation via Self-Supervised Vision Transformer", "link": "https://arxiv.org/pdf/2407.21311", "details": "A Abedi, QM Wu, N Zhang, F Pourpanah - arXiv preprint arXiv:2407.21311, 2024", "abstract": "Unsupervised domain adaptation (UDA) aims to mitigate the domain shift issue, where the distribution of training (source) data differs from that of testing (target) data. Many models have been developed to tackle this problem, and recently vision \u2026"}, {"title": "SCFL: Spatio-temporal consistency federated learning for next POI recommendation", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324002115", "details": "L Zhong, J Zeng, Z Wang, W Zhou, J Wen - Information Processing & Management, 2024", "abstract": "Existing personalized federated learning frameworks fail to significantly improve the personalization of user preference learning in next Point-Of-Interest (POI) recommendations, causing notable performance deficits. These frameworks do not \u2026"}, {"title": "Rethinking Self-Supervised Semantic Segmentation: Achieving End-to-End Segmentation", "link": "https://ieeexplore.ieee.org/iel8/34/4359286/10607955.pdf", "details": "Y Liu, J Zeng, X Tao, G Fang - IEEE Transactions on Pattern Analysis and Machine \u2026, 2024", "abstract": "The challenge of semantic segmentation with scarce pixel-level annotations has induced many self-supervised works, however most of which essentially train an image encoder or a segmentation head that produces finer dense representations \u2026"}, {"title": "Advancing Faithfulness of Large Language Models in Goal-Oriented Dialogue Question Answering", "link": "https://dl.acm.org/doi/abs/10.1145/3640794.3665573", "details": "A Sticha, N Braunschweiler, RS Doddipatla, KM Knill - \u2026 of the 6th ACM Conference on \u2026, 2024", "abstract": "Goal-oriented dialogue systems, such as assistant chatbots and conversational AI systems, have gained prominence for their question-answering capabilities, often utilizing large language models (LLMs) as knowledge bases. However, these \u2026"}, {"title": "AI Safety in Generative AI Large Language Models: A Survey", "link": "https://arxiv.org/pdf/2407.18369", "details": "J Chua, Y Li, S Yang, C Wang, L Yao - arXiv preprint arXiv:2407.18369, 2024", "abstract": "Large Language Model (LLMs) such as ChatGPT that exhibit generative AI capabilities are facing accelerated adoption and innovation. The increased presence of Generative AI (GAI) inevitably raises concerns about the risks and safety \u2026"}, {"title": "Large Language Models for Tabular Data: Progresses and Future Directions", "link": "https://dl.acm.org/doi/pdf/10.1145/3626772.3661384", "details": "H Dong, Z Wang - Proceedings of the 47th International ACM SIGIR \u2026, 2024", "abstract": "Tables contain a significant portion of the world's structured information. The ability to efficiently and accurately understand, process, reason about, analyze, and generate tabular data is critical for achieving Artificial General Intelligence (AGI) systems \u2026"}, {"title": "Cognitive Assessment of Language Models", "link": "https://openreview.net/pdf%3Fid%3DpxRh1meUvN", "details": "D McDuff, D Munday, X Liu, I Galatzer-Levy - ICML 2024 Workshop on LLMs and Cognition", "abstract": "Large language models (LLMs) are a subclass of generative artificial intelligence that can interpret language inputs to generate novel responses. These capabilities are conceptualized as a significant step forward in artificial intelligence because the \u2026"}]
