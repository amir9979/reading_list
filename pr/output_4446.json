[{"title": "Leveraging Language Models and Automatic Summarization in Online Programming Learning Environments", "link": "https://dl.acm.org/doi/full/10.1145/3653323", "details": "C Areces, L Benotti, F Bulgarelli, E Echeveste, N Finzi - Communications of the ACM", "abstract": "Objective A. Enhance the interaction between tutors, the Mumuki platform, and the group of trainee programmers. By utilizing the stochastic language models of learners' errors in each programming language, training errors in the exercise are \u2026"}, {"title": "Towards Adversarially Robust Vision-Language Models: Insights from Design Choices and Prompt Formatting Techniques", "link": "https://arxiv.org/pdf/2407.11121", "details": "R Bhagwatkar, S Nayak, R Bayat, A Roger, DZ Kaplan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-Language Models (VLMs) have witnessed a surge in both research and real- world applications. However, as they are becoming increasingly prevalent, ensuring their robustness against adversarial attacks is paramount. This work systematically \u2026"}, {"title": "IgnitionInnovators at\" Discharge Me!\": Chain-of-Thought Instruction Finetuning Large Language Models for Discharge Summaries", "link": "https://arxiv.org/pdf/2407.17636", "details": "AQ Tang, X Zhang, MN Dinh - arXiv preprint arXiv:2407.17636, 2024", "abstract": "This paper presents our proposed approach to the Discharge Me! shared task, collocated with the 23th Workshop on Biomedical Natural Language Processing (BioNLP). In this work, we develop an LLM-based framework for solving the \u2026"}, {"title": "Artificial Intelligence for Natural Language Processing of Clinical Text in Spanish for Real-World-Data Analysis (Text2RWD Project)", "link": "https://ceur-ws.org/Vol-3729/p1_rev.pdf", "details": "FJ Veredas, F Gallego, G L\u00f3pez-Garc\u00eda, N Ribelles\u2026 - 2024", "abstract": "The Text2RWD project (funded by the Spanish Ministerio de Ciencia e Innovaci\u00f3n, PID2020-116898RB-I00), aims to advance in the creation and de-identification of a specific clinical corpus, which is expected to be of reference as an oncological text \u2026"}, {"title": "The diagnostic and triage accuracy of the GPT-3 artificial intelligence model: an observational study", "link": "https://www.thelancet.com/journals/landig/article/PIIS2589-7500\\(24\\)00097-9/fulltext", "details": "DM Levine, R Tuwani, B Kompa, A Varma\u2026 - The Lancet Digital Health, 2024", "abstract": "Background Artificial intelligence (AI) applications in health care have been effective in many areas of medicine, but they are often trained for a single task using labelled data, making deployment and generalisability challenging. How well a general \u2026"}, {"title": "Direct Preference Knowledge Distillation for Large Language Models", "link": "https://arxiv.org/pdf/2406.19774", "details": "Y Li, Y Gu, L Dong, D Wang, Y Cheng, F Wei - arXiv preprint arXiv:2406.19774, 2024", "abstract": "In the field of large language models (LLMs), Knowledge Distillation (KD) is a critical technique for transferring capabilities from teacher models to student models. However, existing KD methods face limitations and challenges in distillation of LLMs \u2026"}, {"title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "link": "https://arxiv.org/pdf/2407.10817", "details": "T Vu, K Krishna, S Alzubi, C Tar, M Faruqui, YH Sung - arXiv preprint arXiv \u2026, 2024", "abstract": "As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their output due to the high costs of human evaluation. To make progress towards better LLM autoraters, we introduce FLAMe, a family of Foundational Large \u2026"}, {"title": "Demystifying Verbatim Memorization in Large Language Models", "link": "https://arxiv.org/pdf/2407.17817", "details": "J Huang, D Yang, C Potts - arXiv preprint arXiv:2407.17817, 2024", "abstract": "Large Language Models (LLMs) frequently memorize long sequences verbatim, often with serious legal and privacy implications. Much prior work has studied such verbatim memorization using observational data. To complement such work, we \u2026"}, {"title": "Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment", "link": "https://arxiv.org/pdf/2407.03051", "details": "J Lee, S Park, S Hong, M Kim, DS Chang, J Choi - arXiv preprint arXiv:2407.03051, 2024", "abstract": "The rapid advancement of large language models (LLMs) has facilitated their transformation into conversational chatbots that can grasp contextual nuances and generate pertinent sentences, closely mirroring human values through advanced \u2026"}]
