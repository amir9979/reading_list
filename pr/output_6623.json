[{"title": "Designing Retrieval-Augmented Language Models for Clinical Decision", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DWcMbEQAAQBAJ%26oi%3Dfnd%26pg%3DPA159%26ots%3DtCwXx4PHbr%26sig%3DRHmZ_S_5A7GgE1UxrXpdgUfwel8", "details": "K Quigley, T Koker, J Taylor, V Mancuso - AI for Health Equity and Fairness: Leveraging AI to \u2026", "abstract": "Ever-increasing demands for physician expertise drive the need for trust-worthy point- of-care tools that can help aid decision-making in all clinical settings. Retrieval- augmented language models carry potential to relieve the information burden on \u2026"}, {"title": "Language Models Pre-training", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-65647-7_2.pdf", "details": "U Kamath, K Keenan, G Somers, S Sorenson - Large Language Models: A Deep Dive \u2026, 2024", "abstract": "Pre-training forms the foundation for LLMs' capabilities. LLMs gain vital language comprehension and generative language skills by using large-scale datasets. The size and quality of these datasets are essential for maximizing LLMs' potential. It is \u2026"}, {"title": "NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks", "link": "https://arxiv.org/pdf/2408.13106", "details": "H Huang, T Park, K Dhawan, I Medennikov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-supervised learning has been proved to benefit a wide range of speech processing tasks, such as speech recognition/translation, speaker verification and diarization, etc. However, most of these approaches are computationally intensive \u2026"}, {"title": "Towards Harnessing Large Language Models as Autonomous Agents for Semantic Triple Extraction from Unstructured Text", "link": "https://ceur-ws.org/Vol-3747/text2kg_paper1.pdf", "details": "A Ananya, S Tiwari, N Mihindukulasooriya, T Soru\u2026 - 2024", "abstract": "Abstract The use of Large Language Models as autonomous agents interacting with tools has shown to improve the performance of several tasks from code generation to API calling and sequencing. This paper proposes a framework for using Large \u2026"}, {"title": "Legally-Guided Automated Decision-Making System Using Language Model Agents for Autonomous Driving", "link": "https://link.springer.com/chapter/10.1007/978-3-031-72407-7_17", "details": "Y Wang, D Barta, J Hesse, P Buchwald, A Paschke - International Joint Conference \u2026, 2024", "abstract": "Recent advances in language models have facilitated the development of agent- based systems. Despite their encouraging results in various reasoning tasks, these systems often operate as \u201cblack boxes\u201d, raising concerns about potential illegal \u2026"}, {"title": "Focused Large Language Models are Stable Many-Shot Learners", "link": "https://arxiv.org/pdf/2408.13987", "details": "P Yuan, S Feng, Y Li, X Wang, Y Zhang, C Tan, B Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL \u2026"}, {"title": "Automated Mining of Structured Knowledge from Text in the Era of Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671469", "details": "Y Zhang, M Zhong, S Ouyang, Y Jiao, S Zhou, L Ding\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "Massive amount of unstructured text data are generated daily, ranging from news articles to scientific papers. How to mine structured knowledge from the text data remains a crucial research question. Recently, large language models (LLMs) have \u2026"}, {"title": "Importance Weighting Can Help Large Language Models Self-Improve", "link": "https://arxiv.org/pdf/2408.09849", "details": "C Jiang, C Chan, W Xue, Q Liu, Y Guo - arXiv preprint arXiv:2408.09849, 2024", "abstract": "Large language models (LLMs) have shown remarkable capability in numerous tasks and applications. However, fine-tuning LLMs using high-quality datasets under external supervision remains prohibitively expensive. In response, LLM self \u2026"}, {"title": "Generating Synthetic Datasets for Few-shot Prompt Tuning", "link": "https://openreview.net/pdf%3Fid%3DVd0KvChLXr", "details": "X Guo, Z Du, B Li, C Miao - First Conference on Language Modeling", "abstract": "A major limitation of prompt tuning is its dependence on large labeled training datasets. Under few-shot learning settings, prompt tuning lags far behind full-model fine-tuning, limiting its scope of application. In this paper, we leverage the powerful \u2026"}]
