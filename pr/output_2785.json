[{"title": "Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI", "link": "https://openreview.net/pdf%3Fid%3DPrmxFWI1Fr", "details": "T Papamarkou, M Skoularidou, K Palla, L Aitchison\u2026 - Forty-first International \u2026, 2024", "abstract": "In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of \u2026"}, {"title": "Zero-Shot Out-of-Distribution Detection with Outlier Label Exposure", "link": "https://arxiv.org/pdf/2406.01170", "details": "C Ding, G Pang - arXiv preprint arXiv:2406.01170, 2024", "abstract": "As vision-language models like CLIP are widely applied to zero-shot tasks and gain remarkable performance on in-distribution (ID) data, detecting and rejecting out-of- distribution (OOD) inputs in the zero-shot setting have become crucial for ensuring \u2026"}, {"title": "Partially Stochastic Infinitely Deep Bayesian Neural Networks", "link": "https://openreview.net/pdf%3Fid%3DjNab9mXEyj", "details": "SC Ordo\u00f1ez, M Meunier, F Piatti, Y Shi - Forty-first International Conference on Machine \u2026", "abstract": "In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is \u2026"}, {"title": "Posterior Inference on Shallow Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance", "link": "https://openreview.net/pdf%3Fid%3DJ97bdMR7Lv", "details": "J Loria, A Bhadra - The 40th Conference on Uncertainty in Artificial \u2026", "abstract": "From the classical and influential works of Neal (1996), it is known that the infinite width scaling limit of a Bayesian neural network with one hidden layer is a Gaussian process, when the network weights have bounded prior variance. Neal's result has \u2026"}, {"title": "Gaussian processes based data augmentation and expected signature for time series classification", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10546274.pdf", "details": "F Triggiano, M Romito - IEEE Access, 2024", "abstract": "Time series classification tasks play a crucial role in extracting relevant information from data equipped with a temporal structure. In various scientific domains, such as biology or finance, this kind of data comes from complex and hardly predictable \u2026"}, {"title": "Zero-Shot Video Editing through Adaptive Sliding Score Distillation", "link": "https://arxiv.org/pdf/2406.04888", "details": "L Zhu, Y Bao, J Huo, J Wu, YK Lai, W Li, Y Gao - arXiv preprint arXiv:2406.04888, 2024", "abstract": "The burgeoning field of text-based video generation (T2V) has reignited significant interest in the research of controllable video editing. Although pre-trained T2V-based editing models have achieved efficient editing capabilities, current works are still \u2026"}, {"title": "Generative Pre-Trained Diffusion Paradigm for Zero-Shot Time Series Forecasting", "link": "https://arxiv.org/pdf/2406.02212", "details": "J Yang, T Dai, N Li, J Wu, P Liu, J Li, J Bao, H Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, generative pre-trained paradigms such as Large Language Models (LLMs) and Large Vision Models (LVMs) have achieved revolutionary advancements and widespread real-world applications. Particularly, the emergence of pre-trained \u2026"}, {"title": "Demonstration Augmentation for Zero-shot In-context Learning", "link": "https://arxiv.org/pdf/2406.01224", "details": "Y Su, Y Tai, Y Ji, J Li, B Yan, M Zhang - arXiv preprint arXiv:2406.01224, 2024", "abstract": "Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates. However, many studies \u2026"}, {"title": "AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs", "link": "https://arxiv.org/pdf/2405.13358", "details": "A Ghaffari, S Younesian, VP Nia, B Chen, M Asgharian - arXiv preprint arXiv \u2026, 2024", "abstract": "The ever-growing computational complexity of Large Language Models (LLMs) necessitates efficient deployment strategies. The current state-of-the-art approaches for Post-training Quantization (PTQ) often require calibration to achieve the desired \u2026"}]
