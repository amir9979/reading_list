[{"title": "VLEER: Vision and Language Embeddings for Explainable Whole Slide Image Representation", "link": "https://arxiv.org/pdf/2502.20850", "details": "AT Nguyen, K Byeon, K Kim, JT Kwak - arXiv preprint arXiv:2502.20850, 2025", "abstract": "Recent advances in vision-language models (VLMs) have shown remarkable potential in bridging visual and textual modalities. In computational pathology, domain-specific VLMs, which are pre-trained on extensive histopathology image-text \u2026"}, {"title": "PathVQ: Reforming Computational Pathology Foundation Model for Whole Slide Image Analysis via Vector Quantization", "link": "https://arxiv.org/pdf/2503.06482", "details": "H Li, Z Shui, Y Zhang, C Zhu, L Yang - arXiv preprint arXiv:2503.06482, 2025", "abstract": "Computational pathology and whole-slide image (WSI) analysis are pivotal in cancer diagnosis and prognosis. However, the ultra-high resolution of WSIs presents significant modeling challenges. Recent advancements in pathology foundation \u2026"}, {"title": "Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy", "link": "https://arxiv.org/pdf/2503.19757", "details": "Z Hou, T Zhang, Y Xiong, H Duan, H Pu, R Tong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains \u2026"}, {"title": "Correcting Deviations from Normality: A Reformulated Diffusion Model for Multi-Class Unsupervised Anomaly Detection", "link": "https://arxiv.org/pdf/2503.19357", "details": "F Beizaee, GA Lodygensky, C Desrosiers, J Dolz - arXiv preprint arXiv:2503.19357, 2025", "abstract": "Recent advances in diffusion models have spurred research into their application for Reconstruction-based unsupervised anomaly detection. However, these methods may struggle with maintaining structural integrity and recovering the anomaly-free \u2026"}, {"title": "Spatial regularisation for improved accuracy and interpretability in keypoint-based registration", "link": "https://arxiv.org/pdf/2503.04499%3F", "details": "B Billot, R Muthukrishnan, E Abaci-Turk, EP Grant\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Unsupervised registration strategies bypass requirements in ground truth transforms or segmentations by optimising similarity metrics between fixed and moved volumes. Among these methods, a recent subclass of approaches based on unsupervised \u2026"}, {"title": "HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model", "link": "https://arxiv.org/pdf/2503.10631%3F", "details": "J Liu, H Chen, P An, Z Liu, R Zhang, C Gu, X Li, Z Guo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in vision-language models (VLMs) for common-sense reasoning have led to the development of vision-language-action (VLA) models, enabling robots to perform generalized manipulation. Although existing \u2026"}, {"title": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots", "link": "https://arxiv.org/pdf/2503.14734", "details": "J Bjorck, F Casta\u00f1eda, N Cherniadev, X Da, R Ding\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "General-purpose robots need a versatile body and an intelligent mind. Recent advancements in humanoid robots have shown great promise as a hardware platform for building generalist autonomy in the human world. A robot foundation \u2026"}, {"title": "DMFFT: improving the generation quality of diffusion models using fast Fourier transform", "link": "https://www.nature.com/articles/s41598-025-94381-8", "details": "C Yu, C Han, C Zhang - Scientific Reports, 2025", "abstract": "In this study, we demonstrate the significant development potential of diffusion U-Net extraction features transferred to the frequency domain, opening up a new perspective for diffusion models and generating a new optimization idea for diffusion \u2026"}, {"title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs", "link": "https://arxiv.org/pdf/2503.01743%3F", "details": "A Abouelenin, A Ashfaq, A Atkinson, H Awadalla\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent \u2026"}]
