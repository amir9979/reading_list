[{"title": "Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt", "link": "https://arxiv.org/pdf/2406.04031", "details": "Z Ying, A Liu, T Zhang, Z Yu, S Liang, X Liu, D Tao - arXiv preprint arXiv:2406.04031, 2024", "abstract": "In the realm of large vision language models (LVLMs), jailbreak attacks serve as a red-teaming approach to bypass guardrails and uncover safety implications. Existing jailbreaks predominantly focus on the visual modality, perturbing solely visual inputs \u2026"}, {"title": "BrainFounder: Towards Brain Foundation Models for Neuroimage Analysis", "link": "https://arxiv.org/pdf/2406.10395", "details": "J Cox, P Liu, SE Stolte, Y Yang, K Liu, KB See, H Ju\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The burgeoning field of brain health research increasingly leverages artificial intelligence (AI) to interpret and analyze neurological data. This study introduces a novel approach towards the creation of medical foundation models by integrating a \u2026"}]
