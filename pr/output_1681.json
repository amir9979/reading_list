'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [MEDVOC: Vocabulary Adaptation for Fine-tuning Pre-trai'
[{"title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision", "link": "https://arxiv.org/pdf/2405.04086", "details": "Y Tong, S Wang, D Li, Y Wang, S Han, Z Lin, C Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations \u2026"}, {"title": "Causal Evaluation of Language Models", "link": "https://arxiv.org/pdf/2405.00622", "details": "S Chen, B Peng, M Chen, R Wang, M Xu, X Zeng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for \u2026"}, {"title": "Autonomous Data Selection with Language Models for Mathematical Texts", "link": "https://openreview.net/pdf%3Fid%3DbBF077z8LF", "details": "Y Zhang, Y Luo, Y Yuan, AC Yao - ICLR 2024 Workshop on Navigating and \u2026, 2024", "abstract": "To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or \u2026"}, {"title": "Object Registration in Neural Fields", "link": "https://arxiv.org/pdf/2404.18381", "details": "D Hall, S Hausler, S Mahendren, P Moghadam - arXiv preprint arXiv:2404.18381, 2024", "abstract": "Neural fields provide a continuous scene representation of 3D geometry and appearance in a way which has great promise for robotics applications. One functionality that unlocks unique use-cases for neural fields in robotics is object 6 \u2026"}, {"title": "A Foundation Model for Brain Lesion Segmentation with Mixture of Modality Experts", "link": "https://arxiv.org/abs/2405.10246", "details": "X Zhang, N Ou, BD Basaran, M Visentin, M Qiao, R Gu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Brain lesion segmentation plays an essential role in neurological research and diagnosis. As brain lesions can be caused by various pathological alterations, different types of brain lesions tend to manifest with different characteristics on \u2026"}, {"title": "Addax: Memory-Efficient Fine-Tuning of Language Models with a Combination of Forward-Backward and Forward-Only Passes", "link": "https://openreview.net/pdf%3Fid%3DYtZv36CY5p", "details": "Z Li, X Zhang, M Razaviyayn - 5th Workshop on practical ML for limited/low resource \u2026", "abstract": "Fine-tuning language models (LMs) with first-order optimizers often demands excessive memory, limiting accessibility, while zeroth-order optimizers use less memory, but suffer from slow convergence depending on model size. We introduce a \u2026"}, {"title": "Phi-3 technical report: A highly capable language model locally on your phone", "link": "https://arxiv.org/pdf/2404.14219%3Ftrk%3Dpublic_post_comment-text", "details": "M Abdin, SA Jacobs, AA Awan, J Aneja, A Awadallah\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT \u2026"}, {"title": "DynaMo: Accelerating Language Model Inference with Dynamic Multi-Token Sampling", "link": "https://arxiv.org/pdf/2405.00888", "details": "S Tuli, CH Lin, YC Hsu, NK Jha, Y Shen, H Jin - arXiv preprint arXiv:2405.00888, 2024", "abstract": "Traditional language models operate autoregressively, ie, they predict one token at a time. Rapid explosion in model sizes has resulted in high inference times. In this work, we propose DynaMo, a suite of multi-token prediction language models that \u2026"}, {"title": "NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli", "link": "https://arxiv.org/pdf/2405.02814", "details": "X Wang, C Li, Y Chang, J Wang, Y Wu - arXiv preprint arXiv:2405.02814, 2024", "abstract": "Large Language Models (LLMs) have become integral to a wide spectrum of applications, ranging from traditional computing tasks to advanced artificial intelligence (AI) applications. This widespread adoption has spurred extensive \u2026"}]
