'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [MedExpQA: Multilingual Benchmarking of Large Language '
[{"title": "Investigating Regularization of Self-Play Language Models", "link": "https://arxiv.org/html/2404.04291v1", "details": "R Alami, A Abubaker, M Achab, MEA Seddik, S Lahlou - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper explores the effects of various forms of regularization in the context of language model alignment via self-play. While both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) require to collect \u2026"}, {"title": "MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering", "link": "https://arxiv.org/pdf/2403.19116", "details": "C Guan, M Huang, P Zhang - arXiv preprint arXiv:2403.19116, 2024", "abstract": "In today's fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested \u2026"}, {"title": "PRobELM: Plausibility Ranking Evaluation for Language Models", "link": "https://arxiv.org/pdf/2404.03818", "details": "Z Yuan, C Whitehouse, E Chamoun, R Aly, A Vlachos - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces PRobELM (Plausibility Ranking Evaluation for Language Models), a benchmark designed to assess language models' ability to discern more plausible from less plausible scenarios through their parametric knowledge. While \u2026"}, {"title": "Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models", "link": "https://arxiv.org/pdf/2403.12966", "details": "Z Liu, Y Dong, Y Rao, J Zhou, J Lu - arXiv preprint arXiv:2403.12966, 2024", "abstract": "In the realm of vision-language understanding, the proficiency of models in interpreting and reasoning over visual content has become a cornerstone for numerous applications. However, it is challenging for the visual encoder in Large \u2026"}, {"title": "Language models scale reliably with over-training and on downstream tasks", "link": "https://arxiv.org/pdf/2403.08540", "details": "SY Gadre, G Smyrnis, V Shankar, S Gururangan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute \u2026"}, {"title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models", "link": "https://arxiv.org/pdf/2403.18814", "details": "Y Li, Y Zhang, C Wang, Z Zhong, Y Chen, R Chu, S Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared \u2026"}, {"title": "Learn\" No\" to Say\" Yes\" Better: Improving Vision-Language Models via Negations", "link": "https://arxiv.org/html/2403.20312v1", "details": "J Singh, I Shrivastava, M Vatsa, R Singh, A Bharati - arXiv preprint arXiv:2403.20312, 2024", "abstract": "Existing vision-language models (VLMs) treat text descriptions as a unit, confusing individual concepts in a prompt and impairing visual semantic matching and reasoning. An important aspect of reasoning in logic and language is negations. This \u2026"}, {"title": "CLUE: A Clinical Language Understanding Evaluation for LLMs", "link": "https://arxiv.org/pdf/2404.04067", "details": "A Dada, M Bauer, AB Contreras, OA Kora\u015f, CM Seibold\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have shown the potential to significantly contribute to patient care, diagnostics, and administrative processes. Emerging biomedical LLMs address healthcare-specific challenges, including privacy demands and \u2026"}, {"title": "HyperCLOVA X Technical Report", "link": "https://arxiv.org/html/2404.01954v1/", "details": "KM Yoo, J Han, S In, H Jeon, J Jeong, J Kang, H Kim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English \u2026"}]
