[{"title": "RetinalGPT: A Retinal Clinical Preference Conversational Assistant Powered by Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.03987", "details": "W Zhu, X Li, X Chen, P Qiu, VK Vasa, X Dong, Y Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, Multimodal Large Language Models (MLLMs) have gained significant attention for their remarkable ability to process and analyze non-textual data, such as images, videos, and audio. Notably, several adaptations of general-domain MLLMs \u2026"}, {"title": "Better Pseudo-Labeling for Semi-Supervised Domain Generalization in Medical Magnetic Resonance Image Segmentation", "link": "https://link.springer.com/article/10.1007/s44196-025-00786-8", "details": "L Hu, Z Meng, C Tan, Y Zhou - International Journal of Computational Intelligence \u2026, 2025", "abstract": "Magnetic resonance image (MRI) is the primary diagnostic test used clinically for the diagnosis and assessment of a wide range of diseases. In recent years, many studies have employed artificial intelligence techniques for MRI segmentation. Deep \u2026"}, {"title": "A Shared Encoder Approach to Multimodal Representation Learning", "link": "https://arxiv.org/pdf/2503.01654", "details": "S Roy, F Ogidi, A Etemad, E Dolatabadi, A Afkanpour - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal representation learning has demonstrated remarkable potential in enabling models to process and integrate diverse data modalities, such as text and images, for improved understanding and performance. While the medical domain \u2026"}, {"title": "Semantic Retrieval Augmented Contrastive Learning for Sequential Recommendation", "link": "https://arxiv.org/pdf/2503.04162", "details": "Z Cui, Y Weng, X Tang, X Zhang, D Liu, S Li, P Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Sequential recommendation aims to model user preferences based on historical behavior sequences, which is crucial for various online platforms. Data sparsity remains a significant challenge in this area as most users have limited interactions \u2026"}, {"title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures", "link": "https://arxiv.org/pdf/2503.01496", "details": "D Lan, W Sun, J Hu, J Du, Y Cheng - arXiv preprint arXiv:2503.01496, 2025", "abstract": "Transformers with linear recurrent modeling offer linear-time training and constant- memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky \u2026"}, {"title": "Red teaming ChatGPT in medicine to yield real-world insights on model behavior", "link": "https://www.nature.com/articles/s41746-025-01542-0", "details": "CT Chang, H Farah, H Gui, SJ Rezaei, C Bou-Khalil\u2026 - npj Digital Medicine, 2025", "abstract": "Red teaming, the practice of adversarially exposing unexpected or undesired model behaviors, is critical towards improving equity and accuracy of large language models, but non-model creator-affiliated red teaming is scant in healthcare. We \u2026"}, {"title": "Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding", "link": "https://arxiv.org/pdf/2503.01422", "details": "Y Wang, P Zhang, S Huang, B Yang, Z Zhang, F Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Test-time scaling improves large language model performance by adding extra compute during decoding. Best-of-N (BoN) sampling serves as a common scaling technique, broadening the search space for finding better solutions from the model \u2026"}, {"title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems", "link": "https://arxiv.org/pdf/2503.03686", "details": "R Ye, S Tang, R Ge, Y Du, Z Yin, S Chen, J Shao - arXiv preprint arXiv:2503.03686, 2025", "abstract": "LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability \u2026"}]
