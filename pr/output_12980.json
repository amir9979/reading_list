[{"title": "Vulnerability Mitigation for Safety-Aligned Language Models via Debiasing", "link": "https://arxiv.org/pdf/2502.02153", "details": "TQ Tran, A Wachi, R Sato, T Tanabe, Y Akimoto - arXiv preprint arXiv:2502.02153, 2025", "abstract": "Safety alignment is an essential research topic for real-world AI applications. Despite the multifaceted nature of safety and trustworthiness in AI, current safety alignment methods often focus on a comprehensive notion of safety. By carefully assessing \u2026"}, {"title": "Learning Conformal Abstention Policies for Adaptive Risk Management in Large Language and Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06884", "details": "S Tayebati, D Kumar, N Darabi, D Jayasuriya\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language and Vision-Language Models (LLMs/VLMs) are increasingly used in safety-critical applications, yet their opaque decision-making complicates risk assessment and reliability. Uncertainty quantification (UQ) helps assess prediction \u2026"}, {"title": "Scaling Pre-training to One Hundred Billion Data for Vision Language Models", "link": "https://arxiv.org/pdf/2502.07617", "details": "X Wang, I Alabdulmohsin, D Salz, Z Li, K Rong, X Zhai - arXiv preprint arXiv \u2026, 2025", "abstract": "We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric \u2026"}, {"title": "Tool Learning in the Wild: Empowering Language Models as Automatic Tool Agents", "link": "https://openreview.net/pdf%3Fid%3DT4wMdeFEjX", "details": "Z Shi, S Gao, L Yan, Y Feng, X Chen, Z Chen, D Yin\u2026 - THE WEB CONFERENCE 2025", "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a promising approach to extend their utility, enabling them to solve practical tasks. Previous methods manually parse tool documentation and create in-context \u2026"}, {"title": "When the LM misunderstood the human chuckled: Analyzing garden path effects in humans and language models", "link": "https://arxiv.org/pdf/2502.09307", "details": "SJ Amouyal, A Meltzer-Asscher, J Berant - arXiv preprint arXiv:2502.09307, 2025", "abstract": "Modern Large Language Models (LLMs) have shown human-like abilities in many language tasks, sparking interest in comparing LLMs' and humans' language processing. In this paper, we conduct a detailed comparison of the two on a sentence \u2026"}, {"title": "Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models", "link": "https://arxiv.org/pdf/2501.18533%3F", "details": "Y Ding, L Li, B Cao, J Shao - arXiv preprint arXiv:2501.18533, 2025", "abstract": "Large Vision-Language Models (VLMs) have achieved remarkable performance across a wide range of tasks. However, their deployment in safety-critical domains poses significant challenges. Existing safety fine-tuning methods, which focus on \u2026"}, {"title": "Personalization Toolkit: Training Free Personalization of Large Vision Language Models", "link": "https://arxiv.org/pdf/2502.02452%3F", "details": "S Seifi, V Dorovatas, DO Reino, R Aljundi - arXiv preprint arXiv:2502.02452, 2025", "abstract": "Large Vision Language Models (LVLMs) have significant potential to deliver personalized assistance by adapting to individual users' unique needs and preferences. Personalization of LVLMs is an emerging area that involves \u2026"}, {"title": "Personalizing Vision-Language Models With Hybrid Prompts for Zero-Shot Anomaly Detection", "link": "https://ieeexplore.ieee.org/abstract/document/10884560/", "details": "Y Cao, X Xu, Y Cheng, C Sun, Z Du, L Gao, W Shen - IEEE Transactions on \u2026, 2025", "abstract": "Zero-shot anomaly detection (ZSAD) aims to develop a foundational model capable of detecting anomalies across arbitrary categories without relying on reference images. However, since \u201cabnormality\u201d is inherently defined in relation to \u201cnormality\u201d \u2026"}, {"title": "Self-Consistency of the Internal Reward Models Improves Self-Rewarding Language Models", "link": "https://arxiv.org/pdf/2502.08922", "details": "X Zhou, Y Guo, R Ma, T Gui, Q Zhang, X Huang - arXiv preprint arXiv:2502.08922, 2025", "abstract": "Aligning Large Language Models (LLMs) with human preferences is crucial for their deployment in real-world applications. Recent advancements in Self-Rewarding Language Models suggest that an LLM can use its internal reward models (such as \u2026"}]
