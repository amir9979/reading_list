[{"title": "Self-supervised Pre-training via Multi-view Graph Information Bottleneck for Molecular Property Prediction", "link": "https://ieeexplore.ieee.org/abstract/document/10584266/", "details": "X Zang, J Zhang, B Tang - IEEE Journal of Biomedical and Health Informatics, 2024", "abstract": "Molecular representation learning has remarkably accelerated the development of drug analysis and discovery. It implements machine learning methods to encode molecule embeddings for diverse downstream drug-related tasks. Due to the scarcity \u2026"}, {"title": "MFC-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2406.11288", "details": "S Wang, H Lin, Z Luo, Z Ye, G Chen, J Ma - arXiv preprint arXiv:2406.11288, 2024", "abstract": "Large vision-language models (LVLMs) have significantly improved multimodal reasoning tasks, such as visual question answering and image captioning. These models embed multimodal facts within their parameters, rather than relying on \u2026"}, {"title": "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification", "link": "https://arxiv.org/pdf/2406.14496", "details": "G Geigle, R Timofte, G Glava\u0161 - arXiv preprint arXiv:2406.14496, 2024", "abstract": "Recent Large Vision-Language Models (LVLMs) demonstrate impressive abilities on numerous image understanding and reasoning tasks. The task of fine-grained object classification (eg, distinction between\\textit {animal species}), however, has been \u2026"}, {"title": "SaCo Loss: Sample-wise Affinity Consistency for Vision-Language Pre-training", "link": "http://openaccess.thecvf.com/content/CVPR2024/papers/Wu_SaCo_Loss_Sample-wise_Affinity_Consistency_for_Vision-Language_Pre-training_CVPR_2024_paper.pdf", "details": "S Wu, H Tan, Z Tian, Y Chen, X Qi, J Jia - Proceedings of the IEEE/CVF Conference \u2026, 2024", "abstract": "Vision-language pre-training (VLP) aims to learn joint representations of vision and language modalities. The contrastive paradigm is currently dominant in this field. However we observe a notable misalignment phenomenon that is the affinity \u2026"}, {"title": "EgoVideo: Exploring Egocentric Foundation Model and Downstream Adaptation", "link": "https://arxiv.org/pdf/2406.18070", "details": "B Pei, G Chen, J Xu, Y He, Y Liu, K Pan, Y Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this report, we present our solutions to the EgoVis Challenges in CVPR 2024, including five tracks in the Ego4D challenge and three tracks in the EPIC-Kitchens challenge. Building upon the video-language two-tower model and leveraging our \u2026"}, {"title": "Automated ICD Coding via Contrastive Learning With Back-Reference and Synonym Knowledge for Smart Self-Diagnosis Applications", "link": "https://ieeexplore.ieee.org/abstract/document/10585290/", "details": "Z Zhao, W Lu, X Peng, L Xing, W Zhang, C Zheng - IEEE Transactions on Consumer \u2026, 2024", "abstract": "Smart applications are essential in intelligent consumer electronics. With the rising focus on health concerns, self-diagnosis applications on smart devices have gained widespread popularity, attracting significant attention from both consumers and \u2026"}, {"title": "ASIMO: Agent-centric scene representation in multi-object manipulation", "link": "https://journals.sagepub.com/doi/abs/10.1177/02783649241257537", "details": "CH Min, YM Kim - The International Journal of Robotics Research, 2024", "abstract": "Vision-based reinforcement learning (RL) is a generalizable way to control an agent because it is agnostic of specific hardware configurations. As visual observations are highly entangled, attempts for vision-based RL rely on scene representation that \u2026"}]
