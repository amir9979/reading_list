[{"title": "Retrieval-Augmented Generation with **Large Language Models** in Radiology: From Theory to Practice", "link": "https://pubs.rsna.org/doi/abs/10.1148/ryai.240790", "details": "A Fink, A Rau, M Reisert, F Bamberg, MF Russe - Radiology: Artificial Intelligence, 2025", "abstract": "\u2026 This approach enables **large** **language** **models** to function as complementary tools for radiologists, helping optimize patient care and meet \u2026 One promising approach is the use of **large** **language** **models** (LLMs) such as ChatGPT (2), Claude (3) \u2026"}, {"title": "A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization", "link": "https://arxiv.org/pdf/2506.04156", "details": "S Soni, D Demner-Fushman - arXiv preprint arXiv:2506.04156, 2025", "abstract": "\u2026 **answers**. To establish benchmarks for grounded EHR **question** **answering** (QA), we evaluated three openweight **large** **language** **models** (LLMs\u2026 strategies: (1) generating **answers** with citations to **clinical** note sentences, (2) generating **answers** \u2026", "entry_id": "http://arxiv.org/abs/2506.04156v1", "updated": "2025-06-04 16:55:08", "published": "2025-06-04 16:55:08", "authors": "Sarvesh Soni;Dina Demner-Fushman", "summary": "Patients have distinct information needs about their hospitalization that can\nbe addressed using clinical evidence from electronic health records (EHRs).\nWhile artificial intelligence (AI) systems show promise in meeting these needs,\nrobust datasets are needed to evaluate the factual accuracy and relevance of\nAI-generated responses. To our knowledge, no existing dataset captures patient\ninformation needs in the context of their EHRs. We introduce ArchEHR-QA, an\nexpert-annotated dataset based on real-world patient cases from intensive care\nunit and emergency department settings. The cases comprise questions posed by\npatients to public health forums, clinician-interpreted counterparts, relevant\nclinical note excerpts with sentence-level relevance annotations, and\nclinician-authored answers. To establish benchmarks for grounded EHR question\nanswering (QA), we evaluated three open-weight large language models\n(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:\ngenerating (1) answers with citations to clinical note sentences, (2) answers\nbefore citations, and (3) answers from filtered citations. We assessed\nperformance on two dimensions: Factuality (overlap between cited note sentences\nand ground truth) and Relevance (textual and semantic similarity between system\nand reference answers). The final dataset contains 134 patient cases. The\nanswer-first prompting approach consistently performed best, with Llama 4\nachieving the highest scores. Manual error analysis supported these findings\nand revealed common issues such as omitted key clinical evidence and\ncontradictory or hallucinated content. Overall, ArchEHR-QA provides a strong\nbenchmark for developing and evaluating patient-centered EHR QA systems,\nunderscoring the need for further progress toward generating factual and\nrelevant responses in clinical contexts.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.04156v1;http://arxiv.org/pdf/2506.04156v1", "pdf_url": "http://arxiv.org/pdf/2506.04156v1"}, {"title": "Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing", "link": "https://arxiv.org/pdf/2506.03490", "details": "S Chen, L Luo, Z Qiu, Y Cao, C Yang, S Pan - arXiv preprint arXiv:2506.03490, 2025", "abstract": "\u2026 Recently, knowledge editing (KE) has emerged as a promising approach to update specific facts in **Large** **Language** **Models** (\u2026 **answer** a for each **question** q \u2208 Aori. Then, the model\u2019s performance is evaluated on the three test sets Aori UAgen \u2026", "entry_id": "http://arxiv.org/abs/2506.03490v2", "updated": "2025-06-05 03:20:15", "published": "2025-06-04 02:14:43", "authors": "Shigeng Chen;Linhao Luo;Zhangchi Qiu;Yanan Cao;Carl Yang;Shirui Pan", "summary": "Recently, knowledge editing (KE) has emerged as a promising approach to\nupdate specific facts in Large Language Models (LLMs) without the need for full\nretraining. Despite the effectiveness in general-domain benchmarks, their\napplicability to complex medical domain remains largely unexplored. Medical\nknowledge editing is particularly challenging, as it requires LLMs to\ninternalize the knowledge and generalize to unseen scenarios for effective and\ninterpretable decision-making. In this work, we propose a novel framework\ncalled MedEditBench to rigorously evaluate the effectiveness of existing KE\nmethods in the medical domain. In MedEditBench, we introduce a new medical\nknowledge editing benchmark as well as three different knowledge editing\nparadigms, which are designed to assess the impact of different knowledge\nsources for editing. Our findings indicate that current KE methods result in\nonly superficial memorization of the injected information, failing to\ngeneralize to new scenarios. To overcome this limitation, we present\nSelf-Generated Rationale Editing (SGR-Edit), which utilizes model-derived\nrationales as the target knowledge for editing, thereby uncovering the\nunderlying reasoning process and demonstrating significant improvements over\nexisting KE approaches. Additionally, we offer deeper insights into medical\nknowledge editing, including the localization of medical knowledge in LLMs and\nthe impact of sequential editing on evolving knowledge. This could provide\npractical guidance for implementing KE methods in real-world medical\napplications.", "comment": "Under Review", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.03490v2;http://arxiv.org/pdf/2506.03490v2", "pdf_url": "http://arxiv.org/pdf/2506.03490v2"}, {"title": "High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning", "link": "https://arxiv.org/pdf/2506.04051", "details": "T Franzmeyer, A Sravankumar, L Liu, Y Mao, R Hou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Towards safety and helpfulness balanced responses via controllable **large** **language** **models**. \u2026 The art of refusal: A survey of abstention in **large** **language** **models**. arXiv e-prints, pages \u2026 R-tuning: Instructing **large** **language** **models** to say \u2018i \u2026", "entry_id": "http://arxiv.org/abs/2506.04051v1", "updated": "2025-06-04 15:16:21", "published": "2025-06-04 15:16:21", "authors": "Tim Franzmeyer;Archie Sravankumar;Lijuan Liu;Yuning Mao;Rui Hou;Sinong Wang;Jakob N. Foerster;Luke Zettlemoyer;Madian Khabsa", "summary": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.04051v1;http://arxiv.org/pdf/2506.04051v1", "pdf_url": "http://arxiv.org/pdf/2506.04051v1"}, {"title": "Design and Development of an Educational Platform for Psychiatry", "link": "https://dspace.cvut.cz/bitstream/handle/10467/122449/F3-BP-2025-Petrishchev-Iaroslav-Bakalarska%2520prace%2520Petrishchev%2520Iaroslav.pdf%3Fsequence%3D-1", "details": "P Iaroslav - 2025", "abstract": "\u2026 The publication of this work led to a boom in the development of **large** **language** **models** and was marked by the introduction of BERT (from Google) and GPT-1 (from OpenAI) in 2018. After that, LLMs became larger, more advanced, and more \u2026"}]
