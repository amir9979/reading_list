[{"title": "Attention Prompting on Image for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.17143", "details": "R Yu, W Yu, X Wang - arXiv preprint arXiv:2409.17143, 2024", "abstract": "Compared with Large Language Models (LLMs), Large Vision-Language Models (LVLMs) can also accept images as input, thus showcasing more interesting emergent capabilities and demonstrating impressive performance on various vision \u2026"}, {"title": "VLM's Eye Examination: Instruct and Inspect Visual Competency of Vision Language Models", "link": "https://arxiv.org/pdf/2409.14759", "details": "N Hyeon-Woo, M Ye-Bin, W Choi, L Hyun, TH Oh - arXiv preprint arXiv:2409.14759, 2024", "abstract": "Vision language models (VLMs) have shown promising reasoning capabilities across various benchmarks; however, our understanding of their visual perception remains limited. In this work, we propose an eye examination process to investigate \u2026"}, {"title": "Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models", "link": "https://arxiv.org/pdf/2409.12512", "details": "J Rao, X Liu, Z Lin, L Ding, J Li, D Tao - arXiv preprint arXiv:2409.12512, 2024", "abstract": "Knowledge distillation (KD) is a technique that compresses large teacher models by training smaller student models to mimic them. The success of KD in auto-regressive language models mainly relies on Reverse KL for mode-seeking and student \u2026"}, {"title": "Improving the Efficiency of Visually Augmented Language Models", "link": "https://arxiv.org/pdf/2409.11148", "details": "P Ontalvilla, A Ormazabal, G Azkune - arXiv preprint arXiv:2409.11148, 2024", "abstract": "Despite the impressive performance of autoregressive Language Models (LM) it has been shown that due to reporting bias, LMs lack visual knowledge, ie they do not know much about the visual world and its properties. To augment LMs with visual \u2026"}, {"title": "Language Models Learn to Mislead Humans via RLHF", "link": "https://arxiv.org/pdf/2409.12822", "details": "J Wen, R Zhong, A Khan, E Perez, J Steinhardt\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) can produce errors that are hard to detect for humans, especially when the task is complex. RLHF, the most popular post-training method, may exacerbate this problem: to achieve higher rewards, LMs might get better at \u2026"}, {"title": "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models", "link": "https://arxiv.org/pdf/2409.13592", "details": "A Nandy, Y Agarwal, A Patwa, MM Das, A Bansal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Understanding satire and humor is a challenging task for even current Vision- Language models. In this paper, we propose the challenging tasks of Satirical Image Detection (detecting whether an image is satirical), Understanding (generating the \u2026"}, {"title": "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation", "link": "https://arxiv.org/pdf/2409.13928", "details": "S Lee, S Kim, J Jang, H Chon, D Lee, H Yu - arXiv preprint arXiv:2409.13928, 2024", "abstract": "We study the code generation behavior of instruction-tuned models built on top of code pre-trained language models when they could access an auxiliary function to implement a function. We design several ways to provide auxiliary functions to the \u2026"}, {"title": "Aligning Language Models Using Follow-up Likelihood as Reward Signal", "link": "https://arxiv.org/pdf/2409.13948", "details": "C Zhang, D Chong, F Jiang, C Tang, A Gao, G Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In natural human-to-human conversations, participants often receive feedback signals from one another based on their follow-up reactions. These reactions can include verbal responses, facial expressions, changes in emotional state, and other \u2026"}, {"title": "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "link": "https://arxiv.org/pdf/2409.14083", "details": "J Sun, J Zhang, Y Zhou, Z Su, X Qu, Y Cheng - arXiv preprint arXiv:2409.14083, 2024", "abstract": "Large Vision-Language Models (LVLMs) have become pivotal at the intersection of computer vision and natural language processing. However, the full potential of LVLMs Retrieval-Augmented Generation (RAG) capabilities remains underutilized \u2026"}]
