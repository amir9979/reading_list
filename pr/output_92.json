'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Taming Pre-trained LLMs for Generalised Time Series F'
[{"title": "Simple linear attention language models balance the recall-throughput tradeoff", "link": "https://arxiv.org/html/2402.18668v1", "details": "S Arora, S Eyuboglu, M Zhang, A Timalsina, S Alberti\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV \u2026"}, {"title": "Grounding Language Models for Visual Entity Recognition", "link": "https://arxiv.org/pdf/2402.18695", "details": "Z Xiao, M Gong, P Cascante-Bonilla, X Zhang, J Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of \u2026"}, {"title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.07860", "details": "S Zhao, S Hao, B Zi, H Xu, KYK Wong - arXiv preprint arXiv:2403.07860, 2024", "abstract": "Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding \u2026"}, {"title": "Synth $^ 2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings", "link": "https://arxiv.org/pdf/2403.07750", "details": "S Sharifzadeh, C Kaplanis, S Pathak, D Kumaran, A Ilic\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models \u2026"}, {"title": "Unsupervised LLM Adaptation for Question Answering", "link": "https://arxiv.org/pdf/2402.12170", "details": "K Saito, K Sohn, CY Lee, Y Ushiku - arXiv preprint arXiv:2402.12170, 2024", "abstract": "Large language models (LLM) learn diverse knowledge present in the large-scale training dataset via self-supervised training. Followed by instruction-tuning, LLM acquires the ability to return correct information for diverse questions. However \u2026"}, {"title": "$\\textbf {S}^ 2$ IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting", "link": "https://arxiv.org/html/2403.05798v1", "details": "Z Pan, Y Jiang, S Garg, A Schneider, Y Nevmyvaka\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, there has been a growing interest in leveraging pre-trained large language models (LLMs) for various time series applications. However, the semantic space of LLMs, established through the pre-training, is still underexplored and may help yield \u2026"}, {"title": "$\\mathbf {(N, K)} $-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model", "link": "https://arxiv.org/html/2403.07191v1", "details": "Y Zhang, L Chen, B Liu, Y Yang, Q Cui, Y Tao, H Yang - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these \u2026"}, {"title": "Multi-modal Attribute Prompting for Vision-Language Models", "link": "https://arxiv.org/pdf/2403.00219", "details": "X Liu, J Wu, T Zhang - arXiv preprint arXiv:2403.00219, 2024", "abstract": "Large pre-trained Vision-Language Models (VLMs), like CLIP, exhibit strong generalization ability to downstream tasks but struggle in few-shot scenarios. Existing prompting techniques primarily focus on global text and image \u2026"}, {"title": "Rethinking Generative Large Language Model Evaluation for Semantic Comprehension", "link": "https://arxiv.org/pdf/2403.07872", "details": "F Wei, X Chen, L Luo - arXiv preprint arXiv:2403.07872, 2024", "abstract": "Despite their sophisticated capabilities, large language models (LLMs) encounter a major hurdle in effective assessment. This paper first revisits the prevalent evaluation method-multiple choice question answering (MCQA), which allows for \u2026"}]
