[{"title": "Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving", "link": "https://arxiv.org/pdf/2503.21505", "details": "Y Li, M Tian, Z Lin, J Zhu, D Zhu, H Liu, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing benchmarks for Vision-Language Model (VLM) on autonomous driving (AD) primarily assess interpretability through open-form visual question answering (QA) within coarse-grained tasks, which remain insufficient to assess capabilities in \u2026"}, {"title": "Multi-Cue Adaptive Visual Token Pruning for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.08019", "details": "B Luan, W Zhou, H Feng, Z Wang, X Li, H Li - arXiv preprint arXiv:2503.08019, 2025", "abstract": "As the computational needs of Large Vision-Language Models (LVLMs) increase, visual token pruning has proven effective in improving inference speed and memory efficiency. Traditional pruning methods in LVLMs predominantly focus on attention \u2026"}, {"title": "UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces", "link": "https://arxiv.org/pdf/2503.06157", "details": "B Zhao, J Fang, Z Dai, Z Wang, J Zha, W Zhang, C Gao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large multimodal models exhibit remarkable intelligence, yet their embodied cognitive abilities during motion in open-ended urban 3D space remain to be explored. We introduce a benchmark to evaluate whether video-large language \u2026"}, {"title": "Inference retrieval-augmented multi-modal chain-of-thoughts reasoning for language models", "link": "https://ieeexplore.ieee.org/abstract/document/10888701/", "details": "Q He, S Qian, J Zhang, C Wang - ICASSP 2025-2025 IEEE International Conference \u2026, 2025", "abstract": "Recent advancements in Large Language Models (LLMs) have catalyzed the exploration of Chain of Thought (CoT) approaches, particularly in extending their application to multimodal tasks to enhance reasoning capabilities. However, current \u2026"}, {"title": "From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.06260", "details": "M Dai, J Sun, Z Zhao, S Liu, R Li, J Gao, X Li - arXiv preprint arXiv:2503.06260, 2025", "abstract": "Aligning large vision-language models (LVLMs) with human preferences is challenging due to the scarcity of fine-grained, high-quality, and multimodal preference data without human annotations. Existing methods relying on direct \u2026"}, {"title": "Integrating Frequency-Domain Representations with Low-Rank Adaptation in Vision-Language Models", "link": "https://arxiv.org/pdf/2503.06003", "details": "MA Khan, A Gangopadhyay, J Wang, RF Erbacher - arXiv preprint arXiv:2503.06003, 2025", "abstract": "Situational awareness applications rely heavily on real-time processing of visual and textual data to provide actionable insights. Vision language models (VLMs) have become essential tools for interpreting complex environments by connecting visual \u2026"}, {"title": "Balcony: A Lightweight Approach to Dynamic Inference of Generative Language Models", "link": "https://arxiv.org/pdf/2503.05005", "details": "B Jamialahmadi, P Kavehzadeh, M Rezagholizadeh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Deploying large language models (LLMs) in real-world applications is often hindered by strict computational and latency constraints. While dynamic inference offers the flexibility to adjust model behavior based on varying resource budgets, existing \u2026"}, {"title": "Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models", "link": "https://media.roboflow.com/rf100vl/rf100vl.pdf", "details": "P Robicheaux, M Popov, A Madan, I Robinson\u2026", "abstract": "Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to outof-distribution tasks \u2026"}, {"title": "Fine-grained knowledge fusion for retrieval-augmented medical visual question answering", "link": "https://www.sciencedirect.com/science/article/pii/S1566253525001320", "details": "X Liang, D Wang, B Jing, Z Jiao, R Li, R Liu, Q Miao\u2026 - Information Fusion, 2025", "abstract": "Given that medical image analysis often requires experts to recall typical symptoms from diagnostic archives or their own experience, implementing retrieval augmentation in multi-modal tasks like Medical Visual Question Answering \u2026"}]
