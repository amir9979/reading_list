[{"title": "ELAINE-medLLM: Lightweight English Japanese Chinese Trilingual Large Language Model for Bio-medical Domain", "link": "https://aclanthology.org/2025.coling-main.313.pdf", "details": "K Yano, Z Luo, J Huang, Q Xie, M Asada, C Yuan\u2026 - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract We propose ELAINE (EngLish-jApanese-chINesE)-medLLM, a trilingual (English, Japanese, Chinese) large language model adapted for the bio-medical domain based on Llama-3-8B. The training dataset was carefully curated in terms of \u2026"}, {"title": "Verifying Cross-modal Entity Consistency in News using Vision-language Models", "link": "https://arxiv.org/pdf/2501.11403", "details": "S Tahmasebi, E M\u00fcller-Budack, R Ewerth - arXiv preprint arXiv:2501.11403, 2025", "abstract": "The web has become a crucial source of information, but it is also used to spread disinformation, often conveyed through multiple modalities like images and text. The identification of inconsistent cross-modal information, in particular entities such as \u2026"}, {"title": "A Foundation Model for Lesion Segmentation on Brain MRI with Mixture of Modality Experts", "link": "https://ieeexplore.ieee.org/iel8/42/4359023/10879789.pdf", "details": "X Zhang, N Ou, BD Basaran, M Visentin, M Qiao, R Gu\u2026 - IEEE Transactions on \u2026, 2025", "abstract": "Brain lesion segmentation is crucial for neurological disease research and diagnosis. As different types of lesions exhibit distinct characteristics on different imaging modalities, segmentation methods are typically developed in a task-specific \u2026"}, {"title": "Histoires Morales: A French Dataset for Assessing Moral Alignment", "link": "https://arxiv.org/pdf/2501.17117%3F", "details": "T Leteno, I Proskurina, A Gourru, J Velcin, C Laclau\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Aligning language models with human values is crucial, especially as they become more integrated into everyday life. While models are often adapted to user preferences, it is equally important to ensure they align with moral norms and \u2026"}, {"title": "Ocean-OCR: Towards General OCR Application via a Vision-Language Model", "link": "https://arxiv.org/pdf/2501.15558", "details": "S Chen, X Guo, Y Li, T Zhang, M Lin, D Kuang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal large language models (MLLMs) have shown impressive capabilities across various domains, excelling in processing and understanding information from multiple modalities. Despite the rapid progress made previously, insufficient OCR \u2026"}, {"title": "MedS $^ 3$: Towards Medical Small Language Models with Self-Evolved Slow Thinking", "link": "https://arxiv.org/pdf/2501.12051%3F", "details": "S Jiang, Y Liao, Z Chen, Y Zhang, Y Wang, Y Wang - arXiv preprint arXiv:2501.12051, 2025", "abstract": "Medical language models (MLMs) have become pivotal in advancing medical natural language processing. However, prior models that rely on pre-training or supervised fine-tuning often exhibit low data efficiency and limited practicality in real \u2026"}, {"title": "Hierarchical Autoregressive Transformers: Combining Byte-and Word-Level Processing for Robust, Adaptable Language Models", "link": "https://arxiv.org/pdf/2501.10322", "details": "P Neitemeier, B Deiseroth, C Eichenberg, L Balles - arXiv preprint arXiv:2501.10322, 2025", "abstract": "Tokenization is a fundamental step in natural language processing, breaking text into units that computational models can process. While learned subword tokenizers have become the de-facto standard, they present challenges such as large \u2026"}, {"title": "How to Bridge the Gap between Modalities: Survey on Multimodal Large Language Model", "link": "https://ieeexplore.ieee.org/abstract/document/10841938/", "details": "S Song, X Li, S Li, S Zhao, J Yu, J Ma, X Mao, W Zhang\u2026 - IEEE Transactions on \u2026, 2025", "abstract": "We explore Multimodal Large Language Models (MLLMs), which integrate LLMs like GPT-4 to handle multimodal data, including text, images, audio, and more. MLLMs demonstrate capabilities such as generating image captions and answering image \u2026"}, {"title": "RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning", "link": "https://arxiv.org/pdf/2502.00848", "details": "Y Lyu, X Zheng, L Jiang, Y Yan, X Zou, H Zhou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent text-to-image generative models, eg, Stable Diffusion V3 and Flux, have achieved notable progress. However, these models are strongly restricted to their limited knowledge, aka, their own fixed parameters, that are trained with closed \u2026"}]
