[{"title": "Diffusion Bridge AutoEncoders for Unsupervised Representation Learning", "link": "https://arxiv.org/pdf/2405.17111", "details": "Y Kim, K Lee, M Park, B Na, IC Moon - arXiv preprint arXiv:2405.17111, 2024", "abstract": "Diffusion-based representation learning has achieved substantial attention due to its promising capabilities in latent representation and sample generation. Recent studies have employed an auxiliary encoder to identify a corresponding \u2026"}, {"title": "Editable Concept Bottleneck Models", "link": "https://arxiv.org/pdf/2405.15476", "details": "L Hu, C Ren, Z Hu, CL Wang, D Wang - arXiv preprint arXiv:2405.15476, 2024", "abstract": "Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer. However, most previous studies focused on cases where the data, including \u2026"}, {"title": "Recurrent Complex-Weighted Autoencoders for Unsupervised Object Discovery", "link": "https://arxiv.org/pdf/2405.17283", "details": "A Gopalakrishnan, A Stani\u0107, J Schmidhuber, MC Mozer - arXiv preprint arXiv \u2026, 2024", "abstract": "Current state-of-the-art synchrony-based models encode object bindings with complex-valued activations and compute with real-valued weights in feedforward architectures. We argue for the computational advantages of a recurrent architecture \u2026"}, {"title": "Interpretable and Editable Programmatic Tree Policies for Reinforcement Learning", "link": "https://arxiv.org/pdf/2405.14956", "details": "H Kohler, Q Delfosse, R Akrour, K Kersting, P Preux - arXiv preprint arXiv:2405.14956, 2024", "abstract": "Deep reinforcement learning agents are prone to goal misalignments. The black-box nature of their policies hinders the detection and correction of such misalignments, and the trust necessary for real-world deployment. So far, solutions learning \u2026"}, {"title": "Differentiable Cluster Graph Neural Network", "link": "https://arxiv.org/pdf/2405.16185", "details": "Y Dong, MH Dupty, L Deng, Z Liu, YL Goh, WS Lee - arXiv preprint arXiv:2405.16185, 2024", "abstract": "Graph Neural Networks often struggle with long-range information propagation and in the presence of heterophilous neighborhoods. We address both challenges with a unified framework that incorporates a clustering inductive bias into the message \u2026"}, {"title": "Deep Causal Generative Models with Property Control", "link": "https://arxiv.org/pdf/2405.16219", "details": "Q Zhao, S Wang, G Bai, B Pan, Z Qin, L Zhao - arXiv preprint arXiv:2405.16219, 2024", "abstract": "Generating data with properties of interest by external users while following the right causation among its intrinsic factors is important yet has not been well addressed jointly. This is due to the long-lasting challenge of jointly identifying key latent \u2026"}, {"title": "Exposing Image Classifier Shortcuts with Counterfactual Frequency (CoF) Tables", "link": "https://arxiv.org/pdf/2405.15661", "details": "J Hinns, D Martens - arXiv preprint arXiv:2405.15661, 2024", "abstract": "The rise of deep learning in image classification has brought unprecedented accuracy but also highlighted a key issue: the use of'shortcuts' by models. Such shortcuts are easy-to-learn patterns from the training data that fail to generalise to \u2026"}, {"title": "A building aggregation method based on deep clustering of graph vertices", "link": "http://xb.chinasmp.com/EN/Y2024/V53/I4/736", "details": "C Zhanlong, LU Xiechun, XU Yongyang - Acta Geodaetica et Cartographica Sinica", "abstract": "Building element aggregation is pivotal for simplifying spatial structures in cartographic generalization. Conventional rule-based aggregation methods often cannot simultaneously consider the morphological and distributional characteristics \u2026"}, {"title": "Probabilistic Contrastive Learning with Explicit Concentration on the Hypersphere", "link": "https://arxiv.org/pdf/2405.16460", "details": "HB Li, C Ouyang, T Amiranashvili, MS Rosen, B Menze\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-supervised contrastive learning has predominantly adopted deterministic methods, which are not suited for environments characterized by uncertainty and noise. This paper introduces a new perspective on incorporating uncertainty into \u2026"}]
