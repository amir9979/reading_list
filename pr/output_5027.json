[{"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "link": "https://arxiv.org/pdf/2408.00724", "details": "Y Wu, Z Sun, S Li, S Welleck, Y Yang - arXiv preprint arXiv:2408.00724, 2024", "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth \u2026"}, {"title": "Can Language Models Safeguard Themselves, Instantly and For Free?", "link": "https://openreview.net/pdf%3Fid%3DALRWSxT1rl", "details": "D Adila, C Shin, Y Zhang, F Sala - ICML 2024 Next Generation of AI Safety Workshop", "abstract": "Aligning pretrained language models (LMs) to handle a new safety scenario is normally difficult and expensive, often requiring access to large amounts of ground- truth preference data and substantial compute. Are these costs necessary? That is, is \u2026"}, {"title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Trustworthy Response Generation in Chinese", "link": "https://dl.acm.org/doi/pdf/10.1145/3686807", "details": "H Wang, S Zhao, Z Qiang, Z Li, C Liu, N Xi, Y Du, B Qin\u2026 - ACM Transactions on \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to \u2026"}, {"title": "MindLLM: Lightweight large language model pre-training, evaluation and domain application", "link": "https://www.sciencedirect.com/science/article/pii/S2666651024000111", "details": "Y Yang, H Sun, J Li, R Liu, Y Li, Y Liu, Y Gao, H Huang - AI Open, 2024", "abstract": "Abstract Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is \u2026"}, {"title": "In2Core: Leveraging Influence Functions for Coreset Selection in Instruction Finetuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.03560", "details": "AS Joaquin, B Wang, Z Liu, N Asher, B Lim, P Muller\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite advancements, fine-tuning Large Language Models (LLMs) remains costly due to the extensive parameter count and substantial data requirements for model generalization. Accessibility to computing resources remains a barrier for the open \u2026"}, {"title": "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "link": "https://arxiv.org/pdf/2408.01963", "details": "S Ackerman, E Rabinovich, E Farchi, A Anaby-Tavor - arXiv preprint arXiv \u2026, 2024", "abstract": "We evaluate the robustness of several large language models on multiple datasets. Robustness here refers to the relative insensitivity of the model's answers to meaning- preserving variants of their input. Benchmark datasets are constructed by introducing \u2026"}, {"title": "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "link": "https://arxiv.org/pdf/2408.02103", "details": "P Wang, X Wang, C Lou, S Mao, P Xie, Y Jiang - arXiv preprint arXiv:2408.02103, 2024", "abstract": "In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language \u2026"}, {"title": "SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models", "link": "https://arxiv.org/pdf/2408.02632", "details": "M Diao, R Li, S Liu, G Liao, J Wang, X Cai, W Xu - arXiv preprint arXiv:2408.02632, 2024", "abstract": "As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to \u2026"}, {"title": "Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?", "link": "https://arxiv.org/pdf/2408.02651", "details": "MB Karkevandi, N Vishwamitra, P Najafirad - arXiv preprint arXiv:2408.02651, 2024", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in natural language tasks, but their safety and morality remain contentious due to their training on internet text corpora. To address these concerns, alignment techniques \u2026"}]
