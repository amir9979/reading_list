[{"title": "RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics", "link": "https://arxiv.org/pdf/2406.10721", "details": "W Yuan, J Duan, V Blukis, W Pumacay, R Krishna\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "From rearranging objects on a table to putting groceries into shelves, robots must plan precise action points to perform tasks accurately and reliably. In spite of the recent adoption of vision language models (VLMs) to control robot behavior, VLMs \u2026"}, {"title": "ROSA: Random Subspace Adaptation for Efficient Fine-Tuning", "link": "https://arxiv.org/pdf/2407.07802", "details": "MGA Hameed, A Milios, S Reddy, G Rabusseau - arXiv preprint arXiv:2407.07802, 2024", "abstract": "Model training requires significantly more memory, compared with inference. Parameter efficient fine-tuning (PEFT) methods provide a means of adapting large models to downstream tasks using less memory. However, existing methods such as \u2026"}, {"title": "IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model", "link": "https://arxiv.org/pdf/2407.07577", "details": "Y Ji, S Zhang, J Wu, P Sun, W Chen, X Xiao, S Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancement of Large Vision-Language models (LVLMs) has demonstrated a spectrum of emergent capabilities. Nevertheless, current models only focus on the visual content of a single scenario, while their ability to associate \u2026"}, {"title": "Manipulate-Anything: Automating Real-World Robots using Vision-Language Models", "link": "https://arxiv.org/abs/2406.18915", "details": "J Duan, W Yuan, W Pumacay, YR Wang, K Ehsani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large-scale endeavors like RT-1 and widespread community efforts such as Open-X- Embodiment have contributed to growing the scale of robot demonstration data. However, there is still an opportunity to improve the quality, quantity, and diversity of \u2026"}]
