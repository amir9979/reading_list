[{"title": "DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?", "link": "https://arxiv.org/pdf/2505.24173", "details": "T Zhou, Y Xu, Y Zhu, C Xiao, H Bian, L Wei, X Zhang - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) exhibit strong zero-shot generalization on natural images and show early promise in interpretable medical image analysis. However, existing benchmarks do not systematically evaluate whether these models truly \u2026", "entry_id": "http://arxiv.org/abs/2505.24173v1", "updated": "2025-05-30 03:33:25", "published": "2025-05-30 03:33:25", "authors": "Tianhong Zhou;Yin Xu;Yingtao Zhu;Chuxi Xiao;Haiyang Bian;Lei Wei;Xuegong Zhang", "summary": "Vision-language models (VLMs) exhibit strong zero-shot generalization on\nnatural images and show early promise in interpretable medical image analysis.\nHowever, existing benchmarks do not systematically evaluate whether these\nmodels truly reason like human clinicians or merely imitate superficial\npatterns. To address this gap, we propose DrVD-Bench, the first multimodal\nbenchmark for clinical visual reasoning. DrVD-Bench consists of three modules:\nVisual Evidence Comprehension, Reasoning Trajectory Assessment, and Report\nGeneration Evaluation, comprising a total of 7,789 image-question pairs. Our\nbenchmark covers 20 task types, 17 diagnostic categories, and five imaging\nmodalities-CT, MRI, ultrasound, radiography, and pathology. DrVD-Bench is\nexplicitly structured to reflect the clinical reasoning workflow from modality\nrecognition to lesion identification and diagnosis. We benchmark 19 VLMs,\nincluding general-purpose and medical-specific, open-source and proprietary\nmodels, and observe that performance drops sharply as reasoning complexity\nincreases. While some models begin to exhibit traces of human-like reasoning,\nthey often still rely on shortcut correlations rather than grounded visual\nunderstanding. DrVD-Bench offers a rigorous and structured evaluation framework\nto guide the development of clinically trustworthy VLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.24173v1;http://arxiv.org/pdf/2505.24173v1", "pdf_url": "http://arxiv.org/pdf/2505.24173v1"}, {"title": "DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation", "link": "https://arxiv.org/pdf/2505.03401", "details": "S Song, H Tang, H Yang, X Li - arXiv preprint arXiv:2505.03401, 2025", "abstract": "Radiology Report Generation (RRG) automates the creation of radiology reports from medical imaging, enhancing the efficiency of the reporting process. Longitudinal Radiology Report Generation (LRRG) extends RRG by incorporating the ability to \u2026", "entry_id": "http://arxiv.org/abs/2505.03401v1", "updated": "2025-05-06 10:29:23", "published": "2025-05-06 10:29:23", "authors": "Shanshan Song;Hui Tang;Honglong Yang;Xiaomeng Li", "summary": "Radiology Report Generation (RRG) automates the creation of radiology reports\nfrom medical imaging, enhancing the efficiency of the reporting process.\nLongitudinal Radiology Report Generation (LRRG) extends RRG by incorporating\nthe ability to compare current and prior exams, facilitating the tracking of\ntemporal changes in clinical findings. Existing LRRG approaches only extract\nfeatures from prior and current images using a visual pre-trained encoder,\nwhich are then concatenated to generate the final report. However, these\nmethods struggle to effectively capture both spatial and temporal correlations\nduring the feature extraction process. Consequently, the extracted features\ninadequately capture the information of difference across exams and thus\nunderrepresent the expected progressions, leading to sub-optimal performance in\nLRRG. To address this, we develop a novel dynamic difference-aware temporal\nresidual network (DDaTR). In DDaTR, we introduce two modules at each stage of\nthe visual encoder to capture multi-level spatial correlations. The Dynamic\nFeature Alignment Module (DFAM) is designed to align prior features across\nmodalities for the integrity of prior clinical information. Prompted by the\nenriched prior features, the dynamic difference-aware module (DDAM) captures\nfavorable difference information by identifying relationships across exams.\nFurthermore, our DDaTR employs the dynamic residual network to unidirectionally\ntransmit longitudinal information, effectively modelling temporal correlations.\nExtensive experiments demonstrated superior performance over existing methods\non three benchmarks, proving its efficacy in both RRG and LRRG tasks.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.03401v1;http://arxiv.org/pdf/2505.03401v1", "pdf_url": "http://arxiv.org/pdf/2505.03401v1"}, {"title": "Radiology report generation based on adaptive enhanced fusion of multi features", "link": "https://www.sciencedirect.com/science/article/pii/S0010482525008455", "details": "Y Cao, H Ding, Y Zhang, Y Hei - Computers in Biology and Medicine, 2025", "abstract": "The radiology report is essential for doctors' diagnosis and treatment. The automatic generation of radiology reports can assist doctors in diagnosis and treatment, thereby reducing their workload. Some existing studies consider the entire \u2026"}, {"title": "IMGEF: integrated multimodal graph-enhanced framework for radiology report generation", "link": "https://link.springer.com/article/10.1007/s00530-025-01858-7", "details": "M Usman, X Hou, Y Guo, Z Liang, Z Yijia - Multimedia Systems, 2025", "abstract": "Automated radiology report generation significantly reduces radiologists' workload while maintaining high accuracy and readability standards. We propose an Integrated Multimodal Graph-Enhanced Framework (IMGEF) for graph-enhanced \u2026"}, {"title": "A Multimodal Multi-Agent Framework for Radiology Report Generation", "link": "https://arxiv.org/pdf/2505.09787", "details": "Z Yi, T Xiao, MV Albert - arXiv preprint arXiv:2505.09787, 2025", "abstract": "Radiology report generation (RRG) aims to automatically produce diagnostic reports from medical images, with the potential to enhance clinical workflows and reduce radiologists' workload. While recent approaches leveraging multimodal large \u2026", "entry_id": "http://arxiv.org/abs/2505.09787v1", "updated": "2025-05-14 20:28:04", "published": "2025-05-14 20:28:04", "authors": "Ziruo Yi;Ting Xiao;Mark V. Albert", "summary": "Radiology report generation (RRG) aims to automatically produce diagnostic\nreports from medical images, with the potential to enhance clinical workflows\nand reduce radiologists' workload. While recent approaches leveraging\nmultimodal large language models (MLLMs) and retrieval-augmented generation\n(RAG) have achieved strong results, they continue to face challenges such as\nfactual inconsistency, hallucination, and cross-modal misalignment. We propose\na multimodal multi-agent framework for RRG that aligns with the stepwise\nclinical reasoning workflow, where task-specific agents handle retrieval, draft\ngeneration, visual analysis, refinement, and synthesis. Experimental results\ndemonstrate that our approach outperforms a strong baseline in both automatic\nmetrics and LLM-based evaluations, producing more accurate, structured, and\ninterpretable reports. This work highlights the potential of clinically aligned\nmulti-agent frameworks to support explainable and trustworthy clinical AI\napplications.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.09787v1;http://arxiv.org/pdf/2505.09787v1", "pdf_url": "http://arxiv.org/pdf/2505.09787v1"}]
