'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Ehragent: Code empowers large language models for few-'
[{"title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models", "link": "https://arxiv.org/pdf/2403.08281", "details": "N Ding, Y Chen, G Cui, X Lv, R Xie, B Zhou, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models", "link": "https://arxiv.org/pdf/2403.02436", "details": "X Lu, Y Zhao, B Qin - arXiv preprint arXiv:2403.02436, 2024", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Advanced Data Processing of Pancreatic Cancer Data Integrating Ontologies and Machine Learning Techniques to Create Holistic Health Records", "link": "https://www.mdpi.com/1424-8220/24/6/1739/pdf", "details": "G Manias, A Azqueta-Alz\u00faaz, A Dalianis, J Griffiths\u2026 - Sensors, 2024", "abstract": "The modern healthcare landscape is overwhelmed by data derived from heterogeneous IoT data sources and Electronic Health Record (EHR) systems. Based on the advancements in data science and Machine Learning (ML), an \u2026"}, {"title": "Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records", "link": "https://arxiv.org/pdf/2403.09226", "details": "A Ziletti, L D'Ambrosi - arXiv preprint arXiv:2403.09226, 2024", "abstract": "Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. Querying these databases to answer epidemiological questions is challenging due to the intricacy of medical \u2026"}, {"title": "An Integrated Data Processing Framework for Pretraining Foundation Models", "link": "https://arxiv.org/pdf/2402.16358", "details": "Y Sun, F Wang, Y Zhu, WX Zhao, J Mao - arXiv preprint arXiv:2402.16358, 2024", "abstract": "The ability of the foundation models heavily relies on large-scale, diverse, and high- quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and \u2026"}, {"title": "Data-Efficient Sleep Staging with Synthetic Time Series Pretraining", "link": "https://arxiv.org/html/2403.08592v1", "details": "N Grieger, S Mehrkanoon, S Bialonski - arXiv preprint arXiv:2403.08592, 2024", "abstract": "Analyzing electroencephalographic (EEG) time series can be challenging, especially with deep neural networks, due to the large variability among human subjects and often small datasets. To address these challenges, various strategies, such as self \u2026"}, {"title": "ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling", "link": "https://arxiv.org/html/2402.13542v1", "details": "L Zhang, Y Yu, K Wang, C Zhang - arXiv preprint arXiv:2402.13542, 2024", "abstract": "Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge \u2026"}, {"title": "Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models", "link": "https://arxiv.org/html/2402.18154v1", "details": "Z Jin, P Cao, H Yuan, Y Chen, J Xu, H Li, X Jiang, K Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, retrieval augmentation and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context. However, internal memory and external \u2026"}, {"title": "MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases", "link": "https://arxiv.org/pdf/2402.14905", "details": "Z Liu, C Zhao, F Iandola, C Lai, Y Tian, I Fedorov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical \u2026"}]
