[{"title": "ReFF: Reinforcing Format Faithfulness in Language Models across Varied Tasks", "link": "https://arxiv.org/pdf/2412.09173%3F", "details": "J Yao, H Huang, Z Liu, H Wen, W Su, B Qian, Y Guo - arXiv preprint arXiv:2412.09173, 2024", "abstract": "Following formatting instructions to generate well-structured content is a fundamental yet often unmet capability for large language models (LLMs). To study this capability, which we refer to as format faithfulness, we present FormatBench, a comprehensive \u2026"}, {"title": "Too Big to Fool: Resisting Deception in Language Models", "link": "https://arxiv.org/pdf/2412.10558", "details": "MR Samsami, ML Richter, J Rodriguez, M Thakkar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models must balance their weight-encoded knowledge with in- context information from prompts to generate accurate responses. This paper investigates this interplay by analyzing how models of varying capacities within the \u2026"}, {"title": "TinyLLM: A Framework for Training and Deploying Language Models at the Edge Computers", "link": "https://arxiv.org/pdf/2412.15304", "details": "SV Kandala, P Medaranga, A Varshney - arXiv preprint arXiv:2412.15304, 2024", "abstract": "Language models have gained significant interest due to their general-purpose capabilities, which appear to emerge as models are scaled to increasingly larger parameter sizes. However, these large models impose stringent requirements on \u2026"}, {"title": "Refining Salience-Aware Sparse Fine-Tuning Strategies for Language Models", "link": "https://arxiv.org/pdf/2412.13488", "details": "X Liu, A Thomas, C Zhang, J Cheng, Y Zhao, X Gao - arXiv preprint arXiv:2412.13488, 2024", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has gained prominence through low-rank adaptation methods like LoRA. In this paper, we focus on sparsity-based PEFT (SPEFT), which introduces trainable sparse adaptations to the weight matrices in the \u2026"}, {"title": "Eliciting Causal Abilities in Large Language Models for Reasoning Tasks", "link": "https://arxiv.org/pdf/2412.15314", "details": "Y Wang, Z Luo, J Wang, Z Zhou, Y Chen, B Han - arXiv preprint arXiv:2412.15314, 2024", "abstract": "Prompt optimization automatically refines prompting expressions, unlocking the full potential of LLMs in downstream tasks. However, current prompt optimization methods are costly to train and lack sufficient interpretability. This paper proposes \u2026"}, {"title": "A MapReduce Approach to Effectively Utilize Long Context Information in Retrieval Augmented Language Models", "link": "https://arxiv.org/pdf/2412.15271", "details": "G Zhang, Z Xu, Q Jin, F Chen, Y Fang, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While holding great promise for improving and facilitating healthcare, large language models (LLMs) struggle to produce up-to-date responses on evolving topics due to outdated knowledge or hallucination. Retrieval-augmented generation (RAG) is a \u2026"}, {"title": "PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.09613%3F", "details": "C Yang, X Dong, X Zhu, W Su, J Wang, H Tian, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (VLMs) have been extended to understand both images and videos. Visual token compression is leveraged to reduce the considerable token length of visual inputs. To meet the needs of different tasks \u2026"}, {"title": "V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding", "link": "https://arxiv.org/pdf/2412.09616%3F", "details": "J Ge, Z Chen, J Lin, J Zhu, X Liu, J Dai, X Zhu - arXiv preprint arXiv:2412.09616, 2024", "abstract": "Vision-Language Models (VLMs) have shown promising capabilities in handling various multimodal tasks, yet they struggle in long-context scenarios, particularly in tasks involving videos, high-resolution images, or lengthy image-text documents. In \u2026"}, {"title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding", "link": "https://arxiv.org/pdf/2412.16158", "details": "C Tao, S Su, X Zhu, C Zhang, Z Chen, J Liu, W Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advance of Large Language Models (LLMs) has catalyzed the development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid modality-specific encoders, offer a promising alternative to the compositional ones \u2026"}]
