'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Fast Long Sequence Time-Series Forecasting for Edge Service '
[{"title": "Fine-Tuning Language Models with Reward Learning on Policy", "link": "https://arxiv.org/pdf/2403.19279", "details": "H Lang, F Huang, Y Li - arXiv preprint arXiv:2403.19279, 2024", "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences. RLHF contains three steps, ie, human preference collecting, reward learning, and policy \u2026"}, {"title": "Learn\" No\" to Say\" Yes\" Better: Improving Vision-Language Models via Negations", "link": "https://arxiv.org/pdf/2403.20312", "details": "J Singh, I Shrivastava, M Vatsa, R Singh, A Bharati - arXiv preprint arXiv:2403.20312, 2024", "abstract": "Existing vision-language models (VLMs) treat text descriptions as a unit, confusing individual concepts in a prompt and impairing visual semantic matching and reasoning. An important aspect of reasoning in logic and language is negations. This \u2026"}, {"title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models", "link": "https://arxiv.org/pdf/2403.20262", "details": "T Thonet, J Rozen, L Besacier - arXiv preprint arXiv:2403.20262, 2024", "abstract": "Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities \u2026"}, {"title": "Emergent Abilities in Reduced-Scale Generative Language Models", "link": "https://arxiv.org/pdf/2404.02204", "details": "S Muckatira, V Deshpande, V Lialin, A Rumshisky - arXiv preprint arXiv:2404.02204, 2024", "abstract": "Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study \u2026"}, {"title": "Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models", "link": "https://arxiv.org/pdf/2403.16999", "details": "H Shao, S Qian, H Xiao, G Song, Z Zong, L Wang, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper presents Visual CoT, a novel pipeline that leverages the reasoning capabilities of multi-modal large language models (MLLMs) by incorporating visual Chain-of-Thought (CoT) reasoning. While MLLMs have shown promise in various \u2026"}, {"title": "Generative Language Models for Personalized Information Understanding", "link": "https://scholarworks.umass.edu/cgi/viewcontent.cgi%3Farticle%3D4123%26context%3Ddissertations_2", "details": "P Cai - 2024", "abstract": "A major challenge in information understanding stems from the diverse nature of the audience, where individuals possess varying preferences, experiences, educational and cultural backgrounds. Consequently, adopting a one-size-fits-all approach to \u2026"}, {"title": "Tree-of-Reasoning Question Decomposition for Complex Question Answering with Large Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29928/31621", "details": "K Zhang, J Zeng, F Meng, Y Wang, S Sun, L Bai\u2026 - Proceedings of the AAAI \u2026, 2024", "abstract": "Large language models (LLMs) have recently demonstrated remarkable performance across various natual language processing tasks. In the field of multi- hop reasoning, the Chain-of-thought (CoT) prompt method has emerged as a \u2026"}, {"title": "Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning", "link": "https://arxiv.org/pdf/2404.00909", "details": "R Li, Y Wu, X He - arXiv preprint arXiv:2404.00909, 2024", "abstract": "Generative vision-language models (VLMs) have shown impressive performance in zero-shot vision-language tasks like image captioning and visual question answering. However, improving their zero-shot reasoning typically requires second \u2026"}, {"title": "Monotonic Paraphrasing Improves Generalization of Language Model Prompting", "link": "https://arxiv.org/pdf/2403.16038", "details": "Q Liu, F Wang, N Xu, T Yan, T Meng, M Chen - arXiv preprint arXiv:2403.16038, 2024", "abstract": "Performance of large language models (LLMs) may vary with different prompts or instructions of even the same task. One commonly recognized factor for this phenomenon is the model's familiarity with the given prompt or instruction, which is \u2026"}]
