[{"title": "Relation-Preserving Masked Modeling for Semi-Supervised Time-Series Classification", "link": "https://www.sciencedirect.com/science/article/pii/S0020025524011277", "details": "S Lee, C Choi, Y Son - Information Sciences, 2024", "abstract": "In this study, we address the challenge of label sparsity in time-series classification using semi-supervised learning that effectively leverages numerous unlabeled instances. Our approach introduces a pioneering framework for semi-supervised \u2026"}, {"title": "Robustness-Privacy Trade-Off In Bayesian Neural Networks", "link": "https://www.imperial.ac.uk/media/imperial-college/faculty-of-engineering/computing/public/2324-ug-projects/Ghitu-Mihnea---Final-Project-Report.pdf", "details": "M Ghitu, M Wicker, W Knottenbelt - 2024", "abstract": "Substantial developments have recently been made to devise provable methods that ensure the trustworthiness of deep neural networks. Most of these pieces of work study properties that constitute the trustworthy aspect individually, often isolating \u2026"}, {"title": "Revisiting Attention for Multivariate Time Series Forecasting", "link": "https://arxiv.org/pdf/2407.13806", "details": "H Wu - arXiv preprint arXiv:2407.13806, 2024", "abstract": "Current Transformer methods for Multivariate Time-Series Forecasting (MTSF) are all based on the conventional attention mechanism. They involve sequence embedding and performing a linear projection of Q, K, and V, and then computing attention within \u2026"}]
