[{"title": "Boosting Explainability through Selective Rationalization in Pre-trained Language Models", "link": "https://arxiv.org/pdf/2501.03182%3F", "details": "L Yuan, S Hu, K Yu, L Wu - arXiv preprint arXiv:2501.03182, 2025", "abstract": "The widespread application of pre-trained language models (PLMs) in natural language processing (NLP) has led to increasing concerns about their explainability. Selective rationalization is a self-explanatory framework that selects human \u2026"}, {"title": "Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation on Language Models", "link": "https://arxiv.org/pdf/2412.14528", "details": "X Cui, M Zhu, Y Qin, L Xie, W Zhou, H Li - arXiv preprint arXiv:2412.14528, 2024", "abstract": "Knowledge distillation (KD) has become a prevalent technique for compressing large language models (LLMs). Existing KD methods are constrained by the need for identical tokenizers (ie, vocabularies) between teacher and student models, limiting \u2026"}, {"title": "LLM360 K2: Scaling Up 360-Open-Source Large Language Models", "link": "https://arxiv.org/pdf/2501.07124", "details": "Z Liu, B Tan, H Wang, W Neiswanger, T Tao, H Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree OPEN SOURCE approach to the largest and most powerful models under project LLM360. While open-source LLMs continue to advance, the answer to\" How are the \u2026"}, {"title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models", "link": "https://arxiv.org/pdf/2501.04945", "details": "Q Ren, J Zeng, Q He, J Liang, Y Xiao, W Zhou, Z Sun\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "It is crucial for large language models (LLMs) to follow instructions that involve multiple constraints. However, soft constraints are semantically related and difficult to verify through automated methods. These constraints remain a significant challenge \u2026"}, {"title": "Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage Policy Optimization", "link": "https://arxiv.org/pdf/2412.18279%3F", "details": "J Liu, C Wang, CY Liu, L Zeng, R Yan, Y Sun, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The role of reinforcement learning (RL) in enhancing the reasoning of large language models (LLMs) is becoming increasingly significant. Despite the success of RL in many scenarios, there are still many challenges in improving the reasoning of \u2026"}, {"title": "Disentangling Exploration of Large Language Models by Optimal Exploitation", "link": "https://arxiv.org/pdf/2501.08925", "details": "T Grams, P Betz, C Bartelt - arXiv preprint arXiv:2501.08925, 2025", "abstract": "Exploration is a crucial skill for self-improvement and open-ended problem-solving. However, it remains uncertain whether large language models can effectively explore the state-space. Existing evaluations predominantly focus on the trade-off \u2026"}, {"title": "Dynamic Skill Adaptation for Large Language Models", "link": "https://arxiv.org/pdf/2412.19361%3F", "details": "J Chen, D Yang - arXiv preprint arXiv:2412.19361, 2024", "abstract": "We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework to adapt novel and complex skills to Large Language Models (LLMs). Compared with previous work which learns from human-curated and static data in random orders \u2026"}, {"title": "Incorporating Molecular Knowledge in Large Language Models via Multimodal Modeling", "link": "https://ieeexplore.ieee.org/abstract/document/10838383/", "details": "Z Yang, K Lv, J Shu, Z Li, P Xiao - IEEE Transactions on Computational Social \u2026, 2025", "abstract": "In recent years, large language models (LLMs) represented by GPT-4 have achieved tremendous success in natural language-centered tasks. Nevertheless, LLMs face inherent challenges in tasks involving both natural language and molecular \u2026"}, {"title": "Fine-tuning Large Language Models for Improving Factuality in Legal Question Answering", "link": "https://arxiv.org/pdf/2501.06521", "details": "Y Hu, L Gan, W Xiao, K Kuang, F Wu - arXiv preprint arXiv:2501.06521, 2025", "abstract": "Hallucination, or the generation of incorrect or fabricated information, remains a critical challenge in large language models (LLMs), particularly in high-stake domains such as legal question answering (QA). In order to mitigate the hallucination \u2026"}]
