[{"title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2406.13233", "details": "Z Zeng, Y Miao, H Gao, H Zhang, Z Deng - arXiv preprint arXiv:2406.13233, 2024", "abstract": "Mixture of experts (MoE) has become the standard for constructing production-level large language models (LLMs) due to its promise to boost model capacity without causing significant overheads. Nevertheless, existing MoE methods usually enforce \u2026"}, {"title": "Timo: Towards Better Temporal Reasoning for Language Models", "link": "https://arxiv.org/pdf/2406.14192", "details": "Z Su, J Zhang, T Zhu, X Qu, J Li, M Zhang, Y Cheng - arXiv preprint arXiv:2406.14192, 2024", "abstract": "Reasoning about time is essential for Large Language Models (LLMs) to understand the world. Previous works focus on solving specific tasks, primarily on time-sensitive question answering. While these methods have proven effective, they cannot \u2026"}, {"title": "Mitigating Social Biases in Language Models through Unlearning", "link": "https://arxiv.org/pdf/2406.13551", "details": "O Dige, D Singh, TF Yau, Q Zhang, B Bolandraftar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mitigating bias in language models (LMs) has become a critical problem due to the widespread deployment of LMs. Numerous approaches revolve around data pre- processing and fine-tuning of language models, tasks that can be both time \u2026"}, {"title": "Abstraction-of-Thought Makes Language Models Better Reasoners", "link": "https://arxiv.org/pdf/2406.12442", "details": "R Hong, H Zhang, X Pan, D Yu, C Zhang - arXiv preprint arXiv:2406.12442, 2024", "abstract": "Abstract reasoning, the ability to reason from the abstract essence of a problem, serves as a key to generalization in human reasoning. However, eliciting language models to perform reasoning with abstraction remains unexplored. This paper seeks \u2026"}, {"title": "Word Embeddings Are Steers for Language Models", "link": "https://blender.cs.illinois.edu/paper/lmsteer2024.pdf", "details": "C Han, J Xu, M Li, Y Fung, C Sun, N Jiang\u2026", "abstract": "Abstract Language models (LMs) automatically learn word embeddings during pre- training on language corpora. Although word embeddings are usually interpreted as feature vectors for individual words, their roles in language model generation remain \u2026"}, {"title": "DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph", "link": "https://arxiv.org/pdf/2406.17271", "details": "Z Zhang, J Chen, D Yang - arXiv preprint arXiv:2406.17271, 2024", "abstract": "The current paradigm of evaluating Large Language Models (LLMs) through static benchmarks comes with significant limitations, such as vulnerability to data contamination and a lack of adaptability to the evolving capabilities of LLMs \u2026"}, {"title": "BeHonest: Benchmarking Honesty of Large Language Models", "link": "https://arxiv.org/pdf/2406.13261", "details": "S Chern, Z Hu, Y Yang, E Chern, Y Guo, J Jin, B Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Previous works on Large Language Models (LLMs) have mainly focused on evaluating their helpfulness or harmlessness. However, honesty, another crucial alignment criterion, has received relatively less attention. Dishonest behaviors in \u2026"}, {"title": "Entropy-Based Decoding for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2406.17519", "details": "Z Qiu, Z Ou, B Wu, J Li, A Liu, I King - arXiv preprint arXiv:2406.17519, 2024", "abstract": "Augmenting Large Language Models (LLMs) with retrieved external knowledge has proven effective for improving the factual accuracy of generated responses. Despite their success, retrieval-augmented LLMs still face the distractibility issue, where the \u2026"}, {"title": "Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts", "link": "https://arxiv.org/pdf/2406.12034", "details": "J Kang, L Karlinsky, H Luo, Z Wang, J Hansen, J Glass\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Self-MoE, an approach that transforms a monolithic LLM into a compositional, modular system of self-specialized experts, named MiXSE (MiXture of Self-specialized Experts). Our approach leverages self-specialization, which \u2026"}]
