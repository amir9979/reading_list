[{"title": "ELAINE-medLLM: Lightweight English Japanese Chinese Trilingual Large Language Model for Bio-medical Domain", "link": "https://aclanthology.org/2025.coling-main.313.pdf", "details": "K Yano, Z Luo, J Huang, Q Xie, M Asada, C Yuan\u2026 - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract We propose ELAINE (EngLish-jApanese-chINesE)-medLLM, a trilingual (English, Japanese, Chinese) large language model adapted for the bio-medical domain based on Llama-3-8B. The training dataset was carefully curated in terms of \u2026"}, {"title": "Using deep feature distances for evaluating the perceptual quality of MR image reconstructions", "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.30437", "details": "PM Adamson, AD Desai, J Dominic, M Varma\u2026 - Magnetic Resonance in \u2026, 2025", "abstract": "Purpose Commonly used MR image quality (IQ) metrics have poor concordance with radiologist\u2010perceived diagnostic IQ. Here, we develop and explore deep feature distances (DFDs)\u2014distances computed in a lower\u2010dimensional feature space \u2026"}, {"title": "Ocean-OCR: Towards General OCR Application via a Vision-Language Model", "link": "https://arxiv.org/pdf/2501.15558", "details": "S Chen, X Guo, Y Li, T Zhang, M Lin, D Kuang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal large language models (MLLMs) have shown impressive capabilities across various domains, excelling in processing and understanding information from multiple modalities. Despite the rapid progress made previously, insufficient OCR \u2026"}, {"title": "DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control", "link": "https://arxiv.org/pdf/2502.05855", "details": "J Wen, Y Zhu, J Li, Z Tang, C Shen, F Feng - arXiv preprint arXiv:2502.05855, 2025", "abstract": "Enabling robots to perform diverse tasks across varied environments is a central challenge in robot learning. While vision-language-action (VLA) models have shown promise for generalizable robot skills, realizing their full potential requires \u2026"}, {"title": "Leveraging large language models for abstractive summarization of Italian legal news", "link": "https://link.springer.com/article/10.1007/s10506-025-09431-3", "details": "I Benedetto, L Cagliero, M Ferro, F Tarasconi, C Bernini\u2026 - Artificial Intelligence and \u2026, 2025", "abstract": "Condensing the key message conveyed by a long document into an informative summary is particularly helpful to lawyers and legal experts. State-of-the-art approaches to legal document summarization rely on Language Models (LMs) and \u2026"}, {"title": "Foundation Model of Electronic Medical Records for Adaptive Risk Estimation", "link": "https://arxiv.org/pdf/2502.06124", "details": "P Renc, MK Grzeszczyk, N Oufattole, D Goode, Y Jia\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We developed the Enhanced Transformer for Health Outcome Simulation (ETHOS), an AI model that tokenizes patient health timelines (PHTs) from EHRs. ETHOS predicts future PHTs using transformer-based architectures. The Adaptive Risk \u2026"}, {"title": "Efficient Vocabulary Reduction for Small Language Models", "link": "https://aclanthology.org/2025.coling-industry.64.pdf", "details": "Y Nozaki, D Nakashima, R Sato, N Asaba - \u2026 of the 31st International Conference on \u2026, 2025", "abstract": "The increasing size of large language models (LLMs) poses significant challenges due to their high computational costs and energy consumption, making their deployment in industrial settings difficult. Small language models (SLMs) have been \u2026"}, {"title": "ConceptCLIP: Towards Trustworthy Medical AI via Concept-Enhanced Contrastive Langauge-Image Pre-training", "link": "https://arxiv.org/pdf/2501.15579", "details": "Y Nie, S He, Y Bie, Y Wang, Z Chen, S Yang, H Chen - arXiv preprint arXiv \u2026, 2025", "abstract": "Trustworthiness is essential for the precise and interpretable application of artificial intelligence (AI) in medical imaging. Traditionally, precision and interpretability have been addressed as separate tasks, namely medical image analysis and explainable \u2026"}, {"title": "HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding", "link": "https://arxiv.org/pdf/2501.15111", "details": "J Zhao, Q Yang, Y Peng, D Bai, S Yao, B Sun, X Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In human-centric scenes, the ability to simultaneously understand visual and auditory information is crucial. While recent omni models can process multiple modalities, they generally lack effectiveness in human-centric scenes due to the absence of \u2026"}]
