[{"title": "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "link": "https://arxiv.org/pdf/2409.17455", "details": "Y Zhou, R Tang, Z Yao, Z Zhu - arXiv preprint arXiv:2409.17455, 2024", "abstract": "Language models (LMs), despite their advances, often depend on spurious correlations, undermining their accuracy and generalizability. This study addresses the overlooked impact of subtler, more complex shortcuts that compromise model \u2026"}, {"title": "Fine-tuning large language models to improve accuracy and comprehensibility of automated code review", "link": "https://dl.acm.org/doi/pdf/10.1145/3695993", "details": "Y Yu, G Rong, H Shen, H Zhang, D Shao, M Wang\u2026 - ACM Transactions on \u2026, 2024", "abstract": "As code review is a tedious and costly software quality practice, researchers have proposed several machine learning-based methods to automate the process. The primary focus has been on accuracy, that is, how accurately the algorithms are able \u2026"}, {"title": "Probabilistic Active Few-Shot Learning in Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3DsSX9wLMSJT", "details": "A Baumann, M Klasson, R Li, A Solin, M Trapp - NeurIPS 2024 Workshop on Bayesian \u2026", "abstract": "Pre-trained vision-language models (VLMs) have shown to be an useful model class for zero-and few-shot learning tasks. In this work, we investigate probabilistic active few-shot learning in VLMs by leveraging post-hoc uncertainty estimation and \u2026"}, {"title": "Optimal Transport of Diverse Unsupervised Tasks for Robust Learning from Noisy Few-Shot Data", "link": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05600.pdf", "details": "X Que, Q Yu - European Conference on Computer Vision, 2025", "abstract": "Noisy few-shot learning (NFSL) presents novel challenges primarily due to the interplay between noisy labels and limited training data. While data cleansing offers a viable solution to address noisy labels in the general learning settings, it \u2026"}, {"title": "Fusing graph structural information with pre-trained generative model for knowledge graph-to-text generation", "link": "https://link.springer.com/article/10.1007/s10115-024-02235-y", "details": "X Shi, Z Xia, Y Li, X Wang, Y Niu - Knowledge and Information Systems, 2024", "abstract": "Abstract Knowledge graph-to-text generation (KG-to-Text) is a task that involves generating accurate textual descriptions based on a given knowledge graph. Previous efforts have often enhanced pre-trained generative models by incorporating \u2026"}, {"title": "Generative LLM Powered Conversational AI Application for Personalized Risk Assessment: A Case Study in COVID-19", "link": "https://arxiv.org/pdf/2409.15027", "details": "MA Roshani, X Zhou, Y Qiang, S Suresh, S Hicks\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have shown remarkable capabilities in various natural language tasks and are increasingly being applied in healthcare domains. This work demonstrates a new LLM-powered disease risk assessment approach via \u2026"}, {"title": "The Role of Deductive and Inductive Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2410.02892", "details": "C Cai, X Zhao, H Liu, Z Jiang, T Zhang, Z Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have achieved substantial progress in artificial intelligence, particularly in reasoning tasks. However, their reliance on static prompt structures, coupled with limited dynamic reasoning capabilities, often constrains their \u2026"}, {"title": "TAEGAN: Generating Synthetic Tabular Data For Data Augmentation", "link": "https://arxiv.org/pdf/2410.01933", "details": "J Li, Z Zhao, K Yee, U Javaid, B Sikdar - arXiv preprint arXiv:2410.01933, 2024", "abstract": "Synthetic tabular data generation has gained significant attention for its potential in data augmentation, software testing and privacy-preserving data sharing. However, most research has primarily focused on larger datasets and evaluating their quality in \u2026"}, {"title": "An Inference Method for Professional Texts with Computational Expressions under Few-shot Scenarios", "link": "https://splab.sdu.edu.cn/calc_infer.pdf", "details": "L Yang, W Zheng, F Yuan, Y Sun", "abstract": "We propose an inference method for complex professional texts with computational expressions. We use the expert rules to locate and rewrite the expressions. We adopt the pre-trained language model as the initial model and select the high quality \u2026"}]
