[{"title": "Mapping Bias in Vision Language Models: Signposts, Pitfalls, and the Road Ahead", "link": "https://arxiv.org/pdf/2410.13146", "details": "K Sasse, S Chen, J Pond, D Bitterman, J Osborne - arXiv preprint arXiv:2410.13146, 2024", "abstract": "As Vision Language Models (VLMs) gain widespread use, their fairness remains under-explored. In this paper, we analyze demographic biases across five models and six datasets. We find that portrait datasets like UTKFace and CelebA are the best \u2026"}, {"title": "Drivedreamer4d: World models are effective data machines for 4d driving scene representation", "link": "https://arxiv.org/pdf/2410.13571", "details": "G Zhao, C Ni, X Wang, Z Zhu, G Huang, X Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Closed-loop simulation is essential for advancing end-to-end autonomous driving systems. Contemporary sensor simulation methods, such as NeRF and 3DGS, rely predominantly on conditions closely aligned with training data distributions, which \u2026"}, {"title": "Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?", "link": "https://arxiv.org/pdf/2410.13523", "details": "C Liu, Z Wan, H Wang, Y Chen, T Qaiser, C Jin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical Vision-Language Pre-training (MedVLP) has made significant progress in enabling zero-shot tasks for medical image understanding. However, training MedVLP models typically requires large-scale datasets with paired, high-quality \u2026"}, {"title": "Improving Multi-modal Large Language Model through Boosting Vision Capabilities", "link": "https://arxiv.org/pdf/2410.13733", "details": "Y Sun, H Zhang, Q Chen, X Zhang, N Sang, G Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We focus on improving the visual understanding capability for boosting the vision- language models. We propose\\textbf {Arcana}, a multiModal language model, which introduces two crucial techniques. First, we present Multimodal LoRA (MM-LoRA), a \u2026"}, {"title": "A Comparative Study of Recent Large Language Models on Generating Hospital Discharge Summaries for Lung Cancer Patients", "link": "https://arxiv.org/pdf/2411.03805", "details": "Y Li, F Li, K Roberts, L Cui, C Tao, H Xu - arXiv preprint arXiv:2411.03805, 2024", "abstract": "Generating discharge summaries is a crucial yet time-consuming task in clinical practice, essential for conveying pertinent patient information and facilitating continuity of care. Recent advancements in large language models (LLMs) have \u2026"}, {"title": "Large Language Models Can Be Contextual Privacy Protection Learners", "link": "https://aclanthology.org/2024.emnlp-main.785.pdf", "details": "Y Xiao, Y Jin, Y Bai, Y Wu, X Yang, X Luo, W Yu\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers", "link": "https://openreview.net/pdf%3Fid%3D67tRrjgzsh", "details": "X Lu, Y Zhao, B Qin, L Huo, Q Yang, D Xu - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Goldfish: Vision-Language Understanding of Arbitrarily Long Videos", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-73397-0_15.pdf", "details": "E Sleiman, M Zhuge, J Ding, D Zhu, J Schmidhuber\u2026", "abstract": "Most current LLM-based models for video understanding can process videos within minutes. However, they struggle with lengthy videos due to challenges such as \u201cnoise and redundancy\u201d, as well as \u201cmemory and computation\u201d constraints. In this \u2026"}, {"title": "Physics-Informed Neural Fields with Neural Implicit Surface for Fluid Reconstruction", "link": "https://diglib.eg.org/server/api/core/bitstreams/67c0619a-3361-4bb2-bf04-776a856f4d9d/content", "details": "Z Duan, Z Ren - 2024", "abstract": "Recovering fluid density and velocity from multi-view RGB videos poses a formidable challenge. Existing solutions typically assume knowledge of obstacles and lighting, or are designed for simple fluid scenes without obstacles or complex lighting \u2026"}]
