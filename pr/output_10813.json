[{"title": "Normality Aggregation and Abnormality Separation Contrastive Learning for Mechanical Anomaly Detection", "link": "https://ieeexplore.ieee.org/abstract/document/10815970/", "details": "C Hu, J Ren, H Xu, J Wu, C Sun, R Yan - IEEE Transactions on Instrumentation and \u2026, 2024", "abstract": "Accurate anomaly detection is essential for the safe operating of high-end equipment. Intelligent detection approaches have the capacity of automatic abnormality discovery from big data. However, uncontrollable unsupervised training \u2026"}, {"title": "B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions", "link": "https://ieeexplore.ieee.org/abstract/document/10816024/", "details": "H Zhang, W Shao, H Liu, Y Ma, P Luo, Y Qiao, N Zheng\u2026 - IEEE Transactions on \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) have shown significant progress in responding well to visual-instructions from users. However, these instructions, encompassing images and text, are susceptible to both intentional and inadvertent \u2026"}, {"title": "Multi-perspective Pseudo-label Generation and Confidence-weighted Training for Semi-supervised Semantic Segmentation", "link": "https://ieeexplore.ieee.org/abstract/document/10814100/", "details": "K Hu, X Chen, Z Chen, Y Zhang, X Gao - IEEE Transactions on Multimedia, 2024", "abstract": "Self-training has been shown to achieve remarkable gains in semi-supervised semantic segmentation by creating pseudo-labels using unlabeled data. This approach, however, suffers from the quality of the generated pseudo-labels, and \u2026"}, {"title": "Enhancing Interpretability Through Loss-Defined Classification Objective in Structured Latent Spaces", "link": "https://arxiv.org/pdf/2412.08515", "details": "D Geissler, B Zhou, M Liu, P Lukowicz - arXiv preprint arXiv:2412.08515, 2024", "abstract": "Supervised machine learning often operates on the data-driven paradigm, wherein internal model parameters are autonomously optimized to converge predicted outputs with the ground truth, devoid of explicitly programming rules or a priori \u2026"}, {"title": "The Birth of Self Supervised Learning: A Supervised Theory", "link": "https://openreview.net/pdf%3Fid%3DNhYAjAAdQT", "details": "R Balestriero, Y LeCun - NeurIPS 2024 Workshop: Self-Supervised Learning \u2026", "abstract": "Self Supervised Learning (SSL) produces versatile representations from unlabeled datasets, while supervised learning produces overly specialized representations from labeled datasets. While this has been {\\em empirically observed} many times, it \u2026"}, {"title": "Enhancing Fine-Tuning Performance of Text-to-Image Diffusion Models for Few-Shot Image Generation Through", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3Dsuw5EQAAQBAJ%26oi%3Dfnd%26pg%3DPA133%26ots%3DNITVP-DdxU%26sig%3D_BXIJp6r4JoPD4mFi3cd3wtEIQQ", "details": "YL Zhu, P Yang - Image and Graphics Technologies and Applications \u2026", "abstract": "Recent significant progress in the field of few-shot image generation has been achieved by fine-tuning pretrained text-to-image mod-els, notably methods such as Dreambooth and Textual Inversion. To enhance the performance of existing \u2026"}]
