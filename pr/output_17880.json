[{"title": "Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond", "link": "https://arxiv.org/pdf/2506.16982", "details": "A Berthon, M van der Schaar - arXiv preprint arXiv:2506.16982, 2025", "abstract": "Accurately assessing student knowledge is critical for effective education, yet traditional Knowledge Tracing (KT) methods rely on opaque latent embeddings, limiting interpretability. Even LLM-based approaches generate direct predictions or \u2026", "entry_id": "http://arxiv.org/abs/2506.16982v1", "updated": "2025-06-20 13:21:14", "published": "2025-06-20 13:21:14", "authors": "Antonin Berthon;Mihaela van der Schaar", "summary": "Accurately assessing student knowledge is critical for effective education,\nyet traditional Knowledge Tracing (KT) methods rely on opaque latent\nembeddings, limiting interpretability. Even LLM-based approaches generate\ndirect predictions or summaries that may hallucinate without any accuracy\nguarantees. We recast KT as an inverse problem: learning the minimum\nnatural-language summary that makes past answers explainable and future answers\npredictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM\nthat writes an interpretable knowledge summary and a frozen decoder LLM that\nmust reconstruct and predict student responses using only that summary text. By\nconstraining all predictive information to pass through a short\nnatural-language bottleneck, LBMs ensure that the summary contains accurate\ninformation while remaining human-interpretable. Experiments on synthetic\narithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the\naccuracy of state-of-the-art KT and direct LLM methods while requiring\norders-of-magnitude fewer student trajectories. We demonstrate that training\nthe encoder with group-relative policy optimization, using downstream decoding\naccuracy as a reward signal, effectively improves summary quality.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2506.16982v1;http://arxiv.org/pdf/2506.16982v1", "pdf_url": "http://arxiv.org/pdf/2506.16982v1"}, {"title": "One Leaf Reveals the Season: Occlusion-Based Contrastive Learning with Semantic-Aware Views for Efficient Visual Representation", "link": "https://openreview.net/pdf%3Fid%3DtoZOqONu9x", "details": "X Yang, L Xu, H Li, S Zhang - Forty-second International Conference on Machine \u2026, 2025", "abstract": "This paper proposes a scalable and straightforward pre-training paradigm for efficient visual conceptual representation called occluded image contrastive learning (OCL). Our OCL approach is simple: we randomly mask patches to generate different \u2026"}, {"title": "Uncertainty-Aware Variational Information Pursuit for Interpretable Medical Image Analysis", "link": "https://arxiv.org/pdf/2506.16742", "details": "M Nahiduzzaman, R Tennakoon, S Korevaar, Z Ge\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In medical imaging, AI decision-support systems must balance accuracy and interpretability to build user trust and support effective clinical decision-making. Recently, Variational Information Pursuit (V-IP) and its variants have emerged as \u2026", "entry_id": "http://arxiv.org/abs/2506.16742v1", "updated": "2025-06-20 04:25:47", "published": "2025-06-20 04:25:47", "authors": "Md Nahiduzzaman;Ruwan Tennakoon;Steven Korevaar;Zongyuan Ge;Alireza Bab-Hadiashar", "summary": "In medical imaging, AI decision-support systems must balance accuracy and\ninterpretability to build user trust and support effective clinical\ndecision-making. Recently, Variational Information Pursuit (V-IP) and its\nvariants have emerged as interpretable-by-design modeling techniques, aiming to\nexplain AI decisions in terms of human-understandable, clinically relevant\nconcepts. However, existing V-IP methods overlook instance-level uncertainties\nin query-answer generation, which can arise from model limitations (epistemic\nuncertainty) or variability in expert responses (aleatoric uncertainty).\n  This paper introduces Uncertainty-Aware V-IP (UAV-IP), a novel framework that\nintegrates uncertainty quantification into the V-IP process. We evaluate UAV-IP\nacross four medical imaging datasets, PH2, Derm7pt, BrEaST, and SkinCon,\ndemonstrating an average AUC improvement of approximately 3.2% while generating\n20% more concise explanations compared to baseline V-IP, without sacrificing\ninformativeness. These findings highlight the importance of uncertainty-aware\nreasoning in interpretable by design models for robust and reliable medical\ndecision-making.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.16742v1;http://arxiv.org/pdf/2506.16742v1", "pdf_url": "http://arxiv.org/pdf/2506.16742v1"}, {"title": "Learning Vision and Language Concepts for Controllable Image Generation", "link": "https://openreview.net/pdf%3Fid%3DhUHRTaTfvZ", "details": "S Xie, L Kong, Y Zheng, Z Tang, EP Xing, G Chen\u2026 - Forty-second International \u2026", "abstract": "Concept learning seeks to extract semantic and interpretable representations of atomic concepts from high-dimensional data such as images and text, which can be instrumental to a variety of downstream tasks (eg, image generation/editing). Despite \u2026"}, {"title": "DIP: Unsupervised Dense In-Context Post-training of Visual Representations", "link": "https://arxiv.org/pdf/2506.18463", "details": "S Sirko-Galouchenko, S Gidaris, A Vobecky, A Bursuc\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce DIP, a novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation \u2026", "entry_id": "http://arxiv.org/abs/2506.18463v1", "updated": "2025-06-23 10:01:14", "published": "2025-06-23 10:01:14", "authors": "Sophia Sirko-Galouchenko;Spyros Gidaris;Antonin Vobecky;Andrei Bursuc;Nicolas Thome", "summary": "We introduce DIP, a novel unsupervised post-training method designed to\nenhance dense image representations in large-scale pretrained vision encoders\nfor in-context scene understanding. Unlike prior approaches that rely on\ncomplex self-distillation architectures, our method trains the vision encoder\nusing pseudo-tasks that explicitly simulate downstream in-context scenarios,\ninspired by meta-learning principles. To enable post-training on unlabeled\ndata, we propose an automatic mechanism for generating in-context tasks that\ncombines a pretrained diffusion model and the vision encoder itself. DIP is\nsimple, unsupervised, and computationally efficient, requiring less than 9\nhours on a single A100 GPU. By learning dense representations through pseudo\nin-context tasks, it achieves strong performance across a wide variety of\ndownstream real-world in-context scene understanding tasks. It outperforms both\nthe initial vision encoder and prior methods, offering a practical and\neffective solution for improving dense representations. Code available here:\nhttps://github.com/sirkosophia/DIP", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.18463v1;http://arxiv.org/pdf/2506.18463v1", "pdf_url": "http://arxiv.org/pdf/2506.18463v1"}, {"title": "NERO: Explainable Out-of-Distribution Detection with Neuron-level Relevance", "link": "https://arxiv.org/pdf/2506.15404", "details": "A Chhetri, J Korhonen, P Gyawali, B Bhattarai - arXiv preprint arXiv:2506.15404, 2025", "abstract": "Ensuring reliability is paramount in deep learning, particularly within the domain of medical imaging, where diagnostic decisions often hinge on model outputs. The capacity to separate out-of-distribution (OOD) samples has proven to be a valuable \u2026", "entry_id": "http://arxiv.org/abs/2506.15404v1", "updated": "2025-06-18 12:22:17", "published": "2025-06-18 12:22:17", "authors": "Anju Chhetri;Jari Korhonen;Prashnna Gyawali;Binod Bhattarai", "summary": "Ensuring reliability is paramount in deep learning, particularly within the\ndomain of medical imaging, where diagnostic decisions often hinge on model\noutputs. The capacity to separate out-of-distribution (OOD) samples has proven\nto be a valuable indicator of a model's reliability in research. In medical\nimaging, this is especially critical, as identifying OOD inputs can help flag\npotential anomalies that might otherwise go undetected. While many OOD\ndetection methods rely on feature or logit space representations, recent works\nsuggest these approaches may not fully capture OOD diversity. To address this,\nwe propose a novel OOD scoring mechanism, called NERO, that leverages\nneuron-level relevance at the feature layer. Specifically, we cluster\nneuron-level relevance for each in-distribution (ID) class to form\nrepresentative centroids and introduce a relevance distance metric to quantify\na new sample's deviation from these centroids, enhancing OOD separability.\nAdditionally, we refine performance by incorporating scaled relevance in the\nbias term and combining feature norms. Our framework also enables explainable\nOOD detection. We validate its effectiveness across multiple deep learning\narchitectures on the gastrointestinal imaging benchmarks Kvasir and\nGastroVision, achieving improvements over state-of-the-art OOD detection\nmethods.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.LG", "links": "http://arxiv.org/abs/2506.15404v1;http://arxiv.org/pdf/2506.15404v1", "pdf_url": "http://arxiv.org/pdf/2506.15404v1"}, {"title": "Variational Learning of Disentangled Representations", "link": "https://arxiv.org/abs/2506.17182", "details": "Y Slavutsky, O Beker, D Blei, B Dumitrascu - arXiv preprint arXiv:2506.17182, 2025", "abstract": "Disentangled representations enable models to separate factors of variation that are shared across experimental conditions from those that are condition-specific. This separation is essential in domains such as biomedical data analysis, where \u2026", "entry_id": "http://arxiv.org/abs/2506.17182v1", "updated": "2025-06-20 17:36:12", "published": "2025-06-20 17:36:12", "authors": "Yuli Slavutsky;Ozgur Beker;David Blei;Bianca Dumitrascu", "summary": "Disentangled representations enable models to separate factors of variation\nthat are shared across experimental conditions from those that are\ncondition-specific. This separation is essential in domains such as biomedical\ndata analysis, where generalization to new treatments, patients, or species\ndepends on isolating stable biological signals from context-dependent effects.\nWhile extensions of the variational autoencoder (VAE) framework have been\nproposed to address this problem, they frequently suffer from leakage between\nlatent representations, limiting their ability to generalize to unseen\nconditions. Here, we introduce DISCoVeR, a new variational framework that\nexplicitly separates condition-invariant and condition-specific factors.\nDISCoVeR integrates three key components: (i) a dual-latent architecture that\nmodels shared and specific factors separately; (ii) two parallel\nreconstructions that ensure both representations remain informative; and (iii)\na novel max-min objective that encourages clean separation without relying on\nhandcrafted priors, while making only minimal assumptions. Theoretically, we\nshow that this objective maximizes data likelihood while promoting\ndisentanglement, and that it admits a unique equilibrium. Empirically, we\ndemonstrate that DISCoVeR achieves improved disentanglement on synthetic\ndatasets, natural images, and single-cell RNA-seq data. Together, these results\nestablish DISCoVeR as a principled approach for learning disentangled\nrepresentations in multi-condition settings.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;stat.ML", "links": "http://arxiv.org/abs/2506.17182v1;http://arxiv.org/pdf/2506.17182v1", "pdf_url": "http://arxiv.org/pdf/2506.17182v1"}, {"title": "Counterfactual Contrastive Learning with Normalizing Flows for Robust Treatment Effect Estimation", "link": "https://openreview.net/pdf%3Fid%3DVmXgkCmLFD", "details": "J Zhang, E Eldele, Y Wang, X Li, J Liang - Forty-second International Conference on \u2026", "abstract": "Estimating Individual Treatment Effects (ITE) from observational data is challenging due to covariate shift and counterfactual absence. While existing methods attempt to balance distributions globally, they often lack fine-grained sample-level alignment \u2026"}, {"title": "Understanding Refusal in Language Models with Sparse Autoencoders", "link": "https://arxiv.org/pdf/2505.23556", "details": "WJ Yeo, N Prakash, C Neo, RKW Lee, E Cambria\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Refusal is a key safety behavior in aligned language models, yet the internal mechanisms driving refusals remain opaque. In this work, we conduct a mechanistic study of refusal in instruction-tuned LLMs using sparse autoencoders to identify \u2026", "entry_id": "http://arxiv.org/abs/2505.23556v1", "updated": "2025-05-29 15:33:39", "published": "2025-05-29 15:33:39", "authors": "Wei Jie Yeo;Nirmalendu Prakash;Clement Neo;Roy Ka-Wei Lee;Erik Cambria;Ranjan Satapathy", "summary": "Refusal is a key safety behavior in aligned language models, yet the internal\nmechanisms driving refusals remain opaque. In this work, we conduct a\nmechanistic study of refusal in instruction-tuned LLMs using sparse\nautoencoders to identify latent features that causally mediate refusal\nbehaviors. We apply our method to two open-source chat models and intervene on\nrefusal-related features to assess their influence on generation, validating\ntheir behavioral impact across multiple harmful datasets. This enables a\nfine-grained inspection of how refusal manifests at the activation level and\naddresses key research questions such as investigating upstream-downstream\nlatent relationship and understanding the mechanisms of adversarial\njailbreaking techniques. We also establish the usefulness of refusal features\nin enhancing generalization for linear probes to out-of-distribution\nadversarial samples in classification tasks. We open source our code in\nhttps://github.com/wj210/refusal_sae.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.23556v1;http://arxiv.org/pdf/2505.23556v1", "pdf_url": "http://arxiv.org/pdf/2505.23556v1"}]
