[{"title": "Alphaedit: Null-space constrained knowledge editing for language models", "link": "https://arxiv.org/pdf/2410.02355%3F", "details": "J Fang, H Jiang, K Wang, Y Ma, X Wang, X He, T Chua - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) often exhibit hallucinations due to incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating \u2026"}, {"title": "Unraveling cross-modality knowledge conflict in large vision-language models", "link": "https://arxiv.org/pdf/2410.03659", "details": "T Zhu, Q Liu, F Wang, Z Tu, M Chen - arXiv preprint arXiv:2410.03659, 2024", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities for capturing and reasoning over multimodal inputs. However, these models are prone to parametric knowledge conflicts, which arise from inconsistencies of \u2026"}, {"title": "DEPT: Decoupled Embeddings for Pre-training Language Models", "link": "https://arxiv.org/pdf/2410.05021", "details": "A Iacob, L Sani, M Kurmanji, WF Shen, X Qiu, D Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language Model pre-training benefits from a broader data mixture to enhance performance across domains and languages. However, training on such heterogeneous text corpora is complex, requiring extensive and cost-intensive \u2026"}, {"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "link": "https://arxiv.org/pdf/2410.05639", "details": "R Zhao, ZL Thai, Y Zhang, S Hu, Y Ba, J Zhou, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the \u2026"}, {"title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks", "link": "https://arxiv.org/pdf/2410.05160%3F", "details": "Z Jiang, R Meng, X Yang, S Yavuz, Y Zhou, W Chen - arXiv preprint arXiv:2410.05160, 2024", "abstract": "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize \u2026"}, {"title": "Language Models for Fall Risk Assessment in Children with Cerebral Palsy using Electronic Medical Records", "link": "https://openreview.net/pdf%3Fid%3DPtqLTSZyKa", "details": "T Tabashum, SJ Wang, JJ Krzak, KM Kruger, A Graf\u2026 - IEEE-EMBS International \u2026", "abstract": "Children with Cerebral Palsy (CP) face a heightened risk of falls, complicating treatment outcomes. Traditional manual scoring methods like the Cummings Fall Assessment Score are subjective and labor-intensive due to the diverse \u2026"}, {"title": "EMERGE: Enhancing Multimodal Electronic Health Records Predictive Modeling with Retrieval-Augmented Generation", "link": "https://dl.acm.org/doi/abs/10.1145/3627673.3679582", "details": "Y Zhu, C Ren, Z Wang, X Zheng, S Xie, J Feng, X Zhu\u2026 - Proceedings of the 33rd \u2026, 2024", "abstract": "The integration of multimodal Electronic Health Records (EHR) data has significantly advanced clinical predictive capabilities. Existing models, which utilize clinical notes and multivariate time-series EHR data, often fall short of incorporating the necessary \u2026"}, {"title": "Efficient Transfer Learning with Sequential and Multi-Modal Approaches for Electronic Health Records", "link": "https://aaltodoc.aalto.fi/bitstreams/a482ee97-2ad4-444a-895a-e43c89af1fc4/download", "details": "Y Kumar - 2024", "abstract": "The digital transformation in healthcare has dramatically increased data availability, yet the potential for data-driven insights is frequently constrained by the quality of data. Securing high-quality data is particularly challenging in fields like healthcare \u2026"}, {"title": "Mutual Prompt Leaning for Vision Language Models", "link": "https://link.springer.com/article/10.1007/s11263-024-02243-z", "details": "S Long, Z Zhao, J Yuan, Z Tan, J Liu, J Feng, S Wang\u2026 - International Journal of \u2026, 2024", "abstract": "Large pre-trained vision language models (VLMs) have demonstrated impressive representation learning capabilities, but their transferability across various downstream tasks heavily relies on prompt learning. Since VLMs consist of text and \u2026"}]
