[{"title": "Efficient Transfer Learning with Sequential and Multi-Modal Approaches for Electronic Health Records", "link": "https://aaltodoc.aalto.fi/bitstreams/a482ee97-2ad4-444a-895a-e43c89af1fc4/download", "details": "Y Kumar - 2024", "abstract": "The digital transformation in healthcare has dramatically increased data availability, yet the potential for data-driven insights is frequently constrained by the quality of data. Securing high-quality data is particularly challenging in fields like healthcare \u2026"}, {"title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks", "link": "https://arxiv.org/pdf/2410.05160%3F", "details": "Z Jiang, R Meng, X Yang, S Yavuz, Y Zhou, W Chen - arXiv preprint arXiv:2410.05160, 2024", "abstract": "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize \u2026"}, {"title": "Tree of Attributes Prompt Learning for Vision-Language Models", "link": "https://arxiv.org/pdf/2410.11201", "details": "T Ding, W Li, Z Miao, H Pfister - arXiv preprint arXiv:2410.11201, 2024", "abstract": "Prompt learning has proven effective in adapting vision language models for downstream tasks. However, existing methods usually append learnable prompt tokens solely with the category names to obtain textual features, which fails to fully \u2026"}, {"title": "SplitSEE: A Splittable Self-supervised Framework for Single-Channel EEG Representation Learning", "link": "https://arxiv.org/pdf/2410.11200", "details": "R Kotoge, Z Chen, T Kimura, Y Matsubara\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While end-to-end multi-channel electroencephalography (EEG) learning approaches have shown significant promise, their applicability is often constrained in neurological diagnostics, such as intracranial EEG resources. When provided with a \u2026"}, {"title": "Debiasing Large Vision-Language Models by Ablating Protected Attribute Representations", "link": "https://openreview.net/pdf%3Fid%3DpRgFPLFXUz", "details": "N Ratzlaff, ML Olson, M Hinck, SY Tseng, V Lal\u2026 - Neurips Safe Generative AI \u2026", "abstract": "Large Vision Language Models (LVLMs) such as LLaVA have demonstrated impressive capabilities as general-purpose chatbots that can engage in conversations about a provided input image. However, their responses are \u2026"}, {"title": "MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models", "link": "https://openreview.net/forum%3Fid%3DOjUumZhV3s", "details": "P Xia, K Zhu, H Li, T Wang, W Shi, S Wang, L Zhang\u2026 - Neurips Safe Generative AI \u2026", "abstract": "Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for \u2026"}, {"title": "Enhancing Large Language Models with Domain-specific Retrieval Augment Generation: A Case Study on Long-form Consumer Health Question Answering in \u2026", "link": "https://arxiv.org/pdf/2409.13902", "details": "A Gilson, X Ai, T Arunachalam, Z Chen, KX Cheong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the potential of Large Language Models (LLMs) in medicine, they may generate responses lacking supporting evidence or based on hallucinated evidence. While Retrieval Augment Generation (RAG) is popular to address this issue, few \u2026"}, {"title": "Leveraging Coarse-to-Fine Grained Representations in Contrastive Learning for Differential Medical Visual Question Answering", "link": "https://papers.miccai.org/miccai-2024/paper/1957_paper.pdf", "details": "X Liang, Y Wang, D Wang, Z Jiao, H Zhong, M Yang\u2026 - International Conference on \u2026, 2024", "abstract": "Abstract Chest X-ray Differential Medical Visual Question Answering (Diff-MedVQA) is a novel multi-modal task designed to answer questions about diseases, especially their differences, based on a main image and a reference image. Compared to the \u2026"}]
