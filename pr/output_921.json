'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [MiniCPM: Unveiling the Potential of Small Language Mod'
[{"title": "Harnessing the Power of Large Vision Language Models for Synthetic Image Detection", "link": "https://arxiv.org/pdf/2404.02726", "details": "M Keita, W Hamidouche, H Bougueffa, A Hadid\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, the emergence of models capable of generating images from text has attracted considerable interest, offering the possibility of creating realistic images from text descriptions. Yet these advances have also raised concerns about the \u2026"}, {"title": "ATG: Benchmarking Automated Theorem Generation for Generative Language Models", "link": "https://eleanor-h.github.io/publication/confnaacl-2024-atg/confnaacl-2024-atg.pdf", "details": "X Lin, Q Cao, Y Huang, Z Yang, Z Liu, Z Li, X Liang15", "abstract": "Humans can develop new theorems to explore broader and more complex mathematical results. While current generative language models (LMs) have achieved significant improvement in automatically proving theorems, their ability to \u2026"}, {"title": "Improving the spatial resolution of solar images using super-resolution diffusion generative adversarial networks", "link": "https://www.aanda.org/articles/aa/pdf/forth/aa49100-23.pdf", "details": "W Song, Y Ma, H Sun, X Zhao, G Lin - 2024", "abstract": "Context. High-spatial-resolution solar images contribute to the study of small-scale structures on the Sun. The Helioseismic and Magnetic Imager (HMI) conducts continuous full-disk observations of the Sun at a fixed cadence, accumulating a \u2026"}, {"title": "Heron-bench: A benchmark for evaluating vision language models in japanese", "link": "https://arxiv.org/pdf/2404.07824", "details": "Y Inoue, K Sasaki, Y Ochi, K Fujii, K Tanahashi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision Language Models (VLMs) have undergone a rapid evolution, giving rise to significant advancements in the realm of multimodal understanding tasks. However, the majority of these models are trained and evaluated on English-centric datasets \u2026"}, {"title": "FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations", "link": "https://arxiv.org/pdf/2404.06619", "details": "J Dwivedi-Yu, R Dwivedi, T Schick - arXiv preprint arXiv:2404.06619, 2024", "abstract": "The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes \u2026"}, {"title": "Refining Pre-trained Language Models for Domain Adaptation with Entity-Aware Discriminative and Contrastive Learning", "link": "https://epubs.siam.org/doi/pdf/10.1137/1.9781611978032.48", "details": "J Yang, X Hu, Y Shen, G xiao - Proceedings of the 2024 SIAM International \u2026, 2024", "abstract": "With the rapid advancement of pre-trained language models (PLMs), the adaptation of these models to specialized domains has emerged as an essential area of research. However, PLMs encounter substantial challenges when deployed in highly \u2026"}, {"title": "Intrinsic LoRA: A Generalist Approach for Discovering Knowledge in Generative Models", "link": "https://openreview.net/pdf%3Fid%3DxHKWN3Yi6U", "details": "X Du, N Kolkin, G Shakhnarovich, A Bhattad - Synthetic Data for Computer Vision Workshop \u2026", "abstract": "Generative models have been shown to be capable of creating images that closely mimic real scenes, suggesting they inherently encode scene representations. We introduce Intrinsic LoRA (I-LoRA), a general approach that uses Low-Rank \u2026"}, {"title": "HyFit: Hybrid Fine-Tuning With Diverse Sampling for Abstractive Summarization", "link": "https://ieeexplore.ieee.org/abstract/document/10496256/", "details": "S Zhao, Y Cheng, Y Zhang, J Chen, Z Duan, Y Sun\u2026 - IEEE Transactions on Big \u2026, 2024", "abstract": "Abstractive summarization has made significant progress in recent years, which aims to generate a concise and coherent summary that contains the most important facts from the source document. Current fine-tuning approaches based on pre-training \u2026"}, {"title": "Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4", "link": "https://arxiv.org/pdf/2404.00484", "details": "AP Gema, G Hong, P Minervini, L Daines, B Alex - arXiv preprint arXiv:2404.00484, 2024", "abstract": "The NLI4CT task assesses Natural Language Inference systems in predicting whether hypotheses entail or contradict evidence from Clinical Trial Reports. In this study, we evaluate various Large Language Models (LLMs) with multiple strategies \u2026"}]
