[{"title": "Explainable **medical** visual **question answering** via chain of evidence", "link": "https://www.sciencedirect.com/science/article/pii/S095070512500718X", "details": "C Qiu, K Huang, Z Xie, M Liu, J Gu, X Zong - Knowledge-Based Systems, 2025", "abstract": "\u2026 , aiming to **answer** **questions** about **clinical** findings from **medical** images. However\u2026 **medical** knowledge remains a crucial objective. To address these challenges, we reframe the MedVQA **problem** as a task of generating evidence \u2026"}, {"title": "UTSA-NLP at ArchEHR-QA 2025: Improving EHR Question Answering via Self-Consistency Prompting", "link": "https://arxiv.org/pdf/2506.05589", "details": "S Shields-Menard, Z Reimers, J Gardner, D Perry\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 We describe our system for the ArchEHR-QA Shared Task on **answering** **clinical** **questions** using electronic health records (EHRs). Our approach uses **large** **language** **models** in two steps: first, to find sentences in the EHR relevant to a \u2026", "entry_id": "http://arxiv.org/abs/2506.05589v1", "updated": "2025-06-05 21:07:55", "published": "2025-06-05 21:07:55", "authors": "Sara Shields-Menard;Zach Reimers;Joshua Gardner;David Perry;Anthony Rios", "summary": "We describe our system for the ArchEHR-QA Shared Task on answering clinical\nquestions using electronic health records (EHRs). Our approach uses large\nlanguage models in two steps: first, to find sentences in the EHR relevant to a\nclinician's question, and second, to generate a short, citation-supported\nresponse based on those sentences. We use few-shot prompting, self-consistency,\nand thresholding to improve the sentence classification step to decide which\nsentences are essential. We compare several models and find that a smaller 8B\nmodel performs better than a larger 70B model for identifying relevant\ninformation. Our results show that accurate sentence selection is critical for\ngenerating high-quality responses and that self-consistency with thresholding\nhelps make these decisions more reliable.", "comment": "Accepted to BioNLP 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.05589v1;http://arxiv.org/pdf/2506.05589v1", "pdf_url": "http://arxiv.org/pdf/2506.05589v1"}, {"title": "Enhancing Patient-Trial Matching With **Large Language Models** : A Scoping Review of Emerging Applications and Approaches", "link": "https://ascopubs.org/doi/pdfdirect/10.1200/CCI-25-00071", "details": "H Chen, X Li, X He, A Chen, J McGill, EC Webber, H Xu\u2026 - JCO **Clinical** Cancer \u2026, 2025", "abstract": "\u2026 Patient recruitment remains a major bottleneck in **clinical** trial execution, with inefficient patient-trial matching often causing delays and failures. Recent advancements in **large** **language** **models** (LLMs) offer a promising avenue for \u2026"}, {"title": "MLLM-CL: Continual Learning for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2506.05453", "details": "H Zhao, F Zhu, R Wang, G Meng, Z Zhang - arXiv preprint arXiv:2506.05453, 2025", "abstract": "\u2026 Continual learning in **large** **language** **models** has recently gained much attention [63\u2026 ) and general abilities (non-IID testing) for continual learning of multimodal **large** **language** **models**. \u2026 the role of image understanding in visual **question** **answering** \u2026", "entry_id": "http://arxiv.org/abs/2506.05453v1", "updated": "2025-06-05 17:58:13", "published": "2025-06-05 17:58:13", "authors": "Hongbo Zhao;Fei Zhu;Rundong Wang;Gaofeng Meng;Zhaoxiang Zhang", "summary": "Recent Multimodal Large Language Models (MLLMs) excel in vision-language\nunderstanding but face challenges in adapting to dynamic real-world scenarios\nthat require continuous integration of new knowledge and skills. While\ncontinual learning (CL) offers a potential solution, existing benchmarks and\nmethods suffer from critical limitations. In this paper, we introduce MLLM-CL,\na novel benchmark encompassing domain and ability continual learning, where the\nformer focuses on independently and identically distributed (IID) evaluation\nacross evolving mainstream domains, whereas the latter evaluates on non-IID\nscenarios with emerging model ability. Methodologically, we propose preventing\ncatastrophic interference through parameter isolation, along with an MLLM-based\nrouting mechanism. Extensive experiments demonstrate that our approach can\nintegrate domain-specific knowledge and functional abilities with minimal\nforgetting, significantly outperforming existing methods.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2506.05453v1;http://arxiv.org/pdf/2506.05453v1", "pdf_url": "http://arxiv.org/pdf/2506.05453v1"}, {"title": "Challenging Vision-Language Models with Surgical Data: A New Dataset and Broad Benchmarking Study", "link": "https://arxiv.org/pdf/2506.06232", "details": "L Mayer, T R\u00e4dsch, D Michael, L Luttner, A Yamlahi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 However, their performance deteriorates significantly when the tasks require **medical** knowledge. Notably, we find that specialized **medical** \u2026 [8] have been proposed, they employ CLIPstyle [9] evaluation methods but lack open-ended visual \u2026", "entry_id": "http://arxiv.org/abs/2506.06232v1", "updated": "2025-06-06 16:53:12", "published": "2025-06-06 16:53:12", "authors": "Leon Mayer;Tim R\u00e4dsch;Dominik Michael;Lucas Luttner;Amine Yamlahi;Evangelia Christodoulou;Patrick Godau;Marcel Knopp;Annika Reinke;Fiona Kolbinger;Lena Maier-Hein", "summary": "While traditional computer vision models have historically struggled to\ngeneralize to endoscopic domains, the emergence of foundation models has shown\npromising cross-domain performance. In this work, we present the first\nlarge-scale study assessing the capabilities of Vision Language Models (VLMs)\nfor endoscopic tasks with a specific focus on laparoscopic surgery. Using a\ndiverse set of state-of-the-art models, multiple surgical datasets, and\nextensive human reference annotations, we address three key research questions:\n(1) Can current VLMs solve basic perception tasks on surgical images? (2) Can\nthey handle advanced frame-based endoscopic scene understanding tasks? and (3)\nHow do specialized medical VLMs compare to generalist models in this context?\nOur results reveal that VLMs can effectively perform basic surgical perception\ntasks, such as object counting and localization, with performance levels\ncomparable to general domain tasks. However, their performance deteriorates\nsignificantly when the tasks require medical knowledge. Notably, we find that\nspecialized medical VLMs currently underperform compared to generalist models\nacross both basic and advanced surgical tasks, suggesting that they are not yet\noptimized for the complexity of surgical environments. These findings highlight\nthe need for further advancements to enable VLMs to handle the unique\nchallenges posed by surgery. Overall, our work provides important insights for\nthe development of next-generation endoscopic AI systems and identifies key\nareas for improvement in medical visual language models.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.06232v1;http://arxiv.org/pdf/2506.06232v1", "pdf_url": "http://arxiv.org/pdf/2506.06232v1"}, {"title": "BioMol-MQA: A Multi-Modal Question Answering Dataset For LLM Reasoning Over Bio-Molecular Interactions", "link": "https://arxiv.org/pdf/2506.05766", "details": "S Sengupta, S Yang, PK Yu, F Wang, S Wang - arXiv preprint arXiv:2506.05766, 2025", "abstract": "\u2026 Retrieval augmented generation (RAG) has shown great power in improving **Large** **Language** **Models** (LLMs). However, most existing RAG-based LLMs are dedicated to retrieving single modality information, mainly text; while for many realworld \u2026", "entry_id": "http://arxiv.org/abs/2506.05766v1", "updated": "2025-06-06 05:48:22", "published": "2025-06-06 05:48:22", "authors": "Saptarshi Sengupta;Shuhua Yang;Paul Kwong Yu;Fali Wang;Suhang Wang", "summary": "Retrieval augmented generation (RAG) has shown great power in improving Large\nLanguage Models (LLMs). However, most existing RAG-based LLMs are dedicated to\nretrieving single modality information, mainly text; while for many real-world\nproblems, such as healthcare, information relevant to queries can manifest in\nvarious modalities such as knowledge graph, text (clinical notes), and complex\nmolecular structure. Thus, being able to retrieve relevant multi-modality\ndomain-specific information, and reason and synthesize diverse knowledge to\ngenerate an accurate response is important. To address the gap, we present\nBioMol-MQA, a new question-answering (QA) dataset on polypharmacy, which is\ncomposed of two parts (i) a multimodal knowledge graph (KG) with text and\nmolecular structure for information retrieval; and (ii) challenging questions\nthat designed to test LLM capabilities in retrieving and reasoning over\nmultimodal KG to answer questions. Our benchmarks indicate that existing LLMs\nstruggle to answer these questions and do well only when given the necessary\nbackground data, signaling the necessity for strong RAG frameworks.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.05766v1;http://arxiv.org/pdf/2506.05766v1", "pdf_url": "http://arxiv.org/pdf/2506.05766v1"}, {"title": "Dual-Priv Pruning : Efficient Differential Private Fine-Tuning in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2506.07077", "details": "Q Wei, J Li, Z You, Y Zhan, K Li, J Wu, XLH Liu, Y Yu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 task-specific datasets, making it a critical tool for **large** **language** **models**. However, its effectiveness in Multimodal Large Language Models \u2026 Additionally, we incorporate two **medical** visual **question** **answering** dataset, PathVQA [13]and VQA-RAD \u2026", "entry_id": "http://arxiv.org/abs/2506.07077v1", "updated": "2025-06-08 10:33:01", "published": "2025-06-08 10:33:01", "authors": "Qianshan Wei;Jiaqi Li;Zihan You;Yi Zhan;Kecen Li;Jialin Wu;Xinfeng Li Hengjun Liu;Yi Yu;Bin Cao;Yiwen Xu;Yang Liu;Guilin Qi", "summary": "Differential Privacy (DP) is a widely adopted technique, valued for its\neffectiveness in protecting the privacy of task-specific datasets, making it a\ncritical tool for large language models. However, its effectiveness in\nMultimodal Large Language Models (MLLMs) remains uncertain. Applying\nDifferential Privacy (DP) inherently introduces substantial computation\noverhead, a concern particularly relevant for MLLMs which process extensive\ntextual and visual data. Furthermore, a critical challenge of DP is that the\ninjected noise, necessary for privacy, scales with parameter dimensionality,\nleading to pronounced model degradation; This trade-off between privacy and\nutility complicates the application of Differential Privacy (DP) to complex\narchitectures like MLLMs. To address these, we propose Dual-Priv Pruning, a\nframework that employs two complementary pruning mechanisms for DP fine-tuning\nin MLLMs: (i) visual token pruning to reduce input dimensionality by removing\nredundant visual information, and (ii) gradient-update pruning during the DP\noptimization process. This second mechanism selectively prunes parameter\nupdates based on the magnitude of noisy gradients, aiming to mitigate noise\nimpact and improve utility. Experiments demonstrate that our approach achieves\ncompetitive results with minimal performance degradation. In terms of\ncomputational efficiency, our approach consistently utilizes less memory than\nstandard DP-SGD. While requiring only 1.74% more memory than zeroth-order\nmethods which suffer from severe performance issues on A100 GPUs, our method\ndemonstrates leading memory efficiency on H20 GPUs. To the best of our\nknowledge, we are the first to explore DP fine-tuning in MLLMs. Our code is\ncoming soon.", "comment": null, "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR;cs.AI", "links": "http://arxiv.org/abs/2506.07077v1;http://arxiv.org/pdf/2506.07077v1", "pdf_url": "http://arxiv.org/pdf/2506.07077v1"}, {"title": "HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains", "link": "https://arxiv.org/pdf/2506.07837", "details": "S Wang, Y Zhang, Z Lai, D Kong - arXiv preprint arXiv:2506.07837, 2025", "abstract": "\u2026 to generate **question** **answer** pairs from the captions. Recently, there are even more MLLMs [2], [9] to assist in the generation of VQA data. \u2026 However, we will publicly release our data, models, and code to promote the development of **medical** \u2026", "entry_id": "http://arxiv.org/abs/2506.07837v1", "updated": "2025-06-09 15:01:38", "published": "2025-06-09 15:01:38", "authors": "Shijie Wang;Yilun Zhang;Zeyu Lai;Dexing Kong", "summary": "Multimodal large language models (MLLMs) have shown great potential in\ngeneral domains but perform poorly in some specific domains due to a lack of\ndomain-specific data, such as image-text data or vedio-text data. In some\nspecific domains, there is abundant graphic and textual data scattered around,\nbut lacks standardized arrangement. In the field of medical ultrasound, there\nare ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic\ndiagnostic reports, and so on. However, these ultrasonic materials are often\nsaved in the forms of PDF, images, etc., and cannot be directly used for the\ntraining of MLLMs. This paper proposes a novel image-text reasoning supervised\nfine-tuning data generation pipeline to create specific domain quadruplets\n(image, question, thinking trace, and answer) from domain-specific materials. A\nmedical ultrasound domain dataset ReMUD is established, containing over 45,000\nreasoning and non-reasoning supervised fine-tuning Question Answering (QA) and\nVisual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on\nQwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound\nfield. To facilitate research, the ReMUD dataset, data generation codebase, and\nReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD,\naddressing the data shortage issue in specific domain MLLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2506.07837v1;http://arxiv.org/pdf/2506.07837v1", "pdf_url": "http://arxiv.org/pdf/2506.07837v1"}, {"title": "C-PATH: Conversational Patient Assistance and Triage in Healthcare System", "link": "https://arxiv.org/pdf/2506.06737", "details": "Q Shi, Q Han, C Soares - arXiv preprint arXiv:2506.06737, 2025", "abstract": "\u2026 powered by **large** **language** **models** (LLMs) designed to assist patients in recognizing symptoms and recommending appropriate **medical** departments \u2026 1) **Medical** Knowledge Datasets: Three **medical** **question** **answering** dataset are \u2026", "entry_id": "http://arxiv.org/abs/2506.06737v1", "updated": "2025-06-07 09:48:47", "published": "2025-06-07 09:48:47", "authors": "Qi Shi;Qiwei Han;Cl\u00e1udia Soares", "summary": "Navigating healthcare systems can be complex and overwhelming, creating\nbarriers for patients seeking timely and appropriate medical attention. In this\npaper, we introduce C-PATH (Conversational Patient Assistance and Triage in\nHealthcare), a novel conversational AI system powered by large language models\n(LLMs) designed to assist patients in recognizing symptoms and recommending\nappropriate medical departments through natural, multi-turn dialogues. C-PATH\nis fine-tuned on medical knowledge, dialogue data, and clinical summaries using\na multi-stage pipeline built on the LLaMA3 architecture. A core contribution of\nthis work is a GPT-based data augmentation framework that transforms structured\nclinical knowledge from DDXPlus into lay-person-friendly conversations,\nallowing alignment with patient communication norms. We also implement a\nscalable conversation history management strategy to ensure long-range\ncoherence. Evaluation with GPTScore demonstrates strong performance across\ndimensions such as clarity, informativeness, and recommendation accuracy.\nQuantitative benchmarks show that C-PATH achieves superior performance in\nGPT-rewritten conversational datasets, significantly outperforming\ndomain-specific baselines. C-PATH represents a step forward in the development\nof user-centric, accessible, and accurate AI tools for digital health\nassistance and triage.", "comment": "Accepted in IEEE ICDH 2025, 10 pages, 8 figures, 5 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.06737v1;http://arxiv.org/pdf/2506.06737v1", "pdf_url": "http://arxiv.org/pdf/2506.06737v1"}]
