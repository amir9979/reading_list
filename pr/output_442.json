'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Emergent Abilities in Reduced-Scale Generative Langua'
[{"title": "JDocQA: Japanese Document Question Answering Dataset for Generative Language Models", "link": "https://arxiv.org/pdf/2403.19454", "details": "E Onami, S Kurita, T Miyanishi, T Watanabe - arXiv preprint arXiv:2403.19454, 2024", "abstract": "Document question answering is a task of question answering on given documents such as reports, slides, pamphlets, and websites, and it is a truly demanding task as paper and electronic forms of documents are so common in our society. This is \u2026"}, {"title": "Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models", "link": "https://arxiv.org/html/2403.19631v1", "details": "Y Shi, Q Tan, X Wu, S Zhong, K Zhou, N Liu - arXiv preprint arXiv:2403.19631, 2024", "abstract": "Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more \u2026"}, {"title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models", "link": "https://arxiv.org/pdf/2404.02258", "details": "D Raposo, S Ritter, B Richards, T Lillicrap\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence \u2026"}, {"title": "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models", "link": "https://arxiv.org/pdf/2404.02575", "details": "H Chae, Y Kim, S Kim, KT Ong, B Kwak, M Kim, S Kim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution. Such nature of algorithmic reasoning makes it a challenge for large \u2026"}, {"title": "$\\texttt {LM}^\\texttt {2} $: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://arxiv.org/html/2404.02255v1", "details": "G Juneja, S Dutta, T Chakraborty - arXiv preprint arXiv:2404.02255, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}]
