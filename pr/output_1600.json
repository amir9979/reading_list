'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Adaptive Reinforcement Tuning Language Models as Hard '
[{"title": "EFTNAS: Searching for Efficient Language Models in First-Order Weight-Reordered Super-Networks", "link": "https://aclanthology.org/2024.lrec-main.497.pdf", "details": "JP Munoz, Y Zheng, N Jain - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Transformer-based models have demonstrated outstanding performance in natural language processing (NLP) tasks and many other domains, eg, computer vision. Depending on the size of these models, which have grown exponentially in the past \u2026"}, {"title": "Using Pre-Trained Language Models in an End-to-End Pipeline for Antithesis Detection", "link": "https://aclanthology.org/2024.lrec-main.1502.pdf", "details": "R K\u00fchn, K Saadi, J Mitrovi\u0107, M Granitzer - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Rhetorical figures play an important role in influencing readers and listeners. Some of these word constructs that deviate from the usual language structure are known to be persuasive\u2013antithesis is one of them. This figure combines parallel phrases with \u2026"}, {"title": "Redefining Information Retrieval of Structured Database via Large Language Models", "link": "https://arxiv.org/pdf/2405.05508", "details": "M Wang, Y Zhang, Q Zhao, J Yang, H Zhang - arXiv preprint arXiv:2405.05508, 2024", "abstract": "Retrieval augmentation is critical when Language Models (LMs) exploit non- parametric knowledge related to the query through external knowledge bases before reasoning. The retrieved information is incorporated into LMs as context alongside \u2026"}, {"title": "Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.708.pdf", "details": "O Ignat, Z Jin, A Abzaliev, L Biester, S Castro, N Deng\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Recent progress in large language models (LLMs) has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that \u201cit's all been solved.\u201d Not surprisingly, this has, in turn, made \u2026"}, {"title": "Backdoor Removal for Generative Large Language Models", "link": "https://arxiv.org/pdf/2405.07667", "details": "H Li, Y Chen, Z Zheng, Q Hu, C Chan, H Liu, Y Song - arXiv preprint arXiv \u2026, 2024", "abstract": "With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased \u2026"}, {"title": "BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models", "link": "https://arxiv.org/pdf/2405.04756", "details": "CF Luo, A Ghawanmeh, X Zhu, FK Khattak - arXiv preprint arXiv:2405.04756, 2024", "abstract": "Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly. The language model can also \u2026"}, {"title": "LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play", "link": "https://arxiv.org/pdf/2405.06373", "details": "LC Lu, SJ Chen, TM Pai, CH Yu, H Lee, SH Sun - arXiv preprint arXiv:2405.06373, 2024", "abstract": "Large language models (LLMs) have shown exceptional proficiency in natural language processing but often fall short of generating creative and original responses to open-ended questions. To enhance LLM creativity, our key insight is to \u2026"}, {"title": "Lonas: Elastic low-rank adapters for efficient large language models", "link": "https://aclanthology.org/2024.lrec-main.940.pdf", "details": "JP Munoz, J Yuan, Y Zheng, N Jain - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) continue to grow, reaching hundreds of billions of parameters and making it challenging for Deep Learning practitioners with resource-constrained systems to use them, eg, fine-tuning these models for a \u2026"}, {"title": "Correcting Language Model Bias for Text Classification in True Zero-Shot Learning", "link": "https://aclanthology.org/2024.lrec-main.359.pdf", "details": "F Zhao, W Xianlin, C Yan, CK Loo - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Combining pre-trained language models (PLMs) and manual templates is a common practice for text classification in zero-shot scenarios. However, the effect of this approach is highly volatile, ranging from random guesses to near state-of-the-art \u2026"}]
