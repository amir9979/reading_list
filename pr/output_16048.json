[{"title": "Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2504.13945", "details": "Z Wu, T Song, N Xie, W Zhang, M Zhu, S Wu, S Sun\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The rapid advancement of large vision-language models (LVLMs) has significantly propelled applications in document understanding, particularly in optical character recognition (OCR) and multilingual translation. However, current evaluations of \u2026"}, {"title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models", "link": "https://arxiv.org/pdf/2504.14395", "details": "B Jalaian, ND Bastian - arXiv preprint arXiv:2504.14395, 2025", "abstract": "To develop trustworthy Vision-Language Models (VLMs), it is essential to address adversarial robustness and hallucination mitigation, both of which impact factual accuracy in high-stakes applications such as defense and healthcare. Existing \u2026"}, {"title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "link": "https://arxiv.org/pdf/2504.14366", "details": "P Haller, J Golde, A Akbik - arXiv preprint arXiv:2504.14366, 2025", "abstract": "Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures \u2026"}, {"title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "link": "https://arxiv.org/pdf/2504.14194", "details": "X Zhuang, J Peng, R Ma, Y Wang, T Bai, X Wei, J Qiu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural \u2026"}, {"title": "Exploring Multimodal Language Models for Sustainability Disclosure Extraction: A Comparative Study", "link": "https://aclanthology.org/2025.insights-1.13.pdf", "details": "T Gupta, T Goel, I Verma - The Sixth Workshop on Insights from Negative Results \u2026, 2025", "abstract": "Sustainability metrics have increasingly become a crucial non-financial criterion in investment decision-making. Organizations worldwide are recognizing the importance of sustainability and are proactively highlighting their efforts through \u2026"}, {"title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer", "link": "https://arxiv.org/pdf/2504.10462", "details": "W Lei, J Wang, H Wang, X Li, JH Liew, J Feng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained \u2026"}, {"title": "OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding", "link": "https://arxiv.org/pdf/2504.14692", "details": "S Jiang, Y Wang, S Song, Y Zhang, Z Meng, B Lei\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The practical deployment of medical vision-language models (Med-VLMs) necessitates seamless integration of textual data with diverse visual modalities, including 2D/3D images and videos, yet existing models typically employ separate \u2026"}, {"title": "Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training", "link": "https://arxiv.org/pdf/2504.13123%3F", "details": "X Zhang, Y Zeng, X Huang, H Hu, R Xie, H Hu, Z Kang - arXiv preprint arXiv \u2026, 2025", "abstract": "In recent years, the field of vision-language model pre-training has experienced rapid advancements, driven primarily by the continuous enhancement of textual capabilities in large language models. However, existing training paradigms for \u2026"}, {"title": "Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics", "link": "https://arxiv.org/pdf/2504.17665", "details": "Z Al-Khalili, N Howell, D Klakow - arXiv preprint arXiv:2504.17665, 2025", "abstract": "Assisting LLMs with code generation improved their performance on mathematical reasoning tasks. However, the evaluation of code-assisted LLMs is generally restricted to execution correctness, lacking a rigorous evaluation of their generated \u2026"}]
