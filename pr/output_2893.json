[{"title": "AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback", "link": "https://aclanthology.org/2024.findings-naacl.50.pdf", "details": "W Zhang, Z Lu - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have demonstrated significant success across various domains. However, their application in complex decision-making tasks frequently necessitates intricate prompt engineering or fine-tuning, leading to \u2026"}, {"title": "PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models", "link": "https://aclanthology.org/2024.naacl-long.336.pdf", "details": "HJ Kim, YJ Kim, JY Bak - Proceedings of the 2024 Conference of the North \u2026, 2024", "abstract": "Pre-trained language models (PLMs) show impressive performance in various downstream NLP tasks. However, pre-training large language models demands substantial memory and training compute. Furthermore, due to the substantial \u2026"}, {"title": "Adaptive Rank Selections for Low-Rank Approximation of Language Models", "link": "https://aclanthology.org/2024.naacl-long.13.pdf", "details": "S Gao, T Hua, YC Hsu, Y Shen, H Jin - Proceedings of the 2024 Conference of the \u2026, 2024", "abstract": "Abstract Singular Value Decomposition (SVD) or its weighted variants has significantly progressed in compressing language models. Previous works assume the same importance for all operations and assign the same number of ranks for \u2026"}, {"title": "A Resilient and Accessible Distribution-Preserving Watermark for Large Language Models", "link": "https://openreview.net/pdf%3Fid%3Dc8qWiNiqRY", "details": "Y Wu, Z Hu, J Guo, H Zhang, H Huang - Forty-first International Conference on Machine \u2026", "abstract": "Watermarking techniques offer a promising way to identify machine-generated content via embedding covert information into the contents generated from language models. A challenge in the domain lies in preserving the distribution of original \u2026"}, {"title": "Simple and Effective Masked Diffusion Language Models", "link": "https://arxiv.org/pdf/2406.07524", "details": "SS Sahoo, NYC Cornell Tech, M Arriola, Y Schiff\u2026", "abstract": "While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling. In this work, we show that simple masked discrete diffusion is \u2026"}, {"title": "This Land is Your, My Land: Evaluating Geopolitical Bias in Language Models through Territorial Disputes", "link": "https://aclanthology.org/2024.naacl-long.213.pdf", "details": "B Li, S Haider, C Callison-Burch - Proceedings of the 2024 Conference of the North \u2026, 2024", "abstract": "Abstract Do the Spratly Islands belong to China, the Philippines, or Vietnam? A pretrained large language model (LLM) may answer differently if asked in the languages of each claimant country: Chinese, Tagalog, or Vietnamese. This \u2026"}, {"title": "$\\texttt {MoE-RBench} $: Towards Building Reliable Language Models with Sparse Mixture-of-Experts", "link": "https://openreview.net/pdf%3Fid%3DLyJ85kgHFe", "details": "G Chen, X Zhao, T Chen, Y Cheng - Forty-first International Conference on Machine \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new \u2026"}, {"title": "STAR: SocioTechnical Approach to Red Teaming Language Models", "link": "https://arxiv.org/pdf/2406.11757", "details": "L Weidinger, J Mellor, BG Pegueroles, N Marchal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This research introduces STAR, a sociotechnical framework that improves on current best practices for red teaming safety of large language models. STAR makes two key contributions: it enhances steerability by generating parameterised instructions for \u2026"}, {"title": "On Trojans in Refined Language Models", "link": "https://arxiv.org/pdf/2406.07778", "details": "J Raghuram, G Kesidis, DJ Miller - arXiv preprint arXiv:2406.07778, 2024", "abstract": "A Trojan in a language model can be inserted when the model is refined for a particular application such as determining the sentiment of product reviews. In this paper, we clarify and empirically explore variations of the data-poisoning threat \u2026"}]
