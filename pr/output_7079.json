[{"title": "Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting Framework", "link": "https://arxiv.org/pdf/2409.16146", "details": "L Chen, R Zhang, J Guo, Y Fan, X Cheng - arXiv preprint arXiv:2409.16146, 2024", "abstract": "Retrieval-augmented generation (RAG) has emerged as a popular solution to mitigate the hallucination issues of large language models. However, existing studies on RAG seldom address the issue of predictive uncertainty, ie, how likely it is \u2026"}, {"title": "Multi-relational graph contrastive learning with learnable graph augmentation", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006816", "details": "X Mo, J Pang, B Wan, R Tang, H Liu, S Jiang - Neural Networks, 2024", "abstract": "Multi-relational graph learning aims to embed entities and relations in knowledge graphs into low-dimensional representations, which has been successfully applied to various multi-relationship prediction tasks, such as information retrieval, question \u2026"}, {"title": "HelloBench: Evaluating Long Text Generation Capabilities of Large Language Models", "link": "https://arxiv.org/pdf/2409.16191", "details": "H Que, F Duan, L He, Y Mou, W Zhou, J Liu, W Rong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks (eg, long-context understanding), and many benchmarks have been proposed. However, we observe that long text generation capabilities are \u2026"}, {"title": "AXCEL: Automated eXplainable Consistency Evaluation using LLMs", "link": "https://arxiv.org/pdf/2409.16984", "details": "PA Sreekar, S Verma, S Chopra, S Ghazarian\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) are widely used in both industry and academia for various tasks, yet evaluating the consistency of generated text responses continues to be a challenge. Traditional metrics like ROUGE and BLEU show a weak \u2026"}, {"title": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification", "link": "https://arxiv.org/pdf/2409.16718", "details": "M Li, J Zhong, C Li, L Li, N Lin, M Sugiyama - arXiv preprint arXiv:2409.16718, 2024", "abstract": "Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed the success of prompt tuning and adapter tuning, while the classic model fine-tuning on inherent parameters seems to be overlooked. It is believed that fine-tuning the \u2026"}, {"title": "Harnessing Diversity for Important Data Selection in Pretraining Large Language Models", "link": "https://arxiv.org/pdf/2409.16986", "details": "C Zhang, H Zhong, K Zhang, C Chai, R Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Data selection is of great significance in pre-training large language models, given the variation in quality within the large-scale available training corpora. To achieve this, researchers are currently investigating the use of data influence to measure the \u2026"}, {"title": "SPHINX: A Mixer of Weights, Visual Embeddings and Image Scales for Multi-modal Large Language Models", "link": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07894.pdf", "details": "D Liu, R Zhang12, P Gao, L Qiu23, H Xiao, H Qiu\u2026", "abstract": "We present SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, visual embeddings and image scales. First, for stronger vision-language alignment, we unfreeze the large language model (LLM) \u2026"}, {"title": "Efficient Bias Mitigation Without Privileged Information", "link": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/09097.pdf", "details": "ME Zarlenga, JTA Swami Sankaranarayanan\u2026", "abstract": "Deep neural networks trained via empirical risk minimization often exhibit significant performance disparities across groups, particularly when group and task labels are spuriously correlated (eg,\u201cgrassy background\u201d and \u201ccows\u201d). Existing bias mitigation \u2026"}, {"title": "Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents", "link": "https://arxiv.org/pdf/2409.17140", "details": "J Lu, Z Zhang, F Yang, J Zhang, L Wang, C Du, Q Lin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal large language models (MLLMs) have enabled LLM-based agents to directly interact with application user interfaces (UIs), enhancing agents' performance in complex tasks. However, these agents often suffer from high latency and low \u2026"}]
