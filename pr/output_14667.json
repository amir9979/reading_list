[{"title": "Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models", "link": "https://arxiv.org/pdf/2503.16779", "details": "M Wu, T Zhu, H Han, X Zhang, W Shao, W Chen - arXiv preprint arXiv:2503.16779, 2025", "abstract": "Tool learning can further broaden the usage scenarios of large language models (LLMs). However most of the existing methods either need to finetune that the model can only use tools seen in the training data, or add tool demonstrations into the \u2026"}, {"title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources", "link": "https://arxiv.org/pdf/2504.00595", "details": "W Wang, Y Tian, L Yang, H Wang, X Yan - arXiv preprint arXiv:2504.00595, 2025", "abstract": "The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We \u2026"}, {"title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models", "link": "https://arxiv.org/pdf/2503.24235", "details": "Q Zhang, F Lyu, Z Sun, L Wang, W Zhang, Z Guo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as``test-time computing''has emerged as a prominent research focus. Recent studies demonstrate \u2026"}, {"title": "Model Hemorrhage and the Robustness Limits of Large Language Models", "link": "https://arxiv.org/pdf/2503.23924", "details": "Z Ma, Z Li, L Zhang, GS Xia, B Du, L Zhang, D Tao - arXiv preprint arXiv:2503.23924, 2025", "abstract": "Large language models (LLMs) demonstrate strong performance across natural language processing tasks, yet undergo significant performance degradation when modified for deployment through quantization, pruning, or decoding strategy \u2026"}, {"title": "Agentic Large Language Models, a survey", "link": "https://arxiv.org/pdf/2503.23037", "details": "A Plaat, M van Duijn, N van Stein, M Preuss\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "There is great interest in agentic LLMs, large language models that act as agents. We review the growing body of work in this area and provide a research agenda. Agentic LLMs are LLMs that (1) reason,(2) act, and (3) interact. We organize the \u2026"}, {"title": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scale Test-Time Compute", "link": "https://arxiv.org/pdf/2504.00762", "details": "J Chen, Z Xun, B Zhou, H Qi, Q Zhang, Y Chen, W Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper presents a simple, effective, and cost-efficient strategy to improve LLM performance by scaling test-time compute. Our strategy builds upon the repeated- sampling-then-voting framework, with a novel twist: incorporating multiple models \u2026"}, {"title": "CodeIF-Bench: Evaluating Instruction-Following Capabilities of Large Language Models in Interactive Code Generation", "link": "https://arxiv.org/pdf/2503.22688", "details": "P Wang, L Zhang, F Liu, L Shi, M Li, B Shen, A Fu - arXiv preprint arXiv:2503.22688, 2025", "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance in code generation tasks and have become indispensable programming assistants for developers. However, existing code generation benchmarks primarily assess the \u2026"}, {"title": "GPBench: A Comprehensive and Fine-Grained Benchmark for Evaluating Large Language Models as General Practitioners", "link": "https://arxiv.org/pdf/2503.17599", "details": "Z Li, Y Yang, J Lang, W Jiang, Y Zhao, S Li, D Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "General practitioners (GPs) serve as the cornerstone of primary healthcare systems by providing continuous and comprehensive medical services. However, due to community-oriented nature of their practice, uneven training and resource gaps, the \u2026"}, {"title": "Efficient and explainable sequential recommendation with language model", "link": "https://drive.google.com/file/d/11rlrXoCrYwH9mIJCDfw2gHwD8PCImjoK/view", "details": "Z Li, L Zou, C Ma, C Li - Information Processing & Management, 2025", "abstract": "Motivated by the outstanding success of large language models (LLMs) in a broad spectrum of NLP tasks, applying them for explainable recommendation become a cutting-edge recently. However, due to the inherent inconsistency in the information \u2026"}]
