'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Tripod: Three Complementary Inductive Biases for Disen'
[{"title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2404.05567", "details": "B Pan, Y Shen, H Liu, M Mishra, G Zhang, A Oliva\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) language models can reduce computational costs by 2- 4$\\times $ compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios. However, MoE models generally \u2026"}, {"title": "PMC-LLaMA: toward building open-source language models for medicine", "link": "https://academic.oup.com/jamia/advance-article-abstract/doi/10.1093/jamia/ocae045/7645318", "details": "C Wu, W Lin, X Zhang, Y Zhang, W Xie, Y Wang - Journal of the American Medical \u2026, 2024", "abstract": "Objective Recently, large language models (LLMs) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering (QA) situations, these models \u2026"}, {"title": "SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation", "link": "https://arxiv.org/pdf/2404.01923", "details": "S Guo, L Liao, J Zhang, Y Wang, C Li, H Chen - arXiv preprint arXiv:2404.01923, 2024", "abstract": "Knowledge base question generation (KBQG) aims to generate natural language questions from a set of triplet facts extracted from KB. Existing methods have significantly boosted the performance of KBQG via pre-trained language models \u2026"}]
