[{"title": "TokenFLEX: Unified VLM Training for Flexible Visual Tokens Inference", "link": "https://arxiv.org/pdf/2504.03154", "details": "J Hu, J Mao, Z Liu, Z Xia, P Jia, X Lang - arXiv preprint arXiv:2504.03154, 2025", "abstract": "Conventional Vision-Language Models (VLMs) typically utilize a fixed number of vision tokens, regardless of task complexity. This one-size-fits-all strategy introduces notable inefficiencies: using excessive tokens leads to unnecessary computational \u2026"}, {"title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?", "link": "https://arxiv.org/pdf/2504.10000", "details": "Y Wang, J Guan, J Liang, R He - arXiv preprint arXiv:2504.10000, 2025", "abstract": "Multi-modal large language models (MLLMs) have made significant progress, yet their safety alignment remains limited. Typically, current open-source MLLMs rely on the alignment inherited from their language module to avoid harmful generations \u2026"}, {"title": "Describe Anything: Detailed Localized Image and Video Captioning", "link": "https://arxiv.org/pdf/2504.16072", "details": "L Lian, Y Ding, Y Ge, S Liu, H Mao, B Li, M Pavone\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Generating detailed and accurate descriptions for specific regions in images and videos remains a fundamental challenge for vision-language models. We introduce the Describe Anything Model (DAM), a model designed for detailed localized \u2026"}, {"title": "SilVar-Med: A Speech-Driven Visual Language Model for Explainable Abnormality Detection in Medical Imaging", "link": "https://arxiv.org/pdf/2504.10642", "details": "TH Pham, C Ngo, TD Bui, ML Quang, TH Pham, TS Hy - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical Visual Language Models have shown great potential in various healthcare applications, including medical image captioning and diagnostic assistance. However, most existing models rely on text-based instructions, limiting their usability \u2026"}, {"title": "SmolVLM: Redefining small and efficient multimodal models", "link": "https://arxiv.org/pdf/2504.05299", "details": "A Marafioti, O Zohar, M Farr\u00e9, M Noyan, E Bakouch\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision-Language Models (VLMs) deliver exceptional performance but require significant computational resources, limiting their deployment on mobile and edge devices. Smaller VLMs typically mirror design choices of larger models, such as \u2026"}, {"title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models", "link": "https://arxiv.org/pdf/2504.15271", "details": "G Chen, Z Li, S Wang, J Jiang, Y Liu, L Lu, DA Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce Eagle 2.5, a family of frontier vision-language models (VLMs) for long- context multimodal learning. Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing a generalist \u2026"}, {"title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training", "link": "https://arxiv.org/pdf/2504.13161", "details": "S Diao, Y Yang, Y Fu, X Dong, D Su, M Kliegl, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Pre-training datasets are typically collected from web content and lack inherent domain divisions. For instance, widely used datasets like Common Crawl do not include explicit domain labels, while manually curating labeled datasets such as The \u2026"}, {"title": "Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2504.01857", "details": "Z Yu, T Li, C Wang, H Chen, L Zhou - arXiv preprint arXiv:2504.01857, 2025", "abstract": "Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing reasoning capabilities in large language models (LLMs), with self-consistency demonstrating notable promise in boosting performance. However, inherent \u2026"}, {"title": "Falcon Medical Visual Question Answering", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/35346/37501", "details": "A Bawazir, H Alshanqiti, K Wu, F Albreiki - Proceedings of the AAAI Conference on \u2026, 2025", "abstract": "Abstract Vision-Language Models (VLMs) bridge the gap between visual and textual data, enabling multimodal tasks like Visual Question Answering (VQA). Leveraging this capability, Medical VQA systems have the potential to transform clinical decision \u2026"}]
