[{"title": "Limits of transformer language models on learning to compose algorithms", "link": "https://research.ibm.com/publications/limits-of-transformer-language-models-on-learning-to-compose-algorithms", "details": "J Thomm, A Terzic, G Camposampiero, M Hersche\u2026 - Annual Conference on \u2026, 2024", "abstract": "We analyze the capabilities of Transformer language models in learning compositional discrete tasks. To this end, we evaluate training LLaMA models and prompting GPT-4 and Gemini on four tasks demanding to learn a composition of \u2026"}, {"title": "Tree of Attributes Prompt Learning for Vision-Language Models", "link": "https://arxiv.org/pdf/2410.11201", "details": "T Ding, W Li, Z Miao, H Pfister - arXiv preprint arXiv:2410.11201, 2024", "abstract": "Prompt learning has proven effective in adapting vision language models for downstream tasks. However, existing methods usually append learnable prompt tokens solely with the category names to obtain textual features, which fails to fully \u2026"}, {"title": "Axes of Robustness of Neural Language Models", "link": "https://is.muni.cz/th/m805b/PhD_thesis_Michal_Stefanik.pdf", "details": "M \u0160TEF\u00c1NIK", "abstract": "In recent years, language models have emerged into a technology adopted in a wide variety of applications, nowadays largely exceeding traditional natural language processing tasks. Thanks to their versatility and adaptability, modern language \u2026"}]
