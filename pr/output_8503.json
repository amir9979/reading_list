[{"title": "LanFL: Differentially Private Federated Learning with Large Language Models using Synthetic Samples", "link": "https://arxiv.org/pdf/2410.19114", "details": "H Wu, D Klabjan - arXiv preprint arXiv:2410.19114, 2024", "abstract": "Federated Learning (FL) is a collaborative, privacy-preserving machine learning framework that enables multiple participants to train a single global model. However, the recent advent of powerful Large Language Models (LLMs) with tens to hundreds \u2026"}, {"title": "Counterfactual Causal Inference in Natural Language with Large Language Models", "link": "https://arxiv.org/pdf/2410.06392", "details": "G Gendron, JM Ro\u017eanec, M Witbrock, G Dobbie - arXiv preprint arXiv:2410.06392, 2024", "abstract": "Causal structure discovery methods are commonly applied to structured data where the causal variables are known and where statistical testing can be used to assess the causal relationships. By contrast, recovering a causal structure from unstructured \u2026"}, {"title": "The last iterate advantage: Empirical auditing and principled heuristic analysis of differentially private sgd", "link": "https://arxiv.org/pdf/2410.06186", "details": "T Steinke, M Nasr, A Ganesh, B Balle\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We propose a simple heuristic privacy analysis of noisy clipped stochastic gradient descent (DP-SGD) in the setting where only the last iterate is released and the intermediate iterates remain hidden. Namely, our heuristic assumes a linear structure \u2026"}, {"title": "Category-guided multi-interest collaborative metric learning with representation uniformity constraints", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324002966", "details": "L Wang, T Lian - Information Processing & Management, 2025", "abstract": "Multi-interest collaborative metric learning has recently emerged as an effective approach to modeling the multifaceted interests of a user in recommender systems. However, two issues remain unexplored.(1) There is no explicit guidance for the \u2026"}, {"title": "$\\beta $-calibration of Language Model Confidence Scores for Generative QA", "link": "https://arxiv.org/pdf/2410.06615", "details": "P Manggala, A Mastakouri, E Kirschbaum\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim \u2026"}, {"title": "Chest X-ray synthetic data for better testing and evaluation of ML models", "link": "https://cs231n.stanford.edu/2024/papers/chest-x-ray-synthetic-data-for-better-testing-and-evaluation-of-.pdf", "details": "E Bismuth, A Geslin, M Paschali", "abstract": "Abstract The use of Machine Learning (ML) models in radiology has significantly advanced medical imaging diagnostics. However, model performance may not accurately reflect certain underrepresented conditions if such conditions or minorities \u2026"}, {"title": "Evolutionary Contrastive Distillation for Language Model Alignment", "link": "https://arxiv.org/pdf/2410.07513", "details": "J Katz-Samuels, Z Li, H Yun, P Nigam, Y Xu, V Petricek\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The ability of large language models (LLMs) to execute complex instructions is essential for their real-world applications. However, several recent studies indicate that LLMs struggle with challenging instructions. In this paper, we propose \u2026"}, {"title": "Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures", "link": "https://arxiv.org/pdf/2410.06672", "details": "J Wang, X Ge, W Shu, Q Tang, Y Zhou, Z He, X Qiu - arXiv preprint arXiv:2410.06672, 2024", "abstract": "The hypothesis of Universality in interpretability suggests that different neural networks may converge to implement similar algorithms on similar tasks. In this work, we investigate two mainstream architectures for language modeling, namely \u2026"}, {"title": "Falcon mamba: The first competitive attention-free 7b language model", "link": "https://arxiv.org/pdf/2410.05355", "details": "J Zuo, M Velikanov, DE Rhaiem, I Chahed, Y Belkada\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this technical report, we present Falcon Mamba 7B, a new base large language model based on the novel Mamba architecture. Falcon Mamba 7B is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model \u2026"}]
