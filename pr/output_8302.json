[{"title": "Improved Depth Estimation of Bayesian Neural Networks", "link": "https://arxiv.org/pdf/2410.10395", "details": "B van Erp, B de Vries - arXiv preprint arXiv:2410.10395, 2024", "abstract": "This paper proposes improvements over earlier work by Nazareth and Blei (2022) for estimating the depth of Bayesian neural networks. Here, we propose a discrete truncated normal distribution over the network depth to independently learn its mean \u2026"}, {"title": "Time Series Classification with Large Language Models via Linguistic Scaffolding", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10706904.pdf", "details": "H Jang, JY Yang, J Hwang, E Yang - IEEE Access, 2024", "abstract": "Time series classification requires specialized models that can effectively capture temporal structures. Consequently, Large Language Models (LLMs) have emerged as promising candidates due to their proficiency in sequence modeling and semantic \u2026"}, {"title": "Uncertainty-aware Deep Imitation Learning and Deployment for Autonomous Navigation through Crowded Intersections", "link": "http://www.poss.pku.edu.cn/Data/IROS2024%2520Uncertainty-aware%2520Deep%2520Imitation%2520Learning%2520and%2520Deployment%2520for%2520Autonomous%2520Navigation%2520through%2520Crowded%2520Intersections.pdf", "details": "Z Zhu, S Wang, H Zhao", "abstract": "Navigation through crowded intersections is a challenge for autonomous vehicles, where uncertainty arises from interaction with other road users, encountering new scenes and weathers, etc. Recent end-to-end autonomous control deep models \u2026"}, {"title": "Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models", "link": "https://arxiv.org/pdf/2410.09385", "details": "SK Bhethanabhotla, O Swelam, J Siems, D Salinas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces Mamba4Cast, a zero-shot foundation model for time series forecasting. Based on the Mamba architecture and inspired by Prior-data Fitted Networks (PFNs), Mamba4Cast generalizes robustly across diverse time series tasks \u2026"}, {"title": "Enhancing Zero-Shot Vision Models by Label-Free Prompt Distribution Learning and Bias Correcting", "link": "https://arxiv.org/pdf/2410.19294", "details": "X Zhu, B Zhu, Y Tan, S Wang, Y Hao, H Zhang - arXiv preprint arXiv:2410.19294, 2024", "abstract": "Vision-language models, such as CLIP, have shown impressive generalization capacities when using appropriate text descriptions. While optimizing prompts on downstream labeled data has proven effective in improving performance, these \u2026"}, {"title": "Improving Out-of-Distribution Detection with Disentangled Foreground and Background Features", "link": "https://dl.acm.org/doi/abs/10.1145/3664647.3681614", "details": "C Ding, G Pang - Proceedings of the 32nd ACM International \u2026, 2024", "abstract": "Detecting out-of-distribution (OOD) inputs is a principal task for ensuring the safety of deploying deep-neural-network classifiers in open-set scenarios. OOD samples can be drawn from arbitrary distributions and exhibit deviations from in-distribution (ID) \u2026"}, {"title": "Interpolation with skepticism, for highly irregular time series health data", "link": "https://www.diva-portal.org/smash/get/diva2:1908176/FULLTEXT02", "details": "A Baaz, O Zell - 2024", "abstract": "In many domains, such as healthcare, climate science, and biology, the highly irregular time-series data pose major challenges for machine learning. A common approach when making data regular and suitable for machine-learning predictors is \u2026"}, {"title": "Posterior Inferred, Now What? Streamlining Prediction in Bayesian Deep Learning", "link": "https://openreview.net/pdf%3Fid%3Dcx9TXPTzt9", "details": "R Li, M Klasson, A Solin, M Trapp - NeurIPS 2024 Workshop on Bayesian Decision \u2026", "abstract": "The rising interest in Bayesian deep learning (BDL) has led to a plethora of methods for estimating the posterior distribution. However, efficient computation of inferences, such as predictions, has been largely overlooked with Monte Carlo integration \u2026"}, {"title": "DEformer: Dual Embedded Transformer for Multivariate Time Series Forecasting", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10710626.pdf", "details": "M Kim, S Lee, SM Choi - IEEE Access, 2024", "abstract": "Deep learning models have significantly addressed the challenges of multivariate time series forecasting. Recently, Transformer-based models which have primarily focused on either temporal or inter-variate (spatial) dependencies have \u2026"}]
