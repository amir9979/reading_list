[{"title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models", "link": "https://openreview.net/pdf%3Fid%3DLC1QAqhePv", "details": "D Zhang, Z Hu, S Zhoubian, Z Du, K Yang, Z Wang\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving \u2026"}, {"title": "Counterfactual Generation from Language Models", "link": "https://arxiv.org/pdf/2411.07180", "details": "S Ravfogel, A Svete, V Sn\u00e6bjarnarson, R Cotterell - arXiv preprint arXiv:2411.07180, 2024", "abstract": "Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery--eg, model ablations or manipulation \u2026"}, {"title": "Training and Evaluating Language Models with Template-based Data Generation", "link": "https://arxiv.org/pdf/2411.18104", "details": "Y Zhang - arXiv preprint arXiv:2411.18104, 2024", "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, these \u2026"}, {"title": "CPLLM: Clinical prediction with large language models", "link": "https://journals.plos.org/digitalhealth/article%3Fid%3D10.1371/journal.pdig.0000680", "details": "O Ben Shoham, N Rappoport - PLOS Digital Health, 2024", "abstract": "We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for predicting clinical disease and readmission. We utilized quantization and fine-tuned the LLM using \u2026"}, {"title": "CNNSum: Exploring Long-Conext Summarization with Large Language Models in Chinese Novels", "link": "https://arxiv.org/pdf/2412.02819", "details": "L Wei, H Yan, X Lu, J Zhu, J Wang, W Zhang - arXiv preprint arXiv:2412.02819, 2024", "abstract": "Large Language Models (LLMs) have been well-researched in many long-context tasks. However, due to high annotation costs, high-quality long-context summary datasets for training or evaluation are scarce, limiting further research. In this work \u2026"}, {"title": "Measuring Non-Adversarial Reproduction of Training Data in Large Language Models", "link": "https://arxiv.org/pdf/2411.10242%3F", "details": "M Aerni, J Rando, E Debenedetti, N Carlini, D Ippolito\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models memorize parts of their training data. Memorizing short snippets and facts is required to answer questions about the world and to be fluent in any language. But models have also been shown to reproduce long verbatim \u2026"}, {"title": "Do Large Language Models with Reasoning and Acting Meet the Needs of Task-Oriented Dialogue?", "link": "https://arxiv.org/pdf/2412.01262", "details": "M Elizabeth, M Veyret, M Couceiro, O Dusek\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) gained immense popularity due to their impressive capabilities in unstructured conversations. However, they underperform compared to previous approaches in task-oriented dialogue (TOD), wherein reasoning and \u2026"}, {"title": "Causal Inference and Prediction for Network Data", "link": "https://www.birs.ca/workshops/2024/24w5244/report24w5244.pdf", "details": "E Kolaczyk, E Levina, T Li, E Ogburn", "abstract": "The swift evolution of data collection technologies has yielded an abundance of network data across diverse fields such as social sciences, biology, neuroscience, and engineering. These networks represent complex systems where nodes (eg \u2026"}, {"title": "Dynamic Subset Tuning: Expanding the Operational Range of Parameter-Efficient Training for Large Language Models", "link": "https://arxiv.org/pdf/2411.08610%3F", "details": "F Stahlberg, J Lichtarge, S Kumar - arXiv preprint arXiv:2411.08610, 2024", "abstract": "We propose a novel parameter-efficient training (PET) method for large language models that adapts models to downstream tasks by optimizing a small subset of the existing model parameters. Unlike prior methods, this subset is not fixed in location \u2026"}]
