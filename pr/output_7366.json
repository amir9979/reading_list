[{"title": "A Case Demonstration of the Open Health Natural Language Processing Toolkit From the National COVID-19 Cohort Collaborative and the Researching COVID to \u2026", "link": "https://medinform.jmir.org/2024/1/e49997", "details": "A Wen, L Wang, H He, S Fu, S Liu, DA Hanauer\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background A wealth of clinically relevant information is only obtainable within unstructured clinical narratives, leading to great interest in clinical natural language processing (NLP). While a multitude of approaches to NLP exist, current algorithm \u2026"}, {"title": "Fine-Tuned Transformers and Large Language Models for Entity Recognition in Complex Eligibility Criteria for Clinical Trials", "link": "https://aisel.aisnet.org/isd2014/proceedings2024/datascience/19/", "details": "K Kantor, M Morzy - 2024", "abstract": "This paper evaluates the\\texttt {gpt-4-turbo} model's proficiency in recognizing named entities within the clinical trial eligibility criteria. We employ prompt learning to a dataset comprising $49\\, 903$ criteria from $3\\, 314$ trials, with $120\\, 906 \u2026"}, {"title": "Investigating Layer Importance in Large Language Models", "link": "https://arxiv.org/pdf/2409.14381", "details": "Y Zhang, Y Dong, K Kawaguchi - arXiv preprint arXiv:2409.14381, 2024", "abstract": "Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in \u2026"}, {"title": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language Models on a Single GPU", "link": "https://arxiv.org/pdf/2409.09086", "details": "Z Ning, J Zhao, Q Jin, W Ding, M Guo - arXiv preprint arXiv:2409.09086, 2024", "abstract": "Multimodal Large Language Models (MLLMs) are distinguished by their multimodal comprehensive ability and widely used in many real-world applications including GPT-4o, autonomous driving and robotics. Despite their impressive performance, the \u2026"}, {"title": "Parameter Efficiency, Few-Shot, Zero-Shot, Prompting", "link": "https://jonmay.github.io/USC-CS662/assets/files/llm.pdf", "details": "J May - 2024", "abstract": "The models we've discussed so far follow the paradigm that, out of the box, they don't do too much, but when you expose them to some supervised data that is an exemplar of a task and fine-tune their parameters they can do the task when given \u2026"}]
