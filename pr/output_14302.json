[{"title": "Vision-r1: Incentivizing reasoning capability in multimodal large language models", "link": "https://arxiv.org/pdf/2503.06749", "details": "W Huang, B Jia, Z Zhai, S Cao, Z Ye, F Zhao, Y Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning \u2026"}, {"title": "Assessing Robustness of Multi-Modal Large Language Models in Image Classification through Hierarchical WordNet-Based Evaluation", "link": "https://ieeexplore.ieee.org/abstract/document/10889743/", "details": "C Liu, H Chen, B Wang, S Zheng - ICASSP 2025-2025 IEEE International Conference \u2026, 2025", "abstract": "The advancement of multi-modal large language models (MLLMs) has significantly enhanced their capability to process and understand diverse data types, integrating text, images, and other modalities. Despite their impressive performance, evaluating \u2026"}, {"title": "Exploring the Role of CLIP Global Visual Features in Multimodal Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10889200/", "details": "Z Bai, Y Bai - ICASSP 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "The next recognized development direction of large language models (LLMs) is to integrate and enhance multimodal capability. Although current multimodal large language models (MLLMs) have achieved impressive performance by combining the \u2026"}, {"title": "InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2503.06692", "details": "Y Yan, Y Shen, Y Liu, J Jiang, M Zhang, J Shao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Advanced reasoning in large language models has achieved remarkable performance on challenging tasks, but the prevailing long-context reasoning paradigm faces critical limitations: quadratic computational scaling with sequence \u2026"}, {"title": "Alignment for Efficient Tool Calling of Large Language Models", "link": "https://arxiv.org/pdf/2503.06708", "details": "H Xu, Z Wang, Z Zhu, L Pan, X Chen, L Chen, K Yu - arXiv preprint arXiv:2503.06708, 2025", "abstract": "Recent advancements in tool learning have enabled large language models (LLMs) to integrate external tools, enhancing their task performance by expanding their knowledge boundaries. However, relying on tools often introduces tradeoffs between \u2026"}, {"title": "GRU: Mitigating the Trade-off between Unlearning and Retention for Large Language Models", "link": "https://arxiv.org/pdf/2503.09117", "details": "Y Wang, Q Wang, F Liu, W Huang, Y Du, X Du, B Han - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language model (LLM) unlearning has demonstrated its essential role in removing privacy and copyright-related responses, crucial for their legal and safe applications. However, the pursuit of complete unlearning often comes with \u2026"}, {"title": "Enhancing Large Language Models on Domain-specific Tasks: A Novel Training Strategy via Domain Adaptation and Preference Alignment", "link": "https://ieeexplore.ieee.org/abstract/document/10890050/", "details": "J Deng, Z Zhang, JK Cheng, J Ma - \u2026 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "In handling complex, domain-specific tasks, particularly in the context of state-owned assets and enterprises (SOAEs), general LLMs suffer from the knowledge gap due to insufficient exposure to domain-specific corpora, and the value disagreement, as \u2026"}, {"title": "LLM-QE: Improving Query Expansion by Aligning Large Language Models with Ranking Preferences", "link": "https://arxiv.org/pdf/2502.17057", "details": "S Yao, P Huang, Z Liu, Y Gu, Y Yan, S Yu, G Yu - arXiv preprint arXiv:2502.17057, 2025", "abstract": "Query expansion plays a crucial role in information retrieval, which aims to bridge the semantic gap between queries and documents to improve matching performance. This paper introduces LLM-QE, a novel approach that leverages Large Language \u2026"}, {"title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models", "link": "https://arxiv.org/pdf/2503.07605", "details": "X Liang, H Wang, H Lai, S Niu, S Song, J Yang, J Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning \u2026"}]
