[{"title": "CTPT: Continual Test-time Prompt Tuning for vision-language models", "link": "https://www.sciencedirect.com/science/article/pii/S0031320324010513", "details": "F Wang, Z Han, X Liu, Y Yin, X Gao - Pattern Recognition, 2024", "abstract": "Abstract Test-time Prompt Tuning (TPT) aims to further enhance the generalization capabilities of pre-trained vision-language models, eg, CLIP, on streaming test samples from a new distribution. Current TPT methods primarily utilize self-training \u2026"}, {"title": "3VL: Using Trees to Improve Vision-Language Models' Interpretability", "link": "https://ieeexplore.ieee.org/abstract/document/10829542/", "details": "N Yellinek, L Karlinsky, R Giryes - IEEE Transactions on Image Processing, 2025", "abstract": "Vision-Language models (VLMs) have proven to be effective at aligning image and text representations, producing superior zero-shot results when transferred to many downstream tasks. However, these representations suffer from some key \u2026"}, {"title": "Simple controls exceed best deep learning algorithms and reveal foundation model effectiveness for predicting genetic perturbations", "link": "https://www.biorxiv.org/content/10.1101/2025.01.06.631555.full.pdf", "details": "DR Wong, A Hill, R Moccia - bioRxiv, 2025", "abstract": "Modeling genetic perturbations and their effect on the transcriptome is a key area of pharmaceutical research. Due to the complexity of the transcriptome, there has been much excitement and development in deep learning (DL) because of its ability to \u2026"}, {"title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning", "link": "https://arxiv.org/pdf/2412.09078", "details": "Z Bi, K Han, C Liu, Y Tang, Y Wang - arXiv preprint arXiv:2412.09078, 2024", "abstract": "Large Language Models (LLMs) have shown remarkable abilities across various language tasks, but solving complex reasoning problems remains a challenge. While existing methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) enhance \u2026"}, {"title": "Understanding Before Reasoning: Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting", "link": "https://arxiv.org/pdf/2501.04341", "details": "DH Zhu, YJ Xiong, JC Zhang, XJ Xie, CM Xia - arXiv preprint arXiv:2501.04341, 2025", "abstract": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language Models (LLMs) to enhance complex reasoning. It guides LLMs to present multi-step reasoning, rather than generating the final answer directly. However, CoT \u2026"}]
