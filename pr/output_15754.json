[{"title": "CSPLADE: Learned Sparse Retrieval with Causal Language Models", "link": "https://arxiv.org/pdf/2504.10816", "details": "Z Xu, A Feng, Y Tian, H Ding, LL Cheong - arXiv preprint arXiv:2504.10816, 2025", "abstract": "In recent years, dense retrieval has been the focus of information retrieval (IR) research. While effective, dense retrieval produces uninterpretable dense vectors, and suffers from the drawback of large index size. Learned sparse retrieval (LSR) \u2026"}, {"title": "Forest for the Trees: Overarching Prompting Evokes High-Level Reasoning in Large Language Models", "link": "https://aclanthology.org/anthology-files/pdf/naacl/2025.naacl-long.66.pdf", "details": "H Liao, S Hu, Z Zhu, H He, Y Jin", "abstract": "Abstract Chain-of-thought (CoT) and subsequent methods adopted a deductive paradigm that decomposes the reasoning process, demonstrating remarkable performances across NLP tasks. However, such a paradigm faces the challenge of \u2026"}, {"title": "LOFT: Scalable and More Realistic Long-Context Evaluation", "link": "https://aclanthology.org/anthology-files/pdf/naacl/2025.naacl-findings.374.pdf", "details": "J Lee, A Chen, Z Dai, DDDSS Michael, BY Luan\u2026", "abstract": "Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. Leveraging LCLMs' ability to natively ingest and process entire corpora of \u2026"}, {"title": "On the Applicability of Code Language Models to Scientific Computing Programs", "link": "https://ieeexplore.ieee.org/abstract/document/10977820/", "details": "Q Zhao, F Liu, X Long, C Wu, L Zhang - IEEE Transactions on Software Engineering, 2025", "abstract": "Scientific Computing Programming Languages (SCPLs), like MATLAB and R, are popular and widely used for computational mathematics. In recent years, pre-trained code language models (CLMs) have automated many code-related tasks, covering \u2026"}, {"title": "Adapting LLM Agents with Universal Communication Feedback", "link": "https://aclanthology.org/anthology-files/pdf/findings/2025.findings-naacl.339.pdf", "details": "K Wang, Y Lu, M Santacroce, Y Gong, C Zhang\u2026", "abstract": "Recent advances in large language models (LLMs) have demonstrated potential for LLM agents. To facilitate the training for these agents with both linguistic feedback and nonlinguistic reward signals, we introduce Learning through Communication \u2026"}, {"title": "NAT: Enhancing Agent Tuning with Negative Samples", "link": "https://aclanthology.org/anthology-files/pdf/naacl/2025.naacl-long.378.pdf", "details": "R Wang, X Han, Y Zhang, T Baldwin, H Li", "abstract": "Interaction trajectories between agents and environments have proven effective in tuning LLMs into task-specific agents. However, constructing these trajectories, especially successful trajectories, is often computationally and time intensive due to \u2026"}, {"title": "Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data", "link": "https://aclanthology.org/anthology-files/pdf/naacl/2025.naacl-long.191.pdf", "details": "JZMMT Li, B Van Durme, D Khashabi", "abstract": "To trust the fluent generations of large language models (LLMs), humans must be able to verify their correctness against trusted external sources. Recent efforts, such as providing citations via retrieved documents or post-hoc provenance, enhance \u2026"}, {"title": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection", "link": "https://arxiv.org/pdf/2504.00695", "details": "X Zhu, Z Gu, S Zheng, T Wang, T Li, H Feng, Y Xiao - arXiv preprint arXiv:2504.00695, 2025", "abstract": "Pre-training large language models (LLMs) necessitates enormous diverse textual corpora, making effective data selection a key challenge for balancing computational resources and model performance. Current methodologies primarily emphasize data \u2026"}, {"title": "Analyzing and Improving Coherence of Large Language Models in Question Answering", "link": "https://aclanthology.org/anthology-files/pdf/naacl/2025.naacl-long.588.pdf", "details": "I Lauriola, AGI Amazon, S Campese, A Moschitti", "abstract": "Large language models (LLMs) have recently revolutionized natural language processing. These models, however, often suffer from instability or lack of coherence, that is the ability of the models to generate semantically equivalent outputs when \u2026"}]
