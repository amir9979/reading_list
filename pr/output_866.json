'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [A Comparison of Parameter-Efficient ASR Domain Adaptation Me'
[{"title": "Grounding and Enhancing Grid-based Models for Neural Fields", "link": "https://arxiv.org/pdf/2403.20002", "details": "Z Zhao, F Fan, W Liao, J Yan - arXiv preprint arXiv:2403.20002, 2024", "abstract": "Many contemporary studies utilize grid-based models for neural field representation, but a systematic analysis of grid-based models is still missing, hindering the improvement of those models. Therefore, this paper introduces a theoretical \u2026"}, {"title": "Pre-training enhanced unsupervised contrastive domain adaptation for industrial equipment remaining useful life prediction", "link": "https://www.sciencedirect.com/science/article/pii/S1474034624001654", "details": "H Li, P Cao, X Wang, Y Li, B Yi, M Huang - Advanced Engineering Informatics, 2024", "abstract": "An essential task in industrial intelligence is to accurately predict the remaining useful life (RUL) of industrial equipment, and there has been tremendous progress in RUL prediction based on data-driven methods. However, these methods rely heavily \u2026"}, {"title": "HDPNERF: Hybrid Depth Priors for Neural Radiance Fields from Sparse Input Views", "link": "https://ieeexplore.ieee.org/abstract/document/10446844/", "details": "W Xu, Q Wang, X Pan, R Wang - ICASSP 2024-2024 IEEE International Conference \u2026, 2024", "abstract": "Neural Radiance Field (NeRF) shows a high prospect in the task of novel view synthesis. However, performance degrades drastically under limited input views since NeRF heavily relies on a large number of images to fit the geometry in scenes \u2026"}, {"title": "Harnessing the Power of Large Vision Language Models for Synthetic Image Detection", "link": "https://arxiv.org/pdf/2404.02726", "details": "M Keita, W Hamidouche, H Bougueffa, A Hadid\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, the emergence of models capable of generating images from text has attracted considerable interest, offering the possibility of creating realistic images from text descriptions. Yet these advances have also raised concerns about the \u2026"}, {"title": "Language Models Still Struggle to Zero-shot Reason about Time Series", "link": "https://arxiv.org/pdf/2404.11757", "details": "MA Merrill, M Tan, V Gupta, T Hartvigsen, T Althoff - arXiv preprint arXiv:2404.11757, 2024", "abstract": "Time series are critical for decision-making in fields like finance and healthcare. Their importance has driven a recent influx of works passing time series into language models, leading to non-trivial forecasting on some datasets. But it remains \u2026"}, {"title": "Systematic synthesis of design prompts for large language models in conceptual design", "link": "https://www.sciencedirect.com/science/article/pii/S000785062400074X", "details": "Y Tian, A Liu, Y Dai, K Nagato, M Nakao - CIRP Annals, 2024", "abstract": "Recent advancements in large language models (LLMs) demonstrate great potential in supporting engineering design, especially conceptual design. Prompt engineering plays an important role in facilitating designer-LLM collaboration in conceptual \u2026"}, {"title": "M2BART: Multilingual and Multimodal Encoder-Decoder Pre-Training for Any-to-Any Machine Translation", "link": "https://ieeexplore.ieee.org/abstract/document/10446620/", "details": "PJ Chen, B Shi, K Niu, A Lee, WN Hsu - \u2026 2024-2024 IEEE International Conference on \u2026, 2024", "abstract": "Speech and language models are advancing towards universality. A single model can now handle translations across 200 languages and transcriptions for over 100 languages. Universal models simplify development, deployment, and importantly \u2026"}, {"title": "Pre-training Small Base LMs with Fewer Tokens", "link": "https://arxiv.org/pdf/2404.08634", "details": "S Sanyal, S Sanghavi, AG Dimakis - arXiv preprint arXiv:2404.08634, 2024", "abstract": "We study the effectiveness of a simple approach to develop a small base language model (LM) starting from an existing large base LM: first inherit a few transformer blocks from the larger LM, and then train this smaller model on a very small subset \u2026"}, {"title": "SUBTOPIC-ORIENTED BIOMEDICAL SUMMARIZATION USING PRETRAINED LANGUAGE MODELS", "link": "https://amslaurea.unibo.it/29686/1/thesis_xia.pdf", "details": "D MONTESI, TC XIA", "abstract": "The ever-growing number of publications in the biomedical field is causing difficulties in finding insightful knowledge. In this work, we propose a subtopic-oriented summarization framework that aims to provide an overview on the state-of-theart of a \u2026"}]
