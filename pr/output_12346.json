[{"title": "InnerThoughts: Disentangling Representations and Predictions in Large Language Models", "link": "https://arxiv.org/pdf/2501.17994", "details": "D Ch\u00e9telat, J Cotnareanu, R Thompson, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) contain substantial factual knowledge which is commonly elicited by multiple-choice question-answering prompts. Internally, such models process the prompt through multiple transformer layers, building varying \u2026"}, {"title": "EHealth: A Chinese Biomedical Language Model Built via Multi-Level Text Discrimination", "link": "https://ieeexplore.ieee.org/abstract/document/10857372/", "details": "Q Wang, S Dai, B Xu, Y Lyu, H Wu, H Wang - IEEE Transactions on Audio, Speech \u2026, 2025", "abstract": "Pre-trained language models (PLMs) have recently revolutionized the field of natural language processing, impacting not only the general domain but also the biomedical domain. Most previous studies on constructing biomedical PLMs relied simply on \u2026"}]
