[{"title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures", "link": "https://arxiv.org/pdf/2410.07698", "details": "Y Chen, Y Zhang, L Cao, K Yuan, Z Wen - arXiv preprint arXiv:2410.07698, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs when adapting large language models (LLMs) for downstream applications. However, traditional first-order (FO) fine-tuning algorithms incur substantial memory overhead \u2026"}, {"title": "CREAM: Consistency Regularized Self-Rewarding Language Models", "link": "https://arxiv.org/pdf/2410.12735%3F", "details": "Z Wang, W He, Z Liang, X Zhang, C Bansal, Y Wei\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent self-rewarding large language models (LLM) have successfully applied LLM- as-a-Judge to iteratively improve the alignment performance without the need of human annotations for preference data. These methods commonly utilize the same \u2026"}, {"title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style", "link": "https://arxiv.org/pdf/2410.16184%3F", "details": "Y Liu, Z Yao, R Min, Y Cao, L Hou, J Li - arXiv preprint arXiv:2410.16184, 2024", "abstract": "Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward \u2026"}, {"title": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement", "link": "https://arxiv.org/pdf/2410.02108", "details": "X Peng, C Xia, X Yang, C Xiong, CS Wu, C Xing - arXiv preprint arXiv:2410.02108, 2024", "abstract": "Post-training Large Language Models (LLMs) with explicit reasoning trajectories can enhance their reasoning abilities. However, acquiring such high-quality trajectory data typically demands meticulous supervision from humans or superior models \u2026"}, {"title": "POSIX: A Prompt Sensitivity Index For Large Language Models", "link": "https://arxiv.org/pdf/2410.02185", "details": "A Chatterjee, HK Renduchintala, S Bhatia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are found to be surprisingly sensitive to minor variations in prompts, often generating significantly divergent outputs in response to minor variations in the prompts, such as spelling \u2026"}, {"title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs", "link": "https://arxiv.org/pdf/2410.12881%3F", "details": "SN Akter, S Prabhumoye, J Kamalu, S Satheesh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models (LLMs). Yet, these approaches fall inadequate in complex, multi-hop and \u2026"}, {"title": "Beyond Graphs: Can Large Language Models Comprehend Hypergraphs?", "link": "https://arxiv.org/pdf/2410.10083", "details": "Y Feng, C Yang, X Hou, S Du, S Ying, Z Wu, Y Gao - arXiv preprint arXiv:2410.10083, 2024", "abstract": "Existing benchmarks like NLGraph and GraphQA evaluate LLMs on graphs by focusing mainly on pairwise relationships, overlooking the high-order correlations found in real-world data. Hypergraphs, which can model complex beyond-pairwise \u2026"}, {"title": "Ada-K Routing: Boosting the Efficiency of MoE-based LLMs", "link": "https://arxiv.org/pdf/2410.10456", "details": "T Yue, L Guo, J Cheng, X Gao, J Liu - arXiv preprint arXiv:2410.10456, 2024", "abstract": "In the era of Large Language Models (LLMs), Mixture-of-Experts (MoE) architectures offer a promising approach to managing computational costs while scaling up model parameters. Conventional MoE-based LLMs typically employ static Top-K routing \u2026"}, {"title": "Understanding Reasoning in Chain-of-Thought from the Hopfieldian View", "link": "https://arxiv.org/pdf/2410.03595", "details": "L Hu, L Liu, S Yang, X Chen, Z Tan, MA Ali, M Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models have demonstrated remarkable abilities across various tasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to enhance reasoning capabilities. However, existing research primarily focuses on \u2026"}]
