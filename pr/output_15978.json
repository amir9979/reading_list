[{"title": "Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2504.13945", "details": "Z Wu, T Song, N Xie, W Zhang, M Zhu, S Wu, S Sun\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The rapid advancement of large vision-language models (LVLMs) has significantly propelled applications in document understanding, particularly in optical character recognition (OCR) and multilingual translation. However, current evaluations of \u2026"}, {"title": "SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data", "link": "https://arxiv.org/pdf/2504.20648", "details": "M Ogezi, F Shi - arXiv preprint arXiv:2504.20648, 2025", "abstract": "Vision-language models (VLMs) work well in tasks ranging from image captioning to visual question answering (VQA), yet they struggle with spatial reasoning, a key skill for understanding our physical world that humans excel at. We find that spatial \u2026"}, {"title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models", "link": "https://arxiv.org/pdf/2504.14395", "details": "B Jalaian, ND Bastian - arXiv preprint arXiv:2504.14395, 2025", "abstract": "To develop trustworthy Vision-Language Models (VLMs), it is essential to address adversarial robustness and hallucination mitigation, both of which impact factual accuracy in high-stakes applications such as defense and healthcare. Existing \u2026"}, {"title": "Leveraging long context in retrieval augmented language models for medical question answering", "link": "https://www.nature.com/articles/s41746-025-01651-w", "details": "G Zhang, Z Xu, Q Jin, F Chen, Y Fang, Y Liu\u2026 - npj Digital Medicine, 2025", "abstract": "While holding great promise for improving and facilitating healthcare through applications of medical literature summarization, large language models (LLMs) struggle to produce up-to-date responses on evolving topics due to outdated \u2026"}, {"title": "FATE: Feature-Adapted Parameter Tuning for Vision-Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/32975/35130", "details": "Z Xu, Z Peng, X Yang, W Shen - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Following the recent popularity of vision language models, several attempts, eg, parameter-efficient fine-tuning (PEFT), have been made to extend them to different downstream tasks. Previous PEFT works motivate their methods from the view of \u2026"}, {"title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "link": "https://arxiv.org/pdf/2504.14366", "details": "P Haller, J Golde, A Akbik - arXiv preprint arXiv:2504.14366, 2025", "abstract": "Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures \u2026"}, {"title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "link": "https://arxiv.org/pdf/2504.14194", "details": "X Zhuang, J Peng, R Ma, Y Wang, T Bai, X Wei, J Qiu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural \u2026"}, {"title": "Exploring Multimodal Language Models for Sustainability Disclosure Extraction: A Comparative Study", "link": "https://aclanthology.org/2025.insights-1.13.pdf", "details": "T Gupta, T Goel, I Verma - The Sixth Workshop on Insights from Negative Results \u2026, 2025", "abstract": "Sustainability metrics have increasingly become a crucial non-financial criterion in investment decision-making. Organizations worldwide are recognizing the importance of sustainability and are proactively highlighting their efforts through \u2026"}, {"title": "Investigating Zero-Shot Diagnostic Pathology in Vision-Language Models with Efficient Prompt Design", "link": "https://arxiv.org/pdf/2505.00134", "details": "V Sharma, A Alagha, A Khellaf, VQH Trinh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) have gained significant attention in computational pathology due to their multimodal learning capabilities that enhance big-data analytics of giga-pixel whole slide image (WSI). However, their sensitivity to large \u2026"}]
