[{"title": "Infusing Multi-Hop Medical Knowledge Into Smaller Language Models for Biomedical Question Answering", "link": "https://ieeexplore.ieee.org/abstract/document/10932873/", "details": "J Chen, Z Wei, W Shen, R Shang - IEEE Journal of Biomedical and Health Informatics, 2025", "abstract": "MedQA-USMLE is a challenging biomedical question answering (BQA) task, as its questions typically involve multi-hop reasoning. To solve this task, BQA systems should possess substantial medical professional knowledge and strong medical \u2026"}, {"title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis", "link": "https://arxiv.org/pdf/2504.10352", "details": "Y Yang, S Liu, J Li, Y Hu, H Wu, H Wang, J Yu, L Meng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and \u2026"}, {"title": "Guiding Reasoning in Small Language Models with LLM Assistance", "link": "https://arxiv.org/pdf/2504.09923", "details": "Y Kim, E Yi, M Kim, SY Yun, T Kim - arXiv preprint arXiv:2504.09923, 2025", "abstract": "The limited reasoning capabilities of small language models (SLMs) cast doubt on their suitability for tasks demanding deep, multi-step logical deduction. This paper introduces a framework called Small Reasons, Large Hints (SMART), which \u2026"}, {"title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models", "link": "https://arxiv.org/pdf/2504.05258", "details": "A Bazaga, R Blloshmi, B Byrne, A de Gispert - arXiv preprint arXiv:2504.05258, 2025", "abstract": "Large Language Models (LLMs) have emerged as powerful tools for generating coherent text, understanding context, and performing reasoning tasks. However, they struggle with temporal reasoning, which requires processing time-related information \u2026"}, {"title": "Poisson-Process Topic Model for Integrating Knowledge from Pre-trained Language Models", "link": "https://arxiv.org/pdf/2503.17809%3F", "details": "M Austern, Y Guo, ZT Ke, T Liu - arXiv preprint arXiv:2503.17809, 2025", "abstract": "Topic modeling is traditionally applied to word counts without accounting for the context in which words appear. Recent advancements in large language models (LLMs) offer contextualized word embeddings, which capture deeper meaning and \u2026"}, {"title": "Learning to Erase Private Knowledge from Multi-Documents for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2504.09910", "details": "Y Wang, H Zhang, L Pang, Y Tong, B Guo, H Zheng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-Augmented Generation (RAG) is a promising technique for applying LLMs to proprietary domains. However, retrieved documents may contain sensitive knowledge, posing risks of privacy leakage in generative results. Thus, effectively \u2026"}, {"title": "Probing the Symbolic Logical Reasoning Ability of Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3729238", "details": "J Ji, Z Li, S Xu, W Hua, J Tan, H Gong, Y Zhang - ACM Transactions on Intelligent Systems \u2026", "abstract": "Large Language Models (LLMs) have achieved significant successes in various research domains by learning the relationship between words. However, while these models are capable of making predictions and inferences based on the learned \u2026"}, {"title": "Window Token Concatenation for Efficient Visual Large Language Models", "link": "https://arxiv.org/pdf/2504.04024", "details": "Y Li, W Bao, B Ye, Z Tan, T Chen, H Liu, Y Kong - arXiv preprint arXiv:2504.04024, 2025", "abstract": "To effectively reduce the visual tokens in Visual Large Language Models (VLLMs), we propose a novel approach called Window Token Concatenation (WiCo). Specifically, we employ a sliding window to concatenate spatially adjacent visual \u2026"}, {"title": "AD-GPT: Large Language Models in Alzheimer's Disease", "link": "https://arxiv.org/pdf/2504.03071", "details": "Z Liu, L Tang, Z Sun, Z Liu, Y Lyu, W Ruan, Y Xu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have emerged as powerful tools for medical information retrieval, yet their accuracy and depth remain limited in specialized domains such as Alzheimer's disease (AD), a growing global health challenge. To \u2026"}]
