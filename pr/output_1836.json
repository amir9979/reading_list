[{"title": "Adaptive Reinforcement Tuning Language Models as Hard Data Generators for Sentence Representation", "link": "https://aclanthology.org/2024.lrec-main.33.pdf", "details": "B Xu, Y Wu, S Wei, M Du, H Wang - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Sentence representation learning is a fundamental task in NLP. Existing methods use contrastive learning (CL) to learn effective sentence representations, which benefit from high-quality contrastive data but require extensive human annotation \u2026"}, {"title": "WaterPool: A Watermark Mitigating Trade-offs among Imperceptibility, Efficacy and Robustness", "link": "https://arxiv.org/pdf/2405.13517", "details": "B Huang, X Wan - arXiv preprint arXiv:2405.13517, 2024", "abstract": "With the increasing use of large language models (LLMs) in daily life, concerns have emerged regarding their potential misuse and societal impact. Watermarking is proposed to trace the usage of specific models by injecting patterns into their \u2026"}, {"title": "Improving Language Models Trained with Translated Data via Continual Pre-Training and Dictionary Learning Analysis", "link": "https://arxiv.org/pdf/2405.14277", "details": "S Boughorbel, MD Parvez, M Hawasly - arXiv preprint arXiv:2405.14277, 2024", "abstract": "Training LLMs in low resources languages usually utilizes data augmentation with machine translation (MT) from English language. However, translation brings a number of challenges: there are large costs attached to translating and curating huge \u2026"}, {"title": "Lessons from the Trenches on Reproducible Evaluation of Language Models", "link": "https://arxiv.org/pdf/2405.14782", "details": "S Biderman, H Schoelkopf, L Sutawika, L Gao, J Tow\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Effective evaluation of language models remains an open challenge in NLP. Researchers and engineers face methodological issues such as the sensitivity of models to evaluation setup, difficulty of proper comparisons across methods, and the \u2026"}, {"title": "Watermarking Generative Tabular Data", "link": "https://arxiv.org/pdf/2405.14018", "details": "H He, P Yu, J Ren, YN Wu, G Cheng - arXiv preprint arXiv:2405.14018, 2024", "abstract": "In this paper, we introduce a simple yet effective tabular data watermarking mechanism with statistical guarantees. We show theoretically that the proposed watermark can be effectively detected, while faithfully preserving the data fidelity, and \u2026"}, {"title": "Towards Comprehensive and Efficient Post Safety Alignment of Large Language Models via Safety Patching", "link": "https://arxiv.org/pdf/2405.13820", "details": "W Zhao, Y Hu, Z Li, Y Deng, Y Zhao, B Qin, TS Chua - arXiv preprint arXiv \u2026, 2024", "abstract": "Safety alignment of large language models (LLMs) has been gaining increasing attention. However, current safety-aligned LLMs suffer from the fragile and imbalanced safety mechanisms, which can still be induced to generate unsafe \u2026"}, {"title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models", "link": "https://arxiv.org/pdf/2405.14366", "details": "A Liu, J Liu, Z Pan, Y He, G Haffari, B Zhuang - arXiv preprint arXiv:2405.14366, 2024", "abstract": "A critical approach for efficiently deploying computationally demanding large language models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive \u2026"}, {"title": "Unveiling the Achilles' Heel of NLG Evaluators: A Unified Adversarial Framework Driven by Large Language Models", "link": "https://arxiv.org/pdf/2405.14646", "details": "Y Chen, C Zhang, D Luo, LF D'Haro, RT Tan, H Li - arXiv preprint arXiv:2405.14646, 2024", "abstract": "The automatic evaluation of natural language generation (NLG) systems presents a long-lasting challenge. Recent studies have highlighted various neural metrics that align well with human evaluations. Yet, the robustness of these evaluators against \u2026"}, {"title": "Large Language Models Can Self-Correct with Minimal Effort", "link": "https://arxiv.org/pdf/2405.14092", "details": "Z Wu, Q Zeng, Z Zhang, Z Tan, C Shen, M Jiang - arXiv preprint arXiv:2405.14092, 2024", "abstract": "Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple \u2026"}]
