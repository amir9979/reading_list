[{"title": "Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports", "link": "https://arxiv.org/pdf/2505.16624", "details": "FD Serra, P Schrempf, C Wang, Z Meng, F Deligianni\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present a novel approach to Chest X-ray (CXR) Visual Question Answering (VQA), addressing both single-image image-difference questions. Single-image questions focus on abnormalities within a specific CXR (\" What abnormalities are \u2026", "entry_id": "http://arxiv.org/abs/2505.16624v1", "updated": "2025-05-22 12:57:35", "published": "2025-05-22 12:57:35", "authors": "Francesco Dalla Serra;Patrick Schrempf;Chaoyang Wang;Zaiqiao Meng;Fani Deligianni;Alison Q. O'Neil", "summary": "We present a novel approach to Chest X-ray (CXR) Visual Question Answering\n(VQA), addressing both single-image image-difference questions. Single-image\nquestions focus on abnormalities within a specific CXR (\"What abnormalities are\nseen in image X?\"), while image-difference questions compare two longitudinal\nCXRs acquired at different time points (\"What are the differences between image\nX and Y?\"). We further explore how the integration of radiology reports can\nenhance the performance of VQA models. While previous approaches have\ndemonstrated the utility of radiology reports during the pre-training phase, we\nextend this idea by showing that the reports can also be leveraged as\nadditional input to improve the VQA model's predicted answers. First, we\npropose a unified method that handles both types of questions and\nauto-regressively generates the answers. For single-image questions, the model\nis provided with a single CXR. For image-difference questions, the model is\nprovided with two CXRs from the same patient, captured at different time\npoints, enabling the model to detect and describe temporal changes. Taking\ninspiration from 'Chain-of-Thought reasoning', we demonstrate that performance\non the CXR VQA task can be improved by grounding the answer generator module\nwith a radiology report predicted for the same CXR. In our approach, the VQA\nmodel is divided into two steps: i) Report Generation (RG) and ii) Answer\nGeneration (AG). Our results demonstrate that incorporating predicted radiology\nreports as evidence to the AG model enhances performance on both single-image\nand image-difference questions, achieving state-of-the-art results on the\nMedical-Diff-VQA dataset.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.CL", "links": "http://arxiv.org/abs/2505.16624v1;http://arxiv.org/pdf/2505.16624v1", "pdf_url": "http://arxiv.org/pdf/2505.16624v1"}, {"title": "Enhancing Boundary Accuracy in Semantic Segmentation of Chest X-Ray Images Using Gaussian Process Regression", "link": "https://iopscience.iop.org/article/10.1088/2057-1976/addbe9/meta", "details": "B Aljaddouh, D D. Malathi - Biomedical Physics & Engineering Express, 2025", "abstract": "This research aims to enhance X-ray lung segmentation by addressing boundary distortions in anatomical structures, with the objective of refining segmentation boundaries and improving the morphological shape of segmented objects. The \u2026"}]
