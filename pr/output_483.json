'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Smaller Language Models are Better Zero-shot Machine-G'
[{"title": "Can 3D Vision-Language Models Truly Understand Natural Language?", "link": "https://arxiv.org/pdf/2403.14760", "details": "W Deng, R Ding, J Yang, J Liu, Y Li, X Qi, E Ngai - arXiv preprint arXiv:2403.14760, 2024", "abstract": "Rapid advancements in 3D vision-language (3D-VL) tasks have opened up new avenues for human interaction with embodied agents or robots using natural language. Despite this progress, we find a notable limitation: existing 3D-VL models \u2026"}, {"title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models", "link": "https://arxiv.org/pdf/2403.08281", "details": "N Ding, Y Chen, G Cui, X Lv, R Xie, B Zhou, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three \u2026"}, {"title": "Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts", "link": "https://arxiv.org/html/2403.12918v1", "details": "SA Somayajula, Y Liang, A Singh, L Zhang, P Xie - arXiv preprint arXiv:2403.12918, 2024", "abstract": "Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle \u2026"}, {"title": "Adaptive Prompt Routing for Arbitrary Text Style Transfer with Pre-trained Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29832/31446", "details": "Q Liu, J Qin, W Ye, H Mou, Y He, K Wang - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Recently, arbitrary text style transfer (TST) has made significant progress with the paradigm of prompt learning. In this paradigm, researchers often design or search for a fixed prompt for any input. However, existing evidence shows that large language \u2026"}, {"title": "Do Not Worry if You Do Not Have Data: Building Pretrained Language Models Using Translationese", "link": "https://arxiv.org/pdf/2403.13638", "details": "M Doshi, R Dabre, P Bhattacharyya - arXiv preprint arXiv:2403.13638, 2024", "abstract": "In this paper, we explore the utility of\\textit {Translationese} as synthetic data created using machine translation for pre-training language models (LMs). Pre-training requires vast amounts of monolingual data, which is mostly unavailable for \u2026"}, {"title": "Does Data Contamination Make a Difference? Insights from Intentionally Contamination Pre-training Data For Language Models", "link": "https://openreview.net/pdf%3Fid%3DnLtl8JNOxg", "details": "M Jiang, K Liu, M Zhong, R Schaeffer, S Ouyang, J Han\u2026 - ICLR 2024 Workshop on \u2026", "abstract": "Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the \u2026"}, {"title": "Question-answering system extracts information on injection drug use from clinical notes", "link": "https://www.nature.com/articles/s43856-024-00470-6", "details": "M Mahbub, I Goethert, I Danciu, K Knight, S Srinivasan\u2026 - Communications Medicine, 2024", "abstract": "Background Injection drug use (IDU) can increase mortality and morbidity. Therefore, identifying IDU early and initiating harm reduction interventions can benefit individuals at risk. However, extracting IDU behaviors from patients' electronic health \u2026"}, {"title": "Adolescents' reasons for accessing their health records online, perceived usefulness and experienced provider encouragement: a national survey in Sweden", "link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10928755/", "details": "J Hagstr\u00f6m, C Blease, I Scandurra, J Moll, \u00c5 Cajander\u2026 - BMJ Paediatrics Open, 2024", "abstract": "Background Having online access to electronic health records (EHRs) may help patients become engaged in their care at an early age. However, little is known about adolescents using patient portals. A national survey conducted within the \u2026"}, {"title": "DCS: Debiased Contrastive Learning with Weak Supervision for Time Series Classification", "link": "https://ieeexplore.ieee.org/abstract/document/10446381/", "details": "R Cai, L Peng, Z Lu, K Zhang, Y Liu - \u2026 2024-2024 IEEE International Conference on \u2026, 2024", "abstract": "Self-supervised contrastive learning (SSCL) has performed excellently on time series classification tasks. Most SSCL-based classification algorithms generate positive and negative samples in the time or frequency domains, focusing on mining similarities \u2026"}]
