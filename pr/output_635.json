'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Envisioning MedCLIP: A Deep Dive into Explainability f'
[{"title": "Adaptive Prompt Routing for Arbitrary Text Style Transfer with Pre-trained Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29832/31446", "details": "Q Liu, J Qin, W Ye, H Mou, Y He, K Wang - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Recently, arbitrary text style transfer (TST) has made significant progress with the paradigm of prompt learning. In this paradigm, researchers often design or search for a fixed prompt for any input. However, existing evidence shows that large language \u2026"}, {"title": "Emergent Abilities in Reduced-Scale Generative Language Models", "link": "https://arxiv.org/pdf/2404.02204", "details": "S Muckatira, V Deshpande, V Lialin, A Rumshisky - arXiv preprint arXiv:2404.02204, 2024", "abstract": "Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study \u2026"}, {"title": "Fine-Tuning Language Models with Reward Learning on Policy", "link": "https://arxiv.org/pdf/2403.19279", "details": "H Lang, F Huang, Y Li - arXiv preprint arXiv:2403.19279, 2024", "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences. RLHF contains three steps, ie, human preference collecting, reward learning, and policy \u2026"}, {"title": "Investigating Regularization of Self-Play Language Models", "link": "https://arxiv.org/pdf/2404.04291", "details": "R Alami, A Abubaker, M Achab, MEA Seddik, S Lahlou - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper explores the effects of various forms of regularization in the context of language model alignment via self-play. While both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) require to collect \u2026"}, {"title": "LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation", "link": "https://arxiv.org/pdf/2403.12019", "details": "Y Lan, F Hong, S Yang, S Zhou, X Meng, B Dai, X Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The field of neural rendering has witnessed significant progress with advancements in generative models and differentiable rendering techniques. Though 2D diffusion has achieved success, a unified 3D diffusion pipeline remains unsettled. This paper \u2026"}, {"title": "CRS-Diff: Controllable Generative Remote Sensing Foundation Model", "link": "https://arxiv.org/pdf/2403.11614", "details": "D Tang, X Cao, X Hou, Z Jiang, D Meng - arXiv preprint arXiv:2403.11614, 2024", "abstract": "The emergence of diffusion models has revolutionized the field of image generation, providing new methods for creating high-quality, high-resolution images across various applications. However, the potential of these models for generating domain \u2026"}, {"title": "Pure Vision Transformer (CT-ViT) with Noise2Neighbors Interpolation for Low-Dose CT Image Denoising", "link": "https://link.springer.com/article/10.1007/s10278-024-01108-8", "details": "L Marcos, P Babyn, J Alirezaie - Journal of Imaging Informatics in Medicine, 2024", "abstract": "Convolutional neural networks (CNN) have been used for a wide variety of deep learning applications, especially in computer vision. For medical image processing, researchers have identified certain challenges associated with CNNs. These \u2026"}, {"title": "Photo-Realistic Image Restoration in the Wild with Controlled Vision-Language Models", "link": "https://arxiv.org/pdf/2404.09732", "details": "Z Luo, FK Gustafsson, Z Zhao, J Sj\u00f6lund, TB Sch\u00f6n - arXiv preprint arXiv:2404.09732, 2024", "abstract": "Though diffusion models have been successfully applied to various image restoration (IR) tasks, their performance is sensitive to the choice of training datasets. Typically, diffusion models trained in specific datasets fail to recover images that \u2026"}, {"title": "Harnessing the Power of Large Vision Language Models for Synthetic Image Detection", "link": "https://arxiv.org/pdf/2404.02726", "details": "M Keita, W Hamidouche, H Bougueffa, A Hadid\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, the emergence of models capable of generating images from text has attracted considerable interest, offering the possibility of creating realistic images from text descriptions. Yet these advances have also raised concerns about the \u2026"}]
