[{"title": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering", "link": "https://arxiv.org/pdf/2506.09645", "details": "T Yao, H Li, Z Shen, P Li, T Liu, K Zhang - arXiv preprint arXiv:2506.09645, 2025", "abstract": "Large Language Models (LLMs) have shown strong inductive reasoning ability across various domains, but their reliability is hindered by the outdated knowledge and hallucinations. Retrieval-Augmented Generation mitigates these issues by \u2026", "entry_id": "http://arxiv.org/abs/2506.09645v1", "updated": "2025-06-11 12:03:52", "published": "2025-06-11 12:03:52", "authors": "Tianjun Yao;Haoxuan Li;Zhiqiang Shen;Pan Li;Tongliang Liu;Kun Zhang", "summary": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL.", "comment": "32 pages, 28 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.IR;cs.LG;I.2.6", "links": "http://arxiv.org/abs/2506.09645v1;http://arxiv.org/pdf/2506.09645v1", "pdf_url": "http://arxiv.org/pdf/2506.09645v1"}, {"title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models", "link": "https://arxiv.org/pdf/2506.07468", "details": "M Liu, L Jiang, Y Liang, SS Du, Y Choi, T Althoff\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch--attackers \u2026", "entry_id": "http://arxiv.org/abs/2506.07468v1", "updated": "2025-06-09 06:35:12", "published": "2025-06-09 06:35:12", "authors": "Mickel Liu;Liwei Jiang;Yancheng Liang;Simon Shaolei Du;Yejin Choi;Tim Althoff;Natasha Jaques", "summary": "Conventional language model (LM) safety alignment relies on a reactive,\ndisjoint procedure: attackers exploit a static model, followed by defensive\nfine-tuning to patch exposed vulnerabilities. This sequential approach creates\na mismatch -- attackers overfit to obsolete defenses, while defenders\nperpetually lag behind emerging threats. To address this, we propose\nSelf-RedTeam, an online self-play reinforcement learning algorithm where an\nattacker and defender agent co-evolve through continuous interaction. We cast\nsafety alignment as a two-player zero-sum game, where a single model alternates\nbetween attacker and defender roles -- generating adversarial prompts and\nsafeguarding against them -- while a reward LM adjudicates outcomes. This\nenables dynamic co-adaptation. Grounded in the game-theoretic framework of\nzero-sum games, we establish a theoretical safety guarantee which motivates the\ndesign of our method: if self-play converges to a Nash Equilibrium, the\ndefender will reliably produce safe responses to any adversarial input.\nEmpirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared\nto attackers trained against static defenders and achieves higher robustness on\nsafety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained\nagainst static attackers. We further propose hidden Chain-of-Thought, allowing\nagents to plan privately, which boosts adversarial diversity and reduces\nover-refusals. Our results motivate a shift from reactive patching to proactive\nco-evolution in LM safety training, enabling scalable, autonomous, and robust\nself-improvement of LMs via multi-agent reinforcement learning (MARL).", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CL;cs.MA", "links": "http://arxiv.org/abs/2506.07468v1;http://arxiv.org/pdf/2506.07468v1", "pdf_url": "http://arxiv.org/pdf/2506.07468v1"}, {"title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models", "link": "https://arxiv.org/pdf/2506.06395", "details": "P Li, M Skripkin, A Zubrey, A Kuznetsov, I Oseledets - arXiv preprint arXiv:2506.06395, 2025", "abstract": "Large language models (LLMs) excel at reasoning, yet post-training remains critical for aligning their behavior with task goals. Existing reinforcement learning (RL) methods often depend on costly human annotations or external reward models. We \u2026", "entry_id": "http://arxiv.org/abs/2506.06395v3", "updated": "2025-06-11 06:21:59", "published": "2025-06-05 19:55:15", "authors": "Pengyi Li;Matvey Skripkin;Alexander Zubrey;Andrey Kuznetsov;Ivan Oseledets", "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,\nRLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on\nMinerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a\nsimple, scalable post-training method for inference models, requiring only a\nsmall number of samples and unlabelled supervision.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2506.06395v3;http://arxiv.org/pdf/2506.06395v3", "pdf_url": "http://arxiv.org/pdf/2506.06395v3"}, {"title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "link": "https://arxiv.org/pdf/2506.09965", "details": "J Wu, J Guan, K Feng, Q Liu, S Wu, L Wang, W Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As textual reasoning with large language models (LLMs) has advanced significantly, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs). However, existing methods primarily \u2026", "entry_id": "http://arxiv.org/abs/2506.09965v1", "updated": "2025-06-11 17:41:50", "published": "2025-06-11 17:41:50", "authors": "Junfei Wu;Jian Guan;Kaituo Feng;Qiang Liu;Shu Wu;Liang Wang;Wei Wu;Tieniu Tan", "summary": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI;I.2", "links": "http://arxiv.org/abs/2506.09965v1;http://arxiv.org/pdf/2506.09965v1", "pdf_url": "http://arxiv.org/pdf/2506.09965v1"}, {"title": "Revisiting Backdoor Attacks against Large Vision-Language Models from Domain Shift", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liang_Revisiting_Backdoor_Attacks_against_Large_Vision-Language_Models_from_Domain_Shift_CVPR_2025_paper.pdf", "details": "S Liang, J Liang, T Pang, C Du, A Liu, M Zhu, X Cao\u2026 - Proceedings of the \u2026, 2025", "abstract": "Instruction tuning enhances large vision-language models (LVLMs) but increases their vulnerability to backdoor attacks due to their open design. Unlike prior studies in static settings, this paper explores backdoor attacks in LVLM instruction tuning \u2026"}, {"title": "SLADE: Shielding against Dual Exploits in Large Vision-Language Models", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Hossain_SLADE_Shielding_against_Dual_Exploits_in_Large_Vision-Language_Models_CVPR_2025_paper.pdf", "details": "MZ Hossain, A Imteaj - Proceedings of the Computer Vision and Pattern \u2026, 2025", "abstract": "Abstract Large Vision-Language Models (LVLMs) have emerged as transformative tools in multimodal tasks, seamlessly integrating pretrained vision encoders to align visual and textual modalities. Prior works have highlighted the susceptibility of \u2026"}, {"title": "RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics", "link": "https://arxiv.org/pdf/2506.04308", "details": "E Zhou, J An, C Chi, Y Han, S Rong, C Zhang, P Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the \u2026", "entry_id": "http://arxiv.org/abs/2506.04308v1", "updated": "2025-06-04 17:59:27", "published": "2025-06-04 17:59:27", "authors": "Enshen Zhou;Jingkun An;Cheng Chi;Yi Han;Shanyu Rong;Chi Zhang;Pengwei Wang;Zhongyuan Wang;Tiejun Huang;Lu Sheng;Shanghang Zhang", "summary": "Spatial referring is a fundamental capability of embodied robots to interact\nwith the 3D physical world. However, even with the powerful pretrained vision\nlanguage models (VLMs), recent approaches are still not qualified to accurately\nunderstand the complex 3D scenes and dynamically reason about the\ninstruction-indicated locations for interaction. To this end, we propose\nRoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding\nby integrating a disentangled but dedicated depth encoder via supervised\nfine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial\nreasoning via reinforcement fine-tuning (RFT), with metric-sensitive process\nreward functions tailored for spatial referring tasks. To support SFT and RFT\ntraining, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x\nprior), covering 31 spatial relations (vs. 15 prior) and supporting complex\nreasoning processes (up to 5 steps). In addition, we introduce\nRefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial\nreferring with multi-step reasoning. Experiments show that SFT-trained\nRoboRefer achieves state-of-the-art spatial understanding, with an average\nsuccess rate of 89.6%. RFT-trained RoboRefer further outperforms all other\nbaselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average\naccuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various\ncontrol policies to execute long-horizon, dynamic tasks across diverse robots\n(e,g., UR5, G1 humanoid) in cluttered real-world scenes.", "comment": "Project page: https://zhoues.github.io/RoboRefer/", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2506.04308v1;http://arxiv.org/pdf/2506.04308v1", "pdf_url": "http://arxiv.org/pdf/2506.04308v1"}, {"title": "Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models", "link": "https://arxiv.org/pdf/2506.02557", "details": "S Gong, Y Jiang, Q Dou, F Farnia - arXiv preprint arXiv:2506.02557, 2025", "abstract": "Vision-language models, such as CLIP, have achieved significant success in aligning visual and textual representations, becoming essential components of many multi-modal large language models (MLLMs) like LLaVA and OpenFlamingo \u2026", "entry_id": "http://arxiv.org/abs/2506.02557v1", "updated": "2025-06-03 07:44:43", "published": "2025-06-03 07:44:43", "authors": "Shizhan Gong;Yankai Jiang;Qi Dou;Farzan Farnia", "summary": "Vision-language models, such as CLIP, have achieved significant success in\naligning visual and textual representations, becoming essential components of\nmany multi-modal large language models (MLLMs) like LLaVA and OpenFlamingo.\nHowever, numerous studies have identified CLIP's limited fine-grained\nperception as a critical drawback, leading to substantial failures in\ndownstream MLLMs. In contrast, vision-centric foundation models like DINOv2\ndemonstrate remarkable capabilities in capturing fine details from images. In\nthis work, we propose a novel kernel-based method to align CLIP's visual\nrepresentation with that of DINOv2, ensuring that the resulting embeddings\nmaintain compatibility with text embeddings while enhancing perceptual\ncapabilities. Our alignment objective is designed for efficient stochastic\noptimization. Following this image-only alignment fine-tuning, the visual\nencoder retains compatibility with the frozen text encoder and exhibits\nsignificant improvements in zero-shot object recognition, fine-grained spatial\nreasoning, and localization. By integrating the aligned visual encoder,\ndownstream MLLMs also demonstrate enhanced performance.", "comment": "ICML 2025", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.02557v1;http://arxiv.org/pdf/2506.02557v1", "pdf_url": "http://arxiv.org/pdf/2506.02557v1"}, {"title": "Unsupervised Elicitation of Language Models", "link": "https://arxiv.org/pdf/2506.10139", "details": "J Wen, Z Ankner, A Somani, P Hase, S Marks\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "To steer pretrained language models for downstream tasks, today's post-training paradigm relies on humans to specify desired behaviors. However, for models with superhuman capabilities, it is difficult or impossible to get high-quality human \u2026", "entry_id": "http://arxiv.org/abs/2506.10139v1", "updated": "2025-06-11 19:40:08", "published": "2025-06-11 19:40:08", "authors": "Jiaxin Wen;Zachary Ankner;Arushi Somani;Peter Hase;Samuel Marks;Jacob Goldman-Wetzler;Linda Petrini;Henry Sleight;Collin Burns;He He;Shi Feng;Ethan Perez;Jan Leike", "summary": "To steer pretrained language models for downstream tasks, today's\npost-training paradigm relies on humans to specify desired behaviors. However,\nfor models with superhuman capabilities, it is difficult or impossible to get\nhigh-quality human supervision. To address this challenge, we introduce a new\nunsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune\npretrained language models on their own generated labels, \\emph{without\nexternal supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward\nmodeling tasks, our method matches the performance of training on golden\nsupervision and outperforms training on crowdsourced human supervision. On\ntasks where LMs' capabilities are strongly superhuman, our method can elicit\nthose capabilities significantly better than training on human labels. Finally,\nwe show that our method can improve the training of frontier LMs: we use our\nmethod to train an unsupervised reward model and use reinforcement learning to\ntrain a Claude 3.5 Haiku-based assistant. Both the reward model and the\nassistant outperform their human-supervised counterparts.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.10139v1;http://arxiv.org/pdf/2506.10139v1", "pdf_url": "http://arxiv.org/pdf/2506.10139v1"}]
