[{"title": "TracKGE: Transformer with Relation-pattern Adaptive Contrastive Learning for Knowledge Graph Embedding", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124008529", "details": "M Wang, Z Li, J Wang, W Zou, J Zhou, J Gan - Knowledge-Based Systems, 2024", "abstract": "Abstract Knowledge Graphs, fundamental to intelligent applications, are increasingly critical in various domains, enhancing tasks like precise searching and personalized recommendation. Effectively representing entities and relationships in these graphs \u2026"}, {"title": "Advancing Faithfulness of Large Language Models in Goal-Oriented Dialogue Question Answering", "link": "https://dl.acm.org/doi/abs/10.1145/3640794.3665573", "details": "A Sticha, N Braunschweiler, RS Doddipatla, KM Knill - \u2026 of the 6th ACM Conference on \u2026, 2024", "abstract": "Goal-oriented dialogue systems, such as assistant chatbots and conversational AI systems, have gained prominence for their question-answering capabilities, often utilizing large language models (LLMs) as knowledge bases. However, these \u2026"}, {"title": "Large Language Models for Tabular Data: Progresses and Future Directions", "link": "https://dl.acm.org/doi/pdf/10.1145/3626772.3661384", "details": "H Dong, Z Wang - Proceedings of the 47th International ACM SIGIR \u2026, 2024", "abstract": "Tables contain a significant portion of the world's structured information. The ability to efficiently and accurately understand, process, reason about, analyze, and generate tabular data is critical for achieving Artificial General Intelligence (AGI) systems \u2026"}, {"title": "SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training", "link": "https://arxiv.org/pdf/2407.06654", "details": "N He, W Xiong, H Liu, Y Liao, L Ding, K Zhang, G Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The effectiveness of large language models (LLMs) is often hindered by duplicated data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, which risks the loss of valuable information and \u2026"}, {"title": "Pretraining an Encoder: The BERT Language Model", "link": "https://link.springer.com/chapter/10.1007/978-3-031-57549-5_16", "details": "PM Nugues - \u2026 for Natural Language Processing: Programming with \u2026, 2024", "abstract": "Transformers have the capacity to store a huge quantity of information. This chapter describes how we can pretrain the encoder part of a transformer on very large raw corpora to fit their parameters from word associations. It then describes how to fine \u2026"}, {"title": "Token-Supervised Value Models for Enhancing Mathematical Reasoning Capabilities of Large Language Models", "link": "https://arxiv.org/pdf/2407.12863", "details": "JH Lee, JY Yang, B Heo, D Han, KM Yoo - arXiv preprint arXiv:2407.12863, 2024", "abstract": "Large Language Models (LLMs) have demonstrated impressive problem-solving capabilities in mathematics through step-by-step reasoning chains. However, they are susceptible to reasoning errors that impact the quality of subsequent reasoning \u2026"}, {"title": "Molecule Language Model with Augmented Pairs and Expertise Transfer", "link": "https://arxiv.org/pdf/2407.09043", "details": "N Lee, S Laghuvarapu, C Park, J Sun - arXiv preprint arXiv:2407.09043, 2024", "abstract": "Understanding the molecules and their textual descriptions via molecule language models (MoLM) recently got a surge of interest among researchers. However, unique challenges exist in the field of MoLM due to 1) a limited amount of molecule-text \u2026"}, {"title": "ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts", "link": "https://arxiv.org/pdf/2407.09447", "details": "AF Hardy, H Liu, B Lange, MJ Kochenderfer - arXiv preprint arXiv:2407.09447, 2024", "abstract": "Typical schemes for automated red-teaming large language models (LLMs) focus on discovering prompts that trigger a frozen language model (the defender) to generate toxic text. This often results in the prompting model (the adversary) producing text \u2026"}, {"title": "Exploring the Prompt Space of Large Language Models through Evolutionary Sampling", "link": "https://dl.acm.org/doi/pdf/10.1145/3638529.3654049", "details": "M Saletta, C Ferretti - Proceedings of the Genetic and Evolutionary \u2026, 2024", "abstract": "Large language models (LLMs) are increasingly gaining relevance in every-day life, due to their apparent ability in solving tasks that demand intricate linguistic comprehension. Recent studies state that one of the key points that impact their \u2026"}]
