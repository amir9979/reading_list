[{"title": "Bridge and Hint: Extending Pre-trained Language Models for Long-Range Code", "link": "https://arxiv.org/pdf/2405.11233", "details": "Y Chen, C Gao, Z Yang, H Zhang, Q Liao - arXiv preprint arXiv:2405.11233, 2024", "abstract": "In the field of code intelligence, effectively modeling long-range code poses a significant challenge. Existing pre-trained language models (PLMs) such as UniXcoder have achieved remarkable success, but they still face difficulties with long \u2026"}, {"title": "Natural language processing pipeline to extract prostate cancer-related information from clinical notes", "link": "https://link.springer.com/article/10.1007/s00330-024-10812-6", "details": "H Nakai, G Suman, DA Adamo, PJ Navin\u2026 - European Radiology, 2024", "abstract": "Materials and methods This retrospective study included 23,225 patients who underwent prostate MRI between 2017 and 2022. Cancer risk factors (family history of cancer and digital rectal exam findings), pre-MRI prostate pathology, and \u2026"}, {"title": "LG AI Research & KAIST at EHRSQL 2024: Self-Training Large Language Models with Pseudo-Labeled Unanswerable Questions for a Reliable Text-to-SQL System \u2026", "link": "https://arxiv.org/pdf/2405.11162", "details": "Y Jo, S Lee, M Seo, SJ Hwang, M Lee - arXiv preprint arXiv:2405.11162, 2024", "abstract": "Text-to-SQL models are pivotal for making Electronic Health Records (EHRs) accessible to healthcare professionals without SQL knowledge. With the advancements in large language models, these systems have become more adept at \u2026"}, {"title": "Redefining Information Retrieval of Structured Database via Large Language Models", "link": "https://arxiv.org/pdf/2405.05508", "details": "M Wang, Y Zhang, Q Zhao, J Yang, H Zhang - arXiv preprint arXiv:2405.05508, 2024", "abstract": "Retrieval augmentation is critical when Language Models (LMs) exploit non- parametric knowledge related to the query through external knowledge bases before reasoning. The retrieved information is incorporated into LMs as context alongside \u2026"}, {"title": "A Reinforcement Learning Framework for N-Ary Document-Level Relation Extraction", "link": "https://ieeexplore.ieee.org/abstract/document/10549760/", "details": "C Yuan, R Rossi, A Katz, H Eldardiry - IEEE Transactions on Big Data, 2024", "abstract": "Knowledge Bases (KBs) have become more complex because some facts in KBs include more than two entities. The construction and completion of these KBs require a new relation extraction task to retrieve complex facts from the text. To address this \u2026"}, {"title": "Demonstration Augmentation for Zero-shot In-context Learning", "link": "https://arxiv.org/pdf/2406.01224", "details": "Y Su, Y Tai, Y Ji, J Li, B Yan, M Zhang - arXiv preprint arXiv:2406.01224, 2024", "abstract": "Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates. However, many studies \u2026"}, {"title": "CAM: A cross-lingual adaptation framework for low-resource language speech recognition", "link": "https://www.sciencedirect.com/science/article/pii/S1566253524002847", "details": "Q Hu, Y Zhang, X Zhang, Z Han, X Yu - Information Fusion, 2024", "abstract": "In this paper, a novel cross-lingual adaptation framework called CAM is presented for low-resource language speech recognition (LLSR). It is based on the recent popular adapter method. CAM is achieved by adapting self-supervised speech models \u2026"}, {"title": "Towards Comprehensive and Efficient Post Safety Alignment of Large Language Models via Safety Patching", "link": "https://arxiv.org/pdf/2405.13820", "details": "W Zhao, Y Hu, Z Li, Y Deng, Y Zhao, B Qin, TS Chua - arXiv preprint arXiv \u2026, 2024", "abstract": "Safety alignment of large language models (LLMs) has been gaining increasing attention. However, current safety-aligned LLMs suffer from the fragile and imbalanced safety mechanisms, which can still be induced to generate unsafe \u2026"}, {"title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models", "link": "https://arxiv.org/pdf/2406.00548", "details": "T Liu, H Wang, S Wang, Y Cheng, J Gao - arXiv preprint arXiv:2406.00548, 2024", "abstract": "Large language models (LLMs) have achieved impressive performance on various natural language generation tasks. Nonetheless, they suffer from generating negative and harmful contents that are biased against certain demographic groups \u2026"}]
