[{"title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models", "link": "https://arxiv.org/pdf/2407.11484", "details": "N Chen, Y Wang, Y Deng, J Li - arXiv preprint arXiv:2407.11484, 2024", "abstract": "This survey explores the burgeoning field of role-playing with language models, focusing on their development from early persona-based models to advanced character-driven simulations facilitated by Large Language Models (LLMs). Initially \u2026"}, {"title": "Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization", "link": "https://arxiv.org/pdf/2407.07880", "details": "J Wu, Y Xie, Z Yang, J Wu, J Chen, J Gao, B Ding\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study addresses the challenge of noise in training datasets for Direct Preference Optimization (DPO), a method for aligning Large Language Models (LLMs) with human preferences. We categorize noise into pointwise noise, which includes low \u2026"}, {"title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated", "link": "https://arxiv.org/pdf/2407.10969", "details": "H Wang, S Ma, R Wang, F Wei - arXiv preprint arXiv:2407.10969, 2024", "abstract": "We introduce, Q-Sparse, a simple yet effective approach to training sparsely- activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is \u2026"}, {"title": "Investigating Low-Rank Training in Transformer Language Models: Efficiency and Scaling Analysis", "link": "https://arxiv.org/pdf/2407.09835", "details": "X Wei, S Moalla, R Pascanu, C Gulcehre - arXiv preprint arXiv:2407.09835, 2024", "abstract": "State-of-the-art LLMs often rely on scale with high computational costs, which has sparked a research agenda to reduce parameter counts and costs without significantly impacting performance. Our study focuses on Transformer-based LLMs \u2026"}, {"title": "Efficient Training of Language Models with Compact and Consistent Next Token Distributions", "link": "https://arxiv.org/pdf/2407.02819", "details": "A Sathe, S Sarawagi - arXiv preprint arXiv:2407.02819, 2024", "abstract": "Maximizing the likelihood of the next token is an established, statistically sound objective for pre-training language models. In this paper we show that we can train better models faster by pre-aggregating the corpus with a collapsed $ n $-gram \u2026"}, {"title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2407.11681", "details": "H Cheng, M Zhang, JQ Shi - arXiv preprint arXiv:2407.11681, 2024", "abstract": "As Large Language Models (LLMs) grow dramatically in size, there is an increasing trend in compressing and speeding up these models. Previous studies have highlighted the usefulness of gradients for importance scoring in neural network \u2026"}, {"title": "Building Intelligence Identification System via Large Language Model Watermarking: A Survey and Beyond", "link": "https://arxiv.org/pdf/2407.11100", "details": "X Wang, H Jiang, Y Yu, J Yu, Y Lin, P Yi, Y Wang, Q Yu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Large Language Models (LLMs) are increasingly integrated into diverse industries, posing substantial security risks due to unauthorized replication and misuse. To mitigate these concerns, robust identification mechanisms are widely \u2026"}, {"title": "PAG-LLM: Paraphrase and Aggregate with Large Language Models for Minimizing Intent Classification Errors", "link": "https://dl.acm.org/doi/pdf/10.1145/3626772.3657959", "details": "V Yadav, Z Tang, V Srinivasan - Proceedings of the 47th International ACM SIGIR \u2026, 2024", "abstract": "Large language models (LLM) have achieved remarkable success in natural language generation but lesser focus has been given to their applicability in key tasks such as intent-classification. We show that LLMs like LLaMa can achieve high \u2026"}, {"title": "Igea: a Decoder-Only Language Model for Biomedical Text Generation in Italian", "link": "https://arxiv.org/pdf/2407.06011", "details": "TM Buonocore, S Rancati, E Parimbelli - arXiv preprint arXiv:2407.06011, 2024", "abstract": "The development of domain-specific language models has significantly advanced natural language processing applications in various specialized fields, particularly in biomedicine. However, the focus has largely been on English-language models \u2026"}]
