[{"title": "VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models", "link": "https://arxiv.org/pdf/2412.01822", "details": "BK Lee, R Hachiuma, YCF Wang, YM Ro, YH Wu - arXiv preprint arXiv:2412.01822, 2024", "abstract": "The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve \u2026"}, {"title": "Progressive Multi-granular Alignments for Grounded Reasoning in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.08125", "details": "QH Le, LH Dang, N Le, T Tran, TM Le - arXiv preprint arXiv:2412.08125, 2024", "abstract": "Existing Large Vision-Language Models (LVLMs) excel at matching concepts across multi-modal inputs but struggle with compositional concepts and high-level relationships between entities. This paper introduces Progressive multi-granular \u2026"}, {"title": "Active learning for extracting rare adverse events from electronic health records: A study in pediatric cardiology", "link": "https://www.sciencedirect.com/science/article/pii/S1386505624004246", "details": "S Quennelle, S Malekzadeh-Milani, N Garcelon\u2026 - International Journal of \u2026, 2024", "abstract": "Objective Automate the extraction of adverse events from the text of electronic medical records of patients hospitalized for cardiac catheterization. Methods We focused on events related to cardiac catheterization as defined by the NCDR \u2026"}, {"title": "Filipino Benchmarks for Measuring Sexist and Homophobic Bias in Multilingual Language Models from Southeast Asia", "link": "https://arxiv.org/pdf/2412.07303", "details": "LCL Gamboa, M Lee - arXiv preprint arXiv:2412.07303, 2024", "abstract": "Bias studies on multilingual models confirm the presence of gender-related stereotypes in masked models processing languages with high NLP resources. We expand on this line of research by introducing Filipino CrowS-Pairs and Filipino \u2026"}, {"title": "Multimodal Latent Language Modeling with Next-Token Diffusion", "link": "https://arxiv.org/pdf/2412.08635", "details": "Y Sun, H Bao, W Wang, Z Peng, L Dong, S Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal generative models require a unified approach to handle both discrete data (eg, text and code) and continuous data (eg, image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates \u2026"}, {"title": "GeoTool-GPT: a trainable method for facilitating Large Language Models to master GIS tools", "link": "https://www.tandfonline.com/doi/abs/10.1080/13658816.2024.2438937", "details": "C Wei, Y Zhang, X Zhao, Z Zeng, Z Wang, J Lin\u2026 - International Journal of \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) excel in natural language-relevant tasks like text generation and question answering Q&A. To further expand their application, efforts focus on enabling LLMs to utilize real-world tools. However, their tool-use \u2026"}, {"title": "Enhancing the reasoning ability of multimodal large language models via mixed preference optimization", "link": "https://arxiv.org/pdf/2411.10442", "details": "W Wang, Z Chen, W Wang, Y Cao, Y Liu, Z Gao, J Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing open-source multimodal large language models (MLLMs) generally follow a training process involving pre-training and supervised fine-tuning. However, these models suffer from distribution shifts, which limit their multimodal reasoning \u2026"}, {"title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices", "link": "https://arxiv.org/pdf/2411.10640", "details": "X Lu, Y Chen, C Chen, H Tan, B Chen, Y Xie, R Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile \u2026"}, {"title": "Chain of Thought Prompting in Vision-Language Model for Vision Reasoning Tasks", "link": "https://link.springer.com/chapter/10.1007/978-981-96-0351-0_22", "details": "J Ou, J Zhou, Y Dong, F Chen - Australasian Joint Conference on Artificial Intelligence, 2024", "abstract": "The large language model has demonstrated its ability to reason and interpret in text- to-text applications. Current Chain of Thought (CoT) research focuses on either explaining reasoning steps or improving prediction results. This paper proposes a \u2026"}]
