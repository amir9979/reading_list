[{"title": "TRACE: Contrastive learning for multi-trial time-series data in neuroscience", "link": "https://arxiv.org/pdf/2506.04906", "details": "L Schmors, D Gonschorek, JN B\u00f6hm, Y Qiu, N Zhou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Modern neural recording techniques such as two-photon imaging allow to acquire vast time-series datasets with responses of hundreds or thousands of neurons. Contrastive learning is a powerful self-supervised framework for learning \u2026", "entry_id": "http://arxiv.org/abs/2506.04906v1", "updated": "2025-06-05 11:40:12", "published": "2025-06-05 11:40:12", "authors": "Lisa Schmors;Dominic Gonschorek;Jan Niklas B\u00f6hm;Yongrong Qiu;Na Zhou;Dmitry Kobak;Andreas Tolias;Fabian Sinz;Jacob Reimer;Katrin Franke;Sebastian Damrich;Philipp Berens", "summary": "Modern neural recording techniques such as two-photon imaging allow to\nacquire vast time-series datasets with responses of hundreds or thousands of\nneurons. Contrastive learning is a powerful self-supervised framework for\nlearning representations of complex datasets. Existing applications for neural\ntime series rely on generic data augmentations and do not exploit the\nmulti-trial data structure inherent in many neural datasets. Here we present\nTRACE, a new contrastive learning framework that averages across different\nsubsets of trials to generate positive pairs. TRACE allows to directly learn a\ntwo-dimensional embedding, combining ideas from contrastive learning and\nneighbor embeddings. We show that TRACE outperforms other methods, resolving\nfine response differences in simulated data. Further, using in vivo recordings,\nwe show that the representations learned by TRACE capture both biologically\nrelevant continuous variation, cell-type-related cluster structure, and can\nassist data quality control.", "comment": null, "journal_ref": null, "primary_category": "q-bio.NC", "categories": "q-bio.NC", "links": "http://arxiv.org/abs/2506.04906v1;http://arxiv.org/pdf/2506.04906v1", "pdf_url": "http://arxiv.org/pdf/2506.04906v1"}, {"title": "Output Scaling: YINGLONG Delayed Chain of Thought in a Large Pretrained Time Series Forecasting Model", "link": "https://www.researchgate.net/profile/Tian-Zhou-18/publication/392376185_Output_Scaling_YINGLONG_Delayed_Chain_of_Thought_in_a_Large_Pretrained_Time_Series_Forecasting_Model/links/683fe9badf0e3f544f5cd676/Output-Scaling-YINGLONG-Delayed-Chain-of-Thought-in-a-Large-Pretrained-Time-Series-Forecasting-Model.pdf", "details": "X Wang, T Zhou, JGBDJ Zhou", "abstract": "We present a joint forecasting framework for time series prediction that contrasts with traditional direct or recursive methods. This framework achieves state-ofthe-art performance for our designed foundation model, YINGLONG, and reveals a novel \u2026"}, {"title": "MultiVae: A Python package for Multimodal Variational Autoencoders on Partial Datasets.", "link": "https://joss.theoj.org/papers/10.21105/joss.07996.pdf", "details": "A Senellart, C Chadebec, S Allassonni\u00e8re - Journal of Open Source Software, 2025", "abstract": "In recent years, multimodal machine learning has seen significant growth, especially in representation learning and data generation. Recently, Multimodal Variational Autoencoders (VAEs) have been attracting growing interest for both tasks, thanks to \u2026"}]
