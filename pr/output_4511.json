[{"title": "LIONs: An Empirically Optimized Approach to Align Language Models", "link": "https://arxiv.org/pdf/2407.06542", "details": "X Yu, Q Wu, Y Li, Z Yu - arXiv preprint arXiv:2407.06542, 2024", "abstract": "Alignment is a crucial step to enhance the instruction-following and conversational abilities of language models. Despite many recent work proposing new algorithms, datasets, and training pipelines, there is a lack of comprehensive studies measuring \u2026"}, {"title": "OEHR: An Orthopedic Electronic Health Record Dataset", "link": "https://dl.acm.org/doi/abs/10.1145/3626772.3657885", "details": "Y Xie, K Wang, J Zheng, F Liu, X Wang, G Huang - Proceedings of the 47th \u2026, 2024", "abstract": "During the past decades, healthcare institutions continually amassed clinical data that is not intended to support research. Despite the increasing number of publicly available electronic health record (EHR) datasets, it is difficult to find publicly \u2026"}, {"title": "Highly Transferable Diffusion-based Unrestricted Adversarial Attack on Pre-trained Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3DyAygQe3Uxd", "details": "W Xu, K Chen, Z Gao, Z Wei, J Chen, YG Jiang - ACM Multimedia 2024", "abstract": "Pre-trained Vision-Language Models (VLMs) have shown great ability in various Vision-Language tasks. However, these VLMs exhibit inherent vulnerabilities to transferable adversarial examples, which could potentially undermine their \u2026"}, {"title": "AutoTutor meets Large Language Models: A Language Model Tutor with Rich Pedagogy and Guardrails", "link": "https://dl.acm.org/doi/abs/10.1145/3657604.3662041", "details": "S Pal Chowdhury, V Zouhar, M Sachan - \u2026 of the Eleventh ACM Conference on \u2026, 2024", "abstract": "Large Language Models (LLMs) have found several use cases in education, ranging from automatic question generation to essay evaluation. In this paper, we explore the potential of using LLMs to author Intelligent Tutoring Systems. A common pitfall of \u2026"}, {"title": "SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training", "link": "https://arxiv.org/pdf/2407.06654", "details": "N He, W Xiong, H Liu, Y Liao, L Ding, K Zhang, G Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The effectiveness of large language models (LLMs) is often hindered by duplicated data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, which risks the loss of valuable information and \u2026"}, {"title": "Extracting and Encoding: Leveraging Large Language Models and Medical Knowledge to Enhance Radiological Text Representation", "link": "https://arxiv.org/pdf/2407.01948", "details": "P Messina, R Vidal, D Parra, \u00c1 Soto, V Araujo - arXiv preprint arXiv:2407.01948, 2024", "abstract": "Advancing representation learning in specialized fields like medicine remains challenging due to the scarcity of expert annotations for text and images. To tackle this issue, we present a novel two-stage framework designed to extract high-quality \u2026"}, {"title": "SSAT-Adapter: Enhancing Vision-Language Model Few-shot Learning with Auxiliary Tasks", "link": "https://openreview.net/pdf%3Fid%3DuaKuvzR74K", "details": "B Chen, YS Koh, G Dobbie - ACM Multimedia 2024", "abstract": "Traditional deep learning models often struggle in few-shot learning scenarios, where limited labeled data is available. While the Contrastive Language-Image Pre- training (CLIP) model demonstrates impressive zero-shot capabilities, its \u2026"}, {"title": "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models", "link": "https://arxiv.org/pdf/2407.16470", "details": "K Benkirane, L Gongas, S Pelles, N Fuchs, J Darmon\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting \u2026"}, {"title": "CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents", "link": "https://arxiv.org/pdf/2407.01511%3Ftrk%3Dpublic_post_comment-text", "details": "T Xu, L Chen, DJ Wu, Y Chen, Z Zhang, X Yao, Z Xie\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The development of autonomous agents increasingly relies on Multimodal Language Models (MLMs) to perform tasks described in natural language with GUI environments, such as websites, desktop computers, or mobile phones. Existing \u2026"}]
