[{"title": "Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image Analysis", "link": "https://arxiv.org/pdf/2503.20047", "details": "Y Xin, GC Ates, K Gong, W Shao - arXiv preprint arXiv:2503.20047, 2025", "abstract": "Vision-language models (VLMs) have shown promise in 2D medical image analysis, but extending them to 3D remains challenging due to the high computational demands of volumetric data and the difficulty of aligning 3D spatial features with \u2026"}, {"title": "Unlocking language barriers: Assessing pre-trained large language models across multilingual tasks and unveiling the black box with Explainable Artificial \u2026", "link": "https://www.sciencedirect.com/science/article/pii/S0952197625001368", "details": "M Kastrati, AS Imran, E Hashmi, Z Kastrati\u2026 - Engineering Applications of \u2026, 2025", "abstract": "Abstract Large Language Models (LLMs) have revolutionized many industrial applications and paved the way for fostering a new research direction in many fields. Conventional Natural Language Processing (NLP) techniques, for instance, are no \u2026"}, {"title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking", "link": "https://arxiv.org/pdf/2503.19855", "details": "X Tian, S Zhao, H Wang, S Chen, Y Ji, Y Peng, H Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite \u2026"}, {"title": "LACER: Loss-Aware Clustering for Effective Reweighting", "link": "https://openreview.net/pdf%3Fid%3Dah74szSNvA", "details": "S Rastogi, P Kirichenko - Workshop on Spurious Correlation and Shortcut \u2026", "abstract": "Deep neural networks trained with Empirical Risk Minimization (ERM) are prone to rely on simple spurious features\u2014features that are correlated with the target but are not causally related to it. To mitigate this over-reliance, Deep Feature Reweighting \u2026"}, {"title": "3-WL GNNs for Metric Learning on Graphs", "link": "https://www.esann.org/sites/default/files/proceedings/2025/ES2025-49.pdf", "details": "A Moscatelli, M B\u00e9rar, P H\u00e9roux, F Yger, S Adam", "abstract": "Since the advent of Graph Neural Networks (GNNs), many works have computed distances between graphs by embedding them in vector spaces using Message Passing GNNs (MPNNs). However, MPNNs are known for their lack of \u2026"}, {"title": "A Survey on Mixture of Experts in Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10937907/", "details": "W Cai, J Jiang, F Wang, J Tang, S Kim, J Huang - IEEE Transactions on Knowledge \u2026, 2025", "abstract": "Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size \u2026"}, {"title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures", "link": "https://arxiv.org/pdf/2503.01496", "details": "D Lan, W Sun, J Hu, J Du, Y Cheng - arXiv preprint arXiv:2503.01496, 2025", "abstract": "Transformers with linear recurrent modeling offer linear-time training and constant- memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky \u2026"}, {"title": "Large Language Models as Attribution Regularizers for Efficient Model Training", "link": "https://arxiv.org/pdf/2502.20268%3F", "details": "D Vukadin, M \u0160ili\u0107, G Dela\u010d - arXiv preprint arXiv:2502.20268, 2025", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains. However, effectively leveraging their vast knowledge for training smaller downstream models remains an open challenge, especially in domains like \u2026"}, {"title": "Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding", "link": "https://arxiv.org/pdf/2503.01422", "details": "Y Wang, P Zhang, S Huang, B Yang, Z Zhang, F Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Test-time scaling improves large language model performance by adding extra compute during decoding. Best-of-N (BoN) sampling serves as a common scaling technique, broadening the search space for finding better solutions from the model \u2026"}]
