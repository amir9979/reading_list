[{"title": "Cross-modal retrieval of chest X-ray images and diagnostic reports based on report entity graph and dual attention", "link": "https://link.springer.com/article/10.1007/s00530-024-01649-6", "details": "W Ou, Y Chen, L Liang, J Gou, J Xiong, J Zhang, L Lai\u2026 - Multimedia Systems, 2025", "abstract": "Cross-modal retrieval for chest X-ray images and diagnostic reports is the automated process of fetching reports or related images from an extensive medical records database using specific queries. Current methods for cross-modal retrieval of chest X \u2026"}, {"title": "A vision\u2013language foundation model for precision oncology", "link": "https://www.nature.com/articles/s41586-024-08378-w", "details": "J Xiang, X Wang, X Zhang, Y Xi, F Eweje, Y Chen, Y Li\u2026 - Nature, 2025", "abstract": "Clinical decision-making is driven by multimodal data, including clinical notes and pathological characteristics. Artificial intelligence approaches that can effectively integrate multimodal data hold significant promise in advancing clinical care 1, 2 \u2026"}, {"title": "Activating Associative Disease-Aware Vision Token Memory for LLM-Based X-ray Report Generation", "link": "https://arxiv.org/pdf/2501.03458", "details": "X Wang, F Wang, H Wang, B Jiang, C Li, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "X-ray image based medical report generation achieves significant progress in recent years with the help of the large language model, however, these models have not fully exploited the effective information in visual image regions, resulting in reports \u2026"}, {"title": "DomCLP: Domain-wise Contrastive Learning with Prototype Mixup for Unsupervised Domain Generalization", "link": "https://arxiv.org/pdf/2412.09074", "details": "JS Lee, N Kim, JH Lee - arXiv preprint arXiv:2412.09074, 2024", "abstract": "Self-supervised learning (SSL) methods based on the instance discrimination tasks with InfoNCE have achieved remarkable success. Despite their success, SSL models often struggle to generate effective representations for unseen-domain data. To \u2026"}, {"title": "UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for Diverse Medical Imaging Modalities", "link": "https://arxiv.org/pdf/2412.10372", "details": "MU Khattak, S Kunhimon, M Naseer, S Khan, FS Khan - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-Language Models (VLMs) trained via contrastive learning have achieved notable success in natural image tasks. However, their application in the medical domain remains limited due to the scarcity of openly accessible, large-scale medical \u2026"}, {"title": "Do language models understand time?", "link": "https://arxiv.org/pdf/2412.13845", "details": "X Ding, L Wang - arXiv preprint arXiv:2412.13845, 2024", "abstract": "Large language models (LLMs) have revolutionized video-based computer vision applications, including action recognition, anomaly detection, and video summarization. Videos inherently pose unique challenges, combining spatial \u2026"}, {"title": "LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation", "link": "https://arxiv.org/pdf/2412.15188", "details": "W Shi, X Han, C Zhou, W Liang, XV Lin, L Zettlemoyer\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present LlamaFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LlamaFusion \u2026"}, {"title": "GIRAFFE: Design Choices for Extending the Context Length of Visual Language Models", "link": "https://arxiv.org/pdf/2412.12735", "details": "M Li, L Li, S Gong, Q Liu - arXiv preprint arXiv:2412.12735, 2024", "abstract": "Visual Language Models (VLMs) demonstrate impressive capabilities in processing multimodal inputs, yet applications such as visual agents, which require handling multiple images and high-resolution videos, demand enhanced long-range \u2026"}, {"title": "CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology", "link": "https://arxiv.org/pdf/2412.12077", "details": "Y Sun, Y Si, C Zhu, X Gong, K Zhang, P Chen, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence of large multimodal models (LMMs) has brought significant advancements to pathology. Previous research has primarily focused on separately training patch-level and whole-slide image (WSI)-level models, limiting the \u2026"}]
