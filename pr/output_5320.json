[{"title": "Multimodal representations of biomedical knowledge from limited training whole slide images and reports using deep learning", "link": "https://www.sciencedirect.com/science/article/pii/S1361841524002287", "details": "N Marini, S Marchesin, M Wodzinski, A Caputo\u2026 - Medical Image Analysis, 2024", "abstract": "The increasing availability of biomedical data creates valuable resources for developing new deep learning algorithms to support experts, especially in domains where collecting large volumes of annotated data is not trivial. Biomedical data \u2026"}, {"title": "Accuracy of Autonomous Artificial Intelligence-Based Diabetic Retinopathy Screening in Real-Life Clinical Practice", "link": "https://www.mdpi.com/2077-0383/13/16/4776", "details": "E Riotto, S Gasser, J Potic, M Sherif, T Stappler\u2026 - Journal of Clinical Medicine, 2024", "abstract": "Background: In diabetic retinopathy, early detection and intervention are crucial in preventing vision loss and improving patient outcomes. In the era of artificial intelligence (AI) and machine learning, new promising diagnostic tools have \u2026"}, {"title": "Vision Transformer-based Interpretable Metabolic Syndrome Classification Using Retinal Images", "link": "https://www.researchsquare.com/article/rs-4543204/latest.pdf", "details": "KA Sohn, TK Lee, SY Kim, HJ Choi, EK Choe - 2024", "abstract": "Metabolic syndrome is leading to an increased risk of diabetes and cardiovascular disease. Our study developed a model using retinal image data from fundus photographs taken during comprehensive health check-ups to classify metabolic \u2026"}, {"title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios", "link": "https://aclanthology.org/2024.findings-acl.230.pdf", "details": "L Lin, J Fu, P Liu, Q Li, Y Gong, J Wan, F Zhang\u2026 - Findings of the Association \u2026, 2024", "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local \u2026"}, {"title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.06663", "details": "K Sun, M Dredze - arXiv preprint arXiv:2408.06663, 2024", "abstract": "The development of large language models leads to the formation of a pre-train-then- align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream \u2026"}]
