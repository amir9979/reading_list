[{"title": "Efficient Tuning of Large Language Models for Knowledge-Grounded Dialogue Generation", "link": "https://arxiv.org/pdf/2504.07754", "details": "B Zhang, H Ma, D Li, J Ding, J Wang, B Xu, HF Lin - arXiv preprint arXiv:2504.07754, 2025", "abstract": "Large language models (LLMs) demonstrate remarkable text comprehension and generation capabilities but often lack the ability to utilize up-to-date or domain- specific knowledge not included in their training data. To address this gap, we \u2026"}, {"title": "Privacy-Preserving Federated Learning Framework for Multi-Source Electronic Health Records Prognosis Prediction", "link": "https://www.mdpi.com/1424-8220/25/8/2374", "details": "H Zhao, D Sui, Y Wang, L Ma, L Wang - Sensors, 2025", "abstract": "Secure and privacy-preserving health status representation learning has become a critical challenge in clinical prediction systems. While deep learning models require substantial high-quality data for training, electronic health records are often restricted \u2026"}, {"title": "How Robust Are Router-LLMs? Analysis of the Fragility of LLM Routing Capabilities", "link": "https://arxiv.org/pdf/2504.07113", "details": "AM Kassem, B Sch\u00f6lkopf, Z Jin - arXiv preprint arXiv:2504.07113, 2025", "abstract": "Large language model (LLM) routing has emerged as a crucial strategy for balancing computational costs with performance by dynamically assigning queries to the most appropriate model based on query complexity. Despite recent advances showing \u2026"}, {"title": "Synthesizing High-Quality Programming Tasks with LLM-based Expert and Student Agents", "link": "https://arxiv.org/pdf/2504.07655", "details": "MH Nguyen, VA P\u0103durean, A Gotovos, S Tschiatschek\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Generative AI is transforming computing education by enabling the automatic generation of personalized content and feedback. We investigate its capabilities in providing high-quality programming tasks to students. Despite promising \u2026"}, {"title": "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression", "link": "https://arxiv.org/pdf/2503.12340%3F", "details": "X Wang, S Alam, Z Wan, H Shen, M Zhang - arXiv preprint arXiv:2503.12340, 2025", "abstract": "Despite significant advancements, the practical deployment of Large Language Models (LLMs) is often hampered by their immense sizes, highlighting the need for effective compression techniques. Singular Value Decomposition (SVD) is a \u2026"}, {"title": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates", "link": "https://arxiv.org/pdf/2503.16334", "details": "Y Shen, L Huang - arXiv preprint arXiv:2503.16334, 2025", "abstract": "Recent findings reveal that much of the knowledge in a Transformer-based Large Language Model (LLM) is encoded in its feed-forward (FFN) layers, where each FNN layer can be interpreted as the summation of sub-updates, each corresponding to a \u2026"}, {"title": "ThoughtProbe: Classifier-Guided Thought Space Exploration Leveraging LLM Intrinsic Reasoning", "link": "https://arxiv.org/pdf/2504.06650", "details": "Z Wang, C Xu - arXiv preprint arXiv:2504.06650, 2025", "abstract": "Pre-trained large language models (LLMs) have been demonstrated to possess intrinsic reasoning capabilities that can emerge naturally when expanding the response space. However, the neural representation mechanisms underlying these \u2026"}]
