[{"title": "Enhancing vision-language models for medical imaging: bridging the 3D gap with innovative slice selection", "link": "https://openreview.net/pdf%3Fid%3DJrJW21IP9p", "details": "Y Wang, Y Dai, C Jones, HI Sair, J Shen, N Loizou\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Recent approaches to vision-language tasks are built on the remarkable capabilities of large vision-language models (VLMs). These models excel in zero-shot and few- shot learning, enabling them to learn new tasks without parameter updates \u2026"}, {"title": "Hidden in Plain Sight: Evaluating Abstract Shape Recognition in Vision-Language Models", "link": "https://arxiv.org/pdf/2411.06287", "details": "A Hemmat, A Davies, TA Lamb, J Yuan, P Torr\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the importance of shape perception in human vision, early neural image classifiers relied less on shape information for object recognition than other (often spurious) features. While recent research suggests that current large Vision \u2026"}, {"title": "Micro-Bench: A Microscopy Benchmark for Vision-Language Understanding", "link": "https://openreview.net/pdf%3Fid%3DeRleg6vy0Y", "details": "A Lozano, JJ Nirschl, J Burgess, SR Gupte, Y Zhang\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Recent advances in microscopy have enabled the rapid generation of terabytes of image data in cell biology and biomedical research. Vision-language models (VLMs) offer a promising solution for large-scale biological image analysis, enhancing \u2026"}, {"title": "ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for Large Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3DPyTf2jj0SH", "details": "S Liu, K Ying, H Zhang, Y Yang, Y Lin, T Zhang, C Li\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Multi-turn visual conversation is an important ability of real-world AI assistants. However, the related evaluation benchmark is missed. This paper presents ConvBench, a multi-turn conversation benchmark with hierarchical capabilities \u2026"}, {"title": "Automatic TNM staging of colorectal cancer radiology reports using pre-trained language models", "link": "https://www.sciencedirect.com/science/article/pii/S016926072400508X", "details": "M Chizhikova, P L\u00f3pez-\u00dabeda, T Mart\u00edn-Noguerol\u2026 - Computer Methods and \u2026, 2024", "abstract": "Abstract Background and Objective: Colorectal cancer is one of the major causes of cancer death worldwide. Essential for prognosis and treatment planning, TNM staging offers critical insights into the advancement of colorectal cancer. However \u2026"}, {"title": "Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset", "link": "https://arxiv.org/pdf/2411.03554", "details": "Y Ma, J Wang, F Wang, S Ma, J Li, X Li, F Huang, L Sun\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Machine unlearning has emerged as an effective strategy for forgetting specific information in the training data. However, with the increasing integration of visual data, privacy concerns in Vision Language Models (VLMs) remain underexplored. To \u2026"}, {"title": "Code-switching finetuning: Bridging multilingual pretrained language models for enhanced cross-lingual performance", "link": "https://www.sciencedirect.com/science/article/pii/S0952197624016907", "details": "C Zan, L Ding, L Shen, Y Cao, W Liu - Engineering Applications of Artificial \u2026, 2025", "abstract": "In recent years, the development of pre-trained models has significantly propelled advancements in natural language processing. However, multilingual sequence-to- sequence pretrained language models (Seq2Seq PLMs) are pretrained on a wide \u2026"}, {"title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models", "link": "https://openreview.net/pdf%3Fid%3DLC1QAqhePv", "details": "D Zhang, Z Hu, S Zhoubian, Z Du, K Yang, Z Wang\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving \u2026"}, {"title": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning", "link": "https://arxiv.org/pdf/2411.05193", "details": "J Hong, A Dragan, S Levine - arXiv preprint arXiv:2411.05193, 2024", "abstract": "Value-based reinforcement learning (RL) can in principle learn effective policies for a wide range of multi-turn problems, from games to dialogue to robotic control, including via offline RL from static previously collected datasets. However, despite \u2026"}]
