[{"title": "Mixed Distillation Helps Smaller Language Models Reason Better", "link": "https://aclanthology.org/2024.findings-emnlp.91.pdf", "details": "L Chenglin, Q Chen, L Li, C Wang, F Tao, Y Li, Z Chen\u2026 - Findings of the Association \u2026, 2024", "abstract": "As large language models (LLMs) have demonstrated impressive multiple step-by- step reasoning capabilities in recent natural language processing (NLP) reasoning tasks, many studies are interested in distilling reasoning abilities into smaller \u2026"}, {"title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models", "link": "https://openreview.net/pdf%3Fid%3DLC1QAqhePv", "details": "D Zhang, Z Hu, S Zhoubian, Z Du, K Yang, Z Wang\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving \u2026"}, {"title": "Learning predictable and robust neural representations by straightening image sequences", "link": "https://arxiv.org/pdf/2411.01777", "details": "X Niu, C Savin, EP Simoncelli - arXiv preprint arXiv:2411.01777, 2024", "abstract": "Prediction is a fundamental capability of all living organisms, and has been proposed as an objective for learning sensory representations. Recent work demonstrates that in primate visual systems, prediction is facilitated by neural representations that \u2026"}, {"title": "LHRS-Bot-Nova: Improved Multimodal Large Language Model for Remote Sensing Vision-Language Interpretation", "link": "https://arxiv.org/pdf/2411.09301", "details": "Z Li, D Muhtar, F Gu, X Zhang, P Xiao, G He, X Zhu - arXiv preprint arXiv:2411.09301, 2024", "abstract": "Automatically and rapidly understanding Earth's surface is fundamental to our grasp of the living environment and informed decision-making. This underscores the need for a unified system with comprehensive capabilities in analyzing Earth's surface to \u2026"}, {"title": "Micro-Bench: A Microscopy Benchmark for Vision-Language Understanding", "link": "https://openreview.net/pdf%3Fid%3DeRleg6vy0Y", "details": "A Lozano, JJ Nirschl, J Burgess, SR Gupte, Y Zhang\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Recent advances in microscopy have enabled the rapid generation of terabytes of image data in cell biology and biomedical research. Vision-language models (VLMs) offer a promising solution for large-scale biological image analysis, enhancing \u2026"}, {"title": "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "link": "https://aclanthology.org/2024.findings-emnlp.61.pdf", "details": "X Wei, H Chen, H Yu, H Fei, Q Liu - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have achieved notable success in commonsense reasoning tasks, benefiting from their extensive world knowledge acquired through extensive pretraining. While approaches like Chain-of-Thought \u2026"}, {"title": "Accelerating Blockwise Parallel Language Models with Draft Refinement", "link": "https://openreview.net/pdf%3Fid%3DKT6F5Sw0eg", "details": "T Kim, AT Suresh, KA Papineni, M Riley, S Kumar\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Autoregressive language models have achieved remarkable advancements, yet their potential is often limited by the slow inference speeds associated with sequential token generation. Blockwise parallel decoding (BPD) was proposed by Stern et \u2026"}]
