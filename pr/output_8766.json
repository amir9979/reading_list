[{"title": "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models", "link": "https://aclanthology.org/2024.emnlp-main.678.pdf", "details": "L Ranaldi, G Pucci, B Haddow, A Birch - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "In-context learning methods are popular inference strategies where Large Language Models (LLMs) are elicited to solve a task using provided demonstrations without parameter updates. Among these approaches are the reasoning methods, best \u2026"}, {"title": "Comparing SMILES and SELFIES tokenization for enhanced chemical language modeling", "link": "https://www.nature.com/articles/s41598-024-76440-8", "details": "M Leon, Y Perezhohin, F Peres, A Popovi\u010d, M Castelli - Scientific Reports, 2024", "abstract": "Life sciences research and experimentation are resource-intensive, requiring extensive trials and considerable time. Often, experiments do not achieve their intended objectives, but progress is made through trial and error, eventually leading \u2026"}, {"title": "Improving Referring Ability for Biomedical Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.375.pdf", "details": "J Jiang, F Cheng, A Aizawa - Findings of the Association for Computational \u2026, 2024", "abstract": "Existing auto-regressive large language models (LLMs) are primarily trained using documents from general domains. In the biomedical domain, continual pre-training is a prevalent method for domain adaptation to inject professional knowledge into \u2026"}, {"title": "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.1249.pdf", "details": "X Lin, M Li, R Zemel, H Ji, SF Chang - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "Recently, enabling pretrained language models (PLMs) to perform zero-shot crossmodal tasks such as video question answering has been extensively studied. A popular approach is to learn a projection network that projects visual features into the \u2026"}, {"title": "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.884.pdf", "details": "P Gao, T Yamasaki, K Imoto - Findings of the Association for Computational \u2026, 2024", "abstract": "We propose VE-KD, a novel method that balances knowledge distillation and vocabulary expansion with the aim of training efficient domain-specific language models. Compared with traditional pre-training approaches, VE-KD exhibits \u2026"}, {"title": "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering", "link": "https://aclanthology.org/2024.findings-emnlp.95.pdf", "details": "Y Wang, X Ma, W Chen - Findings of the Association for Computational \u2026, 2024", "abstract": "Large-scale language models (LLMs) like ChatGPT have demonstrated impressive abilities in generating responses based on human instructions. However, their use in the medical field can be challenging due to their lack of specific, in-depth knowledge \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers", "link": "https://openreview.net/pdf%3Fid%3D67tRrjgzsh", "details": "X Lu, Y Zhao, B Qin, L Huo, Q Yang, D Xu", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "BendVLM: Test-Time Debiasing of Vision-Language Embeddings", "link": "https://openreview.net/pdf%3Fid%3DutMOhsgXzB", "details": "W Gerych, H Zhang, K Hamidieh, E Pan, M Sharma\u2026 - The Thirty-eighth Annual \u2026, 2024", "abstract": "Vision-language (VL) embedding models have been shown to encode biases present in their training data, such as societal biases that prescribe negative characteristics to members of various racial and gender identities. Due to their wide \u2026"}, {"title": "Self-Supervised Contrastive Learning for Consistent Few-Shot Image", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3D1n4qEQAAQBAJ%26oi%3Dfnd%26pg%3DPA173%26ots%3Db62E548KLh%26sig%3D4-yzeb8-hzNhjNUFw2M7JR-gaXI", "details": "S Karimijafarbigloo\u00b9, R Azad, D Merhof - Predictive Intelligence in Medicine: 7th International \u2026", "abstract": "The central challenge in few-shot learning involves (1) acquiring object proposals through the support representation,(2) ensur-ing consistent representations for images in both support and query sets, and (3) achieving effective metric learning for \u2026"}]
