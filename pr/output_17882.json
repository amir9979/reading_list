[{"title": "OpusLM: A Family of Open Unified Speech Language Models", "link": "https://arxiv.org/pdf/2506.17611", "details": "J Tian, W Chen, Y Peng, J Shi, S Arora, S Bharadwaj\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper presents Open Unified Speech Language Models (OpusLMs), a family of open foundational speech language models (SpeechLMs) up to 7B. Initialized from decoder-only text language models, the OpusLMs are continuously pre-trained on \u2026", "entry_id": "http://arxiv.org/abs/2506.17611v1", "updated": "2025-06-21 06:30:59", "published": "2025-06-21 06:30:59", "authors": "Jinchuan Tian;William Chen;Yifan Peng;Jiatong Shi;Siddhant Arora;Shikhar Bharadwaj;Takashi Maekaku;Yusuke Shinohara;Keita Goto;Xiang Yue;Huck Yang;Shinji Watanabe", "summary": "This paper presents Open Unified Speech Language Models (OpusLMs), a family\nof open foundational speech language models (SpeechLMs) up to 7B. Initialized\nfrom decoder-only text language models, the OpusLMs are continuously\npre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We\ndemonstrate our OpusLMs achieve comparable (or even superior) performance with\nexisting SpeechLMs in speech recognition, speech synthesis, and text-only\ncapabilities. Technically, this paper articulates our SpeechLM designs on\ntokenization, multi-stream language models, and multi-stage training\nstrategies. We experimentally demonstrate the importance of model size scaling\nand the effect of annealing data selection. The OpusLMs are all built from\npublicly available materials and are fully transparent models. We release our\ncode, data, checkpoints, and training logs to facilitate open SpeechLM research", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.SD;eess.AS", "links": "http://arxiv.org/abs/2506.17611v1;http://arxiv.org/pdf/2506.17611v1", "pdf_url": "http://arxiv.org/pdf/2506.17611v1"}, {"title": "Distilling On-device Language Models for Robot Planning with Minimal Human Intervention", "link": "https://arxiv.org/pdf/2506.17486", "details": "Z Ravichandran, I Hounie, F Cladera, A Ribeiro\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) provide robots with powerful contextual reasoning abilities and a natural human interface. Yet, current LLM-enabled robots typically depend on cloud-hosted models, limiting their usability in environments with \u2026", "entry_id": "http://arxiv.org/abs/2506.17486v1", "updated": "2025-06-20 21:44:27", "published": "2025-06-20 21:44:27", "authors": "Zachary Ravichandran;Ignacio Hounie;Fernando Cladera;Alejandro Ribeiro;George J. Pappas;Vijay Kumar", "summary": "Large language models (LLMs) provide robots with powerful contextual\nreasoning abilities and a natural human interface. Yet, current LLM-enabled\nrobots typically depend on cloud-hosted models, limiting their usability in\nenvironments with unreliable communication infrastructure, such as outdoor or\nindustrial settings. We present PRISM, a framework for distilling small\nlanguage model (SLM)-enabled robot planners that run on-device with minimal\nhuman supervision. Starting from an existing LLM-enabled planner, PRISM\nautomatically synthesizes diverse tasks and environments, elicits plans from\nthe LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in\nreplacement of the source model. We apply PRISM to three LLM-enabled planners\nfor mapping and exploration, manipulation, and household assistance, and we\ndemonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of\nGPT-4o's performance to over 93% - using only synthetic data. We further\ndemonstrate that the distilled planners generalize across heterogeneous robotic\nplatforms (ground and aerial) and diverse environments (indoor and outdoor). We\nrelease all software, trained models, and datasets at\nhttps://zacravichandran.github.io/PRISM.", "comment": null, "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2506.17486v1;http://arxiv.org/pdf/2506.17486v1", "pdf_url": "http://arxiv.org/pdf/2506.17486v1"}, {"title": "Interrogating LLM design under copyright law", "link": "https://dl.acm.org/doi/pdf/10.1145/3715275.3732193", "details": "J Tian-Zheng Wei, M Wang, A Godbole, J Choi, R Jia - Proceedings of the 2025 ACM \u2026, 2025", "abstract": "The current discourse on large language models (LLMs) and copyright largely takes a \u201cbehavioral\u201d perspective, focusing on model outputs and evaluating whether they are substantially similar to training data. However, substantial similarity is difficult to \u2026"}, {"title": "Dataflow-Guided Neuro-Symbolic Language Models for Type Inference", "link": "https://openreview.net/pdf%3Fid%3Do5D8i2zZ1l", "details": "G Li, Y Wan, H Zhang, Z Zhao, W Jiang, X Shi, H Jin\u2026 - Forty-second International \u2026", "abstract": "Language Models (LMs) are increasingly used for type inference, aiding in error detection and software development. Some real-world deployments of LMs require the model to run on local machines to safeguard the intellectual property of the \u2026"}, {"title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation", "link": "https://arxiv.org/pdf/2506.18088", "details": "T Chen, Z Chen, B Chen, Z Cai, Y Liu, Q Liang, Z Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Simulation-based data synthesis has emerged as a powerful paradigm for enhancing real-world robotic manipulation. However, existing synthetic datasets remain insufficient for robust bimanual manipulation due to two challenges:(1) the \u2026", "entry_id": "http://arxiv.org/abs/2506.18088v1", "updated": "2025-06-22 16:26:53", "published": "2025-06-22 16:26:53", "authors": "Tianxing Chen;Zanxin Chen;Baijun Chen;Zijian Cai;Yibin Liu;Qiwei Liang;Zixuan Li;Xianliang Lin;Yiheng Ge;Zhenyu Gu;Weiliang Deng;Yubin Guo;Tian Nian;Xuanbing Xie;Qiangyu Chen;Kailun Su;Tianling Xu;Guodong Liu;Mengkang Hu;Huan-ang Gao;Kaixuan Wang;Zhixuan Liang;Yusen Qin;Xiaokang Yang;Ping Luo;Yao Mu", "summary": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation.", "comment": "Project Page: https://robotwin-platform.github.io/", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.AI;cs.CL;cs.CV;cs.MA", "links": "http://arxiv.org/abs/2506.18088v1;http://arxiv.org/pdf/2506.18088v1", "pdf_url": "http://arxiv.org/pdf/2506.18088v1"}, {"title": "Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures", "link": "https://arxiv.org/pdf/2506.06832", "details": "C Hongler, A Emil - arXiv preprint arXiv:2506.06832, 2025", "abstract": "Large Language Models (LLMs) define probability measures on text. By considering the implicit knowledge question of what it means for an LLM to know such a measure and what it entails algorithmically, we are naturally led to formulate a series of tasks \u2026", "entry_id": "http://arxiv.org/abs/2506.06832v2", "updated": "2025-06-22 12:08:32", "published": "2025-06-07 15:25:10", "authors": "Cl\u00e9ment Hongler;Andrew Emil", "summary": "Large Language Models (LLMs) define probability measures on text. By\nconsidering the implicit knowledge question of what it means for an LLM to know\nsuch a measure and what it entails algorithmically, we are naturally led to\nformulate a series of tasks that go beyond generative sampling, involving forms\nof summarization, counterfactual thinking, anomaly detection, originality\nsearch, reverse prompting, debating, creative solving, etc. These tasks can be\nformulated as games based on LLM measures, which we call Cross-Entropy (Xent)\nGames. Xent Games can be single-player or multi-player. They involve\ncross-entropy scores and cross-entropy constraints, and can be expressed as\nsimple computational graphs and programs. We show the Xent Game space is large\nenough to contain a wealth of interesting examples, while being constructible\nfrom basic game-theoretic consistency axioms. We then discuss how the Xent Game\nspace can be used to measure the abilities of LLMs. This leads to the\nconstruction of Xent Game measures: finite families of Xent Games that can be\nused as capability benchmarks, built from a given scope, by extracting a\ncovering measure. To address the unbounded scope problem associated with the\nchallenge of measuring general abilities, we propose to explore the space of\nXent Games in a coherent fashion, using ideas inspired by evolutionary\ndynamics.", "comment": "42 pages, 16 figures", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL;cs.GT;cs.IT;cs.NE;math.IT", "links": "http://arxiv.org/abs/2506.06832v2;http://arxiv.org/pdf/2506.06832v2", "pdf_url": "http://arxiv.org/pdf/2506.06832v2"}, {"title": "S-Eval: Towards Automated and Comprehensive Safety Evaluation for Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3728971", "details": "X Yuan, J Li, D Wang, Y Chen, X Mao, L Huang\u2026 - Proceedings of the ACM on \u2026, 2025", "abstract": "Generative large language models (LLMs) have revolutionized natural language processing with their transformative and emergent capabilities. However, recent evidence indicates that LLMs can produce harmful content that violates social norms \u2026"}, {"title": "Evaluating the Intelligence of large language models: A comparative study using verbal and visual IQ tests", "link": "https://www.sciencedirect.com/science/article/pii/S2949882125000544", "details": "S Abdelkarim, D Lu, DL Flores, S Jaeggi, P Baldi - Computers in Human Behavior \u2026, 2025", "abstract": "Large language models (LLMs) excel on many specialised benchmarks, yet their general-reasoning ability remains opaque. We therefore test 18 models\u2014including GPT-4, Claude 3 and Gemini Pro\u2014on a 14-section IQ suite spanning verbal \u2026"}, {"title": "Demystifying the Visual Quality Paradox in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2506.15645", "details": "S Xing, L Guo, H Hua, S Lee, P Li, Y Wang, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark vision- language tasks, yet little is known about how input visual quality shapes their responses. Does higher perceptual quality of images already translate to better \u2026", "entry_id": "http://arxiv.org/abs/2506.15645v1", "updated": "2025-06-18 17:14:07", "published": "2025-06-18 17:14:07", "authors": "Shuo Xing;Lanqing Guo;Hongyuan Hua;Seoyoung Lee;Peiran Li;Yufei Wang;Zhangyang Wang;Zhengzhong Tu", "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.", "comment": "18 pages", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2506.15645v1;http://arxiv.org/pdf/2506.15645v1", "pdf_url": "http://arxiv.org/pdf/2506.15645v1"}]
