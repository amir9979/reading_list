[{"title": "Graph and text multi-modal representation learning with momentum distillation on Electronic Health Records", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124010074", "details": "Y Cao, X Wang, Q Wang, Z Yuan, Y Shi, D Peng - Knowledge-Based Systems, 2024", "abstract": "The emergence and widespread adoption of electronic health records (EHR) in modern healthcare systems has generated a large amount of data with the potential to significantly improve patient outcomes. However, extracting meaningful insights \u2026"}, {"title": "Designing Retrieval-Augmented Language Models for Clinical Decision", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DWcMbEQAAQBAJ%26oi%3Dfnd%26pg%3DPA159%26ots%3DtCwXqYSHen%26sig%3DKqyQGFR-rUaoX5gi9guGfzoZY9s", "details": "K Quigley, T Koker, J Taylor, V Mancuso - AI for Health Equity and Fairness: Leveraging AI to \u2026", "abstract": "Ever-increasing demands for physician expertise drive the need for trust-worthy point- of-care tools that can help aid decision-making in all clinical settings. Retrieval- augmented language models carry potential to relieve the information burden on \u2026"}, {"title": "Language Models Don't Learn the Physical Manifestation of Language", "link": "https://aclanthology.org/2024.acl-long.195.pdf", "details": "B Lee, J Lim - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "We argue that language-only models don't learn the physical manifestation of language. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-Test. These tasks highlight a \u2026"}, {"title": "Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability", "link": "https://arxiv.org/pdf/2408.07852", "details": "J Hron, L Culp, G Elsayed, R Liu, B Adlam, M Bileschi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While many capabilities of language models (LMs) improve with increased training budget, the influence of scale on hallucinations is not yet fully understood. Hallucinations come in many forms, and there is no universally accepted definition \u2026"}, {"title": "Boosting entity recognition by leveraging cross-task domain models for weak supervision", "link": "https://www.amazon.science/publications/boosting-entity-recognition-by-leveraging-cross-task-domain-models-for-weak-supervision", "details": "S Agrawal, S Merugu, V Sembium - 2024", "abstract": "Entity Recognition (ER) is a common natural language processing task encountered in a number of real-world applications. For common domains and named entities such as places and organisations, there exists sufficient high quality annotated data \u2026"}, {"title": "The advantages of context specific language models: the case of the Erasmian Language Model", "link": "https://arxiv.org/pdf/2408.06931", "details": "J Gon\u00e7alves, N Jelicic, M Murgia, E Stamhuis - arXiv preprint arXiv:2408.06931, 2024", "abstract": "The current trend to improve language model performance seems to be based on scaling up with the number of parameters (eg the state of the art GPT4 model has approximately 1.7 trillion parameters) or the amount of training data fed into the \u2026"}, {"title": "E-code: Mastering Efficient Code Generation through Pretrained Models and Expert Encoder Group", "link": "https://arxiv.org/pdf/2408.12948", "details": "Y Pan, C Lyu, Z Yang, L Li, Q Liu, X Shao - arXiv preprint arXiv:2408.12948, 2024", "abstract": "Context: With the waning of Moore's Law, the software industry is placing increasing importance on finding alternative solutions for continuous performance enhancement. The significance and research results of software performance \u2026"}, {"title": "Towards Harnessing Large Language Models as Autonomous Agents for Semantic Triple Extraction from Unstructured Text", "link": "https://ceur-ws.org/Vol-3747/text2kg_paper1.pdf", "details": "A Ananya, S Tiwari, N Mihindukulasooriya, T Soru\u2026 - 2024", "abstract": "Abstract The use of Large Language Models as autonomous agents interacting with tools has shown to improve the performance of several tasks from code generation to API calling and sequencing. This paper proposes a framework for using Large \u2026"}, {"title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.06663", "details": "K Sun, M Dredze - arXiv preprint arXiv:2408.06663, 2024", "abstract": "The development of large language models leads to the formation of a pre-train-then- align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream \u2026"}]
