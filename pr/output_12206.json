[{"title": "Efficient Architectures for High Resolution Vision-Language Models", "link": "https://arxiv.org/pdf/2501.02584", "details": "M Carvalho, B Martins - arXiv preprint arXiv:2501.02584, 2025", "abstract": "Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This \u2026"}, {"title": "Training Medical Large Vision-Language Models with Abnormal-Aware Feedback", "link": "https://arxiv.org/pdf/2501.01377", "details": "Y Zhou, L Song, J Shen - arXiv preprint arXiv:2501.01377, 2025", "abstract": "Existing Medical Large Vision-Language Models (Med-LVLMs), which encapsulate extensive medical knowledge, demonstrate excellent capabilities in understanding medical images and responding to human queries based on these images \u2026"}, {"title": "GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models", "link": "https://arxiv.org/pdf/2501.01428%3F", "details": "Z Qi, Z Zhang, Y Fang, J Wang, H Zhao - arXiv preprint arXiv:2501.01428, 2025", "abstract": "In recent years, 2D Vision-Language Models (VLMs) have made significant strides in image-text understanding tasks. However, their performance in 3D spatial comprehension, which is critical for embodied intelligence, remains limited. Recent \u2026"}, {"title": "From My View to Yours: Ego-Augmented Learning in Large Vision Language Models for Understanding Exocentric Daily Living Activities", "link": "https://arxiv.org/pdf/2501.05711", "details": "D Reilly, MK Govind, S Das - arXiv preprint arXiv:2501.05711, 2025", "abstract": "Large Vision Language Models (LVLMs) have demonstrated impressive capabilities in video understanding, yet their adoption for Activities of Daily Living (ADL) remains limited by their inability to capture fine-grained interactions and spatial relationships \u2026"}, {"title": "Guiding Medical Vision-Language Models with Explicit Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations", "link": "https://arxiv.org/pdf/2501.02385", "details": "K Zhu, Z Qin, H Yi, Z Jiang, Q Lao, S Zhang, K Li - arXiv preprint arXiv:2501.02385, 2025", "abstract": "With the recent advancements in vision-language models (VLMs) driven by large language models (LLMs), many researchers have focused on models that comprised of an image encoder, an image-to-language projection layer, and a text decoder \u2026"}, {"title": "Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual Information in Long-form Text Generation", "link": "https://arxiv.org/pdf/2501.03545%3F", "details": "C Samarinas, A Krubner, A Salemi, Y Kim, H Zamani - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper presents ICAT, an evaluation framework for measuring coverage of diverse factual information in long-form text generation. ICAT breaks down a long output text into a list of atomic claims and not only verifies each claim through \u2026"}, {"title": "Recalibrated cross-modal alignment network for radiology report generation with weakly supervised contrastive learning", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425000168", "details": "X Hou, X Li, Z Liu, S Sang, M Lu, Y Zhang - Expert Systems with Applications, 2025", "abstract": "Automatic radiology report generation is rapidly becoming an essential method for medical diagnosis and precision medicine, which will help clinical doctors make more informed decisions and achieve better results. Most previous studies mainly \u2026"}, {"title": "Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval", "link": "https://arxiv.org/pdf/2501.09134", "details": "D Deanda, YP Masupalli, J Yang, Y Lee, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical images and reports offer invaluable insights into patient health. The heterogeneity and complexity of these data hinder effective analysis. To bridge this gap, we investigate contrastive learning models for cross-domain retrieval, which \u2026"}, {"title": "Adversarial Neural Radiance Networks: A Unified Approach for High-Quality Image Synthesis and 3D Scene Representation", "link": "https://www.researchgate.net/profile/Matthew-White-61/publication/387538505_Adversarial_Neural_Radiance_Networks_A_Unified_Approach_for_High-Quality_Image_Synthesis_and_3D_Scene_Representation/links/67733b2ffb9aff6eaaf8c204/Adversarial-Neural-Radiance-Networks-A-Unified-Approach-for-High-Quality-Image-Synthesis-and-3D-Scene-Representation.pdf", "details": "J Martin, S Harris, M White", "abstract": "Generative Adversarial Networks (GANs) have revolutionized the field of generative modeling, providing powerful tools for synthesizing realistic and high-quality images. Since their introduction by Goodfellow et al. in 2014, GANs have demonstrated \u2026"}]
