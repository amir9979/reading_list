[{"title": "Shareable artificial intelligence to extract cancer outcomes from electronic health records.", "link": "https://ascopubs.org/doi/abs/10.1200/JCO.2024.42.16_suppl.11000", "details": "KL Kehl, J Jee, K Pichotta, P Trukhanov, C Fong\u2026 - 2024", "abstract": "11000 Background: Clinical outcomes such as response, progression, and metastasis represent crucial data for observational cancer research, but outside of clinical trials, such outcomes are usually recorded only in unstructured notes in \u2026"}, {"title": "Mixture of In-Context Prompters for Tabular PFNs", "link": "https://arxiv.org/pdf/2405.16156", "details": "D Xu, O Cirit, R Asadi, Y Sun, W Wang - arXiv preprint arXiv:2405.16156, 2024", "abstract": "Recent benchmarks found In-Context Learning (ICL) outperforms both deep learning and tree-based algorithms on small tabular datasets. However, on larger datasets, ICL for tabular learning cannot run without severely compromising performance, due \u2026"}, {"title": "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "link": "https://arxiv.org/pdf/2405.16908", "details": "G Yona, R Aharoni, M Geva - arXiv preprint arXiv:2405.16908, 2024", "abstract": "We posit that large language models (LLMs) should be capable of expressing their intrinsic uncertainty in natural language. For example, if the LLM is equally likely to output two contradicting answers to the same question, then its generated response \u2026"}, {"title": "Self-play preference optimization for language model alignment", "link": "https://arxiv.org/pdf/2405.00675", "details": "Y Wu, Z Sun, H Yuan, K Ji, Y Yang, Q Gu - arXiv preprint arXiv:2405.00675, 2024", "abstract": "Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest \u2026"}, {"title": "Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.708.pdf", "details": "O Ignat, Z Jin, A Abzaliev, L Biester, S Castro, N Deng\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Recent progress in large language models (LLMs) has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that \u201cit's all been solved.\u201d Not surprisingly, this has, in turn, made \u2026"}, {"title": "SigBart: Enhanced Pre-training via Salient Content Representation Learning for Social Media Summarization", "link": "https://ir.cs.georgetown.edu/publications/pub-files/sigbart-sotudeh-socialnlp-www24.pdf", "details": "S Sotudeh, N Goharian - Companion Proceedings of the ACM on Web \u2026, 2024", "abstract": "Our approach to automatically summarizing online mental health posts could help counselors by reducing their reading time, enabling quicker and more effective support for individuals seeking mental health assistance. Neural text summarization \u2026"}]
