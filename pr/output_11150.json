[{"title": "Distilling Large Language Models for Efficient Clinical Information Extraction", "link": "https://arxiv.org/pdf/2501.00031", "details": "KS Vedula, A Gupta, A Swaminathan, I Lopez, S Bedi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) excel at clinical information extraction but their computational demands limit practical deployment. Knowledge distillation--the process of transferring knowledge from larger to smaller models--offers a potential \u2026"}, {"title": "Efficient Standardization of Clinical Notes using Large Language Models", "link": "https://arxiv.org/pdf/2501.00644", "details": "DB Hier, MD Carrithers, TS Do, T Obafemi-Ajayi - arXiv preprint arXiv:2501.00644, 2024", "abstract": "Clinician notes are a rich source of patient information but often contain inconsistencies due to varied writing styles, colloquialisms, abbreviations, medical jargon, grammatical errors, and non-standard formatting. These inconsistencies \u2026"}, {"title": "AutoReason: Automatic Few-Shot Reasoning Decomposition", "link": "https://arxiv.org/pdf/2412.06975", "details": "A Sevinc, A Gumus - arXiv preprint arXiv:2412.06975, 2024", "abstract": "Chain of Thought (CoT) was introduced in recent research as a method for improving step-by-step reasoning in Large Language Models. However, CoT has limited applications such as its need for hand-crafted few-shot exemplar prompts and no \u2026"}, {"title": "Contextualized race and ethnicity annotations for clinical text from MIMIC-III", "link": "https://search.proquest.com/openview/6bc23dba101d21a98bf23c696190c1cb/1%3Fpq-origsite%3Dgscholar%26cbl%3D2041912", "details": "J Li, J Joseph, S Kinberg, LR Richter, S Crusco\u2026", "abstract": "Observational health research often relies on accurate and complete race and ethnicity (RE) patient information, such as characterizing cohorts, assessing quality/performance metrics of hospitals and health systems, and identifying health \u2026"}]
