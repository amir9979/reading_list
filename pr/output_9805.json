[{"title": "From Language Models to Medical Diagnoses: Assessing the Potential of GPT-4 and GPT-3.5-Turbo in Digital Health", "link": "https://www.mdpi.com/2673-2688/5/4/128", "details": "J Roos, TI Wilhelm, R Martin, R Kaczmarczyk - AI, 2024", "abstract": "Background: Large language models (LLMs) like GPT-3.5-Turbo and GPT-4 show potential to transform medical diagnostics through their linguistic and analytical capabilities. This study evaluates their diagnostic proficiency using English and \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers", "link": "https://openreview.net/pdf%3Fid%3D67tRrjgzsh", "details": "X Lu, Y Zhao, B Qin, L Huo, Q Yang, D Xu - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Rethinking Generalizability and Discriminability of Self-Supervised Learning from Evolutionary Game Theory Perspective", "link": "https://arxiv.org/pdf/2412.00542", "details": "J Li, Z Zang, Q Ji, C Sun, W Qiang, J Zhang, C Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Representations learned by self-supervised approaches are generally considered to possess sufficient generalizability and discriminability. However, we disclose a nontrivial mutual-exclusion relationship between these critical representation \u2026"}, {"title": "Audiobox TTA-RAG: Improving Zero-Shot and Few-Shot Text-To-Audio with Retrieval-Augmented Generation", "link": "https://arxiv.org/pdf/2411.05141", "details": "M Yang, B Shi, M Le, WN Hsu, A Tjandra - arXiv preprint arXiv:2411.05141, 2024", "abstract": "Current leading Text-To-Audio (TTA) generation models suffer from degraded performance on zero-shot and few-shot settings. It is often challenging to generate high-quality audio for audio events that are unseen or uncommon in the training set \u2026"}, {"title": "Can Large Language Models Serve as Evaluators for Code Summarization?", "link": "https://arxiv.org/pdf/2412.01333", "details": "Y Wu, Y Wan, Z Chu, W Zhao, Y Liu, H Zhang, X Shi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Code summarization facilitates program comprehension and software maintenance by converting code snippets into natural-language descriptions. Over the years, numerous methods have been developed for this task, but a key challenge remains \u2026"}, {"title": "Discovering sparsity allocation for layer-wise pruning of large language models", "link": "https://openreview.net/pdf%3Fid%3DrgtrYVC9n4", "details": "L Li, P Dong, Z Tang, X Liu, Q Wang, W Luo, W Xue\u2026 - The Thirty-eighth Annual \u2026, 2024", "abstract": "In this paper, we present DSA, the first automated framework for discovering sparsity allocation schemes for layer-wise pruning in Large Language Models (LLMs). LLMs have become increasingly powerful, but their large parameter counts make them \u2026"}, {"title": "Towards Resource Efficient and Interpretable Bias Mitigation in Large Language Models", "link": "https://arxiv.org/pdf/2412.01711", "details": "S Tong, E Zemour, R Lohanimit, L Kagal - arXiv preprint arXiv:2412.01711, 2024", "abstract": "Although large language models (LLMs) have demonstrated their effectiveness in a wide range of applications, they have also been observed to perpetuate unwanted biases present in the training data, potentially leading to harm for marginalized \u2026"}, {"title": "Medsafetybench: Evaluating and improving the medical safety of large language models", "link": "https://openreview.net/pdf%3Fid%3DcFyagd2Yh4", "details": "T Han, A Kumar, C Agarwal, H Lakkaraju - The Thirty-eight Conference on Neural \u2026, 2024", "abstract": "As large language models (LLMs) develop increasingly sophisticated capabilities and find applications in medical settings, it becomes important to assess their medical safety due to their far-reaching implications for personal and public health \u2026"}, {"title": "Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language Models", "link": "https://openreview.net/pdf%3Fid%3Dnvn80cscVm", "details": "L Wei, Z Tan, C Li, J Wang, W Huang - The Thirty-eighth Annual Conference on \u2026, 2024", "abstract": "Large Language Models (LLMs) have transformed natural language processing and extended their powerful capabilities to multi-modal domains. As LLMs continue to advance, it is crucial to develop diverse and appropriate metrics for their evaluation \u2026"}]
