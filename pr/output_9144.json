[{"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers", "link": "https://openreview.net/pdf%3Fid%3D67tRrjgzsh", "details": "X Lu, Y Zhao, B Qin, L Huo, Q Yang, D Xu - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "LM2: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.920.pdf", "details": "G Juneja, S Dutta, T Chakraborty - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}, {"title": "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.313.pdf", "details": "M Xu, Y Li, K Sun, T Qian - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Large language models (LLMs) have shown excellent capability for solving reasoning problems. Existing approaches do not differentiate the question difficulty when designing prompting methods for them. Clearly, a simple method cannot elicit \u2026"}, {"title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models", "link": "https://aclanthology.org/2024.emnlp-main.566.pdf", "details": "XH Feng, C Chen, Y Li, Z Lin - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Pre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for \u2026"}, {"title": "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "link": "https://aclanthology.org/2024.emnlp-main.204.pdf", "details": "J Gu, Z Wang, Y Zhang, Z Zhang, P Gong - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "With the proliferation of large language models, Parameter Efficient Fine-Tuning (PEFT) method, which freeze pre-trained parameters and only fine-tune a few task- specific parameters, are playing an increasingly important role. However, previous \u2026"}, {"title": "Learning to Plan by Updating Natural Language", "link": "https://aclanthology.org/2024.findings-emnlp.589.pdf", "details": "Y Guo, Y Liang, C Wu, W Wu, D Zhao, N Duan - Findings of the Association for \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have shown remarkable performance in various basic natural language tasks. For completing the complex task, we still need a plan for the task to guide LLMs to generate the specific solutions step by step \u2026"}, {"title": "Reconfidencing LLM Uncertainty from the Grouping Loss Perspective", "link": "https://hal.science/hal-04750567/file/Preprint_Reconfidencing_LLMs.pdf", "details": "L Chen, A Perez-Lebel, F Suchanek, G Varoquaux - The 2024 Conference on \u2026, 2024", "abstract": "Large Language Models (LLMs), such as GPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone. While previous efforts to elicit and calibrate uncertainty have shown some success, they often overlook biases \u2026"}, {"title": "Factuality of Large Language Models: A Survey", "link": "https://aclanthology.org/2024.emnlp-main.1088.pdf", "details": "Y Wang, M Wang, MA Manzoor, F Liu, G Georgiev\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a \u2026"}, {"title": "Knowledge-Centric Hallucination Detection", "link": "https://aclanthology.org/2024.emnlp-main.395.pdf", "details": "X Hu, D Ru, L Qiu, Q Guo, T Zhang, Y Xu, Y Luo, P Liu\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate. This paper presents RefChecker, a framework that introduces claim-triplets to represent claims in LLM responses \u2026"}]
