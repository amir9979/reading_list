[{"title": "DPDLLM: A Black-box Framework for Detecting Pre-training Data from Large Language Models", "link": "https://aclanthology.org/2024.findings-acl.35.pdf", "details": "B Zhou, Z Wang, L Wang, H Wang, Y Zhang, K Song\u2026 - Findings of the Association \u2026, 2024", "abstract": "The success of large language models (LLM) benefits from large-scale model parameters and large amounts of pre-training data. However, the textual data for training LLM can not be confirmed to be legal because they are crawled from \u2026"}, {"title": "Fact Finder--Enhancing Domain Expertise of Large Language Models by Incorporating Knowledge Graphs", "link": "https://arxiv.org/pdf/2408.03010", "details": "D Steinigen, R Teucher, TH Ruland, M Rudat\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in Large Language Models (LLMs) have showcased their proficiency in answering natural language queries. However, their effectiveness is hindered by limited domain-specific knowledge, raising concerns about the reliability \u2026"}, {"title": "Continual Few-shot Relation Extraction via Adaptive Gradient Correction and Knowledge Decomposition", "link": "https://aclanthology.org/2024.findings-acl.702.pdf", "details": "J Hu, C Tan, J Xu, XK XiangyunKong - Findings of the Association for Computational \u2026, 2024", "abstract": "Continual few-shot relation extraction (CFRE) aims to continually learn new relations with limited samples. However, current methods neglect the instability of embeddings in the process of different task training, which leads to serious catastrophic forgetting \u2026"}]
