[{"title": "Online Explainable Forecasting using Regions of Competence", "link": "https://ceur-ws.org/Vol-3761/extendedabstract2.pdf", "details": "A Saadallah, M Jakobs - 2024", "abstract": "Several machine learning models have been applied to time series forecasting. However, it is generally acknowledged that none of these models is universally valid for every application and over time. This is due to the complex and changing nature \u2026"}, {"title": "Interpretable and Fair Mechanisms for Abstaining Classifiers", "link": "https://link.springer.com/chapter/10.1007/978-3-031-70368-3_25", "details": "D Lenders, A Pugnana, R Pellungrini, T Calders\u2026 - Joint European Conference \u2026, 2024", "abstract": "Abstaining classifiers have the option to refrain from providing a prediction for instances that are difficult to classify. The abstention mechanism is designed to trade off the classifier's performance on the accepted data while ensuring a minimum \u2026"}, {"title": "GRACE: Graph-Based Contextual Debiasing for Fair Visual Question Answering", "link": "https://www-users.cse.umn.edu/~qzhao/publications/pdf/zhang2024grace.pdf", "details": "Y Zhang, M Jiang, Q Zhao", "abstract": "Large language models (LLMs) exhibit exceptional reasoning capabilities and have played significant roles in knowledge-based visual question-answering (VQA) systems. By conditioning on in-context examples and task-specific prompts, they \u2026"}, {"title": "Score Forgetting Distillation: A Swift, Data-Free Method for Machine Unlearning in Diffusion Models", "link": "https://arxiv.org/pdf/2409.11219", "details": "T Chen, S Zhang, M Zhou - arXiv preprint arXiv:2409.11219, 2024", "abstract": "The machine learning community is increasingly recognizing the importance of fostering trust and safety in modern generative AI (GenAI) models. We posit machine unlearning (MU) as a crucial foundation for developing safe, secure, and trustworthy \u2026"}, {"title": "Discffusion: Discriminative Diffusion Models as Few-shot Vision and Language Learners", "link": "https://tsujuifu.github.io/pubs/tmlr24_discffusion.pdf", "details": "XHW Feng, TJ Fu, VJAAP Narayana, S Basu\u2026", "abstract": "Diffusion models, such as Stable Diffusion (Rombach et al., 2022a), have shown incredible performance on text-to-image generation. Since text-to-image generation often requires models to generate visual concepts with fine-grained details and \u2026"}]
