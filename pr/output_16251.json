[{"title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs", "link": "https://arxiv.org/pdf/2504.07866%3F", "details": "Y Yin, W Huang, K Song, Y Tang, X Wu, W Guo, P Guo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing \u2026"}, {"title": "Efficient Tuning of Large Language Models for Knowledge-Grounded Dialogue Generation", "link": "https://arxiv.org/pdf/2504.07754%3F", "details": "B Zhang, H Ma, D Li, J Ding, J Wang, B Xu, HF Lin - arXiv preprint arXiv:2504.07754, 2025", "abstract": "Large language models (LLMs) demonstrate remarkable text comprehension and generation capabilities but often lack the ability to utilize up-to-date or domain- specific knowledge not included in their training data. To address this gap, we \u2026"}]
