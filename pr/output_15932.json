[{"title": "Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation", "link": "https://arxiv.org/pdf/2504.02438", "details": "C Cheng, J Guan, W Wu, R Yan - arXiv preprint arXiv:2504.02438, 2025", "abstract": "Long-form video processing fundamentally challenges vision-language models (VLMs) due to the high computational costs of handling extended temporal sequences. Existing token pruning and feature merging methods often sacrifice \u2026"}, {"title": "PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models", "link": "https://arxiv.org/pdf/2504.08966", "details": "M Dhouib, D Buscaldi, S Vanier, A Shabou - arXiv preprint arXiv:2504.08966, 2025", "abstract": "Visual Language Models require substantial computational resources for inference due to the additional input tokens needed to represent visual information. However, these visual tokens often contain redundant and unimportant information, resulting in \u2026"}, {"title": "Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning", "link": "https://arxiv.org/pdf/2504.05632", "details": "S Kabra, A Jha, C Reddy - arXiv preprint arXiv:2504.05632, 2025", "abstract": "Recent advances in large-scale generative language models have shown that reasoning capabilities can significantly improve model performance across a variety of tasks. However, the impact of reasoning on a model's ability to mitigate \u2026"}, {"title": "Enhancing Multi-task Learning Capability of Medical Generalist Foundation Model via Image-centric Multi-annotation Data", "link": "https://arxiv.org/pdf/2504.09967", "details": "X Zhu, F Mo, Z Zhang, J Wang, Y Shi, M Wu, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The emergence of medical generalist foundation models has revolutionized conventional task-specific model development paradigms, aiming to better handle multiple tasks through joint training on large-scale medical datasets. However, recent \u2026"}, {"title": "Discrete Diffusion Language Model for Efficient Text Summarization", "link": "https://aclanthology.org/2025.findings-naacl.352.pdf", "details": "DA Do, LA Tuan, W Buntine - Findings of the Association for Computational \u2026, 2025", "abstract": "While diffusion models excel at conditionally generating high-quality images, prior works in discrete diffusion models were not evaluated on conditional long-text generation. This work addresses the limitations of prior discrete diffusion models for \u2026"}, {"title": "UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation", "link": "https://arxiv.org/pdf/2504.21336", "details": "L Wu, Y Nie, S He, J Zhuang, H Chen - arXiv preprint arXiv:2504.21336, 2025", "abstract": "Multi-modal interpretation of biomedical images opens up novel opportunities in biomedical image analysis. Conventional AI approaches typically rely on disjointed training, ie, Large Language Models (LLMs) for clinical text generation and \u2026"}, {"title": "ProtHGT: Heterogeneous Graph Transformers for Automated Protein Function Prediction Using Biological Knowledge Graphs and Language Models", "link": "https://www.biorxiv.org/content/10.1101/2025.04.19.649272.full.pdf", "details": "E Ulusoy, T Dogan - bioRxiv, 2025", "abstract": "Motivation: The rapid accumulation of protein sequence data, coupled with the slow pace of experimental annotations, creates a critical need for computational methods to predict protein functions. Existing models often rely on limited data types, such as \u2026"}, {"title": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items", "link": "https://arxiv.org/pdf/2504.11186", "details": "M Zou, S Srinivasan, TWS Lo, K Zou, GD Yang, X Ai\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in reasoning-focused large language models (LLMs) mark a shift from general LLMs toward models designed for complex decision-making, a crucial aspect in medicine. However, their performance in specialized domains like \u2026"}, {"title": "Knowledge-Instruct: Effective Continual Pre-training from Limited Data using Instructions", "link": "https://arxiv.org/pdf/2504.05571", "details": "O Ovadia, M Brief, R Lemberg, E Sheetrit - arXiv preprint arXiv:2504.05571, 2025", "abstract": "While Large Language Models (LLMs) acquire vast knowledge during pre-training, they often lack domain-specific, new, or niche information. Continual pre-training (CPT) attempts to address this gap but suffers from catastrophic forgetting and \u2026"}]
