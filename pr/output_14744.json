[{"title": "ASSESSING ROBUSTNESS TO SPURIOUS CORRELATIONS IN POST-TRAINING LANGUAGE MODELS", "link": "https://openreview.net/pdf%3Fid%3D5FUmGAZZ5w", "details": "J Shuieh, P Singhal, A Shanker, J Heyer, G Pu\u2026 - Workshop on Spurious Correlation \u2026", "abstract": "Supervised and preference-based fine-tuning techniques have become popular for aligning large language models (LLMs) with user intent and correctness criteria. However, real-world training data often exhibits spurious correlations\u2014arising from \u2026"}, {"title": "Continual Multimodal Contrastive Learning", "link": "https://arxiv.org/pdf/2503.14963", "details": "X Liu, X Xia, SK Ng, TS Chua - arXiv preprint arXiv:2503.14963, 2025", "abstract": "Multimodal contrastive learning (MCL) advances in aligning different modalities and generating multimodal representations in a joint space. By leveraging contrastive learning across diverse modalities, large-scale multimodal data enhances \u2026"}, {"title": "Boundary-guided Contrastive Learning for Semi-supervised Medical Image Segmentation", "link": "https://ieeexplore.ieee.org/abstract/document/10946212/", "details": "Y Yang, J Zhuang, G Sun, R Wang, J Su - IEEE Transactions on Medical Imaging, 2025", "abstract": "Semi-supervised learning methods, compared to fully supervised learning, offer significant potential to alleviate the burden of manual annotations on clinicians. By leveraging unlabeled data, these methods can aid in the development of medical \u2026"}, {"title": "Medical foundation large language models for comprehensive text analysis and beyond", "link": "https://www.nature.com/articles/s41746-025-01533-1", "details": "Q Xie, Q Chen, A Chen, C Peng, Y Hu, F Lin, X Peng\u2026 - npj Digital Medicine, 2025", "abstract": "Recent advancements in large language models (LLMs) show significant potential in medical applications but are hindered by limited specialized medical knowledge. We present Me-LLaMA, a family of open-source medical LLMs integrating extensive \u2026"}, {"title": "Leveraging Large Language Models to Develop Heuristics for Emerging Optimization Problems", "link": "https://arxiv.org/pdf/2503.03350%3F", "details": "T B\u00f6mer, N Koltermann, M Disselnmeyer, L D\u00f6rr\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Combinatorial optimization problems often rely on heuristic algorithms to generate efficient solutions. However, the manual design of heuristics is resource-intensive and constrained by the designer's expertise. Recent advances in artificial \u2026"}, {"title": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark", "link": "https://arxiv.org/pdf/2503.20786", "details": "SM Bsharat, M Ranjan, A Myrzakhan, J Liu, B Guo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Rapid advancements in large language models (LLMs) have increased interest in deploying them on mobile devices for on-device AI applications. Mobile users interact differently with LLMs compared to desktop users, creating unique \u2026"}]
