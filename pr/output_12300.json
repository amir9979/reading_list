[{"title": "Complementary Subspace Low-Rank Adaptation of Vision-Language Models for Few-Shot Classification", "link": "https://arxiv.org/pdf/2501.15040", "details": "Z Wang, J Dai, K Li, X Li, Y Guo, M Xiang - arXiv preprint arXiv:2501.15040, 2025", "abstract": "Vision language model (VLM) has been designed for large scale image-text alignment as a pretrained foundation model. For downstream few shot classification tasks, parameter efficient fine-tuning (PEFT) VLM has gained much popularity in the \u2026"}, {"title": "Large Language Models Meet Graph Neural Networks for Text-Numeric Graph Reasoning", "link": "https://arxiv.org/pdf/2501.16361", "details": "H Song, J Feng, G Li, M Province, P Payne, Y Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In real-world scientific discovery, human beings always make use of the accumulated prior knowledge with imagination pick select one or a few most promising hypotheses from large and noisy data analysis results. In this study, we introduce a \u2026"}, {"title": "RelCAT: Advancing Extraction of Clinical Inter-Entity Relationships from Unstructured Electronic Health Records", "link": "https://arxiv.org/pdf/2501.16077", "details": "S Agarwal, V Dinu, T Searle, M Ratas, A Shek, DF Stein\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This study introduces RelCAT (Relation Concept Annotation Toolkit), an interactive tool, library, and workflow designed to classify relations between entities extracted from clinical narratives. Building upon the CogStack MedCAT framework, RelCAT \u2026"}, {"title": "Enhancing Visual Inspection Capability of Multi-Modal Large Language Models on Medical Time Series with Supportive Conformalized and Interpretable Small \u2026", "link": "https://arxiv.org/pdf/2501.16215", "details": "H Li, X Chen, C Zhang, SF Quan, WDS Killgore\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) exhibit remarkable capabilities in visual inspection of medical time-series data, achieving proficiency comparable to human clinicians. However, their broad scope limits domain-specific precision, and proprietary weights \u2026"}, {"title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2501.14851", "details": "MK Chen, X Zhang, D Tao - arXiv preprint arXiv:2501.14851, 2025", "abstract": "Logical reasoning is a critical component of Large Language Models (LLMs), and substantial research efforts in recent years have aimed to enhance their deductive reasoning capabilities. However, existing deductive reasoning benchmarks, which \u2026"}, {"title": "Is Conversational XAI All You Need? Human-AI Decision Making With a Conversational XAI Assistant", "link": "https://arxiv.org/pdf/2501.17546", "details": "G He, N Aishwarya, U Gadiraju - arXiv preprint arXiv:2501.17546, 2025", "abstract": "Explainable artificial intelligence (XAI) methods are being proposed to help interpret and understand how AI systems reach specific predictions. Inspired by prior work on conversational user interfaces, we argue that augmenting existing XAI methods with \u2026"}, {"title": "LCTG Bench: LLM Controlled Text Generation Benchmark", "link": "https://arxiv.org/pdf/2501.15875", "details": "K Kurihara, M Mita, P Zhang, S Sasaki, R Ishigami\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The rise of large language models (LLMs) has led to more diverse and higher-quality machine-generated text. However, their high expressive power makes it difficult to control outputs based on specific business instructions. In response, benchmarks \u2026"}, {"title": "Evaluating and Improving Graph to Text Generation with Large Language Models", "link": "https://arxiv.org/pdf/2501.14497", "details": "J He, Y Yang, W Long, D Xiong, VG Basulto, JZ Pan - arXiv preprint arXiv:2501.14497, 2025", "abstract": "Large language models (LLMs) have demonstrated immense potential across various tasks. However, research for exploring and improving the capabilities of LLMs in interpreting graph structures remains limited. To address this gap, we \u2026"}, {"title": "PSSD: Making Large Language Models Self-denial via Human Psyche Structure", "link": "https://openreview.net/pdf%3Fid%3D80BpkRq6xe", "details": "J Liao, Z Liao, X Zhao - THE WEB CONFERENCE 2025", "abstract": "The enhance of accuracy in reasoning results of LLMs arouses the community's interests, wherein pioneering studies investigate post-hoc strategies to rectify potential mistakes. Despite extensive efforts, they are all stuck in a state of resource \u2026"}]
