[{"title": "Explainable Artificial Intelligence on Biosignals for Clinical Decision Support", "link": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671459", "details": "MC Maurer, JM Metsch, P Hempel, T Bender\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "Deep learning has proven effective in several areas, including computer vision, natural language processing, and disease prediction, which can support clinicians in making decisions along the clinical pathway. However, in order to successfully \u2026"}, {"title": "InstructCoder: Instruction Tuning Large Language Models for Code Editing", "link": "https://aclanthology.org/2024.acl-srw.6.pdf", "details": "K Li, Q Hu, J Zhao, H Chen, Y Xie, T Liu, M Shieh, J He - Proceedings of the 62nd \u2026, 2024", "abstract": "Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data \u2026"}, {"title": "Masked Channel Modeling for Bootstrapping Visual Pre-training", "link": "https://link.springer.com/article/10.1007/s11263-024-02204-6", "details": "Y Liu, X Wang, M Zhu, Y Cao, T Huang, C Shen - International Journal of Computer \u2026, 2024", "abstract": "Large vision models have achieved great success in computer vision recently, eg, CLIP for large-scale image-text contrastive learning. They have prominent potential in representation learning and show strong transfer ability in various downstream \u2026"}, {"title": "FactorLLM: Factorizing Knowledge via Mixture of Experts for Large Language Models", "link": "https://arxiv.org/pdf/2408.11855", "details": "Z Zhao, M Dong, R Zhang, W Zheng, Y Zhang, H Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has demonstrated that Feed-Forward Networks (FFNs) in Large Language Models (LLMs) play a pivotal role in storing diverse linguistic and factual knowledge. Conventional methods frequently face challenges due to knowledge \u2026"}, {"title": "CodeAttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion", "link": "https://aclanthology.org/2024.findings-acl.679.pdf", "details": "Q Ren, C Gao, J Shao, J Yan, X Tan, W Lam, L Ma - Findings of the Association for \u2026, 2024", "abstract": "The rapid advancement of Large Language Models (LLMs) has brought about remarkable generative capabilities but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from \u2026"}]
