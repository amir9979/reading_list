[{"title": "Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents", "link": "https://arxiv.org/pdf/2411.16740", "details": "J Chen, D Xu, J Fei, CM Feng, M Elhoseiny - arXiv preprint arXiv:2411.16740, 2024", "abstract": "Large multimodal models (LMMs) have achieved impressive progress in vision- language understanding, yet they face limitations in real-world applications requiring complex reasoning over a large number of images. Existing benchmarks for multi \u2026"}, {"title": "ACE: Action Concept Enhancement of Video-Language Models in Procedural Videos", "link": "https://arxiv.org/pdf/2411.15628", "details": "R Ghoddoosian, N Agarwal, I Dwivedi, B Darisuh - arXiv preprint arXiv:2411.15628, 2024", "abstract": "Vision-language models (VLMs) are capable of recognizing unseen actions. However, existing VLMs lack intrinsic understanding of procedural action concepts. Hence, they overfit to fixed labels and are not invariant to unseen action synonyms \u2026"}, {"title": "Is' Right'Right? Enhancing Object Orientation Understanding in Multimodal Language Models through Egocentric Instruction Tuning", "link": "https://arxiv.org/pdf/2411.16761", "details": "JH Jung, ET Kim, SY Kim, JH Lee, B Kim, B Chang - arXiv preprint arXiv:2411.16761, 2024", "abstract": "Multimodal large language models (MLLMs) act as essential interfaces, connecting humans with AI technologies in multimodal applications. However, current MLLMs face challenges in accurately interpreting object orientation in images due to \u2026"}, {"title": "COM Kitchens: An Unedited Overhead-View Video Dataset as a Vision-Language Benchmark", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-73650-6_8.pdf", "details": "J Harashima, L Rybicki, Y Fukasawa, Y Ushiku", "abstract": "Procedural video understanding is gaining attention in the vision and language community. Deep learning-based video analysis requires extensive data. Consequently, existing works often use web videos as training resources, making it \u2026"}, {"title": "Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training", "link": "https://arxiv.org/pdf/2411.14318%3F", "details": "Z Luo, X Zhang, X Liu, H Li, Y Gong, C Qi, P Cheng - arXiv preprint arXiv:2411.14318, 2024", "abstract": "It is well-known that a diverse corpus is critical for training large language models, which are typically constructed from a mixture of various domains. In general, previous efforts resort to sampling training data from different domains with static \u2026"}, {"title": "Multimodal Autoregressive Pre-training of Large Vision Encoders", "link": "https://arxiv.org/pdf/2411.14402%3F", "details": "E Fini, M Shukor, X Li, P Dufter, M Klein, D Haldimann\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce a novel method for pre-training of large-scale vision encoders. Building on recent advancements in autoregressive pre-training of vision models, we extend this framework to a multimodal setting, ie, images and text. In this paper, we present \u2026"}, {"title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2411.14432", "details": "Y Dong, Z Liu, HL Sun, J Yang, W Hu, Y Rao, Z Liu - arXiv preprint arXiv:2411.14432, 2024", "abstract": "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high \u2026"}, {"title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models", "link": "https://arxiv.org/pdf/2411.15024%3F", "details": "K Tao, C Qin, H You, Y Sui, H Wang - arXiv preprint arXiv:2411.15024, 2024", "abstract": "Video large language models (VLLMs) have significantly advanced recently in processing complex video content, yet their inference efficiency remains constrained because of the high computational cost stemming from the thousands of visual \u2026"}, {"title": "Evaluating and Advancing Multimodal Large Language Models in Ability Lens", "link": "https://arxiv.org/pdf/2411.14725", "details": "F Chen, C Gou, J Liu, Y Yang, Z Li, J Zhang, Z Sun\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As multimodal large language models (MLLMs) advance rapidly, rigorous evaluation has become essential, providing further guidance for their development. In this work, we focus on a unified and robust evaluation of\\textbf {vision perception} abilities, the \u2026"}]
