[{"title": "Guiding Medical Vision-Language Models with Diverse Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations", "link": "https://aclanthology.org/2025.naacl-long.587.pdf", "details": "K Zhu, Z Qin, H Yi, Z Jiang, Q Lao, S Zhang, K Li - \u2026 of the 2025 Conference of the \u2026, 2025", "abstract": "While mainstream vision-language models (VLMs) have advanced rapidly in understanding image-level information, they still lack the ability to focus on specific areas designated by humans. Rather, they typically rely on large volumes of high \u2026"}, {"title": "DoGA: Enhancing Grounded Object Detection via Grouped Pre-Training with Attributes", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/32603/34758", "details": "Y Liu, F Hou, Y Peng, G Zhang, Y Zhang, D Xie\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "Recent advances in vision-language pre-training have significantly enhanced the model capabilities on grounded object detection. However, these studies often pre- train with coarse-grained text prompts, such as plain category names and brief \u2026"}, {"title": "Vision-language foundation model for generalizable nasal disease diagnosis using unlabeled endoscopic records", "link": "https://www.sciencedirect.com/science/article/pii/S0031320325003061", "details": "X Liu, W Gong, X Chen, Z Li, Y Liu, L Wang, Q Liu\u2026 - Pattern Recognition, 2025", "abstract": "Medical artificial intelligence (AI) holds significant potential in identifying signs of health conditions in nasal endoscopic images, thereby accelerating the diagnosis of diseases and systemic disorders. However, the performance of AI models heavily \u2026"}, {"title": "Prototype Tuning: A Meta-Learning Approach for Few-Shot Document-Level Relation Extraction with Large Language Models", "link": "https://aclanthology.org/2025.findings-naacl.62.pdf", "details": "D Pan, Y Sun, B Xu, J Li, Z Yang, L Luo, H Lin, J Wang - Findings of the Association \u2026, 2025", "abstract": "Abstract Few-Shot Document-Level Relation Extraction (FSDLRE) aims to develop models capable of generalizing to new categories with minimal support examples. Although Large Language Models (LLMs) demonstrate exceptional In-Context \u2026"}, {"title": "Representation Space Augmentation for Effective Self-Supervised Learning on Tabular Data", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/33265/35420", "details": "M Eo, K Lee, HS Cho, D Kim, YS Sim, W Lim - \u2026 of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Tabular data, widely used across industries, remains underexplored in deep learning. Self-supervised learning (SSL) shows promise for pre-training deep neural networks (DNNs) on tabular data, but its potential is hindered by challenges in \u2026"}, {"title": "Dynamic Entity-Masked Graph Diffusion Model for Histopathology Image Representation Learning", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/33202/35357", "details": "Z Zhuang, M Cen, Y Li, F Zhou, L Yu, B Magnier\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "Significant disparities between the features of natural images and those inherent to histopathological images make it challenging to directly apply and transfer pre- trained models from natural images to histopathology tasks. Moreover, the frequent \u2026"}, {"title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability", "link": "https://arxiv.org/pdf/2504.07416%3F", "details": "J Park, S Kim, B Yoon, K Choi - arXiv preprint arXiv:2504.07416, 2025", "abstract": "Recent advancements in multi-modal models have significantly improved vision- language alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning, rely on low-resolution \u2026"}, {"title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?", "link": "https://arxiv.org/pdf/2504.00509", "details": "K Yan, Y Xu, Z Du, X Yao, Z Wang, X Guo, J Chen - arXiv preprint arXiv:2504.00509, 2025", "abstract": "The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' \u2026"}, {"title": "Multi-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation", "link": "https://arxiv.org/pdf/2504.18856", "details": "S Albastaki, A Sohail, II Ganapathi, B Alawode, A Khan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In Computational Pathology (CPath), the introduction of Vision-Language Models (VLMs) has opened new avenues for research, focusing primarily on aligning image- text pairs at a single magnification level. However, this approach might not be \u2026"}]
