[{"title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models", "link": "https://arxiv.org/pdf/2406.14852", "details": "J Wang, Y Ming, Z Shi, V Vineet, X Wang, N Joshi - arXiv preprint arXiv:2406.14852, 2024", "abstract": "Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoning--a fundamental \u2026"}, {"title": "A self-supervised framework for abnormality detection from brain MRI", "link": "https://www.authorea.com/doi/pdf/10.22541/au.171900128.89392883", "details": "D Wood, E Guilhem, S Kafiabadi, A Al Busaidi\u2026 - Authorea Preprints, 2024", "abstract": "AbstractArtificial neural networks trained on large, expert-labelled datasets are considered state-of-the-art for a range of medical image recognition tasks. However, categorically labelled datasets are time-consuming to generate and constrain \u2026"}, {"title": "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning", "link": "https://arxiv.org/pdf/2406.16078", "details": "Y Aoki, K Kudo, T Kuribayashi, S Sone, M Taniguchi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multi-step reasoning is widely adopted in the community to explore the better performance of language models (LMs). We report on the systematic strategy that LMs use in this process. Our controlled experiments reveal that LMs rely more \u2026"}, {"title": "Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level", "link": "https://arxiv.org/pdf/2406.11817", "details": "J Liu, Z Zhou, J Liu, X Bu, C Yang, HS Zhong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Direct Preference Optimization (DPO), a standard method for aligning language models with human preferences, is traditionally applied to offline preferences. Recent studies show that DPO benefits from iterative training with online preferences \u2026"}, {"title": "Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations", "link": "https://arxiv.org/pdf/2406.11801", "details": "R Hazra, S Layek, S Banerjee, S Poria - arXiv preprint arXiv:2406.11801, 2024", "abstract": "Ensuring the safe alignment of large language models (LLMs) with human values is critical as they become integral to applications like translation and question answering. Current alignment methods struggle with dynamic user intentions and \u2026"}]
