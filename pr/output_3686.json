[{"title": "Disentangled Prompt Representation for Domain Generalization", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Cheng_Disentangled_Prompt_Representation_for_Domain_Generalization_CVPR_2024_paper.pdf", "details": "D Cheng, Z Xu, X Jiang, N Wang, D Li, X Gao - \u2026 of the IEEE/CVF Conference on \u2026, 2024", "abstract": "Abstract Domain Generalization (DG) aims to develop a versatile model capable of performing well on unseen target domains. Recent advancements in pre-trained Visual Foundation Models (VFMs) such as CLIP show significant potential in \u2026"}, {"title": "Explainable Image Recognition via Enhanced Slot-attention Based Classifier", "link": "https://arxiv.org/abs/2407.05616", "details": "B Wang, L Li, J Zhang, Y Nakashima, H Nagahara - arXiv preprint arXiv:2407.05616, 2024", "abstract": "The imperative to comprehend the behaviors of deep learning models is of utmost importance. In this realm, Explainable Artificial Intelligence (XAI) has emerged as a promising avenue, garnering increasing interest in recent years. Despite this, most \u2026"}, {"title": "Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning with Interventional Feedback", "link": "https://aclanthology.org/2024.naacl-long.262.pdf", "details": "Y Xia, T Yu, Z He, H Zhao, J McAuley, S Li - Proceedings of the 2024 Conference of \u2026, 2024", "abstract": "Large language models (LLMs) often generate biased outputs containing offensive, toxic, or stereotypical text. Existing LLM alignment methods such as reinforcement learning from human feedback (RLHF) alleviate biases primarily based on reward \u2026"}, {"title": "Unsupervised State Encoding in Video Sequences Using \u03b2-Variational Autoencoders", "link": "https://link.springer.com/chapter/10.1007/978-3-031-64881-6_9", "details": "S Mulder, MC Du Plessis - Annual Conference of South African Institute of \u2026, 2024", "abstract": "Monitoring and providing feedback on the execution of sequential tasks is common in various domains, such as industrial processes for quality control, automated supervision for skill acquisition and even surgical procedures. This research \u2026"}, {"title": "DMTG: One-Shot Differentiable Multi-Task Grouping", "link": "https://arxiv.org/pdf/2407.05082", "details": "Y Gao, S Jiang, M Li, JG Yu, GS Xia - arXiv preprint arXiv:2407.05082, 2024", "abstract": "We aim to address Multi-Task Learning (MTL) with a large number of tasks by Multi- Task Grouping (MTG). Given N tasks, we propose to simultaneously identify the best task groups from 2^ N candidates and train the model weights simultaneously in one \u2026"}, {"title": "Efficient and Long-Tailed Generalization for Pre-trained Vision-Language Model", "link": "https://arxiv.org/pdf/2406.12638", "details": "JX Shi, C Zhang, T Wei, YF Li - arXiv preprint arXiv:2406.12638, 2024", "abstract": "Pre-trained vision-language models like CLIP have shown powerful zero-shot inference ability via image-text matching and prove to be strong few-shot learners in various downstream tasks. However, in real-world scenarios, adapting CLIP to \u2026"}, {"title": "OpenVLA: An Open-Source Vision-Language-Action Model", "link": "https://arxiv.org/pdf/2406.09246", "details": "MJ Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such \u2026"}]
