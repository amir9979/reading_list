[{"title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "link": "https://arxiv.org/pdf/2504.14194", "details": "X Zhuang, J Peng, R Ma, Y Wang, T Bai, X Wei, J Qiu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural \u2026"}, {"title": "Guiding Reasoning in Small Language Models with LLM Assistance", "link": "https://arxiv.org/pdf/2504.09923", "details": "Y Kim, E Yi, M Kim, SY Yun, T Kim - arXiv preprint arXiv:2504.09923, 2025", "abstract": "The limited reasoning capabilities of small language models (SLMs) cast doubt on their suitability for tasks demanding deep, multi-step logical deduction. This paper introduces a framework called Small Reasons, Large Hints (SMART), which \u2026"}, {"title": "Relevant answers to polar questions", "link": "https://files.osf.io/v1/resources/8tnfd_v1/providers/osfstorage/68010a06c2a0ba48b4cc446c%3Faction%3Ddownload%26direct%26version%3D1", "details": "R Hawkins, P Tsvilodub, CA Bergey, ND Goodman\u2026", "abstract": "Imagine you are working as a barista at a coffeeshop. A customer asks \u201cDo you have iced tea?\u201d but you've run out. What do you say? They have asked a polar question (also known as a yes-no question), so in principle you should be able to give a \u2026"}, {"title": "FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models", "link": "https://arxiv.org/pdf/2504.14690", "details": "M Shamsfard, Z Saaberi, SMH Hashemi, Z Vatankhah\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Research on evaluating and analyzing large language models (LLMs) has been extensive for resource-rich languages such as English, yet their performance in languages such as Persian has received considerably less attention. This paper \u2026"}]
