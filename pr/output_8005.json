[{"title": "Bayesian Concept Bottleneck Models with LLM Priors", "link": "https://arxiv.org/pdf/2410.15555", "details": "J Feng, A Kothari, L Zier, C Singh, YS Tan - arXiv preprint arXiv:2410.15555, 2024", "abstract": "Concept Bottleneck Models (CBMs) have been proposed as a compromise between white-box and black-box models, aiming to achieve interpretability without sacrificing accuracy. The standard training procedure for CBMs is to predefine a candidate set \u2026"}, {"title": "Self-Supervised Molecular Representation Learning With Topology and Geometry", "link": "https://ieeexplore.ieee.org/abstract/document/10715653/", "details": "X Zang, J Zhang, B Tang - IEEE Journal of Biomedical and Health Informatics, 2024", "abstract": "Molecular representation learning is of great importance for drug molecular analysis. The development in molecular representation learning has demonstrated great promise through self-supervised pre-training strategy to overcome the scarcity of \u2026"}, {"title": "Interpretable end-to-end Neurosymbolic Reinforcement Learning agents", "link": "https://arxiv.org/pdf/2410.14371", "details": "N Grandien, Q Delfosse, K Kersting - arXiv preprint arXiv:2410.14371, 2024", "abstract": "Deep reinforcement learning (RL) agents rely on shortcut learning, preventing them from generalizing to slightly different environments. To address this problem, symbolic method, that use object-centric states, have been developed. However \u2026"}, {"title": "Self-supervised contrastive learning performs non-linear system identification", "link": "https://arxiv.org/pdf/2410.14673", "details": "RG Laiz, T Schmidt, S Schneider - arXiv preprint arXiv:2410.14673, 2024", "abstract": "Self-supervised learning (SSL) approaches have brought tremendous success across many tasks and domains. It has been argued that these successes can be attributed to a link between SSL and identifiable representation learning: Temporal \u2026"}, {"title": "Mitigating Embedding Collapse in Diffusion Models for Categorical Data", "link": "https://arxiv.org/pdf/2410.14758", "details": "B Nguyen, Y Takida, N Murata, T Uesaka, S Ermon\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Latent diffusion models have enabled continuous-state diffusion models to handle a variety of datasets, including categorical data. However, most methods rely on fixed pretrained embeddings, limiting the benefits of joint training with the diffusion model \u2026"}, {"title": "On Partial Prototype Collapse in the DINO Family of Self-Supervised Methods", "link": "https://arxiv.org/pdf/2410.14060", "details": "H Govindarajan, P Sid\u00e9n, J Roll, F Lindsten - arXiv preprint arXiv:2410.14060, 2024", "abstract": "A prominent self-supervised learning paradigm is to model the representations as clusters, or more generally as a mixture model. Learning to map the data samples to compact representations and fitting the mixture model simultaneously leads to the \u2026"}]
