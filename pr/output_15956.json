[{"title": "Leveraging long context in retrieval augmented language models for medical question answering", "link": "https://www.nature.com/articles/s41746-025-01651-w", "details": "G Zhang, Z Xu, Q Jin, F Chen, Y Fang, Y Liu\u2026 - npj Digital Medicine, 2025", "abstract": "While holding great promise for improving and facilitating healthcare through applications of medical literature summarization, large language models (LLMs) struggle to produce up-to-date responses on evolving topics due to outdated \u2026"}, {"title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "link": "https://arxiv.org/pdf/2504.14194", "details": "X Zhuang, J Peng, R Ma, Y Wang, T Bai, X Wei, J Qiu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural \u2026"}, {"title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "link": "https://arxiv.org/pdf/2504.14366", "details": "P Haller, J Golde, A Akbik - arXiv preprint arXiv:2504.14366, 2025", "abstract": "Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures \u2026"}, {"title": "ScriptSmith: A Unified LLM Framework for Enhancing IT Operations via Automated Bash Script Generation, Assessment, and Refinement", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/35147/37302", "details": "P Aggarwal, O Chatterjee, T Dai, S Samanta\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "In the rapidly evolving landscape of site reliability engineering (SRE), the demand for efficient and effective solutions to manage and resolve issues in site and cloud applications is paramount. This paper presents an innovative approach to action \u2026"}, {"title": "Can Long-Context Language Models Solve Repository-Level Code Generation?", "link": "https://openreview.net/pdf%3Fid%3DpmcWo9DtDw", "details": "Y PENG, ZZ Wang, D Fried - LTI Student Research Symposium 2025", "abstract": "With the advance of real-world tasks that necessitate increasingly long contexts, recent language models (LMs) have begun to support longer context windows. One particularly complex task is repository-level code generation, where retrieval \u2026"}, {"title": "Benchmarking large language models for biomedical natural language processing applications and recommendations", "link": "https://www.nature.com/articles/s41467-025-56989-2", "details": "Q Chen, Y Hu, X Peng, Q Xie, Q Jin, A Gilson\u2026 - Nature Communications, 2025", "abstract": "The rapid growth of biomedical literature poses challenges for manual knowledge curation and synthesis. Biomedical Natural Language Processing (BioNLP) automates the process. While Large Language Models (LLMs) have shown promise \u2026"}, {"title": "Efficient Tuning of Large Language Models for Knowledge-Grounded Dialogue Generation", "link": "https://arxiv.org/pdf/2504.07754%3F", "details": "B Zhang, H Ma, D Li, J Ding, J Wang, B Xu, HF Lin - arXiv preprint arXiv:2504.07754, 2025", "abstract": "Large language models (LLMs) demonstrate remarkable text comprehension and generation capabilities but often lack the ability to utilize up-to-date or domain- specific knowledge not included in their training data. To address this gap, we \u2026"}, {"title": "Leveraging Generative Pre-trained Transformer (GPT) Large Language Models (LLMs) For Interstitial Lung Diseases (ILD) Clinical Research", "link": "https://www.atsjournals.org/doi/abs/10.1164/ajrccm.2025.211.Abstracts.A2086", "details": "S Chen, MV Maddali, C Bluethgen, CP Langlotz, R Raj - American Journal of \u2026, 2025", "abstract": "Rationale: The majority of clinically relevant data is contained in unstructured text such as clinical notes. ILD notes are particularly prone to verbosity and imprecision, making structured data extraction a major bottleneck for clinical research and a costly \u2026"}, {"title": "Has the Creativity of Large-Language Models peaked? An analysis of inter-and intra-LLM variability", "link": "https://arxiv.org/pdf/2504.12320", "details": "J Haase, PHP Hanel, S Pokutta - arXiv preprint arXiv:2504.12320, 2025", "abstract": "Following the widespread adoption of ChatGPT in early 2023, numerous studies reported that large language models (LLMs) can match or even surpass human performance in creative tasks. However, it remains unclear whether LLMs have \u2026"}]
