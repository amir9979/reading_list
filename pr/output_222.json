'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Clinical information extraction for Low-resource langu'
[{"title": "Genetic Auto-prompt Learning for Pre-trained Code Intelligence Language Models", "link": "https://arxiv.org/pdf/2403.13588", "details": "C Feng, Y Sun, K Li, P Zhou, J Lv, A Lu - arXiv preprint arXiv:2403.13588, 2024", "abstract": "As Pre-trained Language Models (PLMs), a popular approach for code intelligence, continue to grow in size, the computational cost of their usage has become prohibitively expensive. Prompt learning, a recent development in the field of natural \u2026"}, {"title": "Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models", "link": "https://arxiv.org/html/2402.15637v1", "details": "Y Xiang, H Yan, L Gui, Y He - arXiv preprint arXiv:2402.15637, 2024", "abstract": "In-context learning has become a popular paradigm in natural language processing. However, its performance can be significantly influenced by the order of in-context demonstration examples. In this paper, we found that causal language models \u2026"}, {"title": "Grounding Language Models for Visual Entity Recognition", "link": "https://arxiv.org/pdf/2402.18695", "details": "Z Xiao, M Gong, P Cascante-Bonilla, X Zhang, J Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of \u2026"}, {"title": "\" My Answer is C\": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models", "link": "https://arxiv.org/pdf/2402.14499", "details": "X Wang, B Ma, C Hu, L Weber-Genzel, P R\u00f6ttger\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The \u2026"}, {"title": "Foresight\u2014a generative pretrained transformer for modelling of patient timelines using electronic health records: a retrospective modelling study", "link": "https://www.thelancet.com/journals/landig/article/PIIS2589-7500\\(24\\)00025-6/fulltext", "details": "Z Kraljevic, D Bean, A Shek, R Bendayan\u2026 - The Lancet Digital Health, 2024", "abstract": "Background An electronic health record (EHR) holds detailed longitudinal information about a patient's health status and general clinical history, a large portion of which is stored as unstructured, free text. Existing approaches to model a patient's \u2026"}, {"title": "Enhancing patient representation learning from electronic health records through predicted family relations", "link": "https://www.medrxiv.org/content/10.1101/2024.03.12.24304163.full.pdf", "details": "X Huang, J Arora, AM Erzurumluoglu, D Lam\u2026 - medRxiv, 2024", "abstract": "Artificial intelligence and machine learning are powerful tools in analyzing electronic health records (EHRs) for healthcare research. Despite the recognized importance of family health history, in healthcare research individual patients are often treated as \u2026"}, {"title": "OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization", "link": "https://arxiv.org/pdf/2403.12983", "details": "X Meng, S Ibrahim, K Behdin, H Hazimeh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Structured pruning is a promising approach for reducing the inference costs of large vision and language models. By removing carefully chosen structures, eg, neurons or attention heads, the improvements from this approach can be realized on standard \u2026"}, {"title": "Advancing entity recognition in biomedicine via instruction tuning of large language models", "link": "https://academic.oup.com/bioinformatics/advance-article-pdf/doi/10.1093/bioinformatics/btae163/57056267/btae163.pdf", "details": "VK Keloth, Y Hu, Q Xie, X Peng, Y Wang, A Zheng\u2026 - Bioinformatics, 2024", "abstract": "Abstract Motivation Large Language Models (LLMs) have the potential to revolutionize the field of Natural Language Processing (NLP), excelling not only in text generation and reasoning tasks but also in their ability for zero/few-shot learning \u2026"}, {"title": "EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings", "link": "https://arxiv.org/pdf/2402.16040", "details": "S Kweon, J Kim, H Kwak, D Cha, H Yoon, K Kim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three \u2026"}]
