[{"title": "Probing the limitations of multimodal language models for chemistry and materials research", "link": "https://arxiv.org/pdf/2411.16955", "details": "N Alampara, M Schilling-Wilhelmi, M R\u00edos-Garc\u00eda\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in artificial intelligence have sparked interest in scientific assistants that could support researchers across the full spectrum of scientific workflows, from literature review to experimental design and data analysis. A key \u2026"}, {"title": "Model selection and inference for estimation of causal parameters", "link": "https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-18/issue-2/Model-selection-and-inference-for-estimation-of-causal-parameters/10.1214/24-EJS2308.pdf", "details": "D Rothenh\u00e4usler - Electronic Journal of Statistics, 2024", "abstract": "In causal inference there are often multiple reasonable estimators for a given target quantity. For example, one may reasonably use inverse probability weighting, an instrumental variables approach, or construct an estimate based on proxy outcomes \u2026"}, {"title": "Training and Evaluating Language Models with Template-based Data Generation", "link": "https://arxiv.org/pdf/2411.18104", "details": "Y Zhang - arXiv preprint arXiv:2411.18104, 2024", "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, these \u2026"}, {"title": "MICD: More intra-class diversity in few-shot text classification with many classes", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124014850", "details": "G Jang, HJ Jeong, MY Yi - Knowledge-Based Systems, 2024", "abstract": "Few-shot learning has gained much interest and achieved remarkable performance in handling limited data scenarios. However, existing few-shot text classification methods typically aim at classifying a limited number of classes, usually ranging from \u2026"}, {"title": "Enhancing LLM Reasoning via Critique Models with Test-Time and Training-Time Supervision", "link": "https://arxiv.org/pdf/2411.16579", "details": "Z Xi, D Yang, J Huang, J Tang, G Li, Y Ding, W He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Training large language models (LLMs) to spend more time thinking and reflection before responding is crucial for effectively solving complex reasoning tasks in fields such as science, coding, and mathematics. However, the effectiveness of \u2026"}]
