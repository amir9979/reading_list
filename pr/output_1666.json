'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Self-Refine Instruction-Tuning for Aligning Reasoning '
[{"title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning", "link": "https://arxiv.org/pdf/2404.17140", "details": "Y Zhang, M Khalifa, L Logeswaran, J Kim, M Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors. This work explores whether \u2026"}, {"title": "Causal Evaluation of Language Models", "link": "https://arxiv.org/pdf/2405.00622", "details": "S Chen, B Peng, M Chen, R Wang, M Xu, X Zeng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for \u2026"}, {"title": "Path-Aware Cross-Attention Network for Question Answering", "link": "https://link.springer.com/chapter/10.1007/978-981-97-2253-2_9", "details": "Z Luo, Y Xiong, B Tang - Pacific-Asia Conference on Knowledge Discovery and \u2026, 2024", "abstract": "Abstract Reasoning is an essential ability in QA systems, and the integration of this ability into QA systems has been the subject of considerable research. A prevalent strategy involves incorporating domain knowledge graphs using Graph Neural \u2026"}, {"title": "Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation", "link": "https://arxiv.org/pdf/2404.17489", "details": "W Cui, R Hosseinzadeh, J Ma, T Wu, Y Sui, K Golestan - arXiv preprint arXiv \u2026, 2024", "abstract": "Contrastive learning is a model pre-training technique by first creating similar views of the original data, and then encouraging the data and its corresponding views to be close in the embedding space. Contrastive learning has witnessed success in image \u2026"}, {"title": "Large Language Models for Scientific Question Answering: An Extensive Analysis of the SciQA Benchmark", "link": "https://link.springer.com/chapter/10.1007/978-3-031-60626-7_11", "details": "J Lehmann, A Meloni, E Motta, F Osborne\u2026 - European Semantic Web \u2026, 2024", "abstract": "The SciQA benchmark for scientific question answering aims to represent a challenging task for next-generation question-answering systems on which vanilla large language models fail. In this article, we provide an analysis of the performance \u2026"}, {"title": "Zero-shot LLM-guided Counterfactual Generation for Text", "link": "https://arxiv.org/pdf/2405.04793", "details": "A Bhattacharjee, R Moraffah, J Garland, H Liu - arXiv preprint arXiv:2405.04793, 2024", "abstract": "Counterfactual examples are frequently used for model development and evaluation in many natural language processing (NLP) tasks. Although methods for automated counterfactual generation have been explored, such methods depend on models \u2026"}, {"title": "Causal Diffusion Autoencoders: Toward Counterfactual Generation via Diffusion Probabilistic Models", "link": "https://arxiv.org/pdf/2404.17735", "details": "A Komanduri, C Zhao, F Chen, X Wu - arXiv preprint arXiv:2404.17735, 2024", "abstract": "Diffusion probabilistic models (DPMs) have become the state-of-the-art in high- quality image generation. However, DPMs have an arbitrary noisy latent space with no interpretable or controllable semantics. Although there has been significant \u2026"}, {"title": "Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class", "link": "https://arxiv.org/pdf/2404.16717", "details": "M Moayeri, M Rabbat, M Ibrahim, D Bouchacourt - arXiv preprint arXiv:2404.16717, 2024", "abstract": "Vision-language models enable open-world classification of objects without the need for any retraining. While this zero-shot paradigm marks a significant advance, even today's best models exhibit skewed performance when objects are dissimilar from \u2026"}, {"title": "MCS-SQL: Leveraging Multiple Prompts and Multiple-Choice Selection For Text-to-SQL Generation", "link": "https://arxiv.org/pdf/2405.07467", "details": "D Lee, C Park, J Kim, H Park - arXiv preprint arXiv:2405.07467, 2024", "abstract": "Recent advancements in large language models (LLMs) have enabled in-context learning (ICL)-based methods that significantly outperform fine-tuning approaches for text-to-SQL tasks. However, their performance is still considerably lower than that \u2026"}]
