[{"title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models", "link": "https://arxiv.org/pdf/2502.17387", "details": "A Albalak, D Phung, N Lile, R Rafailov, K Gandhi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written \u2026"}, {"title": "CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport Alignment for Language Models with Different Tokenizers", "link": "https://arxiv.org/pdf/2502.16806", "details": "AD Le, T Vu, NL Hai, NTN Diep, LN Van, T Le\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) achieve state-of-the-art performance across various NLP tasks but face deployment challenges due to high computational costs and memory constraints. Knowledge distillation (KD) is a promising solution, transferring \u2026"}, {"title": "Mapping 1,000+ Language Models via the Log-Likelihood Vector", "link": "https://arxiv.org/pdf/2502.16173", "details": "M Oyama, H Yamagiwa, Y Takase, H Shimodaira - arXiv preprint arXiv:2502.16173, 2025", "abstract": "To compare autoregressive language models at scale, we propose using log- likelihood vectors computed on a predefined text set as model features. This approach has a solid theoretical basis: when treated as model coordinates, their \u2026"}, {"title": "Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation", "link": "https://ieeexplore.ieee.org/iel8/10887540/10887541/10888998.pdf", "details": "D Lee, TM Luu, Y Lee, CD Yoo - ICASSP 2025-2025 IEEE International Conference \u2026, 2025", "abstract": "Recent research highlights the potential of multi-modal foundation models in tackling complex decision-making challenges. However, their large parameters make real- world deployment resource-intensive and often impractical for constrained systems \u2026"}, {"title": "A Weighted Cross-entropy Loss for Mitigating LLM Hallucinations in Cross-lingual Continual Pretraining", "link": "https://ieeexplore.ieee.org/abstract/document/10888877/", "details": "Y Fan, R Li, G Zhang, C Shi, X Wang - \u2026 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "Recently, due to the explosive advances of large language models (LLMs) on English, cross-lingual continual pretraining has been widely applied in obtaining Chinese LLMs. However, previous studies showed that these LLMs have suffered \u2026"}, {"title": "Less is More: Improving LLM Alignment via Preference Data Selection", "link": "https://arxiv.org/pdf/2502.14560", "details": "X Deng, H Zhong, R Ai, F Feng, Z Wang, X He - arXiv preprint arXiv:2502.14560, 2025", "abstract": "Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from \u2026"}, {"title": "Transparency in Disease Diagnosis: Leveraging Interpretable Machine Learning in Healthcare", "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/9781394249312.ch6", "details": "IU Haq, AH Rather, SZ Rufai, A Shah, Sheetal\u2026 - \u2026 Artificial Intelligence in the \u2026, 2025", "abstract": "This chapter delves into the critical integration of interpretable machine learning (ML) models for disease diagnosis within the healthcare sector aligning with the overarching theme of transparency and accountability in AI\u2010driven healthcare \u2026"}, {"title": "HuDEx: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM responses", "link": "https://arxiv.org/pdf/2502.08109", "details": "S Lee, H Lee, S Heo, W Choi - arXiv preprint arXiv:2502.08109, 2025", "abstract": "Recent advances in large language models (LLMs) have shown promising improvements, often surpassing existing methods across a wide range of downstream tasks in natural language processing. However, these models still face \u2026"}, {"title": "Uncertainty-aware fusion: An ensemble framework for mitigating hallucinations in large language models", "link": "https://www.amazon.science/publications/uncertainty-aware-fusion-an-ensemble-framework-for-mitigating-hallucinations-in-large-language-models", "details": "P Dey, S Merugu, SS Kaveri - 2025", "abstract": "ABSTRACT Large Language Models (LLMs) are known to hallucinate and generate non-factual outputs which can undermine user trust. Traditional methods to directly mitigate hallucinations, such as representation editing and contrastive decoding \u2026"}]
