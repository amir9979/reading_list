'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Verifiable by Design: Aligning Language Models to Quot'
[{"title": "Investigating Regularization of Self-Play Language Models", "link": "https://arxiv.org/html/2404.04291v1", "details": "R Alami, A Abubaker, M Achab, MEA Seddik, S Lahlou - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper explores the effects of various forms of regularization in the context of language model alignment via self-play. While both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) require to collect \u2026"}, {"title": "Inferring the Phylogeny of Large Language Models and Predicting their Performances in Benchmarks", "link": "https://arxiv.org/html/2404.04671v1", "details": "N Yax, PY Oudeyer, S Palminteri - arXiv preprint arXiv:2404.04671, 2024", "abstract": "This paper introduces PhyloLM, a method applying phylogenetic algorithms to Large Language Models to explore their finetuning relationships, and predict their performance characteristics. By leveraging the phylogenetic distance metric, we \u2026"}, {"title": "Seme at semeval-2024 task 2: Comparing masked and generative language models on natural language inference for clinical trials", "link": "https://arxiv.org/pdf/2404.03977", "details": "M Aguiar, P Zweigenbaum, N Naderi - arXiv preprint arXiv:2404.03977, 2024", "abstract": "This paper describes our submission to Task 2 of SemEval-2024: Safe Biomedical Natural Language Inference for Clinical Trials. The Multi-evidence Natural Language Inference for Clinical Trial Data (NLI4CT) consists of a Textual Entailment (TE) task \u2026"}, {"title": "A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A Knowledge-Based Perspective", "link": "https://arxiv.org/pdf/2403.16137", "details": "Z Zhao, Y Li, Y Zou, R Li, R Zhang - arXiv preprint arXiv:2403.16137, 2024", "abstract": "Graph self-supervised learning is now a go-to method for pre-training graph foundation models, including graph neural networks, graph transformers, and more recent large language model (LLM)-based graph models. There is a wide variety of \u2026"}, {"title": "Few-Shot Adversarial Prompt Learning on Vision-Language Models", "link": "https://arxiv.org/html/2403.14774v1", "details": "Y Zhou, X Xia, Z Lin, B Han, T Liu - arXiv preprint arXiv:2403.14774, 2024", "abstract": "The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by \u2026"}, {"title": "Relation Extraction Using Large Language Models: A Case Study on Acupuncture Point Locations", "link": "https://arxiv.org/pdf/2404.05415", "details": "Y Li, X Peng, J Li, X Zuo, S Peng, D Pei, C Tao, H Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In acupuncture therapy, the accurate location of acupoints is essential for its effectiveness. The advanced language understanding capabilities of large language models (LLMs) like Generative Pre-trained Transformers (GPT) present a significant \u2026"}, {"title": "Chinese Sequence Labeling with Semi-Supervised Boundary-Aware Language Model Pre-training", "link": "https://arxiv.org/html/2404.05560v1", "details": "L Zhang, D Long, M Zhang, Y Zhang, P Xie, M Zhang - arXiv preprint arXiv \u2026, 2024", "abstract": "Chinese sequence labeling tasks are heavily reliant on accurate word boundary demarcation. Although current pre-trained language models (PLMs) have achieved substantial gains on these tasks, they rarely explicitly incorporate boundary \u2026"}, {"title": "Data Augmentation and Large Language Model for Legal Case Retrieval and Entailment", "link": "https://link.springer.com/article/10.1007/s12626-024-00158-2", "details": "MQ Bui, DT Do, NK Le, DH Nguyen, KVH Nguyen\u2026 - The Review of Socionetwork \u2026, 2024", "abstract": "Abstract The Competition on Legal Information Extraction and Entailment (COLIEE) is a well-known international competition organized each year with the goal of applying machine learning algorithms and techniques in the analysis and \u2026"}, {"title": "Envisioning MedCLIP: A Deep Dive into Explainability for Medical Vision-Language Models", "link": "https://arxiv.org/html/2403.18996v1", "details": "AUR Hashmi, D Mahapatra, M Yaqub - arXiv preprint arXiv:2403.18996, 2024", "abstract": "Explaining Deep Learning models is becoming increasingly important in the face of daily emerging multimodal models, particularly in safety-critical domains like medical imaging. However, the lack of detailed investigations into the performance of \u2026"}]
