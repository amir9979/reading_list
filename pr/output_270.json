'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Electronic Health Records versus Survey Small Area Estimates'
[{"title": "An Integrated Data Processing Framework for Pretraining Foundation Models", "link": "https://arxiv.org/pdf/2402.16358", "details": "Y Sun, F Wang, Y Zhu, WX Zhao, J Mao - arXiv preprint arXiv:2402.16358, 2024", "abstract": "The ability of the foundation models heavily relies on large-scale, diverse, and high- quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and \u2026"}, {"title": "Learn to Compare: Localize Actions under Weak Supervision", "link": "https://web.pkusz.edu.cn/adsp/files/2024/02/10-CVPR2021-Learn-to-Compare.pdf", "details": "C Zhang, M Cao, D Yang, J Chen, Y Zou", "abstract": "Weakly-supervised temporal action localization (WSTAL) aims to localize actions in untrimmed videos with only video-level labels. Most existing models follow the \u201clocalization by classification\u201d procedure: locate temporal regions contributing most \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models", "link": "https://arxiv.org/pdf/2403.02436", "details": "X Lu, Y Zhao, B Qin - arXiv preprint arXiv:2403.02436, 2024", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models", "link": "https://arxiv.org/html/2402.18154v1", "details": "Z Jin, P Cao, H Yuan, Y Chen, J Xu, H Li, X Jiang, K Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, retrieval augmentation and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context. However, internal memory and external \u2026"}, {"title": "Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods", "link": "https://arxiv.org/pdf/2403.00998", "details": "P Tsvilodub, H Wang, S Grosch, M Franke - arXiv preprint arXiv:2403.00998, 2024", "abstract": "This paper systematically compares different methods of deriving item-level predictions of language models for multiple-choice tasks. It compares scoring methods for answer options based on free generation of responses, various \u2026"}, {"title": "Merino: Entropy-driven Design for Generative Language Models on IoT Devices", "link": "https://arxiv.org/html/2403.07921v1", "details": "Y Zhao, M Lin, H Tang, Q Wu, J Wang - arXiv preprint arXiv:2403.07921, 2024", "abstract": "Generative Large Language Models (LLMs) stand as a revolutionary advancement in the modern era of artificial intelligence (AI). However, directly deploying LLMs in resource-constrained hardware, such as Internet-of-Things (IoT) devices, is difficult \u2026"}, {"title": "Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision", "link": "https://arxiv.org/html/2402.16508v1", "details": "F Jiang, T Drummond, T Cohn - arXiv preprint arXiv:2402.16508, 2024", "abstract": "Cross-lingual question answering (CLQA) is a complex problem, comprising cross- lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language. Both steps are usually tackled by separate \u2026"}, {"title": "Gradient-based domain-augmented meta-learning single-domain generalization for fault diagnosis under variable operating conditions", "link": "https://journals.sagepub.com/doi/abs/10.1177/14759217241230129", "details": "C Jian, H Chen, C Zhong, Y Ao, G Mo - Structural Health Monitoring, 2024", "abstract": "Equipment operating conditions, referred to as domains, can induce domain drift in monitoring data, affecting data-driven fault diagnosis. Researchers have explored multi-domain generalization methods to tackle this issue. However, in actual \u2026"}, {"title": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models", "link": "https://arxiv.org/html/2402.19427v1", "details": "S De, SL Smith, A Fernando, A Botev\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated \u2026"}]
