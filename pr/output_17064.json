[{"title": "A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis", "link": "https://arxiv.org/pdf/2505.23601", "details": "S Liu, B Zheng, W Chen, Z Peng, Z Yin, J Shao, J Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 the capabilities of Multimodal **Large** **Language** **Models** (MLLMs), we employ the GPT-4o-mini API to rephrase **questions** from the original \u2026 bootstrapping strategy to create high-quality datasets, boosting performance in visual **question** **answering** and \u2026", "entry_id": "http://arxiv.org/abs/2505.23601v1", "updated": "2025-05-29 16:14:34", "published": "2025-05-29 16:14:34", "authors": "Shengyuan Liu;Boyun Zheng;Wenting Chen;Zhihao Peng;Zhenfei Yin;Jing Shao;Jiancong Hu;Yixuan Yuan", "summary": "Endoscopic procedures are essential for diagnosing and treating internal\ndiseases, and multi-modal large language models (MLLMs) are increasingly\napplied to assist in endoscopy analysis. However, current benchmarks are\nlimited, as they typically cover specific endoscopic scenarios and a small set\nof clinical tasks, failing to capture the real-world diversity of endoscopic\nscenarios and the full range of skills needed in clinical workflows. To address\nthese issues, we introduce EndoBench, the first comprehensive benchmark\nspecifically designed to assess MLLMs across the full spectrum of endoscopic\npractice with multi-dimensional capacities. EndoBench encompasses 4 distinct\nendoscopic scenarios, 12 specialized clinical tasks with 12 secondary subtasks,\nand 5 levels of visual prompting granularities, resulting in 6,832 rigorously\nvalidated VQA pairs from 21 diverse datasets. Our multi-dimensional evaluation\nframework mirrors the clinical workflow--spanning anatomical recognition,\nlesion analysis, spatial localization, and surgical operations--to holistically\ngauge the perceptual and diagnostic abilities of MLLMs in realistic scenarios.\nWe benchmark 23 state-of-the-art models, including general-purpose,\nmedical-specialized, and proprietary MLLMs, and establish human clinician\nperformance as a reference standard. Our extensive experiments reveal: (1)\nproprietary MLLMs outperform open-source and medical-specialized models\noverall, but still trail human experts; (2) medical-domain supervised\nfine-tuning substantially boosts task-specific accuracy; and (3) model\nperformance remains sensitive to prompt format and clinical task complexity.\nEndoBench establishes a new standard for evaluating and advancing MLLMs in\nendoscopy, highlighting both progress and persistent gaps between current\nmodels and expert clinical reasoning. We publicly release our benchmark and\ncode.", "comment": "36 pages, 18 figures", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.23601v1;http://arxiv.org/pdf/2505.23601v1", "pdf_url": "http://arxiv.org/pdf/2505.23601v1"}, {"title": "Can Large Language Models Match the Conclusions of Systematic Reviews?", "link": "https://arxiv.org/pdf/2505.22787", "details": "C Polzak, A Lozano, MW Sun, J Burgess, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 MedREQAL: Examining **medical** knowledge recall of **large** **language** **models** via **question** **answering**. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 14459\u201314469 \u2026", "entry_id": "http://arxiv.org/abs/2505.22787v1", "updated": "2025-05-28 18:58:09", "published": "2025-05-28 18:58:09", "authors": "Christopher Polzak;Alejandro Lozano;Min Woo Sun;James Burgess;Yuhui Zhang;Kevin Wu;Serena Yeung-Levy", "summary": "Systematic reviews (SR), in which experts summarize and analyze evidence\nacross individual studies to provide insights on a specialized topic, are a\ncornerstone for evidence-based clinical decision-making, research, and policy.\nGiven the exponential growth of scientific articles, there is growing interest\nin using large language models (LLMs) to automate SR generation. However, the\nability of LLMs to critically assess evidence and reason across multiple\ndocuments to provide recommendations at the same proficiency as domain experts\nremains poorly characterized. We therefore ask: Can LLMs match the conclusions\nof systematic reviews written by clinical experts when given access to the same\nstudies? To explore this question, we present MedEvidence, a benchmark pairing\nfindings from 100 SRs with the studies they are based on. We benchmark 24 LLMs\non MedEvidence, including reasoning, non-reasoning, medical specialist, and\nmodels across varying sizes (from 7B-700B). Through our systematic evaluation,\nwe find that reasoning does not necessarily improve performance, larger models\ndo not consistently yield greater gains, and knowledge-based fine-tuning\ndegrades accuracy on MedEvidence. Instead, most models exhibit similar\nbehavior: performance tends to degrade as token length increases, their\nresponses show overconfidence, and, contrary to human experts, all models show\na lack of scientific skepticism toward low-quality findings. These results\nsuggest that more work is still required before LLMs can reliably match the\nobservations from expert-conducted SRs, even though these systems are already\ndeployed and being used by clinicians. We release our codebase and benchmark to\nthe broader research community to further investigate LLM-based SR systems.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.22787v1;http://arxiv.org/pdf/2505.22787v1", "pdf_url": "http://arxiv.org/pdf/2505.22787v1"}, {"title": "Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons", "link": "https://arxiv.org/pdf/2505.23477", "details": "K Vishwanath, A Alyakin, M Ghosh, JV Lee, DA Alber\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **questions** are widely used by neurosurgical residents to prepare for written board examinations. Recently, these **questions** have also served as benchmarks for evaluating **large** **language** **models** \u2026 Robustness of **Large** **Language** **Models** in \u2026", "entry_id": "http://arxiv.org/abs/2505.23477v1", "updated": "2025-05-29 14:27:14", "published": "2025-05-29 14:27:14", "authors": "Krithik Vishwanath;Anton Alyakin;Mrigayu Ghosh;Jin Vivian Lee;Daniel Alexander Alber;Karl L. Sangwon;Douglas Kondziolka;Eric Karl Oermann", "summary": "The Congress of Neurological Surgeons Self-Assessment for Neurological\nSurgeons (CNS-SANS) questions are widely used by neurosurgical residents to\nprepare for written board examinations. Recently, these questions have also\nserved as benchmarks for evaluating large language models' (LLMs) neurosurgical\nknowledge. This study aims to assess the performance of state-of-the-art LLMs\non neurosurgery board-like questions and to evaluate their robustness to the\ninclusion of distractor statements. A comprehensive evaluation was conducted\nusing 28 large language models. These models were tested on 2,904 neurosurgery\nboard examination questions derived from the CNS-SANS. Additionally, the study\nintroduced a distraction framework to assess the fragility of these models. The\nframework incorporated simple, irrelevant distractor statements containing\npolysemous words with clinical meanings used in non-clinical contexts to\ndetermine the extent to which such distractions degrade model performance on\nstandard medical benchmarks. 6 of the 28 tested LLMs achieved board-passing\noutcomes, with the top-performing models scoring over 15.7% above the passing\nthreshold. When exposed to distractions, accuracy across various model\narchitectures was significantly reduced-by as much as 20.4%-with one model\nfailing that had previously passed. Both general-purpose and medical\nopen-source models experienced greater performance declines compared to\nproprietary variants when subjected to the added distractors. While current\nLLMs demonstrate an impressive ability to answer neurosurgery board-like exam\nquestions, their performance is markedly vulnerable to extraneous, distracting\ninformation. These findings underscore the critical need for developing novel\nmitigation strategies aimed at bolstering LLM resilience against in-text\ndistractions, particularly for safe and effective clinical deployment.", "comment": "22 pages, 3 main figures, 3 supplemental figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.23477v1;http://arxiv.org/pdf/2505.23477v1", "pdf_url": "http://arxiv.org/pdf/2505.23477v1"}, {"title": "Large Language Models for Depression Recognition in Spoken Language Integrating Psychological Knowledge", "link": "https://arxiv.org/pdf/2505.22863", "details": "Y Li, S Shao, M Milling, BW Schuller - arXiv preprint arXiv:2505.22863, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) show strong potential but require domain-specific fine-tuning and struggle with non-textual cues. Since \u2026 into LLMs to enhance diagnostic performance, specifically using a **question** and **answer** set to grant \u2026", "entry_id": "http://arxiv.org/abs/2505.22863v1", "updated": "2025-05-28 20:53:05", "published": "2025-05-28 20:53:05", "authors": "Yupei Li;Shuaijie Shao;Manuel Milling;Bj\u00f6rn W. Schuller", "summary": "Depression is a growing concern gaining attention in both public discourse\nand AI research. While deep neural networks (DNNs) have been used for\nrecognition, they still lack real-world effectiveness. Large language models\n(LLMs) show strong potential but require domain-specific fine-tuning and\nstruggle with non-textual cues. Since depression is often expressed through\nvocal tone and behaviour rather than explicit text, relying on language alone\nis insufficient. Diagnostic accuracy also suffers without incorporating\npsychological expertise. To address these limitations, we present, to the best\nof our knowledge, the first application of LLMs to multimodal depression\ndetection using the DAIC-WOZ dataset. We extract the audio features using the\npre-trained model Wav2Vec, and mapped it to text-based LLMs for further\nprocessing. We also propose a novel strategy for incorporating psychological\nknowledge into LLMs to enhance diagnostic performance, specifically using a\nquestion and answer set to grant authorised knowledge to LLMs. Our approach\nyields a notable improvement in both Mean Absolute Error (MAE) and Root Mean\nSquare Error (RMSE) compared to a base score proposed by the related original\npaper. The codes are available at\nhttps://github.com/myxp-lyp/Depression-detection.git", "comment": null, "journal_ref": null, "primary_category": "cs.HC", "categories": "cs.HC;cs.CL", "links": "http://arxiv.org/abs/2505.22863v1;http://arxiv.org/pdf/2505.22863v1", "pdf_url": "http://arxiv.org/pdf/2505.22863v1"}, {"title": "Domain Specific Benchmarks for Evaluating Multimodal **Large Language Models**", "link": "https://www.preprints.org/frontend/manuscript/d56e264011a4a69f9edd5f4fdcfbb212/download_pub", "details": "K Anjum, MA Arshad, K Hayawi, E Polyzos, A Tariq\u2026 - 2025", "abstract": "\u2026 For instance, even leading models like GPT-4 have demonstrated significant limitations in specialized financial **question** **answering** , achieving low accuracy on benchmarks like FinanceBench [6]. Similarly, in software engineering, strong \u2026"}, {"title": "A Survey on the Impact of Pre-Trained Language Models in Sentiment Classification Task", "link": "https://link.springer.com/article/10.1007/s41060-025-00805-z", "details": "H Gautam, A Gaur, DK Yadav - International Journal of Data Science and Analytics, 2025", "abstract": "\u2026 Finally, Section 8 summarizes our findings and discusses potential future directions for sentiment analysis and **large** **language** **models**. \u2026 are essential for understanding downstream language tasks such as sentiment analysis, **question** \u2026"}, {"title": "ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency Room", "link": "https://arxiv.org/pdf/2505.22919", "details": "N Mehandru, N Golchini, D Bamman, T Zack\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) have been extensively evaluated on **medical** question **answering** tasks based on licensing exams. \u2026 LLM benchmarks, several recent efforts have sought to move beyond traditional **medical** **question** **answering** \u2026", "entry_id": "http://arxiv.org/abs/2505.22919v1", "updated": "2025-05-28 22:43:44", "published": "2025-05-28 22:43:44", "authors": "Nikita Mehandru;Niloufar Golchini;David Bamman;Travis Zack;Melanie F. Molina;Ahmed Alaa", "summary": "Large language models (LLMs) have been extensively evaluated on medical\nquestion answering tasks based on licensing exams. However, real-world\nevaluations often depend on costly human annotators, and existing benchmarks\ntend to focus on isolated tasks that rarely capture the clinical reasoning or\nfull workflow underlying medical decisions. In this paper, we introduce\nER-Reason, a benchmark designed to evaluate LLM-based clinical reasoning and\ndecision-making in the emergency room (ER)--a high-stakes setting where\nclinicians make rapid, consequential decisions across diverse patient\npresentations and medical specialties under time pressure. ER-Reason includes\ndata from 3,984 patients, encompassing 25,174 de-identified longitudinal\nclinical notes spanning discharge summaries, progress notes, history and\nphysical exams, consults, echocardiography reports, imaging notes, and ER\nprovider documentation. The benchmark includes evaluation tasks that span key\nstages of the ER workflow: triage intake, initial assessment, treatment\nselection, disposition planning, and final diagnosis--each structured to\nreflect core clinical reasoning processes such as differential diagnosis via\nrule-out reasoning. We also collected 72 full physician-authored rationales\nexplaining reasoning processes that mimic the teaching process used in\nresidency training, and are typically absent from ER documentation. Evaluations\nof state-of-the-art LLMs on ER-Reason reveal a gap between LLM-generated and\nclinician-authored clinical reasoning for ER decisions, highlighting the need\nfor future research to bridge this divide.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.22919v1;http://arxiv.org/pdf/2505.22919v1", "pdf_url": "http://arxiv.org/pdf/2505.22919v1"}, {"title": "Fine-grained entity disambiguation through numeric pattern awareness in transformer models", "link": "https://link.springer.com/article/10.1007/s40747-025-01936-3", "details": "J Jang, S Kim, H Moon, SH Shin, M Yun, C Wiseman - Complex & Intelligent Systems, 2025", "abstract": "\u2026 Knowledge base **question** **answering** systems rely on entity linking to connect textual mentions in natural language with corresponding \u2026 **medical** diagnostics, financial analysis, scientific research, autonomous systems, and numerical \u2026"}, {"title": "Interpreting Chest X-rays Like a Radiologist: A Benchmark with Clinical Reasoning", "link": "https://arxiv.org/pdf/2505.23143", "details": "J Guan, Q Chen, L Liang, Y Liu, VMH Phan, MS To\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Recent advances in **large** **language** **models** (LLMs) have motivated their integration into **medical** image analysis to improve **clinical** reasoning \u2026 taxonomy for our Visual **Question** **Answering** (VQA) dataset, encompassing all feasible **clinical** \u2026", "entry_id": "http://arxiv.org/abs/2505.23143v1", "updated": "2025-05-29 06:30:40", "published": "2025-05-29 06:30:40", "authors": "Jinquan Guan;Qi Chen;Lizhou Liang;Yuhang Liu;Vu Minh Hieu Phan;Minh-Son To;Jian Chen;Yutong Xie", "summary": "Artificial intelligence (AI)-based chest X-ray (CXR) interpretation\nassistants have demonstrated significant progress and are increasingly being\napplied in clinical settings. However, contemporary medical AI models often\nadhere to a simplistic input-to-output paradigm, directly processing an image\nand an instruction to generate a result, where the instructions may be integral\nto the model's architecture. This approach overlooks the modeling of the\ninherent diagnostic reasoning in chest X-ray interpretation. Such reasoning is\ntypically sequential, where each interpretive stage considers the images, the\ncurrent task, and the contextual information from previous stages. This\noversight leads to several shortcomings, including misalignment with clinical\nscenarios, contextless reasoning, and untraceable errors. To fill this gap, we\nconstruct CXRTrek, a new multi-stage visual question answering (VQA) dataset\nfor CXR interpretation. The dataset is designed to explicitly simulate the\ndiagnostic reasoning process employed by radiologists in real-world clinical\nsettings for the first time. CXRTrek covers 8 sequential diagnostic stages,\ncomprising 428,966 samples and over 11 million question-answer (Q&A) pairs,\nwith an average of 26.29 Q&A pairs per sample. Building on the CXRTrek dataset,\nwe propose a new vision-language large model (VLLM), CXRTrekNet, specifically\ndesigned to incorporate the clinical reasoning flow into the VLLM framework.\nCXRTrekNet effectively models the dependencies between diagnostic stages and\ncaptures reasoning patterns within the radiological context. Trained on our\ndataset, the model consistently outperforms existing medical VLLMs on the\nCXRTrek benchmarks and demonstrates superior generalization across multiple\ntasks on five diverse external datasets. The dataset and model can be found in\nour repository (https://github.com/guanjinquan/CXRTrek).", "comment": "10 pages (main text), 18 pages (appendix)", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.23143v1;http://arxiv.org/pdf/2505.23143v1", "pdf_url": "http://arxiv.org/pdf/2505.23143v1"}]
