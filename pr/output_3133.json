[{"title": "Learning from Random Demonstrations: Offline Reinforcement Learning with Importance-Sampled Diffusion Models", "link": "https://arxiv.org/pdf/2405.19878", "details": "Z Fang, T Lan - arXiv preprint arXiv:2405.19878, 2024", "abstract": "Generative models such as diffusion have been employed as world models in offline reinforcement learning to generate synthetic data for more effective learning. Existing work either generates diffusion models one-time prior to training or requires \u2026"}, {"title": "FedGK: Communication-Efficient Federated Learning through Group-Guided Knowledge Distillation", "link": "https://dl.acm.org/doi/pdf/10.1145/3674973", "details": "W Zhang, XL Liu, S Tarkoma - ACM Transactions on Internet Technology, 2024", "abstract": "Federated learning (FL) empowers a cohort of participating devices to contribute collaboratively to a global neural network model, ensuring that their training data remains private and stored locally. Despite its advantages in computational efficiency \u2026"}, {"title": "Improving Generalization in Offline Reinforcement Learning via Adversarial Data Splitting", "link": "https://openreview.net/pdf%3Fid%3DCV9PiQGt0i", "details": "D Wang, L Li, W Wei, Q Yu, HAO Jianye, J Liang - Forty-first International Conference on \u2026", "abstract": "Offline Reinforcement Learning (RL) commonly suffers from the out-of-distribution (OOD) overestimation issue due to the distribution shift. Prior work gradually shifts their focus from suppressing OOD overestimation to avoiding overly conservative \u2026"}, {"title": "MGCP: A Multi-Grained Correlation based Prediction Network for Multivariate Time Series", "link": "https://arxiv.org/pdf/2405.19661", "details": "Z Chen, X Xiao, K Xu, Z Zhang, Y Rong, Q Li, G Gan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multivariate time series prediction is widely used in daily life, which poses significant challenges due to the complex correlations that exist at multi-grained levels. Unfortunately, the majority of current time series prediction models fail to \u2026"}, {"title": "Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation", "link": "https://arxiv.org/pdf/2406.06890", "details": "Y Zhai, K Lin, Z Yang, L Li, J Wang, CC Lin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Image diffusion distillation achieves high-fidelity generation with very few sampling steps. However, applying these techniques directly to video diffusion often results in unsatisfactory frame quality due to the limited visual quality in public video datasets \u2026"}]
