[{"title": "HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/32171/34326", "details": "KHI Arif, JY Yoon, DS Nikolopoulos, H Vandierendonck\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "Abstract High-resolution Vision-Language Models (VLMs) are widely used in multimodal tasks to enhance accuracy by preserving detailed image information. However, these models often generate an excessive number of visual tokens due to \u2026"}, {"title": "Unified Knowledge Maintenance Pruning and Progressive Recovery with Weight Recalling for Large Vision-Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/32923/35078", "details": "Z Wu, J Chen, Y Wang - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Abstract Large Vision-Language Model (LVLM), leveraging Large Language Model (LLM) as the cognitive core, has recently become one of the most representative multimodal model paradigms. However, with the expansion of unimodal \u2026"}, {"title": "QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models", "link": "https://arxiv.org/pdf/2504.11038", "details": "Y Zhang, R Xie, J Chen, X Sun, Z Kang, Y Wang - arXiv preprint arXiv:2504.11038, 2025", "abstract": "In typical multimodal tasks, such as Visual Question Answering (VQA), adversarial attacks targeting a specific image and question can lead large vision-language models (LVLMs) to provide incorrect answers. However, it is common for a single \u2026"}, {"title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting", "link": "https://arxiv.org/pdf/2504.15485", "details": "A Pothiraj, E Stengel-Eskin, J Cho, M Bansal - arXiv preprint arXiv:2504.15485, 2025", "abstract": "Recognizing and reasoning about occluded (partially or fully hidden) objects is vital to understanding visual scenes, as occlusions frequently occur in real-world environments and act as obstacles for spatial comprehension. To test models' ability \u2026"}, {"title": "VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering", "link": "https://arxiv.org/pdf/2504.08269", "details": "QZ Lim, CP Lee, KM Lim, KSM Anbananthen - arXiv preprint arXiv:2504.08269, 2025", "abstract": "The increasing availability of multimodal data across text, tables, and images presents new challenges for developing models capable of complex cross-modal reasoning. Existing methods for Multimodal Multi-hop Question Answering (MMQA) \u2026"}, {"title": "Explore What LLM Does Not Know in Complex Question Answering", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/34638/36793", "details": "X Lin, Z Huang, Z Zhang, J Zhou, E Chen - Proceedings of the AAAI Conference on \u2026, 2025", "abstract": "Complex question answering (QA) is a challenging task in artificial intelligence research which requires reasoning based on related knowledge. The retrieval- augmented generation (RAG) based on large language models (LLMs) have \u2026"}, {"title": "Argumentative Large Language Models for Explainable and Contestable Claim Verification", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/33637/35792", "details": "G Freedman, A Dejl, D Gorur, X Yin, A Rago, F Toni - Proceedings of the AAAI \u2026, 2025", "abstract": "The profusion of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them promising candidates for use in decision-making. However, they are currently limited by their \u2026"}, {"title": "Overcoming Heterogeneous Data in Federated Medical Vision-Language Pre-training: A Triple-Embedding Model Selector Approach", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/32807/34962", "details": "A Wang, Z Zhang, D Wang, F Wang, H Hu, J Guo\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "The scarcity data of medical field brings the collaborative training in medical vision- language pre-training (VLP) cross different clients. Therefore, the collaborative training in medical VLP faces two challenges: First, the medical data requires \u2026"}, {"title": "FedINER: Chinese Industrial Named Entity Recognition with Federated Learning", "link": "https://dl.acm.org/doi/pdf/10.1145/3730401", "details": "X Liu, X Zhao, J Chen, Z Li - ACM Transactions on Asian and Low-Resource \u2026, 2025", "abstract": "Named entity recognition (NER) is foundational in constructing industrial knowledge graphs and is an essential component in the automation of knowledge within the industry. However, due to the particularities of the industrial field, its data involves \u2026"}]
