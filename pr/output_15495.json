[{"title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge", "link": "https://arxiv.org/pdf/2504.07887", "details": "R Cantini, A Orsino, M Ruggiero, D Talia - arXiv preprint arXiv:2504.07887, 2025", "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised \u2026"}, {"title": "Reinforcement Learning from Human Feedback", "link": "https://arxiv.org/pdf/2504.12501", "details": "N Lambert - arXiv preprint arXiv:2504.12501, 2025", "abstract": "Reinforcement learning from human feedback (RLHF) has become an important technical and storytelling tool to deploy the latest machine learning systems. In this book, we hope to give a gentle introduction to the core methods for people with some \u2026"}, {"title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?", "link": "https://arxiv.org/pdf/2504.10000", "details": "Y Wang, J Guan, J Liang, R He - arXiv preprint arXiv:2504.10000, 2025", "abstract": "Multi-modal large language models (MLLMs) have made significant progress, yet their safety alignment remains limited. Typically, current open-source MLLMs rely on the alignment inherited from their language module to avoid harmful generations \u2026"}, {"title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer", "link": "https://arxiv.org/pdf/2504.10462", "details": "W Lei, J Wang, H Wang, X Li, JH Liew, J Feng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained \u2026"}, {"title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data", "link": "https://arxiv.org/pdf/2504.09895", "details": "S Zhao, L Zhu, Y Yang - arXiv preprint arXiv:2504.09895, 2025", "abstract": "Large language models~(LLMs) are expected to be helpful, harmless, and honest. In various alignment scenarios, such as general human preference, safety, and confidence alignment, binary preference data collection and reward modeling are \u2026"}, {"title": "SaRO: Enhancing LLM Safety through Reasoning-based Alignment", "link": "https://arxiv.org/pdf/2504.09420", "details": "Y Mou, Y Luo, S Zhang, W Ye - arXiv preprint arXiv:2504.09420, 2025", "abstract": "Current safety alignment techniques for large language models (LLMs) face two key challenges:(1) under-generalization, which leaves models vulnerable to novel jailbreak attacks, and (2) over-alignment, which leads to the excessive refusal of \u2026"}, {"title": "d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning", "link": "https://arxiv.org/pdf/2504.12216", "details": "S Zhao, D Gupta, Q Zheng, A Grover - arXiv preprint arXiv:2504.12216, 2025", "abstract": "Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) \u2026"}, {"title": "Aligning Anime Video Generation with Human Feedback", "link": "https://arxiv.org/pdf/2504.10044", "details": "B Zhu, Y Jiang, B Xu, S Yang, M Yin, Y Wu, H Sun\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Anime video generation faces significant challenges due to the scarcity of anime data and unusual motion patterns, leading to issues such as motion distortion and flickering artifacts, which result in misalignment with human preferences. Existing \u2026"}, {"title": "DF-MIA: A Distribution-Free Membership Inference Attack on Fine-Tuned Large Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/32012/34167", "details": "Z Huang, Y Liu, D He, Y Li - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Abstract Membership Inference Attack (MIA) aims to determine if a specific sample is present in the training dataset of a target machine learning model. Previous MIAs against fine-tuned Large Language Models (LLMs) either fail to address the unique \u2026"}]
