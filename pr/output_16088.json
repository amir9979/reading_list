[{"title": "Efficient Multivariate Time Series Forecasting via Calibrated Language Models with Privileged Knowledge Distillation", "link": "https://arxiv.org/pdf/2505.02138", "details": "C Liu, S Zhou, H Miao, Q Xu, C Long, Z Li, R Zhao - arXiv preprint arXiv:2505.02138, 2025", "abstract": "Multivariate time series forecasting (MTSF) endeavors to predict future observations given historical data, playing a crucial role in time series data management systems. With advancements in large language models (LLMs), recent studies employ textual \u2026"}, {"title": "Argumentative Large Language Models for Explainable and Contestable Claim Verification", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/33637/35792", "details": "G Freedman, A Dejl, D Gorur, X Yin, A Rago, F Toni - Proceedings of the AAAI \u2026, 2025", "abstract": "The profusion of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them promising candidates for use in decision-making. However, they are currently limited by their \u2026"}, {"title": "Overcoming Heterogeneous Data in Federated Medical Vision-Language Pre-training: A Triple-Embedding Model Selector Approach", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/32807/34962", "details": "A Wang, Z Zhang, D Wang, F Wang, H Hu, J Guo\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "The scarcity data of medical field brings the collaborative training in medical vision- language pre-training (VLP) cross different clients. Therefore, the collaborative training in medical VLP faces two challenges: First, the medical data requires \u2026"}, {"title": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning", "link": "https://arxiv.org/pdf/2504.13500%3F", "details": "J Wang, J Jiang, Y Liu, M Zhang, X Cai - arXiv preprint arXiv:2504.13500, 2025", "abstract": "In this paper, we introduce a new\\emph {process prejudge} strategy in LLM reasoning to demonstrate that bootstrapping with process prejudge allows the LLM to adaptively anticipate the errors encountered when advancing the subsequent \u2026"}, {"title": "KnowEEG: Explainable Knowledge Driven EEG Classification", "link": "https://arxiv.org/pdf/2505.00541", "details": "A Sahota, NM Foumani, R Santos-Rodriguez\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Electroencephalography (EEG) is a method of recording brain activity that shows significant promise in applications ranging from disease classification to emotion detection and brain-computer interfaces. Recent advances in deep learning have \u2026"}, {"title": "Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models", "link": "https://arxiv.org/pdf/2504.19061", "details": "AB Das, S Ahmed, SK Sakib - arXiv preprint arXiv:2504.19061, 2025", "abstract": "Clinical summarization is crucial in healthcare as it distills complex medical data into digestible information, enhancing patient understanding and care management. Large language models (LLMs) have shown significant potential in automating and \u2026"}, {"title": "HalluShift: Measuring Distribution Shifts towards Hallucination Detection in LLMs", "link": "https://arxiv.org/pdf/2504.09482", "details": "S Dasgupta, S Nath, A Basu, P Shamsolmoali, S Das - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have recently garnered widespread attention due to their adeptness at generating innovative responses to the given prompts across a multitude of domains. However, LLMs often suffer from the inherent limitation of \u2026"}, {"title": "Two Heads are Better Than One: Test-time Scaling of Multi-agent Collaborative Reasoning", "link": "https://arxiv.org/pdf/2504.09772%3F", "details": "C Jin, H Peng, Q Zhang, Y Tang, DN Metaxas, T Che - arXiv preprint arXiv \u2026, 2025", "abstract": "Multi-agent systems (MAS) built on large language models (LLMs) offer a promising path toward solving complex, real-world tasks that single-agent systems often struggle to manage. While recent advancements in test-time scaling (TTS) have \u2026"}, {"title": "Reasoning Capabilities and Invariability of Large Language Models", "link": "https://arxiv.org/pdf/2505.00776", "details": "A Raganato, R Pe\u00f1aloza, M Viviani, G Pasi - arXiv preprint arXiv:2505.00776, 2025", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in manipulating natural language across multiple applications, but their ability to handle simple reasoning tasks is often questioned. In this work, we aim to provide a \u2026"}]
