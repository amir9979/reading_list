[{"title": "Multi-turn Response Selection with Commonsense-enhanced Language Models", "link": "https://arxiv.org/pdf/2407.18479", "details": "Y Wang, X Ren, T Chen, Y Dong, NQV Hung, J Tang - arXiv preprint arXiv \u2026, 2024", "abstract": "As a branch of advanced artificial intelligence, dialogue systems are prospering. Multi-turn response selection is a general research problem in dialogue systems. With the assistance of background information and pre-trained language models, the \u2026"}, {"title": "MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models", "link": "https://arxiv.org/pdf/2407.02775", "details": "Y Zhang, Z Yang, S Ji - arXiv preprint arXiv:2407.02775, 2024", "abstract": "Knowledge distillation is an effective technique for pre-trained language model compression. Although existing knowledge distillation methods perform well for the most typical model BERT, they could be further improved in two aspects: the relation \u2026"}, {"title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application", "link": "https://arxiv.org/pdf/2407.01885", "details": "C Yang, W Lu, Y Zhu, Y Wang, Q Chen, C Gao, B Yan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have showcased exceptional capabilities in various domains, attracting significant interest from both academia and industry. Despite their impressive performance, the substantial size and computational demands of LLMs \u2026"}, {"title": "RuMedSpellchecker: A new approach for advanced spelling error correction in Russian electronic health records", "link": "https://www.sciencedirect.com/science/article/pii/S1877750324001868", "details": "D Pogrebnoi, A Funkner, S Kovalchuk - Journal of Computational Science, 2024", "abstract": "In healthcare, a remarkable progress in machine learning has given rise to a diverse range of predictive and decision-making medical models, significantly enhancing treatment efficacy and overall quality of care. These models often rely on electronic \u2026"}, {"title": "Multimodal Pre-training for Sequential Recommendation via Contrastive Learning", "link": "https://dl.acm.org/doi/pdf/10.1145/3682075", "details": "L Zhang, X Zhou, Z Zeng, Z Shen - ACM Transactions on Recommender Systems, 2024", "abstract": "Sequential recommendation systems often suffer from data sparsity, leading to suboptimal performance. While multimodal content, such as images and text, has been utilized to mitigate this issue, its integration within sequential recommendation \u2026"}, {"title": "Large Language Models for Tabular Data: Progresses and Future Directions", "link": "https://dl.acm.org/doi/pdf/10.1145/3626772.3661384", "details": "H Dong, Z Wang - Proceedings of the 47th International ACM SIGIR \u2026, 2024", "abstract": "Tables contain a significant portion of the world's structured information. The ability to efficiently and accurately understand, process, reason about, analyze, and generate tabular data is critical for achieving Artificial General Intelligence (AGI) systems \u2026"}, {"title": "Towards Safe Large Language Models for Medicine", "link": "https://openreview.net/pdf%3Fid%3D1cq9pmwRgG", "details": "T Han, A Kumar, C Agarwal, H Lakkaraju - ICML 2024 Workshop on Models of Human \u2026, 2024", "abstract": "As large language models (LLMs) develop ever-improving capabilities and are applied in real-world settings, it is important to understand their safety. While initial steps have been taken to evaluate the safety of general-knowledge LLMs, exposing \u2026"}, {"title": "Advancing Faithfulness of Large Language Models in Goal-Oriented Dialogue Question Answering", "link": "https://dl.acm.org/doi/abs/10.1145/3640794.3665573", "details": "A Sticha, N Braunschweiler, RS Doddipatla, KM Knill - \u2026 of the 6th ACM Conference on \u2026, 2024", "abstract": "Goal-oriented dialogue systems, such as assistant chatbots and conversational AI systems, have gained prominence for their question-answering capabilities, often utilizing large language models (LLMs) as knowledge bases. However, these \u2026"}, {"title": "Pretraining an Encoder: The BERT Language Model", "link": "https://link.springer.com/chapter/10.1007/978-3-031-57549-5_16", "details": "PM Nugues - \u2026 for Natural Language Processing: Programming with \u2026, 2024", "abstract": "Transformers have the capacity to store a huge quantity of information. This chapter describes how we can pretrain the encoder part of a transformer on very large raw corpora to fit their parameters from word associations. It then describes how to fine \u2026"}]
