[{"title": "Collaboration between clinicians and vision\u2013language models in radiology report generation", "link": "https://www.nature.com/articles/s41591-024-03302-1", "details": "R Tanno, DGT Barrett, A Sellergren, S Ghaisas\u2026 - Nature Medicine, 2024", "abstract": "Automated radiology report generation has the potential to improve patient care and reduce the workload of radiologists. However, the path toward real-world adoption has been stymied by the challenge of evaluating the clinical quality of artificial \u2026"}, {"title": "CriteriaMapper: establishing the automatic identification of clinical trial cohorts from electronic health records by matching normalized eligibility criteria and patient \u2026", "link": "https://www.nature.com/articles/s41598-024-77447-x", "details": "K Lee, Y Mai, Z Liu, K Raja, T Jun, M Ma, T Wang, L Ai\u2026 - Scientific Reports, 2024", "abstract": "The use of electronic health records (EHRs) holds the potential to enhance clinical trial activities. However, the identification of eligible patients within EHRs presents considerable challenges. We aimed to develop a CriteriaMapper system for \u2026"}, {"title": "Policy optimization of language models to align fidelity and efficiency of generative retrieval in multi-turn dialogues", "link": "https://jcur.github.io/publications/RecSysLLM_FallbackOptimization.pdf", "details": "J Curuksu - NeurIPS 2024 Workshop on Fine-Tuning in Modern \u2026, 2024", "abstract": "Combining large languages models (LM) with policy optimization based on human preferences has recently led to new generations of chatbots showing human-level capabilities in helpfulness and safety, with each new release often massively better \u2026"}, {"title": "Tree of Attributes Prompt Learning for Vision-Language Models", "link": "https://arxiv.org/pdf/2410.11201", "details": "T Ding, W Li, Z Miao, H Pfister - arXiv preprint arXiv:2410.11201, 2024", "abstract": "Prompt learning has proven effective in adapting vision language models for downstream tasks. However, existing methods usually append learnable prompt tokens solely with the category names to obtain textual features, which fails to fully \u2026"}, {"title": "Prompt tuning discriminative language models for hierarchical text classification", "link": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/50E5499348A0E72F0C4F3AFC622133A7/S2977042424000517a.pdf/div-class-title-prompt-tuning-discriminative-language-models-for-hierarchical-text-classification-div.pdf", "details": "J du Toit, M Dunaiski - Natural Language Processing", "abstract": "Hierarchical text classification (HTC) is a natural language processing task which aims to categorise a text document into a set of classes from a hierarchical class structure. Recent approaches to solve HTC tasks focus on leveraging pre-trained \u2026"}, {"title": "Empirical Study of Mutual Reinforcement Effect and Application in Few-shot Text Classification Tasks via Prompt", "link": "https://arxiv.org/pdf/2410.09745", "details": "C Gan, T Mori - arXiv preprint arXiv:2410.09745, 2024", "abstract": "The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced \u2026"}, {"title": "AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models", "link": "https://arxiv.org/pdf/2410.10912", "details": "H Lu, Y Zhou, S Liu, Z Wang, MW Mahoney, Y Yang - arXiv preprint arXiv:2410.10912, 2024", "abstract": "Recent work on pruning large language models (LLMs) has shown that one can eliminate a large number of parameters without compromising performance, making pruning a promising strategy to reduce LLM model size. Existing LLM pruning \u2026"}, {"title": "DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2410.11988", "details": "S Gao, CH Lin, T Hua, T Zheng, Y Shen, H Jin, YC Hsu - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have achieved remarkable success in various natural language processing tasks, including language modeling, understanding, and generation. However, the increased memory and computational costs \u2026"}, {"title": "PqE: Zero-Shot Document Expansion for Dense Retrieval with Large Language Models", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9431-7_8", "details": "J Liu, D Zou, N Chai, Y Yang, H Wang, X Song - CCF International Conference on \u2026, 2024", "abstract": "The dense retrieval model offers remarkable capabilities, yet it exhibits inconsistencies in the embedding space of queries and documents due to its dual- encoder structure. Addressing this limitation, we introduce Pseudo-query Embedding \u2026"}]
