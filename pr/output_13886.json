[{"title": "Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More", "link": "https://arxiv.org/pdf/2502.11494", "details": "Z Wen, Y Gao, S Wang, J Zhang, Q Zhang, W Li, C He\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision tokens in multimodal large language models often dominate huge computational overhead due to their excessive length compared to linguistic modality. Abundant recent methods aim to solve this problem with token pruning \u2026"}, {"title": "Prompting, Decoding, Embedding: Leveraging Pretrained Language Models for High-quality and Diverse Open Rule Induction", "link": "https://ieeexplore.ieee.org/abstract/document/10906469/", "details": "W Sun, S He, J Zhao, K Liu - IEEE Transactions on Audio, Speech and Language \u2026, 2025", "abstract": "Open rule induction (OpenRI) is devoted to obtaining reasoning rules expressed in natural languages. Compared with traditional rules with predefined logical symbols and domain predicates, open rules are more expressive and easier to use in real \u2026"}, {"title": "Hierarchical Vision\u2013Language Pre-Training with Freezing Strategy for Multi-Level Semantic Alignment", "link": "https://www.mdpi.com/2079-9292/14/4/816", "details": "H Xie, Y Qin, S Ding - Electronics, 2025", "abstract": "Vision\u2013language pre-training (VLP) faces challenges in aligning hierarchical textual semantics (words/phrases/sentences) with multi-scale visual features (objects/relations/global context). We propose a hierarchical VLP model (HieVLP) \u2026"}, {"title": "Multilingual Language Model Pretraining using Machine-translated Data", "link": "https://arxiv.org/pdf/2502.13252", "details": "J Wang, Y Lu, M Weber, M Ryabinin, D Adelani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-resource languages such as English, enables the pretraining of high-quality large language models (LLMs). The same can not be said for most other languages as LLMs still underperform for non-English languages, likely due to a gap in the \u2026"}, {"title": "AIDE: Agentically Improve Visual Language Model with Domain Experts", "link": "https://arxiv.org/pdf/2502.09051", "details": "MC Chiu, F Liu, K Sapra, A Tao, Y Jacoob, X Ma, Z Yu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The enhancement of Visual Language Models (VLMs) has traditionally relied on knowledge distillation from larger, more capable models. This dependence creates a fundamental bottleneck for improving state-of-the-art systems, particularly when no \u2026"}, {"title": "Comparative Study on the Working of Pre-trained Encoder-Decoder Models and Large Autoregressive Language Models for Abstractive Text Summarization", "link": "https://link.springer.com/content/pdf/10.1007/978-981-97-9045-6.pdf%23page%3D70", "details": "N Sowpari, P Bansal - Adaptive Intelligence", "abstract": "Abstract Natural Language Processing, a field under Data Science, has helped us to retrieve a quick summary of large texts like news articles, stories, etc., so that we do not have to spend a lot of time in reading long paragraphs. Over the years \u2026"}, {"title": "RetriEVAL: Evaluating Text Generation with Contextualized Lexical Match", "link": "https://dl.acm.org/doi/abs/10.1145/3701551.3703581", "details": "Z Li, X Li, C Tao, J Feng, T Shen, C Xu, H Wang\u2026 - Proceedings of the \u2026, 2025", "abstract": "Pre-trained language models have made significant advancements in text generation tasks. Nevertheless, evaluating the generated text with automatic metrics is still challenging. Compared with supervised metrics, unsupervised metrics which \u2026"}, {"title": "ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language Interactions for Computer-Aided Diagnosis", "link": "https://arxiv.org/pdf/2502.10620", "details": "X Li, X Hou, Z Huang, Y Gan - arXiv preprint arXiv:2502.10620, 2025", "abstract": "Recent advancements in large language models (LLMs) have demonstrated extraordinary comprehension capabilities with remarkable breakthroughs on various vision-language tasks. However, the application of LLMs in generating reliable \u2026"}, {"title": "Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models", "link": "https://arxiv.org/pdf/2502.11559", "details": "Y Xu, C Fu, L Xiong, S Yang, W Wang - arXiv preprint arXiv:2502.11559, 2025", "abstract": "Pre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias \u2026"}]
