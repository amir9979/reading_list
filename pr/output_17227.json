[{"title": "Exploring In-context Example Generation for Machine Translation", "link": "https://arxiv.org/pdf/2506.00507", "details": "D Lee, SC Lee, C Yang, Y Baek, J Choo - arXiv preprint arXiv:2506.00507, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) have demonstrated strong performance across various tasks, leveraging their exceptional in-context learning ability with only a few examples. Accordingly, the selection of optimal in-context examples has been \u2026", "entry_id": "http://arxiv.org/abs/2506.00507v1", "updated": "2025-05-31 11:00:49", "published": "2025-05-31 11:00:49", "authors": "Dohyun Lee;Seungil Chad Lee;Chanwoo Yang;Yujin Baek;Jaegul Choo", "summary": "Large language models (LLMs) have demonstrated strong performance across\nvarious tasks, leveraging their exceptional in-context learning ability with\nonly a few examples. Accordingly, the selection of optimal in-context examples\nhas been actively studied in the field of machine translation. However, these\nstudies presuppose the presence of a demonstration pool with human-annotated\npairs, making them less applicable to low-resource languages where such an\nassumption is challenging to meet. To overcome this limitation, this paper\nexplores the research direction of in-context example generation for machine\ntranslation. Specifically, we propose Demonstration Augmentation for\nTranslation (DAT), a simple yet effective approach that generates example pairs\nwithout relying on any external resources. This method builds upon two prior\ncriteria, relevance and diversity, which have been highlighted in previous work\nas key factors for in-context example selection. Through experiments and\nanalysis on low-resource languages where human-annotated pairs are scarce, we\nshow that DAT achieves superior translation quality compared to the baselines.\nFurthermore, we investigate the potential of progressively accumulating\ngenerated pairs during test time to build and reuse a demonstration pool. Our\nimplementation is publicly available at https://github.com/aiclaudev/DAT.", "comment": "Accepted to ACL 2025 Findings", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.00507v1;http://arxiv.org/pdf/2506.00507v1", "pdf_url": "http://arxiv.org/pdf/2506.00507v1"}, {"title": "Analysing the Applicability of Foundational Models for Healthcare", "link": "https://aaltodoc.aalto.fi/bitstreams/b316378f-6fc2-4920-afa4-6b573f9e5d31/download", "details": "R Ahmed - 2025", "abstract": "\u2026 **Large** **Language** **Models** **Large** **Language** **Models** (LLMs) are built using deep learning, which are trained on massive amounts of text \u2026 the optic nerve that connects the eye and the brain is damaged, **diabetic** retinopathy, and also predicting \u2026"}]
