[{"title": "VLADVA: Discriminative Fine-tuning of LVLMs", "link": "https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/106405/Tzimiropoulos%2520VLADVA%253A%2520Discriminative%2520Fine%25202025%2520Accepted.pdf%3Fsequence%3D2", "details": "Y Ouali, A Bulat, A Xenos, I Maniadis Metaxas\u2026 - 2025", "abstract": "Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a \u2026"}, {"title": "Developing ChemDFM as a large language foundation model for chemistry", "link": "https://www.cell.com/cell-reports-physical-science/fulltext/S2666-3864\\(25\\)00122-5", "details": "Z Zhao, D Ma, L Chen, L Sun, Z Li, Y Xia, B Chen, H Xu\u2026 - Cell Reports Physical \u2026, 2025", "abstract": "Artificial intelligence (AI) plays an increasingly important role in chemical research. However, most models currently used in chemistry are specialist models for specific tasks. A more generic and efficient solution would be an AI model that could address \u2026"}, {"title": "Large Language Model Empowered Recommendation Meets All-domain Continual Pre-Training", "link": "https://arxiv.org/pdf/2504.08949", "details": "H Ma, Y Ma, R Xie, L Meng, J Shen, X Sun, Z Kang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent research efforts have investigated how to integrate Large Language Models (LLMs) into recommendation, capitalizing on their semantic comprehension and open-world knowledge for user behavior understanding. These approaches \u2026"}]
