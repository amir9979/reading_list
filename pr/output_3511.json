[{"title": "Graph Structure Learning with Interpretable Bayesian Neural Networks", "link": "https://arxiv.org/pdf/2406.14786", "details": "M Wasserman, G Mateos - arXiv preprint arXiv:2406.14786, 2024", "abstract": "Graphs serve as generic tools to encode the underlying relational structure of data. Often this graph is not given, and so the task of inferring it from nodal observations becomes important. Traditional approaches formulate a convex inverse problem with \u2026"}, {"title": "Interpretable Probabilistic Bayesian Neural Networks for Cybersecurity Intrusion Detection", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10578019.pdf", "details": "T Yang, Y Qiao, B Lee - IEEE Access, 2024", "abstract": "The probabilistic Bayesian neural network (BNN) is good at providing trustworthy outcomes that is important, eg in intrusion detection. Due to the complex of probabilistic BNN, it is looks like a'black box'. The explanation of its prediction is \u2026"}, {"title": "Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation", "link": "https://arxiv.org/pdf/2406.06890", "details": "Y Zhai, K Lin, Z Yang, L Li, J Wang, CC Lin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Image diffusion distillation achieves high-fidelity generation with very few sampling steps. However, applying these techniques directly to video diffusion often results in unsatisfactory frame quality due to the limited visual quality in public video datasets \u2026"}, {"title": "Variational Inference Failures Under Model Symmetries: Permutation Invariant Posteriors for Bayesian Neural Networks", "link": "https://openreview.net/pdf%3Fid%3DVCVnhR4x4v", "details": "Y Gelberg, TFA van der Ouderaa, M van der Wilk, Y Gal - ICML 2024 Workshop on \u2026", "abstract": "Weight space symmetries in neural network architectures, such as permutation symmetries in MLPs, give rise to Bayesian neural network (BNN) posteriors with many equivalent modes. This multimodality poses a challenge for variational \u2026"}, {"title": "LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices", "link": "https://arxiv.org/pdf/2406.08765", "details": "R Jin, Q Xu, M Wu, Y Xu, D Li, X Li, Z Chen - arXiv preprint arXiv:2406.08765, 2024", "abstract": "Limited by the scale and diversity of time series data, the neural networks trained on time series data often overfit and show unsatisfacotry performances. In comparison, large language models (LLMs) recently exhibit impressive generalization in diverse \u2026"}]
