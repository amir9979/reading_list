[{"title": "Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?", "link": "https://arxiv.org/pdf/2505.08468", "details": "MTR Laskar, MS Islam, R Mahbub, A Masry\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Charts are ubiquitous as they help people understand and reason with data. Recently, various downstream tasks, such as chart question answering, chart2text, and fact-checking, have emerged. Large Vision-Language Models (LVLMs) show \u2026"}, {"title": "VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models", "link": "https://arxiv.org/pdf/2505.08455", "details": "P Sarkar, A Etemad - arXiv preprint arXiv:2505.08455, 2025", "abstract": "Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for \u2026"}, {"title": "Exploring Grounding Abilities in Vision-Language Models through Contextual Perception", "link": "https://ieeexplore.ieee.org/abstract/document/10985830/", "details": "W Xu, T Zhou, T Zhang, J Li, P Chen, J Pan, X Liu - IEEE Transactions on Cognitive \u2026, 2025", "abstract": "Vision language models (VLMs) have demonstrated strong general capabilities and achieved great success in areas such as image understanding and reasoning. Visual prompts enhance the focus of VLMs on designated areas, but their fine \u2026"}, {"title": "Exploring Multimodal Language Models for Sustainability Disclosure Extraction: A Comparative Study", "link": "https://aclanthology.org/2025.insights-1.13.pdf", "details": "T Gupta, T Goel, I Verma - The Sixth Workshop on Insights from Negative Results \u2026, 2025", "abstract": "Sustainability metrics have increasingly become a crucial non-financial criterion in investment decision-making. Organizations worldwide are recognizing the importance of sustainability and are proactively highlighting their efforts through \u2026"}, {"title": "Knowledge-enhanced Parameter-efficient Transfer Learning with METER for medical vision-language tasks", "link": "https://www.sciencedirect.com/science/article/pii/S1532046425000693", "details": "X Liang, J Xie, J Wei, M Zhang, H Zhang - Journal of Biomedical Informatics, 2025", "abstract": "Objective: The full fine-tuning paradigm becomes impractical when applying pre- trained models to downstream tasks due to significant computational and storage costs. Parameter-efficient fine-tuning (PEFT) methods can alleviate the issue \u2026"}, {"title": "Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis", "link": "https://arxiv.org/pdf/2505.10046", "details": "B Tang, B Zheng, X Pan, S Paul, S Xie - arXiv preprint arXiv:2505.10046, 2025", "abstract": "This paper does not describe a new method; instead, it provides a thorough exploration of an important yet understudied design space related to recent advances in text-to-image synthesis--specifically, the deep fusion of large language \u2026"}, {"title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning", "link": "https://arxiv.org/pdf/2505.10320", "details": "C Whitehouse, T Wang, P Yu, X Li, J Weston, I Kulikov\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as- a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best \u2026"}, {"title": "Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization", "link": "https://arxiv.org/pdf/2504.21063%3F", "details": "S Gong, C Cui, X Dong, X Nie, L Zhu, X Chang - arXiv preprint arXiv:2504.21063, 2025", "abstract": "Federated domain generalization (FedDG) aims to learn a globally generalizable model from decentralized clients with heterogeneous data while preserving privacy. Recent studies have introduced prompt learning to adapt vision-language models \u2026"}, {"title": "U-Net Encapsulated Transformer for Reducing Dimensionality in Training Large Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3735653", "details": "MJ Ignacio, YG Kim, H Jin, S Yu - ACM Transactions on Intelligent Systems and \u2026", "abstract": "Training language models from scratch presents a critical challenge in Natural Language Processing (NLP), primarily due to the computational demands of pre- trained Large Language Models, which are predominantly trained on English \u2026"}]
