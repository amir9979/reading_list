[{"title": "Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction", "link": "https://arxiv.org/pdf/2505.00814", "details": "M S\u00e4nger, U Leser - arXiv preprint arXiv:2505.00814, 2025", "abstract": "Automatic relationship extraction (RE) from biomedical literature is critical for managing the vast amount of scientific knowledge produced each year. In recent years, utilizing pre-trained language models (PLMs) has become the prevalent \u2026"}, {"title": "Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving", "link": "https://arxiv.org/pdf/2505.08725", "details": "Z Zhao, H Fu, D Liang, X Zhou, D Zhang, H Xie\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The Large Visual-Language Models (LVLMs) have significantly advanced image understanding. Their comprehension and reasoning capabilities enable promising applications in autonomous driving scenarios. However, existing research typically \u2026"}, {"title": "Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning", "link": "https://arxiv.org/pdf/2505.07527", "details": "H Wang, C Ma, I Reid, M Yaqub - arXiv preprint arXiv:2505.07527, 2025", "abstract": "Reward baseline is important for Reinforcement Learning (RL) algorithms to reduce variance in policy gradient estimates. Recently, for language modeling, Group Relative Policy Optimization (GRPO) is proposed to compute the advantage for each \u2026"}, {"title": "Explainable Reinforcement Learning Agents Using World Models", "link": "https://arxiv.org/pdf/2505.08073", "details": "M Singh, A Alabdulkarim, G Mansi, MO Riedl - arXiv preprint arXiv:2505.08073, 2025", "abstract": "Explainable AI (XAI) systems have been proposed to help people understand how AI systems produce outputs and behaviors. Explainable Reinforcement Learning (XRL) has an added complexity due to the temporal nature of sequential decision-making \u2026"}, {"title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models", "link": "https://arxiv.org/pdf/2504.12898%3F", "details": "Z Sun, X Ding, L Du, Y Xu, Y Ma, Y Zhao, B Qin, T Liu - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite significant progress, recent studies indicate that current large language models (LLMs) may still capture dataset biases and utilize them during inference, leading to the poor generalizability of LLMs. However, due to the diversity of dataset \u2026"}, {"title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example", "link": "https://arxiv.org/pdf/2504.20571", "details": "Y Wang, Q Yang, Z Zeng, L Ren, L Liu, B Peng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2. 5-Math \u2026"}, {"title": "Scalable LLM Math Reasoning Acceleration with Low-rank Distillation", "link": "https://arxiv.org/pdf/2505.07861", "details": "H Dong, B Acun, B Chen, Y Chi - arXiv preprint arXiv:2505.07861, 2025", "abstract": "Due to long generations, large language model (LLM) math reasoning demands significant computational resources and time. While many existing efficient inference methods have been developed with excellent performance preservation on \u2026"}, {"title": "HealthBench: Evaluating Large Language Models Towards Improved Human Health", "link": "https://arxiv.org/pdf/2505.08775", "details": "RK Arora, J Wei, RS Hicks, P Bowman\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present HealthBench, an open-source benchmark measuring the performance and safety of large language models in healthcare. HealthBench consists of 5,000 multi-turn conversations between a model and an individual user or healthcare \u2026"}, {"title": "U-Net Encapsulated Transformer for Reducing Dimensionality in Training Large Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3735653", "details": "MJ Ignacio, YG Kim, H Jin, S Yu - ACM Transactions on Intelligent Systems and \u2026", "abstract": "Training language models from scratch presents a critical challenge in Natural Language Processing (NLP), primarily due to the computational demands of pre- trained Large Language Models, which are predominantly trained on English \u2026"}]
