[{"title": "Combining Graph Neural Network and Mamba to Capture Local and Global Tissue Spatial Relationships in Whole Slide Images", "link": "https://arxiv.org/pdf/2406.04377", "details": "R Ding, KD Luong, E Rodriguez, ACAL da Silva, W Hsu - arXiv preprint arXiv \u2026, 2024", "abstract": "In computational pathology, extracting spatial features from gigapixel whole slide images (WSIs) is a fundamental task, but due to their large size, WSIs are typically segmented into smaller tiles. A critical aspect of this analysis is aggregating \u2026"}, {"title": "ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for Whole Slide Image Classification", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_ViLa-MIL_Dual-scale_Vision-Language_Multiple_Instance_Learning_for_Whole_Slide_Image_CVPR_2024_paper.pdf", "details": "J Shi, C Li, T Gong, Y Zheng, H Fu - Proceedings of the IEEE/CVF Conference on \u2026, 2024", "abstract": "Multiple instance learning (MIL)-based framework has become the mainstream for processing the whole slide image (WSI) with giga-pixel size and hierarchical image context in digital pathology. However these methods heavily depend on a substantial \u2026"}, {"title": "A deep-learning framework to predict cancer treatment response from histopathology images through imputed transcriptomics", "link": "https://www.nature.com/articles/s43018-024-00793-2", "details": "DT Hoang, G Dinstag, ED Shulman, LC Hermida\u2026 - Nature Cancer, 2024", "abstract": "Advances in artificial intelligence have paved the way for leveraging hematoxylin and eosin-stained tumor slides for precision oncology. We present ENLIGHT\u2013 DeepPT, an indirect two-step approach consisting of (1) DeepPT, a deep-learning \u2026"}, {"title": "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "link": "https://arxiv.org/pdf/2407.02118", "details": "W Zheng, W Pan, X Xu, L Qin, L Yue, M Zhou - arXiv preprint arXiv:2407.02118, 2024", "abstract": "In recent years, Large Language Models (LLMs) have made significant strides towards Artificial General Intelligence. However, training these models from scratch requires substantial computational resources and vast amounts of text data. In this \u2026"}, {"title": "diff History for Neural Language Agents", "link": "https://openreview.net/pdf%3Fid%3DTJCUrzhbiH", "details": "U Piterbarg, L Pinto, R Fergus - Forty-first International Conference on Machine \u2026", "abstract": "Neural Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with \u2026"}, {"title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models", "link": "https://arxiv.org/pdf/2406.14852", "details": "J Wang, Y Ming, Z Shi, V Vineet, X Wang, N Joshi - arXiv preprint arXiv:2406.14852, 2024", "abstract": "Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoning--a fundamental \u2026"}, {"title": "Alphazero-like tree-search can guide large language model decoding and training", "link": "https://openreview.net/pdf%3Fid%3DC4OpREezgj", "details": "Z Wan, X Feng, M Wen, SM McAleer, Y Wen, W Zhang\u2026 - Forty-first International \u2026, 2024", "abstract": "Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the multi-step reasoning capabilities of LLMs by using tree-search algorithms. These methods rely on prompting a pre-trained model to serve as a value \u2026"}, {"title": "ViLa-MIL: Dual-scale Vision-language Multiple Instance Learning for Whole Slide Image Classification\u2014\u2014Supplementary Materials\u2014\u2014", "link": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Shi_ViLa-MIL_Dual-scale_Vision-Language_CVPR_2024_supplemental.pdf", "details": "J Shi, C Li, T Gong, Y Zheng, H Fu - Training", "abstract": "The specific descriptions of dual-scale visual descriptive text prompts for renal cell carcinoma and lung cancer are shown in Figure S1 and Figure S2, respectively. Note that three experienced pathologists thoroughly examined the text prompts generated \u2026"}]
