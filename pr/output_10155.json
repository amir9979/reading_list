[{"title": "Filipino Benchmarks for Measuring Sexist and Homophobic Bias in Multilingual Language Models from Southeast Asia", "link": "https://arxiv.org/pdf/2412.07303", "details": "LCL Gamboa, M Lee - arXiv preprint arXiv:2412.07303, 2024", "abstract": "Bias studies on multilingual models confirm the presence of gender-related stereotypes in masked models processing languages with high NLP resources. We expand on this line of research by introducing Filipino CrowS-Pairs and Filipino \u2026"}, {"title": "GeoTool-GPT: a trainable method for facilitating Large Language Models to master GIS tools", "link": "https://www.tandfonline.com/doi/abs/10.1080/13658816.2024.2438937", "details": "C Wei, Y Zhang, X Zhao, Z Zeng, Z Wang, J Lin\u2026 - International Journal of \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) excel in natural language-relevant tasks like text generation and question answering Q&A. To further expand their application, efforts focus on enabling LLMs to utilize real-world tools. However, their tool-use \u2026"}, {"title": "Time-varying Representations of Longitudinal Biosignals using Self-supervised Learning", "link": "https://openreview.net/pdf%3Fid%3DSMC0vyc677", "details": "SJ Perochon, S Abbaspourazad, J Futoma, A Miller\u2026 - NeurIPS 2024 Workshop: Self \u2026", "abstract": "Many chronic diseases exhibit complex and slow time courses, and in asymptomatic stages it may be possible to detect signs of disease through longitudinal monitoring with wearables. Properly accounting for temporal dependencies in the learned \u2026"}, {"title": "MRP-LLM: Multitask Reflective Large Language Models for Privacy-Preserving Next POI Recommendation", "link": "https://arxiv.org/pdf/2412.07796", "details": "Z Wu, Z Sun, D Wang, L Zhang, J Zhang, YS Ong - arXiv preprint arXiv:2412.07796, 2024", "abstract": "Large language models (LLMs) have shown promising potential for next Point-of- Interest (POI) recommendation. However, existing methods only perform direct zero- shot prompting, leading to ineffective extraction of user preferences, insufficient \u2026"}, {"title": "One-Shot Classification Is Enough for Automatic Label Mapping", "link": "https://link.springer.com/chapter/10.1007/978-3-031-78169-8_25", "details": "X Lin, A Aysa, K Ubul - International Conference on Pattern Recognition, 2024", "abstract": "In recent years, prompt-based learning has achieved some success in various natural language tasks. In text classification tasks, the construction of prompt templates and label mapping have a significant impact on the results. Therefore \u2026"}, {"title": "A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans", "link": "https://arxiv.org/pdf/2412.01131", "details": "Z Cao, H Yamada, S Teufel, T Tokunaga - arXiv preprint arXiv:2412.01131, 2024", "abstract": "Recently, much work has concerned itself with the enigma of what exactly PLMs (pretrained language models) learn about different aspects of language, and how they learn it. One stream of this type of research investigates the knowledge that \u2026"}, {"title": "Language hooks: a modular framework for augmenting LLM reasoning that decouples tool usage from the model and its prompt", "link": "https://arxiv.org/pdf/2412.05967", "details": "D de Mijolla, W Yang, P Duckett, C Frye, M Worrall - arXiv preprint arXiv:2412.05967, 2024", "abstract": "Prompting and fine-tuning have emerged as two competing paradigms for augmenting language models with new capabilities, such as the use of tools. Prompting approaches are quick to set up but rely on providing explicit \u2026"}, {"title": "Chain of Thought Prompting in Vision-Language Model for Vision Reasoning Tasks", "link": "https://link.springer.com/chapter/10.1007/978-981-96-0351-0_22", "details": "J Ou, J Zhou, Y Dong, F Chen - Australasian Joint Conference on Artificial Intelligence, 2024", "abstract": "The large language model has demonstrated its ability to reason and interpret in text- to-text applications. Current Chain of Thought (CoT) research focuses on either explaining reasoning steps or improving prediction results. This paper proposes a \u2026"}]
