'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Dense Training, Sparse Inference: Rethinking Training'
[{"title": "Fine-Tuning Language Models with Reward Learning on Policy", "link": "https://arxiv.org/html/2403.19279v1", "details": "H Lang, F Huang, Y Li - arXiv preprint arXiv:2403.19279, 2024", "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences. RLHF contains three steps, ie, human preference collecting, reward learning, and policy \u2026"}, {"title": "Investigating Regularization of Self-Play Language Models", "link": "https://arxiv.org/html/2404.04291v1", "details": "R Alami, A Abubaker, M Achab, MEA Seddik, S Lahlou - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper explores the effects of various forms of regularization in the context of language model alignment via self-play. While both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) require to collect \u2026"}, {"title": "Seme at semeval-2024 task 2: Comparing masked and generative language models on natural language inference for clinical trials", "link": "https://arxiv.org/pdf/2404.03977", "details": "M Aguiar, P Zweigenbaum, N Naderi - arXiv preprint arXiv:2404.03977, 2024", "abstract": "This paper describes our submission to Task 2 of SemEval-2024: Safe Biomedical Natural Language Inference for Clinical Trials. The Multi-evidence Natural Language Inference for Clinical Trial Data (NLI4CT) consists of a Textual Entailment (TE) task \u2026"}, {"title": "BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models", "link": "https://arxiv.org/pdf/2404.04113", "details": "J Wiland, M Ploner, A Akbik - arXiv preprint arXiv:2404.04113, 2024", "abstract": "Knowledge probing assesses to which degree a language model (LM) has successfully learned relational knowledge during pre-training. Probing is an inexpensive way to compare LMs of different sizes and training configurations \u2026"}, {"title": "PRobELM: Plausibility Ranking Evaluation for Language Models", "link": "https://arxiv.org/pdf/2404.03818", "details": "Z Yuan, C Whitehouse, E Chamoun, R Aly, A Vlachos - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces PRobELM (Plausibility Ranking Evaluation for Language Models), a benchmark designed to assess language models' ability to discern more plausible from less plausible scenarios through their parametric knowledge. While \u2026"}, {"title": "Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data", "link": "https://arxiv.org/pdf/2404.03862", "details": "J Zhang, M Marone, T Li, B Van Durme, D Khashabi - arXiv preprint arXiv:2404.03862, 2024", "abstract": "For humans to trust the fluent generations of large language models (LLMs), they must be able to verify their correctness against trusted, external sources. Recent efforts aim to increase verifiability through citations of retrieved documents or post \u2026"}, {"title": "Comprehensive Study on German Language Models for Clinical and Biomedical Text Understanding", "link": "https://arxiv.org/html/2404.05694v1", "details": "A Idrissi-Yaghir, A Dada, H Sch\u00e4fer, K Arzideh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in natural language processing (NLP) can be largely attributed to the advent of pre-trained language models such as BERT and RoBERTa. While these models demonstrate remarkable performance on general datasets, they can \u2026"}, {"title": "Towards realistic few-shot relation extraction: A new meta dataset and evaluation", "link": "https://arxiv.org/pdf/2404.04445", "details": "F Alam, MA Islam, R Vacareanu, M Surdeanu - arXiv preprint arXiv:2404.04445, 2024", "abstract": "We introduce a meta dataset for few-shot relation extraction, which includes two datasets derived from existing supervised relation extraction datasets NYT29 (Takanobu et al., 2019; Nayak and Ng, 2020) and WIKIDATA (Sorokin and Gurevych \u2026"}, {"title": "Plug and Play with Prompts: A Prompt Tuning Approach for Controlling Text Generation", "link": "https://arxiv.org/html/2404.05143v1", "details": "RD Ajwani, Z Zhu, J Rose, F Rudzicz - arXiv preprint arXiv:2404.05143, 2024", "abstract": "Transformer-based Large Language Models (LLMs) have shown exceptional language generation capabilities in response to text-based prompts. However, controlling the direction of generation via textual prompts has been challenging \u2026"}]
