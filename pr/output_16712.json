[{"title": "Online Iterative Self-Alignment for Radiology Report Generation", "link": "https://arxiv.org/pdf/2505.11983", "details": "T Xiao, L Shi, Y Zhang, HF Yang, Z Wang, C Bai - arXiv preprint arXiv:2505.11983, 2025", "abstract": "Radiology Report Generation (RRG) is an important research topic for relieving radiologist'heavy workload. Existing RRG models mainly rely on supervised fine- tuning (SFT) based on different model architectures using data pairs of radiological \u2026", "entry_id": "http://arxiv.org/abs/2505.11983v2", "updated": "2025-05-20 14:49:41", "published": "2025-05-17 12:31:12", "authors": "Ting Xiao;Lei Shi;Yang Zhang;HaoFeng Yang;Zhe Wang;Chenjia Bai", "summary": "Radiology Report Generation (RRG) is an important research topic for\nrelieving radiologist' heavy workload. Existing RRG models mainly rely on\nsupervised fine-tuning (SFT) based on different model architectures using data\npairs of radiological images and corresponding radiologist-annotated reports.\nRecent research has shifted focus to post-training improvements, aligning RRG\nmodel outputs with human preferences using reinforcement learning (RL).\nHowever, the limited data coverage of high-quality annotated data poses risks\nof overfitting and generalization. This paper proposes a novel Online Iterative\nSelf-Alignment (OISA) method for RRG that consists of four stages:\nself-generation of diverse data, self-evaluation for multi-objective preference\ndata,self-alignment for multi-objective optimization and self-iteration for\nfurther improvement. Our approach allows for generating varied reports tailored\nto specific clinical objectives, enhancing the overall performance of the RRG\nmodel iteratively. Unlike existing methods, our frame-work significantly\nincreases data quality and optimizes performance through iterative\nmulti-objective optimization. Experimental results demonstrate that our method\nsurpasses previous approaches, achieving state-of-the-art performance across\nmultiple evaluation metrics.", "comment": "Accepted by ACL 2025 Main", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.11983v2;http://arxiv.org/pdf/2505.11983v2", "pdf_url": "http://arxiv.org/pdf/2505.11983v2"}, {"title": "ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling", "link": "https://arxiv.org/pdf/2505.12890", "details": "E \u00d6zsoy, C Pellegrini, D Bani-Harouni, K Yuan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The real-world complexity of surgeries necessitates surgeons to have deep and holistic comprehension to ensure precision, safety, and effective interventions. Computational systems are required to have a similar level of comprehension within \u2026", "entry_id": "http://arxiv.org/abs/2505.12890v1", "updated": "2025-05-19 09:20:29", "published": "2025-05-19 09:20:29", "authors": "Ege \u00d6zsoy;Chantal Pellegrini;David Bani-Harouni;Kun Yuan;Matthias Keicher;Nassir Navab", "summary": "The real-world complexity of surgeries necessitates surgeons to have deep and\nholistic comprehension to ensure precision, safety, and effective\ninterventions. Computational systems are required to have a similar level of\ncomprehension within the operating room. Prior works, limited to single-task\nefforts like phase recognition or scene graph generation, lack scope and\ngeneralizability. In this work, we introduce ORQA, a novel OR question\nanswering benchmark and foundational multimodal model to advance OR\nintelligence. By unifying all four public OR datasets into a comprehensive\nbenchmark, we enable our approach to concurrently address a diverse range of OR\nchallenges. The proposed multimodal large language model fuses diverse OR\nsignals such as visual, auditory, and structured data, for a holistic modeling\nof the OR. Finally, we propose a novel, progressive knowledge distillation\nparadigm, to generate a family of models optimized for different speed and\nmemory requirements. We show the strong performance of ORQA on our proposed\nbenchmark, and its zero-shot generalization, paving the way for scalable,\nunified OR modeling and significantly advancing multimodal surgical\nintelligence. We will release our code and data upon acceptance.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.12890v1;http://arxiv.org/pdf/2505.12890v1", "pdf_url": "http://arxiv.org/pdf/2505.12890v1"}, {"title": "CorBenchX: Large-Scale Chest X-Ray Error Dataset and Vision-Language Model Benchmark for Report Error Correction", "link": "https://arxiv.org/pdf/2505.12057", "details": "J Zou, Q Li, C Lian, L Liu, X Yan, S Wang, J Qin - arXiv preprint arXiv:2505.12057, 2025", "abstract": "AI-driven models have shown great promise in detecting errors in radiology reports, yet the field lacks a unified benchmark for rigorous evaluation of error detection and further correction. To address this gap, we introduce CorBenchX, a comprehensive \u2026", "entry_id": "http://arxiv.org/abs/2505.12057v1", "updated": "2025-05-17 15:39:39", "published": "2025-05-17 15:39:39", "authors": "Jing Zou;Qingqiu Li;Chenyu Lian;Lihao Liu;Xiaohan Yan;Shujun Wang;Jing Qin", "summary": "AI-driven models have shown great promise in detecting errors in radiology\nreports, yet the field lacks a unified benchmark for rigorous evaluation of\nerror detection and further correction. To address this gap, we introduce\nCorBenchX, a comprehensive suite for automated error detection and correction\nin chest X-ray reports, designed to advance AI-assisted quality control in\nclinical practice. We first synthesize a large-scale dataset of 26,326 chest\nX-ray error reports by injecting clinically common errors via prompting\nDeepSeek-R1, with each corrupted report paired with its original text, error\ntype, and human-readable description. Leveraging this dataset, we benchmark\nboth open- and closed-source vision-language models,(e.g., InternVL, Qwen-VL,\nGPT-4o, o4-mini, and Claude-3.7) for error detection and correction under\nzero-shot prompting. Among these models, o4-mini achieves the best performance,\nwith 50.6 % detection accuracy and correction scores of BLEU 0.853, ROUGE\n0.924, BERTScore 0.981, SembScore 0.865, and CheXbertF1 0.954, remaining below\nclinical-level accuracy, highlighting the challenge of precise report\ncorrection. To advance the state of the art, we propose a multi-step\nreinforcement learning (MSRL) framework that optimizes a multi-objective reward\ncombining format compliance, error-type accuracy, and BLEU similarity. We apply\nMSRL to QwenVL2.5-7B, the top open-source model in our benchmark, achieving an\nimprovement of 38.3% in single-error detection precision and 5.2% in\nsingle-error correction over the zero-shot baseline.", "comment": "12 pages, 5figures", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.12057v1;http://arxiv.org/pdf/2505.12057v1", "pdf_url": "http://arxiv.org/pdf/2505.12057v1"}, {"title": "Cross-Modal Translation for Medical Images Augmentation Based on Diagnosis Reports", "link": "https://link.springer.com/chapter/10.1007/978-981-96-5318-8_8", "details": "X Wang, S Yin, W Yin, Y Wang, J Li, S Li - \u2026 on Frontiers of Electronics, Information and \u2026, 2025", "abstract": "As one of the promising approaches to improving efficiency in clinical medicine, deep learning models are still faced with dataset shortages since data collection inevitably incurs extra effort and is time-consuming. Moreover, conventional data \u2026"}]
