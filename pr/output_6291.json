[{"title": "Goldfish: Monolingual Language Models for 350 Languages", "link": "https://arxiv.org/pdf/2408.10441", "details": "TA Chang, C Arnett, Z Tu, BK Bergen - arXiv preprint arXiv:2408.10441, 2024", "abstract": "For many low-resource languages, the only available language models are large multilingual models trained on many languages simultaneously. However, using FLORES perplexity as a metric, we find that these models perform worse than \u2026"}, {"title": "Selective Prefix Tuning for Pre-trained Language Models", "link": "https://aclanthology.org/2024.findings-acl.164.pdf", "details": "H Zhang, Z Li, P Wang, H Zhao - Findings of the Association for Computational \u2026, 2024", "abstract": "The prevalent approach for optimizing pre-trained language models in downstream tasks is fine-tuning. However, it is both time-consuming and memory-inefficient. In response, a more efficient method called Prefix Tuning, which insert learnable \u2026"}, {"title": "VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images", "link": "https://arxiv.org/pdf/2408.16176", "details": "M Maruf, A Daw, KS Mehrab, HB Manogaran, A Neog\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Images are increasingly becoming the currency for documenting biodiversity on the planet, providing novel opportunities for accelerating scientific discoveries in the field of organismal biology, especially with the advent of large vision-language models \u2026"}, {"title": "Teaching Small Language Models to Reason for Knowledge-Intensive Multi-Hop Question Answering", "link": "https://aclanthology.org/2024.findings-acl.464.pdf", "details": "X Li, S He, F Lei, JY JunYang, T Su, K Liu, J Zhao - Findings of the Association for \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) can teach small language models (SLMs) to solve complex reasoning tasks (eg, mathematical question answering) by Chain-of- thought Distillation (CoTD). Specifically, CoTD fine-tunes SLMs by utilizing rationales \u2026"}, {"title": "Fine-Tuning Pre-Trained Language Models with Gaze Supervision", "link": "https://aclanthology.org/2024.acl-short.21.pdf", "details": "S Deng, P Prasse, D Reich, T Scheffer, L J\u00e4ger - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Human gaze data provide cognitive information that reflect human language comprehension and has been effectively integrated into a variety of natural language processing (NLP) tasks, demonstrating improved performance over corresponding \u2026"}, {"title": "PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning", "link": "https://arxiv.org/pdf/2408.16769", "details": "N Hussein, F Shamshad, M Naseer, K Nandakumar - arXiv preprint arXiv:2408.16769, 2024", "abstract": "Medical vision-language models (Med-VLMs) trained on large datasets of medical image-text pairs and later fine-tuned for specific tasks have emerged as a mainstream paradigm in medical image analysis. However, recent studies have \u2026"}, {"title": "Unraveling the Inner Workings of Massive Language Models: Architecture, Training, and Linguistic Capacities", "link": "https://www.igi-global.com/chapter/unraveling-the-inner-workings-of-massive-language-models/354398", "details": "CVS Babu, CSA Anniyappa - Challenges in Large Language Model Development \u2026, 2024", "abstract": "This study explores the evolution of language models, emphasizing the shift from traditional statistical methods to advanced neural networks, particularly the transformer architecture. It aims to understand the impact of these advancements on \u2026"}, {"title": "Improving Sentence Embeddings with Automatic Generation of Training Data Using Few-shot Examples", "link": "https://aclanthology.org/2024.acl-srw.43.pdf", "details": "S Sato, H Tsukagoshi, R Sasano, K Takeda - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best \u2026"}, {"title": "Generating Synthetic Datasets for Few-shot Prompt Tuning", "link": "https://openreview.net/pdf%3Fid%3DVd0KvChLXr", "details": "X Guo, Z Du, B Li, C Miao - First Conference on Language Modeling", "abstract": "A major limitation of prompt tuning is its dependence on large labeled training datasets. Under few-shot learning settings, prompt tuning lags far behind full-model fine-tuning, limiting its scope of application. In this paper, we leverage the powerful \u2026"}]
