[{"title": "Teaching Small Language Models Reasoning through Counterfactual Distillation", "link": "https://aclanthology.org/2024.emnlp-main.333.pdf", "details": "T Feng, Y Li, L Chenglin, H Chen, F Yu, Y Zhang - Proceedings of the 2024 \u2026, 2024", "abstract": "With the rise of large language models (LLMs), many studies are interested in transferring the reasoning capabilities of LLMs to small language models (SLMs). Previous distillation methods usually utilize the capabilities of LLMs to generate \u2026"}, {"title": "Evaluation Metric for Quality Control and Generative Models in Histopathology Images", "link": "https://arxiv.org/pdf/2411.01034", "details": "P Jeevan, N Nixon, A Patil, A Sethi - arXiv preprint arXiv:2411.01034, 2024", "abstract": "Our study introduces ResNet-L2 (RL2), a novel metric for evaluating generative models and image quality in histopathology, addressing limitations of traditional metrics, such as Frechet inception distance (FID), when the data is scarce. RL2 \u2026"}]
