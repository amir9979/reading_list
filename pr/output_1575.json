'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Achieving> 97% on GSM8K: Deeply Understanding the Prob'
[{"title": "Analyzing Chain-of-thought Prompting in Black-Box Large Language Models via Estimated V-information", "link": "https://aclanthology.org/2024.lrec-main.81.pdf", "details": "Z Wang, C Li, Z Yang, Q Liu, Y Hao, X Chen, D Chu\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract Chain-of-Thought (CoT) prompting combined with large language models (LLM) has shown great potential in improving performance on challenging reasoning tasks. While understanding why CoT prompting is effective is crucial for the \u2026"}, {"title": "Correcting Language Model Bias for Text Classification in True Zero-Shot Learning", "link": "https://aclanthology.org/2024.lrec-main.359.pdf", "details": "F Zhao, W Xianlin, C Yan, CK Loo - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Combining pre-trained language models (PLMs) and manual templates is a common practice for text classification in zero-shot scenarios. However, the effect of this approach is highly volatile, ranging from random guesses to near state-of-the-art \u2026"}, {"title": "Retrieval Augmented Generation for Domain-specific Question Answering", "link": "https://arxiv.org/pdf/2404.14760", "details": "S Sharma, DS Yoon, F Dernoncourt, D Sultania\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Question answering (QA) has become an important application in the advanced development of large language models. General pre-trained large language models for question-answering are not trained to properly understand the knowledge or \u2026"}, {"title": "IAD: In-Context Learning Ability Decoupler of Large Language Models in Meta-Training", "link": "https://aclanthology.org/2024.lrec-main.749.pdf", "details": "Y Liu, X Chen, G Xing, J Zhang, R Yan - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) exhibit remarkable In-Context Learning (ICL) ability, where the model learns tasks from prompts consisting of input-output examples. However, the pre-training objectives of LLMs often misalign with ICL \u2026"}, {"title": "Exploring and Mitigating Shortcut Learning for Generative Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.602.pdf", "details": "Z Sun, Y Xiao, J Li, Y Ji, W Chen, M Zhang - Proceedings of the 2024 Joint \u2026, 2024", "abstract": "Recent generative large language models (LLMs) have exhibited incredible instruction-following capabilities while keeping strong task completion ability, even without task-specific fine-tuning. Some works attribute this to the bonus of the new \u2026"}, {"title": "tasksource: A Large Collection of NLP tasks with a Structured Dataset Preprocessing Framework", "link": "https://aclanthology.org/2024.lrec-main.1361.pdf", "details": "D Sileo - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Abstract The HuggingFace Datasets Hub hosts thousands of datasets, offering exciting opportunities for language model training and evaluation. However, datasets for a specific task type often have different structures, making harmonization \u2026"}, {"title": "Prompt Tuning for Few-shot Relation Extraction via Modeling Global and Local Graphs", "link": "https://aclanthology.org/2024.lrec-main.1158.pdf", "details": "Z Zhang, Y Yang, B Chen - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Recently, prompt-tuning has achieved very significant results for few-shot tasks. The core idea of prompt-tuning is to insert prompt templates into the input, thus converting the classification task into a masked language modeling problem. However, for few \u2026"}, {"title": "OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework", "link": "https://arxiv.org/pdf/2404.14619%3Ftrk%3Dpublic_post_comment-text", "details": "S Mehta, MH Sekhavat, Q Cao, M Horton, Y Jin, C Sun\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we \u2026"}, {"title": "UniPSDA: Unsupervised Pseudo Semantic Data Augmentation for Zero-Shot Cross-Lingual Natural Language Understanding", "link": "https://aclanthology.org/2024.lrec-main.1482.pdf", "details": "D Li, T Zhang, J Deng, L Huang, C Wang, X He, H Xue - Proceedings of the 2024 \u2026, 2024", "abstract": "Cross-lingual representation learning transfers knowledge from resource-rich data to resource-scarce ones to improve the semantic understanding abilities of different languages. However, previous works rely on shallow unsupervised data generated \u2026"}]
