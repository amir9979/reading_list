[{"title": "Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding", "link": "https://arxiv.org/pdf/2502.09906", "details": "TD Truong, HQ Nguyen, XB Nguyen, A Dowling, X Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual \u2026"}, {"title": "Utilizing GPT-4 to interpret oral mucosal disease photographs for structured report generation", "link": "https://www.nature.com/articles/s41598-025-89328-y", "details": "ZZ Zhan, YT Xiong, CY Wang, BT Zhang, WJ Lian\u2026 - Scientific Reports, 2025", "abstract": "The aim of this study is to evaluate GPT-4's reasoning ability to interpret oral mucosal disease photos and generate structured reports from free-text inputs, while exploring the role of prompt engineering in enhancing its performance. Prompt received by \u2026"}, {"title": "Language Models Can Predict Their Own Behavior", "link": "https://arxiv.org/pdf/2502.13329", "details": "D Ashok, J May - arXiv preprint arXiv:2502.13329, 2025", "abstract": "Autoregressive Language Models output text by sequentially predicting the next token to generate, with modern methods like Chain-of-Thought (CoT) prompting achieving state-of-the-art reasoning capabilities by scaling the number of generated \u2026"}, {"title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models", "link": "https://arxiv.org/pdf/2502.10250%3F", "details": "GK Kumar, I Chaabane, K Wu - arXiv preprint arXiv:2502.10250, 2025", "abstract": "Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved \u2026"}, {"title": "Adversarial Attacks on Event-Based Pedestrian Detectors: A Physical Approach", "link": "https://arxiv.org/pdf/2503.00377", "details": "G Lin, M Niu, Q Zhu, Z Yin, Z Li, S He, Y Zheng - arXiv preprint arXiv:2503.00377, 2025", "abstract": "Event cameras, known for their low latency and high dynamic range, show great potential in pedestrian detection applications. However, while recent research has primarily focused on improving detection accuracy, the robustness of event-based \u2026"}, {"title": "Pretraining GPT-style models in Hungarian", "link": "https://www.infocommunications.hu/documents/169298/4797540/InfocomJournal_2025_1_EA_1_vj.pdf", "details": "K Szentmih\u00e1lyi, DM Nemeskey, AM Szekeres\u2026", "abstract": "In this paper, we introduce two bilingual large lan-guage models, named OTP-1.5 B and OTP-13B, designed with a focus on both English and Hungarian languages. Both models utilize an 8k token context window and are trained on a dataset of 640 \u2026"}, {"title": "MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for Few-Shot WSI Classification", "link": "https://arxiv.org/pdf/2502.07409", "details": "AT Nguyen, DMH Nguyen, NT Diep, TQ Nguyen, N Ho\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Whole slide pathology image classification presents challenges due to gigapixel image sizes and limited annotation labels, hindering model generalization. This paper introduces a prompt learning method to adapt large vision-language models \u2026"}, {"title": "Improving Domain Generalization for Image Captioning with Unsupervised Prompt Learning", "link": "https://dl.acm.org/doi/pdf/10.1145/3715136", "details": "H Wei, Z Chen - ACM Transactions on Multimedia Computing \u2026, 2025", "abstract": "Pretrained Visual Language Models (PVLMs) have demonstrated impressive zero- shot abilities in image captioning when accompanied by prompt prefixes. However, PVLMs using fixed prompt prefixes may suffer from mode collapse when dealing with \u2026"}, {"title": "Multi-Agent Verification: Scaling Test-Time Compute with Goal Verifiers", "link": "https://openreview.net/pdf%3Fid%3DH22e93wnMe", "details": "S Lifshitz, SA McIlraith, Y Du - Workshop on Reasoning and Planning for Large \u2026", "abstract": "Scaling test-time computation has recently emerged as a promising direction for improving large language model (LLM) performance. A common approach relies on external verifiers---models or programs that assess solution quality---to select \u2026"}]
