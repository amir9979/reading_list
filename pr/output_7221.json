[{"title": "Few-shot Adaptation of Medical Vision-Language Models", "link": "https://arxiv.org/pdf/2409.03868", "details": "F Shakeri, Y Huang, J Silva-Rodr\u00edguez, H Bahig\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Integrating image and text data through multi-modal learning has emerged as a new approach in medical imaging research, following its successful deployment in computer vision. While considerable efforts have been dedicated to establishing \u2026"}, {"title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks", "link": "https://arxiv.org/pdf/2409.07353", "details": "MZ Hossain, A Imteaj - arXiv preprint arXiv:2409.07353, 2024", "abstract": "Large Vision-Language Models (LVLMs), trained on multimodal big datasets, have significantly advanced AI by excelling in vision-language tasks. However, these models remain vulnerable to adversarial attacks, particularly jailbreak attacks, which \u2026"}, {"title": "A Unified Hallucination Mitigation Framework for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.16494", "details": "Y Chang, L Jing, X Zhang, Y Zhang - arXiv preprint arXiv:2409.16494, 2024", "abstract": "Hallucination is a common problem for Large Vision-Language Models (LVLMs) with long generations which is difficult to eradicate. The generation with hallucinations is partially inconsistent with the image content. To mitigate hallucination, current \u2026"}, {"title": "PIP: Detecting Adversarial Examples in Large Vision-Language Models via Attention Patterns of Irrelevant Probe Questions", "link": "https://arxiv.org/pdf/2409.05076", "details": "Y Zhang, R Xie, J Chen, X Sun, Y Wang - arXiv preprint arXiv:2409.05076, 2024", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated their powerful multimodal capabilities. However, they also face serious safety problems, as adversaries can induce robustness issues in LVLMs through the use of well \u2026"}, {"title": "Top-down Activity Representation Learning for Video Question Answering", "link": "https://arxiv.org/pdf/2409.07748", "details": "Y Wang, S Haruta, D Zeng, J Vizcarra, M Kurokawa - arXiv preprint arXiv:2409.07748, 2024", "abstract": "Capturing complex hierarchical human activities, from atomic actions (eg, picking up one present, moving to the sofa, unwrapping the present) to contextual events (eg, celebrating Christmas) is crucial for achieving high-performance video question \u2026"}, {"title": "Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types", "link": "https://arxiv.org/pdf/2409.09269", "details": "N Sinha, V Jain, A Chadha - arXiv preprint arXiv:2409.09269, 2024", "abstract": "Visual Question-Answering (VQA) has become a key use-case in several applications to aid user experience, particularly after Vision-Language Models (VLMs) achieving good results in zero-shot inference. But evaluating different VLMs \u2026"}, {"title": "An Efficient Contrastive Unimodal Pretraining Method for EHR Time Series Data", "link": "https://openreview.net/pdf%3Fid%3DZN5vbwMpgX", "details": "R King, S Kodali, C Krueger, T Yang, BJ Mortazavi - IEEE-EMBS International Conference on \u2026", "abstract": "Machine learning has revolutionized the modeling of clinical timeseries data. Using machine learning, a Deep Neural Network (DNN) can be automatically trained to learn a complex mapping of its input features for a desired task. This is particularly \u2026"}, {"title": "Mitigating Hallucination in Visual-Language Models via Re-Balancing Contrastive Decoding", "link": "https://arxiv.org/pdf/2409.06485", "details": "X Liang, J Yu, L Mu, J Zhuang, J Hu, Y Yang, J Ye, L Lu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Although Visual-Language Models (VLMs) have shown impressive capabilities in tasks like visual question answering and image captioning, they still struggle with hallucinations. Analysis of attention distribution in these models shows that VLMs \u2026"}, {"title": "VTPL: Visual and Text Prompt Learning for visual-language models", "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002360", "details": "B Sun, Z Wu, H Zhang, J He - Journal of Visual Communication and Image \u2026, 2024", "abstract": "Visual-language (VL) models have achieved remarkable success in learning combined visual\u2013textual representations from large web datasets. Prompt learning, as a solution for downstream tasks, can address the forgetting of knowledge \u2026"}]
