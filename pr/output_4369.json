[{"title": "Imperfect Vision Encoders: Efficient and Robust Tuning for Vision-Language Models", "link": "https://arxiv.org/pdf/2407.16526", "details": "A Panos, R Aljundi, DO Reino, RE Turner - arXiv preprint arXiv:2407.16526, 2024", "abstract": "Vision language models (VLMs) demonstrate impressive capabilities in visual question answering and image captioning, acting as a crucial link between visual and language models. However, existing open-source VLMs heavily rely on \u2026"}, {"title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models", "link": "https://arxiv.org/pdf/2407.03181", "details": "H Puerto, T Chubakov, X Zhu, HT Madabushi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model \u2026"}, {"title": "KaPQA: Knowledge-Augmented Product Question-Answering", "link": "https://arxiv.org/pdf/2407.16073", "details": "S Eppalapally, D Dangi, C Bhat, A Gupta, R Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Question-answering for domain-specific applications has recently attracted much interest due to the latest advancements in large language models (LLMs). However, accurately assessing the performance of these applications remains a challenge \u2026"}, {"title": "Semantic Compositions Enhance Vision-Language Contrastive Learning", "link": "https://arxiv.org/pdf/2407.01408", "details": "M Aladago, L Torresani, S Vosoughi - arXiv preprint arXiv:2407.01408, 2024", "abstract": "In the field of vision-language contrastive learning, models such as CLIP capitalize on matched image-caption pairs as positive examples and leverage within-batch non- matching pairs as negatives. This approach has led to remarkable outcomes in zero \u2026"}, {"title": "UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization", "link": "https://arxiv.org/pdf/2407.03525", "details": "MN Uddin, A Saeidi, D Handa, A Seth, TC Son\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces UnSeenTimeQA, a novel time-sensitive question-answering (TSQA) benchmark that diverges from traditional TSQA benchmarks by avoiding factual and web-searchable queries. We present a series of time-sensitive event \u2026"}, {"title": "EAFL: Equilibrium Augmentation Mechanism to Enhance Federated Learning for Aspect Category Sentiment Analysis", "link": "https://www.sciencedirect.com/science/article/pii/S0957417424016956", "details": "KM Ahmad, Q Liu, AA Khan, Y Gan, R Lin - Expert Systems with Applications, 2024", "abstract": "Abstract Aspect Category Sentiment Analysis (ACSA) involves identifying sentiment categories for specific aspects of a sentence. Despite the progress made in pre- trained language models for extracting semantic and categorical representations, the \u2026"}, {"title": "FLMatchQA: a recursive neural network-based question answering with customized federated learning model", "link": "https://peerj.com/articles/cs-2092/", "details": "M Saranya, B Amutha - PeerJ Computer Science, 2024", "abstract": "More sophisticated data access is possible with artificial intelligence (AI) techniques such as question answering (QA), but regulations and privacy concerns have limited their use. Federated learning (FL) deals with these problems, and QA is a viable \u2026"}, {"title": "DDK: Distilling Domain Knowledge for Efficient Large Language Models", "link": "https://arxiv.org/pdf/2407.16154", "details": "J Liu, C Zhang, J Guo, Y Zhang, H Que, K Deng, Z Bai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the advanced intelligence abilities of large language models (LLMs) in various applications, they still face significant computational and storage demands. Knowledge Distillation (KD) has emerged as an effective strategy to improve the \u2026"}, {"title": "A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models", "link": "https://openreview.net/pdf%3Fid%3DrBUbEKOECY", "details": "V Zavarella, JC Gamero, S Consoli - Workshop on Deep Learning and Large Language \u2026", "abstract": "Knowledge graphs (KGs) have been successfully applied to the analysis of complex scientific and technological domains, with automatic KG generation methods typically building upon relation extraction models capturing fine-grained relations between \u2026"}]
