[{"title": "BacPrep: An Experimental Platform for Evaluating LLM-Based Bacalaureat Assessment", "link": "https://arxiv.org/pdf/2506.04989", "details": "DA Marius, D Radu - arXiv preprint arXiv:2506.04989, 2025", "abstract": "\u2026 Offline **LLM** **Evaluation** Setup: Using the stored questions, grading schemes, and the collected student solutions, we will systematically query various LLMs (Both proprietary like: Mistral, Gemini, OpenAI models, Claude models and open-source \u2026", "entry_id": "http://arxiv.org/abs/2506.04989v1", "updated": "2025-06-05 13:02:06", "published": "2025-06-05 13:02:06", "authors": "Dumitran Adrian Marius;Dita Radu", "summary": "Accessing quality preparation and feedback for the Romanian Bacalaureat exam\nis challenging, particularly for students in remote or underserved areas. This\npaper introduces BacPrep, an experimental online platform exploring Large\nLanguage Model (LLM) potential for automated assessment, aiming to offer a\nfree, accessible resource. Using official exam questions from the last 5 years,\nBacPrep employs one of Google's newest models, Gemini 2.0 Flash (released Feb\n2025), guided by official grading schemes, to provide experimental feedback.\nCurrently operational, its primary research function is collecting student\nsolutions and LLM outputs. This focused dataset is vital for planned expert\nvalidation to rigorously evaluate the feasibility and accuracy of this\ncutting-edge LLM in the specific Bacalaureat context before reliable\ndeployment. We detail the design, data strategy, status, validation plan, and\nethics.", "comment": "9 pages Preprint ACCEPTED at BBGI (ITS Workshop)", "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE", "links": "http://arxiv.org/abs/2506.04989v1;http://arxiv.org/pdf/2506.04989v1", "pdf_url": "http://arxiv.org/pdf/2506.04989v1"}, {"title": "Identifying Reliable Evaluation Metrics for Scientific Text Revision", "link": "https://arxiv.org/pdf/2506.04772", "details": "L Jourdan, F Boudin, R Dufour, N Hernandez - arXiv preprint arXiv:2506.04772, 2025", "abstract": "Evaluating text revision in scientific writing remains a challenge, as traditional metrics such as ROUGE and BERTScore primarily focus on similarity rather than capturing meaningful improvements. In this work, we analyse and identify the \u2026", "entry_id": "http://arxiv.org/abs/2506.04772v2", "updated": "2025-06-06 09:54:59", "published": "2025-06-05 09:00:23", "authors": "L\u00e9ane Jourdan;Florian Boudin;Richard Dufour;Nicolas Hernandez", "summary": "Evaluating text revision in scientific writing remains a challenge, as\ntraditional metrics such as ROUGE and BERTScore primarily focus on similarity\nrather than capturing meaningful improvements. In this work, we analyse and\nidentify the limitations of these metrics and explore alternative evaluation\nmethods that better align with human judgments. We first conduct a manual\nannotation study to assess the quality of different revisions. Then, we\ninvestigate reference-free evaluation metrics from related NLP domains.\nAdditionally, we examine LLM-as-a-judge approaches, analysing their ability to\nassess revisions with and without a gold reference. Our results show that LLMs\neffectively assess instruction-following but struggle with correctness, while\ndomain-specific metrics provide complementary insights. We find that a hybrid\napproach combining LLM-as-a-judge evaluation and task-specific metrics offers\nthe most reliable assessment of revision quality.", "comment": "V1 contains only the English version, accepted to ACL 2025 main (26\n  pages). V2 contains both English (ACL 2025) and French (TALN 2025) versions\n  (58 pages)", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.04772v2;http://arxiv.org/pdf/2506.04772v2", "pdf_url": "http://arxiv.org/pdf/2506.04772v2"}, {"title": "DRE: An Effective Dual-Refined Method for Integrating Small and Large Language Models in Open-Domain Dialogue Evaluation", "link": "https://arxiv.org/pdf/2506.04516", "details": "K Zhao, B Yang, C Tang, S Dai, H Tang, C Lin, L Zhan - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) excel at many tasks but struggle with ambiguous scenarios where multiple valid responses exist, often yielding unreliable results. Conversely, Small Language Models (SLMs) demonstrate robustness in such \u2026", "entry_id": "http://arxiv.org/abs/2506.04516v1", "updated": "2025-06-04 23:41:31", "published": "2025-06-04 23:41:31", "authors": "Kun Zhao;Bohao Yang;Chen Tang;Siyuan Dai;Haoteng Tang;Chenghua Lin;Liang Zhan", "summary": "Large Language Models (LLMs) excel at many tasks but struggle with ambiguous\nscenarios where multiple valid responses exist, often yielding unreliable\nresults. Conversely, Small Language Models (SLMs) demonstrate robustness in\nsuch scenarios but are susceptible to misleading or adversarial inputs. We\nobserved that LLMs handle negative examples effectively, while SLMs excel with\npositive examples. To leverage their complementary strengths, we introduce\nSLIDE (Small and Large Integrated for Dialogue Evaluation), a method\nintegrating SLMs and LLMs via adaptive weighting. Building on SLIDE, we further\npropose a Dual-Refinement Evaluation (DRE) method to enhance SLM-LLM\nintegration: (1) SLM-generated insights guide the LLM to produce initial\nevaluations; (2) SLM-derived adjustments refine the LLM's scores for improved\naccuracy. Experiments demonstrate that DRE outperforms existing methods,\nshowing stronger alignment with human judgment across diverse benchmarks. This\nwork illustrates how combining small and large models can yield more reliable\nevaluation tools, particularly for open-ended tasks such as dialogue\nevaluation.", "comment": "arXiv admin note: text overlap with arXiv:2405.15924", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.04516v1;http://arxiv.org/pdf/2506.04516v1", "pdf_url": "http://arxiv.org/pdf/2506.04516v1"}, {"title": "Urania: Differentially Private Insights into AI Use", "link": "https://arxiv.org/pdf/2506.04681", "details": "D Liu, E Cohen, B Ghazi, P Kairouz, P Kamath, A Knop\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Table 2: Embedding space proximity between and comparative **LLM** **evaluation** of private and public summaries. \u2026 **LLM** **Evaluation** Insights. Table 2 shows that LLM evaluators sometimes prefer private summaries over public ones, with comparative \u2026", "entry_id": "http://arxiv.org/abs/2506.04681v1", "updated": "2025-06-05 07:00:31", "published": "2025-06-05 07:00:31", "authors": "Daogao Liu;Edith Cohen;Badih Ghazi;Peter Kairouz;Pritish Kamath;Alexander Knop;Ravi Kumar;Pasin Manurangsi;Adam Sealfon;Da Yu;Chiyuan Zhang", "summary": "We introduce $Urania$, a novel framework for generating insights about LLM\nchatbot interactions with rigorous differential privacy (DP) guarantees. The\nframework employs a private clustering mechanism and innovative keyword\nextraction methods, including frequency-based, TF-IDF-based, and LLM-guided\napproaches. By leveraging DP tools such as clustering, partition selection, and\nhistogram-based summarization, $Urania$ provides end-to-end privacy protection.\nOur evaluation assesses lexical and semantic content preservation, pair\nsimilarity, and LLM-based metrics, benchmarking against a non-private\nClio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple\nempirical privacy evaluation that demonstrates the enhanced robustness of our\nDP pipeline. The results show the framework's ability to extract meaningful\nconversational insights while maintaining stringent user privacy, effectively\nbalancing data utility with privacy preservation.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CL;cs.CR;cs.CY", "links": "http://arxiv.org/abs/2506.04681v1;http://arxiv.org/pdf/2506.04681v1", "pdf_url": "http://arxiv.org/pdf/2506.04681v1"}, {"title": "A Framework Leveraging Large Language Models for Autonomous UAV Control in Flying Networks", "link": "https://arxiv.org/pdf/2506.04404", "details": "D Nunes, R Amorim, P Ribeiro, A Coelho, R Campos - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper proposes FLUC, a modular framework that integrates open-source Large Language Models (LLMs) with Unmanned Aerial Vehicle (UAV) autopilot systems to enable autonomous control in Flying Networks (FNs). FLUC translates high-level \u2026", "entry_id": "http://arxiv.org/abs/2506.04404v1", "updated": "2025-06-04 19:38:09", "published": "2025-06-04 19:38:09", "authors": "Diana Nunes;Ricardo Amorim;Pedro Ribeiro;Andr\u00e9 Coelho;Rui Campos", "summary": "This paper proposes FLUC, a modular framework that integrates open-source\nLarge Language Models (LLMs) with Unmanned Aerial Vehicle (UAV) autopilot\nsystems to enable autonomous control in Flying Networks (FNs). FLUC translates\nhigh-level natural language commands into executable UAV mission code, bridging\nthe gap between operator intent and UAV behaviour.\n  FLUC is evaluated using three open-source LLMs - Qwen 2.5, Gemma 2, and LLaMA\n3.2 - across scenarios involving code generation and mission planning. Results\nshow that Qwen 2.5 excels in multi-step reasoning, Gemma 2 balances accuracy\nand latency, and LLaMA 3.2 offers faster responses with lower logical\ncoherence. A case study on energy-aware UAV positioning confirms FLUC's ability\nto interpret structured prompts and autonomously execute domain-specific logic,\nshowing its effectiveness in real-time, mission-driven control.", "comment": "6 pages, 3 figures, 6 tables", "journal_ref": null, "primary_category": "cs.NI", "categories": "cs.NI;cs.RO", "links": "http://arxiv.org/abs/2506.04404v1;http://arxiv.org/pdf/2506.04404v1", "pdf_url": "http://arxiv.org/pdf/2506.04404v1"}, {"title": "VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal Understanding in Videos", "link": "https://arxiv.org/pdf/2506.05349", "details": "H Rasheed, A Shaker, A Tang, M Maaz, MH Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Mathematical reasoning in real-world video settings presents a fundamentally different challenge than in static images or text. It requires interpreting fine-grained visual information, accurately reading handwritten or digital text, and integrating \u2026", "entry_id": "http://arxiv.org/abs/2506.05349v1", "updated": "2025-06-05 17:59:58", "published": "2025-06-05 17:59:58", "authors": "Hanoona Rasheed;Abdelrahman Shaker;Anqi Tang;Muhammad Maaz;Ming-Hsuan Yang;Salman Khan;Fahad Khan", "summary": "Mathematical reasoning in real-world video settings presents a fundamentally\ndifferent challenge than in static images or text. It requires interpreting\nfine-grained visual information, accurately reading handwritten or digital\ntext, and integrating spoken cues, often dispersed non-linearly over time. In\nsuch multimodal contexts, success hinges not just on perception, but on\nselectively identifying and integrating the right contextual details from a\nrich and noisy stream of content. To this end, we introduce VideoMathQA, a\nbenchmark designed to evaluate whether models can perform such temporally\nextended cross-modal reasoning on videos. The benchmark spans 10 diverse\nmathematical domains, covering videos ranging from 10 seconds to over 1 hour.\nIt requires models to interpret structured visual content, understand\ninstructional narratives, and jointly ground concepts across visual, audio, and\ntextual modalities. We employ graduate-level experts to ensure high quality,\ntotaling over $920$ man-hours of annotation. To reflect real-world scenarios,\nquestions are designed around three core reasoning challenges: direct problem\nsolving, where answers are grounded in the presented question; conceptual\ntransfer, which requires applying learned methods to new problems; and deep\ninstructional comprehension, involving multi-step reasoning over extended\nexplanations and partially worked-out solutions. Each question includes\nmulti-step reasoning annotations, enabling fine-grained diagnosis of model\ncapabilities. Through this benchmark, we highlight the limitations of existing\napproaches and establish a systematic evaluation framework for models that must\nreason, rather than merely perceive, across temporally extended and\nmodality-rich mathematical problem settings. Our benchmark and evaluation code\nare available at: https://mbzuai-oryx.github.io/VideoMathQA", "comment": "VideoMathQA Technical Report", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.05349v1;http://arxiv.org/pdf/2506.05349v1", "pdf_url": "http://arxiv.org/pdf/2506.05349v1"}, {"title": "AutoTA: A Dynamic Intent-Based Virtual Teaching Assistant for Students Using Open Source LLMs", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/11024014.pdf", "details": "R Dahal, G Murray, R Chataut, M Hefeida, A Srivastava\u2026 - IEEE Access, 2025", "abstract": "\u2026 Zero-shot prompting was selected for tasks like lecture/syllabus Q&A and **LLM** **evaluation** , where the input was straightforward, and less ambiguity was present. In qualitative analysis, few-shot prompts helped enforce output structure, while the \u2026"}, {"title": "Context Is Not Comprehension", "link": "https://arxiv.org/pdf/2506.04907", "details": "A Pan, MA Williams - arXiv preprint arXiv:2506.04907, 2025", "abstract": "\u2026 , the VLO generation pipeline is highly extensible, and can support symbolic non-numeric problems, enabling future variants to test reasoning such as abducting causes, inducing general rules, or handling defeasible updates\u2014providing a more \u2026", "entry_id": "http://arxiv.org/abs/2506.04907v2", "updated": "2025-06-08 00:32:54", "published": "2025-06-05 11:41:05", "authors": "Alex Pan;Mary-Anne Williams", "summary": "The dominant evaluation of Large Language Models has centered on their\nability to surface explicit facts from increasingly vast contexts. While\ntoday's best models demonstrate near-perfect recall on these tasks, this\napparent success masks a fundamental failure in multi-step computation when\ninformation is embedded in a narrative. We introduce Verbose ListOps (VLO), a\nnovel benchmark designed to isolate this failure. VLO programmatically weaves\ndeterministic, nested computations into coherent stories, forcing models to\ntrack and update internal state rather than simply locate explicit values. Our\nexperiments show that leading LLMs, capable of solving the raw ListOps\nequations with near-perfect accuracy, collapse in performance on VLO at just\n10k tokens. The VLO framework is extensible to any verifiable reasoning task,\nproviding a critical tool to move beyond simply expanding context windows and\nbegin building models with the robust, stateful comprehension required for\ncomplex knowledge work.", "comment": "24 pages, 2 figures, 4 tables; to appear in AAAI 2026", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.IR;cs.LG", "links": "http://arxiv.org/abs/2506.04907v2;http://arxiv.org/pdf/2506.04907v2", "pdf_url": "http://arxiv.org/pdf/2506.04907v2"}, {"title": "Search Arena: Analyzing Search-Augmented LLMs", "link": "https://arxiv.org/pdf/2506.05334", "details": "M Miroyan, TH Wu, L King, T Li, J Pan, X Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Existing search-augmented **LLM** **evaluation** datasets focus solely on factuality. To study how in-the-wild user prompts from Search Arena differ from SimpleQA [60] and BrowseComp [61] questions, we apply an LLM-based dataset differencing \u2026", "entry_id": "http://arxiv.org/abs/2506.05334v1", "updated": "2025-06-05 17:59:26", "published": "2025-06-05 17:59:26", "authors": "Mihran Miroyan;Tsung-Han Wu;Logan King;Tianle Li;Jiayi Pan;Xinyan Hu;Wei-Lin Chiang;Anastasios N. Angelopoulos;Trevor Darrell;Narges Norouzi;Joseph E. Gonzalez", "summary": "Search-augmented language models combine web search with Large Language\nModels (LLMs) to improve response groundedness and freshness. However,\nanalyzing these systems remains challenging: existing datasets are limited in\nscale and narrow in scope, often constrained to static, single-turn,\nfact-checking questions. In this work, we introduce Search Arena, a\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\ndiverse intents and languages, and contains full system traces with around\n12,000 human preference votes. Our analysis reveals that user preferences are\ninfluenced by the number of citations, even when the cited content does not\ndirectly support the attributed claims, uncovering a gap between perceived and\nactual credibility. Furthermore, user preferences vary across cited sources,\nrevealing that community-driven platforms are generally preferred and static\nencyclopedic sources are not always appropriate and reliable. To assess\nperformance across different settings, we conduct cross-arena analyses by\ntesting search-augmented LLMs in a general-purpose chat environment and\nconventional LLMs in search-intensive settings. We find that web search does\nnot degrade and may even improve performance in non-search settings; however,\nthe quality in search settings is significantly affected if solely relying on\nthe model's parametric knowledge. We open-sourced the dataset to support future\nresearch in this direction. Our dataset and code are available at:\nhttps://github.com/lmarena/search-arena.", "comment": "Preprint. Code: https://github.com/lmarena/search-arena. Dataset:\n  https://huggingface.co/datasets/lmarena-ai/search-arena-24k", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.IR;cs.LG", "links": "http://arxiv.org/abs/2506.05334v1;http://arxiv.org/pdf/2506.05334v1", "pdf_url": "http://arxiv.org/pdf/2506.05334v1"}]
