'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Multimodal Fusion of Echocardiography and Electronic H'
[{"title": "Understanding emergent abilities of language models from the loss perspective", "link": "https://arxiv.org/pdf/2403.15796", "details": "Z Du, A Zeng, Y Dong, J Tang - arXiv preprint arXiv:2403.15796, 2024", "abstract": "Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) \u2026"}, {"title": "An Efficient Approach for Studying Cross-Lingual Transfer in Multilingual Language Models", "link": "https://arxiv.org/pdf/2403.20088", "details": "F Faisal, A Anastasopoulos - arXiv preprint arXiv:2403.20088, 2024", "abstract": "The capacity and effectiveness of pre-trained multilingual models (MLMs) for zero- shot cross-lingual transfer is well established. However, phenomena of positive or negative transfer, and the effect of language choice still need to be fully understood \u2026"}, {"title": "Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data", "link": "https://arxiv.org/pdf/2404.03862", "details": "J Zhang, M Marone, T Li, B Van Durme, D Khashabi - arXiv preprint arXiv:2404.03862, 2024", "abstract": "For humans to trust the fluent generations of large language models (LLMs), they must be able to verify their correctness against trusted, external sources. Recent efforts aim to increase verifiability through citations of retrieved documents or post \u2026"}, {"title": "A Comparison of Parameter-Efficient ASR Domain Adaptation Methods for Universal Speech and Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10445894/", "details": "KC Sim, Z Huo, T Munkhdalai, N Siddhartha, A Stooke\u2026 - ICASSP 2024-2024 IEEE \u2026, 2024", "abstract": "A recent paradigm shift in artificial intelligence has seen the rise of foundation models, such as the large language models and the universal speech models. With billions of model parameters and trained with a wide range of data, these foundation \u2026"}, {"title": "Federated Learning For Heterogeneous Electronic Health Records Utilising Augmented Temporal Graph Attention Networks", "link": "https://proceedings.mlr.press/v238/molaei24a/molaei24a.pdf", "details": "S Molaei, A Thakur, G Niknam, A Soltan, H Zare\u2026 - International Conference on \u2026, 2024", "abstract": "The proliferation of decentralised electronic healthcare records (EHRs) across medical institutions requires innovative federated learning strategies for collaborative data analysis and global model training, prioritising data privacy. A prevalent issue \u2026"}, {"title": "DINGO: Towards Diverse and Fine-Grained Instruction-Following Evaluation", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29768/31322", "details": "Z Gu, X Sun, F Lian, Z Kang, C Xu, J Fan - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Instruction-following is particularly crucial for large language models (LLMs) to support diverse user requests. While existing work has made progress in aligning LLMs with human preferences, evaluating their capabilities on instruction-following \u2026"}, {"title": "Generative Language Models for Personalized Information Understanding", "link": "https://scholarworks.umass.edu/cgi/viewcontent.cgi%3Farticle%3D4123%26context%3Ddissertations_2", "details": "P Cai - 2024", "abstract": "A major challenge in information understanding stems from the diverse nature of the audience, where individuals possess varying preferences, experiences, educational and cultural backgrounds. Consequently, adopting a one-size-fits-all approach to \u2026"}, {"title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models", "link": "https://arxiv.org/pdf/2403.19647", "details": "S Marks, C Rager, EJ Michaud, Y Belinkov, D Bau\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic \u2026"}, {"title": "Learning To Guide Human Decision Makers With Vision-Language Models", "link": "https://arxiv.org/pdf/2403.16501", "details": "D Banerjee, S Teso, BS Grunel, A Passerini - arXiv preprint arXiv:2403.16501, 2024", "abstract": "There is increasing interest in developing AIs for assisting human decision making in\\textit {high-stakes} tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain.% Mainstream approaches team up an \u2026"}]
