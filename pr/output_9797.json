[{"title": "Hidden in Plain Sight: Evaluating Abstract Shape Recognition in Vision-Language Models", "link": "https://arxiv.org/pdf/2411.06287", "details": "A Hemmat, A Davies, TA Lamb, J Yuan, P Torr\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the importance of shape perception in human vision, early neural image classifiers relied less on shape information for object recognition than other (often spurious) features. While recent research suggests that current large Vision \u2026"}, {"title": "ModSCAN: Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities", "link": "https://aclanthology.org/2024.emnlp-main.713.pdf", "details": "Y Jiang, Z Li, X Shen, Y Liu, M Backes, Y Zhang - Proceedings of the 2024 \u2026, 2024", "abstract": "Large vision-language models (LVLMs) have been rapidly developed and widely used in various fields, but the (potential) stereotypical bias in the model is largely unexplored. In this study, we present a pioneering measurement framework \u2026"}, {"title": "ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.15268", "details": "J Chen, T Zhang, S Huang, Y Niu, L Zhang, L Wen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the recent breakthroughs achieved by Large Vision Language Models (LVLMs) in understanding and responding to complex visual-textual contexts, their inherent hallucination tendencies limit their practical application in real-world \u2026"}, {"title": "VERITAS: A Unified Approach to Reliability Evaluation", "link": "https://arxiv.org/pdf/2411.03300%3F", "details": "R Ramamurthy, MA Rajeev, O Molenschot, J Zou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) often fail to synthesize information from their context to generate an accurate response. This renders them unreliable in knowledge intensive settings where reliability of the output is key. A critical component for \u2026"}, {"title": "NYT-Connections: A Deceptively Simple Text Classification Task that Stumps System-1 Thinkers", "link": "https://arxiv.org/pdf/2412.01621", "details": "AYL Lopez, T McDonald, A Emami - arXiv preprint arXiv:2412.01621, 2024", "abstract": "Large Language Models (LLMs) have shown impressive performance on various benchmarks, yet their ability to engage in deliberate reasoning remains questionable. We present NYT-Connections, a collection of 358 simple word \u2026"}, {"title": "DynRank: Improving Passage Retrieval with Dynamic Zero-Shot Prompting Based on Question Classification", "link": "https://arxiv.org/pdf/2412.00600", "details": "A Abdallah, J Mozafari, B Piryani, MM Abdelgwad\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper presents DynRank, a novel framework for enhancing passage retrieval in open-domain question-answering systems through dynamic zero-shot question classification. Traditional approaches rely on static prompts and pre-defined \u2026"}, {"title": "Investigating Large Language Models for Prompt-Based Open-Ended Question Generation in the Technical Domain", "link": "https://link.springer.com/article/10.1007/s42979-024-03464-2", "details": "S Maity, A Deroy, S Sarkar - SN Computer Science, 2024", "abstract": "We explore the automated generation of open-ended questions from technical domain textbooks. These questions are more diverse than those typically examined in the field of question generation (QG) for reading comprehension. To facilitate this \u2026"}, {"title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information", "link": "https://arxiv.org/pdf/2412.00947", "details": "R Kamoi, Y Zhang, SSS Das, RH Zhang, R Zhang - arXiv preprint arXiv:2412.00947, 2024", "abstract": "Errors in understanding visual information in images (ie, visual perception errors) remain a major source of mistakes in Large Vision Language Models (LVLMs). While further analysis is essential, there is a deficiency in datasets for evaluating the visual \u2026"}, {"title": "Lifelong Knowledge Editing for Vision Language Models with Low-Rank Mixture-of-Experts", "link": "https://arxiv.org/pdf/2411.15432", "details": "Q Chen, C Wang, D Wang, T Zhang, W Li, X He - arXiv preprint arXiv:2411.15432, 2024", "abstract": "Model editing aims to correct inaccurate knowledge, update outdated information, and incorporate new data into Large Language Models (LLMs) without the need for retraining. This task poses challenges in lifelong scenarios where edits must be \u2026"}]
