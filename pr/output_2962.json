[{"title": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering", "link": "https://arxiv.org/pdf/2405.12701", "details": "M Jeong, H Hwang, C Yoon, T Lee, J Kang - arXiv preprint arXiv:2405.12701, 2024", "abstract": "In the medical domain, numerous scenarios necessitate the long-form generation ability of large language models (LLMs). Specifically, when addressing patients' questions, it is essential that the model's response conveys factual claims \u2026"}, {"title": "Measuring Retrieval Complexity in Question Answering Systems", "link": "https://arxiv.org/pdf/2406.03592", "details": "M Gabburo, NP Jedema, S Garg, LFR Ribeiro\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we investigate which questions are challenging for retrieval-based Question Answering (QA). We (i) propose retrieval complexity (RC), a novel metric conditioned on the completeness of retrieved documents, which measures the \u2026"}, {"title": "Low-Resource Machine Translation through the Lens of Personalized Federated Learning", "link": "https://arxiv.org/pdf/2406.12564", "details": "V Moskvoretskii, N Tupitsa, C Biemann, S Horv\u00e1th\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present a new approach based on the Personalized Federated Learning algorithm MeritFed that can be applied to Natural Language Tasks with heterogeneous data. We evaluate it on the Low-Resource Machine Translation task \u2026"}, {"title": "FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2405.18218", "details": "Y Zhang, Y Li, X Wang, Q Shen, B Plank, B Bischl\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Overparametrized transformer networks are the state-of-the-art architecture for Large Language Models (LLMs). However, such models contain billions of parameters making large compute a necessity, while raising environmental concerns. To \u2026"}, {"title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via Adaptive Heads Fusion", "link": "https://arxiv.org/pdf/2406.06567", "details": "Y Chen, L Zhang, J Shang, Z Zhang, T Liu, S Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) with billions of parameters demonstrate impressive performance. However, the widely used Multi-Head Attention (MHA) in LLMs incurs substantial computational and memory costs during inference. While some efforts \u2026"}, {"title": "Unlocking the Power of Spatial and Temporal Information in Medical Multimodal Pre-training", "link": "https://arxiv.org/pdf/2405.19654", "details": "J Yang, B Su, WX Zhao, JR Wen - arXiv preprint arXiv:2405.19654, 2024", "abstract": "Medical vision-language pre-training methods mainly leverage the correspondence between paired medical images and radiological reports. Although multi-view spatial images and temporal sequences of image-report pairs are available in off-the-shelf \u2026"}, {"title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback", "link": "https://openreview.net/pdf%3Fid%3DBOorDpKHiJ", "details": "G Cui, L Yuan, N Ding, G Yao, B He, W Zhu, Y Ni, G Xie\u2026 - Forty-first International \u2026, 2024", "abstract": "Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability \u2026"}, {"title": "LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation", "link": "https://arxiv.org/pdf/2406.02876", "details": "Z Sun, Y Liu, F Meng, J Xu, Y Chen, J Zhou - arXiv preprint arXiv:2406.02876, 2024", "abstract": "Multilingual neural machine translation models generally distinguish translation directions by the language tag (LT) in front of the source or target sentences. However, current LT strategies cannot indicate the desired target language as \u2026"}, {"title": "Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning", "link": "https://openreview.net/pdf%3Fid%3DXwnABAdH5y", "details": "Z Zhang, Y Shi, J Zhu, W Zhou, X Qi, H Li - Forty-first International Conference on Machine \u2026", "abstract": "Trustworthiness is an essential prerequisite for the real-world application of large language models. In this paper, we focus on the trustworthiness of language models with respect to retrieval augmentation. Despite being supported with external \u2026"}]
