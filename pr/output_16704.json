[{"title": "FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models", "link": "https://arxiv.org/pdf/2504.20860", "details": "M Singha, S Roy, S Mehrotra, A Jha, M Abdar\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Textual prompt tuning adapts Vision-Language Models (eg, CLIP) in federated learning by tuning lightweight input tokens (or prompts) on local client data, while keeping network weights frozen. Post training, only the prompts are shared by the \u2026", "entry_id": "http://arxiv.org/abs/2504.20860v1", "updated": "2025-04-29 15:36:51", "published": "2025-04-29 15:36:51", "authors": "Mainak Singha;Subhankar Roy;Sarthak Mehrotra;Ankit Jha;Moloud Abdar;Biplab Banerjee;Elisa Ricci", "summary": "Textual prompt tuning adapts Vision-Language Models (e.g., CLIP) in federated\nlearning by tuning lightweight input tokens (or prompts) on local client data,\nwhile keeping network weights frozen. Post training, only the prompts are\nshared by the clients with the central server for aggregation. However, textual\nprompt tuning often struggles with overfitting to known concepts and may be\noverly reliant on memorized text features, limiting its adaptability to unseen\nconcepts. To address this limitation, we propose Federated Multimodal Visual\nPrompt Tuning (FedMVP) that conditions the prompts on comprehensive contextual\ninformation -- image-conditioned features and textual attribute features of a\nclass -- that is multimodal in nature. At the core of FedMVP is a PromptFormer\nmodule that synergistically aligns textual and visual features through\ncross-attention, enabling richer contexual integration. The dynamically\ngenerated multimodal visual prompts are then input to the frozen vision encoder\nof CLIP, and trained with a combination of CLIP similarity loss and a\nconsistency loss. Extensive evaluation on 20 datasets spanning three\ngeneralization settings demonstrates that FedMVP not only preserves performance\non in-distribution classes and domains, but also displays higher\ngeneralizability to unseen classes and domains when compared to\nstate-of-the-art methods. Codes will be released upon acceptance.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2504.20860v1;http://arxiv.org/pdf/2504.20860v1", "pdf_url": "http://arxiv.org/pdf/2504.20860v1"}, {"title": "Zero-shot and few-shot multimodal plastic waste classification with vision-language models", "link": "https://www.sciencedirect.com/science/article/pii/S0956053X25002260", "details": "I Ranjbar, Y Ventikos, M Arashpour - Waste Management, 2025", "abstract": "The construction sector is a large consumer of plastic, generating substantial volumes of plastic waste. Effective recycling of this waste requires accurate classification, as different plastic materials undergo distinct recycling processes to \u2026"}, {"title": "Enhancing pathological myopia diagnosis: a bimodal artificial intelligence approach integrating fundus and optical coherence tomography imaging for precise atrophy \u2026", "link": "https://bjo.bmj.com/content/early/2025/05/20/bjo-2024-326252.abstract", "details": "Z Xu, Y Yang, H Chen, R Han, X Han, J Zhao, W Yu\u2026 - British Journal of \u2026, 2025", "abstract": "Background Pathological myopia (PM) has emerged as a leading cause of global visual impairment, early detection and precise grading of PM are crucial for timely intervention. The atrophy, traction and neovascularisation (ATN) system is applied to \u2026"}, {"title": "PRETI: Patient-Aware Retinal Foundation Model via Metadata-Guided Representation Learning", "link": "https://arxiv.org/pdf/2505.12233", "details": "Y Lee, W Han, Y Jun, H Kim, J Cho, SJ Hwang - arXiv preprint arXiv:2505.12233, 2025", "abstract": "Retinal foundation models have significantly advanced retinal image analysis by leveraging self-supervised learning to reduce dependence on labeled data while achieving strong generalization. Many recent approaches enhance retinal image \u2026", "entry_id": "http://arxiv.org/abs/2505.12233v1", "updated": "2025-05-18 04:59:03", "published": "2025-05-18 04:59:03", "authors": "Yeonkyung Lee;Woojung Han;Youngjun Jun;Hyeonmin Kim;Jungkyung Cho;Seong Jae Hwang", "summary": "Retinal foundation models have significantly advanced retinal image analysis\nby leveraging self-supervised learning to reduce dependence on labeled data\nwhile achieving strong generalization. Many recent approaches enhance retinal\nimage understanding using report supervision, but obtaining clinical reports is\noften costly and challenging. In contrast, metadata (e.g., age, gender) is\nwidely available and serves as a valuable resource for analyzing disease\nprogression. To effectively incorporate patient-specific information, we\npropose PRETI, a retinal foundation model that integrates metadata-aware\nlearning with robust self-supervised representation learning. We introduce\nLearnable Metadata Embedding (LME), which dynamically refines metadata\nrepresentations. Additionally, we construct patient-level data pairs,\nassociating images from the same individual to improve robustness against\nnon-clinical variations. To further optimize retinal image representation, we\npropose Retina-Aware Adaptive Masking (RAAM), a strategy that selectively\napplies masking within the retinal region and dynamically adjusts the masking\nratio during training. PRETI captures both global structures and fine-grained\npathological details, resulting in superior diagnostic performance. Extensive\nexperiments demonstrate that PRETI achieves state-of-the-art results across\ndiverse diseases and biomarker predictions using in-house and public data,\nindicating the importance of metadata-guided foundation models in retinal\ndisease analysis. Our code and pretrained model are available at\nhttps://github.com/MICV-yonsei/PRETI", "comment": "MICCAI2025 early accept", "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.CV", "links": "http://arxiv.org/abs/2505.12233v1;http://arxiv.org/pdf/2505.12233v1", "pdf_url": "http://arxiv.org/pdf/2505.12233v1"}, {"title": "Correlation Between Macular Microstructural Changes with Disease Staging and Visual Acuity in Diabetic Retinopathy", "link": "https://www.tandfonline.com/doi/pdf/10.2147/IJGM.S516938", "details": "W Zhang, F Zhang, Y Yang, J Cao, Z Zhu - International Journal of General Medicine, 2025", "abstract": "Purpose To investigate the changes in macular microvascular structure at different stages of diabetic retinopathy (DR) and the correlation between macular ischemia and visual acuity. Patients and Methods A prospective cross-sectional study was \u2026"}, {"title": "ChemLit-QA: a human evaluated dataset for chemistry RAG tasks", "link": "https://infoscience.epfl.ch/bitstreams/d965efff-2862-483d-8f82-d0f7f144fc33/download", "details": "P Schwaller - MACHINE LEARNING-SCIENCE AND TECHNOLOGY, 2025", "abstract": "Abstract Retrieval-Augmented Generation (RAG) is a widely used strategy in Large- Language Models (LLMs) to extrapolate beyond the inherent pre-trained knowledge. Hence, RAG is crucial when working in data-sparse fields such as Chemistry. The \u2026"}, {"title": "UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System", "link": "https://arxiv.org/pdf/2505.09178", "details": "Y Zhu, Y Yin, Z Shen, Z Zhao, H Song, S Wang, D Shen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The growing complexity and scale of visual model pre-training have made developing and deploying multi-task computer-aided diagnosis (CAD) systems increasingly challenging and resource-intensive. Furthermore, the medical imaging \u2026", "entry_id": "http://arxiv.org/abs/2505.09178v2", "updated": "2025-05-15 12:49:27", "published": "2025-05-14 06:21:27", "authors": "Yitao Zhu;Yuan Yin;Zhenrong Shen;Zihao Zhao;Haiyu Song;Sheng Wang;Dinggang Shen;Qian Wang", "summary": "The growing complexity and scale of visual model pre-training have made\ndeveloping and deploying multi-task computer-aided diagnosis (CAD) systems\nincreasingly challenging and resource-intensive. Furthermore, the medical\nimaging community lacks an open-source CAD platform to enable the rapid\ncreation of efficient and extendable diagnostic models. To address these\nissues, we propose UniCAD, a unified architecture that leverages the robust\ncapabilities of pre-trained vision foundation models to seamlessly handle both\n2D and 3D medical images while requiring only minimal task-specific parameters.\nUniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation\nstrategy is employed to adapt a pre-trained visual model to the medical image\ndomain, achieving performance on par with fully fine-tuned counterparts while\nintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular\narchitecture that combines a frozen foundation model with multiple\nplug-and-play experts, enabling diverse tasks and seamless functionality\nexpansion. Building on this unified CAD architecture, we establish an\nopen-source platform where researchers can share and access lightweight CAD\nexperts, fostering a more equitable and efficient research ecosystem.\nComprehensive experiments across 12 diverse medical datasets demonstrate that\nUniCAD consistently outperforms existing methods in both accuracy and\ndeployment efficiency. The source code and project page are available at\nhttps://mii-laboratory.github.io/UniCAD/.", "comment": "14 pages", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.09178v2;http://arxiv.org/pdf/2505.09178v2", "pdf_url": "http://arxiv.org/pdf/2505.09178v2"}, {"title": "RetinaLogos: Fine-Grained Synthesis of High-Resolution Retinal Images Through Captions", "link": "https://arxiv.org/pdf/2505.12887", "details": "J Ning, C Tang, K Zhou, D Song, L Liu, M Hu, W Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The scarcity of high-quality, labelled retinal imaging data, which presents a significant challenge in the development of machine learning models for ophthalmology, hinders progress in the field. To synthesise Colour Fundus \u2026", "entry_id": "http://arxiv.org/abs/2505.12887v1", "updated": "2025-05-19 09:18:11", "published": "2025-05-19 09:18:11", "authors": "Junzhi Ning;Cheng Tang;Kaijin Zhou;Diping Song;Lihao Liu;Ming Hu;Wei Li;Yanzhou Su;Tianbing Li;Jiyao Liu;Yejin;Sheng Zhang;Yuanfeng Ji;Junjun He", "summary": "The scarcity of high-quality, labelled retinal imaging data, which presents a\nsignificant challenge in the development of machine learning models for\nophthalmology, hinders progress in the field. To synthesise Colour Fundus\nPhotographs (CFPs), existing methods primarily relying on predefined disease\nlabels face significant limitations. However, current methods remain limited,\nthus failing to generate images for broader categories with diverse and\nfine-grained anatomical structures. To overcome these challenges, we first\nintroduce an innovative pipeline that creates a large-scale, synthetic\nCaption-CFP dataset comprising 1.4 million entries, called RetinaLogos-1400k.\nSpecifically, RetinaLogos-1400k uses large language models (LLMs) to describe\nretinal conditions and key structures, such as optic disc configuration,\nvascular distribution, nerve fibre layers, and pathological features.\nFurthermore, based on this dataset, we employ a novel three-step training\nframework, called RetinaLogos, which enables fine-grained semantic control over\nretinal images and accurately captures different stages of disease progression,\nsubtle anatomical variations, and specific lesion types. Extensive experiments\ndemonstrate state-of-the-art performance across multiple datasets, with 62.07%\nof text-driven synthetic images indistinguishable from real ones by\nophthalmologists. Moreover, the synthetic data improves accuracy by 10%-25% in\ndiabetic retinopathy grading and glaucoma detection, thereby providing a\nscalable solution to augment ophthalmic datasets.", "comment": null, "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.CV", "links": "http://arxiv.org/abs/2505.12887v1;http://arxiv.org/pdf/2505.12887v1", "pdf_url": "http://arxiv.org/pdf/2505.12887v1"}, {"title": "ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data", "link": "https://arxiv.org/pdf/2505.10083", "details": "C Wang, Q Qi, Z Rao, L Pan, J Wang, J Liao - arXiv preprint arXiv:2505.10083, 2025", "abstract": "Conventional forecasting methods rely on unimodal time series data, limiting their ability to exploit rich textual information. Recently, large language models (LLMs) and time series foundation models (TSFMs) have demonstrated powerful capability \u2026", "entry_id": "http://arxiv.org/abs/2505.10083v1", "updated": "2025-05-15 08:37:23", "published": "2025-05-15 08:37:23", "authors": "Chengsen Wang;Qi Qi;Zhongwen Rao;Lujia Pan;Jingyu Wang;Jianxin Liao", "summary": "Conventional forecasting methods rely on unimodal time series data, limiting\ntheir ability to exploit rich textual information. Recently, large language\nmodels (LLMs) and time series foundation models (TSFMs) have demonstrated\npowerful capability in textual reasoning and temporal modeling, respectively.\nIntegrating the strengths of both to construct a multimodal model that\nconcurrently leverages both temporal and textual information for future\ninference has emerged as a critical research challenge. To address the scarcity\nof event-series paired data, we propose a decoupled framework: an LLM is\nemployed to transform textual events into revision instructions, which are then\nused to steer the output of TSFM. To implement this framework, we introduce\nChronoSteer, a multimodal TSFM that can be steered through textual revision\ninstructions, effectively bridging LLM and TSFM. Moreover, to mitigate the\nshortage of cross-modal instruction-series paired data, we devise a two-stage\ntraining strategy based on synthetic data. In addition, we also construct a\nhigh-quality multimodal time series forecasting benchmark to address the\ninformation leakage concerns during evaluation. After integrating with an LLM,\nChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%\nimprovement in prediction accuracy compared to the unimodal backbone and a\n22.5% gain over the previous state-of-the-art multimodal method.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.10083v1;http://arxiv.org/pdf/2505.10083v1", "pdf_url": "http://arxiv.org/pdf/2505.10083v1"}]
