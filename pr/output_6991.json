[{"title": "FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs", "link": "https://arxiv.org/pdf/2409.13612", "details": "B Yan, Z Zhang, L Jing, E Hossain, X Du - arXiv preprint arXiv:2409.13612, 2024", "abstract": "The rapid development of Large Vision-Language Models (LVLMs) often comes with widespread hallucination issues, making cost-effective and comprehensive assessments increasingly vital. Current approaches mainly rely on costly annotations \u2026"}, {"title": "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models", "link": "https://arxiv.org/pdf/2409.13592", "details": "A Nandy, Y Agarwal, A Patwa, MM Das, A Bansal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Understanding satire and humor is a challenging task for even current Vision- Language models. In this paper, we propose the challenging tasks of Satirical Image Detection (detecting whether an image is satirical), Understanding (generating the \u2026"}, {"title": "How Does Diverse Interpretability of Textual Prompts Impact Medical Vision-Language Zero-Shot Tasks?", "link": "https://arxiv.org/pdf/2409.00543", "details": "S Wang, C Liu, R Arcucci - arXiv preprint arXiv:2409.00543, 2024", "abstract": "Recent advancements in medical vision-language pre-training (MedVLP) have significantly enhanced zero-shot medical vision tasks such as image classification by leveraging large-scale medical image-text pair pre-training. However, the \u2026"}, {"title": "A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?", "link": "https://arxiv.org/pdf/2409.15277", "details": "Y Xie, J Wu, H Tu, S Yang, B Zhao, Y Zong, Q Jin, C Xie\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across various domains and tasks, pushing the boundaries of our knowledge in learning and cognition. The latest model, OpenAI's o1, stands out as the first LLM with an \u2026"}, {"title": "DP $^ 2$-FedSAM: Enhancing Differentially Private Federated Learning Through Personalized Sharpness-Aware Minimization", "link": "https://arxiv.org/pdf/2409.13645", "details": "Z Zhang, Y Guo, Y Gong - arXiv preprint arXiv:2409.13645, 2024", "abstract": "Federated learning (FL) is a distributed machine learning approach that allows multiple clients to collaboratively train a model without sharing their raw data. To prevent sensitive information from being inferred through the model updates shared \u2026"}, {"title": "Transfer Learning with Clinical Concept Embeddings from Large Language Models", "link": "https://arxiv.org/pdf/2409.13893", "details": "Y Gao, R Bao, Y Ji, Y Sun, C Song, JP Ferraro, Y Ye - arXiv preprint arXiv:2409.13893, 2024", "abstract": "Knowledge sharing is crucial in healthcare, especially when leveraging data from multiple clinical sites to address data scarcity, reduce costs, and enable timely interventions. Transfer learning can facilitate cross-site knowledge transfer, but a \u2026"}, {"title": "Effectively Enhancing Vision Language Large Models by Prompt Augmentation and Caption Utilization", "link": "https://arxiv.org/pdf/2409.14484", "details": "M Zhao, J Wang, Z Li, J Zhang, Z Sun, S Zhou - arXiv preprint arXiv:2409.14484, 2024", "abstract": "Recent studies have shown that Vision Language Large Models (VLLMs) may output content not relevant to the input images. This problem, called the hallucination phenomenon, undoubtedly degrades VLLM performance. Therefore, various anti \u2026"}, {"title": "Pre-trained Language Model and Knowledge Distillation for Lightweight Sequential Recommendation", "link": "https://arxiv.org/pdf/2409.14810", "details": "L Li, M Cheng, Z Liu, H Zhang, Q Liu, E Chen - arXiv preprint arXiv:2409.14810, 2024", "abstract": "Sequential recommendation models user interests based on historical behaviors to provide personalized recommendation. Previous sequential recommendation algorithms primarily employ neural networks to extract features of user interests \u2026"}, {"title": "Knowledge Graph Guided Neural Machine Translation with Dynamic Reinforce-selected Triples", "link": "https://dl.acm.org/doi/pdf/10.1145/3696664", "details": "Y Zhao, X Kang, Y Zhang, J Zhang, Y Zhou, C Zong - ACM Transactions on Asian and Low \u2026", "abstract": "Previous methods incorporating knowledge graphs (KGs) into neural machine translation (NMT) adopt a static knowledge utilization strategy, that introduces many useless knowledge triples and makes the useful triples difficult be utilized by NMT \u2026"}]
