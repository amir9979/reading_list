'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [TiBiX: Leveraging Temporal Information for Bidirection'
[{"title": "Foresight\u2014a generative pretrained transformer for modelling of patient timelines using electronic health records: a retrospective modelling study", "link": "https://www.thelancet.com/journals/landig/article/PIIS2589-7500\\(24\\)00025-6/fulltext", "details": "Z Kraljevic, D Bean, A Shek, R Bendayan\u2026 - The Lancet Digital Health, 2024", "abstract": "Background An electronic health record (EHR) holds detailed longitudinal information about a patient's health status and general clinical history, a large portion of which is stored as unstructured, free text. Existing approaches to model a patient's \u2026"}, {"title": "PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns", "link": "https://arxiv.org/pdf/2403.13315", "details": "YK Chia, VTY Han, D Ghosal, L Bing, S Poria - arXiv preprint arXiv:2403.13315, 2024", "abstract": "Large multimodal models extend the impressive capabilities of large language models by integrating multimodal understanding abilities. However, it is not clear how they can emulate the general intelligence and reasoning ability of humans. As \u2026"}, {"title": "MeDSLIP: Medical Dual-Stream Language-Image Pre-training for Fine-grained Alignment", "link": "https://arxiv.org/html/2403.10635v1", "details": "W Fan, MNI Suvon, S Zhou, X Liu, S Alabed, V Osmani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language pre-training (VLP) models have shown significant advancements in the medical domain. Yet, most VLP models align raw reports to images at a very coarse level, without modeling fine-grained relationships between anatomical and \u2026"}, {"title": "Self-Training Language Models in Arithmetic Reasoning", "link": "https://openreview.net/pdf%3Fid%3DzBh79GuLNO", "details": "M Kadl\u010d\u00edk, M \u0160tef\u00e1nik, O Sotolar, V Martinek - ICLR 2024 Workshop on Large Language \u2026", "abstract": "Recent work shows impressive efficiency of methods for modeling human preferences but achieving further improvements with these methods requires costly human annotations of the quality of model outputs. In this work, we study the \u2026"}, {"title": "A deep transfer learning approach for sleep stage classification and sleep apnea detection using wrist-worn consumer sleep technologies.", "link": "https://europepmc.org/article/med/38498753", "details": "M Olsen, JM Zeitzer, RN Richardson, VH Musgrave\u2026 - IEEE Transactions on Bio \u2026, 2024", "abstract": "Obstructive sleep apnea (OSA) is a common, underdiagnosed sleep-related breathing disorder with serious health implications Objective-We propose a deep transfer learning approach for sleep stage classification and sleep apnea (SA) \u2026"}]
