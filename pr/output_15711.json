[{"title": "Synthetic Data Enhances Mathematical Reasoning of Language Models Based on Artificial Intelligence", "link": "https://www.itc.ktu.lt/index.php/ITC/article/view/39713/16892", "details": "Z Han, W Jiang - Information Technology and Control, 2025", "abstract": "Current large language models (LLMs) training involves extensive training data and computing resources to handle multiple natural language processing (NLP) tasks. This paper endeavors to assist individuals to compose feasible mathematical \u2026"}, {"title": "Can Long-Context Language Models Solve Repository-Level Code Generation?", "link": "https://openreview.net/pdf%3Fid%3DpmcWo9DtDw", "details": "Y PENG, ZZ Wang, D Fried - LTI Student Research Symposium 2025", "abstract": "With the advance of real-world tasks that necessitate increasingly long contexts, recent language models (LMs) have begun to support longer context windows. One particularly complex task is repository-level code generation, where retrieval \u2026"}, {"title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning", "link": "https://arxiv.org/pdf/2504.01005", "details": "N Singhi, H Bansal, A Hosseini, A Grover, KW Chang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC) \u2026"}, {"title": "Evaluating the Confidentiality of Synthetic Clinical Texts Generated by Language Models", "link": "https://hal.science/hal-05046326v1/file/AIME_Confidentiality_of_Synthetic_Clinical_Corpora.pdf", "details": "F Estignard, S Ghannay, J Girard-Satabin, N Hiebel\u2026 - \u2026 International Conference on \u2026, 2025", "abstract": "Large Language Models (LLMs) can be used to produce synthetic documents that mimic real documents when these are not available due to confidentiality or copyright restrictions. Herein, we investigate potential privacy breaches in \u2026"}, {"title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems", "link": "https://arxiv.org/pdf/2504.09037", "details": "Z Ke, F Jiao, Y Ming, XP Nguyen, A Xu, DX Long, M Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reasoning is a fundamental cognitive process that enables logical inference, problem-solving, and decision-making. With the rapid advancement of large language models (LLMs), reasoning has emerged as a key capability that \u2026"}, {"title": "Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal Medical Imaging", "link": "https://arxiv.org/pdf/2504.07336", "details": "S Dai, K Ye, G Liu, H Tang, L Zhan - arXiv preprint arXiv:2504.07336, 2025", "abstract": "Medical image segmentation has achieved remarkable success through the continuous advancement of UNet-based and Transformer-based foundation backbones. However, clinical diagnosis in the real world often requires integrating \u2026"}, {"title": "LLM-Guided Search for Deletion-Correcting Codes", "link": "https://arxiv.org/pdf/2504.00613%3F", "details": "F Weindel, R Heckel - arXiv preprint arXiv:2504.00613, 2025", "abstract": "Finding deletion-correcting codes of maximum size has been an open problem for over 70 years, even for a single deletion. In this paper, we propose a novel approach for constructing deletion-correcting codes. A code is a set of sequences satisfying \u2026"}, {"title": "Large language models are human-like annotators", "link": "https://link.springer.com/chapter/10.1007/978-3-031-88720-8_45", "details": "M Marreddy, SR Oota, M Gupta - European Conference on Information Retrieval, 2025", "abstract": "Large Language Models Are Human-Like Annotators | SpringerLink Skip to main content Advertisement Springer Nature Link Account Menu Find a journal Publish with us Track your research Search Cart 1.Home 2.Advances in Information \u2026"}, {"title": "Knowledge-Centered Dual-Process Reasoning for Math Word Problems with Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10946242/", "details": "J Liu, Z Huang, Q Liu, Z Ma, C Zhai, E Chen - IEEE Transactions on Knowledge and \u2026, 2025", "abstract": "Math word problem (MWP) serves as a critical milestone for assessing the text mining ability and knowledge mastery level of models. Recent advancements have witnessed large language models (LLMs) showcasing remarkable performance on \u2026"}]
