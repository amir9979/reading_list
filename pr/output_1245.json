'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Causal Inference About the Effects of Interventions From Obs'
[{"title": "Structural Pruning of Pre-trained Language Models via Neural Architecture Search", "link": "https://arxiv.org/pdf/2405.02267", "details": "A Klein, J Golebiowski, X Ma, V Perrone\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models (PLM), for example BERT or RoBERTa, mark the state- of-the-art for natural language understanding task when fine-tuned on labeled data. However, their large size poses challenges in deploying them for inference in real \u2026"}, {"title": "MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning", "link": "https://arxiv.org/pdf/2405.05189", "details": "I Nair, L Wang - arXiv preprint arXiv:2405.05189, 2024", "abstract": "We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs). Previous approaches have explored various prompting schemes, yet they suffer from error \u2026"}, {"title": "From Data Imputation to Data Cleaning\u2014Automated Cleaning of Tabular Data Improves Downstream Predictive Performance", "link": "https://proceedings.mlr.press/v238/jager24a/jager24a.pdf", "details": "S J\u00e4ger, F Biessmann - International Conference on Artificial Intelligence and \u2026, 2024", "abstract": "Abstract The translation of Machine Learning (ML) research innovations to real-world applications and the maintenance of ML components are hindered by reoccurring challenges, such as reaching high predictive performance, robustness, complying \u2026"}, {"title": "Continuous Language Model Interpolation for Dynamic and Controllable Text Generation", "link": "https://arxiv.org/pdf/2404.07117", "details": "S Kangaslahti, D Alvarez-Melis - arXiv preprint arXiv:2404.07117, 2024", "abstract": "As large language models (LLMs) have gained popularity for a variety of use cases, making them adaptable and controllable has become increasingly important, especially for user-facing applications. While the existing literature on LLM \u2026"}, {"title": "Zero-shot LLM-guided Counterfactual Generation for Text", "link": "https://arxiv.org/pdf/2405.04793", "details": "A Bhattacharjee, R Moraffah, J Garland, H Liu - arXiv preprint arXiv:2405.04793, 2024", "abstract": "Counterfactual examples are frequently used for model development and evaluation in many natural language processing (NLP) tasks. Although methods for automated counterfactual generation have been explored, such methods depend on models \u2026"}, {"title": "Graph augmentation for node-level few-shot learning", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124005069", "details": "Z Wu, P Zhou, J Ma, J Zhang, G Yuan, X Zhu - Knowledge-Based Systems, 2024", "abstract": "In graph few-shot learning, few-shot node classification (FSNC) at the node-level is a popular downstream task. Previous FSNC methods primarily rely on meta-learning or metric learning techniques, aiming to mine prior knowledge from the base classes \u2026"}, {"title": "EVA-X: A Foundation Model for General Chest X-ray Analysis with Self-supervised Learning", "link": "https://arxiv.org/pdf/2405.05237", "details": "J Yao, X Wang, Y Song, H Zhao, J Ma, Y Chen, W Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The diagnosis and treatment of chest diseases play a crucial role in maintaining human health. X-ray examination has become the most common clinical examination means due to its efficiency and cost-effectiveness. Artificial intelligence analysis \u2026"}, {"title": "THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2405.05256", "details": "P Kaul, Z Li, H Yang, Y Dukler, A Swaminathan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term\" Type I hallucinations\". Instead, they focus on \u2026"}, {"title": "Unraveling the Dilemma of AI Errors: Exploring the Effectiveness of Human and Machine Explanations for Large Language Models", "link": "https://arxiv.org/pdf/2404.07725", "details": "M Pafla, K Larson, M Hancock - arXiv preprint arXiv:2404.07725, 2024", "abstract": "The field of eXplainable artificial intelligence (XAI) has produced a plethora of methods (eg, saliency-maps) to gain insight into artificial intelligence (AI) models, and has exploded with the rise of deep learning (DL). However, human-participant \u2026"}]
