[{"title": "Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training", "link": "https://arxiv.org/pdf/2505.12236", "details": "Q Guo, J Zhang, S Wang, L Tian, Z Kang, B Yan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Few-Shot Relation Extraction (FSRE) remains a challenging task due to the scarcity of annotated data and the limited generalization capabilities of existing models. Although large language models (LLMs) have demonstrated potential in FSRE \u2026", "entry_id": "http://arxiv.org/abs/2505.12236v1", "updated": "2025-05-18 05:17:36", "published": "2025-05-18 05:17:36", "authors": "Quanjiang Guo;Jinchuan Zhang;Sijie Wang;Ling Tian;Zhao Kang;Bin Yan;Weidong Xiao", "summary": "Few-Shot Relation Extraction (FSRE) remains a challenging task due to the\nscarcity of annotated data and the limited generalization capabilities of\nexisting models. Although large language models (LLMs) have demonstrated\npotential in FSRE through in-context learning (ICL), their general-purpose\ntraining objectives often result in suboptimal performance for task-specific\nrelation extraction. To overcome these challenges, we propose TKRE (Two-Stage\nKnowledge-Guided Pre-training for Relation Extraction), a novel framework that\nsynergistically integrates LLMs with traditional relation extraction models,\nbridging generative and discriminative learning paradigms. TKRE introduces two\nkey innovations: (1) leveraging LLMs to generate explanation-driven knowledge\nand schema-constrained synthetic data, addressing the issue of data scarcity;\nand (2) a two-stage pre-training strategy combining Masked Span Language\nModeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational\nreasoning and generalization. Together, these components enable TKRE to\neffectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets\ndemonstrate the efficacy of TKRE, achieving new state-of-the-art performance in\nFSRE and underscoring its potential for broader application in low-resource\nscenarios. \\footnote{The code and data are released on\nhttps://github.com/UESTC-GQJ/TKRE.", "comment": "13 pages, 6 figures, Appear on IJCAI 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.12236v1;http://arxiv.org/pdf/2505.12236v1", "pdf_url": "http://arxiv.org/pdf/2505.12236v1"}, {"title": "FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning", "link": "https://arxiv.org/pdf/2505.14271", "details": "MN Ta, DC Van, DA Hoang, M Le-Anh, T Nguyen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The growing collaboration between humans and AI models in generative tasks has introduced new challenges in distinguishing between human-written, AI-generated, and human-AI collaborative texts. In this work, we collect a multilingual, multi \u2026", "entry_id": "http://arxiv.org/abs/2505.14271v1", "updated": "2025-05-20 12:23:31", "published": "2025-05-20 12:23:31", "authors": "Minh Ngoc Ta;Dong Cao Van;Duc-Anh Hoang;Minh Le-Anh;Truong Nguyen;My Anh Tran Nguyen;Yuxia Wang;Preslav Nakov;Sang Dinh", "summary": "The growing collaboration between humans and AI models in generative tasks\nhas introduced new challenges in distinguishing between human-written,\nAI-generated, and human-AI collaborative texts. In this work, we collect a\nmultilingual, multi-domain, multi-generator dataset FAIDSet. We further\nintroduce a fine-grained detection framework FAID to classify text into these\nthree categories, meanwhile identifying the underlying AI model family. Unlike\nexisting binary classifiers, FAID is built to capture both authorship and\nmodel-specific characteristics. Our method combines multi-level contrastive\nlearning with multi-task auxiliary classification to learn subtle stylistic\ncues. By modeling AI families as distinct stylistic entities, FAID offers\nimproved interpretability. We incorporate an adaptation to address\ndistributional shifts without retraining for unseen data. Experimental results\ndemonstrate that FAID outperforms several baseline approaches, particularly\nenhancing the generalization accuracy on unseen domains and new AI models. It\nprovide a potential solution for improving transparency and accountability in\nAI-assisted writing.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.14271v1;http://arxiv.org/pdf/2505.14271v1", "pdf_url": "http://arxiv.org/pdf/2505.14271v1"}, {"title": "Table Foundation Models: on knowledge pre-training for tabular learning", "link": "https://arxiv.org/pdf/2505.14415", "details": "MJ Kim, F Lefebvre, G Brison, A Perez-Lebel\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Table foundation models bring high hopes to data science: pre-trained on tabular data to embark knowledge or priors, they should facilitate downstream tasks on tables. One specific challenge is that of data semantics: numerical entries take their \u2026", "entry_id": "http://arxiv.org/abs/2505.14415v1", "updated": "2025-05-20 14:27:51", "published": "2025-05-20 14:27:51", "authors": "Myung Jun Kim;F\u00e9lix Lefebvre;Ga\u00ebtan Brison;Alexandre Perez-Lebel;Ga\u00ebl Varoquaux", "summary": "Table foundation models bring high hopes to data science: pre-trained on\ntabular data to embark knowledge or priors, they should facilitate downstream\ntasks on tables. One specific challenge is that of data semantics: numerical\nentries take their meaning from context, e.g., column name. Pre-trained neural\nnetworks that jointly model column names and table entries have recently\nboosted prediction accuracy. While these models outline the promises of world\nknowledge to interpret table values, they lack the convenience of popular\nfoundation models in text or vision. Indeed, they must be fine-tuned to bring\nbenefits, come with sizeable computation costs, and cannot easily be reused or\ncombined with other architectures. Here we introduce TARTE, a foundation model\nthat transforms tables to knowledge-enhanced vector representations using the\nstring to capture semantics. Pre-trained on large relational data, TARTE yields\nrepresentations that facilitate subsequent learning with little additional\ncost. These representations can be fine-tuned or combined with other learners,\ngiving models that push the state-of-the-art prediction performance and improve\nthe prediction/computation performance trade-off. Specialized to a task or a\ndomain, TARTE gives domain-specific representations that facilitate further\nlearning. Our study demonstrates an effective approach to knowledge\npre-training for tabular learning.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.14415v1;http://arxiv.org/pdf/2505.14415v1", "pdf_url": "http://arxiv.org/pdf/2505.14415v1"}, {"title": "ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling", "link": "https://arxiv.org/pdf/2505.12890", "details": "E \u00d6zsoy, C Pellegrini, D Bani-Harouni, K Yuan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The real-world complexity of surgeries necessitates surgeons to have deep and holistic comprehension to ensure precision, safety, and effective interventions. Computational systems are required to have a similar level of comprehension within \u2026", "entry_id": "http://arxiv.org/abs/2505.12890v1", "updated": "2025-05-19 09:20:29", "published": "2025-05-19 09:20:29", "authors": "Ege \u00d6zsoy;Chantal Pellegrini;David Bani-Harouni;Kun Yuan;Matthias Keicher;Nassir Navab", "summary": "The real-world complexity of surgeries necessitates surgeons to have deep and\nholistic comprehension to ensure precision, safety, and effective\ninterventions. Computational systems are required to have a similar level of\ncomprehension within the operating room. Prior works, limited to single-task\nefforts like phase recognition or scene graph generation, lack scope and\ngeneralizability. In this work, we introduce ORQA, a novel OR question\nanswering benchmark and foundational multimodal model to advance OR\nintelligence. By unifying all four public OR datasets into a comprehensive\nbenchmark, we enable our approach to concurrently address a diverse range of OR\nchallenges. The proposed multimodal large language model fuses diverse OR\nsignals such as visual, auditory, and structured data, for a holistic modeling\nof the OR. Finally, we propose a novel, progressive knowledge distillation\nparadigm, to generate a family of models optimized for different speed and\nmemory requirements. We show the strong performance of ORQA on our proposed\nbenchmark, and its zero-shot generalization, paving the way for scalable,\nunified OR modeling and significantly advancing multimodal surgical\nintelligence. We will release our code and data upon acceptance.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.12890v1;http://arxiv.org/pdf/2505.12890v1", "pdf_url": "http://arxiv.org/pdf/2505.12890v1"}, {"title": "CorBenchX: Large-Scale Chest X-Ray Error Dataset and Vision-Language Model Benchmark for Report Error Correction", "link": "https://arxiv.org/pdf/2505.12057", "details": "J Zou, Q Li, C Lian, L Liu, X Yan, S Wang, J Qin - arXiv preprint arXiv:2505.12057, 2025", "abstract": "AI-driven models have shown great promise in detecting errors in radiology reports, yet the field lacks a unified benchmark for rigorous evaluation of error detection and further correction. To address this gap, we introduce CorBenchX, a comprehensive \u2026", "entry_id": "http://arxiv.org/abs/2505.12057v1", "updated": "2025-05-17 15:39:39", "published": "2025-05-17 15:39:39", "authors": "Jing Zou;Qingqiu Li;Chenyu Lian;Lihao Liu;Xiaohan Yan;Shujun Wang;Jing Qin", "summary": "AI-driven models have shown great promise in detecting errors in radiology\nreports, yet the field lacks a unified benchmark for rigorous evaluation of\nerror detection and further correction. To address this gap, we introduce\nCorBenchX, a comprehensive suite for automated error detection and correction\nin chest X-ray reports, designed to advance AI-assisted quality control in\nclinical practice. We first synthesize a large-scale dataset of 26,326 chest\nX-ray error reports by injecting clinically common errors via prompting\nDeepSeek-R1, with each corrupted report paired with its original text, error\ntype, and human-readable description. Leveraging this dataset, we benchmark\nboth open- and closed-source vision-language models,(e.g., InternVL, Qwen-VL,\nGPT-4o, o4-mini, and Claude-3.7) for error detection and correction under\nzero-shot prompting. Among these models, o4-mini achieves the best performance,\nwith 50.6 % detection accuracy and correction scores of BLEU 0.853, ROUGE\n0.924, BERTScore 0.981, SembScore 0.865, and CheXbertF1 0.954, remaining below\nclinical-level accuracy, highlighting the challenge of precise report\ncorrection. To advance the state of the art, we propose a multi-step\nreinforcement learning (MSRL) framework that optimizes a multi-objective reward\ncombining format compliance, error-type accuracy, and BLEU similarity. We apply\nMSRL to QwenVL2.5-7B, the top open-source model in our benchmark, achieving an\nimprovement of 38.3% in single-error detection precision and 5.2% in\nsingle-error correction over the zero-shot baseline.", "comment": "12 pages, 5figures", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.12057v1;http://arxiv.org/pdf/2505.12057v1", "pdf_url": "http://arxiv.org/pdf/2505.12057v1"}, {"title": "AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation", "link": "https://arxiv.org/pdf/2505.11887", "details": "X Zhang, Z Ouyang, L Wang, G de Melo, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the proliferation of large language models (LLMs) in the medical domain, there is increasing demand for improved evaluation techniques to assess their capabilities. However, traditional metrics like F1 and ROUGE, which rely on token overlaps to \u2026", "entry_id": "http://arxiv.org/abs/2505.11887v1", "updated": "2025-05-17 07:44:54", "published": "2025-05-17 07:44:54", "authors": "Xiechi Zhang;Zetian Ouyang;Linlin Wang;Gerard de Melo;Zhu Cao;Xiaoling Wang;Ya Zhang;Yanfeng Wang;Liang He", "summary": "With the proliferation of large language models (LLMs) in the medical domain,\nthere is increasing demand for improved evaluation techniques to assess their\ncapabilities. However, traditional metrics like F1 and ROUGE, which rely on\ntoken overlaps to measure quality, significantly overlook the importance of\nmedical terminology. While human evaluation tends to be more reliable, it can\nbe very costly and may as well suffer from inaccuracies due to limits in human\nexpertise and motivation. Although there are some evaluation methods based on\nLLMs, their usability in the medical field is limited due to their proprietary\nnature or lack of expertise. To tackle these challenges, we present\nAutoMedEval, an open-sourced automatic evaluation model with 13B parameters\nspecifically engineered to measure the question-answering proficiency of\nmedical LLMs. The overarching objective of AutoMedEval is to assess the quality\nof responses produced by diverse models, aspiring to significantly reduce the\ndependence on human evaluation. Specifically, we propose a hierarchical\ntraining method involving curriculum instruction tuning and an iterative\nknowledge introspection mechanism, enabling AutoMedEval to acquire professional\nmedical assessment capabilities with limited instructional data. Human\nevaluations indicate that AutoMedEval surpasses other baselines in terms of\ncorrelation with human judgments.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.11887v1;http://arxiv.org/pdf/2505.11887v1", "pdf_url": "http://arxiv.org/pdf/2505.11887v1"}, {"title": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language Models", "link": "https://arxiv.org/pdf/2505.05130", "details": "M Yi, H Zhang, H Dou, J Zhao, F Shen - arXiv preprint arXiv:2505.05130, 2025", "abstract": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive Language- Image Pre-training (CLIP), have exhibited remarkable zero-shot performance across various image classification tasks. Fine-tuning these models on domain-specific \u2026", "entry_id": "http://arxiv.org/abs/2505.05130v1", "updated": "2025-05-08 11:07:35", "published": "2025-05-08 11:07:35", "authors": "Mengjun Yi;Hanwen Zhang;Hui Dou;Jian Zhao;Furao Shen", "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.", "comment": null, "journal_ref": null, "primary_category": "cs.DC", "categories": "cs.DC", "links": "http://arxiv.org/abs/2505.05130v1;http://arxiv.org/pdf/2505.05130v1", "pdf_url": "http://arxiv.org/pdf/2505.05130v1"}, {"title": "RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection", "link": "https://arxiv.org/pdf/2505.14318", "details": "W Hou, Y Cheng, K Xu, H Li, Y Hu, W Li, J Liu - arXiv preprint arXiv:2505.14318, 2025", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in various domains, including radiology report generation. Previous approaches have attempted to utilize multimodal LLMs for this task, enhancing their performance \u2026", "entry_id": "http://arxiv.org/abs/2505.14318v1", "updated": "2025-05-20 13:05:41", "published": "2025-05-20 13:05:41", "authors": "Wenjun Hou;Yi Cheng;Kaishuai Xu;Heng Li;Yan Hu;Wenjie Li;Jiang Liu", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, including radiology report generation. Previous approaches\nhave attempted to utilize multimodal LLMs for this task, enhancing their\nperformance through the integration of domain-specific knowledge retrieval.\nHowever, these approaches often overlook the knowledge already embedded within\nthe LLMs, leading to redundant information integration and inefficient\nutilization of learned representations. To address this limitation, we propose\nRADAR, a framework for enhancing radiology report generation with supplementary\nknowledge injection. RADAR improves report generation by systematically\nleveraging both the internal knowledge of an LLM and externally retrieved\ninformation. Specifically, it first extracts the model's acquired knowledge\nthat aligns with expert image-based classification outputs. It then retrieves\nrelevant supplementary knowledge to further enrich this information. Finally,\nby aggregating both sources, RADAR generates more accurate and informative\nradiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU\nX-ray demonstrate that our model outperforms state-of-the-art LLMs in both\nlanguage quality and clinical accuracy", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.CL", "links": "http://arxiv.org/abs/2505.14318v1;http://arxiv.org/pdf/2505.14318v1", "pdf_url": "http://arxiv.org/pdf/2505.14318v1"}, {"title": "Online Iterative Self-Alignment for Radiology Report Generation", "link": "https://arxiv.org/pdf/2505.11983", "details": "T Xiao, L Shi, Y Zhang, HF Yang, Z Wang, C Bai - arXiv preprint arXiv:2505.11983, 2025", "abstract": "Radiology Report Generation (RRG) is an important research topic for relieving radiologist'heavy workload. Existing RRG models mainly rely on supervised fine- tuning (SFT) based on different model architectures using data pairs of radiological \u2026", "entry_id": "http://arxiv.org/abs/2505.11983v2", "updated": "2025-05-20 14:49:41", "published": "2025-05-17 12:31:12", "authors": "Ting Xiao;Lei Shi;Yang Zhang;HaoFeng Yang;Zhe Wang;Chenjia Bai", "summary": "Radiology Report Generation (RRG) is an important research topic for\nrelieving radiologist' heavy workload. Existing RRG models mainly rely on\nsupervised fine-tuning (SFT) based on different model architectures using data\npairs of radiological images and corresponding radiologist-annotated reports.\nRecent research has shifted focus to post-training improvements, aligning RRG\nmodel outputs with human preferences using reinforcement learning (RL).\nHowever, the limited data coverage of high-quality annotated data poses risks\nof overfitting and generalization. This paper proposes a novel Online Iterative\nSelf-Alignment (OISA) method for RRG that consists of four stages:\nself-generation of diverse data, self-evaluation for multi-objective preference\ndata,self-alignment for multi-objective optimization and self-iteration for\nfurther improvement. Our approach allows for generating varied reports tailored\nto specific clinical objectives, enhancing the overall performance of the RRG\nmodel iteratively. Unlike existing methods, our frame-work significantly\nincreases data quality and optimizes performance through iterative\nmulti-objective optimization. Experimental results demonstrate that our method\nsurpasses previous approaches, achieving state-of-the-art performance across\nmultiple evaluation metrics.", "comment": "Accepted by ACL 2025 Main", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.11983v2;http://arxiv.org/pdf/2505.11983v2", "pdf_url": "http://arxiv.org/pdf/2505.11983v2"}]
