[{"title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "link": "https://arxiv.org/pdf/2405.00402", "details": "L Ranaldi, A Freitas - arXiv preprint arXiv:2405.00402, 2024", "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches \u2026"}, {"title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning", "link": "https://arxiv.org/pdf/2404.17140", "details": "Y Zhang, M Khalifa, L Logeswaran, J Kim, M Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors. This work explores whether \u2026"}, {"title": "Causal Evaluation of Language Models", "link": "https://arxiv.org/pdf/2405.00622", "details": "S Chen, B Peng, M Chen, R Wang, M Xu, X Zeng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for \u2026"}, {"title": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering", "link": "https://arxiv.org/pdf/2405.13873", "details": "Y Sui, Y He, N Liu, X He, K Wang, B Hooi - arXiv preprint arXiv:2405.13873, 2024", "abstract": "While large language models (LLMs) have achieved significant success in various applications, they often struggle with hallucinations, especially in scenarios that require deep and responsible reasoning. These issues could be partially mitigate by \u2026"}, {"title": "Efficient Medical Question Answering with Knowledge-Augmented Question Generation", "link": "https://arxiv.org/pdf/2405.14654", "details": "J Khlaut, C Dancette, E Ferreres, A Bennani, P H\u00e9rent\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the expanding field of language model applications, medical knowledge representation remains a significant challenge due to the specialized nature of the domain. Large language models, such as GPT-4, obtain reasonable scores on \u2026"}, {"title": "Federated Learning for Exploiting Annotators' Disagreements in Natural Language Processing", "link": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00664/121195", "details": "N Rodr\u00edguez-Barroso, EM C\u00e1mara, JC Collados\u2026 - Transactions of the \u2026, 2024", "abstract": "The annotation of ambiguous or subjective NLP tasks is usually addressed by various annotators. In most datasets, these annotations are aggregated into a single ground truth. However, this omits divergent opinions of annotators, hence missing \u2026"}, {"title": "Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation", "link": "https://arxiv.org/pdf/2404.17489", "details": "W Cui, R Hosseinzadeh, J Ma, T Wu, Y Sui, K Golestan - arXiv preprint arXiv \u2026, 2024", "abstract": "Contrastive learning is a model pre-training technique by first creating similar views of the original data, and then encouraging the data and its corresponding views to be close in the embedding space. Contrastive learning has witnessed success in image \u2026"}, {"title": "Fine-Tuning and Retrieval Augmented Generation for Question Answering Using Affordable Large Language Models", "link": "https://aclanthology.org/2024.unlp-1.10.pdf", "details": "T Boro\u015f, R Chivereanu, S Dumitrescu, O Purcaru - Proceedings of the Third Ukrainian \u2026, 2024", "abstract": "We present our proposed system named Sherlock to UNLP 2024 Shared Task on Question Answering winning first place. We employ a mix of methods, from using automatically translated datasets to perform supervised fine-tuning and direct \u2026"}, {"title": "AraMed: Arabic Medical Question Answering using Pretrained Transformer Language Models", "link": "https://aclanthology.org/2024.osact-1.6.pdf", "details": "A Alasmari, S Alhumoud, W Alshammari - Proceedings of the 6th Workshop on Open \u2026, 2024", "abstract": "Abstract Medical Question Answering systems have gained significant attention in recent years due to their potential to enhance medical decision-making and improve patient care. However, most of the research in this field has focused on English \u2026"}]
