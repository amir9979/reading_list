[{"title": "NetPress: Dynamically Generated LLM Benchmarks for Network Applications", "link": "https://arxiv.org/pdf/2506.03231", "details": "Y Zhou, J Ruan, ES Wang, S Fouladi, FY Yan, K Hsieh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 We propose NETPRESS, a dynamic and general pipeline for building **LLM** **evaluation** benchmarks in real-world network applications. By unifying state-action abstractions across constructive and reactive tasks, NETPRESS enables dynamic \u2026", "entry_id": "http://arxiv.org/abs/2506.03231v1", "updated": "2025-06-03 14:04:22", "published": "2025-06-03 14:04:22", "authors": "Yajie Zhou;Jiajun Ruan;Eric S. Wang;Sadjad Fouladi;Francis Y. Yan;Kevin Hsieh;Zaoxing Liu", "summary": "Despite growing interest in domain-specific benchmarking of large language\nmodels (LLMs) and agents, current evaluations remain limited to static,\nsmall-scale datasets, especially in high-stakes tasks like network operations\nthat demand reliability for deployments. We present NetPress, an automated\nbenchmark generation framework for evaluating LLM agents in network\napplications. NetPress introduces a unified abstraction with state and action,\nenabling dynamic generation of diverse query sets along with corresponding\nground truths. At runtime, users can specify benchmark configurations to\ngenerate millions of queries on the fly. In addition to dynamic benchmark\nconstruction, NetPress integrates with network emulators to provide realistic\nenvironment feedback, supporting comprehensive evaluation across correctness,\nsafety, and latency. We instantiate NetPress on three representative\napplications, revealing interesting fine-grained differences in agent behavior\nthat static, correctness-only benchmarks often miss. NetPress moves LLM\nevaluation toward realistic, scalable testing in infrastructure-centric\ndomains, helping close the gap between benchmark performance and real-world\ndeployment readiness. Code is available at\nhttps://github.com/Froot-NetSys/NetPress.", "comment": null, "journal_ref": null, "primary_category": "cs.NI", "categories": "cs.NI;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2506.03231v1;http://arxiv.org/pdf/2506.03231v1", "pdf_url": "http://arxiv.org/pdf/2506.03231v1"}, {"title": "Engaging Students in Scientific Writing: The STRaWBERRY Checklist Framework with LLM-based Paper Draft Assessment", "link": "https://ieeexplore.ieee.org/abstract/document/11016556/", "details": "A Theissler, M Klaiber, F Gerschner, P Ritzer, J Wang - 2025 IEEE Global \u2026, 2025", "abstract": "\u2026 The **LLM** **evaluation** process encourages active learning (in the educational sense) and allows the drafts to be iteratively refined through feedback from the LLM. We evaluate the STRaWBERRY framework by its use in lectures and the corresponding \u2026"}, {"title": "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games", "link": "https://arxiv.org/pdf/2506.03610", "details": "D Park, M Kim, B Choi, J Kim, K Lee, J Lee, I Park\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Model (LLM) agents are reshaping the game industry, particularly with more intelligent and human-preferable game characters. However, existing game benchmarks fall short of practical needs: they lack evaluations of diverse LLM \u2026", "entry_id": "http://arxiv.org/abs/2506.03610v1", "updated": "2025-06-04 06:40:33", "published": "2025-06-04 06:40:33", "authors": "Dongmin Park;Minkyu Kim;Beongjun Choi;Junhyuck Kim;Keon Lee;Jonghyun Lee;Inkyu Park;Byeong-Uk Lee;Jaeyoung Hwang;Jaewoo Ahn;Ameya S. Mahabaleshwarkar;Bilal Kartal;Pritam Biswas;Yoshi Suhara;Kangwook Lee;Jaewoong Cho", "summary": "Large Language Model (LLM) agents are reshaping the game industry,\nparticularly with more intelligent and human-preferable game characters.\nHowever, existing game benchmarks fall short of practical needs: they lack\nevaluations of diverse LLM capabilities across various game genres, studies of\nagentic modules crucial for complex gameplay, and fine-tuning datasets for\naligning pre-trained LLMs into gaming agents. To fill these gaps, we present\n\\textbf{\\benchname{}}, a foundational benchmark designed to train and evaluate\nLLM agents across diverse real-world video games. Unlike existing benchmarks,\nOrak includes 12 popular video games spanning all major genres, enabling\ncomprehensive studies of LLM capabilities and agentic modules essential for\nintricate game scenarios. To support consistent evaluation of LLMs, we\nintroduce a plug-and-play interface based on Model Context Protocol (MCP) that\nenables LLMs to seamlessly connect with games and manipulate agentic modules.\nAdditionally, we propose a fine-tuning dataset, consisting of LLM gameplay\ntrajectories across diverse game genres. Orak offers a comprehensive evaluation\nframework, encompassing general game score leaderboards, LLM battle arenas, and\nin-depth analyses of visual input state, agentic strategies, and fine-tuning\neffects, establishing a foundation towards building generic gaming agents. Code\nis available at https://github.com/krafton-ai/Orak.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2506.03610v1;http://arxiv.org/pdf/2506.03610v1", "pdf_url": "http://arxiv.org/pdf/2506.03610v1"}, {"title": "More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning", "link": "https://arxiv.org/pdf/2506.03923", "details": "M Shafiei, H Saffari, NS Moosavi - arXiv preprint arXiv:2506.03923, 2025", "abstract": "Large language models (LLMs) are known to be sensitive to input phrasing, but the mechanisms by which semantic cues shape reasoning remain poorly understood. We investigate this phenomenon in the context of comparative math problems with \u2026", "entry_id": "http://arxiv.org/abs/2506.03923v1", "updated": "2025-06-04 13:15:01", "published": "2025-06-04 13:15:01", "authors": "Mohammadamin Shafiei;Hamidreza Saffari;Nafise Sadat Moosavi", "summary": "Large language models (LLMs) are known to be sensitive to input phrasing, but\nthe mechanisms by which semantic cues shape reasoning remain poorly understood.\nWe investigate this phenomenon in the context of comparative math problems with\nobjective ground truth, revealing a consistent and directional framing bias:\nlogically equivalent questions containing the words ``more'', ``less'', or\n``equal'' systematically steer predictions in the direction of the framing\nterm. To study this effect, we introduce MathComp, a controlled benchmark of\n300 comparison scenarios, each evaluated under 14 prompt variants across three\nLLM families. We find that model errors frequently reflect linguistic steering,\nsystematic shifts toward the comparative term present in the prompt.\nChain-of-thought prompting reduces these biases, but its effectiveness varies:\nfree-form reasoning is more robust, while structured formats may preserve or\nreintroduce directional drift. Finally, we show that including demographic\nidentity terms (e.g., ``a woman'', ``a Black person'') in input scenarios\namplifies directional drift, despite identical underlying quantities,\nhighlighting the interplay between semantic framing and social referents. These\nfindings expose critical blind spots in standard evaluation and motivate\nframing-aware benchmarks for diagnosing reasoning robustness and fairness in\nLLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.03923v1;http://arxiv.org/pdf/2506.03923v1", "pdf_url": "http://arxiv.org/pdf/2506.03923v1"}, {"title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents", "link": "https://arxiv.org/pdf/2506.04018", "details": "A Naik, P Quinn, G Bosch, E Goun\u00e9, FJC Zabala\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Traditional **LLM** **evaluation** often probes capability via direct, sometimes adversarial questions, while newer agentic benchmarks seek to quantify real-world propensity for violation of ethical or operational constraints Huang et al. [2024], Park et al. [2023]. \u2026", "entry_id": "http://arxiv.org/abs/2506.04018v1", "updated": "2025-06-04 14:46:47", "published": "2025-06-04 14:46:47", "authors": "Akshat Naik;Patrick Quinn;Guillermo Bosch;Emma Goun\u00e9;Francisco Javier Campos Zabala;Jason Ross Brown;Edward James Young", "summary": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. Prior work has examined agents' ability to enact\nmisaligned behaviour (misalignment capability) and their compliance with\nharmful instructions (misuse propensity). However, the likelihood of agents\nattempting misaligned behaviours in real-world settings (misalignment\npropensity) remains poorly understood. We introduce a misalignment propensity\nbenchmark, AgentMisalignment, consisting of a suite of realistic scenarios in\nwhich LLM agents have the opportunity to display misaligned behaviour. We\norganise our evaluations into subcategories of misaligned behaviours, including\ngoal-guarding, resisting shutdown, sandbagging, and power-seeking. We report\nthe performance of frontier models on our benchmark, observing higher\nmisalignment on average when evaluating more capable models. Finally, we\nsystematically vary agent personalities through different system prompts. We\nfind that persona characteristics can dramatically and unpredictably influence\nmisalignment tendencies -- occasionally far more than the choice of model\nitself -- highlighting the importance of careful system prompt engineering for\ndeployed AI agents. Our work highlights the failure of current alignment\nmethods to generalise to LLM agents, and underscores the need for further\npropensity evaluations as autonomous systems become more prevalent.", "comment": "Prepint, under review for NeurIPS 2025", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL;cs.CY;cs.LG;I.2.7; I.2.11; K.4.1; I.2.6", "links": "http://arxiv.org/abs/2506.04018v1;http://arxiv.org/pdf/2506.04018v1", "pdf_url": "http://arxiv.org/pdf/2506.04018v1"}, {"title": "ScoreRAG: A Retrieval-Augmented Generation Framework with Consistency-Relevance Scoring and Structured Summarization for News Generation", "link": "https://arxiv.org/pdf/2506.03704", "details": "PY Lin, Y Tsai - arXiv preprint arXiv:2506.03704, 2025", "abstract": "\u2026 **LLM** **evaluation** and expert evaluation show that the ScoreRAG approach outperforms the Zeroshot baseline across all dimensions, particularly in accuracy and informativeness, which are critical metrics for news generation. Overall, **LLM** \u2026", "entry_id": "http://arxiv.org/abs/2506.03704v1", "updated": "2025-06-04 08:35:06", "published": "2025-06-04 08:35:06", "authors": "Pei-Yun Lin;Yen-lung Tsai", "summary": "This research introduces ScoreRAG, an approach to enhance the quality of\nautomated news generation. Despite advancements in Natural Language Processing\nand large language models, current news generation methods often struggle with\nhallucinations, factual inconsistencies, and lack of domain-specific expertise\nwhen producing news articles. ScoreRAG addresses these challenges through a\nmulti-stage framework combining retrieval-augmented generation, consistency\nrelevance evaluation, and structured summarization. The system first retrieves\nrelevant news documents from a vector database, maps them to complete news\nitems, and assigns consistency relevance scores based on large language model\nevaluations. These documents are then reranked according to relevance, with\nlow-quality items filtered out. The framework proceeds to generate graded\nsummaries based on relevance scores, which guide the large language model in\nproducing complete news articles following professional journalistic standards.\nThrough this methodical approach, ScoreRAG aims to significantly improve the\naccuracy, coherence, informativeness, and professionalism of generated news\narticles while maintaining stability and consistency throughout the generation\nprocess. The code and demo are available at:\nhttps://github.com/peiyun2260/ScoreRAG.", "comment": "11 pages, 8 figures. Code and demo available at\n  https://github.com/peiyun2260/ScoreRAG. Submitted to arXiv for public access;\n  journal submission planned", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;68T50;I.2.7", "links": "http://arxiv.org/abs/2506.03704v1;http://arxiv.org/pdf/2506.03704v1", "pdf_url": "http://arxiv.org/pdf/2506.03704v1"}, {"title": "N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based Matrix Completion", "link": "https://arxiv.org/pdf/2506.04166", "details": "C Chin, A Khubchandani, H Maskara, K Choi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 We also release a benchmark suite of real-world datasets\u2014from healthcare and recommender systems to causal inference and **LLM** **evaluation** \u2014designed to stress-test matrix completion methods beyond synthetic scenarios. Our experiments \u2026", "entry_id": "http://arxiv.org/abs/2506.04166v1", "updated": "2025-06-04 17:04:34", "published": "2025-06-04 17:04:34", "authors": "Caleb Chin;Aashish Khubchandani;Harshvardhan Maskara;Kyuseong Choi;Jacob Feitelberg;Albert Gong;Manit Paul;Tathagata Sadhukhan;Anish Agarwal;Raaz Dwivedi", "summary": "Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix\ncompletion, offering strong empirical performance and recent theoretical\nguarantees, including entry-wise error bounds, confidence intervals, and\nminimax optimality. Despite their simplicity, recent work has shown that NN\napproaches are robust to a range of missingness patterns and effective across\ndiverse applications. This paper introduces N$^2$, a unified Python package and\ntestbed that consolidates a broad class of NN-based methods through a modular,\nextensible interface. Built for both researchers and practitioners, N$^2$\nsupports rapid experimentation and benchmarking. Using this framework, we\nintroduce a new NN variant that achieves state-of-the-art results in several\nsettings. We also release a benchmark suite of real-world datasets, from\nhealthcare and recommender systems to causal inference and LLM evaluation,\ndesigned to stress-test matrix completion methods beyond synthetic scenarios.\nOur experiments demonstrate that while classical methods excel on idealized\ndata, NN-based techniques consistently outperform them in real-world settings.", "comment": "21 pages, 6 figures", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;stat.CO;stat.ML", "links": "http://arxiv.org/abs/2506.04166v1;http://arxiv.org/pdf/2506.04166v1", "pdf_url": "http://arxiv.org/pdf/2506.04166v1"}, {"title": "From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models", "link": "https://arxiv.org/pdf/2506.03592", "details": "V Hangya, F K\u00fcch, D Gold - arXiv preprint arXiv:2506.03592, 2025", "abstract": "Iterative evaluation of LLMs during training is essential to ensure expected capability development, but can be time- and compute-intensive. While NLU tasks, where the model selects from fixed answer choices, are cheap to evaluate, essential \u2026", "entry_id": "http://arxiv.org/abs/2506.03592v1", "updated": "2025-06-04 05:46:40", "published": "2025-06-04 05:46:40", "authors": "Viktor Hangya;Fabian K\u00fcch;Darina Gold", "summary": "Iterative evaluation of LLMs during training is essential to ensure expected\ncapability development, but can be time- and compute-intensive. While NLU\ntasks, where the model selects from fixed answer choices, are cheap to\nevaluate, essential capabilities like reasoning and code generation rely on the\nmore time-consuming NLG (token-by-token generation) format. In this work, our\naim is to decrease the computational burden of NLG benchmarks in order to\nenable monitoring crucial LLM capabilities during model training. We\nreformulate generative tasks into computationally cheaper NLU alternatives. We\ntest the performance correlation between the original and reformulated tasks\nusing 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code\ngeneration, factual knowledge and reading comprehension. Our results show a\nstrong correlation between task formats, supporting capability assessment via\ncheaper alternatives and achieving over 35x average reduction in evaluation\ntime. We plan to publish our benchmark adaptions.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.03592v1;http://arxiv.org/pdf/2506.03592v1", "pdf_url": "http://arxiv.org/pdf/2506.03592v1"}, {"title": "Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using Model Context Protocol (MCP)", "link": "https://www.researchgate.net/profile/Vincent-Koc-2/publication/392399692_Mind_the_Metrics_Patterns_for_Telemetry-Aware_In-IDE_AI_Application_Development_using_Model_Context_Protocol_MCP/links/684076436a754f72b5907486/Mind-the-Metrics-Patterns-for-Telemetry-Aware-In-IDE-AI-Application-Development-using-Model-Context-Protocol-MCP.pdf", "details": "V Koc, J Verre, D Blank, A Morgan", "abstract": "\u2026 Integration with **LLM** **Evaluation** Research: Our paradigm intertwines with the complex field of **LLM** **evaluation** \u2013 using LLMs as judges or designing new metrics for quality. The telemetry platform provides the data for evaluation, but what metrics \u2026"}]
