'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [RIFF: Learning to Rephrase Inputs for Few-shot Fine-t'
[{"title": "Ensemble pretrained language models to extract biomedical knowledge from literature", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae061/7634192", "details": "Z Li, Q Wei, LC Huang, J Li, Y Hu, YS Chuang, J He\u2026 - Journal of the American \u2026, 2024", "abstract": "Objectives The rapid expansion of biomedical literature necessitates automated techniques to discern relationships between biomedical concepts from extensive free text. Such techniques facilitate the development of detailed knowledge bases and \u2026"}, {"title": "FairBelief-Assessing Harmful Beliefs in Language Models", "link": "https://arxiv.org/pdf/2402.17389", "details": "M Setzu, MM Manerba, P Minervini, D Nozza - arXiv preprint arXiv:2402.17389, 2024", "abstract": "Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real- world applications without careful fairness auditing. This paper proposes FairBelief \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models", "link": "https://arxiv.org/pdf/2403.02436", "details": "X Lu, Y Zhao, B Qin - arXiv preprint arXiv:2403.02436, 2024", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Do Language Models Care About Text Quality? Evaluating Web-Crawled Corpora Across 11 Languages", "link": "https://arxiv.org/pdf/2403.08693", "details": "R van Noord, T Kuzman, P Rupnik, N Ljube\u0161i\u0107\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large, curated, web-crawled corpora play a vital role in training language models (LMs). They form the lion's share of the training data in virtually all recent LMs, such as the well-known GPT, LLaMA and XLM-RoBERTa models. However, despite this \u2026"}, {"title": "Identifying prognostic factors for survival in intensive care unit patients with SIRS or sepsis by machine learning analysis on electronic health records", "link": "https://journals.plos.org/digitalhealth/article%3Fid%3D10.1371/journal.pdig.0000459", "details": "M Mollura, D Chicco, A Paglialonga, R Barbieri - PLOS Digital Health, 2024", "abstract": "Background Systemic inflammatory response syndrome (SIRS) and sepsis are the most common causes of in-hospital death. However, the characteristics associated with the improvement in the patient conditions during the ICU stay were not fully \u2026"}, {"title": "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models", "link": "https://arxiv.org/html/2403.02178v1", "details": "C Chen, X Wang, TE Lin, A Lv, Y Wu, X Gao, JR Wen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains. Earlier fine- tuning approaches sought to mitigate this by leveraging more precise supervisory \u2026"}, {"title": "Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods", "link": "https://arxiv.org/pdf/2403.00998", "details": "P Tsvilodub, H Wang, S Grosch, M Franke - arXiv preprint arXiv:2403.00998, 2024", "abstract": "This paper systematically compares different methods of deriving item-level predictions of language models for multiple-choice tasks. It compares scoring methods for answer options based on free generation of responses, various \u2026"}, {"title": "SPContrastNet: A Self-Paced Contrastive Learning Model for Few-Shot Text Classification", "link": "https://dl.acm.org/doi/pdf/10.1145/3652600", "details": "J Chen, R Zhang, X Jiang, C Hu - ACM Transactions on Information Systems, 2024", "abstract": "Meta-learning has recently promoted few-shot text classification, which identifies target classes based on information transferred from source classes through a series of small tasks or episodes. Existing works constructing their meta-learner on \u2026"}, {"title": "SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization", "link": "https://arxiv.org/pdf/2403.13240", "details": "J Parnell, IJ Unanue, M Piccardi - arXiv preprint arXiv:2403.13240, 2024", "abstract": "Cross-lingual summarization (XLS) generates summaries in a language different from that of the input documents (eg, English to Spanish), allowing speakers of the target language to gain a concise view of their content. In the present day, the \u2026"}]
