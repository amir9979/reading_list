[{"title": "Reinforced Prompt Personalization for Recommendation with Large Language Models", "link": "https://arxiv.org/pdf/2407.17115", "details": "W Mao, J Wu, W Chen, C Gao, X Wang, X He - arXiv preprint arXiv:2407.17115, 2024", "abstract": "Designing effective prompts can empower LLMs to understand user preferences and provide recommendations by leveraging LLMs' intent comprehension and knowledge utilization capabilities. However, existing research predominantly \u2026"}, {"title": "Assessing Student Explanations with Large Language Models Using Fine-Tuning and Few-Shot Learning", "link": "https://aclanthology.org/2024.bea-1.33.pdf", "details": "D Carpenter, W Min, S Lee, G Ozogul, X Zheng\u2026 - \u2026 on Innovative Use of NLP for \u2026, 2024", "abstract": "The practice of soliciting self-explanations from students is widely recognized for its pedagogical benefits. However, the labor-intensive effort required to manually assess students' explanations makes it impractical for classroom settings. As a result \u2026"}, {"title": "SDoH-GPT: Using Large Language Models to Extract Social Determinants of Health (SDoH)", "link": "https://arxiv.org/pdf/2407.17126", "details": "B Consoli, X Wu, S Wang, X Zhao, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Extracting social determinants of health (SDoH) from unstructured medical notes depends heavily on labor-intensive annotations, which are typically task-specific, hampering reusability and limiting sharing. In this study we introduced SDoH-GPT, a \u2026"}, {"title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application", "link": "https://arxiv.org/pdf/2407.01885", "details": "C Yang, W Lu, Y Zhu, Y Wang, Q Chen, C Gao, B Yan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have showcased exceptional capabilities in various domains, attracting significant interest from both academia and industry. Despite their impressive performance, the substantial size and computational demands of LLMs \u2026"}, {"title": "Mitigating Entity-Level Hallucination in Large Language Models", "link": "https://arxiv.org/pdf/2407.09417%3Ftrk%3Dpublic_post_comment-text", "details": "W Su, Y Tang, Q Ai, C Wang, Z Wu, Y Liu - arXiv preprint arXiv:2407.09417, 2024", "abstract": "The emergence of Large Language Models (LLMs) has revolutionized how users access information, shifting from traditional search engines to direct question-and- answer interactions with LLMs. However, the widespread adoption of LLMs has \u2026"}]
