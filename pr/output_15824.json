[{"title": "DSQG-Syn: Synthesizing High-quality Data for Text-to-SQL Parsing by Domain Specific Question Generation", "link": "https://aclanthology.org/2025.findings-naacl.162.pdf", "details": "S Duan, Y Wu, C Liu, Y Zhang, Z Wang, P Han, S Yu\u2026 - Findings of the Association \u2026, 2025", "abstract": "Synthetic data has recently proven effective in enhancing the accuracy of Text-to- SQL parsers. However, existing methods generate SQL queries first by randomly sampling tables and columns based on probability and then synthesize natural \u2026"}, {"title": "GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models", "link": "https://aclanthology.org/2025.naacl-long.420.pdf", "details": "H Kohli, S Kumar, H Sun - Proceedings of the 2025 Conference of the Nations of \u2026, 2025", "abstract": "The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their reasoning to \u2026"}, {"title": "Fine-Tuning Language Models with Collaborative and Semantic Experts", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/34753/36908", "details": "J Yang, B Hui, M Yang, J Yang, L Zhang, Q Qu, J Lin - Proceedings of the AAAI \u2026, 2025", "abstract": "Recent advancements in large language models (LLMs) have broadened their application scope but revealed challenges in balancing capabilities across general knowledge, coding, and mathematics. To address this, we introduce a Collaborative \u2026"}, {"title": "FLEX: Expert-level False-Less EXecution Metric for Text-to-SQL Benchmark", "link": "https://aclanthology.org/2025.naacl-long.228.pdf", "details": "H Kim, J Taeyang, SH Choi, S Choi, H Cho - Proceedings of the 2025 Conference of \u2026, 2025", "abstract": "Text-to-SQL systems have become crucial for translating natural language into SQL queries in various industries, enabling non-technical users to perform complex data operations. The need for accurate evaluation methods has increased as these \u2026"}, {"title": "Enhancing Text-to-SQL with Question Classification and Multi-Agent Collaboration", "link": "https://aclanthology.org/2025.findings-naacl.245.pdf", "details": "Z Shao, S Cai, R Lin, Z Ming - Findings of the Association for Computational \u2026, 2025", "abstract": "Abstract Large Language Models (LLMs) have recently demonstrated remarkable performance in Text-to-SQL tasks. However, existing research primarily focuses on the optimization of prompts and improvements in workflow, with few studies delving \u2026"}, {"title": "HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/32171/34326", "details": "KHI Arif, JY Yoon, DS Nikolopoulos, H Vandierendonck\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "Abstract High-resolution Vision-Language Models (VLMs) are widely used in multimodal tasks to enhance accuracy by preserving detailed image information. However, these models often generate an excessive number of visual tokens due to \u2026"}, {"title": "Unified knowledge maintenance pruning and progressive recovery with weight recalling for large vision-language models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/32923/35078", "details": "Z Wu, J Chen, Y Wang - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Abstract Large Vision-Language Model (LVLM), leveraging Large Language Model (LLM) as the cognitive core, has recently become one of the most representative multimodal model paradigms. However, with the expansion of unimodal \u2026"}, {"title": "ALGOPUZZLEVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Algorithmic Multimodal Puzzles", "link": "https://aclanthology.org/2025.naacl-long.486.pdf", "details": "D Ghosal, V Toh, YK Chia, S Poria - Proceedings of the 2025 Conference of the \u2026, 2025", "abstract": "This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models \u2026"}, {"title": "QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models", "link": "https://arxiv.org/pdf/2504.11038", "details": "Y Zhang, R Xie, J Chen, X Sun, Z Kang, Y Wang - arXiv preprint arXiv:2504.11038, 2025", "abstract": "In typical multimodal tasks, such as Visual Question Answering (VQA), adversarial attacks targeting a specific image and question can lead large vision-language models (LVLMs) to provide incorrect answers. However, it is common for a single \u2026"}]
