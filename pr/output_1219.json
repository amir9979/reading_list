'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Refining Pre-trained Language Models for Domain Adapta'
[{"title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision", "link": "https://arxiv.org/pdf/2405.04086", "details": "Y Tong, S Wang, D Li, Y Wang, S Han, Z Lin, C Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations \u2026"}, {"title": "ATG: Benchmarking Automated Theorem Generation for Generative Language Models", "link": "https://eleanor-h.github.io/publication/confnaacl-2024-atg/confnaacl-2024-atg.pdf", "details": "X Lin, Q Cao, Y Huang, Z Yang, Z Liu, Z Li, X Liang15", "abstract": "Humans can develop new theorems to explore broader and more complex mathematical results. While current generative language models (LMs) have achieved significant improvement in automatically proving theorems, their ability to \u2026"}, {"title": "Structural Pruning of Pre-trained Language Models via Neural Architecture Search", "link": "https://arxiv.org/pdf/2405.02267", "details": "A Klein, J Golebiowski, X Ma, V Perrone\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models (PLM), for example BERT or RoBERTa, mark the state- of-the-art for natural language understanding task when fine-tuned on labeled data. However, their large size poses challenges in deploying them for inference in real \u2026"}, {"title": "A Causal Explainable Guardrails for Large Language Models", "link": "https://arxiv.org/pdf/2405.04160", "details": "Z Chu, Y Wang, L Li, Z Wang, Z Qin, K Ren - arXiv preprint arXiv:2405.04160, 2024", "abstract": "Large Language Models (LLMs) have shown impressive performance in natural language tasks, but their outputs can exhibit undesirable attributes or biases. Existing methods for steering LLMs towards desired attributes often assume \u2026"}, {"title": "Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case Study", "link": "https://arxiv.org/pdf/2404.08259", "details": "WH Her, U Kruschwitz - arXiv preprint arXiv:2404.08259, 2024", "abstract": "Machine Translation has made impressive progress in recent years offering close to human-level performance on many languages, but studies have primarily focused on high-resource languages with broad online presence and resources. With the help of \u2026"}, {"title": "PRE: Vision-Language Prompt Learning with Reparameterization Encoder", "link": "https://openreview.net/pdf%3Fid%3DI7plXUkBkm", "details": "TMA Pham, DA Nguyen, C Svosve, V Argyriou\u2026", "abstract": "Large vision-language foundation models such as CLIP have demonstrated great potential in zero-shot transferabil-ity to downstream tasks. However, manual prompt engineering is the major challenge for deploying such models in practice since it \u2026"}, {"title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models", "link": "https://arxiv.org/pdf/2404.18400", "details": "P Shojaee, K Meidani, S Gupta, AB Farimani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of \u2026"}, {"title": "Measuring Cross-lingual Transfer in Bytes", "link": "https://arxiv.org/pdf/2404.08191", "details": "LR de Souza, TS Almeida, R Lotufo, R Nogueira - arXiv preprint arXiv:2404.08191, 2024", "abstract": "Multilingual pretraining has been a successful solution to the challenges posed by the lack of resources for languages. These models can transfer knowledge to target languages with minimal or no examples. Recent research suggests that monolingual \u2026"}]
