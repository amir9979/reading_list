[{"title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding", "link": "https://arxiv.org/pdf/2412.10302%3F", "details": "Z Wu, X Chen, Z Pan, X Liu, W Liu, D Dai, H Gao, Y Ma\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL, through two key major upgrades. For the vision component, we \u2026"}, {"title": "Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning", "link": "https://arxiv.org/pdf/2412.08614", "details": "F Lu, W Wu, K Zheng, S Ma, B Gong, J Liu, W Zhai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating detailed captions comprehending text-rich visual content in images has received growing attention for Large Vision-Language Models (LVLMs). However, few studies have developed benchmarks specifically tailored for detailed captions to \u2026"}, {"title": "Training Medical Large Vision-Language Models with Abnormal-Aware Feedback", "link": "https://arxiv.org/pdf/2501.01377", "details": "Y Zhou, L Song, J Shen - arXiv preprint arXiv:2501.01377, 2025", "abstract": "Existing Medical Large Vision-Language Models (Med-LVLMs), which encapsulate extensive medical knowledge, demonstrate excellent capabilities in understanding medical images and responding to human queries based on these images \u2026"}, {"title": "PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation", "link": "https://arxiv.org/pdf/2412.15209%3F", "details": "M Wahed, KA Nguyen, AS Juvekar, X Li, X Zhou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite significant advancements in Large Vision-Language Models (LVLMs), existing pixel-grounding models operate on single-image settings, limiting their ability to perform detailed, fine-grained comparisons across multiple images \u2026"}, {"title": "Unveiling the power of language models in chemical research question answering", "link": "https://www.nature.com/articles/s42004-024-01394-x", "details": "X Chen, T Wang, T Guo, K Guo, J Zhou, H Li, Z Song\u2026 - Communications Chemistry, 2025", "abstract": "While the abilities of language models are thoroughly evaluated in areas like general domains and biomedicine, academic chemistry remains less explored. Chemical QA tools also play a crucial role in both education and research by effectively translating \u2026"}, {"title": "Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding", "link": "https://arxiv.org/pdf/2501.00712", "details": "J Zhu, P Wang, R Cai, JD Lee, P Li, Z Wang - arXiv preprint arXiv:2501.00712, 2025", "abstract": "Transformers rely on both content-based and position-based addressing mechanisms to make predictions, but existing positional encoding techniques often diminish the effectiveness of position-based addressing. Many current methods \u2026"}, {"title": "Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection", "link": "https://arxiv.org/pdf/2412.13817", "details": "L Yang, Z Zheng, B Chen, Z Zhao, C Lin, C Shen - arXiv preprint arXiv:2412.13817, 2024", "abstract": "Recent studies have shown that large vision-language models (LVLMs) often suffer from the issue of object hallucinations (OH). To mitigate this issue, we introduce an efficient method that edits the model weights based on an unsafe subspace, which \u2026"}, {"title": "A Dynamic Graph Reasoning Model with an Auxiliary Task for Knowledge Base Question Answering", "link": "https://www.mdpi.com/2079-9292/13/24/5011", "details": "Z Wu, X Tian - Electronics, 2024", "abstract": "In the field of question answering (QA), the methods of large language models (LLMs) cannot learn vertical domain knowledge during the pre-training stage, leading to low accuracy in domain QA. Conversely, knowledge base question \u2026"}, {"title": "ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical Insights from Large Language Models", "link": "https://arxiv.org/pdf/2412.12848", "details": "Y Sun, W Gao, J Ma, H Lin, Z Luo, W Zhang - arXiv preprint arXiv:2412.12848, 2024", "abstract": "With the rise and widespread use of Large Language Models (LLMs), ensuring their safety is crucial to prevent harm to humans and promote ethical behaviors. However, directly assessing value valence (ie, support or oppose) by leveraging large-scale \u2026"}]
