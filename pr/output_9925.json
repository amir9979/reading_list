[{"title": "Training and Evaluating Language Models with Template-based Data Generation", "link": "https://arxiv.org/pdf/2411.18104", "details": "Y Zhang - arXiv preprint arXiv:2411.18104, 2024", "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, these \u2026"}, {"title": "SETLEXSEM CHALLENGE: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models", "link": "https://openreview.net/pdf%3Fid%3DMd1mEoPEaQ", "details": "NA Dronen, B Akhbari, M Gawali - The Thirty-eight Conference on Neural Information \u2026", "abstract": "Set theory is foundational to mathematics and, when sets are finite, to reasoning about the world. An intelligent system should perform set operations consistently, regardless of superficial variations in the operands. Initially designed for semantically \u2026"}]
