[{"title": "Development and Evaluation of Machine Learning Models for the Identification of Surgical Site Infection in Electronic Health Records", "link": "https://www.liebertpub.com/doi/abs/10.1089/sur.2024.266", "details": "A Chakraborty, K Lybarger, JAO Estebane, JY Chen\u2026 - Surgical Infections, 2025", "abstract": "Background: Surgical site infection (SSI) affects 160,000\u2013300,000 patients per year in the United States, adversely impacting a wide range of patient-and health-system outcomes. Surveillance programs for SSI are essential to quality improvement and \u2026"}, {"title": "Generative Binary Memory: Pseudo-Replay Class-Incremental Learning on Binarized Embeddings", "link": "https://arxiv.org/pdf/2503.10333", "details": "Y Basso-Bert, A Molnos, R Lemaire, W Guicquero\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In dynamic environments where new concepts continuously emerge, Deep Neural Networks (DNNs) must adapt by learning new classes while retaining previously acquired ones. This challenge is addressed by Class-Incremental Learning (CIL) \u2026"}, {"title": "Synthetic Data Enhances Mathematical Reasoning of Language Models Based on Artificial Intelligence", "link": "https://www.itc.ktu.lt/index.php/ITC/article/view/39713/16892", "details": "Z Han, W Jiang - Information Technology and Control, 2025", "abstract": "Current large language models (LLMs) training involves extensive training data and computing resources to handle multiple natural language processing (NLP) tasks. This paper endeavors to assist individuals to compose feasible mathematical \u2026"}, {"title": "MAPSparse: Accelerating Pre-filling for Long-Context Visual Language Models via Modality-Aware Permutation Sparse Attention", "link": "https://openreview.net/pdf%3Fid%3DtIilnw5F6I", "details": "Y Li, H Jiang, C Zhang, Q Wu, X Luo, S Ahn, AH Abdi\u2026 - \u2026 on Foundation Models in the Wild", "abstract": "The integration of long-context capabilities with visual understanding opens up new possibilities for Vision Language Models (VLMs). However, the quadratic attention complexity during the prefilling stage remains a major bottleneck, restricting wide \u2026"}, {"title": "Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions", "link": "https://arxiv.org/pdf/2503.22353", "details": "Y Li, Y Miao, X Ding, R Krishnan, R Padman - arXiv preprint arXiv:2503.22353, 2025", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities across various tasks, but their deployment in high-stake domains requires consistent performance across multiple interaction rounds. This paper introduces a comprehensive \u2026"}]
