'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [MFORT-QA: Multi-hop Few-shot Open Rich Table Question '
[{"title": "Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models", "link": "https://arxiv.org/pdf/2403.12966", "details": "Z Liu, Y Dong, Y Rao, J Zhou, J Lu - arXiv preprint arXiv:2403.12966, 2024", "abstract": "In the realm of vision-language understanding, the proficiency of models in interpreting and reasoning over visual content has become a cornerstone for numerous applications. However, it is challenging for the visual encoder in Large \u2026"}, {"title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models", "link": "https://arxiv.org/pdf/2403.18814", "details": "Y Li, Y Zhang, C Wang, Z Zhong, Y Chen, R Chu, S Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared \u2026"}, {"title": "Mechanisms of non-factual hallucinations in language models", "link": "https://arxiv.org/pdf/2403.18167", "details": "L Yu, M Cao, JCK Cheung, Y Dong - arXiv preprint arXiv:2403.18167, 2024", "abstract": "State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains \u2026"}, {"title": "TSLANet: Rethinking Transformers for Time Series Representation Learning", "link": "https://arxiv.org/pdf/2404.08472", "details": "E Eldele, M Ragab, Z Chen, M Wu, X Li - arXiv preprint arXiv:2404.08472, 2024", "abstract": "Time series data, characterized by its intrinsic long and short-range dependencies, poses a unique challenge across analytical applications. While Transformer-based models excel at capturing long-range dependencies, they face limitations in noise \u2026"}, {"title": "Conceptual and Unbiased Reasoning in Language Models", "link": "https://arxiv.org/pdf/2404.00205", "details": "B Zhou, H Zhang, S Chen, D Yu, H Wang, B Peng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Conceptual reasoning, the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on large language models' capability to perform conceptual reasoning. In this work, we \u2026"}, {"title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction", "link": "https://arxiv.org/pdf/2404.02905", "details": "K Tian, Y Jiang, Z Yuan, B Peng, L Wang - arXiv preprint arXiv:2404.02905, 2024", "abstract": "We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine\" next-scale prediction\" or\" next-resolution prediction\", diverging from the standard raster-scan\" \u2026"}, {"title": "Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering", "link": "https://arxiv.org/pdf/2403.12393", "details": "Y Gao, Y Zhu, Y Cao, Y Zhou, Z Wu, Y Chen, S Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Open Domain Multi-Hop Question Answering (ODMHQA) plays a crucial role in Natural Language Processing (NLP) by aiming to answer complex questions through multi-step reasoning over retrieved information from external knowledge sources \u2026"}, {"title": "Is Table Retrieval a Solved Problem? Join-Aware Multi-Table Retrieval", "link": "https://arxiv.org/pdf/2404.09889", "details": "PB Chen, Y Zhang, D Roth - arXiv preprint arXiv:2404.09889, 2024", "abstract": "Retrieving relevant tables containing the necessary information to accurately answer a given question over tables is critical to open-domain question-answering (QA) systems. Previous methods assume the answer to such a question can be found \u2026"}, {"title": "FedGCR: Achieving Performance and Fairness for Federated Learning with Distinct Client Types via Group Customization and Reweighting", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29031/29954", "details": "SL Cheng, CY Yeh, TA Chen, E Pastor, MS Chen - \u2026 of the AAAI Conference on Artificial \u2026, 2024", "abstract": "To achieve better performance and greater fairness in Federated Learning (FL), much of the existing research has centered on individual clients, using domain adaptation techniques and redesigned aggregation schemes to counteract client \u2026"}]
