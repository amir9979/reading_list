[{"title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis", "link": "https://arxiv.org/pdf/2504.10352", "details": "Y Yang, S Liu, J Li, Y Hu, H Wu, H Wang, J Yu, L Meng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and \u2026"}, {"title": "Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation", "link": "https://arxiv.org/pdf/2504.12108", "details": "S Cai, L Ding, D Tao - arXiv preprint arXiv:2504.12108, 2025", "abstract": "The rapid development of Large Language Models (LLMs) has intensified concerns about content traceability and potential misuse. Existing watermarking schemes for sampled text often face trade-offs between maintaining text quality and ensuring \u2026"}, {"title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models", "link": "https://arxiv.org/pdf/2504.02386", "details": "K Sung-Bin, J Choi, P Peng, JS Chung, TH Oh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present VoiceCraft-Dub, a novel approach for automated video dubbing that synthesizes high-quality speech from text and facial cues. This task has broad applications in filmmaking, multimedia creation, and assisting voice-impaired \u2026"}, {"title": "Reinforcement Learning from Human Feedback", "link": "https://arxiv.org/pdf/2504.12501", "details": "N Lambert - arXiv preprint arXiv:2504.12501, 2025", "abstract": "Reinforcement learning from human feedback (RLHF) has become an important technical and storytelling tool to deploy the latest machine learning systems. In this book, we hope to give a gentle introduction to the core methods for people with some \u2026"}, {"title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?", "link": "https://arxiv.org/pdf/2504.10000", "details": "Y Wang, J Guan, J Liang, R He - arXiv preprint arXiv:2504.10000, 2025", "abstract": "Multi-modal large language models (MLLMs) have made significant progress, yet their safety alignment remains limited. Typically, current open-source MLLMs rely on the alignment inherited from their language module to avoid harmful generations \u2026"}, {"title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer", "link": "https://arxiv.org/pdf/2504.10462", "details": "W Lei, J Wang, H Wang, X Li, JH Liew, J Feng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained \u2026"}, {"title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data", "link": "https://arxiv.org/pdf/2504.09895", "details": "S Zhao, L Zhu, Y Yang - arXiv preprint arXiv:2504.09895, 2025", "abstract": "Large language models~(LLMs) are expected to be helpful, harmless, and honest. In various alignment scenarios, such as general human preference, safety, and confidence alignment, binary preference data collection and reward modeling are \u2026"}, {"title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models", "link": "https://arxiv.org/pdf/2504.12898", "details": "Z Sun, X Ding, L Du, Y Xu, Y Ma, Y Zhao, B Qin, T Liu - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite significant progress, recent studies indicate that current large language models (LLMs) may still capture dataset biases and utilize them during inference, leading to the poor generalizability of LLMs. However, due to the diversity of dataset \u2026"}, {"title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models", "link": "https://arxiv.org/pdf/2504.02902", "details": "L Huang, D Li, H Liu, L Cheng - arXiv preprint arXiv:2504.02902, 2025", "abstract": "Large Language Models (LLMs) have demonstrated remarkable self-improvement capabilities, whereby models iteratively revise their outputs through self-generated feedback. While this reflective mechanism has shown promise in enhancing task \u2026"}]
