[{"title": "Probing Fundamental Visual Comprehend Capabilities on Vision Language Models via Visual Phrases from Structural Data", "link": "https://link.springer.com/article/10.1007/s12559-024-10351-8", "details": "P Xie, B Liu - Cognitive Computation, 2024", "abstract": "Does the model demonstrate exceptional proficiency in \u201citem counting,\u201d\u201ccolor recognition,\u201d or other Fundamental Visual Comprehension Capability (FVCC)? There have been remarkable advancements in the field of multimodal, the pretrained \u2026"}, {"title": "Few-shot Adaptation of Medical Vision-Language Models", "link": "https://arxiv.org/pdf/2409.03868", "details": "F Shakeri, Y Huang, J Silva-Rodr\u00edguez, H Bahig\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Integrating image and text data through multi-modal learning has emerged as a new approach in medical imaging research, following its successful deployment in computer vision. While considerable efforts have been dedicated to establishing \u2026"}, {"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "link": "https://arxiv.org/pdf/2408.12337", "details": "KS Phogat, SA Puranam, S Dasaratha, C Harsha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain \u2026"}, {"title": "Qwen2. 5-Coder Technical Report", "link": "https://arxiv.org/pdf/2409.12186", "details": "B Hui, J Yang, Z Cui, J Yang, D Liu, L Zhang, T Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this report, we introduce the Qwen2. 5-Coder series, a significant upgrade from its predecessor, CodeQwen1. 5. This series includes two models: Qwen2. 5-Coder-1.5 B and Qwen2. 5-Coder-7B. As a code-specific model, Qwen2. 5-Coder is built upon \u2026"}, {"title": "TART: An Open-Source Tool-Augmented Framework for Explainable Table-based Reasoning", "link": "https://arxiv.org/pdf/2409.11724", "details": "X Lu, L Pan, Y Ma, P Nakov, MY Kan - arXiv preprint arXiv:2409.11724, 2024", "abstract": "Current Large Language Models (LLMs) exhibit limited ability to understand table structures and to apply precise numerical reasoning, which is crucial for tasks such as table question answering (TQA) and table-based fact verification (TFV). To \u2026"}, {"title": "Knowledge-Aware Reasoning over Multimodal Semi-structured Tables", "link": "https://arxiv.org/pdf/2408.13860", "details": "SV Mathur, JS Bafna, K Kartik, H Khandelwal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing datasets for tabular question answering typically focus exclusively on text within cells. However, real-world data is inherently multimodal, often blending images such as symbols, faces, icons, patterns, and charts with textual content in \u2026"}, {"title": "Balancing Diversity and Risk in LLM Sampling: How to Select Your Method and Parameter for Open-Ended Text Generation", "link": "https://arxiv.org/pdf/2408.13586", "details": "Y Zhou, M Keuper, M Fritz - arXiv preprint arXiv:2408.13586, 2024", "abstract": "Sampling-based decoding strategies have been widely adopted for Large Language Models (LLMs) in numerous applications, which target a balance between diversity and quality via temperature tuning and tail truncation (eg, top-k and top-p sampling) \u2026"}, {"title": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Foundation Models", "link": "https://arxiv.org/pdf/2409.11919", "details": "A Cardiel, E Zablocki, O Sim\u00e9oni, E Ramzi, M Cord - arXiv preprint arXiv:2409.11919, 2024", "abstract": "Vision Language Models (VLMs) have shown impressive performances on numerous tasks but their zero-shot capabilities can be limited compared to dedicated or fine-tuned models. Yet, fine-tuning VLMs comes with limitations as it requireswhite \u2026"}, {"title": "Towards Harnessing Large Language Models as Autonomous Agents for Semantic Triple Extraction from Unstructured Text", "link": "https://ceur-ws.org/Vol-3747/text2kg_paper1.pdf", "details": "A Ananya, S Tiwari, N Mihindukulasooriya, T Soru\u2026 - 2024", "abstract": "Abstract The use of Large Language Models as autonomous agents interacting with tools has shown to improve the performance of several tasks from code generation to API calling and sequencing. This paper proposes a framework for using Large \u2026"}]
