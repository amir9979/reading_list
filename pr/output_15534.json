[{"title": "Unlocking efficient long-to-short llm reasoning with model merging", "link": "https://arxiv.org/pdf/2503.20641%3F", "details": "H Wu, Y Yao, S Liu, Z Liu, X Fu, X Han, X Li, HL Zhen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of \u2026"}, {"title": "A Survey on Mixture of Experts in Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10937907/", "details": "W Cai, J Jiang, F Wang, J Tang, S Kim, J Huang - IEEE Transactions on Knowledge \u2026, 2025", "abstract": "Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size \u2026"}, {"title": "Boosting Large Language Models with Mask Fine-Tuning", "link": "https://arxiv.org/pdf/2503.22764", "details": "M Zhang, Y Bai, H Wang, Y Wang, Q Dong, Y Fu - arXiv preprint arXiv:2503.22764, 2025", "abstract": "The model is usually kept integral in the mainstream large language model (LLM) fine-tuning protocols. No works have questioned whether maintaining the integrity of the model is indispensable for performance. In this work, we introduce Mask Fine \u2026"}, {"title": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning", "link": "https://arxiv.org/pdf/2504.13500", "details": "J Wang, J Jiang, Y Liu, M Zhang, X Cai - arXiv preprint arXiv:2504.13500, 2025", "abstract": "In this paper, we introduce a new\\emph {process prejudge} strategy in LLM reasoning to demonstrate that bootstrapping with process prejudge allows the LLM to adaptively anticipate the errors encountered when advancing the subsequent \u2026"}, {"title": "CSPO: chain-structured prompt optimisation for large language models", "link": "https://www.inderscienceonline.com/doi/abs/10.1504/IJAHUC.2025.145202", "details": "J Wang, S Lin, X Xue, S Chen, Z Tang - International Journal of Ad Hoc and \u2026, 2025", "abstract": "Large language models (LLMs) show promise in improving content distribution in mobile communication networks, but their performance heavily depends on input prompts. Manually crafting effective prompts for complex tasks is time-consuming \u2026"}, {"title": "AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems", "link": "https://arxiv.org/pdf/2504.00587", "details": "Y Yang, H Chai, S Shao, Y Song, S Qi, R Rui, W Zhang - arXiv preprint arXiv \u2026, 2025", "abstract": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of multi-agent systems, where multiple LLM-based agents collaborate to solve complex tasks. However, existing systems predominantly rely on centralized \u2026"}]
