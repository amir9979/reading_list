[{"title": "BAPO: Base-Anchored Preference Optimization for Personalized Alignment in Large Language Models", "link": "https://arxiv.org/pdf/2407.00693", "details": "G Lee, M Jeong, Y Kim, H Jung, J Oh, S Kim, SY Yun - arXiv preprint arXiv \u2026, 2024", "abstract": "While learning to align Large Language Models (LLMs) with human preferences has shown remarkable success, aligning these models to meet the diverse user preferences presents further challenges in preserving previous knowledge. This \u2026"}, {"title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs", "link": "https://arxiv.org/pdf/2407.00653", "details": "Y Zhang, X Wang, J Liang, S Xia, L Chen, Y Xiao - arXiv preprint arXiv:2407.00653, 2024", "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new \u2026"}, {"title": "Universal Approximation Theory: The basic theory for large language models", "link": "https://arxiv.org/pdf/2407.00958", "details": "W Wang, Q Li - arXiv preprint arXiv:2407.00958, 2024", "abstract": "Language models have emerged as a critical area of focus in artificial intelligence, particularly with the introduction of groundbreaking innovations like ChatGPT. Large- scale Transformer networks have quickly become the leading approach for \u2026"}, {"title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "link": "https://arxiv.org/pdf/2407.10817", "details": "T Vu, K Krishna, S Alzubi, C Tar, M Faruqui, YH Sung - arXiv preprint arXiv \u2026, 2024", "abstract": "As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their output due to the high costs of human evaluation. To make progress towards better LLM autoraters, we introduce FLAMe, a family of Foundational Large \u2026"}, {"title": "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models", "link": "https://arxiv.org/pdf/2407.01906", "details": "Z Wang, D Chen, D Dai, R Xu, Z Li, Y Wu - arXiv preprint arXiv:2407.01906, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) is crucial for customizing Large Language Models (LLMs) with constrained resources. Although there have been various PEFT methods for dense-architecture LLMs, PEFT for sparse-architecture LLMs is still \u2026"}, {"title": "Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models", "link": "https://arxiv.org/pdf/2407.10873", "details": "R Zhang, F Liu, X Lin, Z Wang, Z Lu, Q Zhang - arXiv preprint arXiv:2407.10873, 2024", "abstract": "Automated heuristic design (AHD) has gained considerable attention for its potential to automate the development of effective heuristics. The recent advent of large language models (LLMs) has paved a new avenue for AHD, with initial efforts \u2026"}, {"title": "Igea: a Decoder-Only Language Model for Biomedical Text Generation in Italian", "link": "https://arxiv.org/pdf/2407.06011", "details": "TM Buonocore, S Rancati, E Parimbelli - arXiv preprint arXiv:2407.06011, 2024", "abstract": "The development of domain-specific language models has significantly advanced natural language processing applications in various specialized fields, particularly in biomedicine. However, the focus has largely been on English-language models \u2026"}, {"title": "Planning with Large Language Models for Conversational Agents", "link": "https://arxiv.org/pdf/2407.03884", "details": "Z Li, J Peng, Y Wang, T Shen, M Zhang, L Su, S Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Controllability and proactivity are crucial properties of autonomous conversational agents (CAs). Controllability requires the CAs to follow the standard operating procedures (SOPs), such as verifying identity before activating credit cards \u2026"}, {"title": "Collaborative Performance Prediction for Large Language Models", "link": "https://arxiv.org/pdf/2407.01300", "details": "Q Zhang, F Lyu, X Liu, C Ma - arXiv preprint arXiv:2407.01300, 2024", "abstract": "Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works \u2026"}]
