[{"title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models", "link": "https://arxiv.org/pdf/2502.17387", "details": "A Albalak, D Phung, N Lile, R Rafailov, K Gandhi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written \u2026"}, {"title": "A Close Look at Decomposition-based XAI-Methods for Transformer Language Models", "link": "https://arxiv.org/pdf/2502.15886", "details": "L Arras, B Puri, P Kahardipraja, S Lapuschkin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Various XAI attribution methods have been recently proposed for the transformer architecture, allowing for insights into the decision-making process of large language models by assigning importance scores to input tokens and intermediate \u2026"}, {"title": "Mapping 1,000+ Language Models via the Log-Likelihood Vector", "link": "https://arxiv.org/pdf/2502.16173", "details": "M Oyama, H Yamagiwa, Y Takase, H Shimodaira - arXiv preprint arXiv:2502.16173, 2025", "abstract": "To compare autoregressive language models at scale, we propose using log- likelihood vectors computed on a predefined text set as model features. This approach has a solid theoretical basis: when treated as model coordinates, their \u2026"}, {"title": "HaluCheck: Explainable and verifiable automation for detecting hallucinations in LLM responses", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425003343", "details": "S Heo, S Son, H Park - Expert Systems with Applications, 2025", "abstract": "Large language models have become integral to various aspects of modern life, but a critical challenge persists: hallucinations. This work contributes to expert systems research by providing a systematic framework for enhancing AI reliability and \u2026"}, {"title": "MammoVLM: A generative large vision-language model for mammography-related diagnostic assistance", "link": "https://www.sciencedirect.com/science/article/pii/S1566253525000715", "details": "Z Cao, Z Deng, J Ma, J Hu, L Ma - Information Fusion, 2025", "abstract": "Inspired by the recent success of large language models (LLMs) in the general domain, many large multimodal models, such as vision-language models, have been developed to tackle problems across modalities. In the realm of breast cancer, which \u2026"}, {"title": "QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare Text Multi-Label Classification", "link": "https://arxiv.org/pdf/2502.14189", "details": "H Sakai, SS Lam - arXiv preprint arXiv:2502.14189, 2025", "abstract": "The escalating volume of collected healthcare textual data presents a unique challenge for automated Multi-Label Text Classification (MLTC), which is primarily due to the scarcity of annotated texts for training and their nuanced nature \u2026"}, {"title": "Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI", "link": "https://arxiv.org/pdf/2502.16691", "details": "E Noh, J Baek - arXiv preprint arXiv:2502.16691, 2025", "abstract": "Recent research has increasingly focused on training large language models (LLMs) using federated learning, known as FedLLM. However, responsible AI (RAI), which aims to ensure safe responses, remains underexplored in the context of FedLLM. In \u2026"}, {"title": "Large Language Models are Powerful EHR Encoders", "link": "https://arxiv.org/pdf/2502.17403", "details": "S Hegselmann, G von Arnim, T Rheude, N Kronenberg\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific EHR foundation models trained on \u2026"}, {"title": "Automatic Instruction Data Selection for Large Language Models via Uncertainty-Aware Influence Maximization", "link": "https://openreview.net/pdf%3Fid%3DyvN3PilD1S", "details": "J Han, H Liu, J Fang, N Tan, H Xiong - THE WEB CONFERENCE 2025", "abstract": "Recent years have witnessed the prevalent integration of Large Language Models (LLMs) in various Web applications, such as search engines and recommender systems. As an emerging technique, instruction tuning aims to align pre-trained LLMs \u2026"}]
