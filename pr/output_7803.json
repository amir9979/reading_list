[{"title": "Aligning Language Models Using Follow-up Likelihood as Reward Signal", "link": "https://arxiv.org/pdf/2409.13948", "details": "C Zhang, D Chong, F Jiang, C Tang, A Gao, G Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In natural human-to-human conversations, participants often receive feedback signals from one another based on their follow-up reactions. These reactions can include verbal responses, facial expressions, changes in emotional state, and other \u2026"}, {"title": "Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination", "link": "https://arxiv.org/pdf/2409.12746", "details": "ES Salido, R Morante, J Gonzalo, G Marco\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this article we present UNED-ACCESS 2024, a bilingual dataset that consists of 1003 multiple-choice questions of university entrance level exams in Spanish and English. Questions are originally formulated in Spanish and translated manually into \u2026"}, {"title": "Knowledge abstraction and filtering based federated learning over heterogeneous data views in healthcare", "link": "https://www.nature.com/articles/s41746-024-01272-9", "details": "A Thakur, S Molaei, PC Nganjimi, A Soltan, P Schwab\u2026 - npj Digital Medicine, 2024", "abstract": "Robust data privacy regulations hinder the exchange of healthcare data among institutions, crucial for global insights and developing generalised clinical models. Federated learning (FL) is ideal for training global models using datasets from \u2026"}, {"title": "Unsupervised Domain Adaptation Using Soft-Labeled Contrastive Learning with Reversed Monte Carlo Method for Cardiac Image Segmentation", "link": "https://papers.miccai.org/miccai-2024/paper/1593_paper.pdf", "details": "M Gu, M Thies, S Mei, F Wagner, M Fan, Y Sun, Z Pan\u2026 - International Conference on \u2026, 2024", "abstract": "Recent unsupervised domain adaptation methods in medical image segmentation adopt centroid/prototypical contrastive learning (CL) to match the source and target features for their excellent ability of representation learning and semantic feature \u2026"}, {"title": "Exploring the Impact of Backbone Architecture on Explainable CNNs' Interpretability", "link": "https://www.researchgate.net/profile/Zalan-Bodo/publication/384925487_Exploring_the_Impact_of_Backbone_Architecture_on_Explainable_CNNs%27_Interpretability/links/670e3ccf77bab74415a19534/Exploring-the-Impact-of-Backbone-Architecture-on-Explainable-CNNs-Interpretability.pdf", "details": "\u00c1 PORTIK, A BAJCSI, A SZENKOVITS, Z BOD\u00d3 - Acta Univ. Sapientiae, 2024", "abstract": "The growing demand for interpretable models in machine learning underscores the importance of transparency in decision-making processes for building trust and ensuring accountability in AI systems. Unlike complex black-box models \u2026"}, {"title": "On Championing Foundation Models: From Explainability to Interpretability", "link": "https://arxiv.org/pdf/2410.11444", "details": "S Fu, Y Chen, Y Wang, D Tao - arXiv preprint arXiv:2410.11444, 2024", "abstract": "Understanding the inner mechanisms of black-box foundation models (FMs) is essential yet challenging in artificial intelligence and its applications. Over the last decade, the long-running focus has been on their explainability, leading to the \u2026"}, {"title": "SPHINX: A Mixer of Weights, Visual Embeddings and Image Scales for Multi-modal Large Language Models", "link": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07894.pdf", "details": "D Liu, R Zhang12, P Gao, L Qiu23, H Xiao, H Qiu\u2026", "abstract": "We present SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, visual embeddings and image scales. First, for stronger vision-language alignment, we unfreeze the large language model (LLM) \u2026"}, {"title": "TiC-LM: A Multi-Year Benchmark for Continual Pretraining of Language Models", "link": "https://openreview.net/pdf%3Fid%3DPpSDVE5rAy", "details": "J Li, M Armandpour, SI Mirzadeh, S Mehta, V Shankar\u2026 - NeurIPS 2024 Workshop on \u2026", "abstract": "Large language models (LLMs) are trained on data crawled over many years from the web. We investigate how quickly LLMs become outdated as the world evolves with time and how to best update them with newer data. Specifically, we simulate a \u2026"}, {"title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data", "link": "https://arxiv.org/pdf/2409.12437", "details": "J Zhou, A Ghaddar, G Zhang, L Ma, Y Hu, S Pal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the \u2026"}]
