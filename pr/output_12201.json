[{"title": "Aligning, Autoencoding and Prompting Large Language Models for Novel Disease Reporting", "link": "https://www.computer.org/csdl/journal/tp/5555/01/10854911/23QQQouxFXW", "details": "F Liu, X Wu, J Huang, B Yang, K Branson, P Schwab\u2026 - IEEE Transactions on \u2026, 2025", "abstract": "Given radiology images, automatic radiology report generation aims to produce informative text that reports diseases. It can benefit current clinical practice in diagnostic radiology. Existing methods typically rely on large-scale medical datasets \u2026"}, {"title": "Metadata conditioning accelerates language model pre-training", "link": "https://arxiv.org/pdf/2501.01956%3F", "details": "T Gao, A Wettig, L He, Y Dong, S Malladi, D Chen - arXiv preprint arXiv:2501.01956, 2025", "abstract": "The vast diversity of styles, domains, and quality levels present in language model pre-training corpora is essential in developing general model capabilities, but efficiently learning and deploying the correct behaviors exemplified in each of these \u2026"}, {"title": "Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval", "link": "https://arxiv.org/pdf/2501.09134", "details": "D Deanda, YP Masupalli, J Yang, Y Lee, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical images and reports offer invaluable insights into patient health. The heterogeneity and complexity of these data hinder effective analysis. To bridge this gap, we investigate contrastive learning models for cross-domain retrieval, which \u2026"}]
