[{"title": "Mint: Boosting audio-language model via multi-target pre-training and instruction tuning", "link": "https://www.isca-archive.org/interspeech_2024/zhao24h_interspeech.pdf", "details": "H Zhao, Y Xin, Z Yu, B Zhu, L Lu, Z Ma - Interspeech, 2024", "abstract": "In the realm of audio-language pre-training (ALP), the challenge of achieving cross- modal alignment is significant. Moreover, the integration of audio inputs with diverse distributions and task variations poses challenges in developing generic audio \u2026"}, {"title": "DMR 2 G: diffusion model for radiology report generation", "link": "https://link.springer.com/article/10.1007/s11042-024-20206-x", "details": "H Ouyang, Z Chang, B Tang, S Li - Multimedia Tools and Applications, 2024", "abstract": "Radiology report generation aims to generate pathological assessments from given radiographic images accurately. Prior methods largely rely on autoregressive models, where the sequential token-by-token generation process always results in \u2026"}, {"title": "Frequency-Guided Masking for Enhanced Vision Self-Supervised Learning", "link": "https://arxiv.org/pdf/2409.10362", "details": "AK Monsefi, M Zhou, NK Monsefi, SN Lim, WL Chao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present a novel frequency-based Self-Supervised Learning (SSL) approach that significantly enhances its efficacy for pre-training. Prior work in this direction masks out pre-defined frequencies in the input image and employs a reconstruction loss to \u2026"}, {"title": "Clustering-Based Oversampling Algorithm for Multi-class Imbalance Learning", "link": "https://link.springer.com/article/10.1007/s00357-024-09491-1", "details": "H Zhao, J Wu - Journal of Classification, 2024", "abstract": "Multi-class imbalanced data learning faces many challenges. Its complex structural characteristics cause severe intra-class imbalance or overgeneralization in most solution strategies. This negatively affects data learning. This paper proposes a \u2026"}, {"title": "VTPL: Visual and Text Prompt Learning for visual-language models", "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002360", "details": "B Sun, Z Wu, H Zhang, J He - Journal of Visual Communication and Image \u2026, 2024", "abstract": "Visual-language (VL) models have achieved remarkable success in learning combined visual\u2013textual representations from large web datasets. Prompt learning, as a solution for downstream tasks, can address the forgetting of knowledge \u2026"}, {"title": "Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific Expert Pruning", "link": "https://arxiv.org/pdf/2409.01483", "details": "S Sarkar, L Lausen, V Cevher, S Zha, T Brox, G Karypis - arXiv preprint arXiv \u2026, 2024", "abstract": "Sparse Mixture of Expert (SMoE) models have emerged as a scalable alternative to dense models in language modeling. These models use conditionally activated feedforward subnetworks in transformer blocks, allowing for a separation between \u2026"}, {"title": "GP-GPT: Large Language Model for Gene-Phenotype Mapping", "link": "https://arxiv.org/pdf/2409.09825", "details": "Y Lyu, Z Wu, L Zhang, J Zhang, Y Li, W Ruan, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained large language models (LLMs) have attracted increasing attention in biomedical domains due to their success in natural language processing. However, the complex traits and heterogeneity of multi-sources genomics data pose significant \u2026"}, {"title": "Analysis of Plan-based Retrieval for Grounded Text Generation", "link": "https://arxiv.org/pdf/2408.10490", "details": "A Godbole, N Monath, S Kim, AS Rawat, A McCallum\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In text generation, hallucinations refer to the generation of seemingly coherent text that contradicts established knowledge. One compelling hypothesis is that hallucinations occur when a language model is given a generation task outside its \u2026"}]
