[{"title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models", "link": "https://arxiv.org/pdf/2405.01535", "details": "S Kim, J Suk, S Longpre, BY Lin, J Shin, S Welleck\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source \u2026"}, {"title": "Efficient Vision-Language Pre-training by Cluster Masking", "link": "https://arxiv.org/pdf/2405.08815", "details": "Z Wei, Z Pan, A Owens - arXiv preprint arXiv:2405.08815, 2024", "abstract": "We propose a simple strategy for masking image patches during visual-language contrastive learning that improves the quality of the learned representations and the training speed. During each iteration of training, we randomly mask clusters of \u2026"}, {"title": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records", "link": "https://aclanthology.org/2024.cl4health-1.23.pdf", "details": "J Lov\u00f3n-Melgarejo, T Ben-Haddi, J Di Scala\u2026 - Proceedings of the First \u2026, 2024", "abstract": "The lack of standardized evaluation benchmarks in the medical domain for text inputs can be a barrier to widely adopting and leveraging the potential of natural language models for health-related downstream tasks. This paper revisited an \u2026"}, {"title": "HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models", "link": "https://arxiv.org/pdf/2405.10299", "details": "RS Sukthanker, A Zela, B Staffler, JKH Franke, F Hutter - arXiv preprint arXiv \u2026, 2024", "abstract": "The expanding size of language models has created the necessity for a comprehensive examination across various dimensions that reflect the desiderata with respect to the tradeoffs between various hardware metrics, such as latency \u2026"}, {"title": "FITA: Fine-grained Image-Text Aligner for Radiology Report Generation", "link": "https://arxiv.org/pdf/2405.00962", "details": "H Yang, H Tang, X Li - arXiv preprint arXiv:2405.00962, 2024", "abstract": "Radiology report generation aims to automatically generate detailed and coherent descriptive reports alongside radiology images. Previous work mainly focused on refining fine-grained image features or leveraging external knowledge. However, the \u2026"}, {"title": "Align vision-language semantics by multi-task learning for multi-modal summarization", "link": "https://link.springer.com/article/10.1007/s00521-024-09908-3", "details": "C Cui, X Liang, S Wu, Z Li - Neural Computing and Applications, 2024", "abstract": "Most current multi-modal summarization methods follow a cascaded manner, where an off-the-shelf object detector is first used to extract visual features. After that, these visual features are fused with language representations for the decoder to generate \u2026"}, {"title": "Why are Visually-Grounded Language Models Bad at Image Classification?", "link": "https://arxiv.org/pdf/2405.18415", "details": "Y Zhang, A Unell, X Wang, D Ghosh, Y Su, L Schmidt\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Image classification is one of the most fundamental capabilities of machine vision intelligence. In this work, we revisit the image classification task using visually- grounded language models (VLMs) such as GPT-4V and LLaVA. We find that \u2026"}, {"title": "You Only Cache Once: Decoder-Decoder Architectures for Language Models", "link": "https://arxiv.org/pdf/2405.05254%3Ftrk%3Dpublic_post_comment-text", "details": "Y Sun, L Dong, Y Zhu, S Huang, W Wang, S Ma\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce a decoder-decoder architecture, YOCO, for large language models, which only caches key-value pairs once. It consists of two components, ie, a cross- decoder stacked upon a self-decoder. The self-decoder efficiently encodes global \u2026"}, {"title": "A Survey on Vision-Language-Action Models for Embodied AI", "link": "https://arxiv.org/pdf/2405.14093", "details": "Y Ma, Z Song, Y Zhuang, J Hao, I King - arXiv preprint arXiv:2405.14093, 2024", "abstract": "Deep learning has demonstrated remarkable success across many domains, including computer vision, natural language processing, and reinforcement learning. Representative artificial neural networks in these fields span convolutional neural \u2026"}]
