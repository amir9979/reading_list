[{"title": "Generalist medical foundation model improves prostate cancer segmentation from multimodal MRI images", "link": "https://www.nature.com/articles/s41746-025-01756-2", "details": "Y Zhang, X Ma, M Li, K Huang, J Zhu, M Wang, X Wang\u2026 - npj Digital Medicine, 2025", "abstract": "Prostate cancer (PCa) is one of the most common types of cancer, seriously affecting adult male health. Accurate and automated PCa segmentation is essential for radiologists to confirm the location of cancer, evaluate its severity, and design \u2026"}, {"title": "3D-RAD: A Comprehensive 3D Radiology Med-VQA Dataset with Multi-Temporal Analysis and Diverse Diagnostic Tasks", "link": "https://arxiv.org/pdf/2506.11147", "details": "X Gai, J Liu, Y Li, Z Meng, J Wu, Z Liu - arXiv preprint arXiv:2506.11147, 2025", "abstract": "Medical Visual Question Answering (Med-VQA) holds significant potential for clinical decision support, yet existing efforts primarily focus on 2D imaging with limited task diversity. This paper presents 3D-RAD, a large-scale dataset designed to advance \u2026", "entry_id": "http://arxiv.org/abs/2506.11147v1", "updated": "2025-06-11 09:55:42", "published": "2025-06-11 09:55:42", "authors": "Xiaotang Gai;Jiaxiang Liu;Yichen Li;Zijie Meng;Jian Wu;Zuozhu Liu", "summary": "Medical Visual Question Answering (Med-VQA) holds significant potential for\nclinical decision support, yet existing efforts primarily focus on 2D imaging\nwith limited task diversity. This paper presents 3D-RAD, a large-scale dataset\ndesigned to advance 3D Med-VQA using radiology CT scans. The 3D-RAD dataset\nencompasses six diverse VQA tasks: anomaly detection, image observation,\nmedical computation, existence detection, static temporal diagnosis, and\nlongitudinal temporal diagnosis. It supports both open- and closed-ended\nquestions while introducing complex reasoning challenges, including\ncomputational tasks and multi-stage temporal analysis, to enable comprehensive\nbenchmarking. Extensive evaluations demonstrate that existing vision-language\nmodels (VLMs), especially medical VLMs exhibit limited generalization,\nparticularly in multi-temporal tasks, underscoring the challenges of real-world\n3D diagnostic reasoning. To drive future advancements, we release a\nhigh-quality training set 3D-RAD-T of 136,195 expert-aligned samples, showing\nthat fine-tuning on this dataset could significantly enhance model performance.\nOur dataset and code, aiming to catalyze multimodal medical AI research and\nestablish a robust foundation for 3D medical visual understanding, are publicly\navailable at https://github.com/Tang-xiaoxiao/M3D-RAD.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.11147v1;http://arxiv.org/pdf/2506.11147v1", "pdf_url": "http://arxiv.org/pdf/2506.11147v1"}, {"title": "MRI-CORE: A Foundation Model for Magnetic Resonance Imaging", "link": "https://arxiv.org/pdf/2506.12186", "details": "H Dong, Y Chen, H Gu, N Konz, Y Chen, Q Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The widespread use of Magnetic Resonance Imaging (MRI) and the rise of deep learning have enabled the development of powerful predictive models for a wide range of diagnostic tasks in MRI, such as image classification or object \u2026", "entry_id": "http://arxiv.org/abs/2506.12186v1", "updated": "2025-06-13 19:26:56", "published": "2025-06-13 19:26:56", "authors": "Haoyu Dong;Yuwen Chen;Hanxue Gu;Nicholas Konz;Yaqian Chen;Qihang Li;Maciej A. Mazurowski", "summary": "The widespread use of Magnetic Resonance Imaging (MRI) and the rise of deep\nlearning have enabled the development of powerful predictive models for a wide\nrange of diagnostic tasks in MRI, such as image classification or object\nsegmentation. However, training models for specific new tasks often requires\nlarge amounts of labeled data, which is difficult to obtain due to high\nannotation costs and data privacy concerns. To circumvent this issue, we\nintroduce MRI-CORE (MRI COmprehensive Representation Encoder), a vision\nfoundation model pre-trained using more than 6 million slices from over 110,000\nMRI volumes across 18 main body locations. Experiments on five diverse object\nsegmentation tasks in MRI demonstrate that MRI-CORE can significantly improve\nsegmentation performance in realistic scenarios with limited labeled data\navailability, achieving an average gain of 6.97% 3D Dice Coefficient using only\n10 annotated slices per task. We further demonstrate new model capabilities in\nMRI such as classification of image properties including body location,\nsequence type and institution, and zero-shot segmentation. These results\nhighlight the value of MRI-CORE as a generalist vision foundation model for\nMRI, potentially lowering the data annotation resource barriers for many\napplications.", "comment": "19 pages, 5 figures", "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.AI;cs.CV;cs.LG", "links": "http://arxiv.org/abs/2506.12186v1;http://arxiv.org/pdf/2506.12186v1", "pdf_url": "http://arxiv.org/pdf/2506.12186v1"}]
