[{"title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training", "link": "https://arxiv.org/pdf/2405.20978", "details": "F Fang, Y Bai, S Ni, M Yang, X Chen, R Xu - arXiv preprint arXiv:2405.20978, 2024", "abstract": "Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising \u2026"}, {"title": "Memory augmented language models through mixture of word experts", "link": "https://aclanthology.org/2024.naacl-long.249.pdf", "details": "C dos Santos, J Lee-Thorp, I Noble, CC Chang\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Scaling up the number of parameters of language models has proven to be an effective approach to improve performance. For dense models, increasing their size proportionally increases their computational footprint. In this work, we seek to \u2026"}, {"title": "Feature Contamination: Neural Networks Learn Uncorrelated Features and Fail to Generalize", "link": "https://openreview.net/pdf%3Fid%3DLjhrv1Wmbr", "details": "T Zhang, C Zhao, G Chen, Y Jiang, F Chen - Forty-first International Conference on \u2026, 2024", "abstract": "Learning representations that generalize under distribution shifts is critical for building robust machine learning models. However, despite significant efforts in recent years, algorithmic advances in this direction have been limited. In this work \u2026"}, {"title": "LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models", "link": "https://aclanthology.org/2024.naacl-long.360.pdf", "details": "Y Xu, W Wang - Proceedings of the 2024 Conference of the North \u2026, 2024", "abstract": "Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks \u2026"}, {"title": "The Impact of Depth on Compositional Generalization in Transformer Language Models", "link": "https://aclanthology.org/2024.naacl-long.402.pdf", "details": "J Petty, S Steenkiste, I Dasgupta, F Sha, D Garrette\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "To process novel sentences, language models (LMs) must generalize compositionally\u2014combine familiar elements in new ways. What aspects of a model's structure promote compositional generalization? Focusing on transformers, we test \u2026"}, {"title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback", "link": "https://openreview.net/pdf%3Fid%3DBOorDpKHiJ", "details": "G Cui, L Yuan, N Ding, G Yao, B He, W Zhu, Y Ni, G Xie\u2026 - Forty-first International \u2026, 2024", "abstract": "Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability \u2026"}, {"title": "Named Entity Recognition Under Domain Shift via Metric Learning for Life Sciences", "link": "https://aclanthology.org/2024.naacl-long.1.pdf", "details": "H Liu, Q Wang, P Karisani, H Ji - Proceedings of the 2024 Conference of the North \u2026, 2024", "abstract": "Named entity recognition is a key component of Information Extraction (IE), particularly in scientific domains such as biomedicine and chemistry, where large language models (LLMs), eg, ChatGPT, fall short. We investigate the applicability of \u2026"}, {"title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models", "link": "https://aclanthology.org/2024.findings-naacl.18.pdf", "details": "Z Su, Z Lin, B Baixue, H Chen, S Hu, W Zhou, G Ding\u2026 - Findings of the Association \u2026, 2024", "abstract": "Generative language models are usually pre-trained on large text corpus via predicting the next token (ie, sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language \u2026"}, {"title": "A novel causal feature learning-based domain generalization framework for bearing fault diagnosis with a mixture of data from multiple working conditions and \u2026", "link": "https://www.sciencedirect.com/science/article/pii/S1474034624002702", "details": "L Cheng, X Kong, Y Zhang, Y Zhu, H Qi, J Zhang - Advanced Engineering Informatics, 2024", "abstract": "Data distribution shift usually deteriorates the performance of traditional supervised learning-based fault diagnosis models. Such issue can be well addressed by domain generalization fault diagnosis methods even if the test data are unavailable in \u2026"}]
