[{"title": "Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models", "link": "https://arxiv.org/pdf/2407.20271", "details": "H Tang, Y Liu, X Liu, K Zhang, Y Zhang, Q Liu, E Chen - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in machine learning, especially in Natural Language Processing (NLP), have led to the development of sophisticated models trained on vast datasets, but this progress has raised concerns about potential sensitive \u2026"}, {"title": "Scalable information extraction from free text electronic health records using large language models", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/08/10/2024.08.08.24311237.full.pdf", "details": "B Gu, V Shao, Z Liao, V Carducci, S Romero-Brufau\u2026 - medRxiv, 2024", "abstract": "Background: A vast amount of potentially useful information such as description of patient symptoms, family, and social history is recorded as free-text notes in electronic health records (EHRs) but is difficult to reliably extract at scale, limiting \u2026"}, {"title": "Physician experiences of electronic health records interoperability and its practical impact on care delivery in the English NHS: A cross-sectional survey study", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/07/26/2024.07.25.24311018.full.pdf", "details": "E Li, O Lounsbury, M Hasnain, H Ashrafian, A Darzi\u2026 - medRxiv, 2024", "abstract": "Background: The lack of interoperability has been a well-recognised limitation associated with the use of electronic health records (EHR). However, less is known about how it manifests for frontline NHS staff when delivering care, how it impacts \u2026"}, {"title": "Individualized melanoma risk prediction using machine learning with electronic health records", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/07/27/2024.07.26.24311080.full.pdf", "details": "G Wan, S Khattab, K Roster, N Nguyen, B Yan\u2026 - medRxiv, 2024", "abstract": "Background: Melanoma is a lethal form of skin cancer with a high propensity for metastasizing, making early detection crucial. This study aims to develop a machine learning model using electronic health record data to identify patients at high risk of \u2026"}, {"title": "Two Stacks Are Better Than One: A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "link": "https://arxiv.org/pdf/2407.15489", "details": "Z Li, S Ji, T Mickus, V Segonne, J Tiedemann - arXiv preprint arXiv:2407.15489, 2024", "abstract": "Pretrained language models (PLMs) display impressive performances and have captured the attention of the NLP community. Establishing the best practices in pretraining has therefore become a major point of focus for much of NLP research \u2026"}, {"title": "Exploring Universal Intrinsic Task Subspace for Few-shot Learning via Prompt Tuning", "link": "https://ieeexplore.ieee.org/iel8/6570655/6633080/10603438.pdf", "details": "Y Qin, X Wang, Y Su, Y Lin, N Ding, J Yi, W Chen, Z Liu\u2026 - IEEE/ACM Transactions on \u2026, 2024", "abstract": "Why can pre-trained language models (PLMs) learn universal representations and effectively adapt to broad NLP tasks differing a lot superficially? In this work, we empirically find evidence indicating that the adaptations of PLMs to various fewshot \u2026"}, {"title": "Accuracy and transportability of machine learning models for adolescent suicide prediction with longitudinal clinical records", "link": "https://www.nature.com/articles/s41398-024-03034-3", "details": "C Zang, Y Hou, D Lyu, J Jin, S Sacco, K Chen\u2026 - Translational psychiatry, 2024", "abstract": "Abstract Machine Learning models trained from real-world data have demonstrated promise in predicting suicide attempts in adolescents. However, their transportability, namely the performance of a model trained on one dataset and applied to different \u2026"}, {"title": "Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models", "link": "https://gonenhila.github.io/files/Semantic_Leakage.pdf", "details": "H Gonen, T Blevins, A Liu, L Zettlemoyer, NA Smith", "abstract": "Despite their wide adoption, the biases and unintended behaviors of language models remain poorly understood. In this paper, we identify and characterize a phenomenon never discussed before, which we call semantic leakage, where \u2026"}, {"title": "Towards Effective and Efficient Continual Pre-training of Large Language Models", "link": "https://arxiv.org/pdf/2407.18743", "details": "J Chen, Z Chen, J Wang, K Zhou, Y Zhu, J Jiang, Y Min\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Continual pre-training (CPT) has been an important approach for adapting language models to specific domains or tasks. To make the CPT approach more traceable, this paper presents a technical report for continually pre-training Llama-3 (8B), which \u2026"}]
