[{"title": "WISER: Weak supervISion and supErvised Representation learning to improve drug response prediction in cancer", "link": "https://arxiv.org/pdf/2405.04078", "details": "K Shubham, A Jayagopal, SM Danish, P AP, V Rajan - arXiv preprint arXiv \u2026, 2024", "abstract": "Cancer, a leading cause of death globally, occurs due to genomic changes and manifests heterogeneously across patients. To advance research on personalized treatment strategies, the effectiveness of various drugs on cells derived from cancers \u2026"}, {"title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning", "link": "https://arxiv.org/pdf/2405.10292", "details": "Y Zhai, H Bai, Z Lin, J Pan, S Tong, Y Zhou, A Suhr\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision-language models (VLMs) fine-tuned on specialized visual instruction- following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently \u2026"}, {"title": "Lower Bounds on the Expressivity of Recurrent Neural Language Models", "link": "https://arxiv.org/pdf/2405.19222", "details": "A Svete, F Nowak, AM Sahabdeen, R Cotterell - arXiv preprint arXiv:2405.19222, 2024", "abstract": "The recent successes and spread of large neural language models (LMs) call for a thorough understanding of their computational ability. Describing their computational abilities through LMs'\\emph {representational capacity} is a lively area of research \u2026"}, {"title": "ECR-Chain: Advancing Generative Language Models to Better Emotion-Cause Reasoners through Reasoning Chains", "link": "https://arxiv.org/pdf/2405.10860", "details": "Z Huang, J Zhao, Q Jin - arXiv preprint arXiv:2405.10860, 2024", "abstract": "Understanding the process of emotion generation is crucial for analyzing the causes behind emotions. Causal Emotion Entailment (CEE), an emotion-understanding task, aims to identify the causal utterances in a conversation that stimulate the emotions \u2026"}, {"title": "Coding of childhood psychiatric and neurodevelopmental disorders in electronic health records of a large integrated health care system: validation study", "link": "https://mental.jmir.org/2024/1/e56812", "details": "JM Shi, VY Chiu, CC Avila, S Lewis, D Park, MR Peltier\u2026 - JMIR Mental Health, 2024", "abstract": "Background Mental, emotional, and behavioral disorders are chronic pediatric conditions, and their prevalence has been on the rise over recent decades. Affected children have long-term health sequelae and a decline in health-related quality of \u2026"}, {"title": "Annot-Mix: Learning with Noisy Class Labels from Multiple Annotators via a Mixup Extension", "link": "https://arxiv.org/pdf/2405.03386", "details": "M Herde, L L\u00fchrs, D Huseljic, B Sick - arXiv preprint arXiv:2405.03386, 2024", "abstract": "Training with noisy class labels impairs neural networks' generalization performance. In this context, mixup is a popular regularization technique to improve training robustness by making memorizing false class labels more difficult. However, mixup \u2026"}, {"title": "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models", "link": "https://arxiv.org/pdf/2405.06134", "details": "V Raina, R Ma, C McGhee, K Knill, M Gales - arXiv preprint arXiv:2405.06134, 2024", "abstract": "Recent developments in large speech foundation models like Whisper have led to their widespread use in many automatic speech recognition (ASR) applications. These systems incorporatespecial tokens' in their vocabulary, such as $\\texttt {< \u2026"}, {"title": "Recall Them All: Retrieval-Augmented Language Models for Long Object List Extraction from Long Documents", "link": "https://arxiv.org/pdf/2405.02732", "details": "S Singhania, S Razniewski, G Weikum - arXiv preprint arXiv:2405.02732, 2024", "abstract": "Methods for relation extraction from text mostly focus on high precision, at the cost of limited recall. High recall is crucial, though, to populate long lists of object entities that stand in a specific relation with a given subject. Cues for relevant objects can be \u2026"}, {"title": "Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks", "link": "https://arxiv.org/pdf/2405.10548", "details": "A Chatterjee, E Tanwar, S Dutta, T Chakraborty - arXiv preprint arXiv:2405.10548, 2024", "abstract": "Large Language Models (LLMs) have transformed NLP with their remarkable In- context Learning (ICL) capabilities. Automated assistants based on LLMs are gaining popularity; however, adapting them to novel tasks is still challenging. While colossal \u2026"}]
