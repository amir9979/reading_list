[{"title": "TextCAVs: Debugging vision models using text", "link": "https://arxiv.org/pdf/2408.08652", "details": "A Nicolson, Y Gal, JA Noble - arXiv preprint arXiv:2408.08652, 2024", "abstract": "Concept-based interpretability methods are a popular form of explanation for deep learning models which provide explanations in the form of high-level human interpretable concepts. These methods typically find concept activation vectors \u2026"}, {"title": "Accuracy and transportability of machine learning models for adolescent suicide prediction with longitudinal clinical records", "link": "https://www.nature.com/articles/s41398-024-03034-3", "details": "C Zang, Y Hou, D Lyu, J Jin, S Sacco, K Chen\u2026 - Translational psychiatry, 2024", "abstract": "Abstract Machine Learning models trained from real-world data have demonstrated promise in predicting suicide attempts in adolescents. However, their transportability, namely the performance of a model trained on one dataset and applied to different \u2026"}, {"title": "Mitigating Multilingual Hallucination in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2408.00550", "details": "X Qu, M Song, W Wei, J Dong, Y Cheng - arXiv preprint arXiv:2408.00550, 2024", "abstract": "While Large Vision-Language Models (LVLMs) have exhibited remarkable capabilities across a wide range of tasks, they suffer from hallucination problems, where models generate plausible yet incorrect answers given the input image-query \u2026"}]
