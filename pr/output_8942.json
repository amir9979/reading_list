[{"title": "Mixed Distillation Helps Smaller Language Models Reason Better", "link": "https://aclanthology.org/2024.findings-emnlp.91.pdf", "details": "L Chenglin, Q Chen, L Li, C Wang, F Tao, Y Li, Z Chen\u2026 - Findings of the Association \u2026, 2024", "abstract": "As large language models (LLMs) have demonstrated impressive multiple step-by- step reasoning capabilities in recent natural language processing (NLP) reasoning tasks, many studies are interested in distilling reasoning abilities into smaller \u2026"}, {"title": "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "link": "https://aclanthology.org/2024.emnlp-main.565.pdf", "details": "J Li, H Zhang, F Zhang, TW Chang, K Kuang, L Chen\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Reinforcement learning from human feedback (RLHF) and AI-generated feedback (RLAIF) have become prominent techniques that significantly enhance the functionality of pre-trained language models (LMs). These methods harness \u2026"}, {"title": "Improving Referring Ability for Biomedical Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.375.pdf", "details": "J Jiang, F Cheng, A Aizawa - Findings of the Association for Computational \u2026, 2024", "abstract": "Existing auto-regressive large language models (LLMs) are primarily trained using documents from general domains. In the biomedical domain, continual pre-training is a prevalent method for domain adaptation to inject professional knowledge into \u2026"}, {"title": "Counterfactual Generation from Language Models", "link": "https://arxiv.org/pdf/2411.07180", "details": "S Ravfogel, A Svete, V Sn\u00e6bjarnarson, R Cotterell - arXiv preprint arXiv:2411.07180, 2024", "abstract": "Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery--eg, model ablations or manipulation \u2026"}, {"title": "Concept Bottleneck Language Models For protein design", "link": "https://arxiv.org/pdf/2411.06090", "details": "AA Ismail, T Oikarinen, A Wang, J Adebayo, S Stanton\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce Concept Bottleneck Protein Language Models (CB-pLM), a generative masked language model with a layer where each neuron corresponds to an interpretable concept. Our architecture offers three key benefits: i) Control: We can \u2026"}, {"title": "Rethinking the Role of Proxy Rewards in Language Model Alignment", "link": "https://aclanthology.org/2024.emnlp-main.1150.pdf", "details": "S Kim, M Seo - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Learning from human feedback via proxy reward modeling has been studied to align Large Language Models (LLMs) with human values. However, achieving reliable training through that proxy reward model (RM) is not a trivial problem, and its \u2026"}, {"title": "TAT-LLM: A Specialized Language Model for Discrete Reasoning over Financial Tabular and Textual Data", "link": "https://dl.acm.org/doi/abs/10.1145/3677052.3698685", "details": "F Zhu, Z Liu, F Feng, C Wang, M Li, TS Chua - \u2026 ACM International Conference on AI in \u2026, 2024", "abstract": "In this work, we develop a specialized language model with strong discrete reasoning capabilities to tackle question answering (QA) over hybrid tabular and textual data in finance. Compared with adopting online LLMs, specializing smaller \u2026"}, {"title": "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "link": "https://aclanthology.org/2024.emnlp-main.571.pdf", "details": "X Ren, B Wu, L Liu - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "This paper explores an intriguing observation: fine-tuning a large language model (LLM) with responses generated by a LLM often yields better results than using responses generated by humans, particularly in reasoning tasks. We conduct an in \u2026"}, {"title": "Self-Training Large Language and Vision Assistant for Medical Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.1119.pdf", "details": "G Sun, C Qin, H Fu, L Wang, Z Tao - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "Abstract Large Vision-Language Models (LVLMs) have shown significant potential in assisting medical diagnosis by leveraging extensive biomedical datasets. However, the advancement of medical image understanding and reasoning critically depends \u2026"}]
