[{"title": "Goldfish: Monolingual Language Models for 350 Languages", "link": "https://arxiv.org/pdf/2408.10441", "details": "TA Chang, C Arnett, Z Tu, BK Bergen - arXiv preprint arXiv:2408.10441, 2024", "abstract": "For many low-resource languages, the only available language models are large multilingual models trained on many languages simultaneously. However, using FLORES perplexity as a metric, we find that these models perform worse than \u2026"}, {"title": "An Evidence-Based Framework For Heterogeneous Electronic Health Records: A Case Study In Mortality Prediction", "link": "https://link.springer.com/chapter/10.1007/978-3-031-67977-3_9", "details": "Y Ruan, L Huang, Q Xu, M Feng - International Conference on Belief Functions, 2024", "abstract": "Abstract Electronic Health Records (EHRs), characterized by their centralization of patient comprehensive disease and history information, hold significant promise to improve healthcare quality and efficiency. However, the heterogeneous nature of \u2026"}, {"title": "Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language Models", "link": "https://arxiv.org/pdf/2409.01584", "details": "S Ozaki, K Hayashi, Y Sakai, H Kamigaito, K Hayashi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As the performance of Large-scale Vision Language Models (LVLMs) improves, they are increasingly capable of responding in multiple languages, and there is an expectation that the demand for explanations generated by LVLMs will grow \u2026"}, {"title": "Language Models Pre-training", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-65647-7_2.pdf", "details": "U Kamath, K Keenan, G Somers, S Sorenson - Large Language Models: A Deep Dive \u2026, 2024", "abstract": "Pre-training forms the foundation for LLMs' capabilities. LLMs gain vital language comprehension and generative language skills by using large-scale datasets. The size and quality of these datasets are essential for maximizing LLMs' potential. It is \u2026"}, {"title": "Scaling Pre-training Data and Language Models for African Languages", "link": "https://uwspace.uwaterloo.ca/bitstreams/0eb3cf2b-5660-492e-8be6-6f4d4e82f5e1/download", "details": "A Oladipo - 2024", "abstract": "Recent advancements in language models, particularly for high-resource languages, have not been paralleled in low-resource languages spoken across Africa. This thesis addresses this gap by scaling pre-training data and developing improved \u2026"}, {"title": "How Does Code Pretraining Affect Language Model Task Performance?", "link": "https://arxiv.org/pdf/2409.04556", "details": "J Petty, S van Steenkiste, T Linzen - arXiv preprint arXiv:2409.04556, 2024", "abstract": "Large language models are increasingly trained on corpora containing both natural language and non-linguistic data like source code. Aside from aiding programming- related tasks, anecdotal evidence suggests that including code in pretraining corpora \u2026"}, {"title": "Regression Residual Reasoning with Pseudo-labeled Contrastive Learning for Uncovering Multiple Complex Compositional Relations", "link": "https://www.ijcai.org/proceedings/2024/0384.pdf", "details": "C Li, Y He, J Ren, R Bai, Y Zhao, H Yu, X Jiang\u2026", "abstract": "Abstract Abstract Visual Reasoning (AVR) has been widely studied in literature. Our study reveals that AVR models tend to rely on appearance matching rather than a genuine understanding of underlying rules. We hence develop a challenging \u2026"}]
