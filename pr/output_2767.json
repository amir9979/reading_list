[{"title": "Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models", "link": "https://arxiv.org/pdf/2406.09403", "details": "Y Hu, W Shi, X Fu, D Roth, M Ostendorf, L Zettlemoyer\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Humans draw to facilitate reasoning: we draw auxiliary lines when solving geometry problems; we mark and circle when reasoning on maps; we use sketches to amplify our ideas and relieve our limited-capacity working memory. However, such actions \u2026"}, {"title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback", "link": "https://openreview.net/pdf%3Fid%3DBOorDpKHiJ", "details": "G Cui, L Yuan, N Ding, G Yao, B He, W Zhu, Y Ni, G Xie\u2026 - Forty-first International Conference \u2026", "abstract": "Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability \u2026"}, {"title": "Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization", "link": "https://openreview.net/pdf%3Fid%3D1SiEfsCecd", "details": "X Zhao, D Xu, S Yuan - Forty-first International Conference on Machine \u2026", "abstract": "Pre-trained language models (PLMs) are commonly used for various downstream natural language processing tasks via fine-tuning. However, recent studies have demonstrated that PLMs are vulnerable to backdoor attacks, which can mislabel \u2026"}, {"title": "Blessing few-shot segmentation via semi-supervised learning with noisy support images", "link": "https://www.sciencedirect.com/science/article/pii/S0031320324002541", "details": "R Zhang, H Zhu, H Zhang, C Gong, JT Zhou, F Meng - Pattern Recognition, 2024", "abstract": "Mainstream few-shot segmentation methods meet performance bottleneck due to the data scarcity of novel classes with insufficient intra-class variations, which results in a biased model primarily favoring the base classes. Fortunately, owing to the evolution \u2026"}, {"title": "Is On-Device AI Broken and Exploitable? Assessing the Trust and Ethics in Small Language Models", "link": "https://arxiv.org/pdf/2406.05364", "details": "K Nakka, J Dani, N Saxena - arXiv preprint arXiv:2406.05364, 2024", "abstract": "In this paper, we present a very first study to investigate trust and ethical implications of on-device artificial intelligence (AI), focusing on''small''language models (SLMs) amenable for personal devices like smartphones. While on-device SLMs promise \u2026"}, {"title": "Contrastive Learning for Clinical Outcome Prediction with Partial Data Sources", "link": "https://openreview.net/pdf%3Fid%3DelCOPIm4Xw", "details": "M Xia, J Wilson, B Goldstein, R Henao - Forty-first International Conference on Machine \u2026", "abstract": "The use of machine learning models to predict clinical outcomes from (longitudinal) electronic health record (EHR) data is becoming increasingly popular due to advances in deep architectures, representation learning, and the growing availability \u2026"}, {"title": "mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus", "link": "https://arxiv.org/pdf/2406.08707", "details": "M Futeral, A Zebaze, PO Suarez, J Abadji, R Lacroix\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal Large Language Models (mLLMs) are trained on a large amount of text- image data. While most mLLMs are trained on caption-like data only, Alayrac et al.[2022] showed that additionally training them on interleaved sequences of text and \u2026"}, {"title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation", "link": "https://arxiv.org/pdf/2405.13769", "details": "C Chhun, FM Suchanek, C Clavel - arXiv preprint arXiv:2405.13769, 2024", "abstract": "Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high \u2026"}]
