[{"title": "Localizing Before Answering: A Hallucination Evaluation Benchmark for Grounded Medical Multimodal LLMs", "link": "https://arxiv.org/pdf/2505.00744", "details": "D Nguyen, MK Ho, H Ta, TT Nguyen, Q Chen, K Rav\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical Large Multi-modal Models (LMMs) have demonstrated remarkable capabilities in medical data interpretation. However, these models frequently generate hallucinations contradicting source evidence, particularly due to \u2026", "entry_id": "http://arxiv.org/abs/2505.00744v3", "updated": "2025-05-21 04:21:21", "published": "2025-04-30 07:57:51", "authors": "Dung Nguyen;Minh Khoi Ho;Huy Ta;Thanh Tam Nguyen;Qi Chen;Kumar Rav;Quy Duong Dang;Satwik Ramchandre;Son Lam Phung;Zhibin Liao;Minh-Son To;Johan Verjans;Phi Le Nguyen;Vu Minh Hieu Phan", "summary": "Medical Large Multi-modal Models (LMMs) have demonstrated remarkable\ncapabilities in medical data interpretation. However, these models frequently\ngenerate hallucinations contradicting source evidence, particularly due to\ninadequate localization reasoning. This work reveals a critical limitation in\ncurrent medical LMMs: instead of analyzing relevant pathological regions, they\noften rely on linguistic patterns or attend to irrelevant image areas when\nresponding to disease-related queries. To address this, we introduce\nHEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive\nbenchmark designed to evaluate LMMs' localization abilities and hallucination\nrobustness. HEAL-MedVQA features (i) two innovative evaluation protocols to\nassess visual and textual shortcut learning, and (ii) a dataset of 67K VQA\npairs, with doctor-annotated anatomical segmentation masks for pathological\nregions. To improve visual reasoning, we propose the Localize-before-Answer\n(LobA) framework, which trains LMMs to localize target regions of interest and\nself-prompt to emphasize segmented pathological areas, generating grounded and\nreliable answers. Experimental results demonstrate that our approach\nsignificantly outperforms state-of-the-art biomedical LMMs on the challenging\nHEAL-MedVQA benchmark, advancing robustness in medical VQA.", "comment": "Accepted at Joint Conference on Artificial Intelligence (IJCAI) 2025", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.00744v3;http://arxiv.org/pdf/2505.00744v3", "pdf_url": "http://arxiv.org/pdf/2505.00744v3"}]
