[{"title": "Rethinking Backdoor Detection Evaluation for Language Models", "link": "https://arxiv.org/pdf/2409.00399", "details": "J Yan, WJ Mo, X Ren, R Jia - arXiv preprint arXiv:2409.00399, 2024", "abstract": "Backdoor attacks, in which a model behaves maliciously when given an attacker- specified trigger, pose a major security risk for practitioners who depend on publicly released language models. Backdoor detection methods aim to detect whether a \u2026"}, {"title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2409.17539", "details": "T Liu, W Xu, W Huang, X Wang, J Wang, H Yang, J Li - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can \u2026"}]
