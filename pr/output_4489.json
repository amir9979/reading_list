[{"title": "Towards Multimodal-augmented Pre-trained Language Models via Self-balanced Expectation-Maximization Iteration", "link": "https://openreview.net/pdf%3Fid%3DXZi5N7eLV0", "details": "X Zhuang, X Cheng, Z Zhu, Z Chen, H Li, Y Zou - ACM Multimedia 2024", "abstract": "Pre-trained language models (PLMs) that rely solely on textual corpus may present limitations in multimodal semantics comprehension. Existing studies attempt to alleviate this issue by incorporating additional modal information through image \u2026"}, {"title": "Heterogeneous-Graph Reasoning with Context Paraphrase for Commonsense Question Answering", "link": "https://ieeexplore.ieee.org/abstract/document/10612243/", "details": "Y Wang, H Zhang, J Liang, R Li - IEEE/ACM Transactions on Audio, Speech, and \u2026, 2024", "abstract": "Commonsense question answering (CQA) generally means that the machine uses its mastered commonsense to answer questions without relevant background material, which is a challenging task in natural language processing. Existing \u2026"}, {"title": "AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization", "link": "https://arxiv.org/pdf/2407.11591", "details": "A Afzal, R Chalumattu, F Matthes, LM Espuny - arXiv preprint arXiv:2407.11591, 2024", "abstract": "Despite the advances in the abstractive summarization task using Large Language Models (LLM), there is a lack of research that asses their abilities to easily adapt to different domains. We evaluate the domain adaptation abilities of a wide range of \u2026"}, {"title": "Do Multilingual Large Language Models Mitigate Stereotype Bias?", "link": "https://arxiv.org/pdf/2407.05740", "details": "S Nie, M Fromm, C Welch, R G\u00f6rge, A Karimi, J Plepi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While preliminary findings indicate that multilingual LLMs exhibit reduced bias compared to monolingual ones, a comprehensive understanding of the effect of multilingual training on bias mitigation, is lacking. This study addresses this gap by \u2026"}, {"title": "Evaluating Large Language Models with fmeval", "link": "https://arxiv.org/pdf/2407.12872", "details": "P Schw\u00f6bel, L Franceschi, MB Zafar, K Vasist\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "fmeval is an open source library to evaluate large language models (LLMs) in a range of tasks. It helps practitioners evaluate their model for task performance and along multiple responsible AI dimensions. This paper presents the library and \u2026"}, {"title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs", "link": "https://arxiv.org/pdf/2407.00653", "details": "Y Zhang, X Wang, J Liang, S Xia, L Chen, Y Xiao - arXiv preprint arXiv:2407.00653, 2024", "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new \u2026"}, {"title": "RegMix: Data Mixture as Regression for Language Model Pre-training", "link": "https://arxiv.org/pdf/2407.01492", "details": "Q Liu, X Zheng, N Muennighoff, G Zeng, L Dou, T Pang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a \u2026"}]
