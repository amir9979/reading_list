[{"title": "Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2407.11422", "details": "J Zhang, T Wang, H Zhang, P Lu, F Zheng - arXiv preprint arXiv:2407.11422, 2024", "abstract": "Large vision-language models (LVLMs) have shown promising performance on a variety of vision-language tasks. However, they remain susceptible to hallucinations, generating outputs misaligned with visual content or instructions. While various \u2026"}, {"title": "On Speeding Up Language Model Evaluation", "link": "https://arxiv.org/pdf/2407.06172", "details": "JP Zhou, CK Belardi, R Wu, T Zhang, CP Gomes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) currently dominate the field of natural language processing (NLP), representing the state-of-the-art across a diverse array of tasks. Developing a model of this nature, from training to inference, requires making \u2026"}, {"title": "Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering", "link": "https://arxiv.org/pdf/2407.21368", "details": "D Guo, D Terzopoulos - arXiv preprint arXiv:2407.21368, 2024", "abstract": "Large Vision-Language Models (LVLMs) have achieved significant success in recent years, and they have been extended to the medical domain. Although demonstrating satisfactory performance on medical Visual Question Answering (VQA) tasks \u2026"}, {"title": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models", "link": "https://arxiv.org/pdf/2407.10380", "details": "P Pandya, AS Talwarr, V Gupta, T Kataria, V Gupta\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Cognitive textual and visual reasoning tasks, such as puzzles, series, and analogies, demand the ability to quickly reason, decipher, and evaluate patterns both textually and spatially. While LLMs and VLMs, through extensive training on large amounts of \u2026"}, {"title": "SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks", "link": "https://ieeexplore.ieee.org/abstract/document/10620644/", "details": "KW Chang, H Wu, YK Wang, YK Wu, H Shen\u2026 - IEEE/ACM Transactions on \u2026, 2024", "abstract": "Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both \u2026"}, {"title": "Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning", "link": "https://arxiv.org/pdf/2407.06112", "details": "Y Zhang, S Mao, W Wu, Y Xia, T Ge, M Lan, F Wei - arXiv preprint arXiv:2407.06112, 2024", "abstract": "This paper introduces BI-Directional DEliberation Reasoning (BIDDER), a novel reasoning approach to enhance the decision rationality of language models. Traditional reasoning methods typically rely on historical information and employ uni \u2026"}, {"title": "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages", "link": "https://arxiv.org/pdf/2407.05975", "details": "Y Lu, W Zhu, L Li, Y Qiao, F Yuan - arXiv preprint arXiv:2407.05975, 2024", "abstract": "Large Language Models~(LLMs) demonstrate remarkable translation capabilities in high-resource language tasks, yet their performance in low-resource languages is hindered by insufficient multilingual data during pre-training. To address this, we \u2026"}, {"title": "Distilling System 2 into System 1", "link": "https://arxiv.org/pdf/2407.06023", "details": "P Yu, J Xu, J Weston, I Kulikov - arXiv preprint arXiv:2407.06023, 2024", "abstract": "Large language models (LLMs) can spend extra compute during inference to generate intermediate thoughts, which helps to produce better final responses. Since Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have been \u2026"}, {"title": "Patch-Level Training for Large Language Models", "link": "https://arxiv.org/pdf/2407.12665", "details": "C Shao, F Meng, J Zhou - arXiv preprint arXiv:2407.12665, 2024", "abstract": "As Large Language Models (LLMs) achieve remarkable progress in language understanding and generation, their training efficiency has become a critical concern. Traditionally, LLMs are trained to predict the next token in a sequence \u2026"}]
