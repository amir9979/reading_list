[{"title": "Do Vision and Language Models Share Concepts? A Vector Space Alignment Study", "link": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00698/124631", "details": "J Li, Y Kementchedjhieva, C Fierro, A S\u00f8gaard - Transactions of the Association for \u2026, 2024", "abstract": "Large-scale pretrained language models (LMs) are said to \u201clack the ability to connect utterances to the world\u201d(Bender and Koller,), because they do not have \u201cmental models of the world\u201d(Mitchell and Krakauer,). If so, one would expect LM \u2026"}, {"title": "How to Train Long-Context Language Models (Effectively)", "link": "https://arxiv.org/pdf/2410.02660", "details": "T Gao, A Wettig, H Yen, D Chen - arXiv preprint arXiv:2410.02660, 2024", "abstract": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development--Instead of perplexity or simple \u2026"}, {"title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models", "link": "https://arxiv.org/pdf/2410.01335", "details": "L Bandarkar, B Muller, P Yuvraj, R Hou, N Singhal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large \u2026"}, {"title": "Fine-tuning Large Language Models to Improve Accuracy and Comprehensibility of Automated Code Review", "link": "https://dl.acm.org/doi/pdf/10.1145/3695993", "details": "Y Yu, G Rong, H Shen, H Zhang, D Shao, M Wang\u2026 - ACM Transactions on \u2026, 2024", "abstract": "As code review is a tedious and costly software quality practice, researchers have proposed several machine learning-based methods to automate the process. The primary focus has been on accuracy, that is, how accurately the algorithms are able \u2026"}, {"title": "RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models", "link": "https://arxiv.org/pdf/2409.19886", "details": "S Chen, W Jiang, B Lin, JT Kwok, Y Zhang - arXiv preprint arXiv:2409.19886, 2024", "abstract": "Recent works show that assembling multiple off-the-shelf large language models (LLMs) can harness their complementary abilities. To achieve this, routing is a promising method, which learns a router to select the most suitable LLM for each \u2026"}, {"title": "HarmAug: Effective Data Augmentation for Knowledge Distillation of Safety Guard Models", "link": "https://arxiv.org/pdf/2410.01524", "details": "S Lee, H Seong, DB Lee, M Kang, X Chen, D Wagner\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Safety guard models that detect malicious queries aimed at large language models (LLMs) are essential for ensuring the secure and responsible deployment of LLMs in real-world applications. However, deploying existing safety guard models with \u2026"}, {"title": "An Inference Method for Professional Texts with Computational Expressions under Few-shot Scenarios", "link": "https://splab.sdu.edu.cn/calc_infer.pdf", "details": "L Yang, W Zheng, F Yuan, Y Sun", "abstract": "We propose an inference method for complex professional texts with computational expressions. We use the expert rules to locate and rewrite the expressions. We adopt the pre-trained language model as the initial model and select the high quality \u2026"}, {"title": "G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models", "link": "https://arxiv.org/pdf/2410.02198", "details": "Z Yu, X Xu, H Gao - arXiv preprint arXiv:2410.02198, 2024", "abstract": "We introduce G2T-LLM, a novel approach for molecule generation that uses graph-to- tree text encoding to transform graph-based molecular structures into a hierarchical text format optimized for large language models (LLMs). This encoding converts \u2026"}, {"title": "IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2410.02429", "details": "T An, Y Zhou, H Zou, J Yang - arXiv preprint arXiv:2410.02429, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across textual and visual domains but often generate outputs that violate physical laws, revealing a gap in their understanding of the physical world. Inspired by human \u2026"}]
