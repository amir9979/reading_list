[{"title": "MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models", "link": "https://arxiv.org/pdf/2410.09733", "details": "H Hua, Y Tang, Z Zeng, L Cao, Z Yang, H He, C Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video \u2026"}, {"title": "Zero-shot extraction of seizure outcomes from clinical notes using generative pretrained transformers", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/11/04/2024.11.01.24316573.full.pdf", "details": "WKS Ojemann, K Xie, K Liu, E Chang, D Roth, B Litt\u2026 - medRxiv, 2024", "abstract": "Purpose Pre\u2013trained encoder transformer models have extracted information from unstructured clinic note text but require manual annotation for supervised fine\u2013 tuning. Large, Generative Pre\u2013trained Transformers (GPTs) may streamline this \u2026"}, {"title": "IPO: Interpretable Prompt Optimization for Vision-Language Models", "link": "https://arxiv.org/pdf/2410.15397", "details": "Y Du, W Sun, CGM Snoek - arXiv preprint arXiv:2410.15397, 2024", "abstract": "Pre-trained vision-language models like CLIP have remarkably adapted to various downstream tasks. Nonetheless, their performance heavily depends on the specificity of the input text prompts, which requires skillful prompt template \u2026"}, {"title": "Prompt tuning discriminative language models for hierarchical text classification", "link": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/50E5499348A0E72F0C4F3AFC622133A7/S2977042424000517a.pdf/div-class-title-prompt-tuning-discriminative-language-models-for-hierarchical-text-classification-div.pdf", "details": "J du Toit, M Dunaiski - Natural Language Processing", "abstract": "Hierarchical text classification (HTC) is a natural language processing task which aims to categorise a text document into a set of classes from a hierarchical class structure. Recent approaches to solve HTC tasks focus on leveraging pre-trained \u2026"}, {"title": "Identifying Semantic Relationships Between Research Topics Using Large Language Models in a Zero-Shot Learning Setting", "link": "https://ceur-ws.org/Vol-3780/paper3.pdf", "details": "T Aggarwal, A Salatino, F Osborne, E Motta - CEUR Workshop Proceedings, 2024", "abstract": "Abstract Knowledge Organization Systems (KOS), such as ontologies, taxonomies, and thesauri, play a crucial role in organising scientific knowledge. They help scientists navigate the vast landscape of research literature and are essential for \u2026"}, {"title": "AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models", "link": "https://arxiv.org/pdf/2410.10912", "details": "H Lu, Y Zhou, S Liu, Z Wang, MW Mahoney, Y Yang - arXiv preprint arXiv:2410.10912, 2024", "abstract": "Recent work on pruning large language models (LLMs) has shown that one can eliminate a large number of parameters without compromising performance, making pruning a promising strategy to reduce LLM model size. Existing LLM pruning \u2026"}, {"title": "Web-Scale Visual Entity Recognition: An LLM-Driven Data Approach", "link": "https://arxiv.org/pdf/2410.23676", "details": "M Caron, A Fathi, C Schmid, A Iscen - arXiv preprint arXiv:2410.23676, 2024", "abstract": "Web-scale visual entity recognition, the task of associating images with their corresponding entities within vast knowledge bases like Wikipedia, presents significant challenges due to the lack of clean, large-scale training data. In this paper \u2026"}, {"title": "Sparse autoencoders reveal universal feature spaces across large language models", "link": "https://arxiv.org/pdf/2410.06981", "details": "M Lan, P Torr, A Meek, A Khakzar, D Krueger, F Barez - arXiv preprint arXiv \u2026, 2024", "abstract": "We investigate feature universality in large language models (LLMs), a research field that aims to understand how different models similarly represent concepts in the latent spaces of their intermediate layers. Demonstrating feature universality allows \u2026"}, {"title": "Improving Multimodal Large Language Models Using Continual Learning", "link": "https://arxiv.org/pdf/2410.19925", "details": "S Srivastava, MY Harun, R Shrestha, C Kanan - arXiv preprint arXiv:2410.19925, 2024", "abstract": "Generative large language models (LLMs) exhibit impressive capabilities, which can be further augmented by integrating a pre-trained vision model into the original LLM to create a multimodal LLM (MLLM). However, this integration often significantly \u2026"}]
