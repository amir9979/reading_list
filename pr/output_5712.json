[{"title": "Zero\u2010and few\u2010shot prompting of generative large language models provides weak assessment of risk of bias in clinical trials", "link": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1749", "details": "S \u0160uster, T Baldwin, K Verspoor - Research Synthesis Methods", "abstract": "Existing systems for automating the assessment of risk\u2010of\u2010bias (RoB) in medical studies are supervised approaches that require substantial training data to work well. However, recent revisions to RoB guidelines have resulted in a scarcity of available \u2026"}, {"title": "Do Language Models Have a Critical Period for Language Acquisition?", "link": "https://arxiv.org/pdf/2407.19325", "details": "I Constantinescu, T Pimentel, R Cotterell, A Warstadt - arXiv preprint arXiv \u2026, 2024", "abstract": "Humans appear to have a critical period (CP) for language acquisition: Second language (L2) acquisition becomes harder after early childhood, and ceasing exposure to a first language (L1) after this period (but not before) typically does not \u2026"}, {"title": "Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering", "link": "https://arxiv.org/pdf/2407.21368", "details": "D Guo, D Terzopoulos - arXiv preprint arXiv:2407.21368, 2024", "abstract": "Large Vision-Language Models (LVLMs) have achieved significant success in recent years, and they have been extended to the medical domain. Although demonstrating satisfactory performance on medical Visual Question Answering (VQA) tasks \u2026"}, {"title": "Unleash The Power of Pre-Trained Language Models for Irregularly Sampled Time Series", "link": "https://arxiv.org/pdf/2408.08328", "details": "W Zhang, C Yin, H Liu, H Xiong - arXiv preprint arXiv:2408.08328, 2024", "abstract": "Pre-trained Language Models (PLMs), such as ChatGPT, have significantly advanced the field of natural language processing. This progress has inspired a series of innovative studies that explore the adaptation of PLMs to time series \u2026"}, {"title": "Designing Retrieval-Augmented Language Models for Clinical Decision", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DWcMbEQAAQBAJ%26oi%3Dfnd%26pg%3DPA159%26ots%3DtCwVx1UHdn%26sig%3D86G44wYe33k8KDgqH4qfi-EdW84", "details": "K Quigley, T Koker, J Taylor, V Mancuso - AI for Health Equity and Fairness: Leveraging AI to \u2026", "abstract": "Ever-increasing demands for physician expertise drive the need for trust-worthy point- of-care tools that can help aid decision-making in all clinical settings. Retrieval- augmented language models carry potential to relieve the information burden on \u2026"}, {"title": "Enhanced Prompt Learning for Few-shot Text Classification Method", "link": "https://search.proquest.com/openview/58909c856764791ee70d5ec9bee01321/1%3Fpq-origsite%3Dgscholar%26cbl%3D2048897", "details": "L Ruifan, W Zhiyu, F Yuantao, Y Shuqin, Z Guangwei - Beijing Da Xue Xue Bao, 2024", "abstract": "An enhanced prompt learning method (EPL4FTC) for few-shot text classification task is proposed. This algorithm first converts the text classification task into the form of prompt learning based on natural language inference. Thus, the implicit data \u2026"}, {"title": "Zero-shot Cross-lingual Alignment for Embedding Initialization", "link": "https://aclanthology.org/2024.findings-acl.358.pdf", "details": "X Ai, Z Huang - Findings of the Association for Computational \u2026, 2024", "abstract": "For multilingual training, we present CrossInit, an initialization method that initializes embeddings into similar geometrical structures across languages in an unsupervised manner. CrossInit leverages a common cognitive linguistic mechanism, Zipf's law \u2026"}, {"title": "Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models", "link": "https://aclanthology.org/2024.acl-long.525.pdf", "details": "Y Sun, C Liu, K Zhou, J Huang, R Song, WX Zhao\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Humans often interact with large language models (LLMs) in multi-turn interaction to obtain desired answers or more information. However, most existing studies overlook the multi-turn instruction following ability of LLMs, in terms of training dataset, training \u2026"}, {"title": "Towards Harnessing Large Language Models as Autonomous Agents for Semantic Triple Extraction from Unstructured Text", "link": "https://ceur-ws.org/Vol-3747/text2kg_paper1.pdf", "details": "A Ananya, S Tiwari, N Mihindukulasooriya, T Soru\u2026 - 2024", "abstract": "Abstract The use of Large Language Models as autonomous agents interacting with tools has shown to improve the performance of several tasks from code generation to API calling and sequencing. This paper proposes a framework for using Large \u2026"}]
