[{"title": "From Predictions to Analyses: Rationale-Augmented Fake News Detection with Large Vision-Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3696410.3714532", "details": "X Zheng, Z Zeng, H Wang, Y Bai, Y Liu, M Luo - Proceedings of the ACM on Web \u2026, 2025", "abstract": "The rapid development of social media has led to a surge of eye-catching fake news on the Internet, with multimodal news comprising both images and text being particularly prevalent. To address the challenges of Multimodal Fake News Detection \u2026"}, {"title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB", "link": "https://arxiv.org/pdf/2504.01157", "details": "A Dorbani, S Yasser, J Lin, A Mhedhbi - arXiv preprint arXiv:2504.01157, 2025", "abstract": "Knowledge-intensive analytical applications retrieve context from both structured tabular data and unstructured, text-free documents for effective decision-making. Large language models (LLMs) have made it significantly easier to prototype such \u2026"}, {"title": "Agentic Large Language Models, a survey", "link": "https://arxiv.org/pdf/2503.23037", "details": "A Plaat, M van Duijn, N van Stein, M Preuss\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "There is great interest in agentic LLMs, large language models that act as agents. We review the growing body of work in this area and provide a research agenda. Agentic LLMs are LLMs that (1) reason,(2) act, and (3) interact. We organize the \u2026"}, {"title": "Evaluating Logical Reasoning Ability of Large Language Models", "link": "https://www.preprints.org/frontend/manuscript/9e037bed340c892b7e0f2084f1f17252/download_pub", "details": "E Chan - 2025", "abstract": "Large language models (LLMs) such as ChatGPT and DeepSeek have recently made significant progress in natural language processing, demonstrating reasoning ability close to human intelligence. This has sparked considerable research interest \u2026"}, {"title": "Firm or fickle? evaluating large language models consistency in sequential interactions", "link": "https://arxiv.org/pdf/2503.22353%3F", "details": "Y Li, Y Miao, X Ding, R Krishnan, R Padman - arXiv preprint arXiv:2503.22353, 2025", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities across various tasks, but their deployment in high-stake domains requires consistent performance across multiple interaction rounds. This paper introduces a comprehensive \u2026"}, {"title": "The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models", "link": "https://arxiv.org/pdf/2504.15068", "details": "R Pradeep, N Thakur, S Upadhyay, D Campos\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have significantly enhanced the capabilities of information access systems, especially with retrieval-augmented generation (RAG). Nevertheless, the evaluation of RAG systems remains a barrier to continued \u2026"}, {"title": "Distilling Structured Rationale from Large Language Models to Small Language Models for Abstractive Summarization", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/34727/36882", "details": "L Wang, L Wu, S Song, Y Wang, C Gao, K Wang - \u2026 of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Abstract Large Language Models (LLMs) have permeated various Natural Language Processing (NLP) tasks. For the summarization tasks, LLMs can generate well- structured rationales, which consist of Essential Aspects (EA), Associated Sentences \u2026"}, {"title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization", "link": "https://arxiv.org/pdf/2503.23733", "details": "Y Du, X Wang, C Chen, J Ye, Y Wang, P Li, M Yan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous \u2026"}, {"title": "Functional Abstraction of Knowledge Recall in Large Language Models", "link": "https://arxiv.org/pdf/2504.14496", "details": "Z Wang, C Xu - arXiv preprint arXiv:2504.14496, 2025", "abstract": "Pre-trained transformer large language models (LLMs) demonstrate strong knowledge recall capabilities. This paper investigates the knowledge recall mechanism in LLMs by abstracting it into a functional structure. We propose that \u2026"}]
