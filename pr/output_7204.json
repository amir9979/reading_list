[{"title": "FuzzCoder: Byte-level Fuzzing Test via Large Language Model", "link": "https://arxiv.org/pdf/2409.01944", "details": "L Yang, J Yang, C Wei, G Niu, G Zhang, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Fuzzing is an important dynamic program analysis technique designed for finding vulnerabilities in complex software. Fuzzing involves presenting a target program with crafted malicious input to cause crashes, buffer overflows, memory errors, and \u2026"}, {"title": "Joint Biomedical Entity and Relation Extraction Based on Multi-Granularity Convolutional Tokens Pairs of Labeling", "link": "https://cdn.techscience.cn/files/cmc/2024/TSP_CMC-80-3/TSP_CMC_53588/TSP_CMC_53588.pdf", "details": "Z Sun, L Xing, L Zhang, H Cai, M Guo - Computers, Materials and Continua, 2024", "abstract": "Extracting valuable information from biomedical texts is one of the current research hotspots of concern to a wide range of scholars. The biomedical corpus contains numerous complex long sentences and overlapping relational triples, making most \u2026"}, {"title": "Masked Momentum Contrastive Learning for Semantic Understanding by Observation", "link": "https://ieeexplore.ieee.org/abstract/document/10647831/", "details": "J Wu, S Mo, S Atito, Z Feng, J Kittler, SS Husain\u2026 - 2024 IEEE International \u2026, 2024", "abstract": "Large language models (LLMs) have shown excellent performance in zero-shot learning using natural language prompts. However, in the domain of computer vision (CV), the paradigm of pretraining followed by finetuning remains dominant. The aim \u2026"}, {"title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "link": "https://arxiv.org/pdf/2409.13853", "details": "Z Wang, R Bao, Y Wu, J Taylor, C Xiao, F Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pretrained large language models (LLMs) have revolutionized natural language processing (NLP) tasks such as summarization, question answering, and translation. However, LLMs pose significant security risks due to their tendency to memorize \u2026"}]
