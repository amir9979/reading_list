[{"title": "The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It", "link": "https://arxiv.org/pdf/2505.24119", "details": "ZX Yong, B Ermis, M Fadaee, SH Bach, J Kreutzer - arXiv preprint arXiv:2505.24119, 2025", "abstract": "This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. Through a systematic review of nearly 300 publications from 2020--2024 across major NLP \u2026", "entry_id": "http://arxiv.org/abs/2505.24119v1", "updated": "2025-05-30 01:32:44", "published": "2025-05-30 01:32:44", "authors": "Zheng-Xin Yong;Beyza Ermis;Marzieh Fadaee;Stephen H. Bach;Julia Kreutzer", "summary": "This paper presents a comprehensive analysis of the linguistic diversity of\nLLM safety research, highlighting the English-centric nature of the field.\nThrough a systematic review of nearly 300 publications from 2020--2024 across\nmajor NLP conferences and workshops at *ACL, we identify a significant and\ngrowing language gap in LLM safety research, with even high-resource\nnon-English languages receiving minimal attention. We further observe that\nnon-English languages are rarely studied as a standalone language and that\nEnglish safety research exhibits poor language documentation practice. To\nmotivate future research into multilingual safety, we make several\nrecommendations based on our survey, and we then pose three concrete future\ndirections on safety evaluation, training data generation, and crosslingual\nsafety generalization. Based on our survey and proposed directions, the field\ncan develop more robust, inclusive AI safety practices for diverse global\npopulations.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.24119v1;http://arxiv.org/pdf/2505.24119v1", "pdf_url": "http://arxiv.org/pdf/2505.24119v1"}, {"title": "A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs", "link": "https://arxiv.org/pdf/2505.23816", "details": "T Chang, T Schnabel, A Swaminathan, J Wiens - arXiv preprint arXiv:2505.23816, 2025", "abstract": "\u2026 Here, we formalize steerability in the context of **LLM** **evaluation** (Section 2.1), and introduce metrics for LLM performance in the space of \u2026 Our steerability definition fills an underexplored niche in **LLM** **evaluation** strategies (Table 1). In particular, our \u2026", "entry_id": "http://arxiv.org/abs/2505.23816v1", "updated": "2025-05-27 21:29:52", "published": "2025-05-27 21:29:52", "authors": "Trenton Chang;Tobias Schnabel;Adith Swaminathan;Jenna Wiens", "summary": "Despite advances in large language models (LLMs) on reasoning and\ninstruction-following benchmarks, it remains unclear whether they can reliably\nproduce outputs aligned with a broad variety of user goals, a concept we refer\nto as steerability. The abundance of methods proposed to modify LLM behavior\nmakes it unclear whether current LLMs are already steerable, or require further\nintervention. In particular, LLMs may exhibit (i) poor coverage, where rare\nuser goals are underrepresented; (ii) miscalibration, where models overshoot\nrequests; and (iii) side effects, where changes to one dimension of text\ninadvertently affect others. To systematically evaluate these failures, we\nintroduce a framework based on a multi-dimensional goal space that models user\ngoals and LLM outputs as vectors with dimensions corresponding to text\nattributes (e.g., reading difficulty). Applied to a text-rewriting task, we\nfind that current LLMs struggle with steerability, as side effects are\npersistent. Interventions to improve steerability, such as prompt engineering,\nbest-of-$N$ sampling, and reinforcement learning fine-tuning, have varying\neffectiveness, yet side effects remain problematic. Our findings suggest that\neven strong LLMs struggle with steerability, and existing alignment strategies\nmay be insufficient. We open-source our steerability evaluation framework at\nhttps://github.com/MLD3/steerability.", "comment": "10 pages, 8 figures. 26 pages of references and supplementary\n  material, 20 additional figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.23816v1;http://arxiv.org/pdf/2505.23816v1", "pdf_url": "http://arxiv.org/pdf/2505.23816v1"}, {"title": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX", "link": "https://arxiv.org/pdf/2505.24616", "details": "N Martynov, A Mordasheva, D Gorbetskiy, D Astafurov\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 The key contributions of this work include: \u2022 A general methodology for **LLM** **evaluation** , comprising: \u2013 A hierarchical taxonomy of generative tasks, categorized by complexity and domain. \u2013 A fine-grained taxonomy of criteria for systematic \u2026", "entry_id": "http://arxiv.org/abs/2505.24616v1", "updated": "2025-05-30 14:08:17", "published": "2025-05-30 14:08:17", "authors": "Nikita Martynov;Anastasia Mordasheva;Dmitriy Gorbetskiy;Danil Astafurov;Ulyana Isaeva;Elina Basyrova;Sergey Skachkov;Victoria Berestova;Nikolay Ivanov;Valeriia Zanina;Alena Fenogenova", "summary": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments.", "comment": "179 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.24616v1;http://arxiv.org/pdf/2505.24616v1", "pdf_url": "http://arxiv.org/pdf/2505.24616v1"}, {"title": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks", "link": "https://arxiv.org/pdf/2505.23802", "details": "S Bedi, H Cui, M Fuentes, A Unell, M Wornow\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While large language models (LLMs) achieve near-perfect scores on medical licensing exams, these evaluations inadequately reflect the complexity and diversity of real-world clinical practice. We introduce MedHELM, an extensible evaluation \u2026", "entry_id": "http://arxiv.org/abs/2505.23802v2", "updated": "2025-06-02 04:19:10", "published": "2025-05-26 22:55:49", "authors": "Suhana Bedi;Hejie Cui;Miguel Fuentes;Alyssa Unell;Michael Wornow;Juan M. Banda;Nikesh Kotecha;Timothy Keyes;Yifan Mai;Mert Oez;Hao Qiu;Shrey Jain;Leonardo Schettini;Mehr Kashyap;Jason Alan Fries;Akshay Swaminathan;Philip Chung;Fateme Nateghi;Asad Aali;Ashwin Nayak;Shivam Vedak;Sneha S. Jain;Birju Patel;Oluseyi Fayanju;Shreya Shah;Ethan Goh;Dong-han Yao;Brian Soetikno;Eduardo Reis;Sergios Gatidis;Vasu Divi;Robson Capasso;Rachna Saralkar;Chia-Chun Chiang;Jenelle Jindal;Tho Pham;Faraz Ghoddusi;Steven Lin;Albert S. Chiou;Christy Hong;Mohana Roy;Michael F. Gensheimer;Hinesh Patel;Kevin Schulman;Dev Dash;Danton Char;Lance Downing;Francois Grolleau;Kameron Black;Bethel Mieso;Aydin Zahedivash;Wen-wai Yim;Harshita Sharma;Tony Lee;Hannah Kirsch;Jennifer Lee;Nerissa Ambers;Carlene Lugtu;Aditya Sharma;Bilal Mawji;Alex Alekseyev;Vicky Zhou;Vikas Kakkar;Jarrod Helzer;Anurang Revri;Yair Bannett;Roxana Daneshjou;Jonathan Chen;Emily Alsentzer;Keith Morse;Nirmal Ravi;Nima Aghaeepour;Vanessa Kennedy;Akshay Chaudhari;Thomas Wang;Sanmi Koyejo;Matthew P. Lungren;Eric Horvitz;Percy Liang;Mike Pfeffer;Nigam H. Shah", "summary": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.23802v2;http://arxiv.org/pdf/2505.23802v2", "pdf_url": "http://arxiv.org/pdf/2505.23802v2"}, {"title": "Live Prototyping for Evaluating Generative AI: Exploring the Potential and the Pitfalls", "link": "https://link.springer.com/chapter/10.1007/978-3-031-93418-6_3", "details": "Y Guo, J Sliter, C Oelsen - International Conference on Human-Computer \u2026, 2025", "abstract": "\u2026 These user-driven criteria can then be incorporated into **LLM** **evaluation** frameworks, ultimately contributing to the development of higher-quality models that better meet user needs. This approach moves beyond pre-defined metrics and \u2026"}, {"title": "Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs", "link": "https://arxiv.org/pdf/2505.23996", "details": "YO Wang, N Sivakumar, FA Khan, RM Susa, A Golinski\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 10, offering a two-dimensional, comprehensive perspective on modern **LLM** **evaluation**. The x-axis represents the accuracy of the model, while the y-axis represents fairness (UCerF). The area between each point and the origin represents \u2026", "entry_id": "http://arxiv.org/abs/2505.23996v1", "updated": "2025-05-29 20:45:18", "published": "2025-05-29 20:45:18", "authors": "Yinong Oliver Wang;Nivedha Sivakumar;Falaah Arif Khan;Rin Metcalf Susa;Adam Golinski;Natalie Mackraz;Barry-John Theobald;Luca Zappella;Nicholas Apostoloff", "summary": "The recent rapid adoption of large language models (LLMs) highlights the\ncritical need for benchmarking their fairness. Conventional fairness metrics,\nwhich focus on discrete accuracy-based evaluations (i.e., prediction\ncorrectness), fail to capture the implicit impact of model uncertainty (e.g.,\nhigher model confidence about one group over another despite similar accuracy).\nTo address this limitation, we propose an uncertainty-aware fairness metric,\nUCerF, to enable a fine-grained evaluation of model fairness that is more\nreflective of the internal bias in model decisions compared to conventional\nfairness measures. Furthermore, observing data size, diversity, and clarity\nissues in current datasets, we introduce a new gender-occupation fairness\nevaluation dataset with 31,756 samples for co-reference resolution, offering a\nmore diverse and suitable dataset for evaluating modern LLMs. We establish a\nbenchmark, using our metric and dataset, and apply it to evaluate the behavior\nof ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness\ndue to high confidence in incorrect predictions, a detail overlooked by\nEqualized Odds but captured by UCerF. Overall, our proposed LLM benchmark,\nwhich evaluates fairness with uncertainty awareness, paves the way for\ndeveloping more transparent and accountable AI systems.", "comment": "9 pages, 8 figures, and 1 table in main paper. Supplementary appendix\n  attached. Accepted at ICML 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.23996v1;http://arxiv.org/pdf/2505.23996v1", "pdf_url": "http://arxiv.org/pdf/2505.23996v1"}, {"title": "Aftina: enhancing stability and preventing hallucination in AI-based Islamic fatwa generation using LLMs and RAG", "link": "https://link.springer.com/article/10.1007/s00521-025-11229-y", "details": "MY Mohammed, SA Ali, SK Ali, AA Majeed\u2026 - Neural Computing and \u2026, 2025", "abstract": "\u2026 Additionally, we conducted an LLM-to- **LLM** **evaluation** and engaged a domain expert to qualitatively assess a sample of the dataset. \u2026 LLM-to- **LLM** **evaluation** To expand our evaluation, we applied an LLM-to- **LLM** **evaluation** using GPT-4o [45] \u2026"}, {"title": "DEEPQUESTION: Systematic Generation of Real-World Challenges for Evaluating LLMs Performance", "link": "https://arxiv.org/pdf/2505.24532", "details": "A Khoramfar, A Ramezani, MM Mohajeri, MJ Dousti\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "LLMs often excel on standard benchmarks but falter on real-world tasks. We introduce DeepQuestion, a scalable automated framework that augments existing datasets based on Bloom's taxonomy and creates novel questions that trace original \u2026", "entry_id": "http://arxiv.org/abs/2505.24532v1", "updated": "2025-05-30 12:39:42", "published": "2025-05-30 12:39:42", "authors": "Ali Khoramfar;Ali Ramezani;Mohammad Mahdi Mohajeri;Mohammad Javad Dousti;Majid Nili Ahmadabadi;Heshaam Faili", "summary": "LLMs often excel on standard benchmarks but falter on real-world tasks. We\nintroduce DeepQuestion, a scalable automated framework that augments existing\ndatasets based on Bloom's taxonomy and creates novel questions that trace\noriginal solution paths to probe evaluative and creative skills. Extensive\nexperiments across ten open-source and proprietary models, covering both\ngeneral-purpose and reasoning LLMs, reveal substantial performance drops (even\nup to 70% accuracy loss) on higher-order tasks, underscoring persistent gaps in\ndeep reasoning. Our work highlights the need for cognitively diverse benchmarks\nto advance LLM progress. DeepQuestion and related datasets will be released\nupon acceptance of the paper.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.24532v1;http://arxiv.org/pdf/2505.24532v1", "pdf_url": "http://arxiv.org/pdf/2505.24532v1"}, {"title": "System Prompt Extraction Attacks and Defenses in Large Language Models", "link": "https://arxiv.org/pdf/2505.23817", "details": "BC Das, MH Amini, Y Wu - arXiv preprint arXiv:2505.23817, 2025", "abstract": "\u2026 **LLM** **evaluation** [2], to measure the performance of the system prompt extraction attacks. Despite the success of existing studies, the extracted prompts obtained through these techniques often contain extraneous text and characters along with \u2026", "entry_id": "http://arxiv.org/abs/2505.23817v1", "updated": "2025-05-27 21:36:27", "published": "2025-05-27 21:36:27", "authors": "Badhan Chandra Das;M. Hadi Amini;Yanzhao Wu", "summary": "The system prompt in Large Language Models (LLMs) plays a pivotal role in\nguiding model behavior and response generation. Often containing private\nconfiguration details, user roles, and operational instructions, the system\nprompt has become an emerging attack target. Recent studies have shown that LLM\nsystem prompts are highly susceptible to extraction attacks through\nmeticulously designed queries, raising significant privacy and security\nconcerns. Despite the growing threat, there is a lack of systematic studies of\nsystem prompt extraction attacks and defenses. In this paper, we present a\ncomprehensive framework, SPE-LLM, to systematically evaluate System Prompt\nExtraction attacks and defenses in LLMs. First, we design a set of novel\nadversarial queries that effectively extract system prompts in state-of-the-art\n(SOTA) LLMs, demonstrating the severe risks of LLM system prompt extraction\nattacks. Second, we propose three defense techniques to mitigate system prompt\nextraction attacks in LLMs, providing practical solutions for secure LLM\ndeployments. Third, we introduce a set of rigorous evaluation metrics to\naccurately quantify the severity of system prompt extraction attacks in LLMs\nand conduct comprehensive experiments across multiple benchmark datasets, which\nvalidates the efficacy of our proposed SPE-LLM framework.", "comment": null, "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR", "links": "http://arxiv.org/abs/2505.23817v1;http://arxiv.org/pdf/2505.23817v1", "pdf_url": "http://arxiv.org/pdf/2505.23817v1"}]
