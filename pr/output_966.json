"*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Predicting Risk of Alzheimer's Diseases and Related De"
[{"title": "FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations", "link": "https://arxiv.org/pdf/2404.06619", "details": "J Dwivedi-Yu, R Dwivedi, T Schick - arXiv preprint arXiv:2404.06619, 2024", "abstract": "The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes \u2026"}, {"title": "PMC-LLaMA: toward building open-source language models for medicine", "link": "https://academic.oup.com/jamia/advance-article-abstract/doi/10.1093/jamia/ocae045/7645318", "details": "C Wu, W Lin, X Zhang, Y Zhang, W Xie, Y Wang - Journal of the American Medical \u2026, 2024", "abstract": "Objective Recently, large language models (LLMs) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering (QA) situations, these models \u2026"}, {"title": "Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data", "link": "https://arxiv.org/pdf/2404.03862", "details": "J Zhang, M Marone, T Li, B Van Durme, D Khashabi - arXiv preprint arXiv:2404.03862, 2024", "abstract": "For humans to trust the fluent generations of large language models (LLMs), they must be able to verify their correctness against trusted, external sources. Recent efforts aim to increase verifiability through citations of retrieved documents or post \u2026"}, {"title": "FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS By admin No Comments", "link": "https://youraisales.com/finetuned-language-models-are-zero-shot-learners-2/", "details": "J Wei, M Bosma, VY Zhao, K Guu, AW Yu, B Lester\u2026", "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning\u2014\ufb01netuning language models on a collection of datasets described via instructions\u2014substantially improves zero-shot \u2026"}, {"title": "CodecLM: Aligning Language Models with Tailored Synthetic Data", "link": "https://arxiv.org/pdf/2404.05875", "details": "Z Wang, CL Li, V Perot, LT Le, J Miao, Z Zhang, CY Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Instruction tuning has emerged as the key in aligning large language models (LLMs) with specific task instructions, thereby mitigating the discrepancy between the next- token prediction objective and users' actual goals. To reduce the labor and time cost \u2026"}, {"title": "Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models", "link": "https://arxiv.org/pdf/2404.02823", "details": "H Sun, L Liu, J Li, F Wang, B Dong, R Lin, R Huang - arXiv preprint arXiv:2404.02823, 2024", "abstract": "The ability of large language models (LLMs) to follow instructions is crucial to real- world applications. Despite recent advances, several studies have highlighted that LLMs struggle when faced with challenging instructions, especially those that include \u2026"}, {"title": "ExcluIR: Exclusionary Neural Information Retrieval", "link": "https://arxiv.org/pdf/2404.17288", "details": "W Zhang, M Zhang, S Wu, J Pei, Z Ren, M de Rijke\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Exclusion is an important and universal linguistic skill that humans use to express what they do not want. However, in information retrieval community, there is little research on exclusionary retrieval, where users express what they do not want in \u2026"}, {"title": "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models", "link": "https://arxiv.org/pdf/2404.02575", "details": "H Chae, Y Kim, S Kim, KT Ong, B Kwak, M Kim, S Kim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution. Such nature of algorithmic reasoning makes it a challenge for large \u2026"}, {"title": "Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think", "link": "https://arxiv.org/pdf/2404.08382", "details": "X Wang, C Hu, B Ma, P R\u00f6ttger, B Plank - arXiv preprint arXiv:2404.08382, 2024", "abstract": "Multiple choice questions (MCQs) are commonly used to evaluate the capabilities of large language models (LLMs). One common way to evaluate the model response is to rank the candidate answers based on the log probability of the first token \u2026"}]
