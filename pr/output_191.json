'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Hyperdimensional Computing with Holographic and Adaptive Enc'
[{"title": "MSLM-S2ST: A Multitask Speech Language Model for Textless Speech-to-Speech Translation with Speaker Style Preservation", "link": "https://arxiv.org/pdf/2403.12408", "details": "Y Peng, I Kulikov, Y Yang, S Popuri, H Lu, C Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "There have been emerging research interest and advances in speech-to-speech translation (S2ST), translating utterances from one language to another. This work proposes Multitask Speech Language Model (MSLM), which is a decoder-only \u2026"}, {"title": "An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis", "link": "https://arxiv.org/pdf/2403.12402", "details": "Y Peng, I Kulikov, Y Yang, S Popuri, H Lu, C Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Speech language models (LMs) are promising for high-quality speech synthesis through in-context learning. A typical speech LM takes discrete semantic units as content and a short utterance as prompt, and synthesizes speech which preserves \u2026"}, {"title": "TRELM: Towards Robust and Efficient Pre-training for Knowledge-Enhanced Language Models", "link": "https://arxiv.org/html/2403.11203v1", "details": "J Yan, C Wang, T Zhang, X He, J Huang, L Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "KEPLMs are pre-trained models that utilize external knowledge to enhance language understanding. Previous language models facilitated knowledge acquisition by incorporating knowledge-related pre-training tasks learned from \u2026"}, {"title": "Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models", "link": "https://arxiv.org/pdf/2403.11838", "details": "Y Luo, Z Lin, Y Zhang, J Sun, C Lin, C Xu, X Su, Y Shen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from \u2026"}, {"title": "Few-Shot Image Classification and Segmentation as Visual Question Answering Using Vision-Language Models", "link": "https://arxiv.org/html/2403.10287v1", "details": "T Meng, Y Tao, R Lyu, W Yin - arXiv preprint arXiv:2403.10287, 2024", "abstract": "The task of few-shot image classification and segmentation (FS-CS) involves classifying and segmenting target objects in a query image, given only a few examples of the target classes. We introduce the Vision-Instructed Segmentation and \u2026"}, {"title": "Concept-aware Data Construction Improves In-context Learning of Language Models", "link": "https://arxiv.org/pdf/2403.09703", "details": "M \u0160tef\u00e1nik, M Kadl\u010d\u00edk, P Sojka - arXiv preprint arXiv:2403.09703, 2024", "abstract": "Many recent language models (LMs) are capable of in-context learning (ICL), manifested in the LMs' ability to perform a new task solely from natural-language instruction. Previous work curating in-context learners assumes that ICL emerges \u2026"}]
