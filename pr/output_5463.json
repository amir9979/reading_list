[{"title": "Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models", "link": "https://arxiv.org/pdf/2407.20271", "details": "H Tang, Y Liu, X Liu, K Zhang, Y Zhang, Q Liu, E Chen - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in machine learning, especially in Natural Language Processing (NLP), have led to the development of sophisticated models trained on vast datasets, but this progress has raised concerns about potential sensitive \u2026"}, {"title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT", "link": "https://arxiv.org/pdf/2407.21058", "details": "M Ali, S Panda, Q Shen, M Wick, A Kobren - arXiv preprint arXiv:2407.21058, 2024", "abstract": "In the current landscape of language model research, larger models, larger datasets and more compute seems to be the only way to advance towards intelligence. While there have been extensive studies of scaling laws and models' scaling behaviors, the \u2026"}, {"title": "Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?", "link": "https://arxiv.org/pdf/2407.17417", "details": "MA Panaitescu-Liess, Z Che, B An, Y Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in generating diverse and contextually rich text. However, concerns regarding copyright infringement arise as LLMs may inadvertently produce copyrighted material. In this \u2026"}, {"title": "Imposter. AI: Adversarial Attacks with Hidden Intentions towards Aligned Large Language Models", "link": "https://arxiv.org/pdf/2407.15399", "details": "X Liu, L Li, T Xiang, F Ye, L Wei, W Li, N Garcia - arXiv preprint arXiv:2407.15399, 2024", "abstract": "With the development of large language models (LLMs) like ChatGPT, both their vast applications and potential vulnerabilities have come to the forefront. While developers have integrated multiple safety mechanisms to mitigate their misuse, a \u2026"}, {"title": "Reinforced Prompt Personalization for Recommendation with Large Language Models", "link": "https://arxiv.org/pdf/2407.17115", "details": "W Mao, J Wu, W Chen, C Gao, X Wang, X He - arXiv preprint arXiv:2407.17115, 2024", "abstract": "Designing effective prompts can empower LLMs to understand user preferences and provide recommendations by leveraging LLMs' intent comprehension and knowledge utilization capabilities. However, existing research predominantly \u2026"}, {"title": "Can Language Models Safeguard Themselves, Instantly and For Free?", "link": "https://openreview.net/pdf%3Fid%3DALRWSxT1rl", "details": "D Adila, C Shin, Y Zhang, F Sala - ICML 2024 Next Generation of AI Safety Workshop", "abstract": "Aligning pretrained language models (LMs) to handle a new safety scenario is normally difficult and expensive, often requiring access to large amounts of ground- truth preference data and substantial compute. Are these costs necessary? That is, is \u2026"}, {"title": "Large Language Models Can Learn Representation in Natural Language", "link": "https://aclanthology.org/2024.findings-acl.542.pdf", "details": "Y Guo, Y Liang, D Zhao, N Duan - Findings of the Association for Computational \u2026, 2024", "abstract": "One major challenge for Large Language Models (LLMs) is completing complex tasks involving multiple entities, such as tool APIs. To tackle this, one approach is to retrieve relevant entities to enhance LLMs in task completion. A crucial issue here is \u2026"}]
