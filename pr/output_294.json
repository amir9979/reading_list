'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Adaptive Prompt Routing for Arbitrary Text Style Trans'
[{"title": "Distilling Named Entity Recognition Models for Endangered Species from Large Language Models", "link": "https://arxiv.org/pdf/2403.15430", "details": "J Atuhurra, SC Dujohn, H Kamigaito, H Shindo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Natural language processing (NLP) practitioners are leveraging large language models (LLM) to create structured datasets from semi-structured and unstructured data sources such as patents, papers, and theses, without having domain-specific \u2026"}, {"title": "PMRC: Prompt-Based Machine Reading Comprehension for Few-Shot Named Entity Recognition", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/29791/31368", "details": "J Huang, D Yan, Y Cai - Proceedings of the AAAI Conference on Artificial \u2026, 2024", "abstract": "The prompt-based method has been proven effective in improving the performance of pre-trained language models (PLMs) on sentence-level few-shot tasks. However, when applying prompting to token-level tasks such as Named Entity Recognition \u2026"}, {"title": "Few-Shot Adversarial Prompt Learning on Vision-Language Models", "link": "https://arxiv.org/html/2403.14774v1", "details": "Y Zhou, X Xia, Z Lin, B Han, T Liu - arXiv preprint arXiv:2403.14774, 2024", "abstract": "The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by \u2026"}, {"title": "FairBelief-Assessing Harmful Beliefs in Language Models", "link": "https://arxiv.org/pdf/2402.17389", "details": "M Setzu, MM Manerba, P Minervini, D Nozza - arXiv preprint arXiv:2402.17389, 2024", "abstract": "Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real- world applications without careful fairness auditing. This paper proposes FairBelief \u2026"}, {"title": "Few-Shot Relation Extraction With Automatically Generated Prompts", "link": "https://ieeexplore.ieee.org/abstract/document/10476632/", "details": "X Zhao, M Yang, Q Qu, R Xu - IEEE Transactions on Neural Networks and Learning \u2026, 2024", "abstract": "Relation extraction (RE) tends to struggle when the supervised training data is few and difficult to be collected. In this article, we elicit relational and factual knowledge from large pretrained language models (PLMs) for few-shot RE (FSRE) with \u2026"}, {"title": "Understanding Emergent Abilities of Language Models from the Loss Perspective", "link": "https://arxiv.org/pdf/2403.15796", "details": "Z Du, A Zeng, Y Dong, J Tang - arXiv preprint arXiv:2403.15796, 2024", "abstract": "Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) \u2026"}, {"title": "One prompt word is enough to boost adversarial robustness for pre-trained vision-language models", "link": "https://arxiv.org/html/2403.01849v1", "details": "L Li, H Guan, J Qiu, M Spratling - arXiv preprint arXiv:2403.01849, 2024", "abstract": "Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of \u2026"}, {"title": "ALaRM: Align Language Models via Hierarchical Rewards Modeling", "link": "https://arxiv.org/html/2403.06754v1", "details": "Y Lai, S Wang, S Liu, X Huang, Z Wei - arXiv preprint arXiv:2403.06754, 2024", "abstract": "We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The \u2026"}, {"title": "Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning", "link": "https://arxiv.org/html/2403.16543v1", "details": "P Borchert, J De Weerdt, MF Moens - arXiv preprint arXiv:2403.16543, 2024", "abstract": "Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in few-shot relation classification. Representations of textual data extract rich information spanning the domain, entities, and relations. In this \u2026"}]
