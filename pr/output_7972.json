[{"title": "Alphaedit: Null-space constrained knowledge editing for language models", "link": "https://arxiv.org/pdf/2410.02355%3F", "details": "J Fang, H Jiang, K Wang, Y Ma, X Wang, X He, T Chua - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) often exhibit hallucinations due to incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating \u2026"}, {"title": "Unraveling cross-modality knowledge conflict in large vision-language models", "link": "https://arxiv.org/pdf/2410.03659", "details": "T Zhu, Q Liu, F Wang, Z Tu, M Chen - arXiv preprint arXiv:2410.03659, 2024", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities for capturing and reasoning over multimodal inputs. However, these models are prone to parametric knowledge conflicts, which arise from inconsistencies of \u2026"}, {"title": "DEPT: Decoupled Embeddings for Pre-training Language Models", "link": "https://arxiv.org/pdf/2410.05021", "details": "A Iacob, L Sani, M Kurmanji, WF Shen, X Qiu, D Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language Model pre-training benefits from a broader data mixture to enhance performance across domains and languages. However, training on such heterogeneous text corpora is complex, requiring extensive and cost-intensive \u2026"}, {"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "link": "https://arxiv.org/pdf/2410.05639", "details": "R Zhao, ZL Thai, Y Zhang, S Hu, Y Ba, J Zhou, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the \u2026"}, {"title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks", "link": "https://arxiv.org/pdf/2410.05160%3F", "details": "Z Jiang, R Meng, X Yang, S Yavuz, Y Zhou, W Chen - arXiv preprint arXiv:2410.05160, 2024", "abstract": "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize \u2026"}, {"title": "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "link": "https://arxiv.org/pdf/2410.05077", "details": "FM Molfese, S Conia, R Orlando, R Navigli - arXiv preprint arXiv:2410.05077, 2024", "abstract": "Current Large Language Models (LLMs) have shown strong reasoning capabilities in commonsense question answering benchmarks, but the process underlying their success remains largely opaque. As a consequence, recent approaches have \u2026"}, {"title": "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes", "link": "https://arxiv.org/pdf/2410.05052", "details": "K Nishida, K Nishida, K Saito - arXiv preprint arXiv:2410.05052, 2024", "abstract": "Loss spikes, a phenomenon in which the loss value diverges suddenly, is a fundamental issue in the pre-training of large language models. This paper supposes that the non-uniformity of the norm of the parameters is one of the causes \u2026"}, {"title": "Heterogeneous graph contrastive learning with adaptive data augmentation for semi\u2010supervised short text classification", "link": "https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.13744", "details": "M Wu, Z Xu, L Zheng - Expert Systems", "abstract": "Short text classification has been widely used in many fields. Due to the scarcity of labelled data, implementing short text classification under semi\u2010supervised learning setting has become increasingly popular. Semi\u2010supervised short text classification \u2026"}, {"title": "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "link": "https://asgordon.github.io/publications/EMNLP2024.PDF", "details": "M Roemmele, AS Gordon", "abstract": "LLMs can now perform a variety of complex writing tasks. They also excel in answering questions pertaining to natural language inference and commonsense reasoning. Composing these questions is itself a skilled writing task, so in this paper \u2026"}]
