[{"title": "Mental Modeling of Reinforcement Learning Agents by Language Models", "link": "https://arxiv.org/pdf/2406.18505", "details": "W Lu, X Zhao, J Spisak, JH Lee, S Wermter - arXiv preprint arXiv:2406.18505, 2024", "abstract": "Can emergent language models faithfully model the intelligence of decision-making agents? Though modern language models exhibit already some reasoning ability, and theoretically can potentially express any probable distribution over tokens, it \u2026"}, {"title": "Contrastive Learning with Counterfactual Explanations for Radiology Report Generation", "link": "https://arxiv.org/pdf/2407.14474", "details": "M Li, H Lin, L Qiu, X Liang, L Chen, A Elsaddik\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Due to the common content of anatomy, radiology images with their corresponding reports exhibit high similarity. Such inherent data bias can predispose automatic report generation models to learn entangled and spurious representations resulting \u2026"}, {"title": "Unsupervised Latent Stain Adaption for Digital Pathology", "link": "https://arxiv.org/pdf/2406.19081", "details": "D Reisenb\u00fcchler, L Luttner, NS Schaadt, F Feuerhake\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In digital pathology, deep learning (DL) models for tasks such as segmentation or tissue classification are known to suffer from domain shifts due to different staining techniques. Stain adaptation aims to reduce the generalization error between \u2026"}, {"title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings", "link": "https://arxiv.org/pdf/2406.16611", "details": "A Posada, D Rueckert, F Meissen, P M\u00fcller - arXiv preprint arXiv:2406.16611, 2024", "abstract": "Since the emergence of the Transformer architecture, language model development has increased, driven by their promising potential. However, releasing these models into production requires properly understanding their behavior, particularly in \u2026"}, {"title": "The Art of Saying No: Contextual Noncompliance in Language Models", "link": "https://arxiv.org/pdf/2407.12043", "details": "F Brahman, S Kumar, V Balachandran, P Dasigi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Chat-based language models are designed to be helpful, yet they should not comply with every user request. While most existing work primarily focuses on refusal of\" unsafe\" queries, we posit that the scope of noncompliance should be broadened. We \u2026"}, {"title": "IHCSurv: Effective Immunohistochemistry Priors for Cancer Survival Analysis in Gigapixel Multi-stain Whole Slide Images", "link": "https://www.cs.jhu.edu/~lelu/publication/MICCAI2024-0495.pdf", "details": "Y Zhang, H Chao, Z Qiu, W Liu, Y Shen, N Sapkota\u2026", "abstract": "Recent cancer survival prediction approaches have made great strides in analyzing H&E-stained gigapixel whole-slide images. However, methods targeting the immunohistochemistry (IHC) modality remain largely unexplored. We remedy this \u2026"}, {"title": "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference", "link": "https://arxiv.org/pdf/2406.17626", "details": "E Yu, J Li, M Liao, S Wang, Z Gao, F Mi, L Hong - arXiv preprint arXiv:2406.17626, 2024", "abstract": "As large language models (LLMs) constantly evolve, ensuring their safety remains a critical research problem. Previous red-teaming approaches for LLM safety have primarily focused on single prompt attacks or goal hijacking. To the best of our \u2026"}]
