[{"title": "Investigating Layer Importance in Large Language Models", "link": "https://arxiv.org/pdf/2409.14381", "details": "Y Zhang, Y Dong, K Kawaguchi - arXiv preprint arXiv:2409.14381, 2024", "abstract": "Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in \u2026"}, {"title": "InfoDisent: Explainability of Image Classification Models by Information Disentanglement", "link": "https://arxiv.org/pdf/2409.10329", "details": "\u0141 Struski, J Tabor - arXiv preprint arXiv:2409.10329, 2024", "abstract": "Understanding the decisions made by image classification networks is a critical area of research in deep learning. This task is traditionally divided into two distinct approaches: post-hoc methods and intrinsic methods. Post-hoc methods, such as \u2026"}, {"title": "Self-Evolutionary Large Language Models through Uncertainty-Enhanced Preference Optimization", "link": "https://arxiv.org/pdf/2409.11212", "details": "J Wang, Y Zhou, X Zhang, M Bao, P Yan - arXiv preprint arXiv:2409.11212, 2024", "abstract": "Iterative preference optimization has recently become one of the de-facto training paradigms for large language models (LLMs), but the performance is still underwhelming due to too much noisy preference data yielded in the loop. To \u2026"}, {"title": "Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style", "link": "https://arxiv.org/pdf/2409.10955", "details": "Y Li, K Zhou, Q Qiao, B Nguyen, Q Wang, Q Li - arXiv preprint arXiv:2409.10955, 2024", "abstract": "Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by incorporating external information into the response generation process. However, how context-faithful LLMs are and what factors influence LLMs' context \u2026"}, {"title": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models", "link": "https://arxiv.org/pdf/2409.11353", "details": "M Liang, A Arun, Z Wu, C Munoz, J Lutch, E Kazim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs). Existing detection and mitigation methods are often isolated and insufficient for domain-specific needs, lacking a standardized pipeline \u2026"}]
