[{"title": "Towards Latent Masked Image Modeling for Self-Supervised Visual Representation Learning", "link": "https://arxiv.org/pdf/2407.15837", "details": "Y Wei, A Gupta, P Morgado - arXiv preprint arXiv:2407.15837, 2024", "abstract": "Masked Image Modeling (MIM) has emerged as a promising method for deriving visual representations from unlabeled image data by predicting missing pixels from masked portions of images. It excels in region-aware learning and provides strong \u2026"}, {"title": "Geometry-Aware Generative Autoencoders for Metric Learning and Generative Modeling on Data Manifolds", "link": "https://openreview.net/pdf%3Fid%3DEYQZjMcn4l", "details": "X Sun, D Liao, K MacDonald, Y Zhang, G Huguet\u2026 - ICML 2024 Workshop on \u2026", "abstract": "Non-linear dimensionality reduction methods have proven successful at learning low- dimensional representations of high-dimensional point clouds on or near data manifolds. However, existing methods are not easily extensible\u2014that is, for large \u2026"}, {"title": "CADS: A Self-supervised Learner via Cross-modal Alignment and Deep Self-distillation for CT Volume Segmentation", "link": "https://ieeexplore.ieee.org/abstract/document/10605840/", "details": "Y Ye, J Zhang, Z Chen, Y Xia - IEEE Transactions on Medical Imaging, 2024", "abstract": "Self-supervised learning (SSL) has long had great success in advancing the field of annotation-efficient learning. However, when applied to CT volume segmentation, most SSL methods suffer from two limitations, including rarely using the information \u2026"}, {"title": "On Learning Discriminative Features from Synthesized Data for Self-Supervised Fine-Grained Visual Recognition", "link": "https://arxiv.org/pdf/2407.14676", "details": "Z Wang, L Liu, SRF Weston, S Tian, P Li - arXiv preprint arXiv:2407.14676, 2024", "abstract": "Self-Supervised Learning (SSL) has become a prominent approach for acquiring visual representations across various tasks, yet its application in fine-grained visual recognition (FGVR) is challenged by the intricate task of distinguishing subtle \u2026"}, {"title": "Ensembled Cold-Diffusion Restorations for Unsupervised Anomaly Detection", "link": "https://arxiv.org/pdf/2407.06635", "details": "SN Marimont, V Siomos, M Baugh, C Tzelepis, B Kainz\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Unsupervised Anomaly Detection (UAD) methods aim to identify anomalies in test samples comparing them with a normative distribution learned from a dataset known to be anomaly-free. Approaches based on generative models offer interpretability by \u2026"}, {"title": "Improving Interpretability and Accuracy of Artificial Intelligence in Natural Language and Image Understanding", "link": "https://etd.auburn.edu/bitstream/handle/10415/9330/PhD_Dissertation_ThangPham.pdf%3Fsequence%3D4%26isAllowed%3Dy", "details": "T Pham - 2024", "abstract": "Transformer-based models, such as BERT, have revolutionized natural language processing (NLP) by setting new standards for accuracy and capability. BERT has achieved state-of-the-art (SOTA) results on many NLP tasks and benchmarks, such \u2026"}]
