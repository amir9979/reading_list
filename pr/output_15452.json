[{"title": "Towards Streaming Land Use Classification of Images with Temporal Distribution Shifts", "link": "https://www.esann.org/sites/default/files/proceedings/2025/ES2025-166.pdf", "details": "L Iovine, G Ziffer, A Proia, E Della Valle - ESANN Proceedings, 2025", "abstract": "In this study, we introduce a new pipeline that integrates Streaming Machine Learning (SML) models and the Momentum Contrastive Learning (MoCo) technique for the streaming classification of satellite images subject to temporal variations in \u2026"}, {"title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis", "link": "https://arxiv.org/pdf/2503.23145", "details": "A Wei, T Suresh, J Cao, N Kannan, Y Wu, K Yan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by \u2026"}, {"title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning", "link": "https://arxiv.org/pdf/2504.01005", "details": "N Singhi, H Bansal, A Hosseini, A Grover, KW Chang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC) \u2026"}, {"title": "2-D Transformer: Extending Large Language Models to Long-Context With Few Memory", "link": "https://ieeexplore.ieee.org/abstract/document/10937248/", "details": "X He, J Liu, Y Duan - IEEE Transactions on Neural Networks and Learning \u2026, 2025", "abstract": "The ability of processing long contexts is crucial for large language models (LLMs), but training LLMs with a long-context window requires substantial computational resources. Many sought to mitigate this through the sparse attention mechanism \u2026"}, {"title": "Every Sample Matters: Leveraging Mixture-of-Experts and High-Quality Data for Efficient and Accurate Code LLM", "link": "https://arxiv.org/pdf/2503.17793", "details": "L Team, W Cai, Y Cao, C Chen, C Chen, S Chen, Q Cui\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in code large language models (LLMs) have demonstrated remarkable capabilities in code generation and understanding. It is still challenging to build a code LLM with comprehensive performance yet ultimate efficiency. Many \u2026"}, {"title": "Zero-shot evaluation reveals limitations of single-cell foundation models", "link": "https://genomebiology.biomedcentral.com/articles/10.1186/s13059-025-03574-x", "details": "KZ Kedzierska, L Crawford, AP Amini, AX Lu - Genome Biology, 2025", "abstract": "Foundation models such as scGPT and Geneformer have not been rigorously evaluated in a setting where they are used without any further training (ie, zero-shot). Understanding the performance of models in zero-shot settings is critical to \u2026"}, {"title": "Unlocking efficient long-to-short llm reasoning with model merging", "link": "https://arxiv.org/pdf/2503.20641%3F", "details": "H Wu, Y Yao, S Liu, Z Liu, X Fu, X Han, X Li, HL Zhen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of \u2026"}, {"title": "A Survey on Mixture of Experts in Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10937907/", "details": "W Cai, J Jiang, F Wang, J Tang, S Kim, J Huang - IEEE Transactions on Knowledge \u2026, 2025", "abstract": "Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size \u2026"}, {"title": "Boosting Large Language Models with Mask Fine-Tuning", "link": "https://arxiv.org/pdf/2503.22764", "details": "M Zhang, Y Bai, H Wang, Y Wang, Q Dong, Y Fu - arXiv preprint arXiv:2503.22764, 2025", "abstract": "The model is usually kept integral in the mainstream large language model (LLM) fine-tuning protocols. No works have questioned whether maintaining the integrity of the model is indispensable for performance. In this work, we introduce Mask Fine \u2026"}]
