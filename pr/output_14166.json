[{"title": "QA-Calibration of language model confidence scores", "link": "https://www.amazon.science/publications/qa-calibration-of-language-model-confidence-scores", "details": "A Mastakouri, E Kirschbaum, S Kasiviswanathan\u2026 - 2025", "abstract": "To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim \u2026"}, {"title": "R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization", "link": "https://arxiv.org/pdf/2503.12937", "details": "J Zhang, J Huang, H Yao, S Liu, X Zhang, S Lu, D Tao - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised fine- tuning on high-quality chain-of-thought reasoning data, which often leads models to merely imitate successful reasoning paths without understanding what the wrong \u2026"}, {"title": "DAST: Difficulty-Aware Self-Training on Large Language Models", "link": "https://arxiv.org/pdf/2503.09029", "details": "B Xue, Q Zhu, H Wang, R Wang, S Wang, H Xu, F Mi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Present Large Language Models (LLM) self-training methods always under-sample on challenging queries, leading to inadequate learning on difficult problems which limits LLMs' ability. Therefore, this work proposes a difficulty-aware self-training \u2026"}, {"title": "Towards reasoning era: A survey of long chain-of-thought for reasoning large language models", "link": "https://arxiv.org/pdf/2503.09567", "details": "Q Chen, L Qin, J Liu, D Peng, J Guan, P Wang, M Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains like mathematics and coding. A central factor in their success lies \u2026"}, {"title": "Exploring the Role of CLIP Global Visual Features in Multimodal Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10889200/", "details": "Z Bai, Y Bai - ICASSP 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "The next recognized development direction of large language models (LLMs) is to integrate and enhance multimodal capability. Although current multimodal large language models (MLLMs) have achieved impressive performance by combining the \u2026"}, {"title": "XIFBench: Evaluating Large Language Models on Multilingual Instruction Following", "link": "https://arxiv.org/pdf/2503.07539%3F", "details": "Z Li, K Chen, Y Long, X Bai, Y Zhang, X Wei, J Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have demonstrated remarkable instruction-following capabilities across various applications. However, their performance in multilingual settings remains poorly understood, as existing evaluations lack fine-grained \u2026"}, {"title": "Knowledge Graph Question Answering and Large Language Models", "link": "https://ebooks.iospress.nl/doi/10.3233/FAIA250220", "details": "D Banerjee, N Hu, Y Tan, D Min, Y Wu, R Usbeck, G Qi - Handbook on Neurosymbolic \u2026, 2025", "abstract": "Abstract Knowledge Graph Question Answering (KGQA) is an evolving field that aims to leverage structured knowledge graphs to provide precise answers to user queries. As Knowledge Graphs continue to expand in complexity and size, efficiently \u2026"}, {"title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models", "link": "https://arxiv.org/pdf/2503.07605", "details": "X Liang, H Wang, H Lai, S Niu, S Song, J Yang, J Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning \u2026"}, {"title": "SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression", "link": "https://arxiv.org/pdf/2503.12340", "details": "X Wang, S Alam, Z Wan, H Shen, M Zhang - arXiv preprint arXiv:2503.12340, 2025", "abstract": "Despite significant advancements, the practical deployment of Large Language Models (LLMs) is often hampered by their immense sizes, highlighting the need for effective compression techniques. Singular Value Decomposition (SVD) is a \u2026"}]
