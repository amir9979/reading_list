[{"title": "Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models", "link": "https://arxiv.org/pdf/2505.11326", "details": "KP Yu, J Chai - arXiv preprint arXiv:2505.11326, 2025", "abstract": "Vision-language models (VLMs) have shown remarkable progress in offline tasks such as image captioning and video question answering. However, real-time interactive environments impose new demands on VLMs, requiring them to generate \u2026", "entry_id": "http://arxiv.org/abs/2505.11326v1", "updated": "2025-05-16 14:48:30", "published": "2025-05-16 14:48:30", "authors": "Keunwoo Peter Yu;Joyce Chai", "summary": "Vision-language models (VLMs) have shown remarkable progress in offline tasks\nsuch as image captioning and video question answering. However, real-time\ninteractive environments impose new demands on VLMs, requiring them to generate\nutterances that are not only semantically accurate but also precisely timed. We\nidentify two core capabilities necessary for such settings --\n$\\textit{perceptual updating}$ and $\\textit{contingency awareness}$ -- and\npropose a new benchmark task, $\\textbf{Temporally-Grounded Language Generation\n(TGLG)}$, to evaluate them. TGLG requires models to generate utterances in\nresponse to streaming video such that both content and timing align with\ndynamic visual input. To support this benchmark, we curate evaluation datasets\nfrom sports broadcasting and egocentric human interaction domains, and\nintroduce a new metric, $\\textbf{TRACE}$, to evaluate TGLG by jointly measuring\nsemantic similarity and temporal alignment. Finally, we present\n$\\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$,\na model that interleaves visual and linguistic tokens in a time-synchronized\nmanner, enabling real-time language generation without relying on turn-based\nassumptions. Experimental results show that VLM-TSI significantly outperforms a\nstrong baseline, yet overall performance remains modest -- highlighting the\ndifficulty of TGLG and motivating further research in real-time VLMs. Code and\ndata available $\\href{https://github.com/yukw777/tglg}{here}$.", "comment": "18 pages", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.11326v1;http://arxiv.org/pdf/2505.11326v1", "pdf_url": "http://arxiv.org/pdf/2505.11326v1"}, {"title": "Measuring Sycophancy of Language Models in Multi-turn Dialogues", "link": "https://arxiv.org/pdf/2505.23840", "details": "J Hong, G Byun, S Kim, K Shu - arXiv preprint arXiv:2505.23840, 2025", "abstract": "Large Language Models (LLMs) are expected to provide helpful and harmless responses, yet they often exhibit sycophancy--conforming to user beliefs regardless of factual accuracy or ethical soundness. Prior research on sycophancy has primarily \u2026", "entry_id": "http://arxiv.org/abs/2505.23840v1", "updated": "2025-05-28 14:05:46", "published": "2025-05-28 14:05:46", "authors": "Jiseung Hong;Grace Byun;Seungone Kim;Kai Shu", "summary": "Large Language Models (LLMs) are expected to provide helpful and harmless\nresponses, yet they often exhibit sycophancy--conforming to user beliefs\nregardless of factual accuracy or ethical soundness. Prior research on\nsycophancy has primarily focused on single-turn factual correctness,\noverlooking the dynamics of real-world interactions. In this work, we introduce\nSYCON Bench, a novel benchmark for evaluating sycophantic behavior in\nmulti-turn, free-form conversational settings. Our benchmark measures how\nquickly a model conforms to the user (Turn of Flip) and how frequently it\nshifts its stance under sustained user pressure (Number of Flip). Applying\nSYCON Bench to 17 LLMs across three real-world scenarios, we find that\nsycophancy remains a prevalent failure mode. Our analysis shows that alignment\ntuning amplifies sycophantic behavior, whereas model scaling and reasoning\noptimization strengthen the model's ability to resist undesirable user views.\nReasoning models generally outperform instruction-tuned models but often fail\nwhen they over-index on logical exposition instead of directly addressing the\nuser's underlying beliefs. Finally, we evaluate four additional prompting\nstrategies and demonstrate that adopting a third-person perspective reduces\nsycophancy by up to 63.8% in debate scenario. We release our code and data at\nhttps://github.com/JiseungHong/SYCON-Bench.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.23840v1;http://arxiv.org/pdf/2505.23840v1", "pdf_url": "http://arxiv.org/pdf/2505.23840v1"}, {"title": "GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models", "link": "https://arxiv.org/pdf/2505.24340", "details": "GQ Hacheme, GA Tadesse, C Robinson, A Zaytar\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Classifying geospatial imagery remains a major bottleneck for applications such as disaster response and land-use monitoring-particularly in regions where annotated data is scarce or unavailable. Existing tools (eg, RS-CLIP) that claim zero-shot \u2026", "entry_id": "http://arxiv.org/abs/2505.24340v1", "updated": "2025-05-30 08:32:37", "published": "2025-05-30 08:32:37", "authors": "Gilles Quentin Hacheme;Girmaw Abebe Tadesse;Caleb Robinson;Akram Zaytar;Rahul Dodhia;Juan M. Lavista Ferres", "summary": "Classifying geospatial imagery remains a major bottleneck for applications\nsuch as disaster response and land-use monitoring-particularly in regions where\nannotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that\nclaim zero-shot classification capabilities for satellite imagery nonetheless\nrely on task-specific pretraining and adaptation to reach competitive\nperformance. We introduce GeoVision Labeler (GVL), a strictly zero-shot\nclassification framework: a vision Large Language Model (vLLM) generates rich,\nhuman-readable image descriptions, which are then mapped to user-defined\nclasses by a conventional Large Language Model (LLM). This modular, and\ninterpretable pipeline enables flexible image classification for a large range\nof use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced,\nand RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary\nBuildings vs. No Buildings task on SpaceNet v7. For complex multi-class\nclassification tasks (UC Merced, RESISC45), we implemented a recursive\nLLM-driven clustering to form meta-classes at successive depths, followed by\nhierarchical classification-first resolving coarse groups, then finer\ndistinctions-to deliver competitive zero-shot performance. GVL is open-sourced\nat https://github.com/microsoft/geo-vision-labeler to catalyze adoption in\nreal-world geospatial workflows.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.CL;cs.LG;I.2.10; I.2.7; I.4.8; I.5.3", "links": "http://arxiv.org/abs/2505.24340v1;http://arxiv.org/pdf/2505.24340v1", "pdf_url": "http://arxiv.org/pdf/2505.24340v1"}, {"title": "Evaluating Fairness and Bias in Large Language Models for Tabular Data", "link": "https://link.springer.com/chapter/10.1007/978-3-031-93415-5_7", "details": "A Tayebi, OO Garibay - International Conference on Human-Computer \u2026, 2025", "abstract": "Abstract Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. Their adaptability through in-context learning and fine-tuning has enabled applications in \u2026"}, {"title": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks", "link": "https://arxiv.org/pdf/2505.23802", "details": "S Bedi, H Cui, M Fuentes, A Unell, M Wornow\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While large language models (LLMs) achieve near-perfect scores on medical licensing exams, these evaluations inadequately reflect the complexity and diversity of real-world clinical practice. We introduce MedHELM, an extensible evaluation \u2026", "entry_id": "http://arxiv.org/abs/2505.23802v2", "updated": "2025-06-02 04:19:10", "published": "2025-05-26 22:55:49", "authors": "Suhana Bedi;Hejie Cui;Miguel Fuentes;Alyssa Unell;Michael Wornow;Juan M. Banda;Nikesh Kotecha;Timothy Keyes;Yifan Mai;Mert Oez;Hao Qiu;Shrey Jain;Leonardo Schettini;Mehr Kashyap;Jason Alan Fries;Akshay Swaminathan;Philip Chung;Fateme Nateghi;Asad Aali;Ashwin Nayak;Shivam Vedak;Sneha S. Jain;Birju Patel;Oluseyi Fayanju;Shreya Shah;Ethan Goh;Dong-han Yao;Brian Soetikno;Eduardo Reis;Sergios Gatidis;Vasu Divi;Robson Capasso;Rachna Saralkar;Chia-Chun Chiang;Jenelle Jindal;Tho Pham;Faraz Ghoddusi;Steven Lin;Albert S. Chiou;Christy Hong;Mohana Roy;Michael F. Gensheimer;Hinesh Patel;Kevin Schulman;Dev Dash;Danton Char;Lance Downing;Francois Grolleau;Kameron Black;Bethel Mieso;Aydin Zahedivash;Wen-wai Yim;Harshita Sharma;Tony Lee;Hannah Kirsch;Jennifer Lee;Nerissa Ambers;Carlene Lugtu;Aditya Sharma;Bilal Mawji;Alex Alekseyev;Vicky Zhou;Vikas Kakkar;Jarrod Helzer;Anurang Revri;Yair Bannett;Roxana Daneshjou;Jonathan Chen;Emily Alsentzer;Keith Morse;Nirmal Ravi;Nima Aghaeepour;Vanessa Kennedy;Akshay Chaudhari;Thomas Wang;Sanmi Koyejo;Matthew P. Lungren;Eric Horvitz;Percy Liang;Mike Pfeffer;Nigam H. Shah", "summary": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.23802v2;http://arxiv.org/pdf/2505.23802v2", "pdf_url": "http://arxiv.org/pdf/2505.23802v2"}, {"title": "System Prompt Extraction Attacks and Defenses in Large Language Models", "link": "https://arxiv.org/pdf/2505.23817", "details": "BC Das, MH Amini, Y Wu - arXiv preprint arXiv:2505.23817, 2025", "abstract": "The system prompt in Large Language Models (LLMs) plays a pivotal role in guiding model behavior and response generation. Often containing private configuration details, user roles, and operational instructions, the system prompt has become an \u2026", "entry_id": "http://arxiv.org/abs/2505.23817v1", "updated": "2025-05-27 21:36:27", "published": "2025-05-27 21:36:27", "authors": "Badhan Chandra Das;M. Hadi Amini;Yanzhao Wu", "summary": "The system prompt in Large Language Models (LLMs) plays a pivotal role in\nguiding model behavior and response generation. Often containing private\nconfiguration details, user roles, and operational instructions, the system\nprompt has become an emerging attack target. Recent studies have shown that LLM\nsystem prompts are highly susceptible to extraction attacks through\nmeticulously designed queries, raising significant privacy and security\nconcerns. Despite the growing threat, there is a lack of systematic studies of\nsystem prompt extraction attacks and defenses. In this paper, we present a\ncomprehensive framework, SPE-LLM, to systematically evaluate System Prompt\nExtraction attacks and defenses in LLMs. First, we design a set of novel\nadversarial queries that effectively extract system prompts in state-of-the-art\n(SOTA) LLMs, demonstrating the severe risks of LLM system prompt extraction\nattacks. Second, we propose three defense techniques to mitigate system prompt\nextraction attacks in LLMs, providing practical solutions for secure LLM\ndeployments. Third, we introduce a set of rigorous evaluation metrics to\naccurately quantify the severity of system prompt extraction attacks in LLMs\nand conduct comprehensive experiments across multiple benchmark datasets, which\nvalidates the efficacy of our proposed SPE-LLM framework.", "comment": null, "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR", "links": "http://arxiv.org/abs/2505.23817v1;http://arxiv.org/pdf/2505.23817v1", "pdf_url": "http://arxiv.org/pdf/2505.23817v1"}, {"title": "DenseLoRA: Dense Low-Rank Adaptation of Large Language Models", "link": "https://arxiv.org/pdf/2505.23808", "details": "L Mu, X Wang, L Ni, Y Li, Z Wu, P Jin, Y Zhang - arXiv preprint arXiv:2505.23808, 2025", "abstract": "Low-rank adaptation (LoRA) has been developed as an efficient approach for adapting large language models (LLMs) by fine-tuning two low-rank matrices, thereby reducing the number of trainable parameters. However, prior research \u2026", "entry_id": "http://arxiv.org/abs/2505.23808v1", "updated": "2025-05-27 08:19:07", "published": "2025-05-27 08:19:07", "authors": "Lin Mu;Xiaoyu Wang;Li Ni;Yang Li;Zhize Wu;Peiquan Jin;Yiwen Zhang", "summary": "Low-rank adaptation (LoRA) has been developed as an efficient approach for\nadapting large language models (LLMs) by fine-tuning two low-rank matrices,\nthereby reducing the number of trainable parameters. However, prior research\nindicates that many of the weights in these matrices are redundant, leading to\ninefficiencies in parameter utilization. To address this limitation, we\nintroduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances\nparameter efficiency while achieving superior performance compared to LoRA.\nDenseLoRA builds upon the concept of representation fine-tuning, incorporating\na single Encoder-Decoder to refine and compress hidden representations across\nall adaptation layers before applying adaptation. Instead of relying on two\nredundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense\nlow-rank matrix, improving parameter utilization and adaptation efficiency. We\nevaluate DenseLoRA on various benchmarks, showing that it achieves 83.8%\naccuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8%\naccuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we\nconduct extensive experiments to systematically assess the impact of\nDenseLoRA's components on overall model performance. Code is available at\nhttps://github.com/mulin-ahu/DenseLoRA.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.23808v1;http://arxiv.org/pdf/2505.23808v1", "pdf_url": "http://arxiv.org/pdf/2505.23808v1"}, {"title": "HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America", "link": "https://arxiv.org/pdf/2505.24712", "details": "G Ivetta, MJ Gomez, S Martinelli, P Palombini\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Most resources for evaluating social biases in Large Language Models are developed without co-design from the communities affected by these biases, and rarely involve participatory approaches. We introduce HESEIA, a dataset of 46,499 \u2026", "entry_id": "http://arxiv.org/abs/2505.24712v1", "updated": "2025-05-30 15:32:48", "published": "2025-05-30 15:32:48", "authors": "Guido Ivetta;Marcos J. Gomez;Sof\u00eda Martinelli;Pietro Palombini;M. Emilia Echeveste;Nair Carolina Mazzeo;Beatriz Busaniche;Luciana Benotti", "summary": "Most resources for evaluating social biases in Large Language Models are\ndeveloped without co-design from the communities affected by these biases, and\nrarely involve participatory approaches. We introduce HESEIA, a dataset of\n46,499 sentences created in a professional development course. The course\ninvolved 370 high-school teachers and 5,370 students from 189 Latin-American\nschools. Unlike existing benchmarks, HESEIA captures intersectional biases\nacross multiple demographic axes and school subjects. It reflects local\ncontexts through the lived experience and pedagogical expertise of educators.\nTeachers used minimal pairs to create sentences that express stereotypes\nrelevant to their school subjects and communities. We show the dataset\ndiversity in term of demographic axes represented and also in terms of the\nknowledge areas included. We demonstrate that the dataset contains more\nstereotypes unrecognized by current LLMs than previous datasets. HESEIA is\navailable to support bias assessments grounded in educational communities.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.CY", "links": "http://arxiv.org/abs/2505.24712v1;http://arxiv.org/pdf/2505.24712v1", "pdf_url": "http://arxiv.org/pdf/2505.24712v1"}, {"title": "Analyzing Logical Fallacies in Large Language Models: A Study on Hallucination in Mathematical Reasoning", "link": "https://link.springer.com/chapter/10.1007/978-981-96-7071-0_12", "details": "DH Anh, V Tran, LM Nguyen - JSAI International Symposium on Artificial Intelligence, 2025", "abstract": "Abstract Large Language Models (LLMs) are powerful tools for natural language processing tasks, but their capabilities in formal reasoning, especially mathematical logic, are still limited. This study focuses on the phenomenon of hallucinations in \u2026"}]
