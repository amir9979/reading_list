[{"title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "link": "https://arxiv.org/pdf/2407.21417", "details": "Z Wu, Y Zhang, P Qi, Y Xu, R Han, Y Zhang, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (ie, follow open-ended instructions) and \u2026"}, {"title": "Conversational Question Answering with Language Models Generated Reformulations over Knowledge Graph", "link": "https://aclanthology.org/2024.findings-acl.48.pdf", "details": "L Liu, B Hill, B Du, F Wang, H Tong - Findings of the Association for Computational \u2026, 2024", "abstract": "Conversational question answering (ConvQA) over knowledge graphs (KGs) involves answering multi-turn natural language questions about information contained in a KG. State-of-the-art methods of ConvQA often struggle with inexplicit \u2026"}, {"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "link": "https://arxiv.org/pdf/2408.00724", "details": "Y Wu, Z Sun, S Li, S Welleck, Y Yang - arXiv preprint arXiv:2408.00724, 2024", "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth \u2026"}, {"title": "Collaborative Training of Tiny-Large Vision Language Models", "link": "https://openreview.net/pdf%3Fid%3DUgy9DyhYYD", "details": "S Lu, L Guo, W Wang, Z Zhao, T Yue, J Liu, S Liu - ACM Multimedia 2024", "abstract": "In recent years, large vision language models (LVLMs) have significantly advanced artificial intelligence, especially in integrating visual and linguistic data for complex tasks like visual conversation, image captioning and visual question answering \u2026"}, {"title": "MixPrompt: Enhancing Generalizability and Adversarial Robustness for Vision-Language Models via Prompt Fusion", "link": "https://link.springer.com/chapter/10.1007/978-981-97-5606-3_28", "details": "H Fan, Z Ma, Y Li, R Tian, Y Chen, C Gao - International Conference on Intelligent \u2026, 2024", "abstract": "Abstract Pretrained Vision-Language Models (VLMs) like CLIP have exhibited remarkable capacities across downstream tasks, while their image encoders are vulnerable to adversarial examples. A recently introduced lightweight approach \u2026"}, {"title": "CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning", "link": "https://arxiv.org/pdf/2407.21011", "details": "Y Du, B Chang, NC Dvornek - arXiv preprint arXiv:2407.21011, 2024", "abstract": "Recent advancements in Contrastive Language-Image Pre-training (CLIP) have demonstrated notable success in self-supervised representation learning across various tasks. However, the existing CLIP-like approaches often demand extensive \u2026"}, {"title": "Robust and resource-efficient table-based fact verification through multi-aspect adversarial contrastive learning", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324002127", "details": "R Liu, Y Zhang, B Yang, Q Shi, L Tian - Information Processing & Management, 2024", "abstract": "Table-based fact verification focuses on determining the truthfulness of statements by cross-referencing data in tables. This task is challenging due to the complex interactions inherent in table structures. To address this challenge, existing methods \u2026"}, {"title": "Continual Learning with Semi-supervised Contrastive Distillation for Incremental Neural Machine Translation", "link": "https://aclanthology.org/2024.acl-long.588.pdf", "details": "Y Liang, F Meng, J Wang, J Xu, Y Chen, J Zhou - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Incrementally expanding the capability of an existing translation model to solve new domain tasks over time is a fundamental and practical problem, which usually suffers from catastrophic forgetting. Generally, multi-domain learning can be seen as a good \u2026"}, {"title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT", "link": "https://arxiv.org/pdf/2407.21058", "details": "M Ali, S Panda, Q Shen, M Wick, A Kobren - arXiv preprint arXiv:2407.21058, 2024", "abstract": "In the current landscape of language model research, larger models, larger datasets and more compute seems to be the only way to advance towards intelligence. While there have been extensive studies of scaling laws and models' scaling behaviors, the \u2026"}]
