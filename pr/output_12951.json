[{"title": "Estimating the Observability of an Outcome from an Electronic Health Records Dataset Using External Data", "link": "https://academic.oup.com/aje/advance-article-abstract/doi/10.1093/aje/kwaf013/7994438", "details": "M Yan, H Hong, J Wilson, BA Goldstein - American Journal of Epidemiology, 2025", "abstract": "One of the key limitations of electronic health records (EHR) data is that not all health care encounters are observed. The degree to which patient information is captured is referred to as observability. Poor observability, and in particular differential \u2026"}, {"title": "Identification of Social Risk\u2010Related Referrals in Discrete Primary Care Electronic Health Record Data: Lessons Learned From a Novel Methodology", "link": "https://onlinelibrary.wiley.com/doi/pdf/10.1111/1475-6773.14443", "details": "J Dankovchik, R Gold, A Ochoa, J Donovan, R Gunn\u2026 - Health Services Research, 2025", "abstract": "Objective To assess the utility of using discrete primary care electronic health record (EHR) data to identify social risk referrals in a national network of community\u2010based clinics. Data Sources and Study Setting Primary data were abstracted from the \u2026"}, {"title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models", "link": "https://arxiv.org/pdf/2501.13629", "details": "Z Lin, Z Tang, X Liu, Y Gong, Y Cheng, Q Chen, H Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce Sigma, an efficient large language model specialized for the system domain, empowered by a novel architecture including DiffQKV attention, and pre- trained on our meticulously collected system domain data. DiffQKV attention \u2026"}, {"title": "Sft memorizes, rl generalizes: A comparative study of foundation model post-training", "link": "https://arxiv.org/pdf/2501.17161%3F", "details": "T Chu, Y Zhai, J Yang, S Tong, S Xie, D Schuurmans\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post- training techniques for foundation models. However, their roles in enhancing model generalization capabilities remain unclear. This paper studies the difference \u2026"}, {"title": "OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for Small-Large Language Models", "link": "https://arxiv.org/pdf/2501.12975", "details": "C Sun, Y Li, D Wu, B Boulet - arXiv preprint arXiv:2501.12975, 2025", "abstract": "Large Language Models (LLMs) are highly capable but require significant computational resources for both training and inference. Within the LLM family, smaller models (those with fewer than 10 billion parameters) also perform well \u2026"}]
