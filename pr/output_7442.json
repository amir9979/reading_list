[{"title": "3D-CT-GPT: Generating 3D Radiology Reports through Integration of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.19330", "details": "H Chen, W Zhao, Y Li, T Zhong, Y Wang, Y Shang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical image analysis is crucial in modern radiological diagnostics, especially given the exponential growth in medical imaging data. The demand for automated report generation systems has become increasingly urgent. While prior research has \u2026"}, {"title": "LMOD: A Large Multimodal Ophthalmology Dataset and Benchmark for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.01620", "details": "Z Qin, Y Yin, D Campbell, X Wu, K Zou, YC Tham, N Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Ophthalmology relies heavily on detailed image analysis for diagnosis and treatment planning. While large vision-language models (LVLMs) have shown promise in understanding complex visual information, their performance on ophthalmology \u2026"}, {"title": "Advancing Medical Radiograph Representation Learning: A Hybrid Pre-training Paradigm with Multilevel Semantic Granularity", "link": "https://arxiv.org/pdf/2410.00448", "details": "H Jiang, X Hao, Y Huang, C Ma, J Zhang, Y Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces an innovative approach to Medical Vision-Language Pre- training (Med-VLP) area in the specialized context of radiograph representation learning. While conventional methods frequently merge textual annotations into \u2026"}, {"title": "Visual-Textual Matching Attention for Lesion Segmentation in Chest Images", "link": "https://papers.miccai.org/miccai-2024/paper/2773_paper.pdf", "details": "PN Bui, DT Le, H Choo - International Conference on Medical Image Computing \u2026, 2024", "abstract": "Lesion segmentation in chest images is crucial for AI-assisted diagnostic systems of pulmonary conditions. The multi-modal approach, which combines image and text description, has achieved notable performance in medical image segmentation \u2026"}, {"title": "UniAff: A Unified Representation of Affordances for Tool Usage and Articulation with Vision-Language Models", "link": "https://arxiv.org/pdf/2409.20551", "details": "Q Yu, S Huang, X Yuan, Z Jiang, C Hao, X Li, H Chang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Previous studies on robotic manipulation are based on a limited understanding of the underlying 3D motion constraints and affordances. To address these challenges, we propose a comprehensive paradigm, termed UniAff, that integrates 3D object-centric \u2026"}, {"title": "Robot Navigation Using Physically Grounded Vision-Language Models in Outdoor Environments", "link": "https://arxiv.org/pdf/2409.20445", "details": "M Elnoor, K Weerakoon, G Seneviratne, R Xian\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present a novel autonomous robot navigation algorithm for outdoor environments that is capable of handling diverse terrain traversability conditions. Our approach, VLM-GroNav, uses vision-language models (VLMs) and integrates them with \u2026"}, {"title": "Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models", "link": "https://arxiv.org/pdf/2410.02681", "details": "S Wang, Y Li, H Wei - arXiv preprint arXiv:2410.02681, 2024", "abstract": "Confidence calibration is critical for the safe deployment of machine learning models in the real world. However, such issue in vision-language models like CLIP, particularly after fine-tuning, has not been fully addressed. In this work, we \u2026"}, {"title": "MM1. 5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning", "link": "https://arxiv.org/pdf/2409.20566", "details": "H Zhang, M Gao, Z Gan, P Dufter, N Wenzel, F Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present MM1. 5, a new family of multimodal large language models (MLLMs) designed to enhance capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. Building upon the MM1 architecture \u2026"}, {"title": "Pretrained Patient Trajectories for Adverse Drug Event Prediction Using Common Data Model-based Electronic Health Records", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/09/30/2024.09.30.24314595.full.pdf", "details": "J Kim, JS Kim, JH Lee, MG Kim, T Kim, C Cho, RW Park\u2026 - medRxiv, 2024", "abstract": "Pretraining electronic health record (EHR) data through language models by treating patient trajectories as natural language sentences has improved various medical tasks. However, EHR pretraining models have never been utilized in adverse drug \u2026"}]
