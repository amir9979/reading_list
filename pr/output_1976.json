[{"title": "Object Registration in Neural Fields", "link": "https://arxiv.org/pdf/2404.18381", "details": "D Hall, S Hausler, S Mahendren, P Moghadam - arXiv preprint arXiv:2404.18381, 2024", "abstract": "Neural fields provide a continuous scene representation of 3D geometry and appearance in a way which has great promise for robotics applications. One functionality that unlocks unique use-cases for neural fields in robotics is object 6 \u2026"}, {"title": "NeuroGauss4D-PCI: 4D Neural Fields and Gaussian Deformation Fields for Point Cloud Interpolation", "link": "https://arxiv.org/pdf/2405.14241", "details": "C Jiang, D Du, J Liu, S Zhu, Z Liu, Z Ma, Z Liang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Point Cloud Interpolation confronts challenges from point sparsity, complex spatiotemporal dynamics, and the difficulty of deriving complete 3D point clouds from sparse temporal information. This paper presents NeuroGauss4D-PCI, which excels \u2026"}, {"title": "Texture preserving low dose CT image denoising using Pearson divergence", "link": "https://iopscience.iop.org/article/10.1088/1361-6560/ad45a4/meta", "details": "J Oh, D Wu, B Hong, D Lee, M Kang, Q Li, K Kim - Physics in Medicine and Biology, 2024", "abstract": "Objective: The mean squared error (MSE), also known as L_2 loss, has been widely used as a loss function to optimize image denoising models due to its strong performance as a mean estimator of the Gaussian noise model. Recently, various \u2026"}, {"title": "DynaMo: Accelerating Language Model Inference with Dynamic Multi-Token Sampling", "link": "https://arxiv.org/pdf/2405.00888", "details": "S Tuli, CH Lin, YC Hsu, NK Jha, Y Shen, H Jin - arXiv preprint arXiv:2405.00888, 2024", "abstract": "Traditional language models operate autoregressively, ie, they predict one token at a time. Rapid explosion in model sizes has resulted in high inference times. In this work, we propose DynaMo, a suite of multi-token prediction language models that \u2026"}, {"title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "link": "https://arxiv.org/pdf/2405.00402", "details": "L Ranaldi, A Freitas - arXiv preprint arXiv:2405.00402, 2024", "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches \u2026"}, {"title": "Toward Data-driven Skill Identification for General-purpose Vision-language Models", "link": "https://openreview.net/pdf%3Fid%3DqURmfSHKqx", "details": "A Tiong, J Zhao, J Li, S Hoi, C Xiong, B Li - ICLR 2024 Workshop on Navigating and \u2026", "abstract": "The evolution of vision-language (VL) models towards broad competencies has complicated benchmarking, necessitating diverse tasks for accurate evaluation. Moving beyond intuition-guided task selection common in existing benchmarks, we \u2026"}, {"title": "HFT: Half Fine-Tuning for Large Language Models", "link": "https://arxiv.org/pdf/2404.18466", "details": "T Hui, Z Zhang, S Wang, W Xu, Y Sun, H Wu - arXiv preprint arXiv:2404.18466, 2024", "abstract": "Large language models (LLMs) with one or more fine-tuning phases have become a necessary step to unlock various capabilities, enabling LLMs to follow natural language instructions or align with human preferences. However, it carries the risk of \u2026"}, {"title": "Effective Integration of Text Diffusion and Pre-Trained Language Models with Linguistic Easy-First Schedule", "link": "https://aclanthology.org/2024.lrec-main.493.pdf", "details": "Y Ou, P Jian - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Diffusion models have become a powerful generative modeling paradigm, achieving great success in continuous data patterns. However, the discrete nature of text data results in compatibility issues between continuous diffusion models (CDMs) and pre \u2026"}, {"title": "Probe Then Retrieve and Reason: Distilling Probing and Reasoning Capabilities into Smaller Language Models", "link": "https://aclanthology.org/2024.lrec-main.1140.pdf", "details": "Y Zhao, S Zhou, H Zhu - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Step-by-step reasoning methods, such as the Chain-of-Thought (CoT), have been demonstrated to be highly effective in harnessing the reasoning capabilities of Large Language Models (LLMs). Recent research efforts have sought to distill LLMs into \u2026"}]
