[{"title": "A Federated Registration System for Artificial Intelligence in Health", "link": "https://jamanetwork.com/journals/jama/article-abstract/2822175", "details": "MJ Pencina, J McCall, NJ Economou-Zavlanos - JAMA", "abstract": "In 2000, a web-based federal registry of information about clinical trials was first made available to the public. This registry, ClinicalTrials. gov, provided descriptive information about interventional trials performed under US regulatory oversight and \u2026"}, {"title": "A Systematic Evaluation of GPT-4V's Multimodal Capability for Chest X-ray Image Analysis", "link": "https://www.sciencedirect.com/science/article/pii/S2950162824000535", "details": "Y Liu, Y Li, Z Wang, X Liang, L Liu, L Wang, L Cui, Z Tu\u2026 - Meta-Radiology, 2024", "abstract": "This work evaluates GPT-4V's multimodal capability for medical image analysis, focusing on three representative tasks radiology report generation, medical visual question answering, and medical visual grounding. For the evaluation, a set of \u2026"}, {"title": "EVLM: An Efficient Vision-Language Model for Visual Understanding", "link": "https://arxiv.org/pdf/2407.14177", "details": "K Chen, D Shen, H Zhong, H Zhong, K Xia, D Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the field of multi-modal language models, the majority of methods are built on an architecture similar to LLaVA. These models use a single-layer ViT feature as a visual prompt, directly feeding it into the language models alongside textual tokens \u2026"}, {"title": "Advancing entity alignment with dangling cases: a structure-aware approach through optimal transport learning and contrastive learning", "link": "https://link.springer.com/article/10.1007/s00521-024-10276-1", "details": "J Xu, Y Li, X Xie, N Hu, Y Li, HT Zheng, Y Jiang - Neural Computing and Applications, 2024", "abstract": "Entity alignment (EA) aims to discover the equivalent entities in different knowledge graphs (KGs), which plays an important role in knowledge engineering. Recently, EA with dangling entities has been proposed as a more realistic setting, which assumes \u2026"}, {"title": "AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization", "link": "https://arxiv.org/pdf/2407.11591", "details": "A Afzal, R Chalumattu, F Matthes, LM Espuny - arXiv preprint arXiv:2407.11591, 2024", "abstract": "Despite the advances in the abstractive summarization task using Large Language Models (LLM), there is a lack of research that asses their abilities to easily adapt to different domains. We evaluate the domain adaptation abilities of a wide range of \u2026"}, {"title": "Computational Expressivity of Neural Language Models", "link": "https://aclanthology.org/2024.acl-tutorials.3/", "details": "A Butoi, R Cotterell, A Svete - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Abstract Language models (LMs) are currently at the forefront of NLP researchdue to their remarkable versatility across diverse tasks. However, a largegap exists between their observed capabilities and the explanations proposedby established formal \u2026"}, {"title": "CROCODILE: Causality aids RObustness via COntrastive DIsentangled LEarning", "link": "https://arxiv.org/pdf/2408.04949", "details": "G Carloni, SA Tsaftaris, S Colantonio - arXiv preprint arXiv:2408.04949, 2024", "abstract": "Due to domain shift, deep learning image classifiers perform poorly when applied to a domain different from the training one. For instance, a classifier trained on chest X- ray (CXR) images from one hospital may not generalize to images from another \u2026"}, {"title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment", "link": "https://arxiv.org/pdf/2407.10804", "details": "J Jiang, J Li, WX Zhao, Y Song, T Zhang, JR Wen - arXiv preprint arXiv:2407.10804, 2024", "abstract": "Adapting general large language models (LLMs) to specialized domains presents great challenges due to varied data distributions. This adaptation typically requires continual pre-training on massive domain-specific corpora to facilitate knowledge \u2026"}, {"title": "Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education", "link": "https://arxiv.org/pdf/2407.17022", "details": "S Kim, S Kim - arXiv preprint arXiv:2407.17022, 2024", "abstract": "Large language model (LLM)-based evaluation pipelines have demonstrated their capability to robustly evaluate machine-generated text. Extending this methodology to assess human-written text could significantly benefit educational settings by \u2026"}]
