'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [DeViDe: Faceted medical knowledge for improved medica'
[{"title": "Tree-of-Reasoning Question Decomposition for Complex Question Answering with Large Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29928/31621", "details": "K Zhang, J Zeng, F Meng, Y Wang, S Sun, L Bai\u2026 - Proceedings of the AAAI \u2026, 2024", "abstract": "Large language models (LLMs) have recently demonstrated remarkable performance across various natual language processing tasks. In the field of multi- hop reasoning, the Chain-of-thought (CoT) prompt method has emerged as a \u2026"}, {"title": "TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale", "link": "https://arxiv.org/pdf/2403.10351", "details": "P Jiang, C Xiao, Z Wang, P Bhatia, J Sun, J Han - arXiv preprint arXiv:2403.10351, 2024", "abstract": "The advent of large language models (LLMs) has significantly advanced natural language processing tasks like text summarization. However, their large size and computational demands, coupled with privacy concerns in data transmission, limit \u2026"}]
