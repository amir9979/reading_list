[{"title": "Designing Retrieval-Augmented Language Models for Clinical Decision", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DWcMbEQAAQBAJ%26oi%3Dfnd%26pg%3DPA159%26ots%3DtCwXt5QBal%26sig%3DepmwqKt-uzrhebDQBZayU4Bzzx0", "details": "K Quigley, T Koker, J Taylor, V Mancuso - AI for Health Equity and Fairness: Leveraging AI to \u2026", "abstract": "Ever-increasing demands for physician expertise drive the need for trust-worthy point- of-care tools that can help aid decision-making in all clinical settings. Retrieval- augmented language models carry potential to relieve the information burden on \u2026"}, {"title": "Revisiting Prompt Pretraining of Vision-Language Models", "link": "https://arxiv.org/pdf/2409.06166", "details": "Z Chen, L Yang, S Chen, Z Chen, J Liang, X Li - arXiv preprint arXiv:2409.06166, 2024", "abstract": "Prompt learning is an effective method to customize Vision-Language Models (VLMs) for various downstream tasks, involving tuning very few parameters of input prompt tokens. Recently, prompt pretraining in large-scale dataset (eg, ImageNet \u2026"}, {"title": "A neural network approach to predict opioid misuse among previously hospitalized patients using electronic health records", "link": "https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0309424", "details": "L Vega, W Conneen, MA Veronin, RP Schumaker - Plos one, 2024", "abstract": "Can Electronic Health Records (EHR) predict opioid misuse in general patient populations? This research trained three backpropagation neural networks to explore EHR predictors using existing patient data. Model 1 used patient diagnosis \u2026"}, {"title": "Leveraging an Electronic Health Record Patient Portal to Help Patients Formulate Their Health Care Goals: Mixed Methods Evaluation of Pilot Interventions", "link": "https://formative.jmir.org/2024/1/e56332", "details": "J Naimark, ME Tinetti, T Delbanco, Z Dong, K Harcourt\u2026 - JMIR Formative Research, 2024", "abstract": "Background Persons with multiple chronic conditions face complex medical regimens and clinicians may not focus on what matters most to these patients who vary widely in their health priorities. Patient Priorities Care is a facilitator-led process \u2026"}, {"title": "A Case Demonstration of the Open Health Natural Language Processing Toolkit From the National COVID-19 Cohort Collaborative and the Researching COVID to \u2026", "link": "https://medinform.jmir.org/2024/1/e49997", "details": "A Wen, L Wang, H He, S Fu, S Liu, DA Hanauer\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background A wealth of clinically relevant information is only obtainable within unstructured clinical narratives, leading to great interest in clinical natural language processing (NLP). While a multitude of approaches to NLP exist, current algorithm \u2026"}, {"title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks", "link": "https://arxiv.org/pdf/2409.06173", "details": "G Chochlakis, NM Pandiyan, K Lerman, S Narayanan - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the dominant technique for performing natural language tasks, as it does not require updating the model parameters with gradient-based methods. ICL promises to\" \u2026"}, {"title": "Language Models Pre-training", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-65647-7_2.pdf", "details": "U Kamath, K Keenan, G Somers, S Sorenson - Large Language Models: A Deep Dive \u2026, 2024", "abstract": "Pre-training forms the foundation for LLMs' capabilities. LLMs gain vital language comprehension and generative language skills by using large-scale datasets. The size and quality of these datasets are essential for maximizing LLMs' potential. It is \u2026"}, {"title": "NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks", "link": "https://arxiv.org/pdf/2408.13106", "details": "H Huang, T Park, K Dhawan, I Medennikov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-supervised learning has been proved to benefit a wide range of speech processing tasks, such as speech recognition/translation, speaker verification and diarization, etc. However, most of these approaches are computationally intensive \u2026"}, {"title": "Towards Harnessing Large Language Models as Autonomous Agents for Semantic Triple Extraction from Unstructured Text", "link": "https://ceur-ws.org/Vol-3747/text2kg_paper1.pdf", "details": "A Ananya, S Tiwari, N Mihindukulasooriya, T Soru\u2026 - 2024", "abstract": "Abstract The use of Large Language Models as autonomous agents interacting with tools has shown to improve the performance of several tasks from code generation to API calling and sequencing. This paper proposes a framework for using Large \u2026"}]
