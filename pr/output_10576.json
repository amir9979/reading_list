[{"title": "Large language models for accurate disease detection in electronic health records: the examples of crystal arthropathies", "link": "https://rmdopen.bmj.com/content/10/4/e005003", "details": "N B\u00fcrgisser, E Chalot, S Mehouachi, CP Buclin\u2026 - RMD Open, 2024", "abstract": "Objectives We propose and test a framework to detect disease diagnosis using a recent large language model (LLM), Meta's Llama-3-8B, on French-language electronic health record (EHR) documents. Specifically, it focuses on detecting gout \u2026"}, {"title": "segWCD: A new segmentation-based weak supervision neural network for building change detection", "link": "https://link.springer.com/article/10.1007/s10489-024-06003-x", "details": "Y Wu, X Zhang, X Zhao, Y Sun, T Li - Applied Intelligence, 2025", "abstract": "Manual annotation of changes in high-resolution remote sensing images is labor- intensive and limits advancements in change detection. We introduce the Segmentation-based Weakly Supervised Change Detection (segWCD) framework to \u2026"}, {"title": "Evaluating the Limitations of Large Language Models in Therapeutic Decision-making for patients with Aortic Stenosis", "link": "https://www.medrxiv.org/content/10.1101/2024.11.20.24313385.full.pdf", "details": "T Roeschl, M Hoffmann, D Hashemi, F Rarreck\u2026 - medRxiv, 2024", "abstract": "Aims: Large language models (LLMs) have shown promise in therapeutic decision- making comparable to medical experts, but these studies have used specially prepared patient data. The aim of this study was to determine whether LLMs can \u2026"}, {"title": "When Word Order Matters: Unraveling Redundancy and Sequence Sensitivity in Language Models through Information Theory", "link": "https://xdchen2.github.io/order.pdf", "details": "X Chen, S Reddy, TJ O'Donnell", "abstract": "Abstract Language models (LMs) often seem insensitive to word order changes in natural language understanding (NLU) tasks. We propose that this insensitivity is due to linguistic redundancy, where word order and other cues, like case markers \u2026"}, {"title": "Core Context Aware Attention for Long Context Language Modeling", "link": "https://arxiv.org/pdf/2412.12465", "details": "Y Chen, Z You, S Zhang, H Li, Y Li, Y Wang, M Tan - arXiv preprint arXiv:2412.12465, 2024", "abstract": "Transformer-based Large Language Models (LLMs) have exhibited remarkable success in various natural language processing tasks primarily attributed to self- attention mechanism, which requires a token to consider all preceding tokens as its \u2026"}, {"title": "Dialogue Language Model with Large-Scale Persona Data Engineering", "link": "https://arxiv.org/pdf/2412.09034", "details": "M Hong, C Zhang, C Chen, R Lian, D Jiang - arXiv preprint arXiv:2412.09034, 2024", "abstract": "Maintaining persona consistency is paramount in the application of open-domain dialogue systems, as exemplified by models like ChatGPT. Despite significant advancements, the limited scale and diversity of current persona dialogue datasets \u2026"}]
