[{"title": "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "link": "https://arxiv.org/pdf/2407.02118", "details": "W Zheng, W Pan, X Xu, L Qin, L Yue, M Zhou - arXiv preprint arXiv:2407.02118, 2024", "abstract": "In recent years, Large Language Models (LLMs) have made significant strides towards Artificial General Intelligence. However, training these models from scratch requires substantial computational resources and vast amounts of text data. In this \u2026"}, {"title": "SCALED SIAMESE CNN-BASED AUTOMATIC CLASSIFICATION ALGORITHM FOR DETECTING PULMONARY EMBOLISM", "link": "https://www.worldscientific.com/doi/abs/10.4015/S1016237224500285", "details": "A Sekhar, L Padma Suresh - Biomedical Engineering: Applications, Basis and \u2026, 2024", "abstract": "The majority of patients today have vascular illness that could cause pulmonary hypertension and pulmonary emboli which can lead to serious conditions. To diagnose changes in veins and arteries, manual, semi-automatic and automatic \u2026"}, {"title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models", "link": "https://arxiv.org/pdf/2407.03181", "details": "H Puerto, T Chubakov, X Zhu, HT Madabushi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model \u2026"}, {"title": "Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models: Enhancing Performance and Reducing Inference Costs", "link": "https://arxiv.org/pdf/2407.00945", "details": "E Liu, J Zhu, Z Lin, X Ning, MB Blaschko, S Yan, G Dai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancement of large language models (LLMs) has led to architectures with billions to trillions of parameters, posing significant deployment challenges due to their substantial demands on memory, processing power, and energy \u2026"}, {"title": "Speculative Speech Recognition by Audio-Prefixed Low-Rank Adaptation of Language Models", "link": "https://arxiv.org/pdf/2407.04641", "details": "B Yusuf, MK Baskar, A Rosenberg, B Ramabhadran - arXiv preprint arXiv:2407.04641, 2024", "abstract": "This paper explores speculative speech recognition (SSR), where we empower conventional automatic speech recognition (ASR) with speculation capabilities, allowing the recognizer to run ahead of audio. We introduce a metric for measuring \u2026"}, {"title": "Large Language Models in the Clinic: A Comprehensive Benchmark", "link": "https://www.researchgate.net/profile/Fenglin-Liu-2/publication/381732507_Large_Language_Models_in_the_Clinic_A_Comprehensive_Benchmark/links/667c54891dec0c3c6fa5bee9/Large-Language-Models-in-the-Clinic-A-Comprehensive-Benchmark.pdf", "details": "F Liu, H Zhou, Y Hua, O Rohanian, A Thakur, L Clifton\u2026", "abstract": "The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the closeended question- answering (QA) task with answer options for evaluation. However, many clinical \u2026"}, {"title": "Applying Information Retrieval to the Electronic Health Record for Cohort Discovery", "link": "https://dmice.ohsu.edu/hersh/irehr-24.pdf", "details": "W Hersh", "abstract": "Hersh, W., Pentecost, J., Hickam, D., 1996. A task-oriented approach to information retrieval evaluation. Journal of the American Society for Information Science 47, 50\u2013 56\\. https://doi. org/10.1002/(SICI) 1097-4571 (199601) 47: 1< 50:: AID-ASI5> 3.0 \u2026"}, {"title": "Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning", "link": "https://arxiv.org/pdf/2407.06112", "details": "Y Zhang, S Mao, W Wu, Y Xia, T Ge, M Lan, F Wei - arXiv preprint arXiv:2407.06112, 2024", "abstract": "This paper introduces BI-Directional DEliberation Reasoning (BIDDER), a novel reasoning approach to enhance the decision rationality of language models. Traditional reasoning methods typically rely on historical information and employ uni \u2026"}, {"title": "$\\text {Memory}^ 3$: Language Modeling with Explicit Memory", "link": "https://arxiv.org/pdf/2407.01178", "details": "H Yang, Z Lin, W Wang, H Wu, Z Li, B Tang, W Wei\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The training and inference of large language models (LLMs) are together a costly process that transports knowledge from raw data to meaningful computation. Inspired by the memory hierarchy of the human brain, we reduce this cost by equipping LLMs \u2026"}]
