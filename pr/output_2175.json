[{"title": "Knowledge-grounded Adaptation Strategy for Vision-language Models: Building Unique Case-set for Screening Mammograms for Residents Training", "link": "https://arxiv.org/pdf/2405.19675", "details": "AU Khan, J Garrett, T Bradshaw, L Salkowski, JJ Jeong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "A visual-language model (VLM) pre-trained on natural images and text pairs poses a significant barrier when applied to medical contexts due to domain shift. Yet, adapting or fine-tuning these VLMs for medical use presents considerable hurdles \u2026"}, {"title": "NeuroGauss4D-PCI: 4D Neural Fields and Gaussian Deformation Fields for Point Cloud Interpolation", "link": "https://arxiv.org/pdf/2405.14241", "details": "C Jiang, D Du, J Liu, S Zhu, Z Liu, Z Ma, Z Liang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Point Cloud Interpolation confronts challenges from point sparsity, complex spatiotemporal dynamics, and the difficulty of deriving complete 3D point clouds from sparse temporal information. This paper presents NeuroGauss4D-PCI, which excels \u2026"}, {"title": "Enhancing Large Vision Language Models with Self-Training on Image Comprehension", "link": "https://arxiv.org/pdf/2405.19716", "details": "Y Deng, P Lu, F Yin, Z Hu, S Shen, J Zou, KW Chang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision language models (LVLMs) integrate large language models (LLMs) with pre-trained vision encoders, thereby activating the perception capability of the model to understand image inputs for different queries and conduct subsequent reasoning \u2026"}, {"title": "CheXpert Plus: Hundreds of Thousands of Aligned Radiology Texts, Images and Patients", "link": "https://arxiv.org/pdf/2405.19538", "details": "P Chambon, JB Delbrouck, T Sounack, SC Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Since the release of the original CheXpert paper five years ago, CheXpert has become one of the most widely used and cited clinical AI datasets. The emergence of vision language models has sparked an increase in demands for sharing reports \u2026"}, {"title": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records", "link": "https://aclanthology.org/2024.cl4health-1.23.pdf", "details": "J Lov\u00f3n-Melgarejo, T Ben-Haddi, J Di Scala\u2026 - Proceedings of the First \u2026, 2024", "abstract": "The lack of standardized evaluation benchmarks in the medical domain for text inputs can be a barrier to widely adopting and leveraging the potential of natural language models for health-related downstream tasks. This paper revisited an \u2026"}, {"title": "BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models", "link": "https://arxiv.org/pdf/2405.04756", "details": "CF Luo, A Ghawanmeh, X Zhu, FK Khattak - arXiv preprint arXiv:2405.04756, 2024", "abstract": "Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly. The language model can also \u2026"}, {"title": "Reducing the cost of posterior sampling in linear inverse problems via task-dependent score learning", "link": "https://arxiv.org/pdf/2405.15643", "details": "F Schneider, DL Duong, M Lassas, MV de Hoop\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Score-based diffusion models (SDMs) offer a flexible approach to sample from the posterior distribution in a variety of Bayesian inverse problems. In the literature, the prior score is utilized to sample from the posterior by different methods that require \u2026"}, {"title": "HINT: Learning Complete Human Neural Representations from Limited Viewpoints", "link": "https://arxiv.org/pdf/2405.19712", "details": "A Sanvito, A Ramazzina, S Walz, M Bijelic, F Heide - arXiv preprint arXiv:2405.19712, 2024", "abstract": "No augmented application is possible without animated humanoid avatars. At the same time, generating human replicas from real-world monocular hand-held or robotic sensor setups is challenging due to the limited availability of views. Previous \u2026"}, {"title": "Fast Samplers for Inverse Problems in Iterative Refinement Models", "link": "https://arxiv.org/pdf/2405.17673", "details": "K Pandey, R Yang, S Mandt - arXiv preprint arXiv:2405.17673, 2024", "abstract": "Constructing fast samplers for unconditional diffusion and flow-matching models has received much attention recently; however, existing methods for solving inverse problems, such as super-resolution, inpainting, or deblurring, still require hundreds \u2026"}]
