[{"title": "Scalable Bayesian Monte Carlo: fast uncertainty estimation beyond deep ensembles", "link": "https://arxiv.org/pdf/2505.13585", "details": "X Liang, JM Lukens, S Lohani, BT Kirby, TA Searles\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This work introduces a new method called scalable Bayesian Monte Carlo (SBMC). The model interpolates between a point estimator and the posterior, and the algorithm is a parallel implementation of a consistent (asymptotically unbiased) \u2026", "entry_id": "http://arxiv.org/abs/2505.13585v1", "updated": "2025-05-19 17:55:32", "published": "2025-05-19 17:55:32", "authors": "Xinzhu Liang;Joseph M. Lukens;Sanjaya Lohani;Brian T. Kirby;Thomas A. Searles;Xin Qiu;Kody J. H. Law", "summary": "This work introduces a new method called scalable Bayesian Monte Carlo\n(SBMC). The model interpolates between a point estimator and the posterior, and\nthe algorithm is a parallel implementation of a consistent (asymptotically\nunbiased) Bayesian deep learning algorithm: sequential Monte Carlo (SMC) or\nMarkov chain Monte Carlo (MCMC). The method is motivated theoretically, and its\nutility is demonstrated on practical examples: MNIST, CIFAR, IMDb. A systematic\nnumerical study reveals that parallel implementations of SMC and MCMC are\ncomparable to serial implementations in terms of performance and total cost,\nand they achieve accuracy at or beyond the state-of-the-art (SOTA) methods like\ndeep ensembles at convergence, along with substantially improved uncertainty\nquantification (UQ)--in particular, epistemic UQ. But even parallel\nimplementations are expensive, with an irreducible time barrier much larger\nthan the cost of the MAP estimator. Compressing time further leads to rapid\ndegradation of accuracy, whereas UQ remains valuable. By anchoring to a point\nestimator we can recover accuracy, while retaining valuable UQ, ultimately\ndelivering strong performance across metrics for a cost comparable to the SOTA.", "comment": "56 pages, 44 figures, 35 tables", "journal_ref": null, "primary_category": "stat.ML", "categories": "stat.ML;cs.LG;stat.CO", "links": "http://arxiv.org/abs/2505.13585v1;http://arxiv.org/pdf/2505.13585v1", "pdf_url": "http://arxiv.org/pdf/2505.13585v1"}, {"title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment", "link": "https://arxiv.org/pdf/2505.13232", "details": "Y Kim, J Jeong, S Kwak, K Lee, J Lee, J Shin - arXiv preprint arXiv:2505.13232, 2025", "abstract": "Learning robust representations from data often requires scale, which has led to the success of recent zero-shot models such as CLIP. However, the obtained robustness can easily be deteriorated when these models are fine-tuned on other downstream \u2026", "entry_id": "http://arxiv.org/abs/2505.13232v2", "updated": "2025-05-20 12:27:33", "published": "2025-05-19 15:15:35", "authors": "Younghyun Kim;Jongheon Jeong;Sangkyung Kwak;Kyungmin Lee;Juho Lee;Jinwoo Shin", "summary": "Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions. We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features. Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance.", "comment": "IJCAI 2025; Code is available at https://github.com/alinlab/StarFT", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CV", "links": "http://arxiv.org/abs/2505.13232v2;http://arxiv.org/pdf/2505.13232v2", "pdf_url": "http://arxiv.org/pdf/2505.13232v2"}, {"title": "Unsupervised Specific Emitter Identification: A Multi-Scale Feature Adaptive Fusion Contrastive Learning Algorithm", "link": "https://ieeexplore.ieee.org/abstract/document/11002864/", "details": "J Wang, G Ding, Y Jiao, D Zhang, P Tang, G Wei - IEEE Wireless Communications \u2026, 2025", "abstract": "Specific Emitter Identification (SEI) plays a vital role in the fields of electromagnetic spectrum management and electronic warfare. However, existing SEI algorithms typically require labeled information, which is often unavailable in non-cooperative \u2026"}, {"title": "Zero-Shot Forecasting Mortality Rates: A Global Study", "link": "https://arxiv.org/pdf/2505.13521", "details": "G Petnehazi, LA Shaggah, J Gall, B Aradi - arXiv preprint arXiv:2505.13521, 2025", "abstract": "This study explores the potential of zero-shot time series forecasting, an innovative approach leveraging pre-trained foundation models, to forecast mortality rates without task-specific fine-tuning. We evaluate two state-of-the-art foundation models \u2026", "entry_id": "http://arxiv.org/abs/2505.13521v1", "updated": "2025-05-17 13:27:39", "published": "2025-05-17 13:27:39", "authors": "Gabor Petnehazi;Laith Al Shaggah;Jozsef Gall;Bernadett Aradi", "summary": "This study explores the potential of zero-shot time series forecasting, an\ninnovative approach leveraging pre-trained foundation models, to forecast\nmortality rates without task-specific fine-tuning. We evaluate two\nstate-of-the-art foundation models, TimesFM and CHRONOS, alongside traditional\nand machine learning-based methods across three forecasting horizons (5, 10,\nand 20 years) using data from 50 countries and 111 age groups. In our\ninvestigations, zero-shot models showed varying results: while CHRONOS\ndelivered competitive shorter-term forecasts, outperforming traditional methods\nlike ARIMA and the Lee-Carter model, TimesFM consistently underperformed.\nFine-tuning CHRONOS on mortality data significantly improved long-term\naccuracy. A Random Forest model, trained on mortality data, achieved the best\noverall performance. These findings underscore the potential of zero-shot\nforecasting while highlighting the need for careful model selection and\ndomain-specific adaptation.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;q-fin.RM;stat.AP", "links": "http://arxiv.org/abs/2505.13521v1;http://arxiv.org/pdf/2505.13521v1", "pdf_url": "http://arxiv.org/pdf/2505.13521v1"}, {"title": "One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling", "link": "https://arxiv.org/pdf/2505.13358", "details": "N Berman, I Naiman, M Eliasof, H Zisling, O Azencot - arXiv preprint arXiv \u2026, 2025", "abstract": "Diffusion-based generative models have demonstrated exceptional performance, yet their iterative sampling procedures remain computationally expensive. A prominent strategy to mitigate this cost is distillation, with offline distillation offering particular \u2026", "entry_id": "http://arxiv.org/abs/2505.13358v2", "updated": "2025-05-20 14:05:02", "published": "2025-05-19 16:59:47", "authors": "Nimrod Berman;Ilan Naiman;Moshe Eliasof;Hedi Zisling;Omri Azencot", "summary": "Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model KDM, a novel\noffline distillation approach grounded in Koopman theory-a classical framework\nfor representing nonlinear dynamics linearly in a transformed space. KDM\nencodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. Empirically, KDM achieves state-of-the-art performance across\nstandard offline distillation benchmarks, improving FID scores by up to 40% in\na single generation step. All implementation details and code for the\nexperimental setups are provided in our GitHub -\nhttps://github.com/azencot-group/KDM, or in our project page -\nhttps://sites.google.com/view/koopman-distillation-model.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.13358v2;http://arxiv.org/pdf/2505.13358v2", "pdf_url": "http://arxiv.org/pdf/2505.13358v2"}]
