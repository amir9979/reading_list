[{"title": "A review on synergizing knowledge graphs and **large language models**", "link": "https://link.springer.com/article/10.1007/s00607-025-01499-8", "details": "Z Yang, S Yuan, Z Shao, W Li, R Liu - Computing, 2025", "abstract": "\u2026 applications ranging from text summarization to complex **question** **answering** [2]. However, \u2026 in high-stakes domains such as **healthcare** or legal advice. Updating knowledge to remain \u2026 to adapt to specific applications, such as **question** \u2026"}, {"title": "ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding", "link": "https://arxiv.org/pdf/2506.04353", "details": "A Pal, JO Lee, X Zhang, M Sankarasubbu, S Roh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 for visual **question** **answering** (VQA) in chest radiology, comprising approximately 696,000 **questions** \u2026 We evaluate eight state-of-the-art multimodal **large** **language** **models** , including MedGemma\u2026 of how multimodal **large** **language** **models** (LLMs) \u2026", "entry_id": "http://arxiv.org/abs/2506.04353v1", "updated": "2025-06-04 18:11:59", "published": "2025-06-04 18:11:59", "authors": "Ankit Pal;Jung-Oh Lee;Xiaoman Zhang;Malaikannan Sankarasubbu;Seunghyeon Roh;Won Jung Kim;Meesun Lee;Pranav Rajpurkar", "summary": "We present ReXVQA, the largest and most comprehensive benchmark for visual\nquestion answering (VQA) in chest radiology, comprising approximately 696,000\nquestions paired with 160,000 chest X-rays studies across training, validation,\nand test sets. Unlike prior efforts that rely heavily on template based\nqueries, ReXVQA introduces a diverse and clinically authentic task suite\nreflecting five core radiological reasoning skills: presence assessment,\nlocation analysis, negation detection, differential diagnosis, and geometric\nreasoning. We evaluate eight state-of-the-art multimodal large language models,\nincluding MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The\nbest-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge\nthe gap between AI performance and clinical expertise, we conducted a\ncomprehensive human reader study involving 3 radiology residents on 200\nrandomly sampled cases. Our evaluation demonstrates that MedGemma achieved\nsuperior performance (83.84% accuracy) compared to human readers (best\nradiology resident: 77.27%), representing a significant milestone where AI\nperformance exceeds expert human evaluation on chest X-ray interpretation. The\nreader study reveals distinct performance patterns between AI models and human\nexperts, with strong inter-reader agreement among radiologists while showing\nmore variable agreement patterns between human readers and AI models. ReXVQA\nestablishes a new standard for evaluating generalist radiological AI systems,\noffering public leaderboards, fine-grained evaluation splits, structured\nexplanations, and category-level breakdowns. This benchmark lays the foundation\nfor next-generation AI systems capable of mimicking expert-level clinical\nreasoning beyond narrow pathology classification. Our dataset will be\nopen-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI;cs.CE;cs.CL;cs.LG", "links": "http://arxiv.org/abs/2506.04353v1;http://arxiv.org/pdf/2506.04353v1", "pdf_url": "http://arxiv.org/pdf/2506.04353v1"}, {"title": "PathVLM-Eval: Evaluation of open vision language models in histopathology", "link": "https://www.sciencedirect.com/science/article/pii/S2153353925000409", "details": "NU Gilal, R Zegour, K Al-Thelaya, E \u00d6zer, M Agus\u2026 - Journal of Pathology \u2026, 2025", "abstract": "\u2026 imaging, particularly digital pathology, could significantly benefit from VLMs for histological interpretation and diagnosis, enabling pathologists to use a complementary tool for faster and comprehensive reporting and efficient **healthcare** \u2026"}, {"title": "Comparison of Multiple State-of-the-Art **Large Language Models** for Patient Education Prior to CT and MRI Examinations", "link": "https://www.mdpi.com/2075-4426/15/6/235", "details": "S Eminovic, B Levita, A Dell'Orco, JA Leppig, J Nawabi\u2026 - \u2026 of Personalized **Medicine** , 2025", "abstract": "\u2026 Background/Objectives: This study compares the accuracy of responses from state-of-the-art **large** **language** **models** (LLMs) to patient **questions** before CT and MRI imaging. We aim to demonstrate the potential of LLMs in improving workflow efficiency, while also \u2026"}, {"title": "Instruction tuning and cot prompting for contextual **medical** qa with llms", "link": "https://www.preprints.org/frontend/manuscript/3040bf0fe0df8ad5b9f2af5867cd840d/download_pub", "details": "C Le, Z Gong, C Wang, H Ni, P Li, X Chen - 2025 International Conference on Artificial \u2026, 2025", "abstract": "\u2026 In this study, we explored the impact of prompt design and instruction fine-tuning on the performance of **large** **language** **models** for biomedical **question** **answering**. We find that Chain-ofThought prompting improves zero-shot performance, while \u2026"}, {"title": "Medication counseling with **large language models** : improving self-evaluation through multi-agent systems", "link": "https://www.doria.fi/bitstream/handle/10024/192693/sabel_joar.pdf%3Fsequence%3D3", "details": "J Sabel - 2025", "abstract": "\u2026 the use of **Large** **Language** **Models** , have remained hesitant to do so. One of the fields that has remained hesitant is the field of **medicine**. \u2026 I had known for some time that I wanted my master\u2019s thesis to somehow revolve around the topic of **large** \u2026"}, {"title": "Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models", "link": "https://arxiv.org/pdf/2506.04689", "details": "T Nguyen, Y Li, O Golovneva, L Zettlemoyer, S Oh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Scaling laws predict that the performance of **large** **language** **models** improves with increasing \u2026 2024): extracted knowledge and diverse **question** **answer** pairs synthesized from high-quality \u2026 **medical** science continues to advance, the role of \u2026", "entry_id": "http://arxiv.org/abs/2506.04689v1", "updated": "2025-06-05 07:12:12", "published": "2025-06-05 07:12:12", "authors": "Thao Nguyen;Yang Li;Olga Golovneva;Luke Zettlemoyer;Sewoong Oh;Ludwig Schmidt;Xian Li", "summary": "Scaling laws predict that the performance of large language models improves\nwith increasing model size and data size. In practice, pre-training has been\nrelying on massive web crawls, using almost all data sources publicly available\non the internet so far. However, this pool of natural data does not grow at the\nsame rate as the compute supply. Furthermore, the availability of high-quality\ntexts is even more limited: data filtering pipelines often remove up to 99% of\nthe initial web scrapes to achieve state-of-the-art. To address the \"data wall\"\nof pre-training scaling, our work explores ways to transform and recycle data\ndiscarded in existing filtering processes. We propose REWIRE, REcycling the Web\nwith guIded REwrite, a method to enrich low-quality documents so that they\ncould become useful for training. This in turn allows us to increase the\nrepresentation of synthetic data in the final pre-training set. Experiments at\n1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw\ntexts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points\nimprovement respectively across 22 diverse tasks, compared to training on only\nfiltered web data. Training on the raw-synthetic data mix is also more\neffective than having access to 2x web data. Through further analysis, we\ndemonstrate that about 82% of the mixed in texts come from transforming\nlower-quality documents that would otherwise be discarded. REWIRE also\noutperforms related approaches of generating synthetic data, including\nWikipedia-style paraphrasing, question-answer synthesizing and knowledge\nextraction. These results suggest that recycling web texts holds the potential\nfor being a simple and effective approach for scaling pre-training data.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2506.04689v1;http://arxiv.org/pdf/2506.04689v1", "pdf_url": "http://arxiv.org/pdf/2506.04689v1"}, {"title": " **Large Language Models** in Rehabilitation: A Review of Approaches, Interaction Dynamics, and Emerging Trends", "link": "https://link.springer.com/chapter/10.1007/978-3-031-94150-4_40", "details": "W Wang, H Xu, Y Zhang - International Conference on Human-Computer \u2026, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) like ChatGPT, with the ability to **answer** sophisticated **questions** in natural language, appear promising in addressing this issue by assisting rehabilitation in the absence of professionals. However \u2026"}, {"title": "SPARQL Query Generation Using LLMs for **Medical** Information Retrieval", "link": "https://www.researchgate.net/profile/Giannis-Vassiliou/publication/392432188_SPARQL_Query_Generation_Using_LLMs_for_Medical_Information_Retrieval/links/68417a30d1054b0207f9eb0e/SPARQL-Query-Generation-Using-LLMs-for-Medical-Information-Retrieval.pdf", "details": "C Doulaverakis, G Vassiliou, S Batsakis, N Papadakis\u2026", "abstract": "\u2026 **problem** by making use of Linked Open Data as a source of reliable information. Specifically, we propose an approach that leverages **Large** **Language** **Models** (\u2026 Knowledge graph **question** **answering** using graph-pattern isomorphism. In Further \u2026"}]
