[{"title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding", "link": "https://arxiv.org/pdf/2412.10302%3F", "details": "Z Wu, X Chen, Z Pan, X Liu, W Liu, D Dai, H Gao, Y Ma\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL, through two key major upgrades. For the vision component, we \u2026"}, {"title": "Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning", "link": "https://arxiv.org/pdf/2412.08614", "details": "F Lu, W Wu, K Zheng, S Ma, B Gong, J Liu, W Zhai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating detailed captions comprehending text-rich visual content in images has received growing attention for Large Vision-Language Models (LVLMs). However, few studies have developed benchmarks specifically tailored for detailed captions to \u2026"}, {"title": "AdvDreamer Unveils: Are Vision-Language Models Truly Ready for Real-World 3D Variations?", "link": "https://arxiv.org/pdf/2412.03002", "details": "S Ruan, H Liu, Y Huang, X Wang, C Kang, H Su\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision Language Models (VLMs) have exhibited remarkable generalization capabilities, yet their robustness in dynamic real-world scenarios remains largely unexplored. To systematically evaluate VLMs' robustness to real-world 3D variations \u2026"}, {"title": "PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation", "link": "https://arxiv.org/pdf/2412.15209", "details": "M Wahed, KA Nguyen, AS Juvekar, X Li, X Zhou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite significant advancements in Large Vision-Language Models (LVLMs), existing pixel-grounding models operate on single-image settings, limiting their ability to perform detailed, fine-grained comparisons across multiple images \u2026"}, {"title": "An Approach to Complex Visual Data Interpretation with Vision-Language Models", "link": "https://openaccess.thecvf.com/content/ACCV2024W/LAVA/papers/Nguyen_An_Approach_to_Complex_Visual_Data_Interpretation_with_Vision-Language_Models_ACCVW_2024_paper.pdf", "details": "TS Nguyen, VT Huynh, VL Nguyen, MT Tran - \u2026 of the Asian Conference on Computer \u2026, 2024", "abstract": "The LAVA Workshop 2024 challenge aimed to assess the capability of Large Vision- Language Models (VLMs) to interpret and understand complex visual data accurately. This includes intricate visual formats such as data flow diagrams, class \u2026"}, {"title": "Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.06775", "details": "YL Lee, YH Tsai, WC Chiu - arXiv preprint arXiv:2412.06775, 2024", "abstract": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To \u2026"}, {"title": "Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation on Language Models", "link": "https://arxiv.org/pdf/2412.14528", "details": "X Cui, M Zhu, Y Qin, L Xie, W Zhou, H Li - arXiv preprint arXiv:2412.14528, 2024", "abstract": "Knowledge distillation (KD) has become a prevalent technique for compressing large language models (LLMs). Existing KD methods are constrained by the need for identical tokenizers (ie, vocabularies) between teacher and student models, limiting \u2026"}, {"title": "Beyond decomposition: Hierarchical dependency management in multi\u2010document question answering", "link": "https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.24971", "details": "X Zheng, Z Li, Q Chen, Y Zhang - Journal of the Association for Information Science and \u2026", "abstract": "When using retrieval\u2010augmented generation (RAG) to handle multi\u2010document question answering (MDQA) tasks, it is beneficial to decompose complex queries into multiple simpler ones to enhance retrieval results. However, previous strategies \u2026"}, {"title": "GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking", "link": "https://arxiv.org/pdf/2412.14140", "details": "D Deshpande, SS Ravi, S CH-Wang, B Mielczarek\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The LLM-as-judge paradigm is increasingly being adopted for automated evaluation of model outputs. While LLM judges have shown promise on constrained evaluation tasks, closed source LLMs display critical shortcomings when deployed in real world \u2026"}]
