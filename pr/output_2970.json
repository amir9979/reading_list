[{"title": "Zero-Shot Out-of-Distribution Detection with Outlier Label Exposure", "link": "https://arxiv.org/pdf/2406.01170", "details": "C Ding, G Pang - arXiv preprint arXiv:2406.01170, 2024", "abstract": "As vision-language models like CLIP are widely applied to zero-shot tasks and gain remarkable performance on in-distribution (ID) data, detecting and rejecting out-of- distribution (OOD) inputs in the zero-shot setting have become crucial for ensuring \u2026"}, {"title": "Gaussian processes based data augmentation and expected signature for time series classification", "link": "https://ieeexplore.ieee.org/iel8/6287639/10380310/10546274.pdf", "details": "F Triggiano, M Romito - IEEE Access, 2024", "abstract": "Time series classification tasks play a crucial role in extracting relevant information from data equipped with a temporal structure. In various scientific domains, such as biology or finance, this kind of data comes from complex and hardly predictable \u2026"}, {"title": "Posterior Inference on Shallow Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance", "link": "https://openreview.net/pdf%3Fid%3DJ97bdMR7Lv", "details": "J Loria, A Bhadra - The 40th Conference on Uncertainty in Artificial \u2026", "abstract": "From the classical and influential works of Neal (1996), it is known that the infinite width scaling limit of a Bayesian neural network with one hidden layer is a Gaussian process, when the network weights have bounded prior variance. Neal's result has \u2026"}, {"title": "Generative Pre-Trained Diffusion Paradigm for Zero-Shot Time Series Forecasting", "link": "https://arxiv.org/pdf/2406.02212", "details": "J Yang, T Dai, N Li, J Wu, P Liu, J Li, J Bao, H Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, generative pre-trained paradigms such as Large Language Models (LLMs) and Large Vision Models (LVMs) have achieved revolutionary advancements and widespread real-world applications. Particularly, the emergence of pre-trained \u2026"}, {"title": "Demonstration Augmentation for Zero-shot In-context Learning", "link": "https://arxiv.org/pdf/2406.01224", "details": "Y Su, Y Tai, Y Ji, J Li, B Yan, M Zhang - arXiv preprint arXiv:2406.01224, 2024", "abstract": "Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates. However, many studies \u2026"}, {"title": "Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models", "link": "https://arxiv.org/pdf/2406.04320", "details": "A Behrouz, M Santacatterina, R Zabih - arXiv preprint arXiv:2406.04320, 2024", "abstract": "Modeling multivariate time series is a well-established problem with a wide range of applications from healthcare to financial markets. Traditional State Space Models (SSMs) are classical approaches for univariate time series modeling due to their \u2026"}, {"title": "Evaluating Bayesian deep learning for radio galaxy classification", "link": "https://arxiv.org/pdf/2405.18351", "details": "D Mohan, AMM Scaife - arXiv preprint arXiv:2405.18351, 2024", "abstract": "The radio astronomy community is rapidly adopting deep learning techniques to deal with the huge data volumes expected from the next generation of radio observatories. Bayesian neural networks (BNNs) provide a principled way to model \u2026"}, {"title": "FedHPL: Efficient Heterogeneous Federated Learning with Prompt Tuning and Logit Distillation", "link": "https://arxiv.org/pdf/2405.17267", "details": "Y Ma, L Cheng, Y Wang, Z Zhong, X Xu, M Wang - arXiv preprint arXiv:2405.17267, 2024", "abstract": "Federated learning (FL) is a popular privacy-preserving paradigm that enables distributed clients to collaboratively train models with a central server while keeping raw data locally. In practice, distinct model architectures, varying data distributions \u2026"}, {"title": "Large language models can be zero-shot anomaly detectors for time series?", "link": "https://arxiv.org/pdf/2405.14755", "details": "S Alnegheimish, L Nguyen, L Berti-Equille\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent studies have shown the ability of large language models to perform a variety of tasks, including time series forecasting. The flexible nature of these models allows them to be used for many applications. In this paper, we present a novel study of \u2026"}]
