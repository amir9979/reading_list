[{"title": "Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?", "link": "https://arxiv.org/pdf/2406.16316", "details": "Y Jinnai - arXiv preprint arXiv:2406.16316, 2024", "abstract": "Alignment of the language model with human preferences is a common approach to making a language model useful to end users. However, most alignment work is done in English, and human preference datasets are dominated by English \u2026"}, {"title": "On Pre-training of Multimodal Language Models Customized for Chart Understanding", "link": "https://arxiv.org/pdf/2407.14506", "details": "WC Fan, YC Chen, M Liu, L Yuan, L Sigal - arXiv preprint arXiv:2407.14506, 2024", "abstract": "Recent studies customizing Multimodal Large Language Models (MLLMs) for domain-specific tasks have yielded promising results, especially in the field of scientific chart comprehension. These studies generally utilize visual instruction \u2026"}, {"title": "Scaling Laws for Linear Complexity Language Models", "link": "https://arxiv.org/pdf/2406.16690", "details": "X Shen, D Li, R Leng, Z Qin, W Sun, Y Zhong - arXiv preprint arXiv:2406.16690, 2024", "abstract": "The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their \u2026"}, {"title": "Meta-GPS++: Enhancing Graph Meta-Learning with Contrastive Learning and Self-Training", "link": "https://dl.acm.org/doi/abs/10.1145/3679018", "details": "Y Liu, M Li, X Li, L Huang, F Giunchiglia, Y Liang\u2026 - ACM Transactions on Knowledge \u2026", "abstract": "Node classification is an essential problem in graph learning. However, many models typically obtain unsatisfactory performance when applied to few-shot scenarios. Some studies have attempted to combine meta-learning with graph neural \u2026"}, {"title": "Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization", "link": "https://arxiv.org/pdf/2406.16743", "details": "Z Zhao, X Zhang, K Xu, X Hu, R Zhang, Z Du, Q Guo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the widespread application of Large Language Models (LLMs), it has become a significant concern to ensure their safety and prevent harmful responses. While current safe-alignment methods based on instruction fine-tuning and Reinforcement \u2026"}, {"title": "Adversarial Black Box Attacks to Disrupt Large Language Models via Reinforcement Learning", "link": "https://practical-dl.github.io/2024/short_paper/4/CameraReady/4.pdf", "details": "WTJ Le, LJ Sern, YXM Tan", "abstract": "Large Language Models (LLMs) are effective in solving natural language processing (NLP) tasks, ie question answering and text generation. Recent works showed the possibility of generating adversarial suffixes to get valid responses from LLMs to \u2026"}, {"title": "M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models", "link": "https://arxiv.org/pdf/2406.16783", "details": "R Maheshwary, V Yadav, H Nguyen, K Mahajan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Instruction finetuning (IFT) is critical for aligning Large Language Models (LLMs) to follow instructions. Numerous effective IFT datasets have been proposed in the recent past, but most focus on high resource languages such as English. In this work \u2026"}, {"title": "BEExAI: Benchmark to Evaluate Explainable AI", "link": "https://link.springer.com/chapter/10.1007/978-3-031-63787-2_23", "details": "S Sithakoul, S Meftah, C Feutry - World Conference on Explainable Artificial \u2026, 2024", "abstract": "Recent research in explainability has given rise to numerous post-hoc attribution methods aimed at enhancing our comprehension of the outputs of black-box machine learning models. However, evaluating the quality of explanations lacks a \u2026"}]
