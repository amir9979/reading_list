[{"title": "Empirical Evaluation of Large Language Models in Automated Program Repair", "link": "https://arxiv.org/pdf/2506.13186", "details": "J Sun, F Li, X Qi, H Zhang, J Jiang - arXiv preprint arXiv:2506.13186, 2025", "abstract": "\u2026 The rapid evolution of **large** **language** **models** (LLMs) offers new opportunities for advancing \u2026 To address this gap, we conducted a comprehensive empirical study **evaluating** four rep\u2026 Our **evaluation** spans (1) two distinct bug scenarios (enterprise-grades \u2026", "entry_id": "http://arxiv.org/abs/2506.13186v1", "updated": "2025-06-16 07:52:15", "published": "2025-06-16 07:52:15", "authors": "Jiajun Sun;Fengjie Li;Xinzhu Qi;Hongyu Zhang;Jiajun Jiang", "summary": "The increasing prevalence of software bugs has made automated program repair\n(APR) a key research focus. Large language models (LLMs) offer new\nopportunities for APR, but existing studies mostly rely on smaller,\nearlier-generation models and Java benchmarks. The repair capabilities of\nmodern, large-scale LLMs across diverse languages and scenarios remain\nunderexplored. To address this, we conduct a comprehensive empirical study of\nfour open-source LLMs, CodeLlama, LLaMA, StarCoder, and DeepSeek-Coder,\nspanning 7B to 33B parameters, diverse architectures, and purposes. We evaluate\nthem across two bug scenarios (enterprise-grades and algorithmic), three\nlanguages (Java, C/C++, Python), and four prompting strategies, analyzing over\n600K generated patches on six benchmarks. Key findings include: (1) model\nspecialization (e.g., CodeLlama) can outperform larger general-purpose models\n(e.g., LLaMA); (2) repair performance does not scale linearly with model size;\n(3) correct patches often appear early in generation; and (4) prompts\nsignificantly affect results. These insights offer practical guidance for\ndesigning effective and efficient LLM-based APR systems.", "comment": null, "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE", "links": "http://arxiv.org/abs/2506.13186v1;http://arxiv.org/pdf/2506.13186v1", "pdf_url": "http://arxiv.org/pdf/2506.13186v1"}, {"title": "CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios", "link": "https://arxiv.org/pdf/2506.13977", "details": "S Huang, Z Fang, Z Chen, S Yuan, J Ye, Y Zeng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 We conduct **evaluations** on CRITICTOOL using a diverse set of 14 LLMs, to establish a comprehensive self-critique benchmark for assessing the capabilities of current **large** **language** **models**. For closed-source LLMs, we select three prominent \u2026", "entry_id": "http://arxiv.org/abs/2506.13977v1", "updated": "2025-06-11 17:59:18", "published": "2025-06-11 17:59:18", "authors": "Shiting Huang;Zhen Fang;Zehui Chen;Siyu Yuan;Junjie Ye;Yu Zeng;Lin Chen;Qi Mao;Feng Zhao", "summary": "The ability of large language models (LLMs) to utilize external tools has\nenabled them to tackle an increasingly diverse range of tasks. However, as the\ntasks become more complex and long-horizon, the intricate tool utilization\nprocess may trigger various unexpected errors. Therefore, how to effectively\nhandle such errors, including identifying, diagnosing, and recovering from\nthem, has emerged as a key research direction for advancing tool learning. In\nthis work, we first extensively analyze the types of errors encountered during\nthe function-calling process on several competitive tool evaluation benchmarks.\nBased on it, we introduce CRITICTOOL, a comprehensive critique evaluation\nbenchmark specialized for tool learning. Building upon a novel evolutionary\nstrategy for dataset construction, CRITICTOOL holds diverse tool-use errors\nwith varying complexities, which better reflects real-world scenarios. We\nconduct extensive experiments on CRITICTOOL, and validate the generalization\nand effectiveness of our constructed benchmark strategy. We also provide an\nin-depth analysis of the tool reflection ability on various LLMs, offering a\nnew perspective on the field of tool learning in LLMs. The code is available at\n\\href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}.", "comment": null, "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE;cs.CL", "links": "http://arxiv.org/abs/2506.13977v1;http://arxiv.org/pdf/2506.13977v1", "pdf_url": "http://arxiv.org/pdf/2506.13977v1"}, {"title": "Confidence-linked and uncertainty-based staged framework for phenotype validation using **large language models**", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocaf099/8165643", "details": "S Lee, HH Lee, H Lee, KS Yum, JH Baek, J Khil, J Lee\u2026 - Journal of the American \u2026, 2025", "abstract": "\u2026 Fourth, although CLUES framework demonstrated promising performance, further **evaluation** is necessary to ensure robustness in real-world \u2026 to enhance interpretability, their validity was not formally **evaluated** , as rationale **evaluation** was \u2026"}, {"title": "RealFactBench: A Benchmark for Evaluating Large Language Models in Real-World Fact-Checking", "link": "https://arxiv.org/pdf/2506.12538", "details": "S Yang, Y Dai, G Wang, X Zheng, J Xu, J Li, Z Ying\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) hold significant potential for advancing fact-checking by \u2026 fail to comprehensively **evaluate** LLMs and Multimodal **Large** **Language** **Models** (MLLMs) in \u2026 Our **evaluation** framework further introduces the Unknown \u2026", "entry_id": "http://arxiv.org/abs/2506.12538v1", "updated": "2025-06-14 15:27:44", "published": "2025-06-14 15:27:44", "authors": "Shuo Yang;Yuqin Dai;Guoqing Wang;Xinran Zheng;Jinfeng Xu;Jinze Li;Zhenzhe Ying;Weiqiang Wang;Edith C. H. Ngai", "summary": "Large Language Models (LLMs) hold significant potential for advancing\nfact-checking by leveraging their capabilities in reasoning, evidence\nretrieval, and explanation generation. However, existing benchmarks fail to\ncomprehensively evaluate LLMs and Multimodal Large Language Models (MLLMs) in\nrealistic misinformation scenarios. To bridge this gap, we introduce\nRealFactBench, a comprehensive benchmark designed to assess the fact-checking\ncapabilities of LLMs and MLLMs across diverse real-world tasks, including\nKnowledge Validation, Rumor Detection, and Event Verification. RealFactBench\nconsists of 6K high-quality claims drawn from authoritative sources,\nencompassing multimodal content and diverse domains. Our evaluation framework\nfurther introduces the Unknown Rate (UnR) metric, enabling a more nuanced\nassessment of models' ability to handle uncertainty and balance between\nover-conservatism and over-confidence. Extensive experiments on 7\nrepresentative LLMs and 4 MLLMs reveal their limitations in real-world\nfact-checking and offer valuable insights for further research. RealFactBench\nis publicly available at https://github.com/kalendsyang/RealFactBench.git.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.12538v1;http://arxiv.org/pdf/2506.12538v1", "pdf_url": "http://arxiv.org/pdf/2506.12538v1"}, {"title": "Language Surgery in Multilingual Large Language Models", "link": "https://arxiv.org/pdf/2506.12450", "details": "JA Lopo, MRS Habibi, TH Wong, MI Ghozali, F Koto\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **evaluation** with native annotators on English (EN), Indonesian (ID), and Thai (TH) in both EN\u2192XX and XX\u2192EN directions to further validate the generation quality. The human **evaluation** \u2026 Mitigating Cross-lingual Language Confusion We \u2026", "entry_id": "http://arxiv.org/abs/2506.12450v1", "updated": "2025-06-14 11:09:50", "published": "2025-06-14 11:09:50", "authors": "Joanito Agili Lopo;Muhammad Ravi Shulthan Habibi;Tack Hwa Wong;Muhammad Ilham Ghozali;Fajri Koto;Genta Indra Winata;Peerat Limkonchotiwat;Alham Fikri Aji;Samuel Cahyawijaya", "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across tasks and languages, revolutionizing natural language\nprocessing. This paper investigates the naturally emerging representation\nalignment in LLMs, particularly in the middle layers, and its implications for\ndisentangling language-specific and language-agnostic information. We\nempirically confirm the existence of this alignment, analyze its behavior in\ncomparison to explicitly designed alignment models, and demonstrate its\npotential for language-specific manipulation without semantic degradation.\nBuilding on these findings, we propose Inference-Time Language Control (ITLC),\na novel method that leverages latent injection to enable precise cross-lingual\nlanguage control and mitigate language confusion in LLMs. Our experiments\nhighlight ITLC's strong cross-lingual control capabilities while preserving\nsemantic integrity in target languages. Furthermore, we demonstrate its\neffectiveness in alleviating the cross-lingual language confusion problem,\nwhich persists even in current large-scale LLMs, leading to inconsistent\nlanguage generation. This work advances our understanding of representation\nalignment in LLMs and introduces a practical solution for enhancing their\ncross-lingual performance.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.12450v1;http://arxiv.org/pdf/2506.12450v1", "pdf_url": "http://arxiv.org/pdf/2506.12450v1"}, {"title": "Exploring the Secondary Risks of Large Language Models", "link": "https://arxiv.org/pdf/2506.12382", "details": "J Chen, Z Fang, X Yang, C Yu, Z Yin, H Su - arXiv preprint arXiv:2506.12382, 2025", "abstract": "\u2026 Ensuring the safety and alignment of **Large** **Language** **Models** (LLMs) is a significant challenge \u2026 To enable systematic **evaluation** , we introduce two risk primitives\u2014verbose response and \u2026 \": Characterizing and **evaluating** in-the-wild jailbreak prompts on \u2026", "entry_id": "http://arxiv.org/abs/2506.12382v1", "updated": "2025-06-14 07:31:52", "published": "2025-06-14 07:31:52", "authors": "Jiawei Chen;Zhengwei Fang;Xiao Yang;Chao Yu;Zhaoxia Yin;Hang Su", "summary": "Ensuring the safety and alignment of Large Language Models is a significant\nchallenge with their growing integration into critical applications and\nsocietal functions. While prior research has primarily focused on jailbreak\nattacks, less attention has been given to non-adversarial failures that subtly\nemerge during benign interactions. We introduce secondary risks a novel class\nof failure modes marked by harmful or misleading behaviors during benign\nprompts. Unlike adversarial attacks, these risks stem from imperfect\ngeneralization and often evade standard safety mechanisms. To enable systematic\nevaluation, we introduce two risk primitives verbose response and speculative\nadvice that capture the core failure patterns. Building on these definitions,\nwe propose SecLens, a black-box, multi-objective search framework that\nefficiently elicits secondary risk behaviors by optimizing task relevance, risk\nactivation, and linguistic plausibility. To support reproducible evaluation, we\nrelease SecRiskBench, a benchmark dataset of 650 prompts covering eight diverse\nreal-world risk categories. Experimental results from extensive evaluations on\n16 popular models demonstrate that secondary risks are widespread, transferable\nacross models, and modality independent, emphasizing the urgent need for\nenhanced safety mechanisms to address benign yet harmful LLM behaviors in\nreal-world deployments.", "comment": "18 pages, 5 figures", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CR", "links": "http://arxiv.org/abs/2506.12382v1;http://arxiv.org/pdf/2506.12382v1", "pdf_url": "http://arxiv.org/pdf/2506.12382v1"}, {"title": "Evaluating Large Language Models for Phishing Detection, Self-Consistency, Faithfulness, and Explainability", "link": "https://arxiv.org/pdf/2506.13746", "details": "S Kuikel, A Piplai, P Aggarwal - arXiv preprint arXiv:2506.13746, 2025", "abstract": "\u2026 By keeping only query and value as target modules for LoRA, we fine-tuned our three different **large** **language** **models** : Llama-2-7B, Llama-3-8B, Wizard-7B. Additionally, we also included BERT model for fine tuning with general architecture as it is \u2026", "entry_id": "http://arxiv.org/abs/2506.13746v1", "updated": "2025-06-16 17:54:28", "published": "2025-06-16 17:54:28", "authors": "Shova Kuikel;Aritran Piplai;Palvi Aggarwal", "summary": "Phishing attacks remain one of the most prevalent and persistent\ncybersecurity threat with attackers continuously evolving and intensifying\ntactics to evade the general detection system. Despite significant advances in\nartificial intelligence and machine learning, faithfully reproducing the\ninterpretable reasoning with classification and explainability that underpin\nphishing judgments remains challenging. Due to recent advancement in Natural\nLanguage Processing, Large Language Models (LLMs) show a promising direction\nand potential for improving domain specific phishing classification tasks.\nHowever, enhancing the reliability and robustness of classification models\nrequires not only accurate predictions from LLMs but also consistent and\ntrustworthy explanations aligning with those predictions. Therefore, a key\nquestion remains: can LLMs not only classify phishing emails accurately but\nalso generate explanations that are reliably aligned with their predictions and\ninternally self-consistent? To answer these questions, we have fine-tuned\ntransformer based models, including BERT, Llama models, and Wizard, to improve\ndomain relevance and make them more tailored to phishing specific distinctions,\nusing Binary Sequence Classification, Contrastive Learning (CL) and Direct\nPreference Optimization (DPO). To that end, we examined their performance in\nphishing classification and explainability by applying the ConsistenCy measure\nbased on SHAPley values (CC SHAP), which measures prediction explanation token\nalignment to test the model's internal faithfulness and consistency and uncover\nthe rationale behind its predictions and reasoning. Overall, our findings show\nthat Llama models exhibit stronger prediction explanation token alignment with\nhigher CC SHAP scores despite lacking reliable decision making accuracy,\nwhereas Wizard achieves better prediction accuracy but lower CC SHAP scores.", "comment": null, "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2506.13746v1;http://arxiv.org/pdf/2506.13746v1", "pdf_url": "http://arxiv.org/pdf/2506.13746v1"}, {"title": "Quality Assessment of Python Tests Generated by Large Language Models", "link": "https://arxiv.org/pdf/2506.14297", "details": "V Alves, C Bezerra, I Machado, L Rocha, T Virg\u00ednio\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) have shown great promise in this domain, leveraging their \u2026 We **evaluate** the structural reliability of test suites generated under two distinct prompt \u2026 Among the **evaluated** LLMs, GPT-4o produced the \u2026", "entry_id": "http://arxiv.org/abs/2506.14297v1", "updated": "2025-06-17 08:16:15", "published": "2025-06-17 08:16:15", "authors": "Victor Alves;Carla Bezerra;Ivan Machado;Larissa Rocha;T\u00e1ssio Virg\u00ednio;Publio Silva", "summary": "The manual generation of test scripts is a time-intensive, costly, and\nerror-prone process, indicating the value of automated solutions. Large\nLanguage Models (LLMs) have shown great promise in this domain, leveraging\ntheir extensive knowledge to produce test code more efficiently. This study\ninvestigates the quality of Python test code generated by three LLMs: GPT-4o,\nAmazon Q, and LLama 3.3. We evaluate the structural reliability of test suites\ngenerated under two distinct prompt contexts: Text2Code (T2C) and Code2Code\n(C2C). Our analysis includes the identification of errors and test smells, with\na focus on correlating these issues to inadequate design patterns. Our findings\nreveal that most test suites generated by the LLMs contained at least one error\nor test smell. Assertion errors were the most common, comprising 64% of all\nidentified errors, while the test smell Lack of Cohesion of Test Cases was the\nmost frequently detected (41%). Prompt context significantly influenced test\nquality; textual prompts with detailed instructions often yielded tests with\nfewer errors but a higher incidence of test smells. Among the evaluated LLMs,\nGPT-4o produced the fewest errors in both contexts (10% in C2C and 6% in T2C),\nwhereas Amazon Q had the highest error rates (19% in C2C and 28% in T2C). For\ntest smells, Amazon Q had fewer detections in the C2C context (9%), while LLama\n3.3 performed best in the T2C context (10%). Additionally, we observed a strong\nrelationship between specific errors, such as assertion or indentation issues,\nand test case cohesion smells. These findings demonstrate opportunities for\nimproving the quality of test generation by LLMs and highlight the need for\nfuture research to explore optimized generation scenarios and better prompt\nengineering strategies.", "comment": "International Conference on Evaluation and Assessment in Software\n  Engineering (EASE), 2025 edition", "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE", "links": "http://arxiv.org/abs/2506.14297v1;http://arxiv.org/pdf/2506.14297v1", "pdf_url": "http://arxiv.org/pdf/2506.14297v1"}, {"title": "Democratic or Authoritarian? Probing a New Dimension of Political Biases in Large Language Models", "link": "https://arxiv.org/pdf/2506.12758", "details": "DG Piedrahita, I Strauss, B Sch\u00f6lkopf, R Mihalcea\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 To capture variation across linguistic, cultural, and architectural dimensions, we **evaluate** a diverse set of LLMs that differ in training data and developer origin. We test models released by different geographical regions, including GPT-4o, Claude \u2026", "entry_id": "http://arxiv.org/abs/2506.12758v1", "updated": "2025-06-15 07:52:07", "published": "2025-06-15 07:52:07", "authors": "David Guzman Piedrahita;Irene Strauss;Bernhard Sch\u00f6lkopf;Rada Mihalcea;Zhijing Jin", "summary": "As Large Language Models (LLMs) become increasingly integrated into everyday\nlife and information ecosystems, concerns about their implicit biases continue\nto persist. While prior work has primarily examined socio-demographic and\nleft--right political dimensions, little attention has been paid to how LLMs\nalign with broader geopolitical value systems, particularly the\ndemocracy--authoritarianism spectrum. In this paper, we propose a novel\nmethodology to assess such alignment, combining (1) the F-scale, a psychometric\ntool for measuring authoritarian tendencies, (2) FavScore, a newly introduced\nmetric for evaluating model favorability toward world leaders, and (3)\nrole-model probing to assess which figures are cited as general role-models by\nLLMs. We find that LLMs generally favor democratic values and leaders, but\nexhibit increases favorability toward authoritarian figures when prompted in\nMandarin. Further, models are found to often cite authoritarian figures as role\nmodels, even outside explicit political contexts. These results shed light on\nways LLMs may reflect and potentially reinforce global political ideologies,\nhighlighting the importance of evaluating bias beyond conventional\nsocio-political axes. Our code is available at:\nhttps://github.com/irenestrauss/Democratic-Authoritarian-Bias-LLMs", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.12758v1;http://arxiv.org/pdf/2506.12758v1", "pdf_url": "http://arxiv.org/pdf/2506.12758v1"}]
