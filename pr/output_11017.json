[{"title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration", "link": "https://arxiv.org/pdf/2412.13551", "details": "X Zuo, M Wang, T Zhu, S Yu, W Zhou - arXiv preprint arXiv:2412.13551, 2024", "abstract": "Large language models (LLMs) have transformed the way computers understand and process human language, but using them effectively across different organizations remains still difficult. When organizations work together to improve \u2026"}, {"title": "Is Split Learning Privacy-Preserving for Fine-Tuning Large Language Models?", "link": "https://ieeexplore.ieee.org/abstract/document/10818584/", "details": "D Yao, B Li - IEEE Transactions on Big Data, 2024", "abstract": "With the success of pre-trained large language models in various tasks, users, individuals and enterprises alike, may need to fine-tune these models with their own datasets. Split learning was proposed to divide the model and place a portion on \u2026"}, {"title": "LLM2: Let Large Language Models Harness System 2 Reasoning", "link": "https://arxiv.org/pdf/2412.20372", "details": "C Yang, C Shi, S Li, B Shui, Y Yang, W Lam - arXiv preprint arXiv:2412.20372, 2024", "abstract": "Large language models (LLMs) have exhibited impressive capabilities across a myriad of tasks, yet they occasionally yield undesirable outputs. We posit that these limitations are rooted in the foundational autoregressive architecture of LLMs, which \u2026"}]
