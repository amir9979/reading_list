[{"title": "Too Big to Fool: Resisting Deception in Language Models", "link": "https://arxiv.org/pdf/2412.10558", "details": "MR Samsami, ML Richter, J Rodriguez, M Thakkar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models must balance their weight-encoded knowledge with in- context information from prompts to generate accurate responses. This paper investigates this interplay by analyzing how models of varying capacities within the \u2026"}, {"title": "Refining Salience-Aware Sparse Fine-Tuning Strategies for Language Models", "link": "https://arxiv.org/pdf/2412.13488", "details": "X Liu, A Thomas, C Zhang, J Cheng, Y Zhao, X Gao - arXiv preprint arXiv:2412.13488, 2024", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has gained prominence through low-rank adaptation methods like LoRA. In this paper, we focus on sparsity-based PEFT (SPEFT), which introduces trainable sparse adaptations to the weight matrices in the \u2026"}, {"title": "Do Large Language Models have Shared Weaknesses in Medical Question Answering?", "link": "https://openreview.net/pdf%3Fid%3DZjQ04tsRQl", "details": "AM Bean, K Korgul, F Krones, R McCraith, A Mahdi - Advancements In Medical \u2026, 2024", "abstract": "Large language models (LLMs) have made rapid improvement on medical benchmarks, but their unreliability remains a persistent challenge for safe real-world uses. To design for the use LLMs as a category, rather than for specific models \u2026"}, {"title": "ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models", "link": "https://arxiv.org/pdf/2412.07012", "details": "J Zhang, L Xue, L Song, J Wang, W Huang, M Shu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the rise of multimodal applications, instruction data has become critical for training multimodal language models capable of understanding complex image- based queries. Existing practices rely on powerful but costly large language models \u2026"}, {"title": "PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.09613%3F", "details": "C Yang, X Dong, X Zhu, W Su, J Wang, H Tian, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (VLMs) have been extended to understand both images and videos. Visual token compression is leveraged to reduce the considerable token length of visual inputs. To meet the needs of different tasks \u2026"}, {"title": "V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding", "link": "https://arxiv.org/pdf/2412.09616%3F", "details": "J Ge, Z Chen, J Lin, J Zhu, X Liu, J Dai, X Zhu - arXiv preprint arXiv:2412.09616, 2024", "abstract": "Vision-Language Models (VLMs) have shown promising capabilities in handling various multimodal tasks, yet they struggle in long-context scenarios, particularly in tasks involving videos, high-resolution images, or lengthy image-text documents. In \u2026"}, {"title": "FastVLM: Efficient Vision Encoding for Vision Language Models", "link": "https://arxiv.org/pdf/2412.13303", "details": "PKA Vasu, F Faghri, CL Li, C Koc, N True, A Antony\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high \u2026"}, {"title": "CTPT: Continual Test-time Prompt Tuning for vision-language models", "link": "https://www.sciencedirect.com/science/article/pii/S0031320324010513", "details": "F Wang, Z Han, X Liu, Y Yin, X Gao - Pattern Recognition, 2024", "abstract": "Abstract Test-time Prompt Tuning (TPT) aims to further enhance the generalization capabilities of pre-trained vision-language models, eg, CLIP, on streaming test samples from a new distribution. Current TPT methods primarily utilize self-training \u2026"}, {"title": "RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models", "link": "https://arxiv.org/pdf/2412.11050", "details": "Y Wang, Q Liu, J Fan, J Hong, H Chu, M Tian, B Gao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Understanding and addressing corner cases is essential for ensuring the safety and reliability of autonomous driving systems. Vision-Language Models (VLMs) play a crucial role in enhancing scenario comprehension, yet they face significant \u2026"}]
