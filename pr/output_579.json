'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Prevalence and risk factors for long COVID among adul'
[{"title": "MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering", "link": "https://arxiv.org/pdf/2403.19116", "details": "C Guan, M Huang, P Zhang - arXiv preprint arXiv:2403.19116, 2024", "abstract": "In today's fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested \u2026"}, {"title": "EVALUATION OF MEDIUM-SIZED LANGUAGE MODELS IN GERMAN AND ENGLISH LANGUAGE", "link": "https://www.researchgate.net/profile/Seth-Darren/publication/378909184_Evaluation_of_Medium-Sized_Language_Models_in_German_and_English_Language/links/65f18aa3c05fd26880074304/Evaluation-of-Medium-Sized-Language-Models-in-German-and-English-Language.pdf", "details": "R Peinl, J Wirth", "abstract": "Large language models (LLMs) have garnered significant attention, but the definition of \u201clarge\u201d lacks clarity. This paper focuses on medium-sized language models (MLMs), defined as having at least six billion parameters but less than 100 billion \u2026"}, {"title": "Jamba: A Hybrid Transformer-Mamba Language Model", "link": "https://arxiv.org/pdf/2403.19887", "details": "O Lieber, B Lenz, H Bata, G Cohen, J Osin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both \u2026"}, {"title": "Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models", "link": "https://arxiv.org/pdf/2403.17359", "details": "Z Pan, H Luo, M Li, H Liu - arXiv preprint arXiv:2403.17359, 2024", "abstract": "We present a Chain-of-Action (CoA) framework for multimodal and retrieval- augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications:(i) unfaithful hallucination that is \u2026"}]
