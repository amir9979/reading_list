[{"title": "EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models", "link": "https://arxiv.org/pdf/2502.06663", "details": "X Xing, Z Liu, S Xiao, B Gao, Y Liang, W Zhang, H Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Modern large language models (LLMs) driven by scaling laws, achieve intelligence emergency in large model sizes. Recently, the increasing concerns about cloud costs, latency, and privacy make it an urgent requirement to develop compact edge \u2026"}, {"title": "DeepThink: Aligning Language Models with Domain-Specific User Intents", "link": "https://arxiv.org/pdf/2502.05497", "details": "Y Li, M Luo, Y Gong, C Lin, J Jiao, Y Liu, K Huang - arXiv preprint arXiv:2502.05497, 2025", "abstract": "Supervised fine-tuning with synthesized instructions has been a common practice for adapting LLMs to domain-specific QA tasks. However, the synthesized instructions deviate from real user questions and expected answers. This study proposes a novel \u2026"}, {"title": "Scaling Embedding Layers in Language Models", "link": "https://arxiv.org/pdf/2502.01637%3F", "details": "D Yu, E Cohen, B Ghazi, Y Huang, P Kamath, R Kumar\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We propose SCONE ($\\textbf {S} $ calable, $\\textbf {C} $ ontextualized, $\\textbf {O} $ ffloaded, $\\textbf {N} $-gram $\\textbf {E} $ mbedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To \u2026"}, {"title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models", "link": "https://arxiv.org/pdf/2502.04404", "details": "XW Yang, XY Zhu, WD Wei, DC Zhang, JJ Shao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including \u2026"}, {"title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06788", "details": "H Diao, X Li, Y Cui, Y Wang, H Deng, T Pan, W Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient \u2026"}, {"title": "Cost-Efficient Domain-Adaptive Pretraining of Language Models for Optoelectronics Applications", "link": "https://pubs.acs.org/doi/full/10.1021/acs.jcim.4c02029", "details": "D Huang, JM Cole - Journal of Chemical Information and Modeling, 2025", "abstract": "Pretrained language models have demonstrated strong capability and versatility in natural language processing (NLP) tasks, and they have important applications in optoelectronics research, such as data mining and topic modeling. Many language \u2026"}, {"title": "Rethinking Homogeneity of Vision and Text Tokens in Large Vision-and-Language Models", "link": "https://arxiv.org/pdf/2502.01906", "details": "CW Kuo, S Zhu, F Chen, X Shen, L Wen - arXiv preprint arXiv:2502.01906, 2025", "abstract": "Large vision-and-language models (LVLMs) typically treat visual and textual embeddings as homogeneous inputs to a large language model (LLM). However, these inputs are inherently different: visual inputs are multi-dimensional and \u2026"}, {"title": "SHARP: Accelerating Language Model Inference by SHaring Adjacent layers with Recovery Parameters", "link": "https://arxiv.org/pdf/2502.07832", "details": "Y Wang, H Huang, Y Chen, J Zhao, SS Du, Y Tian - arXiv preprint arXiv:2502.07832, 2025", "abstract": "While Large language models (LLMs) have advanced natural language processing tasks, their growing computational and memory demands make deployment on resource-constrained devices like mobile phones increasingly challenging. In this \u2026"}, {"title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG", "link": "https://arxiv.org/pdf/2502.08356", "details": "K Bhushan, Y Nandwani, D Khandelwal, S Gupta\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the \u2026"}]
