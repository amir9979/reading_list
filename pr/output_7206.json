[{"title": "FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs", "link": "https://arxiv.org/pdf/2409.13612", "details": "B Yan, Z Zhang, L Jing, E Hossain, X Du - arXiv preprint arXiv:2409.13612, 2024", "abstract": "The rapid development of Large Vision-Language Models (LVLMs) often comes with widespread hallucination issues, making cost-effective and comprehensive assessments increasingly vital. Current approaches mainly rely on costly annotations \u2026"}, {"title": "Interpreting and Controlling Linguistic Features in Multilingual Language Models", "link": "https://dspace.cuni.cz/bitstream/handle/20.500.11956/192821/140123221.pdf%3Fsequence%3D1", "details": "T Limisiewicz - 2024", "abstract": "Language models based on neural networks have become the foundation for solving diverse tasks, yet their inner workings remain opaque. This dissertation investigates which components of language models are crucial for representing and processing \u2026"}, {"title": "CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks", "link": "https://arxiv.org/pdf/2409.03381", "details": "Y Deng, X Qiu, X Tan, C Qu, J Pan, Y Cheng, Y Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Cognitive psychology investigates perception, attention, memory, language, problem- solving, decision-making, and reasoning. Kahneman's dual-system theory elucidates the human decision-making process, distinguishing between the rapid, intuitive \u2026"}, {"title": "Targeted training for numerical reasoning with large language models", "link": "https://link.springer.com/article/10.1007/s10115-024-02216-1", "details": "X Li, S Liu, Y Zhu, G Cheng - Knowledge and Information Systems, 2024", "abstract": "After recent gains achieved by large language models (LLMs) on numerical reasoning tasks, it has become of interest to have LLMs teach small models to improve on numerical reasoning. Instructing LLMs to generate Chains of Thought to \u2026"}]
