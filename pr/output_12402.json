[{"title": "Guiding Medical Vision-Language Models with Explicit Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations", "link": "https://arxiv.org/pdf/2501.02385", "details": "K Zhu, Z Qin, H Yi, Z Jiang, Q Lao, S Zhang, K Li - arXiv preprint arXiv:2501.02385, 2025", "abstract": "With the recent advancements in vision-language models (VLMs) driven by large language models (LLMs), many researchers have focused on models that comprised of an image encoder, an image-to-language projection layer, and a text decoder \u2026"}, {"title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2501.12370", "details": "S Abnar, H Shah, D Busbridge, AME Ali, J Susskind\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Scaling the capacity of language models has consistently proven to be a reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the \u2026"}, {"title": "Linguistic Entity Masking to Improve Cross-Lingual Representation of Multilingual Language Models for Low-Resource Languages", "link": "https://arxiv.org/pdf/2501.05700", "details": "A Fernando, S Ranathunga - arXiv preprint arXiv:2501.05700, 2025", "abstract": "Multilingual Pre-trained Language models (multiPLMs), trained on the Masked Language Modelling (MLM) objective are commonly being used for cross-lingual tasks such as bitext mining. However, the performance of these models is still \u2026"}, {"title": "DiffuSETS: 12-lead ECG Generation Conditioned on Clinical Text Reports and Patient-Specific Information", "link": "https://arxiv.org/pdf/2501.05932", "details": "Y Lai, J Chen, D Zhang, Y Wang, S Geng, H Li, S Hong - arXiv preprint arXiv \u2026, 2025", "abstract": "Heart disease remains a significant threat to human health. As a non-invasive diagnostic tool, the electrocardiogram (ECG) is one of the most widely used methods for cardiac screening. However, the scarcity of high-quality ECG data, driven by \u2026"}, {"title": "Clear Minds Think Alike: What Makes LLM Fine-tuning Robust? A Study of Token Perplexity", "link": "https://arxiv.org/pdf/2501.14315%3F", "details": "CC Wu, ZR Tam, CY Lin, H Lee, YN Chen - arXiv preprint arXiv:2501.14315, 2025", "abstract": "Maintaining consistent model performance across domains is a fundamental challenge in machine learning. While recent work has explored using LLM- generated data for fine-tuning, its impact on cross-domain generalization remains \u2026"}]
