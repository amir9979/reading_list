[{"title": "CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs", "link": "https://arxiv.org/pdf/2408.10433", "details": "Y Ouali, A Bulat, B Martinez, G Tzimiropoulos - arXiv preprint arXiv:2408.10433, 2024", "abstract": "Despite recent successes, LVLMs or Large Vision Language Models are prone to hallucinating details like objects and their properties or relations, limiting their real- world deployment. To address this and improve their robustness, we present CLIP \u2026"}, {"title": "TableBench: A Comprehensive and Complex Benchmark for Table Question Answering", "link": "https://arxiv.org/pdf/2408.09174", "details": "X Wu, J Yang, L Chai, G Zhang, J Liu, X Du, D Liang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in Large Language Models (LLMs) have markedly enhanced the interpretation and processing of tabular data, introducing previously unimaginable capabilities. Despite these achievements, LLMs still encounter \u2026"}, {"title": "How Well Can Vision Language Models See Image Details?", "link": "https://arxiv.org/pdf/2408.03940", "details": "C Gou, A Felemban, FF Khan, D Zhu, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Model-based Vision-Language Models (LLM-based VLMs) have demonstrated impressive results in various vision-language understanding tasks. However, how well these VLMs can see image detail beyond the semantic level \u2026"}, {"title": "Making Long-Context Language Models Better Multi-Hop Reasoners", "link": "https://arxiv.org/pdf/2408.03246", "details": "Y Li, S Liang, MR Lyu, L Wang - arXiv preprint arXiv:2408.03246, 2024", "abstract": "Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased \u2026"}, {"title": "Contrastive Learning on Medical Intents for Sequential Prescription Recommendation", "link": "https://arxiv.org/pdf/2408.10259", "details": "AH Moghaddam, MN Kerdabadi, M Liu, Z Yao - arXiv preprint arXiv:2408.10259, 2024", "abstract": "Recent advancements in sequential modeling applied to Electronic Health Records (EHR) have greatly influenced prescription recommender systems. While the recent literature on drug recommendation has shown promising performance, the study of \u2026"}, {"title": "MedSyn: LLM-based Synthetic Medical Text Generation Framework", "link": "https://arxiv.org/pdf/2408.02056", "details": "G Kumichev, P Blinov, Y Kuzkina, V Goncharov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating synthetic text addresses the challenge of data availability in privacy- sensitive domains such as healthcare. This study explores the applicability of synthetic data in real-world medical settings. We introduce MedSyn, a novel medical \u2026"}, {"title": "MixPrompt: Enhancing Generalizability and Adversarial Robustness for Vision-Language Models via Prompt Fusion", "link": "https://link.springer.com/chapter/10.1007/978-981-97-5606-3_28", "details": "H Fan, Z Ma, Y Li, R Tian, Y Chen, C Gao - International Conference on Intelligent \u2026, 2024", "abstract": "Abstract Pretrained Vision-Language Models (VLMs) like CLIP have exhibited remarkable capacities across downstream tasks, while their image encoders are vulnerable to adversarial examples. A recently introduced lightweight approach \u2026"}, {"title": "CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning", "link": "https://arxiv.org/pdf/2407.21011", "details": "Y Du, B Chang, NC Dvornek - arXiv preprint arXiv:2407.21011, 2024", "abstract": "Recent advancements in Contrastive Language-Image Pre-training (CLIP) have demonstrated notable success in self-supervised representation learning across various tasks. However, the existing CLIP-like approaches often demand extensive \u2026"}, {"title": "Entity Retrieval for Answering Entity-Centric Questions", "link": "https://arxiv.org/pdf/2408.02795", "details": "HS Shavarani, A Sarkar - arXiv preprint arXiv:2408.02795, 2024", "abstract": "The similarity between the question and indexed documents is a crucial factor in document retrieval for retrieval-augmented question answering. Although this is typically the only method for obtaining the relevant documents, it is not the sole \u2026"}]
