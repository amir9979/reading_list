'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Understanding emergent abilities of language models fr'
[{"title": "Language Models for Text Classification: Is In-Context Learning Enough?", "link": "https://arxiv.org/pdf/2403.17661", "details": "A Edwards, J Camacho-Collados - arXiv preprint arXiv:2403.17661, 2024", "abstract": "Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero-and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand \u2026"}, {"title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.07860", "details": "S Zhao, S Hao, B Zi, H Xu, KYK Wong - arXiv preprint arXiv:2403.07860, 2024", "abstract": "Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding \u2026"}, {"title": "$\\mathbf {(N, K)} $-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model", "link": "https://arxiv.org/html/2403.07191v1", "details": "Y Zhang, L Chen, B Liu, Y Yang, Q Cui, Y Tao, H Yang - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these \u2026"}, {"title": "Generative Language Models for Personalized Information Understanding", "link": "https://scholarworks.umass.edu/cgi/viewcontent.cgi%3Farticle%3D4123%26context%3Ddissertations_2", "details": "P Cai - 2024", "abstract": "A major challenge in information understanding stems from the diverse nature of the audience, where individuals possess varying preferences, experiences, educational and cultural backgrounds. Consequently, adopting a one-size-fits-all approach to \u2026"}, {"title": "Grounding and Enhancing Grid-based Models for Neural Fields", "link": "https://arxiv.org/html/2403.20002v1", "details": "Z Zhao, F Fan, W Liao, J Yan - arXiv preprint arXiv:2403.20002, 2024", "abstract": "Many contemporary studies utilize grid-based models for neural field representation, but a systematic analysis of grid-based models is still missing, hindering the improvement of those models. Therefore, this paper introduces a theoretical \u2026"}, {"title": "RAFT: Adapting Language Model to Domain Specific RAG", "link": "https://arxiv.org/pdf/2403.10131", "details": "T Zhang, SG Patil, N Jain, S Shen, M Zaharia, I Stoica\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (eg, time-critical news, or private \u2026"}, {"title": "Source-Aware Training Enables Knowledge Attribution in Language Models", "link": "https://arxiv.org/pdf/2404.01019", "details": "M Khalifa, D Wadden, E Strubell, H Lee, L Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source (s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining \u2026"}, {"title": "Invertible Diffusion Models for Compressed Sensing", "link": "https://arxiv.org/html/2403.17006v1/", "details": "B Chen, Z Zhang, W Li, C Zhao, J Yu, S Zhao, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although \u2026"}, {"title": "NeRF-MAE: Masked AutoEncoders for Self Supervised 3D Representation Learning for Neural Radiance Fields", "link": "https://arxiv.org/pdf/2404.01300", "details": "MZ Irshad, S Zakahrov, V Guizilini, A Gaidon, Z Kira\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we \u2026"}]
