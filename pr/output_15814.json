[{"title": "Insightbuddy-ai: Medication extraction and entity linking using pre-trained language models and ensemble learning", "link": "https://aclanthology.org/2025.naacl-srw.2.pdf", "details": "P Romero, L Han, G Nenadic - Proceedings of the 2025 Conference of the Nations of \u2026, 2025", "abstract": "This paper presents our system, InsightBuddy-AI, designed for extracting medication mentions and their associated attributes, and for linking these entities to established clinical terminology resources, including SNOMED-CT, the British National \u2026"}, {"title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models", "link": "https://arxiv.org/pdf/2504.12526", "details": "J Zhang, T Zhu, C Luo, A Anandkumar - arXiv preprint arXiv:2504.12526, 2025", "abstract": "Long-context language models exhibit impressive performance but remain challenging to deploy due to high GPU memory demands during inference. We propose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that \u2026"}, {"title": "GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models", "link": "https://aclanthology.org/2025.naacl-long.420.pdf", "details": "H Kohli, S Kumar, H Sun - Proceedings of the 2025 Conference of the Nations of \u2026, 2025", "abstract": "The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their reasoning to \u2026"}, {"title": "Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?", "link": "https://aclanthology.org/2025.findings-naacl.98.pdf", "details": "C Zhang, J Lu, VQ Tran, T Schuster, D Metzler, J Lin - Findings of the Association for \u2026, 2025", "abstract": "Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual \u2026"}, {"title": "Platonic Grounding for Efficient Multimodal Language Models", "link": "https://arxiv.org/pdf/2504.19327", "details": "M Choraria, X Wu, A Bhimaraju, N Sekhar, Y Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The hyperscaling of data and parameter count in Transformer-based models is yielding diminishing performance improvement, especially when weighed against training costs. Such plateauing indicates the importance of methods for more efficient \u2026"}, {"title": "Prototype Tuning: A Meta-Learning Approach for Few-Shot Document-Level Relation Extraction with Large Language Models", "link": "https://aclanthology.org/2025.findings-naacl.62.pdf", "details": "D Pan, Y Sun, B Xu, J Li, Z Yang, L Luo, H Lin, J Wang - Findings of the Association \u2026, 2025", "abstract": "Abstract Few-Shot Document-Level Relation Extraction (FSDLRE) aims to develop models capable of generalizing to new categories with minimal support examples. Although Large Language Models (LLMs) demonstrate exceptional In-Context \u2026"}, {"title": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "link": "https://www.biorxiv.org/content/10.1101/2025.04.14.648850.full.pdf", "details": "SA Rizvi, D Levine, A Patel, S Zhang, E Wang, S He\u2026 - bioRxiv, 2025", "abstract": "Single-cell RNA sequencing has transformed our understanding of cellular diversity, yet current single-cell foundation models (scFMs) remain limited in their scalability, flexibility across diverse tasks, and ability to natively integrate textual information. In \u2026"}, {"title": "Towards trustworthy and reliable language models", "link": "https://dr.ntu.edu.sg/bitstream/10356/184392/2/Amended%2520Thesis.pdf", "details": "R Zhao - 2025", "abstract": "This thesis addresses the critical challenge of developing trustworthy and reliable Natural Language Processing (NLP) systems, specifically the newly emerged Large Language Models (LLMs). As LLMs become increasingly prevalent in various \u2026"}, {"title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning", "link": "https://arxiv.org/pdf/2504.01005", "details": "N Singhi, H Bansal, A Hosseini, A Grover, KW Chang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC) \u2026"}]
