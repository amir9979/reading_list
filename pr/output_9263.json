[{"title": "Safe+ Safe= Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.11496", "details": "C Cui, G Deng, A Zhang, J Zheng, Y Li, L Gao, T Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in Large Vision-Language Models (LVLMs) have showcased strong reasoning abilities across multiple modalities, achieving significant breakthroughs in various real-world applications. Despite this great success, the \u2026"}, {"title": "On-Board Vision-Language Models for Personalized Autonomous Vehicle Motion Control: System Design and Real-World Validation", "link": "https://arxiv.org/pdf/2411.11913", "details": "C Cui, Z Yang, Y Zhou, J Peng, SY Park, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Personalized driving refers to an autonomous vehicle's ability to adapt its driving behavior or control strategies to match individual users' preferences and driving styles while maintaining safety and comfort standards. However, existing works \u2026"}, {"title": "TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in Vision-Language Models", "link": "https://arxiv.org/pdf/2411.13136", "details": "X Wang, K Chen, J Zhang, J Chen, X Ma - arXiv preprint arXiv:2411.13136, 2024", "abstract": "Large pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated excellent zero-shot generalizability across various downstream tasks. However, recent studies have shown that the inference performance of CLIP can be greatly \u2026"}, {"title": "LHRS-Bot-Nova: Improved Multimodal Large Language Model for Remote Sensing Vision-Language Interpretation", "link": "https://arxiv.org/pdf/2411.09301", "details": "Z Li, D Muhtar, F Gu, X Zhang, P Xiao, G He, X Zhu - arXiv preprint arXiv:2411.09301, 2024", "abstract": "Automatically and rapidly understanding Earth's surface is fundamental to our grasp of the living environment and informed decision-making. This underscores the need for a unified system with comprehensive capabilities in analyzing Earth's surface to \u2026"}, {"title": "Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks", "link": "https://arxiv.org/pdf/2410.18387", "details": "L Wang, H Wang, H Yang, J Mao, Z Yang, J Shen, X Li - arXiv preprint arXiv \u2026, 2024", "abstract": "Several medical Multimodal Large Languange Models (MLLMs) have been developed to address tasks involving visual images with textual instructions across various medical modalities, achieving impressive results. Most current medical \u2026"}, {"title": "The quality and safety of using generative AI to produce patient-centred discharge instructions", "link": "https://www.nature.com/articles/s41746-024-01336-w", "details": "K Stanceski, S Zhong, X Zhang, S Khadra, M Tracy\u2026 - npj Digital Medicine, 2024", "abstract": "Patient-centred instructions on discharge can improve adherence and outcomes. Using GPT-3.5 to generate patient-centred discharge instructions, we evaluated responses for safety, accuracy and language simplification. When tested on 100 \u2026"}, {"title": "Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training", "link": "https://www.researchgate.net/profile/Wenbin-Li-31/publication/385969255_Uni-Mlip_Unified_Self-supervision_for_Medical_Vision_Language_Pre-training/links/673db047440ad82b189feba5/Uni-Mlip-Unified-Self-supervision-for-Medical-Vision-Language-Pre-training.pdf", "details": "A Bawazir, K Wu, W Li - 2024", "abstract": "Recent advancements in vision-language pre-training via contrastive learning have significantly improved performance across computer vision tasks. However, in the medical domain, obtaining multimodal data is often costly and challenging due to \u2026"}, {"title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large Language Model", "link": "https://arxiv.org/pdf/2411.10803", "details": "T Liu, L Shi, R Hong, Y Hu, Q Yin, L Zhang - arXiv preprint arXiv:2411.10803, 2024", "abstract": "The vision tokens in multimodal large language models usually exhibit significant spatial and temporal redundancy and take up most of the input tokens, which harms their inference efficiency. To solve this problem, some recent works were introduced \u2026"}, {"title": "Automated anonymization of radiology reports: comparison of publicly available natural language processing and large language models", "link": "https://link.springer.com/article/10.1007/s00330-024-11148-x", "details": "MC Langenbach, B Foldyna, I Hadzic, IL Langenbach\u2026 - European Radiology, 2024", "abstract": "Purpose Medical reports, governed by HIPAA regulations, contain personal health information (PHI), restricting secondary data use. Utilizing natural language processing (NLP) and large language models (LLM), we sought to employ publicly \u2026"}]
