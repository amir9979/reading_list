[{"title": "Scaling Laws of Synthetic Data for Language Models", "link": "https://arxiv.org/pdf/2503.19551", "details": "Z Qin, Q Dong, X Zhang, L Dong, X Huang, Z Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) achieve strong performance across diverse tasks, largely driven by high-quality web data used in pre-training. However, recent studies indicate this data source is rapidly depleting. Synthetic data emerges as a promising \u2026"}, {"title": "Language Model Uncertainty Quantification with Attention Chain", "link": "https://arxiv.org/pdf/2503.19168", "details": "Y Li, R Qiang, L Moukheiber, C Zhang - arXiv preprint arXiv:2503.19168, 2025", "abstract": "Accurately quantifying a large language model's (LLM) predictive uncertainty is crucial for judging the reliability of its answers. While most existing research focuses on short, directly answerable questions with closed-form outputs (eg, multiple \u2026"}, {"title": "Language Models Predict Empathy Gaps Between Social In-groups and Out-groups", "link": "https://arxiv.org/pdf/2503.01030", "details": "Y Hou, H Daum\u00e9 III, R Rudinger - arXiv preprint arXiv:2503.01030, 2025", "abstract": "Studies of human psychology have demonstrated that people are more motivated to extend empathy to in-group members than out-group members (Cikara et al., 2011). In this study, we investigate how this aspect of intergroup relations in humans is \u2026"}, {"title": "DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question Answering in Large Language Models", "link": "https://arxiv.org/pdf/2503.19426", "details": "S Bae, YS Choi, JH Lee - arXiv preprint arXiv:2503.19426, 2025", "abstract": "While Large Language Models (LLMs) excel in zero-shot Question Answering (QA), they tend to expose biases in their internal knowledge when faced with socially sensitive questions, leading to a degradation in performance. Existing zero-shot \u2026"}, {"title": "Towards better understanding of program-of-thought reasoning in cross-lingual and multilingual environments", "link": "https://arxiv.org/pdf/2502.17956", "details": "P Payoungkhamdee, P Tuchinda, J Baek\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multi-step reasoning is essential for large language models (LLMs), yet multilingual performance remains challenging. While Chain-of-Thought (CoT) prompting improves reasoning, it struggles with non-English languages due to the \u2026"}, {"title": "Few-Shot Whole Slide Pathology Classification with Multi-Granular Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3DnJZtYrOeoV", "details": "AT Nguyen, DMH Nguyen, NT Diep, TQ Nguyen, N Ho\u2026 - \u2026 on Foundation Models in the Wild", "abstract": "In this study, we propose a novel architecture for a large vision-language model adapted with a multi-granular prompt learning method to advance few-shot pathol- ogy classification. Starting with the Prov-GigaPath foundation model-pre-trained on \u2026"}]
