[{"title": "Causal Evaluation of Language Models", "link": "https://arxiv.org/pdf/2405.00622", "details": "S Chen, B Peng, M Chen, R Wang, M Xu, X Zeng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for \u2026"}, {"title": "KFEX-N: A table-text data question-answering model based on knowledge-fusion encoder and EX-N tree decoder", "link": "https://www.sciencedirect.com/science/article/pii/S0925231224005666", "details": "Y Tao, J Liu, H Li, W Cao, X Qin, Y Tian, Y Du - Neurocomputing, 2024", "abstract": "Answering questions about hybrid data combining tables and text is challenging. Recent research has employed encoder-tree decoder frameworks to simulate the reasoning process of arithmetic expressions for generating answers. However, this \u2026"}, {"title": "Zero-shot LLM-guided Counterfactual Generation for Text", "link": "https://arxiv.org/pdf/2405.04793", "details": "A Bhattacharjee, R Moraffah, J Garland, H Liu - arXiv preprint arXiv:2405.04793, 2024", "abstract": "Counterfactual examples are frequently used for model development and evaluation in many natural language processing (NLP) tasks. Although methods for automated counterfactual generation have been explored, such methods depend on models \u2026"}, {"title": "Compositional Text-to-Image Generation with Dense Blob Representations", "link": "https://arxiv.org/pdf/2405.08246", "details": "W Nie, S Liu, M Mardani, C Liu, B Eckart, A Vahdat - arXiv preprint arXiv:2405.08246, 2024", "abstract": "Existing text-to-image models struggle to follow complex text prompts, raising the need for extra grounding inputs for better controllability. In this work, we propose to decompose a scene into visual primitives-denoted as dense blob representations \u2026"}, {"title": "SUTRA: Scalable Multilingual Language Model Architecture", "link": "https://arxiv.org/pdf/2405.06694", "details": "A Bendale, M Sapienza, S Ripplinger, S Gibbs, J Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we introduce SUTRA, multilingual Large Language Model architecture capable of understanding, reasoning, and generating text in over 50 languages. SUTRA's design uniquely decouples core conceptual understanding from language \u2026"}, {"title": "Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in Radiology with General-Domain Large Language Model", "link": "https://arxiv.org/pdf/2405.01591", "details": "S Cho, C Kim, J Lee, C Chilkunda, S Choi, JH Yoon - arXiv preprint arXiv:2405.01591, 2024", "abstract": "Recent advancements in Large Multimodal Models (LMMs) have attracted interest in their generalization capability with only a few samples in the prompt. This progress is particularly relevant to the medical domain, where the quality and sensitivity of data \u2026"}, {"title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models", "link": "https://arxiv.org/pdf/2404.18400", "details": "P Shojaee, K Meidani, S Gupta, AB Farimani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of \u2026"}, {"title": "Improving Transformers with Dynamically Composable Multi-Head Attention", "link": "https://arxiv.org/pdf/2405.08553", "details": "D Xiao, Q Meng, S Li, X Yuan - arXiv preprint arXiv:2405.08553, 2024", "abstract": "Multi-Head Attention (MHA) is a key component of Transformer. In MHA, attention heads work independently, causing problems such as low-rank bottleneck of attention score matrices and head redundancy. We propose Dynamically \u2026"}]
