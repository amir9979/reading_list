[{"title": "Bridging Operator Learning and Conditioned Neural Fields: A Unifying Perspective", "link": "https://arxiv.org/pdf/2405.13998", "details": "S Wang, JH Seidman, S Sankaran, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Operator learning is an emerging area of machine learning which aims to learn mappings between infinite dimensional function spaces. Here we uncover a connection between operator learning architectures and conditioned neural fields \u2026"}, {"title": "GECKO: Generative Language Model for English, Code and Korean", "link": "https://arxiv.org/pdf/2405.15640", "details": "S Oh, D Kim - arXiv preprint arXiv:2405.15640, 2024", "abstract": "We introduce GECKO, a bilingual large language model (LLM) optimized for Korean and English, along with programming languages. GECKO is pretrained on the balanced, high-quality corpus of Korean and English employing LLaMA architecture \u2026"}, {"title": "A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques", "link": "https://arxiv.org/pdf/2406.04879", "details": "M Thakkar, Q Fournier, MD Riemer, PY Chen, A Zouaq\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models are first pre-trained on trillions of tokens and then instruction- tuned or aligned to specific preferences. While pre-training remains out of reach for most researchers due to the compute required, fine-tuning has become affordable \u2026"}, {"title": "Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?", "link": "https://openreview.net/pdf%3Fid%3Do8AaRKbP9K", "details": "K Gatmiry, N Saunshi, SJ Reddi, S Jegelka, S Kumar - Forty-first International Conference on \u2026", "abstract": "Transformers to do reasoning and few-shot learning, without any fine-tuning, is widely conjectured to stem from their ability to implicitly simulate a multi-step algorithms--such as gradient descent--with their weights in a single forward pass \u2026"}, {"title": "Impact of high-quality, mixed-domain data on the performance of medical language models", "link": "https://academic.oup.com/jamia/advance-article-abstract/doi/10.1093/jamia/ocae120/7680487", "details": "M Griot, C Hemptinne, J Vanderdonckt, D Yuksel - Journal of the American Medical \u2026, 2024", "abstract": "Objective To optimize the training strategy of large language models for medical applications, focusing on creating clinically relevant systems that efficiently integrate into healthcare settings, while ensuring high standards of accuracy and reliability \u2026"}, {"title": "Observational Scaling Laws and the Predictability of Language Model Performance", "link": "https://arxiv.org/pdf/2405.10938", "details": "Y Ruan, CJ Maddison, T Hashimoto - arXiv preprint arXiv:2405.10938, 2024", "abstract": "Understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different \u2026"}, {"title": "Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration", "link": "https://openreview.net/pdf%3Fid%3DDLTjFFiuUJ", "details": "Z Yu, Z Wang, Y Fu, H Shi, K Shaikh, YC Lin - Forty-first International Conference on Machine \u2026", "abstract": "Attention is a fundamental component behind the remarkable achievements of large language models (LLMs). However, our current understanding of the attention mechanism, especially regarding how attention distributions are established \u2026"}, {"title": "PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting", "link": "https://arxiv.org/pdf/2405.16829", "details": "Z Wang, D Xu - arXiv preprint arXiv:2405.16829, 2024", "abstract": "Neural Radiance Fields (NeRFs) have demonstrated remarkable proficiency in synthesizing photorealistic images of large-scale scenes. However, they are often plagued by a loss of fine details and long rendering durations. 3D Gaussian Splatting \u2026"}, {"title": "Concept Formation and Alignment in Language Models: Bridging Statistical Patterns in Latent Space to Concept Taxonomy", "link": "https://arxiv.org/abs/2406.05315", "details": "M Khatir, CK Reddy - arXiv preprint arXiv:2406.05315, 2024", "abstract": "This paper explores the concept formation and alignment within the realm of language models (LMs). We propose a mechanism for identifying concepts and their hierarchical organization within the semantic representations learned by various \u2026"}]
