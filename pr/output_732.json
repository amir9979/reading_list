'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Dense Training, Sparse Inference: Rethinking Training '
[{"title": "SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation", "link": "https://arxiv.org/pdf/2404.01923", "details": "S Guo, L Liao, J Zhang, Y Wang, C Li, H Chen - arXiv preprint arXiv:2404.01923, 2024", "abstract": "Knowledge base question generation (KBQG) aims to generate natural language questions from a set of triplet facts extracted from KB. Existing methods have significantly boosted the performance of KBQG via pre-trained language models \u2026"}, {"title": "ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models", "link": "https://arxiv.org/pdf/2403.16187", "details": "Z Liu, J Lyn, W Zhu, X Tian, Y Graham - arXiv preprint arXiv:2403.16187, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models. Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method \u2026"}, {"title": "KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion", "link": "https://arxiv.org/pdf/2403.17532", "details": "Y Wang, M Hu, Z Huang, D Li, D Yang, X Lu - arXiv preprint arXiv:2403.17532, 2024", "abstract": "The goal of knowledge graph completion (KGC) is to predict missing facts among entities. Previous methods for KGC re-ranking are mostly built on non-generative language models to obtain the probability of each candidate. Recently, generative \u2026"}, {"title": "Enhanced Transfer Learning with Efficient Modeling and Adaptive Fusion of Knowledge Via Prompt Tuning", "link": "https://ieeexplore.ieee.org/abstract/document/10445861/", "details": "M Xu, Z Guo, Y Zeng, D Xiong - ICASSP 2024-2024 IEEE International Conference \u2026, 2024", "abstract": "This work presents a novel and parameter-efficient transfer learning framework. The framework consists of two phases: knowledge modeling based on prompt decomposition and knowledge transfer based on attention. Specifically, during the \u2026"}]
