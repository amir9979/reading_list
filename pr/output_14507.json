[{"title": "Contrastive Learning via Randomly Generated Deep Supervision", "link": "https://ieeexplore.ieee.org/abstract/document/10890867/", "details": "S Wang, Z Ma, KH Chan, Y Liu, T Tong, Q Gao, G Zhai\u2026 - ICASSP 2025-2025 IEEE \u2026, 2025", "abstract": "Unsupervised visual representation learning has gained significant attention in the computer vision community, driven by recent advancements in contrastive learning. Most existing contrastive learning frameworks rely on instance discrimination as a \u2026"}, {"title": "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions", "link": "https://arxiv.org/pdf/2503.03278", "details": "J Li, C Liu, W Bai, R Arcucci, CI Bercea, JA Schnabel - arXiv preprint arXiv \u2026, 2025", "abstract": "Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains \u2026"}, {"title": "Multidimensional Consistency Improves Reasoning in Language Models", "link": "https://arxiv.org/pdf/2503.02670", "details": "H Lai, X Zhang, M Nissim - arXiv preprint arXiv:2503.02670, 2025", "abstract": "While Large language models (LLMs) have proved able to address some complex reasoning tasks, we also know that they are highly sensitive to input variation, which can lead to different solution paths and final answers. Answer consistency across \u2026"}, {"title": "Evolution-based Region Adversarial Prompt Learning for Robustness Enhancement in Vision-Language Models", "link": "https://arxiv.org/pdf/2503.12874", "details": "X Jia, S Gao, S Qin, K Ma, X Li, Y Huang, W Dong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large pre-trained vision-language models (VLMs), such as CLIP, demonstrate impressive generalization but remain highly vulnerable to adversarial examples (AEs). Previous work has explored robust text prompts through adversarial training \u2026"}, {"title": "MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data with Vision Generation Task using Discrete Visual Representations", "link": "https://arxiv.org/pdf/2503.01019", "details": "Z Zhang, Y Yu, Y Chen, X Yang, SY Yeo - arXiv preprint arXiv:2503.01019, 2025", "abstract": "Despite significant progress in Vision-Language Pre-training (VLP), current approaches predominantly emphasize feature extraction and cross-modal comprehension, with limited attention to generating or transforming visual content \u2026"}, {"title": "PC-Match: Semi-Supervised Learning with Progressive Contrastive and Consistency Regularization", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10918676.pdf", "details": "M Kang, S Seo, M Min - IEEE Access, 2025", "abstract": "As artificial intelligence developed rapidly, deep learning models have been applied in various domains. While labeling is crucial to training models in fields that demand specific knowledge, producing such labeled datasets is expensive. Semi-supervised \u2026"}, {"title": "Impact of Glyph Information on Latent Space Diffusion Models for Accurate Handwritten Text Generation", "link": "https://ieeexplore.ieee.org/abstract/document/10890644/", "details": "YL Lin, HC Cheng, CI Huang, CY Wang, JC Wang - ICASSP 2025-2025 IEEE \u2026, 2025", "abstract": "The generation of high-quality stylized handwritten text images is a challenging task in computer vision and artificial intelligence. While advanced approaches using Latent Diffusion Models (LDMs) for generating stylized handwritten text have shown \u2026"}, {"title": "Can Generative Geospatial Diffusion Models Excel as Discriminative Geospatial Foundation Models?", "link": "https://arxiv.org/pdf/2503.07890", "details": "Y Jia, V Marsocci, Z Gong, X Yang, M Vergauwen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Self-supervised learning (SSL) has revolutionized representation learning in Remote Sensing (RS), advancing Geospatial Foundation Models (GFMs) to leverage vast unlabeled satellite imagery for diverse downstream tasks. Currently, GFMs primarily \u2026"}, {"title": "Generative Large Language Model\u2014Powered Conversational AI App for Personalized Risk Assessment: Case Study in COVID-19", "link": "https://ai.jmir.org/2025/1/e67363/", "details": "MA Roshani, X Zhou, Y Qiang, S Suresh, S Hicks\u2026 - JMIR AI, 2025", "abstract": "Background: Large language models (LLMs) have demonstrated powerful capabilities in natural language tasks and are increasingly being integrated into health care for tasks like disease risk assessment. Traditional machine learning \u2026"}]
