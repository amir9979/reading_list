[{"title": "The current status of large language models in summarizing radiology report impressions", "link": "https://arxiv.org/pdf/2406.02134", "details": "D Hu, S Zhang, Q Liu, X Zhu, B Liu - arXiv preprint arXiv:2406.02134, 2024", "abstract": "Large language models (LLMs) like ChatGPT show excellent capabilities in various natural language processing tasks, especially for text generation. The effectiveness of LLMs in summarizing radiology report impressions remains unclear. In this study \u2026"}, {"title": "LG AI Research & KAIST at EHRSQL 2024: Self-Training Large Language Models with Pseudo-Labeled Unanswerable Questions for a Reliable Text-to-SQL System \u2026", "link": "https://arxiv.org/pdf/2405.11162", "details": "Y Jo, S Lee, M Seo, SJ Hwang, M Lee - arXiv preprint arXiv:2405.11162, 2024", "abstract": "Text-to-SQL models are pivotal for making Electronic Health Records (EHRs) accessible to healthcare professionals without SQL knowledge. With the advancements in large language models, these systems have become more adept at \u2026"}, {"title": "MEDVOC: Vocabulary Adaptation for Fine-tuning Pre-trained Language Models on Medical Text Summarization", "link": "https://arxiv.org/pdf/2405.04163", "details": "G Balde, S Roy, M Mondal, N Ganguly - arXiv preprint arXiv:2405.04163, 2024", "abstract": "This work presents a dynamic vocabulary adaptation strategy, MEDVOC, for fine- tuning pre-trained language models (PLMs) like BertSumAbs, BART, and PEGASUS for improved medical text summarization. In contrast to existing domain adaptation \u2026"}, {"title": "Likelihood-based fine-tuning of protein language models for few-shot fitness prediction and design", "link": "https://www.biorxiv.org/content/biorxiv/early/2024/06/02/2024.05.28.596156.full.pdf", "details": "A Hawkins-Hooker, J Kmec, O Bent, P Duckworth - bioRxiv, 2024", "abstract": "In order to correctly predict amino acid identities within natural proteins, protein language models (PLMs) must implicitly learn distributional constraints on protein sequences upheld over the course of evolution. As a consequence, the sequence \u2026"}, {"title": "Mammo-CLIP: A Vision Language Foundation Model to Enhance Data Efficiency and Robustness in Mammography", "link": "https://arxiv.org/pdf/2405.12255", "details": "S Ghosh, CB Poynton, S Visweswaran\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The lack of large and diverse training data on Computer-Aided Diagnosis (CAD) in breast cancer detection has been one of the concerns that impedes the adoption of the system. Recently, pre-training with large-scale image text datasets via Vision \u2026"}]
