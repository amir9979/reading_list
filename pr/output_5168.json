[{"title": "KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models", "link": "https://arxiv.org/pdf/2408.03297", "details": "R Zhang, Y Xu, Y Xiao, R Zhu, X Jiang, X Chu, J Zhao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks \u2026"}, {"title": "EXAONE 3.0 7.8 B Instruction Tuned Language Model", "link": "https://arxiv.org/pdf/2408.03541", "details": "LG Research, S An, K Bae, E Choi, SJ Choi, Y Choi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce EXAONE 3.0 instruction-tuned language model, the first open model in the family of Large Language Models (LLMs) developed by LG AI Research. Among different model sizes, we publicly release the 7.8 B instruction-tuned model to \u2026"}, {"title": "Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement", "link": "https://arxiv.org/pdf/2408.03092", "details": "L Yu, B Yu, H Yu, F Huang, Y Li - arXiv preprint arXiv:2408.03092, 2024", "abstract": "Merging Large Language Models (LLMs) aims to amalgamate multiple homologous LLMs into one with all the capabilities. Ideally, any LLMs sharing the same backbone should be mergeable, irrespective of whether they are Fine-Tuned (FT) with minor \u2026"}, {"title": "Fine-tuning language models for joint rewriting and completion of code with potential bugs", "link": "https://www.amazon.science/publications/fine-tuning-language-models-for-joint-rewriting-and-completion-of-code-with-potential-bugs", "details": "D Wang, J Zhao, H Pei, S Tan, S Zha - 2024", "abstract": "Handling drafty partial code remains a notable challenge in real-time code suggestion applications. Previous work has demonstrated shortcomings of large language models of code (CodeLLMs) in completing partial code with potential bugs \u2026"}, {"title": "Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models", "link": "https://arxiv.org/pdf/2408.02416", "details": "Z Liang, H Hu, Q Ye, Y Xiao, H Li - arXiv preprint arXiv:2408.02416, 2024", "abstract": "The drastic increase of large language models'(LLMs) parameters has led to a new research direction of fine-tuning-free downstream customization by prompts, ie, task descriptions. While these prompt-based services (eg OpenAI's GPTs) play an \u2026"}, {"title": "Fine-tuning Language Models for Triple Extraction with Data Augmentation", "link": "https://aclanthology.org/2024.kallm-1.12.pdf", "details": "Y Zhang, T Sadler, MR Taesiri, W Xu, M Reformat - Proceedings of the 1st Workshop \u2026, 2024", "abstract": "Advanced language models with impressive capabilities to process textual information can more effectively extract high-quality triples, which are the building blocks of knowledge graphs. Our work examines language models' abilities to extract \u2026"}, {"title": "Mol2Lang-VLM: Vision-and Text-Guided Generative Pre-trained Language Models for Advancing Molecule Captioning through Multimodal Fusion", "link": "https://aclanthology.org/2024.langmol-1.12.pdf", "details": "D Tran, NT Pham, N Nguyen, B Manavalan - Proceedings of the 1st Workshop on \u2026, 2024", "abstract": "Abstract This paper introduces Mol2Lang-VLM, an enhanced method for refining generative pre-trained language models for molecule captioning using multimodal features to achieve more accurate caption generation. Our approach leverages the \u2026"}, {"title": "Cool-Fusion: Fuse Large Language Models without Training", "link": "https://arxiv.org/pdf/2407.19807", "details": "C Liu, X Quan, Y Pan, L Lin, W Wu, X Chen - arXiv preprint arXiv:2407.19807, 2024", "abstract": "We focus on the problem of fusing two or more heterogeneous large language models (LLMs) to facilitate their complementary strengths. One of the challenges on model fusion is high computational load, ie to fine-tune or to align vocabularies via \u2026"}, {"title": "SNFinLLM: Systematic and Nuanced Financial Domain Adaptation of Chinese Large Language Models", "link": "https://arxiv.org/pdf/2408.02302", "details": "S Zhao, L Qiao, K Luo, QW Zhang, J Lu, D Yin - arXiv preprint arXiv:2408.02302, 2024", "abstract": "Large language models (LLMs) have become powerful tools for advancing natural language processing applications in the financial industry. However, existing financial LLMs often face challenges such as hallucinations or superficial parameter \u2026"}]
