[{"title": "Unsupervised Latent Stain Adaption for Digital Pathology", "link": "https://arxiv.org/pdf/2406.19081", "details": "D Reisenb\u00fcchler, L Luttner, NS Schaadt, F Feuerhake\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In digital pathology, deep learning (DL) models for tasks such as segmentation or tissue classification are known to suffer from domain shifts due to different staining techniques. Stain adaptation aims to reduce the generalization error between \u2026"}, {"title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings", "link": "https://arxiv.org/pdf/2406.16611", "details": "A Posada, D Rueckert, F Meissen, P M\u00fcller - arXiv preprint arXiv:2406.16611, 2024", "abstract": "Since the emergence of the Transformer architecture, language model development has increased, driven by their promising potential. However, releasing these models into production requires properly understanding their behavior, particularly in \u2026"}, {"title": "The Art of Saying No: Contextual Noncompliance in Language Models", "link": "https://arxiv.org/pdf/2407.12043", "details": "F Brahman, S Kumar, V Balachandran, P Dasigi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Chat-based language models are designed to be helpful, yet they should not comply with every user request. While most existing work primarily focuses on refusal of\" unsafe\" queries, we posit that the scope of noncompliance should be broadened. We \u2026"}, {"title": "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference", "link": "https://arxiv.org/pdf/2406.17626", "details": "E Yu, J Li, M Liao, S Wang, Z Gao, F Mi, L Hong - arXiv preprint arXiv:2406.17626, 2024", "abstract": "As large language models (LLMs) constantly evolve, ensuring their safety remains a critical research problem. Previous red-teaming approaches for LLM safety have primarily focused on single prompt attacks or goal hijacking. To the best of our \u2026"}]
