[{"title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models", "link": "https://arxiv.org/pdf/2501.02955", "details": "W Hong, Y Cheng, Z Yang, W Wang, L Wang, X Gu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In recent years, vision language models (VLMs) have made significant advancements in video understanding. However, a crucial capability-fine-grained motion comprehension-remains under-explored in current benchmarks. To address \u2026"}, {"title": "DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision Language Models in Real-World Scenarios with Driving Theory Tests", "link": "https://arxiv.org/pdf/2501.04671", "details": "C Corbi\u00e8re, S Roburin, S Montariol, A Bosselut, A Alahi - arXiv preprint arXiv \u2026, 2025", "abstract": "Large vision-language models (LVLMs) augment language models with visual understanding, enabling multimodal reasoning. However, due to the modality gap between textual and visual data, they often face significant challenges, such as over \u2026"}, {"title": "Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models", "link": "https://arxiv.org/pdf/2501.18119", "details": "Q Lin, T Zhao, K He, Z Peng, F Xu, L Huang, J Ma\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To \u2026"}, {"title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models", "link": "https://arxiv.org/pdf/2501.05752", "details": "S Lee, H Park, J Kim, J Ok - arXiv preprint arXiv:2501.05752, 2025", "abstract": "Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer \u2026"}, {"title": "Mee-SLAM: Memory efficient endoscopic RGB SLAM with implicit scene representation", "link": "https://www.sciencedirect.com/science/article/pii/S0957417424031026", "details": "Y Zhou, T Li, Y Dai, J Zhang - Expert Systems with Applications, 2025", "abstract": "Endoscopic dense simultaneous localization and mapping (SLAM) plays a critical role in robot assisted surgery. Recently, SLAM systems based on neural implicit representation have demonstrated superior localization and real-time mapping \u2026"}, {"title": "Expanding the generality of neural fields", "link": "https://dr.ntu.edu.sg/bitstream/10356/182229/2/yslan-thesis-final-copy.pdf", "details": "Y Lan - 2025", "abstract": "Neural fields have emerged as a groundbreaking approach to representing 3D shapes, garnering significant attention due to their compatibility with modern deep- learning techniques. Neural fields, which parameterize physical properties of scenes \u2026"}, {"title": "How to Bridge the Gap between Modalities: Survey on Multimodal Large Language Model", "link": "https://ieeexplore.ieee.org/abstract/document/10841938/", "details": "S Song, X Li, S Li, S Zhao, J Yu, J Ma, X Mao, W Zhang\u2026 - IEEE Transactions on \u2026, 2025", "abstract": "We explore Multimodal Large Language Models (MLLMs), which integrate LLMs like GPT-4 to handle multimodal data, including text, images, audio, and more. MLLMs demonstrate capabilities such as generating image captions and answering image \u2026"}, {"title": "EHealth: A Chinese Biomedical Language Model Built via Multi-Level Text Discrimination", "link": "https://ieeexplore.ieee.org/abstract/document/10857372/", "details": "Q Wang, S Dai, B Xu, Y Lyu, H Wu, H Wang - IEEE Transactions on Audio, Speech \u2026, 2025", "abstract": "Pre-trained language models (PLMs) have recently revolutionized the field of natural language processing, impacting not only the general domain but also the biomedical domain. Most previous studies on constructing biomedical PLMs relied simply on \u2026"}, {"title": "MedFILIP: Medical Fine-Grained Language-Image Pre-Training", "link": "https://ieeexplore.ieee.org/abstract/document/10836674/", "details": "X Liang, X Li, F Li, J Jiang, Q Dong, W Wang, K Wang\u2026 - IEEE Journal of Biomedical \u2026, 2025", "abstract": "Medical vision-language pretraining (VLP) that leverages naturally-paired medical image-report data is crucial for medical image analysis. However, existing methods struggle to accurately characterize associations between images and diseases \u2026"}]
