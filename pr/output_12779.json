[{"title": "Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement", "link": "https://arxiv.org/pdf/2502.02573", "details": "S Abbasloo - arXiv preprint arXiv:2502.02573, 2025", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem- solving, a crucial, ubiquitous, and complex domain. This paper explores the \u2026"}, {"title": "Advancing Math Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages", "link": "https://arxiv.org/pdf/2501.14002", "details": "Z Chen, T Liu, M Tian, Q Tong, W Luo, Z Liu - arXiv preprint arXiv:2501.14002, 2025", "abstract": "Advancements in LLMs have significantly expanded their capabilities across various domains. However, mathematical reasoning remains a challenging area, prompting the development of math-specific LLMs. These models typically follow a two-stage \u2026"}, {"title": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques", "link": "https://arxiv.org/pdf/2501.14492", "details": "Z Tang, Z Li, Z Xiao, T Ding, R Sun, B Wang, D Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Critiques are important for enhancing the performance of Large Language Models (LLMs), enabling both self-improvement and constructive feedback for others by identifying flaws and suggesting improvements. However, evaluating the critique \u2026"}, {"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models", "link": "https://arxiv.org/pdf/2502.03199", "details": "J Wu, Y Shen, S Liu, Y Tang, S Song, X Wang, L Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the \u2026"}, {"title": "PSSD: Making Large Language Models Self-denial via Human Psyche Structure", "link": "https://arxiv.org/pdf/2502.01344", "details": "J Liao, Z Liao, X Zhao - arXiv preprint arXiv:2502.01344, 2025", "abstract": "The enhance of accuracy in reasoning results of LLMs arouses the community's interests, wherein pioneering studies investigate post-hoc strategies to rectify potential mistakes. Despite extensive efforts, they are all stuck in a state of resource \u2026"}, {"title": "Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models", "link": "https://arxiv.org/pdf/2502.02444", "details": "H Ye, T Zhang, Y Xie, L Zhang, Y Ren, X Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz's Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross \u2026"}, {"title": "Assessing and Post-Processing Black Box Large Language Models for Knowledge Editing", "link": "https://openreview.net/pdf%3Fid%3DaGhk1VNcRJ", "details": "X Song, Z Wang, K He, G Dong, Y Mou, J Zhao, W Xu - THE WEB CONFERENCE 2025", "abstract": "The rapid evolution of the Web as a key platform for information dissemination has led to the growing integration of large language models (LLMs) in Web-based applications. However, the swift changes in web content present challenges in \u2026"}, {"title": "Multi-Turn Jailbreaking Large Language Models via Attention Shifting", "link": "https://xhdu.github.io/files/AAAI25.pdf", "details": "X Du, F Mo, M Wen, T Gu, H Zheng, H Jin, J Shi - 2025", "abstract": "Abstract Large Language Models (LLMs) have achieved significant performance in various natural language processing tasks but also pose safety and ethical threats, thus requiring red teaming and alignment processes to bolster their safety. To \u2026"}, {"title": "Do as We Do, Not as You Think: the Conformity of Large Language Models", "link": "https://arxiv.org/pdf/2501.13381", "details": "Z Weng, G Chen, W Wang - arXiv preprint arXiv:2501.13381, 2025", "abstract": "Recent advancements in large language models (LLMs) revolutionize the field of intelligent agents, enabling collaborative multi-agent systems capable of tackling complex problems across various domains. However, the potential of conformity \u2026"}]
