'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [An Efficient Approach for Studying Cross-Lingual Trans'
[{"title": "HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding", "link": "https://arxiv.org/pdf/2403.19723", "details": "R Jin, Y Li, G Qi, N Hu, YF Li, J Chen, J Wang, Y Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures. To address these challenges, we propose HGT, a framework with a \u2026"}, {"title": "Gecko: Versatile Text Embeddings Distilled from Large Language Models", "link": "https://arxiv.org/html/2403.20327v1", "details": "J Lee, Z Dai, X Ren, B Chen, D Cer, JR Cole, K Hui\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Gecko, a compact and versatile text embedding model. Gecko achieves strong retrieval performance by leveraging a key idea: distilling knowledge from large language models (LLMs) into a retriever. Our two-step distillation process \u2026"}, {"title": "Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering", "link": "https://aclanthology.org/2024.eacl-long.12.pdf", "details": "M Chen, X Chen, W Yih - Proceedings of the 18th Conference of the European \u2026, 2024", "abstract": "Few-shot learning for open domain multi-hop question answering typically relies on the in-context learning capability of large language models (LLMs). While powerful, these LLMs usually contain tens or hundreds of billions of parameters, making them \u2026"}, {"title": "Small Language Models are Good Too: An Empirical Study of Zero-Shot Classification", "link": "https://hal.science/hal-04519930/document", "details": "P Lepagnol, T Gerald, S Ghannay, C Servan, S Rosset - LREC-COLING 2024, 2024", "abstract": "This study is part of the debate on the efficiency of large versus small language models for text classification by prompting. We assess the performance of small language models in zero-shot text classification, challenging the prevailing \u2026"}, {"title": "Enhanced Transfer Learning with Efficient Modeling and Adaptive Fusion of Knowledge Via Prompt Tuning", "link": "https://ieeexplore.ieee.org/abstract/document/10445861/", "details": "M Xu, Z Guo, Y Zeng, D Xiong - ICASSP 2024-2024 IEEE International Conference \u2026, 2024", "abstract": "This work presents a novel and parameter-efficient transfer learning framework. The framework consists of two phases: knowledge modeling based on prompt decomposition and knowledge transfer based on attention. Specifically, during the \u2026"}]
