[{"title": "SEQFUSION: Sequential Fusion of Pre-Trained Models for Zero-Shot Time-Series Forecasting", "link": "https://arxiv.org/pdf/2503.02836", "details": "TJ Huang, XY Chen, HJ Ye - arXiv preprint arXiv:2503.02836, 2025", "abstract": "Unlike traditional time-series forecasting methods that require extensive in-task data for training, zero-shot forecasting can directly predict future values given a target time series without additional training data. Current zero-shot approaches primarily rely \u2026"}, {"title": "Decoupled Doubly Contrastive Learning for Cross Domain Facial Action Unit Detection", "link": "https://arxiv.org/pdf/2503.08977", "details": "Y Li, M Liu, Z Cui, Y Ding, Y Zong, W Zheng, S Shan\u2026 - IEEE Transactions on Image \u2026, 2025", "abstract": "Despite the impressive performance of current vision-based facial action unit (AU) detection approaches, they are heavily susceptible to the variations across different domains and the cross-domain AU detection methods are under-explored. In \u2026"}, {"title": "VisTa: Visual-contextual and Text-augmented Zero-shot Object-level OOD Detection", "link": "https://ieeexplore.ieee.org/abstract/document/10888875/", "details": "B Zhang, X Qu, G Li, J Wan, J Wang - \u2026 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "As object detectors are increasingly deployed as black-box cloud services or pre- trained models with restricted access to the original training data, the challenge of zero-shot object-level out-of-distribution (OOD) detection arises. This task becomes \u2026"}, {"title": "Mapping representations in Reinforcement Learning via Semantic Alignment for Zero-Shot Stitching", "link": "https://arxiv.org/pdf/2503.01881", "details": "AP Ricciardi, V Maiorca, L Moschella, R Marin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Deep Reinforcement Learning (RL) models often fail to generalize when even small changes occur in the environment's observations or task requirements. Addressing these shifts typically requires costly retraining, limiting the reusability of learned \u2026"}, {"title": "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via Cross-Architecture Distillation", "link": "https://arxiv.org/pdf/2502.15016", "details": "J Ni, Z Liu, S Wang, M Jin, W Jin - arXiv preprint arXiv:2502.15016, 2025", "abstract": "Transformer-based and CNN-based methods demonstrate strong performance in long-term time series forecasting. However, their high computational and storage requirements can hinder large-scale deployment. To address this limitation, we \u2026"}]
