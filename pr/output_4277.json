[{"title": "DEAL: Disentangle and Localize Concept-level Explanations for VLMs", "link": "https://arxiv.org/pdf/2407.14412", "details": "T Li, M Ma, X Peng - arXiv preprint arXiv:2407.14412, 2024", "abstract": "Large pre-trained Vision-Language Models (VLMs) have become ubiquitous foundational components of other models and downstream tasks. Although powerful, our empirical results reveal that such models might not be able to identify fine \u2026"}, {"title": "Self-Supervised Video Representation Learning in a Heuristic Decoupled Perspective", "link": "https://arxiv.org/pdf/2407.14069", "details": "Z Song, J Wang, J Zhang, C Zheng, W Qiang - arXiv preprint arXiv:2407.14069, 2024", "abstract": "Video contrastive learning (v-CL) has gained prominence as a leading framework for unsupervised video representation learning, showcasing impressive performance across various tasks such as action classification and detection. In the field of video \u2026"}, {"title": "Explainable Artificial Intelligence in Decoding Human Emotions Through Vision Transformers", "link": "https://portal.sinteza.singidunum.ac.rs/Media/files/2024/168-174.pdf", "details": "M Marjanovi\u0107 - Sinteza 2024-International Scientific Conference on \u2026, 2024", "abstract": "In artificial intelligence and psychological research, understanding how artificial intelligence interprets human emotions through facial expressions is challenging, thus emotion recognition became a crucial task in many computer vision \u2026"}, {"title": "DEPICT: Diffusion-Enabled Permutation Importance for Image Classification Tasks", "link": "https://arxiv.org/pdf/2407.14509", "details": "S Jabbour, G Kondas, E Kazerooni, M Sjoding\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We propose a permutation-based explanation method for image classifiers. Current image-model explanations like activation maps are limited to instance-based explanations in the pixel space, making it difficult to understand global model \u2026"}, {"title": "Joint-Embedding Predictive Architecture for Self-Supervised Learning of Mask Classification Architecture", "link": "https://arxiv.org/pdf/2407.10733", "details": "DH Kim, S Cho, H Cho, C Park, J Kim, WH Kim - arXiv preprint arXiv:2407.10733, 2024", "abstract": "In this work, we introduce Mask-JEPA, a self-supervised learning framework tailored for mask classification architectures (MCA), to overcome the traditional constraints associated with training segmentation models. Mask-JEPA combines a Joint \u2026"}]
