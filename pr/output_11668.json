[{"title": "Privacy-ensuring open-weights large language models are competitive with closed-weights GPT-4o in extracting chest radiography findings from free-text reports", "link": "https://pubs.rsna.org/doi/pdf/10.1148/radiol.240895", "details": "S Nowak, B Wulff, YC Layer, M Theis, A Isaak, B Salam\u2026 - Radiology, 2025", "abstract": "Background Large-scale secondary use of clinical databases requires automated tools for retrospective extraction of structured content from free-text radiology reports. Purpose To share data and insights on the application of privacy-preserving open \u2026"}, {"title": "A vision\u2013language foundation model for precision oncology", "link": "https://www.nature.com/articles/s41586-024-08378-w", "details": "J Xiang, X Wang, X Zhang, Y Xi, F Eweje, Y Chen, Y Li\u2026 - Nature, 2025", "abstract": "Clinical decision-making is driven by multimodal data, including clinical notes and pathological characteristics. Artificial intelligence approaches that can effectively integrate multimodal data hold significant promise in advancing clinical care \u2026"}, {"title": "Do language models understand time?", "link": "https://arxiv.org/pdf/2412.13845", "details": "X Ding, L Wang - arXiv preprint arXiv:2412.13845, 2024", "abstract": "Large language models (LLMs) have revolutionized video-based computer vision applications, including action recognition, anomaly detection, and video summarization. Videos inherently pose unique challenges, combining spatial \u2026"}, {"title": "LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation", "link": "https://arxiv.org/pdf/2412.15188", "details": "W Shi, X Han, C Zhou, W Liang, XV Lin, L Zettlemoyer\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present LlamaFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LlamaFusion \u2026"}, {"title": "Synthetic Feature Augmentation Improves Generalization Performance of Language Models", "link": "https://arxiv.org/pdf/2501.06434", "details": "A Choudhary, C Thiels, H Salehinejad - arXiv preprint arXiv:2501.06434, 2025", "abstract": "Training and fine-tuning deep learning models, especially large language models (LLMs), on limited and imbalanced datasets poses substantial challenges. These issues often result in poor generalization, where models overfit to dominant classes \u2026"}, {"title": "Improving Self-Supervised Medical Image Pre-Training by Early Alignment with Human Eye Gaze Information", "link": "https://ieeexplore.ieee.org/abstract/document/10839445/", "details": "S Wang, Z Zhao, Z Shen, B Wang, Q Wang, D Shen - IEEE Transactions on Medical \u2026, 2025", "abstract": "Alignment between human knowledge and machine learning models is crucial for achieving efficient and interpretable AI systems. However, conventional self- supervised pre-training methods often suffer from low efficiency, as they do not \u2026"}, {"title": "Focus Your Attention: Multiple Instance Learning with Attention Modification for Whole Slide Pathological Image Classification", "link": "https://ieeexplore.ieee.org/iel8/76/4358651/10838539.pdf", "details": "H Cheng, S Huang, L Cai, Y Xu, R Wang, Y Zhang - IEEE Transactions on Circuits \u2026, 2025", "abstract": "Computer-aided pathology diagnosis based on whole slide images, which is often formulated as a weakly supervised multiple instance learning (MIL) paradigm. Current approaches generally employ attention mechanisms to aggregate instance \u2026"}, {"title": "Supervision-free Vision-Language Alignment", "link": "https://arxiv.org/pdf/2501.04568%3F", "details": "G Giannone, R Li, Q Feng, E Perevodchikov, R Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) have demonstrated remarkable potential in integrating visual and linguistic information, but their performance is often constrained by the need for extensive, high-quality image-text training data. Curation \u2026"}, {"title": "Token Preference Optimization with Self-Calibrated Visual-Anchored Rewards for Hallucination Mitigation", "link": "https://arxiv.org/pdf/2412.14487", "details": "J Gu, Y Wang, M Cao, P Bu, J Song, Y He, S Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Direct Preference Optimization (DPO) has been demonstrated to be highly effective in mitigating hallucinations in Large Vision Language Models (LVLMs) by aligning their outputs more closely with human preferences. Despite the recent progress \u2026"}]
