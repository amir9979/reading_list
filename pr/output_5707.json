[{"title": "Relation labeling in product knowledge graphs with large language models for e-commerce", "link": "https://link.springer.com/article/10.1007/s13042-024-02274-5", "details": "J Chen, L Ma, X Li, J Xu, JHD Cho, K Nag, E Korpeoglu\u2026 - International Journal of \u2026, 2024", "abstract": "Abstract Product Knowledge Graphs (PKGs) play a crucial role in enhancing e- commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between \u2026"}, {"title": "Mitigating Privacy Seesaw in Large Language Models: Augmented Privacy Neuron Editing via Activation Patching", "link": "https://aclanthology.org/2024.findings-acl.315.pdf", "details": "X Wu, W Dong, S Xu, D Xiong - Findings of the Association for Computational \u2026, 2024", "abstract": "Protecting privacy leakage in large language models remains a paramount challenge. In this paper, we reveal Privacy Seesaw in LLM privacy safeguarding, a phenomenon where measures to secure specific private information inadvertently \u2026"}, {"title": "Adversarial preference optimization: Enhancing your alignment via rm-llm game", "link": "https://aclanthology.org/2024.findings-acl.221.pdf", "details": "P Cheng, Y Yang, J Li, Y Dai, T Hu, P Cao, N Du, X Li - Findings of the Association for \u2026, 2024", "abstract": "Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing alignment methods depend on manually annotated preference data to guide the LLM optimization directions. However \u2026"}, {"title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language Models", "link": "https://arxiv.org/pdf/2408.08545", "details": "KK Maurya, KV Srivatsa, E Kochmar - arXiv preprint arXiv:2408.08545, 2024", "abstract": "Large language models (LLMs) have gained increased popularity due to their remarkable success across various tasks, which has led to the active development of a large set of diverse LLMs. However, individual LLMs have limitations when applied \u2026"}, {"title": "MoE-LPR: Multilingual Extension of Large Language Models through Mixture-of-Experts with Language Priors Routing", "link": "https://arxiv.org/pdf/2408.11396", "details": "H Zhou, Z Wang, S Huang, X Huang, X Han, J Feng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) are often English-centric due to the disproportionate distribution of languages in their pre-training data. Enhancing non-English language capabilities through post-pretraining often results in catastrophic forgetting of the \u2026"}, {"title": "EEG-Defender: Defending against Jailbreak through Early Exit Generation of Large Language Models", "link": "https://arxiv.org/pdf/2408.11308", "details": "C Zhao, Z Dou, K Huang - arXiv preprint arXiv:2408.11308, 2024", "abstract": "Large Language Models (LLMs) are increasingly attracting attention in various applications. Nonetheless, there is a growing concern as some users attempt to exploit these models for malicious purposes, including the synthesis of controlled \u2026"}, {"title": "Unlocking Adversarial Suffix Optimization Without Affirmative Phrases: Efficient Black-box Jailbreaking via LLM as Optimizer", "link": "https://arxiv.org/pdf/2408.11313", "details": "W Jiang, Z Wang, J Zhai, S Ma, Z Zhao, C Shen - arXiv preprint arXiv:2408.11313, 2024", "abstract": "Despite prior safety alignment efforts, mainstream LLMs can still generate harmful and unethical content when subjected to jailbreaking attacks. Existing jailbreaking methods fall into two main categories: template-based and optimization-based \u2026"}, {"title": "Efficient Detection of Toxic Prompts in Large Language Models", "link": "https://arxiv.org/pdf/2408.11727", "details": "Y Liu, J Yu, H Sun, L Shi, G Deng, Y Chen, Y Liu - arXiv preprint arXiv:2408.11727, 2024", "abstract": "Large language models (LLMs) like ChatGPT and Gemini have significantly advanced natural language processing, enabling various applications such as chatbots and automated content generation. However, these models can be \u2026"}, {"title": "Towards Harnessing Large Language Models as Autonomous Agents for Semantic Triple Extraction from Unstructured Text", "link": "https://ceur-ws.org/Vol-3747/text2kg_paper1.pdf", "details": "A Ananya, S Tiwari, N Mihindukulasooriya, T Soru\u2026 - 2024", "abstract": "Abstract The use of Large Language Models as autonomous agents interacting with tools has shown to improve the performance of several tasks from code generation to API calling and sequencing. This paper proposes a framework for using Large \u2026"}]
