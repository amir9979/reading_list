[{"title": "Efficient Architectures for High Resolution Vision-Language Models", "link": "https://arxiv.org/pdf/2501.02584", "details": "M Carvalho, B Martins - arXiv preprint arXiv:2501.02584, 2025", "abstract": "Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This \u2026"}, {"title": "Optimizing Fine-Tuning in Quantized Language Models: An In-Depth Analysis of Key Variables.", "link": "https://search.ebscohost.com/login.aspx%3Fdirect%3Dtrue%26profile%3Dehost%26scope%3Dsite%26authtype%3Dcrawler%26jrnl%3D15462218%26AN%3D182195836%26h%3DDkM7H%252BNd4BEWKjE7TEkwhwLcnO19xZgFOBIOwZt8r9Fzq0LyL73eEmZznOSwUtStliyGVMcyE3esDPWIq7NApw%253D%253D%26crl%3Dc", "details": "A Shen, Z Lai, D Li, X Hu - Computers, Materials & Continua, 2025", "abstract": "Abstract Large-scale Language Models (LLMs) have achieved significant breakthroughs in Natural Language Processing (NLP), driven by the pre-training and fine-tuning paradigm. While this approach allows models to specialize in specific \u2026"}, {"title": "VRCP: Vocabulary Replacement Continued Pretraining for Efficient Multilingual Language Models", "link": "https://aclanthology.org/2025.sumeval-2.5.pdf", "details": "Y Nozaki, D Nakashima, R Sato, N Asaba - Proceedings of the Second Workshop on \u2026, 2025", "abstract": "Building large language models (LLMs) for non-English languages involves leveraging extensively trained English models through continued pre-training on the target language corpora. This approach harnesses the rich semantic knowledge \u2026"}, {"title": "Instruction-Following Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2501.02086", "details": "B Hou, Q Chen, J Wang, G Yin, C Wang, N Du, R Pang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from \u2026"}, {"title": "The Power of Negative Zero: Datatype Customization for Quantized Large Language Models", "link": "https://arxiv.org/pdf/2501.04052", "details": "Y Chen, X Dai, C Chang, Y Akhauri, MS Abdelfattah - arXiv preprint arXiv:2501.04052, 2025", "abstract": "Large language models (LLMs) have demonstrated remarkable performance across various machine learning tasks, quickly becoming one of the most prevalent AI workloads. Yet the substantial memory requirement of LLMs significantly hinders \u2026"}, {"title": "Exploring the Reliability of Large Language Models as Customized Evaluators for Diverse NLP Tasks", "link": "https://aclanthology.org/2025.coling-main.688.pdf", "details": "Q Li, L Cui, L Kong, W Bi - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Previous work adopts large language models (LLMs) as evaluators to evaluate natural language process (NLP) tasks. However, certain shortcomings, eg, fairness, scope, and accuracy, persist for current LLM evaluators. To analyze whether LLMs \u2026"}, {"title": "Analyzing Memorization in Large Language Models through the Lens of Model Attribution", "link": "https://arxiv.org/pdf/2501.05078", "details": "TR Menta, S Agrawal, C Agarwal - arXiv preprint arXiv:2501.05078, 2025", "abstract": "Large Language Models (LLMs) are prevalent in modern applications but often memorize training data, leading to privacy breaches and copyright issues. Existing research has mainly focused on posthoc analyses, such as extracting memorized \u2026"}]
