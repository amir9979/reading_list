[{"title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs", "link": "https://arxiv.org/pdf/2503.01743%3F", "details": "A Abouelenin, A Ashfaq, A Atkinson, H Awadalla\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent \u2026"}, {"title": "Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model for Advanced Medical Decision Support", "link": "https://arxiv.org/pdf/2502.18274%3F", "details": "G Wang, M Gao, S Yang, Y Zhang, L He, L Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs), particularly those with reasoning capabilities, have rapidly advanced in recent years, demonstrating significant potential across a wide range of applications. However, their deployment in healthcare, especially in disease \u2026"}, {"title": "Rethinking Data: Towards Better Performing Domain-Specific Small Language Models", "link": "https://arxiv.org/pdf/2503.01464", "details": "B Nazarov, D Frolova, Y Lubarsky, A Gaissinski\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Fine-tuning of Large Language Models (LLMs) for downstream tasks, performed on domain-specific data has shown significant promise. However, commercial use of such LLMs is limited by the high computational cost required for their deployment at \u2026"}, {"title": "Unifying 2D and 3D Vision-Language Understanding", "link": "https://arxiv.org/pdf/2503.10745", "details": "A Jain, A Swerdlow, Y Wang, S Arnaud, A Martin, A Sax\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Progress in 3D vision-language learning has been hindered by the scarcity of large- scale 3D datasets. We introduce UniVLG, a unified architecture for 2D and 3D vision- language understanding that bridges the gap between existing 2D-centric models \u2026"}, {"title": "Hierarchical Vision\u2013Language Pre-Training with Freezing Strategy for Multi-Level Semantic Alignment", "link": "https://www.mdpi.com/2079-9292/14/4/816", "details": "H Xie, Y Qin, S Ding - Electronics, 2025", "abstract": "Vision\u2013language pre-training (VLP) faces challenges in aligning hierarchical textual semantics (words/phrases/sentences) with multi-scale visual features (objects/relations/global context). We propose a hierarchical VLP model (HieVLP) \u2026"}, {"title": "Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection", "link": "https://arxiv.org/pdf/2503.11794", "details": "B Li, F Wang, W Zhou, N Xu, B Zhou, S Zhang, H Poon\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-Language Models (VLMs) leverage aligned visual encoders to transform images into visual tokens, allowing them to be processed similarly to text by the backbone large language model (LLM). This unified input paradigm enables VLMs to \u2026"}, {"title": "Multilingual Language Model Pretraining using Machine-translated Data", "link": "https://arxiv.org/pdf/2502.13252", "details": "J Wang, Y Lu, M Weber, M Ryabinin, D Adelani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-resource languages such as English, enables the pretraining of high-quality large language models (LLMs). The same can not be said for most other languages as LLMs still underperform for non-English languages, likely due to a gap in the \u2026"}, {"title": "RetriEVAL: Evaluating Text Generation with Contextualized Lexical Match", "link": "https://dl.acm.org/doi/abs/10.1145/3701551.3703581", "details": "Z Li, X Li, C Tao, J Feng, T Shen, C Xu, H Wang\u2026 - Proceedings of the \u2026, 2025", "abstract": "Pre-trained language models have made significant advancements in text generation tasks. Nevertheless, evaluating the generated text with automatic metrics is still challenging. Compared with supervised metrics, unsupervised metrics which \u2026"}, {"title": "X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography from Chest Radiography via Tri-Modal Contrastive Learning", "link": "https://arxiv.org/pdf/2503.02162", "details": "J You, Y Gao, S Kim, C Mcintosh - arXiv preprint arXiv:2503.02162, 2025", "abstract": "Computed tomography (CT) is a key imaging modality for diagnosis, yet its clinical utility is marred by high radiation exposure and long turnaround times, restricting its use for larger-scale screening. Although chest radiography (CXR) is more accessible \u2026"}]
