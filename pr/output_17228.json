[{"title": "MIMO: A **Medical** Vision Language Model with Visual Referring Multimodal Input and Pixel Grounding Multimodal Output", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_MIMO_A_Medical_Vision_Language_Model_with_Visual_Referring_Multimodal_CVPR_2025_paper.pdf", "details": "Y Chen, D Xu, Y Huang, S Zhan, H Wang, D Chen\u2026 - Proceedings of the \u2026, 2025", "abstract": "\u2026 , **medical** vision language models are widely used in **medical** vision **question** **answering** tasks. \u2026 pre-training with frozen image encoders and **large** **language** **models**. arXiv preprint arXiv:\u2026 -labeled knowledgeenhanced dataset for **medical** \u2026"}, {"title": "PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain", "link": "https://arxiv.org/pdf/2506.00250", "details": "MJR Kalahroodi, A Sheikholselami, S Karimi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) have achieved remarkable performance on a wide range of NLP benchmarks, often surpassing human-\u2026 (on the **answered** subset) and coverage (the fraction of **questions** **answered** ). Aggregating across all \u2026", "entry_id": "http://arxiv.org/abs/2506.00250v2", "updated": "2025-06-03 00:22:37", "published": "2025-05-30 21:34:30", "authors": "Mohammad Javad Ranjbar Kalahroodi;Amirhossein Sheikholselami;Sepehr Karimi;Sepideh Ranjbar Kalahroodi;Heshaam Faili;Azadeh Shakery", "summary": "Large Language Models (LLMs) have achieved remarkable performance on a wide\nrange of NLP benchmarks, often surpassing human-level accuracy. However, their\nreliability in high-stakes domains such as medicine, particularly in\nlow-resource languages, remains underexplored. In this work, we introduce\nPersianMedQA, a large-scale, expert-validated dataset of multiple-choice\nPersian medical questions, designed to evaluate LLMs across both Persian and\nEnglish. We benchmark over 40 state-of-the-art models, including\ngeneral-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and\nchain-of-thought (CoT) settings. Our results show that closed-source general\nmodels (e.g., GPT-4.1) consistently outperform all other categories, achieving\n83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models\nsuch as Dorna underperform significantly (e.g., 35.9% in Persian), often\nstruggling with both instruction-following and domain reasoning. We also\nanalyze the impact of translation, showing that while English performance is\ngenerally higher, Persian responses are sometimes more accurate due to cultural\nand clinical contextual cues. Finally, we demonstrate that model size alone is\ninsufficient for robust performance without strong domain or language\nadaptation. PersianMedQA provides a foundation for evaluating multilingual and\nculturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be\naccessed at: https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.IT;math.IT", "links": "http://arxiv.org/abs/2506.00250v2;http://arxiv.org/pdf/2506.00250v2", "pdf_url": "http://arxiv.org/pdf/2506.00250v2"}, {"title": "ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists", "link": "https://arxiv.org/pdf/2506.01241", "details": "J Ruan, I Nair, S Cao, A Liu, S Munir\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Beyond **question** **answering** , the application-driven tasks in EXPERTLONGBENCH demand long-form outputs that can exceed 5,000 \u2026 We benchmark 11 **large** **language** **models** (LLMs) and analyze components in CLEAR \u2026", "entry_id": "http://arxiv.org/abs/2506.01241v1", "updated": "2025-06-02 01:39:02", "published": "2025-06-02 01:39:02", "authors": "Jie Ruan;Inderjeet Nair;Shuyang Cao;Amy Liu;Sheza Munir;Micah Pollens-Dempsey;Tiffany Chiang;Lucy Kates;Nicholas David;Sihan Chen;Ruxin Yang;Yuqian Yang;Jasmine Gump;Tessa Bialek;Vivek Sankaran;Margo Schlanger;Lu Wang", "summary": "This paper introduces ExpertLongBench, an expert-level benchmark containing\n11 tasks from 9 domains that reflect realistic expert workflows and\napplications. Beyond question answering, the application-driven tasks in\nExpertLongBench demand long-form outputs that can exceed 5,000 tokens and\nstrict adherence to domain-specific requirements. Notably, each task in\nExpertLongBench includes a rubric, designed or validated by domain experts, to\nspecify task requirements and guide output evaluation. Furthermore, we propose\nCLEAR, an evaluation framework that supports accurate evaluation of long-form\nmodel outputs in our benchmark. To achieve fine-grained, expert-aligned\nevaluation, CLEAR derives checklists from both model outputs and references by\nextracting information corresponding to items in the task-specific rubric.\nChecklist items for model outputs are then compared with corresponding items\nfor reference outputs to assess their correctness, enabling grounded\nevaluation. We benchmark 11 large language models (LLMs) and analyze components\nin CLEAR, showing that (1) existing LLMs, with the top performer achieving only\na 26.8% F1 score, require significant improvement for expert-level tasks; (2)\nmodels can generate content corresponding to the required aspects, though often\nnot accurately; and (3) accurate checklist extraction and comparison in CLEAR\ncan be achieved by open-weight models for more scalable and low-cost usage.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.01241v1;http://arxiv.org/pdf/2506.01241v1", "pdf_url": "http://arxiv.org/pdf/2506.01241v1"}, {"title": "Natural Language Processing (NLP) in Healthcare AI: Enhancing **Clinical** Insight Extraction from Unstructured Patient Data", "link": "https://www.authorea.com/doi/pdf/10.22541/au.174845034.41890518", "details": "V Derek, P Collings - 2025", "abstract": "\u2026 **clinical** insights. It details key NLP tasks\u2014including Named Entity Recognition, Relation Extraction, Text Summarization, and **Question** **Answering** \u2014\u2026 Finally, the paper looks toward future directions, including the impact of **Large** **Language** \u2026"}, {"title": "VM14K: First Vietnamese Medical Benchmark", "link": "https://arxiv.org/pdf/2506.01305", "details": "T Nguyen, D Nguyen, M Dang, T Dao, L Nguyen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) in healthcare has become more prominent than ever, projecting tremendous impacts across various fields such as **clinical** research, diagnosis support systems and other different **medical** \u2026 dataset for **medical** domain \u2026", "entry_id": "http://arxiv.org/abs/2506.01305v1", "updated": "2025-06-02 04:32:15", "published": "2025-06-02 04:32:15", "authors": "Thong Nguyen;Duc Nguyen;Minh Dang;Thai Dao;Long Nguyen;Quan H. Nguyen;Dat Nguyen;Kien Tran;Minh Tran", "summary": "Medical benchmarks are indispensable for evaluating the capabilities of\nlanguage models in healthcare for non-English-speaking communities,therefore\nhelp ensuring the quality of real-life applications. However, not every\ncommunity has sufficient resources and standardized methods to effectively\nbuild and design such benchmark, and available non-English medical data is\nnormally fragmented and difficult to verify. We developed an approach to tackle\nthis problem and applied it to create the first Vietnamese medical question\nbenchmark, featuring 14,000 multiple-choice questions across 34 medical\nspecialties. Our benchmark was constructed using various verifiable sources,\nincluding carefully curated medical exams and clinical records, and eventually\nannotated by medical experts. The benchmark includes four difficulty levels,\nranging from foundational biological knowledge commonly found in textbooks to\ntypical clinical case studies that require advanced reasoning. This design\nenables assessment of both the breadth and depth of language models' medical\nunderstanding in the target language thanks to its extensive coverage and\nin-depth subject-specific expertise. We release the benchmark in three parts: a\nsample public set (4k questions), a full public set (10k questions), and a\nprivate set (2k questions) used for leaderboard evaluation. Each set contains\nall medical subfields and difficulty levels. Our approach is scalable to other\nlanguages, and we open-source our data construction pipeline to support the\ndevelopment of future multilingual benchmarks in the medical domain.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.01305v1;http://arxiv.org/pdf/2506.01305v1", "pdf_url": "http://arxiv.org/pdf/2506.01305v1"}, {"title": "Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation", "link": "https://arxiv.org/pdf/2506.00612", "details": "R Yang, W Deng, M Chen, Y Zhou, X Li - arXiv preprint arXiv:2506.00612, 2025", "abstract": "\u2026 Given a Multi-Choice **question** **answer** pair (Q, A, Oori), we ulize the Language Model LLM \u2026 We evaluate six **large** **language** **models** in different sizes across all datasets: DeepSeek V3 (\u2026 ing **large** **language** **models** on **answering** and explaining \u2026", "entry_id": "http://arxiv.org/abs/2506.00612v2", "updated": "2025-06-03 05:28:26", "published": "2025-05-31 15:51:09", "authors": "Running Yang;Wenlong Deng;Minghui Chen;Yuyin Zhou;Xiaoxiao Li", "summary": "Clinical tasks such as diagnosis and treatment require strong decision-making\nabilities, highlighting the importance of rigorous evaluation benchmarks to\nassess the reliability of large language models (LLMs). In this work, we\nintroduce a knowledge-guided data augmentation framework that enhances the\ndifficulty of clinical multiple-choice question (MCQ) datasets by generating\ndistractors (i.e., incorrect choices that are similar to the correct one and\nmay confuse existing LLMs). Using our KG-based pipeline, the generated choices\nare both clinically plausible and deliberately misleading. Our approach\ninvolves multi-step, semantically informed walks on a medical knowledge graph\nto identify distractor paths-associations that are medically relevant but\nfactually incorrect-which then guide the LLM in crafting more deceptive\ndistractors. We apply the designed knowledge graph guided distractor generation\n(KGGDG) pipline, to six widely used medical QA benchmarks and show that it\nconsistently reduces the accuracy of state-of-the-art LLMs. These findings\nestablish KGGDG as a powerful tool for enabling more robust and diagnostic\nevaluations of medical LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.00612v2;http://arxiv.org/pdf/2506.00612v2", "pdf_url": "http://arxiv.org/pdf/2506.00612v2"}, {"title": "Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs", "link": "https://arxiv.org/pdf/2506.00072", "details": "N Naderi, Z Atf, PR Lewis, SAA Safavi-Naini, A Soroush - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 This paper investigates the efficacy of prompt engineering techniques in enhancing both the accuracy and confidence elicitation of **Large** **Language** **Models** (LLMs) when applied to high-stakes **medical** contexts. A stratified dataset of Persian board \u2026", "entry_id": "http://arxiv.org/abs/2506.00072v1", "updated": "2025-05-29 17:13:26", "published": "2025-05-29 17:13:26", "authors": "Nariman Naderi;Zahra Atf;Peter R Lewis;Aref Mahjoub far;Seyed Amir Ahmad Safavi-Naini;Ali Soroush", "summary": "This paper investigates how prompt engineering techniques impact both\naccuracy and confidence elicitation in Large Language Models (LLMs) applied to\nmedical contexts. Using a stratified dataset of Persian board exam questions\nacross multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini,\nLlama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These\nconfigurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles\n(Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales\n(1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error\n(ECE) to evaluate alignment between confidence and actual performance.\nChain-of-Thought prompts improved accuracy but also led to overconfidence,\nhighlighting the need for calibration. Emotional prompting further inflated\nconfidence, risking poor decisions. Smaller models like Llama-3.1-8b\nunderperformed across all metrics, while proprietary models showed higher\naccuracy but still lacked calibrated confidence. These results suggest prompt\nengineering must address both accuracy and uncertainty to be effective in\nhigh-stakes medical tasks.", "comment": "This paper was accepted for presentation at the 7th International\n  Workshop on EXplainable, Trustworthy, and Responsible AI and Multi-Agent\n  Systems (EXTRAAMAS 2025). Workshop website:\n  https://extraamas.ehealth.hevs.ch/index.html", "journal_ref": null, "primary_category": "cs.CY", "categories": "cs.CY;cs.AI;cs.CL;cs.LG", "links": "http://arxiv.org/abs/2506.00072v1;http://arxiv.org/pdf/2506.00072v1", "pdf_url": "http://arxiv.org/pdf/2506.00072v1"}, {"title": "ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases", "link": "https://arxiv.org/pdf/2506.00095", "details": "Y Li, X Zeng, C Fang, J Yang, L Zhang - arXiv preprint arXiv:2506.00095, 2025", "abstract": "\u2026 Although **large** **language** **models** (LLMs) have shown promising performance in general **medical** **question** - **answering** tasks, the current evaluation \u2026 All models are evaluated in the five subsets of the two tasks: multiple-choice **question** **answering** \u2026", "entry_id": "http://arxiv.org/abs/2506.00095v3", "updated": "2025-06-04 03:25:49", "published": "2025-05-30 11:35:05", "authors": "Yuchong Li;Xiaojun Zeng;Chihua Fang;Jian Yang;Fucang Jia;Lei Zhang", "summary": "Hepato-pancreato-biliary (HPB) disorders represent a global public health\nchallenge due to their high morbidity and mortality. Although large language\nmodels (LLMs) have shown promising performance in general medical\nquestion-answering tasks, the current evaluation benchmarks are mostly derived\nfrom standardized examinations or manually designed questions, lacking HPB\ncoverage and clinical cases. To address these issues, we systematically\neatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended\nmultiple-choice questions and 337 open-ended real diagnosis cases, which\nencompasses all the 33 main categories and 465 subcategories of HPB diseases\ndefined in the International Statistical Classification of Diseases, 10th\nRevision (ICD-10). The multiple-choice questions are curated from public\ndatasets and synthesized data, and the clinical cases are collected from\nprestigious medical journals, case-sharing platforms, and collaborating\nhospitals. By evalauting commercial and open-source general and medical LLMs on\nour established benchmark, namely ClinBench-HBP, we find that while commercial\nLLMs perform competently on medical exam questions, they exhibit substantial\nperformance degradation on HPB diagnosis tasks, especially on complex,\ninpatient clinical cases. Those medical LLMs also show limited generalizability\nto HPB diseases. Our results reveal the critical limitations of current LLMs in\nthe domain of HPB diseases, underscoring the imperative need for future medical\nLLMs to handle real, complex clinical diagnostics rather than simple medical\nexam questions. The benchmark will be released at\nhttps://clinbench-hpb.github.io.", "comment": null, "journal_ref": null, "primary_category": "cs.CY", "categories": "cs.CY;cs.AI;cs.CL", "links": "http://arxiv.org/abs/2506.00095v3;http://arxiv.org/pdf/2506.00095v3", "pdf_url": "http://arxiv.org/pdf/2506.00095v3"}, {"title": "MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine", "link": "https://arxiv.org/pdf/2506.01252", "details": "S Kong, X Yang, Y Wei, Z Wang, H Tang, J Qin, S Lan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) have demonstrated remarkable potential in processing \u2026 focus narrowly on factual **question** **answering** or lack domain-specific tasks and **clinical** realism. To \u2026 To comprehensively evaluate the performance of \u2026", "entry_id": "http://arxiv.org/abs/2506.01252v1", "updated": "2025-06-02 02:01:40", "published": "2025-06-02 02:01:40", "authors": "Shufeng Kong;Xingru Yang;Yuanyuan Wei;Zijie Wang;Hao Tang;Jiuqi Qin;Shuting Lan;Yingheng Wang;Junwen Bai;Zhuangbin Chen;Zibin Zheng;Caihua Liu;Hao Liang", "summary": "Traditional Chinese Medicine (TCM) is a holistic medical system with\nmillennia of accumulated clinical experience, playing a vital role in global\nhealthcare-particularly across East Asia. However, the implicit reasoning,\ndiverse textual forms, and lack of standardization in TCM pose major challenges\nfor computational modeling and evaluation. Large Language Models (LLMs) have\ndemonstrated remarkable potential in processing natural language across diverse\ndomains, including general medicine. Yet, their systematic evaluation in the\nTCM domain remains underdeveloped. Existing benchmarks either focus narrowly on\nfactual question answering or lack domain-specific tasks and clinical realism.\nTo fill this gap, we introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs\non TCM Knowledge, Reasoning, and Safety. Developed in collaboration with\ncertified TCM experts, MTCMB comprises 12 sub-datasets spanning five major\ncategories: knowledge QA, language understanding, diagnostic reasoning,\nprescription generation, and safety evaluation. The benchmark integrates\nreal-world case records, national licensing exams, and classical texts,\nproviding an authentic and comprehensive testbed for TCM-capable models.\nPreliminary results indicate that current LLMs perform well on foundational\nknowledge but fall short in clinical reasoning, prescription planning, and\nsafety compliance. These findings highlight the urgent need for domain-aligned\nbenchmarks like MTCMB to guide the development of more competent and\ntrustworthy medical AI systems. All datasets, code, and evaluation tools are\npublicly available at: https://github.com/Wayyuanyuan/MTCMB.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.01252v1;http://arxiv.org/pdf/2506.01252v1", "pdf_url": "http://arxiv.org/pdf/2506.01252v1"}]
