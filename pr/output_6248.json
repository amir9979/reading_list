[{"title": "Towards Case-based Interpretability for Medical Federated Learning", "link": "https://arxiv.org/pdf/2408.13626", "details": "L Latorre, L Petrychenko, R Beets-Tan, T Kopytova\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We explore deep generative models to generate case-based explanations in a medical federated learning setting. Explaining AI model decisions through case- based interpretability is paramount to increasing trust and allowing widespread \u2026"}, {"title": "Advancing graph counterfactual fairness through fair representation learning", "link": "https://link.springer.com/chapter/10.1007/978-3-031-70368-3_3", "details": "Z Wang, Z Chu, R Blanco, Z Chen, SC Chen, W Zhang - Joint European Conference \u2026, 2024", "abstract": "Graph neural networks (GNNs) have shown remarkable success in various domains. Nonetheless, studies have shown that GNNs may inherit and amplify societal bias, which critically hinders their application in high-stakes scenarios. Although efforts \u2026"}, {"title": "Fair Federated Learning with Biased Vision-Language Models", "link": "https://aclanthology.org/2024.findings-acl.595.pdf", "details": "H Zeng, Z Yue, Y Zhang, L Shang, D Wang - Findings of the Association for \u2026, 2024", "abstract": "Existing literature that integrates CLIP into federated learning (FL) largely ignores the inherent group unfairness within CLIP and its ethical implications on FL applications. Furthermore, such CLIP bias may be amplified in FL, due to the unique issue of data \u2026"}, {"title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding", "link": "https://arxiv.org/pdf/2409.03363", "details": "C Wang, Y Wang, B Hooi, Y Cai, N Peng, KW Chang - arXiv preprint arXiv \u2026, 2024", "abstract": "The training data in large language models is key to their success, but it also presents privacy and security risks, as it may contain sensitive information. Detecting pre-training data is crucial for mitigating these concerns. Existing methods typically \u2026"}, {"title": "InstructCoder: Instruction Tuning Large Language Models for Code Editing", "link": "https://aclanthology.org/2024.acl-srw.6.pdf", "details": "K Li, Q Hu, J Zhao, H Chen, Y Xie, T Liu, M Shieh, J He - Proceedings of the 62nd \u2026, 2024", "abstract": "Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data \u2026"}, {"title": "Safety Layers of Aligned Large Language Models: The Key to LLM Security", "link": "https://arxiv.org/pdf/2408.17003", "details": "S Li, L Yao, L Zhang, Y Li - arXiv preprint arXiv:2408.17003, 2024", "abstract": "Aligned LLMs are highly secure, capable of recognizing and refusing to answer malicious questions. However, the role of internal parameters in maintaining this security is not well understood, further these models are vulnerable to security \u2026"}, {"title": "CodeAttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion", "link": "https://aclanthology.org/2024.findings-acl.679.pdf", "details": "Q Ren, C Gao, J Shao, J Yan, X Tan, W Lam, L Ma - Findings of the Association for \u2026, 2024", "abstract": "The rapid advancement of Large Language Models (LLMs) has brought about remarkable generative capabilities but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from \u2026"}]
