[{"title": "MedOrch: Medical Diagnosis with Tool-Augmented Reasoning Agents for Flexible Extensibility", "link": "https://arxiv.org/pdf/2506.00235", "details": "Y He, A Li, B Liu, Z Yao, Y He - arXiv preprint arXiv:2506.00235, 2025", "abstract": "Healthcare decision-making represents one of the most challenging domains for Artificial Intelligence (AI), requiring the integration of diverse knowledge sources, complex reasoning, and various external analytical tools. Current AI systems often \u2026", "entry_id": "http://arxiv.org/abs/2506.00235v1", "updated": "2025-05-30 21:13:12", "published": "2025-05-30 21:13:12", "authors": "Yexiao He;Ang Li;Boyi Liu;Zhewei Yao;Yuxiong He", "summary": "Healthcare decision-making represents one of the most challenging domains for\nArtificial Intelligence (AI), requiring the integration of diverse knowledge\nsources, complex reasoning, and various external analytical tools. Current AI\nsystems often rely on either task-specific models, which offer limited\nadaptability, or general language models without grounding with specialized\nexternal knowledge and tools. We introduce MedOrch, a novel framework that\norchestrates multiple specialized tools and reasoning agents to provide\ncomprehensive medical decision support. MedOrch employs a modular, agent-based\narchitecture that facilitates the flexible integration of domain-specific tools\nwithout altering the core system. Furthermore, it ensures transparent and\ntraceable reasoning processes, enabling clinicians to meticulously verify each\nintermediate step underlying the system's recommendations. We evaluate MedOrch\nacross three distinct medical applications: Alzheimer's disease diagnosis,\nchest X-ray interpretation, and medical visual question answering, using\nauthentic clinical datasets. The results demonstrate MedOrch's competitive\nperformance across these diverse medical tasks. Notably, in Alzheimer's disease\ndiagnosis, MedOrch achieves an accuracy of 93.26%, surpassing the\nstate-of-the-art baseline by over four percentage points. For predicting\nAlzheimer's disease progression, it attains a 50.35% accuracy, marking a\nsignificant improvement. In chest X-ray analysis, MedOrch exhibits superior\nperformance with a Macro AUC of 61.2% and a Macro F1-score of 25.5%. Moreover,\nin complex multimodal visual question answering (Image+Table), MedOrch achieves\nan accuracy of 54.47%. These findings underscore MedOrch's potential to advance\nhealthcare AI by enabling reasoning-driven tool utilization for multimodal\nmedical data processing and supporting intricate cognitive tasks in clinical\ndecision-making.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.00235v1;http://arxiv.org/pdf/2506.00235v1", "pdf_url": "http://arxiv.org/pdf/2506.00235v1"}, {"title": "Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive Vision-Language Pretraining", "link": "https://arxiv.org/pdf/2506.00633", "details": "D Molino, CM Caruso, F Ruffini, P Soda, V Guarrasi - arXiv preprint arXiv:2506.00633, 2025", "abstract": "Objective: While recent advances in text-conditioned generative models have enabled the synthesis of realistic medical images, progress has been largely confined to 2D modalities such as chest X-rays. Extending text-to-image generation \u2026", "entry_id": "http://arxiv.org/abs/2506.00633v1", "updated": "2025-05-31 16:41:55", "published": "2025-05-31 16:41:55", "authors": "Daniele Molino;Camillo Maria Caruso;Filippo Ruffini;Paolo Soda;Valerio Guarrasi", "summary": "Objective: While recent advances in text-conditioned generative models have\nenabled the synthesis of realistic medical images, progress has been largely\nconfined to 2D modalities such as chest X-rays. Extending text-to-image\ngeneration to volumetric Computed Tomography (CT) remains a significant\nchallenge, due to its high dimensionality, anatomical complexity, and the\nabsence of robust frameworks that align vision-language data in 3D medical\nimaging. Methods: We introduce a novel architecture for Text-to-CT generation\nthat combines a latent diffusion model with a 3D contrastive vision-language\npretraining scheme. Our approach leverages a dual-encoder CLIP-style model\ntrained on paired CT volumes and radiology reports to establish a shared\nembedding space, which serves as the conditioning input for generation. CT\nvolumes are compressed into a low-dimensional latent space via a pretrained\nvolumetric VAE, enabling efficient 3D denoising diffusion without requiring\nexternal super-resolution stages. Results: We evaluate our method on the\nCT-RATE dataset and conduct a comprehensive assessment of image fidelity,\nclinical relevance, and semantic alignment. Our model achieves competitive\nperformance across all tasks, significantly outperforming prior baselines for\ntext-to-CT generation. Moreover, we demonstrate that CT scans synthesized by\nour framework can effectively augment real data, improving downstream\ndiagnostic performance. Conclusion: Our results show that modality-specific\nvision-language alignment is a key component for high-quality 3D medical image\ngeneration. By integrating contrastive pretraining and volumetric diffusion,\nour method offers a scalable and controllable solution for synthesizing\nclinically meaningful CT volumes from text, paving the way for new applications\nin data augmentation, medical education, and automated clinical simulation.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2506.00633v1;http://arxiv.org/pdf/2506.00633v1", "pdf_url": "http://arxiv.org/pdf/2506.00633v1"}, {"title": "Towards All-in-One Medical Image Re-Identification Supplementary Material", "link": "https://openaccess.thecvf.com/content/CVPR2025/supplemental/Tian_Towards_All-in-One_Medical_CVPR_2025_supplemental.pdf", "details": "Y Tian, K Ji, R Zhang, Y Jiang, C Li, X Wang, G Zhai - Histopathology", "abstract": "We assess the performance of Large Multi-modality Models (LMMs) on ReID task, by using Yes/No question-answer pairs. We input the LMMs with a pair of medical images, and query if they are from the same patient. Table 1 shows that general \u2026"}, {"title": "VISTA3D: A unified segmentation foundation model for 3D medical imaging", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/He_VISTA3D_A_Unified_Segmentation_Foundation_Model_For_3D_Medical_Imaging_CVPR_2025_paper.pdf", "details": "Y He, P Guo, Y Tang, A Myronenko, V Nath, Z Xu\u2026 - Proceedings of the \u2026, 2025", "abstract": "Foundation models for interactive segmentation in 2D natural images and videos have sparked significant interest in building 3D foundation models for medical imaging. However, the domain gaps and clinical use cases for 3D medical imaging \u2026"}]
