[{"title": "Multistain Pretraining for Slide Representation Learning in Pathology", "link": "https://arxiv.org/pdf/2408.02859", "details": "G Jaume, A Vaidya, A Zhang, AH Song, RJ Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Developing self-supervised learning (SSL) models that can learn universal and transferable representations of H&E gigapixel whole-slide images (WSIs) is becoming increasingly valuable in computational pathology. These models hold the \u2026"}, {"title": "Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement", "link": "https://arxiv.org/pdf/2408.03092", "details": "L Yu, B Yu, H Yu, F Huang, Y Li - arXiv preprint arXiv:2408.03092, 2024", "abstract": "Merging Large Language Models (LLMs) aims to amalgamate multiple homologous LLMs into one with all the capabilities. Ideally, any LLMs sharing the same backbone should be mergeable, irrespective of whether they are Fine-Tuned (FT) with minor \u2026"}, {"title": "GPT-4o and the Quest for Machine Learning Interpretability in ICU Mortality Prediction", "link": "https://www.researchsquare.com/article/rs-4816139/latest.pdf", "details": "ME Samadi, K Nikulina, SJ Fritsch, A Schuppert - 2024", "abstract": "Background Clinical utilization of machine learning is hampered by the lack of interpretability inherent in most non-linear black box modeling approaches, reducing trust among clinicians and regulators. Advanced large language models, such as \u2026"}, {"title": "UIT-2Q2T at ImageCLEFmedical 2024 Caption: Multimodal medical image captioning using Bootstrapping Language-Image Pre-training", "link": "https://ceur-ws.org/Vol-3740/paper-159.pdf", "details": "TV Phan, TK Nguyen, QADD Hoang, QT Phan\u2026 - 2024", "abstract": "Introduction: Medical image captioning is an important AI task in healthcare, automating the generation of text descriptions to support the management and interpretation of medical images. Our team, UIT-2Q2T, participated in the second task \u2026"}]
