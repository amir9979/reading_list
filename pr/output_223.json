'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [RIFF: Learning to Rephrase Inputs for Few-shot Fine-t'
[{"title": "Anatomical Structure-Guided Medical Vision-Language Pre-training", "link": "https://arxiv.org/html/2403.09294v1", "details": "Q Li, X Yan, J Xu, R Yuan, Y Zhang, R Feng, Q Shen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Learning medical visual representations through vision-language pre-training has reached remarkable progress. Despite the promising performance, it still faces challenges, ie, local alignment lacks interpretability and clinical relevance, and the \u2026"}, {"title": "Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods", "link": "https://arxiv.org/pdf/2403.00998", "details": "P Tsvilodub, H Wang, S Grosch, M Franke - arXiv preprint arXiv:2403.00998, 2024", "abstract": "This paper systematically compares different methods of deriving item-level predictions of language models for multiple-choice tasks. It compares scoring methods for answer options based on free generation of responses, various \u2026"}, {"title": "GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture", "link": "https://arxiv.org/pdf/2403.11858", "details": "S Yang, Z Yuan, S Li, R Peng, K Liu, P Yang - arXiv preprint arXiv:2403.11858, 2024", "abstract": "In the rapidly evolving field of artificial intelligence (AI), the application of large language models (LLMs) in agriculture, particularly in pest management, remains nascent. We aimed to prove the feasibility by evaluating the content of the pest \u2026"}, {"title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models", "link": "https://arxiv.org/pdf/2403.08281", "details": "N Ding, Y Chen, G Cui, X Lv, R Xie, B Zhou, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three \u2026"}, {"title": "$\\texttt {COSMIC} $: Mutual Information for Task-Agnostic Summarization Evaluation", "link": "https://arxiv.org/pdf/2402.19457", "details": "M Darrin, P Formont, JCK Cheung, P Piantanida - arXiv preprint arXiv:2402.19457, 2024", "abstract": "Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks \u2026"}, {"title": "MagicClay: Sculpting Meshes With Generative Neural Fields", "link": "https://arxiv.org/pdf/2403.02460", "details": "A Barda, VG Kim, N Aigerman, AH Bermano, T Groueix - arXiv preprint arXiv \u2026, 2024", "abstract": "The recent developments in neural fields have brought phenomenal capabilities to the field of shape generation, but they lack crucial properties, such as incremental control-a fundamental requirement for artistic work. Triangular meshes, on the other \u2026"}, {"title": "SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large Vision Language Models", "link": "https://arxiv.org/pdf/2403.13263", "details": "T Yue, J Cheng, L Guo, X Dai, Z Zhao, X He, G Xiong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent trends in Large Vision Language Models (LVLMs) research have been increasingly focusing on advancing beyond general image understanding towards more nuanced, object-level referential comprehension. In this paper, we present and \u2026"}, {"title": "Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting", "link": "https://arxiv.org/pdf/2403.13369", "details": "P Richter-Pechanski, P Wiesenbach, DM Schwab\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent \u2026"}, {"title": "Language models scale reliably with over-training and on downstream tasks", "link": "https://arxiv.org/pdf/2403.08540", "details": "SY Gadre, G Smyrnis, V Shankar, S Gururangan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute \u2026"}]
