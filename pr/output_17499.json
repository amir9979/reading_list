[{"title": "Evaluating Large Language Models on Non-Code Software Engineering Tasks", "link": "https://arxiv.org/pdf/2506.10833", "details": "FC Pe\u00f1a, S Herbold - arXiv preprint arXiv:2506.10833, 2025", "abstract": "**Large** **Language** **Models** (LLMs) have demonstrated remarkable capabilities in code understanding and generation; however, their effectiveness on non-code Software Engineering (SE) tasks remains underexplored. We present the first comprehensive \u2026", "entry_id": "http://arxiv.org/abs/2506.10833v1", "updated": "2025-06-12 15:52:32", "published": "2025-06-12 15:52:32", "authors": "Fabian C. Pe\u00f1a;Steffen Herbold", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode understanding and generation; however, their effectiveness on non-code\nSoftware Engineering (SE) tasks remains underexplored. We present the first\ncomprehensive benchmark, which we name `Software Engineering Language\nUnderstanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from\nidentifying whether a requirement is functional or non-functional to estimating\nthe effort and complexity of backlog items. SELU covers classification,\nregression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)\ntargets, with data drawn from diverse sources such as code repositories, issue\ntracking systems, and developer forums. We fine-tune 22 open-source LLMs,\nprompt two proprietary alternatives, and train two baselines. Performance is\nmeasured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and\ncompared via the Bayesian signed-rank test. Our results show that\nmoderate-scale decoder-only models consistently form a top-tier, exhibiting\nhigh mean performance and low across-task variance, while domain adaptation via\ncode-focused pre-training might yield only modest improvements. These insights\nguide model selection for non-code SE workflows and highlight directions for\nexpanding SELU to generative and design-oriented scenarios.", "comment": null, "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE", "links": "http://arxiv.org/abs/2506.10833v1;http://arxiv.org/pdf/2506.10833v1", "pdf_url": "http://arxiv.org/pdf/2506.10833v1"}, {"title": "**Evaluating Large Language Models** for Preoperative Patient Education in Superior Capsular Reconstruction: Comparative Study of Claude, GPT, and Gemini", "link": "https://periop.jmir.org/2025/1/e70047", "details": "Y Liu, H Li, J Ouyang, Z Xue, M Wang, H He, B Song\u2026 - JMIR Perioperative \u2026, 2025", "abstract": "\u2026 The subjects of this study are LLMs ( **large** **language** **models** ). Besides being used as operational models, LLMs also serve as tools for translating Chinese content into English, as detailed in Multimedia Appendix 1. The specific types of models used \u2026"}, {"title": "Primer on **large language models** : an educational overview for intensivists", "link": "https://ccforum.biomedcentral.com/articles/10.1186/s13054-025-05479-4", "details": "D Idan, S Einav - Critical Care, 2025", "abstract": "The integration of artificial intelligence (AI) and machine learning-enabled medical technologies into clinical practice is expanding at an unprecedented pace. Among these, **large** **language** **models** (LLMs) represent a subset of machine learning \u2026"}, {"title": "Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph Learning", "link": "https://arxiv.org/pdf/2506.10282", "details": "J Liu, D Fan, J Shen, C Ji, D Zha, Q Tan - arXiv preprint arXiv:2506.10282, 2025", "abstract": "\u2026 Some efforts [32, 36] have **evaluated** the potential of MLLM-as-Encoder for GNN-based methods \u2026 Moreover, they fail to provide a comprehensive **evaluation** of MLLMs used directly as graph \u2026 Second, we **evaluate** LLMbased methods in MMGs by \u2026", "entry_id": "http://arxiv.org/abs/2506.10282v1", "updated": "2025-06-12 01:44:46", "published": "2025-06-12 01:44:46", "authors": "Jiajin Liu;Dongzhe Fan;Jiacheng Shen;Chuanhao Ji;Daochen Zha;Qiaoyu Tan", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in representing and understanding diverse modalities. However,\nthey typically focus on modality alignment in a pairwise manner while\noverlooking structural relationships across data points. Integrating\nmultimodality with structured graph information (i.e., multimodal graphs, MMGs)\nis essential for real-world applications such as social networks, healthcare,\nand recommendation systems. Existing MMG learning methods fall into three\nparadigms based on how they leverage MLLMs: Encoder, Aligner, and Predictor.\nMLLM-as-Encoder focuses on enhancing graph neural networks (GNNs) via\nmultimodal feature fusion; MLLM-as-Aligner aligns multimodal attributes in\nlanguage or hidden space to enable LLM-based graph reasoning; MLLM-as-Predictor\ntreats MLLMs as standalone reasoners with in-context learning or fine-tuning.\nDespite their advances, the MMG field lacks a unified benchmark to fairly\nevaluate across these approaches, making it unclear what progress has been\nmade. To bridge this gap, we present Graph-MLLM, a comprehensive benchmark for\nmultimodal graph learning by systematically evaluating these three paradigms\nacross six datasets with different domains. Through extensive experiments, we\nobserve that jointly considering the visual and textual attributes of the nodes\nbenefits graph learning, even when using pre-trained text-to-image alignment\nmodels (e.g., CLIP) as encoders. We also find that converting visual attributes\ninto textual descriptions further improves performance compared to directly\nusing visual inputs. Moreover, we observe that fine-tuning MLLMs on specific\nMMGs can achieve state-of-the-art results in most scenarios, even without\nexplicit graph structure information. We hope that our open-sourced library\nwill facilitate rapid, equitable evaluation and inspire further innovative\nresearch in this field.", "comment": "16 pages, 4 figures", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2506.10282v1;http://arxiv.org/pdf/2506.10282v1", "pdf_url": "http://arxiv.org/pdf/2506.10282v1"}, {"title": "Query-Level Uncertainty in Large Language Models", "link": "https://arxiv.org/pdf/2506.09669", "details": "L Chen, G Varoquaux - arXiv preprint arXiv:2506.09669, 2025", "abstract": "\u2026 **Evaluation** Metrics We **evaluate** uncertainty by assessing whether a method can distinguish known and unknown queries, which can be treated as ranking problems, ie, a lower uncertainty means a model is more likely to know the answer to the query \u2026", "entry_id": "http://arxiv.org/abs/2506.09669v1", "updated": "2025-06-11 12:39:48", "published": "2025-06-11 12:39:48", "authors": "Lihu Chen;Ga\u00ebl Varoquaux", "summary": "It is important for Large Language Models to be aware of the boundary of\ntheir knowledge, the mechanism of identifying known and unknown queries. This\ntype of awareness can help models perform adaptive inference, such as invoking\nRAG, engaging in slow and deep thinking, or adopting the abstention mechanism,\nwhich is beneficial to the development of efficient and trustworthy AI. In this\nwork, we propose a method to detect knowledge boundaries via Query-Level\nUncertainty, which aims to determine if the model is able to address a given\nquery without generating any tokens. To this end, we introduce a novel and\ntraining-free method called \\emph{Internal Confidence}, which leverages\nself-evaluations across layers and tokens. Empirical results on both factual QA\nand mathematical reasoning tasks demonstrate that our internal confidence can\noutperform several baselines. Furthermore, we showcase that our proposed method\ncan be used for efficient RAG and model cascading, which is able to reduce\ninference costs while maintaining performance.", "comment": "In Progress", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.09669v1;http://arxiv.org/pdf/2506.09669v1", "pdf_url": "http://arxiv.org/pdf/2506.09669v1"}, {"title": "Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?", "link": "https://arxiv.org/pdf/2506.10415", "details": "Y Song, Y Du, D Paperno, A Gatt - arXiv preprint arXiv:2506.10415, 2025", "abstract": "\u2026 Additionally, we focus on multiplechoice questions to ensure structured **evaluation** and \u2026 employ published datasets and pretrained multimodal **large** **language** **models** , with no known \u2026 Mibench: **Evaluating** multimodal **large** **language** \u2026", "entry_id": "http://arxiv.org/abs/2506.10415v1", "updated": "2025-06-12 07:12:31", "published": "2025-06-12 07:12:31", "authors": "Yingjin Song;Yupei Du;Denis Paperno;Albert Gatt", "summary": "This paper introduces the TempVS benchmark, which focuses on temporal\ngrounding and reasoning capabilities of Multimodal Large Language Models\n(MLLMs) in image sequences. TempVS consists of three main tests (i.e., event\nrelation inference, sentence ordering and image ordering), each accompanied\nwith a basic grounding test. TempVS requires MLLMs to rely on both visual and\nlinguistic modalities to understand the temporal order of events. We evaluate\n38 state-of-the-art MLLMs, demonstrating that models struggle to solve TempVS,\nwith a substantial performance gap compared to human capabilities. We also\nprovide fine-grained insights that suggest promising directions for future\nresearch. Our TempVS benchmark data and code are available at\nhttps://github.com/yjsong22/TempVS.", "comment": "27 pages, 14 figures. Accepted to ACL 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.CV", "links": "http://arxiv.org/abs/2506.10415v1;http://arxiv.org/pdf/2506.10415v1", "pdf_url": "http://arxiv.org/pdf/2506.10415v1"}, {"title": "TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving", "link": "https://arxiv.org/pdf/2506.10674", "details": "V Colle, M Sana, N Piovesan, A De Domenico, F Ayed\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 PERFORMANCE **EVALUATION** In this section, we present the performance of popular opensource language models on TeleMath. In our \u2026 From the generated responses, we **evaluate** the model performance on TeleMath using two metrics: pass@1 \u2026", "entry_id": "http://arxiv.org/abs/2506.10674v1", "updated": "2025-06-12 13:04:18", "published": "2025-06-12 13:04:18", "authors": "Vincenzo Colle;Mohamed Sana;Nicola Piovesan;Antonio De Domenico;Fadhel Ayed;Merouane Debbah", "summary": "The increasing adoption of artificial intelligence in telecommunications has\nraised interest in the capability of Large Language Models (LLMs) to address\ndomain-specific, mathematically intensive tasks. Although recent advancements\nhave improved the performance of LLMs in general mathematical reasoning, their\neffectiveness within specialized domains, such as signal processing, network\noptimization, and performance analysis, remains largely unexplored. To address\nthis gap, we introduce TeleMath, the first benchmark dataset specifically\ndesigned to evaluate LLM performance in solving mathematical problems with\nnumerical solutions in the telecommunications domain. Comprising 500\nquestion-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the\ntelecommunications field. This paper outlines the proposed QnAs generation\npipeline, starting from a selected seed of problems crafted by Subject Matter\nExperts. The evaluation of a wide range of open-source LLMs reveals that best\nperformance on TeleMath is achieved by recent models explicitly designed for\nmathematical or logical reasoning. In contrast, general-purpose models, even\nthose with a large number of parameters, often struggle with these challenges.\nWe have released the dataset and the evaluation code to ease result\nreproducibility and support future research.", "comment": "6 pages", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL", "links": "http://arxiv.org/abs/2506.10674v1;http://arxiv.org/pdf/2506.10674v1", "pdf_url": "http://arxiv.org/pdf/2506.10674v1"}, {"title": "G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration", "link": "https://arxiv.org/pdf/2506.09272", "details": "S Holt, MR Luyten, A Berthon, M van der Schaar - arXiv preprint arXiv:2506.09272, 2025", "abstract": "\u2026 We adopt the Wasserstein distance as our primary **evaluation** metric. From each initial state in the held-out test set, we simulate N trajectories under both the ground-truth and comparison simulators, then compute the Wasserstein distance between these \u2026", "entry_id": "http://arxiv.org/abs/2506.09272v1", "updated": "2025-06-10 22:14:34", "published": "2025-06-10 22:14:34", "authors": "Samuel Holt;Max Ruiz Luyten;Antonin Berthon;Mihaela van der Schaar", "summary": "Constructing robust simulators is essential for asking \"what if?\" questions\nand guiding policy in critical domains like healthcare and logistics. However,\nexisting methods often struggle, either failing to generalize beyond historical\ndata or, when using Large Language Models (LLMs), suffering from inaccuracies\nand poor empirical alignment. We introduce G-Sim, a hybrid framework that\nautomates simulator construction by synergizing LLM-driven structural design\nwith rigorous empirical calibration. G-Sim employs an LLM in an iterative loop\nto propose and refine a simulator's core components and causal relationships,\nguided by domain knowledge. This structure is then grounded in reality by\nestimating its parameters using flexible calibration techniques. Specifically,\nG-Sim can leverage methods that are both likelihood-free and gradient-free with\nrespect to the simulator, such as gradient-free optimization for direct\nparameter estimation or simulation-based inference for obtaining a posterior\ndistribution over parameters. This allows it to handle non-differentiable and\nstochastic simulators. By integrating domain priors with empirical evidence,\nG-Sim produces reliable, causally-informed simulators, mitigating\ndata-inefficiency and enabling robust system-level interventions for complex\ndecision-making.", "comment": "Accepted at the 42nd International Conference on Machine Learning\n  (ICML 2025). 9 pages, 3 figures", "journal_ref": "Proceedings of the 42nd International Conference on Machine\n  Learning, Vancouver, Canada. PMLR 267, 2025", "primary_category": "cs.LG", "categories": "cs.LG;stat.ML;68T05, 68U20, 62F15;I.2.6; I.6.5; G.3", "links": "http://arxiv.org/abs/2506.09272v1;http://arxiv.org/pdf/2506.09272v1", "pdf_url": "http://arxiv.org/pdf/2506.09272v1"}, {"title": "AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine", "link": "https://arxiv.org/pdf/2506.10365", "details": "S Hou, Z Shen, H Wu, H Jiao, Z Liu, L Xie, C Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 However, standardised automated **evaluation** tools for this task remain lacking. This study introduces AutoGEEval++, an enhanced **evaluation** framework built upon AutoGEEval. It is the first automated assessment framework tailored for **large** \u2026", "entry_id": "http://arxiv.org/abs/2506.10365v1", "updated": "2025-06-12 05:42:37", "published": "2025-06-12 05:42:37", "authors": "Shuyang Hou;Zhangxiao Shen;Huayi Wu;Haoyue Jiao;Ziqi Liu;Lutong Xie;Chang Liu;Jianyuan Liang;Yaxian Qing;Xiaopu Zhang;Dehua Peng;Zhipeng Gui;Xuefeng Guan", "summary": "Geospatial code generation is becoming a key frontier in integrating\nartificial intelligence with geo-scientific analysis, yet standardised\nautomated evaluation tools for this task remain absent. This study presents\nAutoGEEval++, an enhanced framework building on AutoGEEval, and the first\nautomated assessment system for large language models (LLMs) generating\ngeospatial code on Google Earth Engine (GEE). It supports diverse data\nmodalities and varying task complexities. Built on the GEE Python API,\nAutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test\ncases across 26 data types and three task categories: unit, combo, and theme\ntests. It includes a submission programme and a judge module to realise an\nend-to-end automated evaluation pipeline from code generation to\nexecution-based validation. The framework adopts multi-dimensional\nmetrics-accuracy, resource usage, run-time efficiency, and error\ntypes-balancing hallucination control and efficiency, and enabling boundary\ntesting and error pattern analysis. Using AutoGEEval++, we evaluate 24\nstate-of-the-art LLMs (as of June 2025), including general-purpose,\nreasoning-enhanced, code-centric, and geoscience-specific models. Results\nreveal clear performance, stability, and error differences across task types,\nmodel designs, and deployment settings, confirming AutoGEEval++'s practical\nvalue and scalability in vertical-domain code generation. This work establishes\nthe first standardised evaluation protocol and foundational benchmark for\nGEE-based LLM code generation, providing a unified basis for performance\ncomparison and a methodological framework for systematic, domain-specific code\nevaluation.", "comment": null, "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE", "links": "http://arxiv.org/abs/2506.10365v1;http://arxiv.org/pdf/2506.10365v1", "pdf_url": "http://arxiv.org/pdf/2506.10365v1"}]
