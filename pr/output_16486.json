[{"title": "Leveraging long context in retrieval augmented language models for medical question answering", "link": "https://www.nature.com/articles/s41746-025-01651-w", "details": "G Zhang, Z Xu, Q Jin, F Chen, Y Fang, Y Liu\u2026 - npj Digital Medicine, 2025", "abstract": "While holding great promise for improving and facilitating healthcare through applications of medical literature summarization, large language models (LLMs) struggle to produce up-to-date responses on evolving topics due to outdated \u2026"}, {"title": "MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks", "link": "https://arxiv.org/pdf/2505.06152", "details": "W Zeng, Y Sun, C Ma, W Tan, B Yan - arXiv preprint arXiv:2505.06152, 2025", "abstract": "Medical vision-language models (VLMs) have shown promise as clinical assistants across various medical fields. However, specialized dermatology VLM capable of delivering professional and detailed diagnostic analysis remains underdeveloped \u2026"}, {"title": "ALGOPUZZLEVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Algorithmic Multimodal Puzzles", "link": "https://aclanthology.org/2025.naacl-long.486.pdf", "details": "D Ghosal, V Toh, YK Chia, S Poria - Proceedings of the 2025 Conference of the \u2026, 2025", "abstract": "This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models \u2026"}, {"title": "Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning", "link": "https://arxiv.org/pdf/2505.09118", "details": "D Liang, C Zheng, Z Wen, Y Cai, XY Wei, Q Li - arXiv preprint arXiv:2505.09118, 2025", "abstract": "Traditional scene graphs primarily focus on spatial relationships, limiting vision- language models'(VLMs) ability to reason about complex interactions in visual scenes. This paper addresses two key challenges:(1) conventional detection-to \u2026"}, {"title": "A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2505.01958%3F", "details": "L Jing, GH Chen, E Aghazadeh, XE Wang, X Du - arXiv preprint arXiv:2505.01958, 2025", "abstract": "Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. It refers to scenarios where models generate inaccurate visual object-related information \u2026"}, {"title": "Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction", "link": "https://arxiv.org/pdf/2505.00814", "details": "M S\u00e4nger, U Leser - arXiv preprint arXiv:2505.00814, 2025", "abstract": "Automatic relationship extraction (RE) from biomedical literature is critical for managing the vast amount of scientific knowledge produced each year. In recent years, utilizing pre-trained language models (PLMs) has become the prevalent \u2026"}, {"title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning", "link": "https://arxiv.org/pdf/2504.19627%3F", "details": "R Luo, R Shan, L Chen, Z Liu, L Wang, M Yang, X Xia - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like embodied intelligence due to their strong vision-language reasoning abilities. However, current LVLMs process entire images at the token level, which is inefficient \u2026"}, {"title": "FedHERO: A Federated Learning Approach for Node Classification Task on Heterophilic Graphs", "link": "https://arxiv.org/pdf/2504.21206%3F", "details": "Z Chen, X Fu, Y Dong, J Li, C Shen - arXiv preprint arXiv:2504.21206, 2025", "abstract": "Federated Graph Learning (FGL) empowers clients to collaboratively train Graph neural networks (GNNs) in a distributed manner while preserving data privacy. However, FGL methods usually require that the graph data owned by all clients is \u2026"}, {"title": "Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput", "link": "https://arxiv.org/pdf/2505.09498", "details": "B Zhang, S Li, R Tian, Y Yang, J Tang, J Zhou, L Ma - arXiv preprint arXiv:2505.09498, 2025", "abstract": "In this paper, we introduce Flash-VL 2B, a novel approach to optimizing Vision- Language Models (VLMs) for real-time applications, targeting ultra-low latency and high throughput without sacrificing accuracy. Leveraging advanced architectural \u2026"}]
