[{"title": "Benchmarking Vision Language Models for Cultural Understanding", "link": "https://arxiv.org/pdf/2407.10920", "details": "S Nayak, K Jain, R Awal, S Reddy, S van Steenkiste\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene \u2026"}, {"title": "Optimizing EEG-based Sleep Staging: Adversarial Deep Learning Joint Domain Adaptation", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10597569.pdf", "details": "R GhasemiGarjan, M Mikaeili, SK Setarehdan - IEEE Access, 2024", "abstract": "Sleep-stage classification is a critical aspect of understanding sleep patterns in sleep research and healthcare. However, challenges arise when dealing with a limited number of labeled samples in the target domain. Traditional methods in Deep \u2026"}, {"title": "Quantized Prompt for Efficient Generalization of Vision-Language Models", "link": "https://arxiv.org/pdf/2407.10704", "details": "T Hao, X Ding, J Feng, Y Yang, H Chen, G Ding - arXiv preprint arXiv:2407.10704, 2024", "abstract": "In the past few years, large-scale pre-trained vision-language models like CLIP have achieved tremendous success in various fields. Naturally, how to transfer the rich knowledge in such huge pre-trained models to downstream tasks and datasets \u2026"}]
