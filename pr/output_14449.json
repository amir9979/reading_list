[{"title": "Applications of Generative AI Models", "link": "https://link.springer.com/chapter/10.1007/978-3-031-82062-5_8", "details": "R Gupta, S Tiwari, P Chaudhary - Generative AI: Techniques, Models and \u2026, 2025", "abstract": "Artificial Intelligence is a transformative technology that seeks to emulate human intelligence in machines. It is pervasive in the modern era, with applications ranging from virtual assistants to autonomous vehicles, showcasing the vast potential and \u2026"}, {"title": "Probench: Benchmarking large language models in competitive programming", "link": "https://arxiv.org/pdf/2502.20868", "details": "L Yang, R Jin, L Shi, J Peng, Y Chen, D Xiong - arXiv preprint arXiv:2502.20868, 2025", "abstract": "With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging, large language models (LLMs) have entered a new phase of development. However, existing benchmarks for coding evaluation are gradually inadequate to assess the \u2026"}, {"title": "DAST: Difficulty-Aware Self-Training on Large Language Models", "link": "https://arxiv.org/pdf/2503.09029", "details": "B Xue, Q Zhu, H Wang, R Wang, S Wang, H Xu, F Mi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Present Large Language Models (LLM) self-training methods always under-sample on challenging queries, leading to inadequate learning on difficult problems which limits LLMs' ability. Therefore, this work proposes a difficulty-aware self-training \u2026"}, {"title": "Assessing Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks", "link": "https://openreview.net/pdf%3Fid%3D3YyyiyV4B6", "details": "F Lin, S Mao, E La Malfa, V Hofmann, A de Wynter\u2026 - Workshop on Reasoning and \u2026", "abstract": "Language is not monolithic. While benchmarks, including those designed for multiple languages, are often used as proxies to evaluate the performance of Large Language Models (LLMs), they tend to overlook the nuances of within-language \u2026"}, {"title": "Towards Improving Translation Ability of Large Language Models on Low Resource Languages", "link": "https://www.scitepress.org/Papers/2025/133190/133190.pdf", "details": "AR Dash, Y Sharma", "abstract": "With advancements in Natural Language Processing (NLP) and Large Language Models (LLMs), there is a growing need to understand their capabilities with low resource languages. This study focuses on benchmarking and improving the \u2026"}, {"title": "Language Model Personalization via Reward Factorization", "link": "https://arxiv.org/pdf/2503.06358", "details": "I Shenfeld, F Faltings, P Agrawal, A Pacchiano - arXiv preprint arXiv:2503.06358, 2025", "abstract": "Modern large language models (LLMs) are optimized for human-aligned responses using Reinforcement Learning from Human Feedback (RLHF). However, existing RLHF approaches assume a universal preference model and fail to account for \u2026"}, {"title": "Zero-Shot Prediction of Conversational Derailment with Large Language Models", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10938545.pdf", "details": "K Nonaka, M Yoshida - IEEE Access, 2025", "abstract": "Online discussion platforms often show a tendency for conversations to stray from the topic and devolve into personal attacks. Previous studies have trained machine learning algorithms to detect conversational derailment using supervised methods \u2026"}]
