[{"title": "Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models", "link": "https://arxiv.org/pdf/2503.15888", "details": "B Bi, S Liu, Y Wang, Y Xu, J Fang, L Mei, X Cheng - arXiv preprint arXiv:2503.15888, 2025", "abstract": "Retrieval-Augmented Generation (RAG) mitigates hallucinations in Large Language Models (LLMs) by integrating external knowledge. However, conflicts between parametric knowledge and retrieved context pose challenges, particularly when \u2026"}, {"title": "Corrective In-Context Learning: Evaluating Self-Correction in Large Language Models", "link": "https://arxiv.org/pdf/2503.16022", "details": "M Sanz-Guerrero, K von der Wense - arXiv preprint arXiv:2503.16022, 2025", "abstract": "In-context learning (ICL) has transformed the use of large language models (LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled examples without finetuning. Despite its effectiveness, ICL is prone to errors, especially for \u2026"}, {"title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling", "link": "https://arxiv.org/pdf/2503.15450", "details": "T Zhu, Q Liu, H Wang, S Chen, X Gu, T Pang, MY Kan - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in LLM pretraining have featured ever-expanding context windows to process longer sequences. However, our pilot study reveals that models pretrained with shorter context windows consistently outperform their long-context \u2026"}, {"title": "Promote, Suppress, Iterate: How Language Models Answer One-to-Many Factual Queries", "link": "https://arxiv.org/pdf/2502.20475", "details": "TL Yan, R Jia - arXiv preprint arXiv:2502.20475, 2025", "abstract": "To answer one-to-many factual queries (eg, listing cities of a country), a language model (LM) must simultaneously recall knowledge and avoid repeating previous answers. How are these two subtasks implemented and integrated internally? Across \u2026"}, {"title": "MMSciBench: Benchmarking Language Models on Multimodal Scientific Problems", "link": "https://arxiv.org/pdf/2503.01891", "details": "X Ye, C Li, S Chen, X Tang, W Wei - arXiv preprint arXiv:2503.01891, 2025", "abstract": "Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present \u2026"}, {"title": "Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence", "link": "https://arxiv.org/pdf/2503.14749", "details": "S Hager, D Mueller, K Duh, N Andrews - arXiv preprint arXiv:2503.14749, 2025", "abstract": "As large language models (LLMs) are increasingly used for factual question- answering, it becomes more important for LLMs to have the capability to communicate the likelihood that their answer is correct. For these verbalized \u2026"}, {"title": "FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models", "link": "https://arxiv.org/pdf/2502.17924", "details": "H Lin, Y Deng, Y Gu, W Zhang, J Ma, SK Ng, TS Chua - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have significantly advanced the fact-checking studies. However, existing automated fact-checking evaluation methods rely on static datasets and classification metrics, which fail to automatically evaluate the \u2026"}, {"title": "ProBench: Benchmarking Large Language Models in Competitive Programming", "link": "https://arxiv.org/pdf/2502.20868", "details": "L Yang, R Jin, L Shi, J Peng, Y Chen, D Xiong - arXiv preprint arXiv:2502.20868, 2025", "abstract": "With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging, large language models (LLMs) have entered a new phase of development. However, existing benchmarks for coding evaluation are gradually inadequate to assess the \u2026"}, {"title": "Scalable Best-of-N Selection for Large Language Models via Self-Certainty", "link": "https://arxiv.org/pdf/2502.18581", "details": "Z Kang, X Zhao, D Song - arXiv preprint arXiv:2502.18581, 2025", "abstract": "Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for \u2026"}]
