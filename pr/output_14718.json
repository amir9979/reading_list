[{"title": "Skip-Vision: A Comprehensive Framework for Accelerating Vision-Language Models", "link": "https://arxiv.org/pdf/2503.21817", "details": "W Zeng, Z Huang, K Ji, Y Yan - arXiv preprint arXiv:2503.21817, 2025", "abstract": "Transformer-based models have driven significant advancements in Multimodal Large Language Models (MLLMs), yet their computational costs surge drastically when scaling resolution, training data, and model parameters. A key bottleneck \u2026"}, {"title": "From head to tail: Towards balanced representation in large vision-language models through adaptive data calibration", "link": "https://arxiv.org/pdf/2503.12821", "details": "M Song, X Qu, J Zhou, Y Cheng - arXiv preprint arXiv:2503.12821, 2025", "abstract": "Large Vision-Language Models (LVLMs) have achieved significant progress in combining visual comprehension with language generation. Despite this success, the training data of LVLMs still suffers from Long-Tail (LT) problems, where the data \u2026"}, {"title": "REVAL: A Comprehension Evaluation on Reliability and Values of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.16566", "details": "J Zhang, Z Yuan, Z Wang, B Yan, S Wang, X Cao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The rapid evolution of Large Vision-Language Models (LVLMs) has highlighted the necessity for comprehensive evaluation frameworks that assess these models across diverse dimensions. While existing benchmarks focus on specific aspects such as \u2026"}, {"title": "Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization", "link": "https://arxiv.org/pdf/2503.22577%3F", "details": "I Pikabea, I Lacunza, O Pareras, C Escolano\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Rapid advancements in Visual Language Models (VLMs) have transformed multimodal understanding but are often constrained by generating English responses regardless of the input language. This phenomenon has been termed as \u2026"}, {"title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks", "link": "https://arxiv.org/pdf/2504.01308", "details": "J Wang, Y Zuo, Y Chai, Z Liu, Y Fu, Y Feng, K Lam - arXiv preprint arXiv:2504.01308, 2025", "abstract": "Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing \u2026"}, {"title": "Unveiling the mist over 3d vision-language understanding: Object-centric evaluation with chain-of-analysis", "link": "https://arxiv.org/pdf/2503.22420", "details": "J Huang, B Jia, Y Wang, Z Zhu, X Linghu, Q Li, SC Zhu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VL models, creating a\" mist\" that obscures rigorous insights into model capabilities and 3D-VL tasks. This mist persists due to three key limitations. First, flawed test data, like \u2026"}, {"title": "2-D Transformer: Extending Large Language Models to Long-Context With Few Memory", "link": "https://ieeexplore.ieee.org/abstract/document/10937248/", "details": "X He, J Liu, Y Duan - IEEE Transactions on Neural Networks and Learning \u2026, 2025", "abstract": "The ability of processing long contexts is crucial for large language models (LLMs), but training LLMs with a long-context window requires substantial computational resources. Many sought to mitigate this through the sparse attention mechanism \u2026"}, {"title": "Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization", "link": "https://arxiv.org/pdf/2503.17928", "details": "Z Zhang, H Tang, J Sheng, Z Zhang, Y Ren, Z Li, D Yin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal Large Language Models excel in various tasks, yet often struggle with modality bias, where the model tends to rely heavily on a single modality and overlook critical information in other modalities, which leads to incorrect focus and \u2026"}, {"title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language Models", "link": "https://arxiv.org/pdf/2503.18923", "details": "M Cao, P Hu, Y Wang, J Gu, H Tang, H Zhao, J Dong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in Large Video Language Models (LVLMs) have highlighted their potential for multi-modal understanding, yet evaluating their factual grounding in video contexts remains a critical unsolved challenge. To address this gap, we \u2026"}]
