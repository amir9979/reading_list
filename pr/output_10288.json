[{"title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2411.14432", "details": "Y Dong, Z Liu, HL Sun, J Yang, W Hu, Y Rao, Z Liu - arXiv preprint arXiv:2411.14432, 2024", "abstract": "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high \u2026"}, {"title": "Empowering LLM Agents with Zero-Shot Optimal Decision-Making through Q-learning", "link": "https://openreview.net/pdf%3Fid%3DktBgkw2Ggk", "details": "J Chai, S Li, Y Fu, D Zhao, Y Zhu - Adaptive Foundation Models: Evolving AI for \u2026", "abstract": "Current Large language model (LLM) agents succeed in making zero-shot decisions but struggle to make optimal decisions, as they rely on pre-trained probabilities rather than maximizing expected future rewards. In contrast, agents trained via \u2026"}]
