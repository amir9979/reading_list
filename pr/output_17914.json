[{"title": "MedErr-CT: A Visual Question Answering Benchmark for Identifying and Correcting Errors in CT Reports", "link": "https://arxiv.org/pdf/2506.19217", "details": "S Kyung, H Park, J Seo, J Sung, J Kim, D Kim, W Jo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **Language** **Models** (MLLMs) demonstrate promising comprehension of **medical** knowledge, their tendency to produce inaccurate information highlights the need for rigorous validation. However, existing **medical** visual **question** **answering** \u2026", "entry_id": "http://arxiv.org/abs/2506.19217v1", "updated": "2025-06-24 00:51:03", "published": "2025-06-24 00:51:03", "authors": "Sunggu Kyung;Hyungbin Park;Jinyoung Seo;Jimin Sung;Jihyun Kim;Dongyeong Kim;Wooyoung Jo;Yoojin Nam;Sangah Park;Taehee Kwon;Sang Min Lee;Namkug Kim", "summary": "Computed Tomography (CT) plays a crucial role in clinical diagnosis, but the\ngrowing demand for CT examinations has raised concerns about diagnostic errors.\nWhile Multimodal Large Language Models (MLLMs) demonstrate promising\ncomprehension of medical knowledge, their tendency to produce inaccurate\ninformation highlights the need for rigorous validation. However, existing\nmedical visual question answering (VQA) benchmarks primarily focus on simple\nvisual recognition tasks, lacking clinical relevance and failing to assess\nexpert-level knowledge. We introduce MedErr-CT, a novel benchmark for\nevaluating medical MLLMs' ability to identify and correct errors in CT reports\nthrough a VQA framework. The benchmark includes six error categories - four\nvision-centric errors (Omission, Insertion, Direction, Size) and two lexical\nerror types (Unit, Typo) - and is organized into three task levels:\nclassification, detection, and correction. Using this benchmark, we\nquantitatively assess the performance of state-of-the-art 3D medical MLLMs,\nrevealing substantial variation in their capabilities across different error\ntypes. Our benchmark contributes to the development of more reliable and\nclinically applicable MLLMs, ultimately helping reduce diagnostic errors and\nimprove accuracy in clinical practice. The code and datasets are available at\nhttps://github.com/babbu3682/MedErr-CT.", "comment": "14 pages, 5 figures, submitted to CVPR 2025", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2506.19217v1;http://arxiv.org/pdf/2506.19217v1", "pdf_url": "http://arxiv.org/pdf/2506.19217v1"}, {"title": "Evaluating the Performance of **Large Language Models** in Oral and Maxillofacial Surgery Board Examinations", "link": "https://www.ijoms.com/article/S0901-5027\\(25\\)01190-7/abstract", "details": "R Mahmoud - International Journal of Oral and Maxillofacial Surgery, 2025", "abstract": "Background: Artificial intelligence (AI) has significantly impacted the **medical** field, yet its application in oral and maxillofacial surgery (OMS) remains largely underexplored. This study aims to evaluate the performance of four leading **large** \u2026"}, {"title": "From Documents to Disclosures: Automating Sustainability Reporting with **Large Language Models**", "link": "https://lup.lub.lu.se/luur/download%3Ffunc%3DdownloadFile%26recordOId%3D9204394%26fileOId%3D9204401", "details": "A Nystedt, O Wiksten - 2025", "abstract": "\u2026 This study explores the use of **Large** **Language** **Models** (LLMs) to automate sustainability reporting in accordance with the European \u2026 F\u00f6r att unders\u00f6ka potentialen hos LLM:er att hantera dessa utmaningar utvecklades en prototyp i samarbete **med** \u2026"}, {"title": "MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration", "link": "https://arxiv.org/pdf/2506.19835", "details": "Y Zhou, L Song, J Shen - arXiv preprint arXiv:2506.19835, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) in multimodal medical diagnosis. Our experiments leverage a diverse collection of publicly available **medical** \u2026 For video-based **medical** **question** **answering** on **Med** VidQA (Table 6), MAM achieves leading \u2026", "entry_id": "http://arxiv.org/abs/2506.19835v1", "updated": "2025-06-24 17:52:43", "published": "2025-06-24 17:52:43", "authors": "Yucheng Zhou;Lingran Song;Jianbing Shen", "summary": "Recent advancements in medical Large Language Models (LLMs) have showcased\ntheir powerful reasoning and diagnostic capabilities. Despite their success,\ncurrent unified multimodal medical LLMs face limitations in knowledge update\ncosts, comprehensiveness, and flexibility. To address these challenges, we\nintroduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis\n(MAM). Inspired by our empirical findings highlighting the benefits of role\nassignment and diagnostic discernment in LLMs, MAM decomposes the medical\ndiagnostic process into specialized roles: a General Practitioner, Specialist\nTeam, Radiologist, Medical Assistant, and Director, each embodied by an\nLLM-based agent. This modular and collaborative framework enables efficient\nknowledge updates and leverages existing medical LLMs and knowledge bases.\nExtensive experimental evaluations conducted on a wide range of publicly\naccessible multimodal medical datasets, incorporating text, image, audio, and\nvideo modalities, demonstrate that MAM consistently surpasses the performance\nof modality-specific LLMs. Notably, MAM achieves significant performance\nimprovements ranging from 18% to 365% compared to baseline models. Our code is\nreleased at https://github.com/yczhou001/MAM.", "comment": "ACL 2025 Findings", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.19835v1;http://arxiv.org/pdf/2506.19835v1", "pdf_url": "http://arxiv.org/pdf/2506.19835v1"}, {"title": "IMPROVING INFORMATION ACCESS IN THE **HEALTHCARE** SECTOR WITH RAG: ENHANCING THE EFFECTIVENESS OF PATIENT INFORMATION FORMS", "link": "https://www.researchgate.net/profile/Hueseyin-Parmaksiz/publication/392967571_Intrusion_Detection_in_5G_Core_Networks_Using_Artificial_Neural_Networks_5G_Cekirdek_Aginda_Yapay_Sinir_Agi_Modeli_ile_Saldiri_Tespiti/links/685ad148e8fa0f5c28262b3a/Intrusion-Detection-in-5G-Core-Networks-Using-Artificial-Neural-Networks-5G-Cekirdek-Aginda-Yapay-Sinir-Agi-Modeli-ile-Saldiri-Tespiti.pdf%23page%3D433", "details": "M Gen\u00e7 - As \u0130zmir K\u00e2tip \u00c7elebi University Artificial In-telligence \u2026", "abstract": "\u2026 RAG technology shows promise in improving the accuracy and reliability of **large** **language** **models** (LLMs) through external sources of information [4]. RAG is a combination of retrieval with text generation models that allows the model to access \u2026"}, {"title": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty", "link": "https://arxiv.org/pdf/2506.19563", "details": "J He, Y Lu, Z Lin, K Chen, Y Zhao - arXiv preprint arXiv:2506.19563, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) are widely used in sensitive domains, including **healthcare** , finance, and legal services, raising concerns about potential private information leaks dur\u2026 **Large** **Language** **Models** (LLMs) have become integral to \u2026", "entry_id": "http://arxiv.org/abs/2506.19563v1", "updated": "2025-06-24 12:22:59", "published": "2025-06-24 12:22:59", "authors": "Jinwen He;Yiyang Lu;Zijin Lin;Kai Chen;Yue Zhao", "summary": "Large Language Models (LLMs) are widely used in sensitive domains, including\nhealthcare, finance, and legal services, raising concerns about potential\nprivate information leaks during inference. Privacy extraction attacks, such as\njailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the\nmodels to output sensitive information. However, these attacks cannot verify\nwhether the extracted private information is accurate, as no public datasets\nexist for cross-validation, leaving a critical gap in private information\ndetection during inference. To address this, we propose PrivacyXray, a novel\nframework detecting privacy breaches by analyzing LLM inner states. Our\nanalysis reveals that LLMs exhibit higher semantic coherence and probabilistic\ncertainty when generating correct private outputs. Based on this, PrivacyXray\ndetects privacy breaches using four metrics: intra-layer and inter-layer\nsemantic similarity, token-level and sentence-level probability distributions.\nPrivacyXray addresses critical challenges in private information detection by\novercoming the lack of open-source private datasets and eliminating reliance on\nexternal data for validation. It achieves this through the synthesis of\nrealistic private data and a detection mechanism based on the inner states of\nLLMs. Experiments show that PrivacyXray achieves consistent performance, with\nan average accuracy of 92.69% across five LLMs. Compared to state-of-the-art\nmethods, PrivacyXray achieves significant improvements, with an average\naccuracy increase of 20.06%, highlighting its stability and practical utility\nin real-world applications.", "comment": null, "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR;cs.AI", "links": "http://arxiv.org/abs/2506.19563v1;http://arxiv.org/pdf/2506.19563v1", "pdf_url": "http://arxiv.org/pdf/2506.19563v1"}, {"title": "Deontology in AI", "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/9781394238651.ch11", "details": "TM Powers - A Companion to Applied Philosophy of AI, 2025", "abstract": "\u2026 **large** **language** **models** in general have created much excitement and hype, and indeed one can even prompt ChatgPT with **questions** about ethics and get quite impressive **answers** \u2026 which are generically useful but can give unsafe results when \u2026"}, {"title": "Generative artificial intelligence applications in different industries", "link": "https://lutpub.lut.fi/bitstream/handle/10024/170107/Mastersthesis_Kunnap_Vivian.pdf%3Fsequence%3D1", "details": "V K\u00fcnnap - 2025", "abstract": "\u2026 They briefly touch on each subfield of **healthcare** by providing a few example cases, introducing **healthcare** -specific GenAI models, and \u2026 2.5 **Large** **language** **models** **Large** **language** **models** (LLMs) are machine learning models specialised in \u2026"}]
