[{"title": "ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists", "link": "https://arxiv.org/pdf/2506.01241", "details": "J Ruan, I Nair, S Cao, A Liu, S Munir\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper introduces ExpertLongBench, an expert-level benchmark containing 11 tasks from 9 domains that reflect realistic expert workflows and applications. Beyond question answering, the application-driven tasks in ExpertLongBench demand long \u2026", "entry_id": "http://arxiv.org/abs/2506.01241v1", "updated": "2025-06-02 01:39:02", "published": "2025-06-02 01:39:02", "authors": "Jie Ruan;Inderjeet Nair;Shuyang Cao;Amy Liu;Sheza Munir;Micah Pollens-Dempsey;Tiffany Chiang;Lucy Kates;Nicholas David;Sihan Chen;Ruxin Yang;Yuqian Yang;Jasmine Gump;Tessa Bialek;Vivek Sankaran;Margo Schlanger;Lu Wang", "summary": "This paper introduces ExpertLongBench, an expert-level benchmark containing\n11 tasks from 9 domains that reflect realistic expert workflows and\napplications. Beyond question answering, the application-driven tasks in\nExpertLongBench demand long-form outputs that can exceed 5,000 tokens and\nstrict adherence to domain-specific requirements. Notably, each task in\nExpertLongBench includes a rubric, designed or validated by domain experts, to\nspecify task requirements and guide output evaluation. Furthermore, we propose\nCLEAR, an evaluation framework that supports accurate evaluation of long-form\nmodel outputs in our benchmark. To achieve fine-grained, expert-aligned\nevaluation, CLEAR derives checklists from both model outputs and references by\nextracting information corresponding to items in the task-specific rubric.\nChecklist items for model outputs are then compared with corresponding items\nfor reference outputs to assess their correctness, enabling grounded\nevaluation. We benchmark 11 large language models (LLMs) and analyze components\nin CLEAR, showing that (1) existing LLMs, with the top performer achieving only\na 26.8% F1 score, require significant improvement for expert-level tasks; (2)\nmodels can generate content corresponding to the required aspects, though often\nnot accurately; and (3) accurate checklist extraction and comparison in CLEAR\ncan be achieved by open-weight models for more scalable and low-cost usage.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.01241v1;http://arxiv.org/pdf/2506.01241v1", "pdf_url": "http://arxiv.org/pdf/2506.01241v1"}, {"title": "SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models", "link": "https://arxiv.org/pdf/2506.01062", "details": "T Pham, N Nguyen, P Zunjare, W Chen, YM Tseng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce SealQA, a new challenge benchmark for evaluating SEarch- Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:(1) Seal-0 \u2026", "entry_id": "http://arxiv.org/abs/2506.01062v1", "updated": "2025-06-01 16:04:34", "published": "2025-06-01 16:04:34", "authors": "Thinh Pham;Nguyen Nguyen;Pratibha Zunjare;Weiyuan Chen;Yu-Min Tseng;Tu Vu", "summary": "We introduce SealQA, a new challenge benchmark for evaluating\nSEarch-Augmented Language models on fact-seeking questions where web search\nyields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:\n(1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and\nreasoning capabilities, with Seal-0 focusing on the most challenging questions\nwhere chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3)\nLongSeal, which extends SealQA to test long-context, multi-document reasoning\nin \"needle-in-a-haystack\" settings. Our evaluation reveals critical limitations\nin current models: Even frontier LLMs perform poorly across all SealQA flavors.\nOn Seal-0, frontier agentic models equipped with tools like o3 and o4-mini\nachieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning\nefforts. We find that advanced reasoning models such as DeepSeek-R1-671B and\no3-mini are highly vulnerable to noisy search results. Notably, increasing\ntest-time compute does not yield reliable gains across o3-mini, o4-mini, and\no3, with performance often plateauing or even declining early. Additionally,\nwhile recent models are less affected by the \"lost-in-the-middle\" issue, they\nstill fail to reliably identify relevant documents in LongSeal when faced with\nnumerous distractors. To facilitate future work, we release SealQA at\nhuggingface.co/datasets/vtllms/sealqa.", "comment": "Preprint. 22 pages, 7 figures, 11 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2506.01062v1;http://arxiv.org/pdf/2506.01062v1", "pdf_url": "http://arxiv.org/pdf/2506.01062v1"}, {"title": "The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning", "link": "https://arxiv.org/pdf/2506.02139", "details": "EY Chang - arXiv preprint arXiv:2506.02139, 2025", "abstract": "Few-shot learning in large language models (LLMs) reveals a deep paradox: Some tasks generalize from minimal examples, while others require extensive supervision. We address this through the Unified Cognitive Consciousness Theory (UCCT), which \u2026", "entry_id": "http://arxiv.org/abs/2506.02139v2", "updated": "2025-06-04 02:44:46", "published": "2025-06-02 18:12:43", "authors": "Edward Y. Chang", "summary": "Few-shot learning in large language models (LLMs) reveals a core paradox:\ncertain tasks generalize from just a few examples, while others demand\nextensive supervision. To explain this, we introduce the Unified Cognitive\nConsciousness Theory (UCCT), which reconceptualizes LLMs not as deficient\nagents, but as unconscious substrates: dense, distributed repositories of\nlinguistic and conceptual patterns that operate without explicit semantics,\nintention, or goal-directed reasoning. Under this view, LLMs are not flawed\nsimulations of cognition but foundational substrates for general intelligence.\nUCCT posits that semantic anchoring, via prompts, role assignments, and\nstructured interaction, functions as a conscious control layer that modulates\nlatent representations toward task-relevant semantics and enables coherent,\nstructured reasoning. It unifies prompting, fine-tuning, retrieval-augmented\ngeneralization, and multi-agent collaboration within a single framework,\ngrounded in the probabilistic alignment between unconscious pattern space and\nexternally imposed semantic constraints (e.g., prompts, supervision, task\nobjectives). The core implication is not to replace LLMs, but to integrate and\nunify them through a structured cognitive layer that supports intentional\nreasoning. This enables collections of LLMs to operate within\ndomain-specialized verticals (e.g., legal reasoning, medical diagnosis) that\nreason, regulate, and adapt together. Such integration is characterized by\nphase-transition behavior, wherein anchored representations cross coherence\nthresholds as a function of semantic constraint strength and interaction\ncontext.", "comment": "12 pages, 2 figure, 1 table", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;I.2.7", "links": "http://arxiv.org/abs/2506.02139v2;http://arxiv.org/pdf/2506.02139v2", "pdf_url": "http://arxiv.org/pdf/2506.02139v2"}, {"title": "RAP: Retrieval-Augmented Personalization for Multimodal Large Language Models", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Hao_RAP_Retrieval-Augmented_Personalization_for_Multimodal_Large_Language_Models_CVPR_2025_paper.pdf", "details": "H Hao, J Han, C Li, YF Li, X Yue - Proceedings of the Computer Vision and Pattern \u2026, 2025", "abstract": "The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in human's daily life. In this \u2026"}, {"title": "NegVQA: Can Vision Language Models Understand Negation?", "link": "https://arxiv.org/pdf/2505.22946", "details": "Y Zhang, Y Su, Y Liu, S Yeung-Levy - arXiv preprint arXiv:2505.22946, 2025", "abstract": "Negation is a fundamental linguistic phenomenon that can entirely reverse the meaning of a sentence. As vision language models (VLMs) continue to advance and are deployed in high-stakes applications, assessing their ability to comprehend \u2026", "entry_id": "http://arxiv.org/abs/2505.22946v1", "updated": "2025-05-28 23:58:37", "published": "2025-05-28 23:58:37", "authors": "Yuhui Zhang;Yuchang Su;Yiming Liu;Serena Yeung-Levy", "summary": "Negation is a fundamental linguistic phenomenon that can entirely reverse the\nmeaning of a sentence. As vision language models (VLMs) continue to advance and\nare deployed in high-stakes applications, assessing their ability to comprehend\nnegation becomes essential. To address this, we introduce NegVQA, a visual\nquestion answering (VQA) benchmark consisting of 7,379 two-choice questions\ncovering diverse negation scenarios and image-question distributions. We\nconstruct NegVQA by leveraging large language models to generate negated\nversions of questions from existing VQA datasets. Evaluating 20\nstate-of-the-art VLMs across seven model families, we find that these models\nstruggle significantly with negation, exhibiting a substantial performance drop\ncompared to their responses to the original questions. Furthermore, we uncover\na U-shaped scaling trend, where increasing model size initially degrades\nperformance on NegVQA before leading to improvements. Our benchmark reveals\ncritical gaps in VLMs' negation understanding and offers insights into future\nVLM development. Project page available at\nhttps://yuhui-zh15.github.io/NegVQA/.", "comment": "Published at ACL 2025 Findings", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.CV;cs.CY;cs.LG", "links": "http://arxiv.org/abs/2505.22946v1;http://arxiv.org/pdf/2505.22946v1", "pdf_url": "http://arxiv.org/pdf/2505.22946v1"}, {"title": "Dynamic few-shot prompting for clinical note section classification using lightweight, open-source large language models", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocaf084/8155977", "details": "K Miller, S Bedrick, Q Lu, A Wen, W Hersh, K Roberts\u2026 - Journal of the American \u2026, 2025", "abstract": "Objective Unlocking clinical information embedded in clinical notes has been hindered to a significant degree by domain-specific and context-sensitive language. Identification of note sections and structural document elements has been shown to \u2026"}, {"title": "PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain", "link": "https://arxiv.org/pdf/2506.00250", "details": "MJR Kalahroodi, A Sheikholselami, S Karimi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have achieved remarkable performance on a wide range of NLP benchmarks, often surpassing human-level accuracy. However, their reliability in high-stakes domains such as medicine, particularly in low-resource \u2026", "entry_id": "http://arxiv.org/abs/2506.00250v2", "updated": "2025-06-03 00:22:37", "published": "2025-05-30 21:34:30", "authors": "Mohammad Javad Ranjbar Kalahroodi;Amirhossein Sheikholselami;Sepehr Karimi;Sepideh Ranjbar Kalahroodi;Heshaam Faili;Azadeh Shakery", "summary": "Large Language Models (LLMs) have achieved remarkable performance on a wide\nrange of NLP benchmarks, often surpassing human-level accuracy. However, their\nreliability in high-stakes domains such as medicine, particularly in\nlow-resource languages, remains underexplored. In this work, we introduce\nPersianMedQA, a large-scale, expert-validated dataset of multiple-choice\nPersian medical questions, designed to evaluate LLMs across both Persian and\nEnglish. We benchmark over 40 state-of-the-art models, including\ngeneral-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and\nchain-of-thought (CoT) settings. Our results show that closed-source general\nmodels (e.g., GPT-4.1) consistently outperform all other categories, achieving\n83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models\nsuch as Dorna underperform significantly (e.g., 35.9% in Persian), often\nstruggling with both instruction-following and domain reasoning. We also\nanalyze the impact of translation, showing that while English performance is\ngenerally higher, Persian responses are sometimes more accurate due to cultural\nand clinical contextual cues. Finally, we demonstrate that model size alone is\ninsufficient for robust performance without strong domain or language\nadaptation. PersianMedQA provides a foundation for evaluating multilingual and\nculturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be\naccessed at: https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.IT;math.IT", "links": "http://arxiv.org/abs/2506.00250v2;http://arxiv.org/pdf/2506.00250v2", "pdf_url": "http://arxiv.org/pdf/2506.00250v2"}, {"title": "Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge", "link": "https://arxiv.org/pdf/2506.00777", "details": "MTR Laskar, I Jahan, E Dolatabadi, C Peng, E Hoque\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in biomedical relation extraction, even in zero-shot scenarios. However, evaluating LLMs in this task remains challenging due to their ability to generate human-like text \u2026", "entry_id": "http://arxiv.org/abs/2506.00777v1", "updated": "2025-06-01 02:01:52", "published": "2025-06-01 02:01:52", "authors": "Md Tahmid Rahman Laskar;Israt Jahan;Elham Dolatabadi;Chun Peng;Enamul Hoque;Jimmy Huang", "summary": "Large Language Models (LLMs) have demonstrated impressive performance in\nbiomedical relation extraction, even in zero-shot scenarios. However,\nevaluating LLMs in this task remains challenging due to their ability to\ngenerate human-like text, often producing synonyms or abbreviations of\ngold-standard answers, making traditional automatic evaluation metrics\nunreliable. On the other hand, while human evaluation is more reliable, it is\ncostly and time-consuming, making it impractical for real-world applications.\nThis paper investigates the use of LLMs-as-the-Judge as an alternative\nevaluation method for biomedical relation extraction. We benchmark 8 LLMs as\njudges to evaluate the responses generated by 5 other LLMs across 3 biomedical\nrelation extraction datasets. Unlike other text-generation tasks, we observe\nthat LLM-based judges perform quite poorly (usually below 50% accuracy) in the\nbiomedical relation extraction task. Our findings reveal that it happens mainly\nbecause relations extracted by LLMs do not adhere to any standard format. To\naddress this, we propose structured output formatting for LLM-generated\nresponses that helps LLM-Judges to improve their performance by about 15% (on\naverage). We also introduce a domain adaptation technique to further enhance\nLLM-Judge performance by effectively transferring knowledge between datasets.\nWe release both our human-annotated and LLM-annotated judgment data (36k\nsamples in total) for public use here:\nhttps://github.com/tahmedge/llm_judge_biomedical_re.", "comment": "Accepted at ACL 2025 (Main Conference)", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.00777v1;http://arxiv.org/pdf/2506.00777v1", "pdf_url": "http://arxiv.org/pdf/2506.00777v1"}, {"title": "Can Large Language Models Predict Audio Effects Parameters from Natural Language?", "link": "https://arxiv.org/pdf/2505.20770", "details": "S Doh, J Koo, MA Mart\u00ednez-Ram\u00edrez, WH Liao, J Nam\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In music production, manipulating audio effects (Fx) parameters through natural language has the potential to reduce technical barriers for non-experts. We present LLM2Fx, a framework leveraging Large Language Models (LLMs) to predict Fx \u2026", "entry_id": "http://arxiv.org/abs/2505.20770v1", "updated": "2025-05-27 06:21:56", "published": "2025-05-27 06:21:56", "authors": "Seungheon Doh;Junghyun Koo;Marco A. Mart\u00ednez-Ram\u00edrez;Wei-Hsiang Liao;Juhan Nam;Yuki Mitsufuji", "summary": "In music production, manipulating audio effects (Fx) parameters through\nnatural language has the potential to reduce technical barriers for\nnon-experts. We present LLM2Fx, a framework leveraging Large Language Models\n(LLMs) to predict Fx parameters directly from textual descriptions without\nrequiring task-specific training or fine-tuning. Our approach address the\ntext-to-effect parameter prediction (Text2Fx) task by mapping natural language\ndescriptions to the corresponding Fx parameters for equalization and\nreverberation. We demonstrate that LLMs can generate Fx parameters in a\nzero-shot manner that elucidates the relationship between timbre semantics and\naudio effects in music production. To enhance performance, we introduce three\ntypes of in-context examples: audio Digital Signal Processing (DSP) features,\nDSP function code, and few-shot examples. Our results demonstrate that\nLLM-based Fx parameter generation outperforms previous optimization approaches,\noffering competitive performance in translating natural language descriptions\nto appropriate Fx settings. Furthermore, LLMs can serve as text-driven\ninterfaces for audio production, paving the way for more intuitive and\naccessible music production tools.", "comment": "Submitted to WASPAA 2025", "journal_ref": null, "primary_category": "cs.SD", "categories": "cs.SD;cs.MM;eess.AS", "links": "http://arxiv.org/abs/2505.20770v1;http://arxiv.org/pdf/2505.20770v1", "pdf_url": "http://arxiv.org/pdf/2505.20770v1"}]
