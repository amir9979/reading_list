[{"title": "Vision-Language Model Dialog Games for Self-Improvement", "link": "https://arxiv.org/pdf/2502.02740", "details": "K Konyushkova, C Kaplanis, S Cabi, M Denil - arXiv preprint arXiv:2502.02740, 2025", "abstract": "The increasing demand for high-quality, diverse training data poses a significant bottleneck in advancing vision-language models (VLMs). This paper presents VLM Dialog Games, a novel and scalable self-improvement framework for VLMs. Our \u2026"}, {"title": "HaluCheck: Explainable and verifiable automation for detecting hallucinations in LLM responses", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425003343", "details": "S Heo, S Son, H Park - Expert Systems with Applications, 2025", "abstract": "Large language models have become integral to various aspects of modern life, but a critical challenge persists: hallucinations. This work contributes to expert systems research by providing a systematic framework for enhancing AI reliability and \u2026"}, {"title": "HuDEx: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM responses", "link": "https://arxiv.org/pdf/2502.08109", "details": "S Lee, H Lee, S Heo, W Choi - arXiv preprint arXiv:2502.08109, 2025", "abstract": "Recent advances in large language models (LLMs) have shown promising improvements, often surpassing existing methods across a wide range of downstream tasks in natural language processing. However, these models still face \u2026"}, {"title": "AIDE: Agentically Improve Visual Language Model with Domain Experts", "link": "https://arxiv.org/pdf/2502.09051", "details": "MC Chiu, F Liu, K Sapra, A Tao, Y Jacoob, X Ma, Z Yu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The enhancement of Visual Language Models (VLMs) has traditionally relied on knowledge distillation from larger, more capable models. This dependence creates a fundamental bottleneck for improving state-of-the-art systems, particularly when no \u2026"}, {"title": "ARIES: Stimulating Self-Refinement of Large Language Models by Iterative Preference Optimization", "link": "https://arxiv.org/pdf/2502.05605", "details": "Y Zeng, X Cui, X Jin, G Liu, Z Sun, Q He, D Li, N Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "A truly intelligent Large Language Model (LLM) should be capable of correcting errors in its responses through external interactions. However, even the most advanced models often face challenges in improving their outputs. In this paper, we \u2026"}, {"title": "A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene Graphs with Large-Language-Models (LLMs)", "link": "https://arxiv.org/pdf/2502.03450", "details": "Y Chen, H Sawhney, N Gyd\u00e9, Y Jian, J Saunders\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Scene graphs have emerged as a structured and serializable environment representation for grounded spatial reasoning with Large Language Models (LLMs). In this work, we propose SG-RwR, a Schema-Guided Retrieve-while-Reason \u2026"}, {"title": "A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks", "link": "https://arxiv.org/pdf/2502.06470", "details": "HM Nguyen - arXiv preprint arXiv:2502.06470, 2025", "abstract": "Theory of Mind (ToM), the ability to attribute mental states to others and predict their behaviour, is fundamental to social intelligence. In this paper, we survey studies evaluating behavioural and representational ToM in Large Language Models \u2026"}, {"title": "LR ${}^{2} $ Bench: Evaluating Long-chain Reflective Reasoning Capabilities of Large Language Models via Constraint Satisfaction Problems", "link": "https://arxiv.org/pdf/2502.17848", "details": "J Chen, Z Wei, Z Ren, Z Li, J Zhang - arXiv preprint arXiv:2502.17848, 2025", "abstract": "Recent progress in o1-like models has significantly enhanced the reasoning abilities of Large Language Models (LLMs), empowering them to tackle increasingly complex tasks through reflection capabilities, such as making assumptions, backtracking, and \u2026"}, {"title": "Towards Principled Training and Serving of Large Language Models", "link": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-6.pdf", "details": "B Zhu - 2025", "abstract": "Large Language Models (LLMs) have emerged as a transformative technology in artificial intelligence (AI), demonstrating unprecedented capabilities in tasks ranging from translation and summarization to code generation, complex reasoning, and \u2026"}]
