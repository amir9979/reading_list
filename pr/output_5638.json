[{"title": "Aligning (Medical) LLMs for (Counterfactual) Fairness", "link": "https://arxiv.org/pdf/2408.12055", "details": "R Poulain, H Fayyaz, R Beheshti - arXiv preprint arXiv:2408.12055, 2024", "abstract": "Large Language Models (LLMs) have emerged as promising solutions for a variety of medical and clinical decision support applications. However, LLMs are often subject to different types of biases, which can lead to unfair treatment of individuals \u2026"}, {"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "link": "https://arxiv.org/pdf/2408.12337", "details": "KS Phogat, SA Puranam, S Dasaratha, C Harsha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain \u2026"}, {"title": "When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?", "link": "https://arxiv.org/pdf/2408.11854", "details": "Y Gao, S Myers, S Chen, D Dligach, TA Miller\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The introduction of Large Language Models (LLMs) has advanced data representation and analysis, bringing significant progress in their use for medical questions and answering. Despite these advancements, integrating tabular data \u2026"}, {"title": "Differentially Private and Heterogeneity-Robust Federated Learning with Theoretical Guarantee", "link": "https://ieeexplore.ieee.org/abstract/document/10643038/", "details": "X Wang, S Wang, Y Li, F Fan, S Li, X Lin - IEEE Transactions on Artificial Intelligence, 2024", "abstract": "Federated learning (FL) is a popular distributed paradigm where enormous clients collaboratively train a machine learning (ML) model under the orchestration of a central server without knowing the clients' private raw data. The development of \u2026"}, {"title": "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "link": "https://arxiv.org/pdf/2408.02103", "details": "P Wang, X Wang, C Lou, S Mao, P Xie, Y Jiang - arXiv preprint arXiv:2408.02103, 2024", "abstract": "In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language \u2026"}, {"title": "Evaluating the necessity of the multiple metrics for assessing explainable AI: A critical examination", "link": "https://www.sciencedirect.com/science/article/pii/S0925231224010531", "details": "M Pawlicki, A Pawlicka, F Uccello, S Szelest\u2026 - Neurocomputing, 2024", "abstract": "This paper investigates the specific properties of Explainable Artificial Intelligence (xAI), particularly when implemented in AI/ML models across high-stakes sectors, in this case cybersecurity. The authors execute a comprehensive systematic review of \u2026"}, {"title": "SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models", "link": "https://arxiv.org/pdf/2408.02632", "details": "M Diao, R Li, S Liu, G Liao, J Wang, X Cai, W Xu - arXiv preprint arXiv:2408.02632, 2024", "abstract": "As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to \u2026"}, {"title": "MedSyn: LLM-based Synthetic Medical Text Generation Framework", "link": "https://arxiv.org/pdf/2408.02056", "details": "G Kumichev, P Blinov, Y Kuzkina, V Goncharov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating synthetic text addresses the challenge of data availability in privacy- sensitive domains such as healthcare. This study explores the applicability of synthetic data in real-world medical settings. We introduce MedSyn, a novel medical \u2026"}, {"title": "Demystifying Verbatim Memorization in Large Language Models", "link": "https://arxiv.org/pdf/2407.17817", "details": "J Huang, D Yang, C Potts - arXiv preprint arXiv:2407.17817, 2024", "abstract": "Large Language Models (LLMs) frequently memorize long sequences verbatim, often with serious legal and privacy implications. Much prior work has studied such verbatim memorization using observational data. To complement such work, we \u2026"}]
