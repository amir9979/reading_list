[{"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "link": "https://arxiv.org/pdf/2410.05639", "details": "R Zhao, ZL Thai, Y Zhang, S Hu, Y Ba, J Zhou, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the \u2026"}, {"title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures", "link": "https://arxiv.org/pdf/2410.07698", "details": "Y Chen, Y Zhang, L Cao, K Yuan, Z Wen - arXiv preprint arXiv:2410.07698, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs when adapting large language models (LLMs) for downstream applications. However, traditional first-order (FO) fine-tuning algorithms incur substantial memory overhead \u2026"}, {"title": "No Need to Talk: Asynchronous Mixture of Language Models", "link": "https://arxiv.org/pdf/2410.03529", "details": "A Filippova, A Katharopoulos, D Grangier, R Collobert - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce SmallTalk LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each model of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth \u2026"}, {"title": "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "link": "https://arxiv.org/pdf/2410.03122", "details": "Z Zhao, Y Yang, Y Li, Y Cao - arXiv preprint arXiv:2410.03122, 2024", "abstract": "The ripple effect poses a significant challenge in knowledge editing for large language models. Namely, when a single fact is edited, the model struggles to accurately update the related facts in a sequence, which is evaluated by multi-hop \u2026"}, {"title": "Are Expert-Level Language Models Expert-Level Annotators?", "link": "https://arxiv.org/pdf/2410.03254", "details": "YM Tseng, WL Chen, CC Chen, HH Chen - arXiv preprint arXiv:2410.03254, 2024", "abstract": "Data annotation refers to the labeling or tagging of textual data with relevant information. A large body of works have reported positive results on leveraging LLMs as an alternative to human annotators. However, existing studies focus on classic \u2026"}, {"title": "Scaling Parameter-Constrained Language Models with Quality Data", "link": "https://arxiv.org/pdf/2410.03083", "details": "E Chang, M Paltenghi, Y Li, PJ Lin, C Zhao, P Huber\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws in language modeling traditionally quantify training loss as a function of dataset size and model parameters, providing compute-optimal estimates but often neglecting the impact of data quality on model generalization. In this paper, we \u2026"}, {"title": "From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency", "link": "https://arxiv.org/pdf/2410.05459", "details": "K Wen, H Zhang, H Lin, J Zhang - arXiv preprint arXiv:2410.05459, 2024", "abstract": "Chain-of-thought (CoT) significantly enhances the reasoning performance of large language models (LLM). While current theoretical studies often attribute this improvement to increased expressiveness and computational capacity, we argue \u2026"}, {"title": "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "link": "https://arxiv.org/pdf/2410.06554", "details": "Y Chen, D Zhu, Y Sun, X Chen, W Zhang, X Shen - arXiv preprint arXiv:2410.06554, 2024", "abstract": "Reinforcement Learning from Human Feedback significantly enhances Natural Language Processing by aligning language models with human expectations. A critical factor in this alignment is the strength of reward models used during training \u2026"}, {"title": "AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "link": "https://arxiv.org/pdf/2410.07706", "details": "Y Song, W Xiong, X Zhao, D Zhu, W Wu, K Wang, C Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Fine-tuning on agent-environment interaction trajectory data holds significant promise for surfacing generalized agent capabilities in open-source large language models (LLMs). In this work, we introduce AgentBank, by far the largest trajectory \u2026"}]
