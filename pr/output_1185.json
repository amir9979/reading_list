'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Recall Them All: Retrieval-Augmented Language Models f'
[{"title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training", "link": "https://arxiv.org/pdf/2405.03133", "details": "Z Zhong, M Xia, D Chen, M Lewis - arXiv preprint arXiv:2405.03133, 2024", "abstract": "Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture, SMEAR, was proposed \u2026"}, {"title": "Skimming of Electronic Health Records Highlighted by an Interface Terminology Curated with Machine Learning Mining", "link": "https://www.scitepress.org/Papers/2024/123916/123916.pdf", "details": "MKH Dehkordi, NM Kollapally, Y Perl, J Geller\u2026", "abstract": "Clinical notes in Electronic Health Records (EHRs) contain large amounts of nuanced information. Healthcare professionals, eg, clinicians, routinely review numerous EHR notes, further burdening their busy schedules. To capture the \u2026"}, {"title": "Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation", "link": "https://arxiv.org/pdf/2404.17489", "details": "W Cui, R Hosseinzadeh, J Ma, T Wu, Y Sui, K Golestan - arXiv preprint arXiv \u2026, 2024", "abstract": "Contrastive learning is a model pre-training technique by first creating similar views of the original data, and then encouraging the data and its corresponding views to be close in the embedding space. Contrastive learning has witnessed success in image \u2026"}]
