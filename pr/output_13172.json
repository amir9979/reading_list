[{"title": "DR. GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting with Demonstration and Reasoning", "link": "https://arxiv.org/pdf/2502.11603", "details": "H Qiu, Y Xu, M Qiu, W Wang - arXiv preprint arXiv:2502.11603, 2025", "abstract": "Large Language Models (LLMs) exhibit strong natural language processing capabilities but also inherit and amplify societal biases, including gender bias, raising fairness concerns. Existing debiasing methods face significant limitations \u2026"}, {"title": "Policy-Guided Causal State Representation for Offline Reinforcement Learning Recommendation", "link": "https://arxiv.org/pdf/2502.02327", "details": "S Wang, X Chen, L Yao - arXiv preprint arXiv:2502.02327, 2025", "abstract": "In offline reinforcement learning-based recommender systems (RLRS), learning effective state representations is crucial for capturing user preferences that directly impact long-term rewards. However, raw state representations often contain high \u2026"}, {"title": "Less is More: Improving LLM Alignment via Preference Data Selection", "link": "https://arxiv.org/pdf/2502.14560", "details": "X Deng, H Zhong, R Ai, F Feng, Z Wang, X He - arXiv preprint arXiv:2502.14560, 2025", "abstract": "Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from \u2026"}, {"title": "Uncertainty-aware fusion: An ensemble framework for mitigating hallucinations in large language models", "link": "https://www.amazon.science/publications/uncertainty-aware-fusion-an-ensemble-framework-for-mitigating-hallucinations-in-large-language-models", "details": "P Dey, S Merugu, SS Kaveri - 2025", "abstract": "ABSTRACT Large Language Models (LLMs) are known to hallucinate and generate non-factual outputs which can undermine user trust. Traditional methods to directly mitigate hallucinations, such as representation editing and contrastive decoding \u2026"}, {"title": "Analyzing patient perspectives with large language models: a cross-sectional study of sentiment and thematic classification on exception from informed consent", "link": "https://www.nature.com/articles/s41598-025-89996-w", "details": "AE Kornblith, C Singh, JC Innes, TP Chang\u2026 - Scientific Reports, 2025", "abstract": "Large language models (LLMs) can improve text analysis efficiency in healthcare. This study explores the application of LLMs to analyze patient perspectives within the exception from informed consent (EFIC) process, which waives consent in \u2026"}, {"title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models", "link": "https://arxiv.org/pdf/2502.12420", "details": "S Liu, H Wu, B He, X Han, M Yuan, L Song - arXiv preprint arXiv:2502.12420, 2025", "abstract": "Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task \u2026"}, {"title": "A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks", "link": "https://arxiv.org/pdf/2502.06470", "details": "HM Nguyen - arXiv preprint arXiv:2502.06470, 2025", "abstract": "Theory of Mind (ToM), the ability to attribute mental states to others and predict their behaviour, is fundamental to social intelligence. In this paper, we survey studies evaluating behavioural and representational ToM in Large Language Models \u2026"}, {"title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models", "link": "https://arxiv.org/pdf/2502.10743", "details": "S Liu, H Wu, B He, Z Liu, X Han, M Yuan, L Song - arXiv preprint arXiv:2502.10743, 2025", "abstract": "Recent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they \u2026"}]
