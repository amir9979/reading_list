[{"title": "Semantic Compositions Enhance Vision-Language Contrastive Learning", "link": "https://arxiv.org/pdf/2407.01408", "details": "M Aladago, L Torresani, S Vosoughi - arXiv preprint arXiv:2407.01408, 2024", "abstract": "In the field of vision-language contrastive learning, models such as CLIP capitalize on matched image-caption pairs as positive examples and leverage within-batch non- matching pairs as negatives. This approach has led to remarkable outcomes in zero \u2026"}, {"title": "Learning Unsupervised Gaze Representation via Eye Mask Driven Information Bottleneck", "link": "https://arxiv.org/pdf/2407.00315", "details": "Y Jiang, Y Lin, Y Wang, T Li, B Ke, B Ni - arXiv preprint arXiv:2407.00315, 2024", "abstract": "Appearance-based supervised methods with full-face image input have made tremendous advances in recent gaze estimation tasks. However, intensive human annotation requirement inhibits current methods from achieving industrial level \u2026"}, {"title": "A Deep Multimodal Representation Learning Framework for Accurate Molecular Properties Prediction", "link": "https://dl.acm.org/doi/abs/10.1145/3649476.3660377", "details": "Y Yang, Z Wang, P Ahadian, A Jerger, J Zucker, S Feng\u2026 - Proceedings of the Great \u2026, 2024", "abstract": "Drug discovery is a challenging process, requiring the optimization of compounds to become safe and effective. Predicting molecular properties is an indispensable step in the drug discovery pipeline. Traditionally, this process is costly, involving multiple \u2026"}, {"title": "Training Dynamics of Nonlinear Contrastive Learning Model in the High Dimensional Limit", "link": "https://arxiv.org/pdf/2406.06909", "details": "L Meng, C Wang - arXiv preprint arXiv:2406.06909, 2024", "abstract": "This letter presents a high-dimensional analysis of the training dynamics for a single- layer nonlinear contrastive learning model. The empirical distribution of the model weights converges to a deterministic measure governed by a McKean-Vlasov \u2026"}, {"title": "Towards Multimodal Open-Set Domain Generalization and Adaptation through Self-supervision", "link": "https://arxiv.org/pdf/2407.01518", "details": "H Dong, E Chatzi, O Fink - arXiv preprint arXiv:2407.01518, 2024", "abstract": "The task of open-set domain generalization (OSDG) involves recognizing novel classes within unseen domains, which becomes more challenging with multiple modalities as input. Existing works have only addressed unimodal OSDG within the \u2026"}]
