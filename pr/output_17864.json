[{"title": "Re-Evaluating Code LLM Benchmarks Under Semantic Mutation", "link": "https://arxiv.org/pdf/2506.17369", "details": "Z Pan, X Hu, X Xia, X Yang - arXiv preprint arXiv:2506.17369, 2025", "abstract": "\u2026 This is a common practice in **LLM** **evaluation** , since LLMs often generate extraneous content (eg, explanations and comments), which causes the evaluation code to fail and further leads to untrustworthy conclusions. As described in Section \u2026", "entry_id": "http://arxiv.org/abs/2506.17369v1", "updated": "2025-06-20 15:30:36", "published": "2025-06-20 15:30:36", "authors": "Zhiyuan Pan;Xing Hu;Xin Xia;Xiaohu Yang", "summary": "In the era of large language models (LLMs), code benchmarks have become an\nimportant research area in software engineering and are widely used by\npractitioners. These benchmarks evaluate the performance of LLMs on specific\ncode-related tasks, such as code understanding and generation. A critical step\nin constructing code benchmarks is the design of prompts. However, as existing\ncode benchmarks typically rely on a single prompt template per task, they are\nprone to the issue of prompt sensitivity, where minor prompt variations could\nresult in substantial performance variations, leading to unreliable evaluations\nof model capabilities.\n  While previous studies have explored prompt sensitivity, their experimental\ndesigns and findings are limited to traditional natural language processing\n(NLP) tasks. In this paper, we present an empirical study to investigate prompt\nsensitivity in code benchmarks. We first propose a general framework that\nmodifies prompt templates in a manner that preserves both their semantics and\ntheir structure as much as possible. Based on the framework, we conduct\nextensive experiments across eight code benchmark tasks on 10 representative\nopen-source LLMs, with each task featuring 100 semantically similar prompt\ntemplates. We then analyze the evaluation results using various statistical\nmetrics, focusing on both absolute and relative model performance. Our findings\nsuggest that even slight prompt variations can lead to significant shifts in\nperformance. Additionally, we observe that such variations can introduce\ninconsistencies in the performance rankings across different models. These\ninsights highlight the need for considering prompt sensitivity when designing\nfuture code benchmarks, to ensure more reliable and accurate evaluation of LLM\ncapabilities.", "comment": null, "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE;cs.AI", "links": "http://arxiv.org/abs/2506.17369v1;http://arxiv.org/pdf/2506.17369v1", "pdf_url": "http://arxiv.org/pdf/2506.17369v1"}, {"title": "When LLM-Generated Code Perpetuates User Interface Accessibility Barriers, How Can We Break the Cycle?", "link": "https://mintviz.usv.ro/publications/2025.W4A.3.pdf", "details": "AE Guri\u0163\u0103, RD Vatavu - 2025", "abstract": "\u2026 Self-reflection **LLM** **evaluation**. This approach is inspired by Othman et al.\u2019s [38] findings about ChatGPT achieving 94% accuracy in identifying and self-remediating accessibility issues. For each generated UI, we asked the LLMs to perform a self-assessment \u2026"}, {"title": "Position: Principles of Animal Cognition to Improve LLM Evaluations", "link": "https://openreview.net/pdf%3Fid%3DgCPJFcHskT", "details": "S Rane, CF Kirkman, G Todd, A Royka, RMC Law\u2026 - Forty-second International \u2026", "abstract": "\u2026 Taking inspiration from a wide range of such studies and model organisms, we present five \u201ccore principles\u201d that offer useful lessons to improve **LLM** **evaluation**. We first list the principles, along with several representative papers exemplifying each \u2026"}, {"title": "LLMs for Customized Marketing Content Generation and Evaluation at Scale", "link": "https://arxiv.org/pdf/2506.17863", "details": "H Liu, A Tahmasbi, ES Haque, P Jain - arXiv preprint arXiv:2506.17863, 2025", "abstract": "\u2026 Table 8: Ablation study of false positive rate (FPR, concerned rate), false negative rate (FNR, wasted rate), and total misalignment rate between human and **LLM** **evaluation** across different methods and threshold settings. \u201cContext (X) + General (Y)\u201d \u2026", "entry_id": "http://arxiv.org/abs/2506.17863v1", "updated": "2025-06-22 00:28:35", "published": "2025-06-22 00:28:35", "authors": "Haoran Liu;Amir Tahmasbi;Ehtesham Sam Haque;Purak Jain", "summary": "Offsite marketing is essential in e-commerce, enabling businesses to reach\ncustomers through external platforms and drive traffic to retail websites.\nHowever, most current offsite marketing content is overly generic,\ntemplate-based, and poorly aligned with landing pages, limiting its\neffectiveness. To address these limitations, we propose MarketingFM, a\nretrieval-augmented system that integrates multiple data sources to generate\nkeyword-specific ad copy with minimal human intervention. We validate\nMarketingFM via offline human and automated evaluations and large-scale online\nA/B tests. In one experiment, keyword-focused ad copy outperformed templates,\nachieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,\ndemonstrating gains in ad ranking and cost efficiency. Despite these gains,\nhuman review of generated ads remains costly. To address this, we propose\nAutoEval-Main, an automated evaluation system that combines rule-based metrics\nwith LLM-as-a-Judge techniques to ensure alignment with marketing principles.\nIn experiments with large-scale human annotations, AutoEval-Main achieved\n89.57% agreement with human reviewers. Building on this, we propose\nAutoEval-Update, a cost-efficient LLM-human collaborative framework to\ndynamically refine evaluation prompts and adapt to shifting criteria with\nminimal human input. By selectively sampling representative ads for human\nreview and using a critic LLM to generate alignment reports, AutoEval-Update\nimproves evaluation consistency while reducing manual effort. Experiments show\nthe critic LLM suggests meaningful refinements, improving LLM-human agreement.\nNonetheless, human oversight remains essential for setting thresholds and\nvalidating refinements before deployment.", "comment": "KDD LLM4ECommerce Workshop 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.17863v1;http://arxiv.org/pdf/2506.17863v1", "pdf_url": "http://arxiv.org/pdf/2506.17863v1"}, {"title": "Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection", "link": "https://dl.acm.org/doi/pdf/10.1145/3728878", "details": "L Yu, Z Huang, H Yuan, S Cheng, L Yang, F Zhang\u2026 - Proceedings of the ACM on \u2026, 2025", "abstract": "\u2026 two parts: **LLM** **Evaluation** and Human Evaluation, each assessing three dimensions: Correctness, Thoroughness, and Clarity. Our model significantly outperformed the best baseline (iAudit) in both **LLM** **evaluation** and human \u2026"}, {"title": "CUE-X: A Framework for the Automatic Evaluation of Clinical Usefulness of Explanations for the Multimorbidity Problem", "link": "https://link.springer.com/chapter/10.1007/978-3-031-95838-0_29", "details": "M Michalowski, S Wilk, JM Bauer, M Carrier, H Viktor\u2026 - International Conference on \u2026, 2025", "abstract": "\u2026 [1] found that only 5 out of 519 reviewed studies discussing the applications of LLMs in healthcare used actual patient data for **LLM** **evaluation**. Moreover, in most studies, LLMs were assessed based on accuracy, while more complex dimensions \u2026"}, {"title": "Position: Medical Large Language Model Benchmarks Should Prioritize Construct Validity", "link": "https://openreview.net/pdf%3Fid%3DYuMEUNNpeb", "details": "A Alaa, T Hartvigsen, N Golchini, S Dutta, F Dean\u2026 - Forty-second International \u2026", "abstract": "\u2026 Finally, we outline a vision for a new ecosystem of medical **LLM** **evaluation** centered around the creation of valid benchmarks. \u2026 This points to a deeper issue in the current discourse surrounding **LLM** **evaluation** : the lack of a principled framework for \u2026"}, {"title": "InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating", "link": "https://arxiv.org/pdf/2506.18102", "details": "F Wang, J Li, K Zhu, C Jiang - arXiv preprint arXiv:2506.18102, 2025", "abstract": "\u2026 Together, these contributions establish a systematic foundation for **LLM** **evaluation** in debate scenarios and pave the way for future advancements in autonomous debate agent optimization, fostering more structured, transparent, and \u2026", "entry_id": "http://arxiv.org/abs/2506.18102v1", "updated": "2025-06-22 17:14:29", "published": "2025-06-22 17:14:29", "authors": "Fuyu Wang;Jiangtong Li;Kun Zhu;Changjun Jiang", "summary": "With the rapid advancements in large language models (LLMs), debating tasks,\nsuch as argument quality assessment and debate process simulation, have made\nsignificant progress. However, existing LLM-based debating systems focus on\nresponding to specific arguments while neglecting objective assessments such as\nauthenticity and logical validity. Furthermore, these systems lack a structured\napproach to optimize across various dimensions$-$including evaluation metrics,\nchain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby\nlimiting their effectiveness. To address these interconnected challenges, we\npropose a dual-component framework: (1) $\\textbf{InspireScore}$, a novel\nevaluation system that establishes a multi-dimensional assessment architecture\nincorporating four subjective criteria (emotional appeal, argument clarity,\nargument arrangement, and topic relevance) alongside two objective metrics\n(fact authenticity and logical validity); and (2) $\\textbf{InspireDebate}$, an\noptimized debating framework employing a phased optimization approach through\nCoT reasoning enhancement, multi-dimensional Direct Preference Optimization\n(DPO), and real-time knowledge grounding via web-based Retrieval Augmented\nGeneration (Web-RAG). Empirical evaluations demonstrate that\n$\\textbf{InspireScore}$ achieves 44$\\%$ higher correlation with expert\njudgments compared to existing methods, while $\\textbf{InspireDebate}$ shows\nsignificant improvements, outperforming baseline models by 57$\\%$. Source code\nis available at https://github.com/fywang12/InspireDebate.", "comment": "20 pages; Accepted to ACL 2025 Main", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.18102v1;http://arxiv.org/pdf/2506.18102v1", "pdf_url": "http://arxiv.org/pdf/2506.18102v1"}, {"title": "EncryptedLLM: Privacy-Preserving Large Language Model Inference via GPU-Accelerated Fully Homomorphic Encryption", "link": "https://openreview.net/pdf%3Fid%3DPGNff6H1TV", "details": "L de Castro, D Escudero, A Agrawal, A Polychroniadou\u2026 - Forty-second International \u2026", "abstract": "\u2026 2024), we approximate all functions required in the **LLM** **evaluation** with low-degree polynomials. Keeping the degree low is important as this minimizes the levels consumed in the polynomial evaluation, resulting in fewer bootstrapping calls \u2026"}]
