[{"title": "Explainable Multi-Modal Learning in Remote Sensing: Challenges and Future Directions", "link": "https://ieeexplore.ieee.org/abstract/document/10537186/", "details": "A G\u00fcnther, H Najjar, A Dengel - IEEE Geoscience and Remote Sensing Letters, 2024", "abstract": "Earth observation applications effectively leverage deep learning models to harness the abundantly available remote sensing data. In order to use all the different modalities relevant to a specific task, the fusion of these data sources can be \u2026"}, {"title": "Interpretable Tensor Fusion", "link": "https://arxiv.org/pdf/2405.04671", "details": "S Varshneya, A Ledent, P Liznerski, A Balinskyy\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Conventional machine learning methods are predominantly designed to predict outcomes based on a single data type. However, practical applications may encompass data of diverse types, such as text, images, and audio. We introduce \u2026"}, {"title": "Ensemble feature selection and tabular data augmentation with generative adversarial networks to enhance cutaneous melanoma identification and interpretability", "link": "https://www.researchsquare.com/article/rs-4402096/latest.pdf", "details": "V G\u00f3mez-Mart\u00ednez, D Chushig-Muzo, MB Veier\u00f8d\u2026 - 2024", "abstract": "Background: Cutaneous melanoma, the most serious skin cancer, constitutes a considerable health burden in fair-skinned populations. Its increasing incidence highlights the need to develop automated computer-aided approaches that support \u2026"}]
