[{"title": "SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models", "link": "https://arxiv.org/pdf/2505.17470", "details": "X Liu, Z Liu, P Wang, K Wang, H Hu, K Wang, S Lian - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 To validate the effectiveness of our method, we created two datasets: the SFT datasets for the **medical** and agricultural domains. For the **medical** domain, we engaged multiple **medical** experts to manually craft 35,000 QA pairs, which are \u2026", "entry_id": "http://arxiv.org/abs/2505.17470v1", "updated": "2025-05-23 04:50:54", "published": "2025-05-23 04:50:54", "authors": "Xiang Liu;Zhaoxiang Liu;Peng Wang;Kohou Wang;Huan Hu;Kai Wang;Shiguo Lian", "summary": "When using supervised fine-tuning (SFT) to adapt large language models (LLMs)\nto specific domains, a significant challenge arises: should we use the entire\nSFT dataset for fine-tuning? Common practice often involves fine-tuning\ndirectly on the entire dataset due to limited information on the LLM's past\ntraining data. However, if the SFT dataset largely overlaps with the model's\nexisting knowledge, the performance gains are minimal, leading to wasted\ncomputational resources. Identifying the unknown knowledge within the SFT\ndataset and using it to fine-tune the model could substantially improve the\ntraining efficiency. To address this challenge, we propose a self-learning\nframework for LLMs inspired by human learning pattern. This framework takes a\nfine-tuning (SFT) dataset in a specific domain as input. First, the LLMs answer\nthe questions in the SFT dataset. The LLMs then objectively grade the responses\nand filter out the incorrectly answered QA pairs. Finally, we fine-tune the\nLLMs based on this filtered QA set. Experimental results in the fields of\nagriculture and medicine demonstrate that our method substantially reduces\ntraining time while achieving comparable improvements to those attained with\nfull dataset fine-tuning. By concentrating on the unknown knowledge within the\nSFT dataset, our approach enhances the efficiency of fine-tuning LLMs.", "comment": "12 pages, 5 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.17470v1;http://arxiv.org/pdf/2505.17470v1", "pdf_url": "http://arxiv.org/pdf/2505.17470v1"}, {"title": "Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL", "link": "https://arxiv.org/pdf/2505.17952", "details": "C Liu, H Wang, J Pan, Z Wan, Y Dai, F Lin, W Bai\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Improving performance on complex tasks and enabling interpretable decision making in **large** **language** **models** (LLMs), especially for clinical \u2026 **questions** from Indian **medical** entrance exams (AIIMS, NEET). PubMedQA [34] focuses on \u2026", "entry_id": "http://arxiv.org/abs/2505.17952v1", "updated": "2025-05-23 14:27:37", "published": "2025-05-23 14:27:37", "authors": "Che Liu;Haozhe Wang;Jiazhen Pan;Zhongwei Wan;Yong Dai;Fangzhen Lin;Wenjia Bai;Daniel Rueckert;Rossella Arcucci", "summary": "Improving performance on complex tasks and enabling interpretable decision\nmaking in large language models (LLMs), especially for clinical applications,\nrequires effective reasoning. Yet this remains challenging without supervised\nfine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from\nclosed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the\nfirst medical LLM to show that reasoning capability can emerge purely through\nreinforcement learning (RL), using minimalist rule-based rewards on public\nmultiple-choice QA datasets, without relying on SFT or distilled CoT data.\nAlphaMed achieves state-of-the-art results on six medical QA benchmarks,\noutperforming models trained with conventional SFT+RL pipelines. On challenging\nbenchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source\nmodels such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the\nfactors behind this success, we conduct a comprehensive data-centric analysis\nguided by three questions: (i) Can minimalist rule-based RL incentivize\nreasoning without distilled CoT supervision? (ii) How do dataset quantity and\ndiversity impact reasoning? (iii) How does question difficulty shape the\nemergence and generalization of reasoning? Our findings show that dataset\ninformativeness is a key driver of reasoning performance, and that minimalist\nRL on informative, multiple-choice QA data is effective at inducing reasoning\nwithout CoT supervision. We also observe divergent trends across benchmarks,\nunderscoring limitations in current evaluation and the need for more\nchallenging, reasoning-oriented medical QA benchmarks.", "comment": "Under Review", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.17952v1;http://arxiv.org/pdf/2505.17952v1", "pdf_url": "http://arxiv.org/pdf/2505.17952v1"}, {"title": "MEDMKG: Benchmarking Medical Knowledge Exploitation with Multimodal Knowledge Graph", "link": "https://arxiv.org/pdf/2505.17214", "details": "X Wang, Y Zhong, L Zhang, L Dai, T Wang, F Ma - arXiv preprint arXiv:2505.17214, 2025", "abstract": "\u2026 To address this, we design a two-stage pipeline that leverages the complementary strengths of rule-based systems and **large** **language** **models** (\u2026 -augmented visual **question** **answering** methods with our proposed MEDMKG, we adopt three \u2026", "entry_id": "http://arxiv.org/abs/2505.17214v1", "updated": "2025-05-22 18:41:46", "published": "2025-05-22 18:41:46", "authors": "Xiaochen Wang;Yuan Zhong;Lingwei Zhang;Lisong Dai;Ting Wang;Fenglong Ma", "summary": "Medical deep learning models depend heavily on domain-specific knowledge to\nperform well on knowledge-intensive clinical tasks. Prior work has primarily\nleveraged unimodal knowledge graphs, such as the Unified Medical Language\nSystem (UMLS), to enhance model performance. However, integrating multimodal\nmedical knowledge graphs remains largely underexplored, mainly due to the lack\nof resources linking imaging data with clinical concepts. To address this gap,\nwe propose MEDMKG, a Medical Multimodal Knowledge Graph that unifies visual and\ntextual medical information through a multi-stage construction pipeline. MEDMKG\nfuses the rich multimodal data from MIMIC-CXR with the structured clinical\nknowledge from UMLS, utilizing both rule-based tools and large language models\nfor accurate concept extraction and relationship modeling. To ensure graph\nquality and compactness, we introduce Neighbor-aware Filtering (NaF), a novel\nfiltering algorithm tailored for multimodal knowledge graphs. We evaluate\nMEDMKG across three tasks under two experimental settings, benchmarking\ntwenty-four baseline methods and four state-of-the-art vision-language\nbackbones on six datasets. Results show that MEDMKG not only improves\nperformance in downstream medical tasks but also offers a strong foundation for\ndeveloping adaptive and robust strategies for multimodal knowledge integration\nin medical artificial intelligence.", "comment": "Submitted to Neurips 2025", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.17214v1;http://arxiv.org/pdf/2505.17214v1", "pdf_url": "http://arxiv.org/pdf/2505.17214v1"}, {"title": "WiNGPT-3.0 Technical Report", "link": "https://arxiv.org/pdf/2505.17387", "details": "B Zhuang, C Song, H Lu, J Qiao, M Liu, M Yu, P Hong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Current **Large** **Language** **Models** (LLMs) demonstrate significant limitations when performing **medical** reasoning that must be structured, \u2026 MedMCQA: A large-scale multi-subject multi-choice dataset for **medical** domain **question** **answering**. In \u2026", "entry_id": "http://arxiv.org/abs/2505.17387v1", "updated": "2025-05-23 01:53:04", "published": "2025-05-23 01:53:04", "authors": "Boqin Zhuang;Chenxiao Song;Huitong Lu;Jiacheng Qiao;Mingqian Liu;Mingxing Yu;Ping Hong;Rui Li;Xiaoxia Song;Xiangjun Xu;Xu Chen;Yaoyao Ma;Yujie Gao", "summary": "Current Large Language Models (LLMs) exhibit significant limitations, notably\nin structured, interpretable, and verifiable medical reasoning, alongside\npractical deployment challenges related to computational resources and data\nprivacy. This report focused on the development of WiNGPT-3.0, the 32-billion\nparameter LLMs, engineered with the objective of enhancing its capacity for\nmedical reasoning and exploring its potential for effective integration within\nhealthcare IT infrastructures. The broader aim is to advance towards clinically\napplicable models. The approach involved a multi-stage training pipeline\ntailored for general, medical, and clinical reasoning. This pipeline\nincorporated supervised fine-tuning (SFT) and reinforcement learning (RL),\nleveraging curated Long Chain-of-Thought (CoT) datasets, auxiliary reward\nmodels, and an evidence-based diagnostic chain simulation. WiNGPT-3.0\ndemonstrated strong performance: specific model variants achieved scores of\n66.6 on MedCalc and 87.1 on MedQA-USMLE. Furthermore, targeted training\nimproved performance on a clinical reasoning task from a baseline score of 58.1\nto 62.5. These findings suggest that reinforcement learning, even when applied\nwith a limited dataset of only a few thousand examples, can enhance medical\nreasoning accuracy. Crucially, this demonstration of RL's efficacy with limited\ndata and computation paves the way for more trustworthy and practically\ndeployable LLMs within clinical workflows and health information\ninfrastructures.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.17387v1;http://arxiv.org/pdf/2505.17387v1", "pdf_url": "http://arxiv.org/pdf/2505.17387v1"}, {"title": "A Comparative Analysis of GPT-4o and ERNIE Bot in a Chinese Radiation Oncology Exam", "link": "https://link.springer.com/article/10.1007/s13187-025-02652-9", "details": "W Wang, J Fu, Y Zhang, K Hu - Journal of Cancer Education, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) are increasingly utilized in **medical** education and practice, yet their application in niche fields such as radiation oncology remains underexplored. This study evaluates and compares the performance of OpenAI\u2019s \u2026"}, {"title": "Assessing ChatGPT responses to patient **questions** on epidural steroid injections: A comparative study of general vs specific queries", "link": "https://www.sciencedirect.com/science/article/pii/S2772594425000536", "details": "T Olivier, Z Ma, A Patel, W Shi, M Murtuza, NE Hatchard\u2026 - Interventional Pain **Medicine** , 2025", "abstract": "\u2026 integrated into **healthcare** , with **large** **language** **models** (LLMs) like ChatGPT being widely used by patients to **answer** **medical** **questions**. Given the \u2026 patient concerns, especially in procedural **medicine**. To date, no studies have specifically \u2026"}, {"title": "Factual Knowledge-Enhanced **Question Answering** in Dynamic Environments", "link": "https://search.proquest.com/openview/b2506b9fd9016ccd8804ff2f724dff3e/1%3Fpq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy", "details": "S Shaier - 2025", "abstract": "\u2026 As a result, **large** **language** **models** (LLMs) are often trained on corpora containing billions of words, sourced from diverse domains such \u2026 to a specific domain, such as **medical** **question** **answering** , by fine-tuning on a smaller labeled \u2026"}, {"title": "AI-Generated Draft Replies to Patient Messages: Exploring Effects of Implementation", "link": "https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2025.1588143/abstract", "details": "CM Bootsma-Robroeks, J Workum, S Schuit, T Mehri\u2026 - Frontiers in Digital Health", "abstract": "\u2026 In this study, the effect of implementing LLM-generated draft responses to patient **questions** in our EHR is evaluated with regard to adoption, use and potential time savings. Material and MethodsPhysicians across 14 **medical** specialties in a non-English \u2026"}, {"title": "Using Language for Efficient, Explainable, and Interactive Machine Learning", "link": "https://cdr.lib.unc.edu/downloads/q524k351v", "details": "R Radhakrishnan Menon - 2025", "abstract": "\u2026 enabling **large** **language** **models** to acquire and refine concepts through **question** -driven \u2026 ) leverages large-scale data and models to power applications in **healthcare** diagnostics [49\u2026 Furthermore, explanations for generative tasks, such as closed-book \u2026"}]
