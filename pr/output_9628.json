[{"title": "Mixed Distillation Helps Smaller Language Models Reason Better", "link": "https://aclanthology.org/2024.findings-emnlp.91.pdf", "details": "L Chenglin, Q Chen, L Li, C Wang, F Tao, Y Li, Z Chen\u2026 - Findings of the Association \u2026, 2024", "abstract": "As large language models (LLMs) have demonstrated impressive multiple step-by- step reasoning capabilities in recent natural language processing (NLP) reasoning tasks, many studies are interested in distilling reasoning abilities into smaller \u2026"}, {"title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models", "link": "https://openreview.net/pdf%3Fid%3DLC1QAqhePv", "details": "D Zhang, Z Hu, S Zhoubian, Z Du, K Yang, Z Wang\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving \u2026"}, {"title": "Metaaligner: Towards generalizable multi-objective alignment of language models", "link": "https://openreview.net/pdf%3Fid%3DdIVb5C0QFf", "details": "K Yang, Z Liu, Q Xie, J Huang, T Zhang, S Ananiadou - The Thirty-eighth Annual \u2026, 2024", "abstract": "Recent advancements in large language models (LLMs) focus on aligning to heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are dependent on the policy model \u2026"}, {"title": "Reducing Distraction in Long-Context Language Models by Focused Learning", "link": "https://arxiv.org/pdf/2411.05928", "details": "Z Wu, B Liu, R Yan, L Chen, T Delteil - arXiv preprint arXiv:2411.05928, 2024", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced their capacity to process long contexts. However, effectively utilizing this long context remains a challenge due to the issue of distraction, where irrelevant \u2026"}, {"title": "De-identification is not enough: a comparison between de-identified and synthetic clinical notes", "link": "https://www.nature.com/articles/s41598-024-81170-y", "details": "AR Sarkar, YS Chuang, N Mohammed, X Jiang - Scientific Reports, 2024", "abstract": "For sharing privacy-sensitive data, de-identification is commonly regarded as adequate for safeguarding privacy. Synthetic data is also being considered as a privacy-preserving alternative. Recent successes with numerical and tabular data \u2026"}, {"title": "S $^{2} $ FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity", "link": "https://openreview.net/pdf%3Fid%3DlEUle8S4xQ", "details": "X Yang, J Leng, G Guo, J Zhao, R Nakada, L Zhang\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in \u2026"}, {"title": "CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models", "link": "https://arxiv.org/pdf/2411.04329", "details": "J Li, H Le, Y Zhou, C Xiong, S Savarese, D Sahoo - arXiv preprint arXiv:2411.04329, 2024", "abstract": "Pre-trained on massive amounts of code and text data, large language models (LLMs) have demonstrated remarkable achievements in performing code generation tasks. With additional execution-based feedback, these models can act as agents \u2026"}, {"title": "BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment", "link": "https://aclanthology.org/2024.emnlp-main.623.pdf", "details": "W Xu, J Li, WY Wang, L Li - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Direct alignment from preferences (DAP) has emerged as a promising paradigm for aligning large language models (LLMs) to human desiderata from pre-collected, offline preference datasets. While recent studies indicate that existing offline DAP \u2026"}, {"title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "link": "https://aclanthology.org/2024.emnlp-main.128.pdf", "details": "Y Wan, W Wang, Y Yang, Y Yuan, J Huang, P He\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT- 4\\. Despite LLMs' prowess in tasks like writing assistance, code generation, and \u2026"}]
