[{"title": "Distributed Synthetic Time-Series Data Generation with Local Differentially Private Federated Learning", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10720010.pdf", "details": "X Jiang, X Zhou, J Grossklags - IEEE Access, 2024", "abstract": "Devices such as cell phones, vehicles, and smart home systems often generate time- series data that is crucial for developing effective AI applications. However, directly analyzing and using this local data could violate user privacy. Although synthetic \u2026"}, {"title": "Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting Framework", "link": "https://arxiv.org/pdf/2409.16146", "details": "L Chen, R Zhang, J Guo, Y Fan, X Cheng - arXiv preprint arXiv:2409.16146, 2024", "abstract": "Retrieval-augmented generation (RAG) has emerged as a popular solution to mitigate the hallucination issues of large language models. However, existing studies on RAG seldom address the issue of predictive uncertainty, ie, how likely it is \u2026"}, {"title": "Geometry-Aware Generative Autoencoders for Warped Riemannian Metric Learning and Generative Modeling on Data Manifolds", "link": "https://arxiv.org/pdf/2410.12779", "details": "X Sun, D Liao, K MacDonald, Y Zhang, C Liu, G Huguet\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Rapid growth of high-dimensional datasets in fields such as single-cell RNA sequencing and spatial genomics has led to unprecedented opportunities for scientific discovery, but it also presents unique computational and statistical \u2026"}, {"title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "link": "https://arxiv.org/pdf/2410.11829", "details": "Y Cao, Y Liu, Z Chen, G Shi, W Wang, D Zhao, T Lu - arXiv preprint arXiv:2410.11829, 2024", "abstract": "Despite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating \u2026"}, {"title": "Harnessing Diversity for Important Data Selection in Pretraining Large Language Models", "link": "https://arxiv.org/pdf/2409.16986", "details": "C Zhang, H Zhong, K Zhang, C Chai, R Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Data selection is of great significance in pre-training large language models, given the variation in quality within the large-scale available training corpora. To achieve this, researchers are currently investigating the use of data influence to measure the \u2026"}, {"title": "Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents", "link": "https://arxiv.org/pdf/2409.17140", "details": "J Lu, Z Zhang, F Yang, J Zhang, L Wang, C Du, Q Lin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal large language models (MLLMs) have enabled LLM-based agents to directly interact with application user interfaces (UIs), enhancing agents' performance in complex tasks. However, these agents often suffer from high latency and low \u2026"}, {"title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE", "link": "https://arxiv.org/pdf/2409.17508", "details": "X Zhu, Y Hu, F Mo, M Li, J Wu - arXiv preprint arXiv:2409.17508, 2024", "abstract": "Multi-modal large language models (MLLMs) have shown impressive capabilities as a general-purpose interface for various visual and linguistic tasks. However, building a unified MLLM for multi-task learning in the medical field remains a thorny \u2026"}, {"title": "Probing Language Models on Their Knowledge Source", "link": "https://arxiv.org/pdf/2410.05817", "details": "Z Tighidet, A Mogini, J Mei, B Piwowarski, P Gallinari - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) often encounter conflicts between their learned, internal (parametric knowledge, PK) and external knowledge provided during inference (contextual knowledge, CK). Understanding how LLMs models prioritize \u2026"}, {"title": "Inference-Time Language Model Alignment via Integrated Value Guidance", "link": "https://arxiv.org/pdf/2409.17819%3F", "details": "Z Liu, Z Zhou, Y Wang, C Yang, Y Qiao - arXiv preprint arXiv:2409.17819, 2024", "abstract": "Large language models are typically fine-tuned to align with human preferences, but tuning large models is computationally intensive and complex. In this work, we introduce $\\textit {Integrated Value Guidance} $(IVG), a method that uses implicit and \u2026"}]
