[{"title": "Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation", "link": "https://arxiv.org/pdf/2505.19529", "details": "TH Sakib, MT Hosain, MK Morol - arXiv preprint arXiv:2505.19529, 2025", "abstract": "Small Language Models (SLMs) have gained substantial attention due to their ability to execute diverse language tasks successfully while using fewer computer resources. These models are particularly ideal for deployment in limited \u2026", "entry_id": "http://arxiv.org/abs/2505.19529v2", "updated": "2025-05-29 16:57:36", "published": "2025-05-26 05:29:47", "authors": "Tanjil Hasan Sakib;Md. Tanzib Hosain;Md. Kishor Morol", "summary": "Small Language Models (SLMs) have gained substantial attention due to their\nability to execute diverse language tasks successfully while using fewer\ncomputer resources. These models are particularly ideal for deployment in\nlimited environments, such as mobile devices, on-device processing, and edge\nsystems. In this study, we present a complete assessment of SLMs, focussing on\ntheir design frameworks, training approaches, and techniques for lowering model\nsize and complexity. We offer a novel classification system to organize the\noptimization approaches applied for SLMs, encompassing strategies like pruning,\nquantization, and model compression. Furthermore, we assemble SLM's studies of\nevaluation suite with some existing datasets, establishing a rigorous platform\nfor measuring SLM capabilities. Alongside this, we discuss the important\ndifficulties that remain unresolved in this sector, including trade-offs\nbetween efficiency and performance, and we suggest directions for future study.\nWe anticipate this study to serve as a beneficial guide for researchers and\npractitioners who aim to construct compact, efficient, and high-performing\nlanguage models.", "comment": "9 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.19529v2;http://arxiv.org/pdf/2505.19529v2", "pdf_url": "http://arxiv.org/pdf/2505.19529v2"}, {"title": "LLaFS++: Few-Shot Image Segmentation With Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/11015274/", "details": "L Zhu, T Chen, D Ji, P Xu, J Ye, J Liu - IEEE Transactions on Pattern Analysis and \u2026, 2025", "abstract": "Despite the rapid advancements in few-shot segmentation (FSS), most of existing methods in this domain are hampered by their reliance on the limited and biased information from only a small number of labeled samples. This limitation inherently \u2026"}, {"title": "Optimization-Inspired Few-Shot Adaptation for Large Language Models", "link": "https://arxiv.org/pdf/2505.19107", "details": "B Gao, X Wang, Y Yang, D Clifton - arXiv preprint arXiv:2505.19107, 2025", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in real- world applications. However, adapting LLMs to novel tasks via fine-tuning often requires substantial training data and computational resources that are impractical in \u2026", "entry_id": "http://arxiv.org/abs/2505.19107v1", "updated": "2025-05-25 11:54:23", "published": "2025-05-25 11:54:23", "authors": "Boyan Gao;Xin Wang;Yibo Yang;David Clifton", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nreal-world applications. However, adapting LLMs to novel tasks via fine-tuning\noften requires substantial training data and computational resources that are\nimpractical in few-shot scenarios. Existing approaches, such as in-context\nlearning and Parameter-Efficient Fine-Tuning (PEFT), face key limitations:\nin-context learning introduces additional inference computational overhead with\nlimited performance gains, while PEFT models are prone to overfitting on the\nfew demonstration examples. In this work, we reinterpret the forward pass of\nLLMs as an optimization process, a sequence of preconditioned gradient descent\nsteps refining internal representations. Based on this connection, we propose\nOptimization-Inspired Few-Shot Adaptation (OFA), integrating a parameterization\nthat learns preconditioners without introducing additional trainable\nparameters, and an objective that improves optimization efficiency by learning\npreconditioners based on a convergence bound, while simultaneously steering the\noptimization path toward the flat local minimum. Our method overcomes both\nissues of ICL-based and PEFT-based methods, and demonstrates superior\nperformance over the existing methods on a variety of few-shot adaptation tasks\nin experiments.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.19107v1;http://arxiv.org/pdf/2505.19107v1", "pdf_url": "http://arxiv.org/pdf/2505.19107v1"}, {"title": "PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language", "link": "https://arxiv.org/pdf/2505.18331", "details": "N Jamali, M Mohammadi, D Baledi, Z Rezvani, H Faili - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical consumer question answering (CQA) is crucial for empowering patients by providing personalized and reliable health information. Despite recent advances in large language models (LLMs) for medical QA, consumer-oriented and multilingual \u2026", "entry_id": "http://arxiv.org/abs/2505.18331v1", "updated": "2025-05-23 19:39:01", "published": "2025-05-23 19:39:01", "authors": "Naghmeh Jamali;Milad Mohammadi;Danial Baledi;Zahra Rezvani;Hesham Faili", "summary": "Medical consumer question answering (CQA) is crucial for empowering patients\nby providing personalized and reliable health information. Despite recent\nadvances in large language models (LLMs) for medical QA, consumer-oriented and\nmultilingual resources, particularly in low-resource languages like Persian,\nremain sparse. To bridge this gap, we present PerMedCQA, the first\nPersian-language benchmark for evaluating LLMs on real-world,\nconsumer-generated medical questions. Curated from a large medical QA forum,\nPerMedCQA contains 68,138 question-answer pairs, refined through careful data\ncleaning from an initial set of 87,780 raw entries. We evaluate several\nstate-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a\nnovel rubric-based evaluation framework driven by an LLM grader, validated\nagainst expert human annotators. Our results highlight key challenges in\nmultilingual medical QA and provide valuable insights for developing more\naccurate and context-aware medical assistance systems. The data is publicly\navailable on https://huggingface.co/datasets/NaghmehAI/PerMedCQA", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.18331v1;http://arxiv.org/pdf/2505.18331v1", "pdf_url": "http://arxiv.org/pdf/2505.18331v1"}, {"title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models", "link": "https://arxiv.org/pdf/2505.07558", "details": "R Higuchi, T Suzuki - arXiv preprint arXiv:2505.07558, 2025", "abstract": "Aligning large language models (LLMs) with human preferences is crucial for safe deployment, yet existing methods assume specific preference models like Bradley- Terry model. This assumption leads to statistical inconsistency, where more data \u2026", "entry_id": "http://arxiv.org/abs/2505.07558v2", "updated": "2025-05-19 20:28:26", "published": "2025-05-12 13:36:25", "authors": "Rei Higuchi;Taiji Suzuki", "summary": "Aligning large language models (LLMs) with human preferences is crucial for\nsafe deployment, yet existing methods assume specific preference models like\nBradley-Terry model. This assumption leads to statistical inconsistency, where\nmore data doesn't guarantee convergence to true human preferences. To address\nthis critical gap, we introduce a novel alignment method Direct Density Ratio\nOptimization (DDRO). DDRO directly estimates the density ratio between\npreferred and unpreferred output distributions, circumventing the need for\nexplicit human preference modeling. We theoretically prove that DDRO is\nstatistically consistent, ensuring convergence to the true preferred\ndistribution as the data size grows, regardless of the underlying preference\nstructure. Experiments demonstrate that DDRO achieves superior performance\ncompared to existing methods on many major benchmarks. DDRO unlocks the\npotential for truly data-driven alignment, paving the way for more reliable and\nhuman-aligned LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CL;stat.ML", "links": "http://arxiv.org/abs/2505.07558v2;http://arxiv.org/pdf/2505.07558v2", "pdf_url": "http://arxiv.org/pdf/2505.07558v2"}, {"title": "prompt4vis: prompting large language models with example mining for tabular data visualization", "link": "https://link.springer.com/article/10.1007/s00778-025-00912-0", "details": "S Li, X Chen, Y Song, Y Song, CJ Zhang, F Hao\u2026 - The VLDB Journal, 2025", "abstract": "We are currently in the epoch of Large Language Models (LLMs), which have transformed numerous technological domains within the database community. In this paper, we examine the application of LLMs in text-to-visualization (text-to-vis). The \u2026"}, {"title": "Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning", "link": "https://arxiv.org/pdf/2505.21354", "details": "B Paul, JJ Era, MR Zim, TS Aothoi, FM Shah - arXiv preprint arXiv:2505.21354, 2025", "abstract": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in natural language processing (NLP) due to the language's low-resource status and the multi- step reasoning required. Existing models struggle with complex Bengali MWPs \u2026", "entry_id": "http://arxiv.org/abs/2505.21354v1", "updated": "2025-05-27 15:47:10", "published": "2025-05-27 15:47:10", "authors": "Bidyarthi Paul;Jalisha Jashim Era;Mirazur Rahman Zim;Tahmid Sattar Aothoi;Faisal Muhammad Shah", "summary": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.21354v1;http://arxiv.org/pdf/2505.21354v1", "pdf_url": "http://arxiv.org/pdf/2505.21354v1"}, {"title": "SAS-Prompt: Large Language Models as Numerical Optimizers for Robot Self-Improvement", "link": "https://arxiv.org/pdf/2504.20459", "details": "HB Amor, L Graesser, A Iscen, D D'Ambrosio\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We demonstrate the ability of large language models (LLMs) to perform iterative self- improvement of robot policies. An important insight of this paper is that LLMs have a built-in ability to perform (stochastic) numerical optimization and that this property \u2026", "entry_id": "http://arxiv.org/abs/2504.20459v1", "updated": "2025-04-29 06:39:20", "published": "2025-04-29 06:39:20", "authors": "Heni Ben Amor;Laura Graesser;Atil Iscen;David D'Ambrosio;Saminda Abeyruwan;Alex Bewley;Yifan Zhou;Kamalesh Kalirathinam;Swaroop Mishra;Pannag Sanketi", "summary": "We demonstrate the ability of large language models (LLMs) to perform\niterative self-improvement of robot policies. An important insight of this\npaper is that LLMs have a built-in ability to perform (stochastic) numerical\noptimization and that this property can be leveraged for explainable robot\npolicy search. Based on this insight, we introduce the SAS Prompt (Summarize,\nAnalyze, Synthesize) -- a single prompt that enables iterative learning and\nadaptation of robot behavior by combining the LLM's ability to retrieve, reason\nand optimize over previous robot traces in order to synthesize new, unseen\nbehavior. Our approach can be regarded as an early example of a new family of\nexplainable policy search methods that are entirely implemented within an LLM.\nWe evaluate our approach both in simulation and on a real-robot table tennis\ntask. Project website: sites.google.com/asu.edu/sas-llm/", "comment": "ICRA 2025", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO", "links": "http://arxiv.org/abs/2504.20459v1;http://arxiv.org/pdf/2504.20459v1", "pdf_url": "http://arxiv.org/pdf/2504.20459v1"}, {"title": "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models", "link": "https://arxiv.org/pdf/2505.19743", "details": "Y Zhang, Y Yu, B Tang, Y Zhu, C Sun, W Wei, J Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the rapid development of Large Language Models (LLMs), aligning these models with human preferences and values is critical to ensuring ethical and safe applications. However, existing alignment techniques such as RLHF or DPO often \u2026", "entry_id": "http://arxiv.org/abs/2505.19743v2", "updated": "2025-05-27 04:07:01", "published": "2025-05-26 09:24:36", "authors": "Yang Zhang;Yu Yu;Bo Tang;Yu Zhu;Chuxiong Sun;Wenqiang Wei;Jie Hu;Zipeng Xie;Zhiyu Li;Feiyu Xiong;Edward Chung", "summary": "With the rapid development of Large Language Models (LLMs), aligning these\nmodels with human preferences and values is critical to ensuring ethical and\nsafe applications. However, existing alignment techniques such as RLHF or DPO\noften require direct fine-tuning on LLMs with billions of parameters, resulting\nin substantial computational costs and inefficiencies. To address this, we\npropose Micro token-level Accept-Reject Aligning (MARA) approach designed to\noperate independently of the language models. MARA simplifies the alignment\nprocess by decomposing sentence-level preference learning into token-level\nbinary classification, where a compact three-layer fully-connected network\ndetermines whether candidate tokens are \"Accepted\" or \"Rejected\" as part of the\nresponse. Extensive experiments across seven different LLMs and three\nopen-source datasets show that MARA achieves significant improvements in\nalignment performance while reducing computational costs. The source code and\nimplementation details are publicly available at\nhttps://github.com/IAAR-Shanghai/MARA, and the trained models are released at\nhttps://huggingface.co/IAAR-Shanghai/MARA_AGENTS.", "comment": "Accepted to 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.19743v2;http://arxiv.org/pdf/2505.19743v2", "pdf_url": "http://arxiv.org/pdf/2505.19743v2"}]
