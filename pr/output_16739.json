[{"title": "SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models", "link": "https://arxiv.org/pdf/2505.15094", "details": "J Yu, Y Tang, K Feng, M Rao, L Liang, Z Zhang, M Sun\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 In this work, we introduced SciCUEval, a comprehensive dataset for **evaluating** context understanding capabilities in **large** **language** **models** within scientific domains. SciCUEval encompasses multiple data modalities (structured tables \u2026", "entry_id": "http://arxiv.org/abs/2505.15094v1", "updated": "2025-05-21 04:33:26", "published": "2025-05-21 04:33:26", "authors": "Jing Yu;Yuqi Tang;Kehua Feng;Mingyang Rao;Lei Liang;Zhiqiang Zhang;Mengshu Sun;Wen Zhang;Qiang Zhang;Keyan Ding;Huajun Chen", "summary": "Large Language Models (LLMs) have shown impressive capabilities in contextual\nunderstanding and reasoning. However, evaluating their performance across\ndiverse scientific domains remains underexplored, as existing benchmarks\nprimarily focus on general domains and fail to capture the intricate complexity\nof scientific data. To bridge this gap, we construct SciCUEval, a comprehensive\nbenchmark dataset tailored to assess the scientific context understanding\ncapability of LLMs. It comprises ten domain-specific sub-datasets spanning\nbiology, chemistry, physics, biomedicine, and materials science, integrating\ndiverse data modalities including structured tables, knowledge graphs, and\nunstructured texts. SciCUEval systematically evaluates four core competencies:\nRelevant information identification, Information-absence detection,\nMulti-source information integration, and Context-aware inference, through a\nvariety of question formats. We conduct extensive evaluations of\nstate-of-the-art LLMs on SciCUEval, providing a fine-grained analysis of their\nstrengths and limitations in scientific context understanding, and offering\nvaluable insights for the future development of scientific-domain LLMs.", "comment": "25 pages, 4 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15094v1;http://arxiv.org/pdf/2505.15094v1", "pdf_url": "http://arxiv.org/pdf/2505.15094v1"}, {"title": "Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought", "link": "https://arxiv.org/pdf/2505.15431", "details": "A Liu, B Zhou, C Xu, C Zhou, CC Zhang, C Xu, C Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **evaluations** demonstrate the competitive capabilities of Hunyuan-TurboS. In the LMSys Chatbot Arena, known for its blind, side-by-side human **evaluations** that \u2026 For comprehensive automated assessment, we **evaluated** HunyuanTurboS across \u2026", "entry_id": "http://arxiv.org/abs/2505.15431v2", "updated": "2025-05-22 06:44:25", "published": "2025-05-21 12:11:53", "authors": "Tencent Hunyuan Team;Ao Liu;Botong Zhou;Can Xu;Chayse Zhou;ChenChen Zhang;Chengcheng Xu;Chenhao Wang;Decheng Wu;Dengpeng Wu;Dian Jiao;Dong Du;Dong Wang;Feng Zhang;Fengzong Lian;Guanghui Xu;Guanwei Zhang;Hai Wang;Haipeng Luo;Han Hu;Huilin Xu;Jiajia Wu;Jianchen Zhu;Jianfeng Yan;Jiaqi Zhu;Jihong Zhang;Jinbao Xue;Jun Xia;Junqiang Zheng;Kai Liu;Kai Zhang;Kai Zheng;Kejiao Li;Keyao Wang;Lan Jiang;Lixin Liu;Lulu Wu;Mengyuan Huang;Peijie Yu;Peiqi Wang;Qian Wang;Qianbiao Xiang;Qibin Liu;Qingfeng Sun;Richard Guo;Ruobing Xie;Saiyong Yang;Shaohua Chen;Shihui Hu;Shuai Li;Shuaipeng Li;Shuang Chen;Suncong Zheng;Tao Yang;Tian Zhang;Tinghao Yu;Weidong Han;Weijie Liu;Weijin Zhou;Weikang Wang;Wesleye Chen;Xiao Feng;Xiaoqin Ren;Xingwu Sun;Xiong Kuang;Xuemeng Huang;Xun Cao;Yanfeng Chen;Yang Du;Yang Zhen;Yangyu Tao;Yaping Deng;Yi Shen;Yigeng Hong;Yiqi Chen;Yiqing Huang;Yuchi Deng;Yue Mao;Yulong Wang;Yuyuan Zeng;Zenan Xu;Zhanhui Kang;Zhe Zhao;ZhenXiang Yan;Zheng Fang;Zhichao Hu;Zhongzhi Chen;Zhuoyu Li;Zongwei Li;Alex Yan;Ande Liang;Baitong Liu;Beiping Pan;Bin Xing;Binghong Wu;Bingxin Qu;Bolin Ni;Boyu Wu;Chen Li;Cheng Jiang;Cheng Zhang;Chengjun Liu;Chengxu Yang;Chengzhong Xu;Chiyu Wang;Chong Zha;Daisy Yi;Di Wang;Fanyang Lu;Fei Chen;Feifei Liu;Feng Zheng;Guanghua Yu;Guiyang Li;Guohua Wang;Haisheng Lin;Han Liu;Han Wang;Hao Fei;Hao Lu;Haoqing Jiang;Haoran Sun;Haotian Zhu;Huangjin Dai;Huankui Chen;Huawen Feng;Huihui Cai;Huxin Peng;Jackson Lv;Jiacheng Shi;Jiahao Bu;Jianbo Li;Jianglu Hu;Jiangtao Guan;Jianing Xu;Jianwei Cai;Jiarong Zhang;Jiawei Song;Jie Jiang;Jie Liu;Jieneng Yang;Jihong Zhang;Jin lv;Jing Zhao;Jinjian Li;Jinxing Liu;Jun Zhao;Juntao Guo;Kai Wang;Kan Wu;Lei Fu;Lei He;Lei Wang;Li Liu;Liang Dong;Liya Zhan;Long Cheng;Long Xu;Mao Zheng;Meng Liu;Mengkang Hu;Nanli Chen;Peirui Chen;Peng He;Pengju Pan;Pengzhi Wei;Qi Yang;Qi Yi;Roberts Wang;Rongpeng Chen;Rui Sun;Rui Yang;Ruibin Chen;Ruixu Zhou;Shaofeng Zhang;Sheng Zhang;Shihao Xu;Shuaishuai Chang;Shulin Liu;SiQi Wang;Songjia Feng;Songling Yuan;Tao Zhang;Tianjiao Lang;Tongkai Li;Wei Deng;Wei Li;Weichao Wang;Weigang Zhang;Weixuan Sun;Wen Ouyang;Wenxiang Jiao;Wenzhi Sun;Wenzhuo Jia;Xiang Zhang;Xiangyu He;Xianshun Ren;XiaoYing Zhu;Xiaolong Guo;Xiaoxue Li;Xiaoyu Ma;Xican Lu;Xinhua Feng;Xinting Huang;Xinyu Guan;Xirui Li;Xu Zhang;Xudong Gao;Xun Luo;Xuxiang Qi;Yangkun Chen;Yangyu Tao;Yanling Xiao;Yantao Mai;Yanze Chen;Yao Ding;Yeting Yang;YiFan Song;Yifan Yang;Yijiao Zhu;Yinhe Wu;Yixian Liu;Yong Yang;Yuanjun Cai;Yuanlin Tu;Yue Zhang;Yufei Huang;Yuhang Zhou;Yuhao Jiang;Yuhong Liu;Yuhui Hu;Yujin Lin;Yun Yang;Yunhao Wang;Yusong Zhang;Zekun Wu;Zelong Zhang;Zhan Yu;Zhaoliang Yang;Zhe Zhao;Zheng Li;Zhenyu Huang;Zhiguang Liu;Zhijiang Xu;Zhiqing Kui;Zhiyin Zeng;Zhiyuan Xiong;Zhuo Han;Zifan Wu;Zigang Geng;Zilong Zhao;Ziyan Tang;Ziyuan Zhu;Zonglei Zhu;Zhijiang Xu", "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15431v2;http://arxiv.org/pdf/2505.15431v2", "pdf_url": "http://arxiv.org/pdf/2505.15431v2"}, {"title": " **Evaluating Large Language Models** on ABA-Style Anesthesiology Questions: Accuracy, Domain Consistency, and Clinical Implications", "link": "https://www.sciencedirect.com/science/article/pii/S1053077025004239", "details": "S Patel, V Ngo, B Wilhelmi - Journal of Cardiothoracic and Vascular Anesthesia, 2025", "abstract": "\u2026 This study demonstrates that **large** **language** **models** can consistently surpass the passing threshold for the American Board of Anesthesiology Basic Exam, with no significant performance variation across major knowledge domains. These results \u2026"}, {"title": "LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models", "link": "https://arxiv.org/pdf/2505.15475", "details": "Z Qin, Y Ding, D Liu, Q Liu, J Cai, X Chen, Z Tu, D Chu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Nowadays, **Large** **Language** **Models** (LLMs) have attracted widespread attention due to their powerful performance. However, due to the \u2026 with the accompanying **evaluation** metric AFGB-Score. \u2022 We propose GenHintEval dataset, which is used to \u2026", "entry_id": "http://arxiv.org/abs/2505.15475v1", "updated": "2025-05-21 12:49:37", "published": "2025-05-21 12:49:37", "authors": "Zhanyue Qin;Yue Ding;Deyuan Liu;Qingbin Liu;Junxian Cai;Xi Chen;Zhiying Tu;Dianhui Chu;Cuiyun Gao;Dianbo Sui", "summary": "Nowadays, Large Language Models (LLMs) have attracted widespread attention\ndue to their powerful performance. However, due to the unavoidable exposure to\nsocially biased data during training, LLMs tend to exhibit social biases,\nparticularly gender bias. To better explore and quantifying the degree of\ngender bias in LLMs, we propose a pair of datasets named GenBiasEval and\nGenHintEval, respectively. The GenBiasEval is responsible for evaluating the\ndegree of gender bias in LLMs, accompanied by an evaluation metric named\nAFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is\nused to assess whether LLMs can provide responses consistent with prompts that\ncontain gender hints, along with the accompanying evaluation metric UB-Score\n(UnBias Score). Besides, in order to mitigate gender bias in LLMs more\neffectively, we present the LFTF (Locating First and Then Fine-Tuning)\nalgorithm.The algorithm first ranks specific LLM blocks by their relevance to\ngender bias in descending order using a metric called BMI (Block Mitigating\nImportance Score). Based on this ranking, the block most strongly associated\nwith gender bias is then fine-tuned using a carefully designed loss function.\nNumerous experiments have shown that our proposed LFTF algorithm can\nsignificantly mitigate gender bias in LLMs while maintaining their general\ncapabilities.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.15475v1;http://arxiv.org/pdf/2505.15475v1", "pdf_url": "http://arxiv.org/pdf/2505.15475v1"}, {"title": "Harnessing On-Device Large Language Model: Empirical Results and Implications for AI PC", "link": "https://arxiv.org/pdf/2505.15030", "details": "Q Song, P Liao, W Zhao, Y Wang, S Hu, HL Zhen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 We adopt lm- **evaluation** -harness [23] (version 0.4.8) as the **evaluation** tool to align with Open LLM Leaderboard so that all our **evaluations** and \u2026 Given the incomplete native support of lm- **evaluation** -harness (version 0.4.8) for llama.cpp, we develop a \u2026", "entry_id": "http://arxiv.org/abs/2505.15030v2", "updated": "2025-05-22 01:26:36", "published": "2025-05-21 02:23:01", "authors": "Qingyu Song;Peiyu Liao;Wenqian Zhao;Yiwen Wang;Shoubo Hu;Hui-Ling Zhen;Ning Jiang;Mingxuan Yuan", "summary": "The increasing deployment of Large Language Models (LLMs) on edge devices,\ndriven by model advancements and hardware improvements, offers significant\nprivacy benefits. However, these on-device LLMs inherently face performance\nlimitations due to reduced model capacity and necessary compression techniques.\nTo address this, we introduce a systematic methodology -- encompassing model\ncapability, development efficiency, and system resources -- for evaluating\non-device LLMs. Our comprehensive evaluation, encompassing models from 0.5B to\n14B parameters and seven post-training quantization (PTQ) methods on commodity\nlaptops, yields several critical insights: 1) System-level metrics exhibit\nnear-linear scaling with effective bits-per-weight (BPW). 2) A practical\nthreshold exists around $\\sim$3.5 effective BPW, larger models subjected to\nlow-bit quantization consistently outperform smaller models utilizing higher\nbit-precision. 3) Quantization with low BPW incurs marginal accuracy loss but\nsignificant memory savings. 4) Determined by low-level implementation specifics\npower consumption on CPU, where computation-intensive operations spend more\npower than memory-intensive ones. These findings offer crucial insights and\npractical guidelines for the efficient deployment and optimized configuration\nof LLMs on resource-constrained edge devices. Our codebase is available at\nhttps://github.com/simmonssong/LLMOnDevice.", "comment": "18 pages, 14 figures", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.15030v2;http://arxiv.org/pdf/2505.15030v2", "pdf_url": "http://arxiv.org/pdf/2505.15030v2"}, {"title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models", "link": "https://arxiv.org/pdf/2505.15801", "details": "Y Yan, J Jiang, Z Ren, Y Li, X Cai, Y Liu, X Xu, M Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Reference-free Reward Models In reinforcement learning (RL) for **large** **language** **models** (LLMs), the reward model plays a crucial role by approximating real-\u2026 This section presents the **evaluation** results and analyses of our proposed benchmark \u2026", "entry_id": "http://arxiv.org/abs/2505.15801v1", "updated": "2025-05-21 17:54:43", "published": "2025-05-21 17:54:43", "authors": "Yuchen Yan;Jin Jiang;Zhenbang Ren;Yijun Li;Xudong Cai;Yang Liu;Xin Xu;Mengdi Zhang;Jian Shao;Yongliang Shen;Jun Xiao;Yueting Zhuang", "summary": "Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved\nremarkable performance in the domain of reasoning. A key component of their\ntraining is the incorporation of verifiable rewards within reinforcement\nlearning (RL). However, existing reward benchmarks do not evaluate\nreference-based reward systems, leaving researchers with limited understanding\nof the accuracy of verifiers used in RL. In this paper, we introduce two\nbenchmarks, VerifyBench and VerifyBench-Hard, designed to assess the\nperformance of reference-based reward systems. These benchmarks are constructed\nthrough meticulous data collection and curation, followed by careful human\nannotation to ensure high quality. Current models still show considerable room\nfor improvement on both VerifyBench and VerifyBench-Hard, especially\nsmaller-scale models. Furthermore, we conduct a thorough and comprehensive\nanalysis of evaluation results, offering insights for understanding and\ndeveloping reference-based reward systems. Our proposed benchmarks serve as\neffective tools for guiding the development of verifier accuracy and the\nreasoning capabilities of models trained via RL in reasoning tasks.", "comment": "Dataset: https://huggingface.co/datasets/ZJU-REAL/VerifyBench", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.15801v1;http://arxiv.org/pdf/2505.15801v1", "pdf_url": "http://arxiv.org/pdf/2505.15801v1"}, {"title": "Retrieval-augmented generation for answering Breast Imaging Reporting and Data System (BI-RADS)-related questions with **large language models**", "link": "https://www.dirjournal.org/pdf/beb8919b-f013-4ea1-b1c8-40332e840fe1/articles/dir.2025.253272/DIR-2025.253272.pdf", "details": "E Kaba - Diagn Interv Radiol, 2025", "abstract": "\u2026 titled \u201c **Evaluating** text and visual diagnostic capabilities of **large** **language** **models** on questions related to the Breast Imaging Reporting and Data System Atlas 5th edition\u201d published in Diagnostic and Interventional Radiology.The study explores \u2026"}, {"title": "Can Large Language Models be Effective Online Opinion Miners?", "link": "https://arxiv.org/pdf/2505.15695", "details": "R Heo, Y Seo, J Lee, D Lee - arXiv preprint arXiv:2505.15695, 2025", "abstract": "\u2026 dataset and evaluation protocol designed to assess the ability of **large** **language** **models** (LLMs) to mine opinions effectively from diverse \u2026 extent **large** **language** **models** (LLMs) can effectively perform opinion mining. This gap poses a significant \u2026", "entry_id": "http://arxiv.org/abs/2505.15695v1", "updated": "2025-05-21 16:09:44", "published": "2025-05-21 16:09:44", "authors": "Ryang Heo;Yongsik Seo;Junseong Lee;Dongha Lee", "summary": "The surge of user-generated online content presents a wealth of insights into\ncustomer preferences and market trends. However, the highly diverse, complex,\nand context-rich nature of such contents poses significant challenges to\ntraditional opinion mining approaches. To address this, we introduce Online\nOpinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol\ndesigned to assess the ability of large language models (LLMs) to mine opinions\neffectively from diverse and intricate online environments. OOMB provides\nextensive (entity, feature, opinion) tuple annotations and a comprehensive\nopinion-centric summary that highlights key opinion topics within each content,\nthereby enabling the evaluation of both the extractive and abstractive\ncapabilities of models. Through our proposed benchmark, we conduct a\ncomprehensive analysis of which aspects remain challenging and where LLMs\nexhibit adaptability, to explore whether they can effectively serve as opinion\nminers in realistic online scenarios. This study lays the foundation for\nLLM-based opinion mining and discusses directions for future research in this\nfield.", "comment": "8 pages, 6 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15695v1;http://arxiv.org/pdf/2505.15695v1", "pdf_url": "http://arxiv.org/pdf/2505.15695v1"}, {"title": "Large Language Models for Data Synthesis", "link": "https://arxiv.org/pdf/2505.14752", "details": "Y Tang, M Kong, L Sun - arXiv preprint arXiv:2505.14752, 2025", "abstract": "\u2026 Recent progress in **Large** **Language** **Models** (LLMs) reveals their potential as flexible, high-dimensional priors over real-world distributions. \u2026 We **evaluate** LLMSYNTHOR in both controlled and real-world settings using heterogeneous \u2026", "entry_id": "http://arxiv.org/abs/2505.14752v1", "updated": "2025-05-20 13:35:38", "published": "2025-05-20 13:35:38", "authors": "Yihong Tang;Menglin Kong;Lijun Sun", "summary": "Generating synthetic data that faithfully captures the statistical structure\nof real-world distributions is a fundamental challenge in data modeling.\nClassical approaches often depend on strong parametric assumptions or manual\nstructural design and struggle in high-dimensional or heterogeneous domains.\nRecent progress in Large Language Models (LLMs) reveals their potential as\nflexible, high-dimensional priors over real-world distributions. However, when\napplied to data synthesis, standard LLM-based sampling is inefficient,\nconstrained by fixed context limits, and fails to ensure statistical alignment.\nGiven this, we introduce LLMSynthor, a general framework for data synthesis\nthat transforms LLMs into structure-aware simulators guided by distributional\nfeedback. LLMSynthor treats the LLM as a nonparametric copula simulator for\nmodeling high-order dependencies and introduces LLM Proposal Sampling to\ngenerate grounded proposal distributions that improve sampling efficiency\nwithout requiring rejection. By minimizing discrepancies in the summary\nstatistics space, the iterative synthesis loop aligns real and synthetic data\nwhile gradually uncovering and refining the latent generative structure. We\nevaluate LLMSynthor in both controlled and real-world settings using\nheterogeneous datasets in privacy-sensitive domains (e.g., e-commerce,\npopulation, and mobility) that encompass both structured and unstructured\nformats. The synthetic data produced by LLMSynthor shows high statistical\nfidelity, practical utility, and cross-data adaptability, positioning it as a\nvaluable tool across economics, social science, urban studies, and beyond.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.14752v1;http://arxiv.org/pdf/2505.14752v1", "pdf_url": "http://arxiv.org/pdf/2505.14752v1"}]
