[{"title": "MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts", "link": "https://arxiv.org/pdf/2407.21770", "details": "XV Lin, A Shrivastava, L Luo, S Iyer, M Lewis, G Gosh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce MoMa, a novel modality-aware mixture-of-experts (MoE) architecture designed for pre-training mixed-modal, early-fusion language models. MoMa processes images and text in arbitrary sequences by dividing expert modules into \u2026"}, {"title": "GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction", "link": "https://arxiv.org/pdf/2407.21384", "details": "Y Mao, P Liu, T Cui - arXiv preprint arXiv:2407.21384, 2024", "abstract": "Document-level relation extraction (DocRE) aims to extract relations between entities from unstructured document text. Compared to sentence-level relation extraction, it requires more complex semantic understanding from a broader text context \u2026"}, {"title": "Overview of# SMM4H 2024\u2013Task 2: Cross-Lingual Few-Shot Relation Extraction for Pharmacovigilance in French, German, and Japanese", "link": "https://aclanthology.org/2024.smm4h-1.39.pdf", "details": "L Raithel, P Thomas, B Verma, R Roller, HS Yeh\u2026 - Proceedings of The 9th \u2026, 2024", "abstract": "This paper provides an overview of Task 2 from the Social Media Mining for Health 2024 shared task (# SMM4H 2024), which focused on Named Entity Recognition (NER, Subtask 2a) and the joint task of NER and Relation Extraction (RE, Subtask \u2026"}, {"title": "XLIT: A Method to Bridge Task Discrepancy in Machine Translation Pre-training", "link": "https://dl.acm.org/doi/pdf/10.1145/3689630", "details": "K Pham, L Nguyen, D Dinh - ACM Transactions on Asian and Low-Resource \u2026, 2024", "abstract": "Transfer learning from pre-trained language models to encoder-decoder translation models faces a challenge due to the mismatch between the tasks of pre-training and fine-tuning. Pre-trained models are not explicitly trained to understand the semantic \u2026"}, {"title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism", "link": "https://arxiv.org/pdf/2408.10473", "details": "G Li, X Zhao, L Liu, Z Li, D Li, L Tian, J He, A Sirasao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational \u2026"}, {"title": "Masked autoencoder: influence of self-supervised pretraining on object segmentation in industrial images", "link": "https://link.springer.com/article/10.1007/s44244-024-00020-y", "details": "A Witte, S Lange, C Lins - Industrial Artificial Intelligence, 2024", "abstract": "The amount of labelled data in industrial use cases is limited because the annotation process is time-consuming and costly. As in research, self-supervised pretraining such as MAE resulted in training segmentation models with fewer labels, this is also \u2026"}]
