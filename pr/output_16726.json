[{"title": "Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE", "link": "https://arxiv.org/pdf/2505.12533", "details": "V Arzt, A Hanbury, M Wiegand, G Recski, T Blevins - arXiv preprint arXiv:2505.12533, 2025", "abstract": "Analysing the generalisation capabilities of relation extraction (RE) models is crucial for assessing whether they learn robust relational patterns or rely on spurious correlations. Our cross-dataset experiments find that RE models struggle with \u2026", "entry_id": "http://arxiv.org/abs/2505.12533v1", "updated": "2025-05-18 20:22:14", "published": "2025-05-18 20:22:14", "authors": "Varvara Arzt;Allan Hanbury;Michael Wiegand;G\u00e1bor Recski;Terra Blevins", "summary": "Analysing the generalisation capabilities of relation extraction (RE) models\nis crucial for assessing whether they learn robust relational patterns or rely\non spurious correlations. Our cross-dataset experiments find that RE models\nstruggle with unseen data, even within similar domains. Notably, higher\nintra-dataset performance does not indicate better transferability, instead\noften signaling overfitting to dataset-specific artefacts. Our results also\nshow that data quality, rather than lexical similarity, is key to robust\ntransfer, and the choice of optimal adaptation strategy depends on the quality\nof data available: while fine-tuning yields the best cross-dataset performance\nwith high-quality data, few-shot in-context learning (ICL) is more effective\nwith noisier data. However, even in these cases, zero-shot baselines\noccasionally outperform all cross-dataset results. Structural issues in RE\nbenchmarks, such as single-relation per sample constraints and non-standardised\nnegative class definitions, further hinder model transferability.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.12533v1;http://arxiv.org/pdf/2505.12533v1", "pdf_url": "http://arxiv.org/pdf/2505.12533v1"}, {"title": "AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation", "link": "https://arxiv.org/pdf/2505.11887", "details": "X Zhang, Z Ouyang, L Wang, G de Melo, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the proliferation of large language models (LLMs) in the medical domain, there is increasing demand for improved evaluation techniques to assess their capabilities. However, traditional metrics like F1 and ROUGE, which rely on token overlaps to \u2026", "entry_id": "http://arxiv.org/abs/2505.11887v1", "updated": "2025-05-17 07:44:54", "published": "2025-05-17 07:44:54", "authors": "Xiechi Zhang;Zetian Ouyang;Linlin Wang;Gerard de Melo;Zhu Cao;Xiaoling Wang;Ya Zhang;Yanfeng Wang;Liang He", "summary": "With the proliferation of large language models (LLMs) in the medical domain,\nthere is increasing demand for improved evaluation techniques to assess their\ncapabilities. However, traditional metrics like F1 and ROUGE, which rely on\ntoken overlaps to measure quality, significantly overlook the importance of\nmedical terminology. While human evaluation tends to be more reliable, it can\nbe very costly and may as well suffer from inaccuracies due to limits in human\nexpertise and motivation. Although there are some evaluation methods based on\nLLMs, their usability in the medical field is limited due to their proprietary\nnature or lack of expertise. To tackle these challenges, we present\nAutoMedEval, an open-sourced automatic evaluation model with 13B parameters\nspecifically engineered to measure the question-answering proficiency of\nmedical LLMs. The overarching objective of AutoMedEval is to assess the quality\nof responses produced by diverse models, aspiring to significantly reduce the\ndependence on human evaluation. Specifically, we propose a hierarchical\ntraining method involving curriculum instruction tuning and an iterative\nknowledge introspection mechanism, enabling AutoMedEval to acquire professional\nmedical assessment capabilities with limited instructional data. Human\nevaluations indicate that AutoMedEval surpasses other baselines in terms of\ncorrelation with human judgments.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.11887v1;http://arxiv.org/pdf/2505.11887v1", "pdf_url": "http://arxiv.org/pdf/2505.11887v1"}, {"title": "COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3717867.3717923", "details": "P Govil, H Jain, V Bonagiri, A Chadha, P Kumaraguru\u2026 - Proceedings of the 17th \u2026, 2025", "abstract": "Large Language Models (LLMs) often inherit biases from the web data they are trained on, which contains stereotypes and prejudices. Current methods for evaluating and mitigating these biases rely on bias-benchmark datasets. These \u2026"}, {"title": "Search-Based Correction of Reasoning Chains for Language Models", "link": "https://arxiv.org/pdf/2505.11824", "details": "M Kim, JP Falet, OE Richardson, X Chen, M Jain\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Chain-of-Thought (CoT) reasoning has advanced the capabilities and transparency of language models (LMs); however, reasoning chains can contain inaccurate statements that reduce performance and trustworthiness. To address this, we \u2026", "entry_id": "http://arxiv.org/abs/2505.11824v1", "updated": "2025-05-17 04:16:36", "published": "2025-05-17 04:16:36", "authors": "Minsu Kim;Jean-Pierre Falet;Oliver E. Richardson;Xiaoyin Chen;Moksh Jain;Sungjin Ahn;Sungsoo Ahn;Yoshua Bengio", "summary": "Chain-of-Thought (CoT) reasoning has advanced the capabilities and\ntransparency of language models (LMs); however, reasoning chains can contain\ninaccurate statements that reduce performance and trustworthiness. To address\nthis, we introduce a new self-correction framework that augments each reasoning\nstep in a CoT with a latent variable indicating its veracity, enabling modeling\nof all possible truth assignments rather than assuming correctness throughout.\nTo efficiently explore this expanded space, we introduce Search Corrector, a\ndiscrete search algorithm over boolean-valued veracity assignments. It\nefficiently performs otherwise intractable inference in the posterior\ndistribution over veracity assignments by leveraging the LM's joint likelihood\nover veracity and the final answer as a proxy reward. This efficient\ninference-time correction method facilitates supervised fine-tuning of an\nAmortized Corrector by providing pseudo-labels for veracity. The Amortized\nCorrector generalizes self-correction, enabling accurate zero-shot veracity\ninference in novel contexts. Empirical results demonstrate that Search\nCorrector reliably identifies errors in logical (ProntoQA) and mathematical\nreasoning (GSM8K) benchmarks. The Amortized Corrector achieves comparable\nzero-shot accuracy and improves final answer accuracy by up to 25%.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.11824v1;http://arxiv.org/pdf/2505.11824v1", "pdf_url": "http://arxiv.org/pdf/2505.11824v1"}, {"title": "Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency", "link": "https://arxiv.org/pdf/2505.14309", "details": "E Doostmohammadi, M Kuhlmann - arXiv preprint arXiv:2505.14309, 2025", "abstract": "Retrieval-augmented language models have demonstrated performance comparable to much larger models while requiring fewer computational resources. The effectiveness of these models crucially depends on the overlap between query \u2026", "entry_id": "http://arxiv.org/abs/2505.14309v1", "updated": "2025-05-20 12:58:07", "published": "2025-05-20 12:58:07", "authors": "Ehsan Doostmohammadi;Marco Kuhlmann", "summary": "Retrieval-augmented language models have demonstrated performance comparable\nto much larger models while requiring fewer computational resources. The\neffectiveness of these models crucially depends on the overlap between query\nand retrieved context, but the optimal degree of this overlap remains\nunexplored. In this paper, we systematically investigate how varying levels of\nquery--context overlap affect model performance during both training and\ninference. Our experiments reveal that increased overlap initially has minimal\neffect, but substantially improves test-time perplexity and accelerates model\nlearning above a critical threshold. Building on these findings, we demonstrate\nthat deliberately increasing overlap through synthetic context can enhance data\nefficiency and reduce training time by approximately 40\\% without compromising\nperformance. We specifically generate synthetic context through paraphrasing\nqueries. We validate our perplexity-based findings on question-answering tasks,\nconfirming that the benefits of retrieval-augmented language modeling extend to\npractical applications. Our results provide empirical evidence of significant\noptimization potential for retrieval mechanisms in language model pretraining.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.14309v1;http://arxiv.org/pdf/2505.14309v1", "pdf_url": "http://arxiv.org/pdf/2505.14309v1"}, {"title": "0463 LLM-Driven Classification of Multidimensional Sleep Health Mentions from Free-Text Clinical Notes", "link": "https://academic.oup.com/sleep/article-abstract/48/Supplement_1/A202/8135949", "details": "SA Hussain, A Calloway, J Sirrianni, E Fosler-Lussier\u2026 - Sleep, 2025", "abstract": "Abstract Introduction Large language models (LLMs) provide an opportunity for high- throughput extraction of multidimensional sleep health (MSH) information from unstructured clinical text to support cohort identification and the development of \u2026"}, {"title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "link": "https://arxiv.org/pdf/2505.12983", "details": "J Wang, F Meng, Z Sun, Y Liang, Y Cao, J Xu, H Shi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Many-to-many summarization (M2MS) aims to process documents in any language and generate the corresponding summaries also in any language. Recently, large language models (LLMs) have shown strong multi-lingual abilities, giving them the \u2026", "entry_id": "http://arxiv.org/abs/2505.12983v1", "updated": "2025-05-19 11:18:54", "published": "2025-05-19 11:18:54", "authors": "Jiaan Wang;Fandong Meng;Zengkui Sun;Yunlong Liang;Yuxuan Cao;Jiarong Xu;Haoxiang Shi;Jie Zhou", "summary": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch.", "comment": "Accepted to ACL 2025 main conference", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.12983v1;http://arxiv.org/pdf/2505.12983v1", "pdf_url": "http://arxiv.org/pdf/2505.12983v1"}, {"title": "What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization", "link": "https://arxiv.org/pdf/2505.12474", "details": "W Zhou, J Zhu, G Li, X Cheng, X Liang, F Zhai, Z Li - arXiv preprint arXiv:2505.12474, 2025", "abstract": "In this work, we investigate the performance of LLMs on a new task that requires combining discussion with background knowledge for summarization. This aims to address the limitation of outside observer confusion in existing dialogue \u2026", "entry_id": "http://arxiv.org/abs/2505.12474v1", "updated": "2025-05-18 15:52:24", "published": "2025-05-18 15:52:24", "authors": "Weixiao Zhou;Junnan Zhu;Gengyao Li;Xianfu Cheng;Xinnian Liang;Feifei Zhai;Zhoujun Li", "summary": "In this work, we investigate the performance of LLMs on a new task that\nrequires combining discussion with background knowledge for summarization. This\naims to address the limitation of outside observer confusion in existing\ndialogue summarization systems due to their reliance solely on discussion\ninformation. To achieve this, we model the task output as background and\nopinion summaries and define two standardized summarization patterns. To\nsupport assessment, we introduce the first benchmark comprising high-quality\nsamples consistently annotated by human experts and propose a novel\nhierarchical evaluation framework with fine-grained, interpretable metrics. We\nevaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our\nfindings reveal: (1) LLMs struggle with background summary retrieval,\ngeneration, and opinion summary integration. (2) Even top LLMs achieve less\nthan 69% average performance across both patterns. (3) Current LLMs lack\nadequate self-evaluation and self-correction capabilities for this task.", "comment": "Submitted to EMNLP 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.12474v1;http://arxiv.org/pdf/2505.12474v1", "pdf_url": "http://arxiv.org/pdf/2505.12474v1"}, {"title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs", "link": "https://arxiv.org/pdf/2505.13173", "details": "V Akavarapu, H Terdalkar, P Bhattacharyya, S Agarwal\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across diverse tasks and languages. In this study, we focus on natural language understanding in three classical languages--Sanskrit, Ancient Greek and \u2026", "entry_id": "http://arxiv.org/abs/2505.13173v1", "updated": "2025-05-19 14:30:10", "published": "2025-05-19 14:30:10", "authors": "V. S. D. S. Mahesh Akavarapu;Hrishikesh Terdalkar;Pramit Bhattacharyya;Shubhangi Agarwal;Vishakha Deulgaonkar;Pralay Manna;Chaitali Dangarikar;Arnab Bhattacharya", "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies.", "comment": "Accepted to ACL 2025 Findings", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;I.2.7", "links": "http://arxiv.org/abs/2505.13173v1;http://arxiv.org/pdf/2505.13173v1", "pdf_url": "http://arxiv.org/pdf/2505.13173v1"}]
