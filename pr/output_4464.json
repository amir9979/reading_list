[{"title": "How do you know that? Teaching Generative Language Models to Reference Answers to Biomedical Questions", "link": "https://arxiv.org/pdf/2407.05015", "details": "B Ba\u0161aragin, A Ljaji\u0107, D Medvecki, L Cassano\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have recently become the leading source of answers for users' questions online. Despite their ability to offer eloquent answers, their accuracy and reliability can pose a significant challenge. This is especially true for \u2026"}, {"title": "Automated Ensemble Multimodal Machine Learning for Healthcare", "link": "https://arxiv.org/pdf/2407.18227", "details": "F Imrie, S Denner, LS Brunschwig, K Maier-Hein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The application of machine learning in medicine and healthcare has led to the creation of numerous diagnostic and prognostic models. However, despite their success, current approaches generally issue predictions using data from a single \u2026"}, {"title": "GCON: Differentially Private Graph Convolutional Network via Objective Perturbation", "link": "https://arxiv.org/pdf/2407.05034", "details": "J Wei, Y Zhu, X Xiao, E Bao, Y Yang, K Cai, BC Ooi - arXiv preprint arXiv:2407.05034, 2024", "abstract": "Graph Convolutional Networks (GCNs) are a popular machine learning model with a wide range of applications in graph analytics, including healthcare, transportation, and finance. Similar to other neural networks, a GCN may memorize parts of the \u2026"}, {"title": "How Chinese are Chinese Language Models? The Puzzling Lack of Language Policy in China's LLMs", "link": "https://arxiv.org/pdf/2407.09652", "details": "AW Wen-Yi, UES Jo, LJ Lin, D Mimno - arXiv preprint arXiv:2407.09652, 2024", "abstract": "Contemporary language models are increasingly multilingual, but Chinese LLM developers must navigate complex political and business considerations of language diversity. Language policy in China aims at influencing the public \u2026"}, {"title": "EVLM: An Efficient Vision-Language Model for Visual Understanding", "link": "https://arxiv.org/pdf/2407.14177", "details": "K Chen, D Shen, H Zhong, H Zhong, K Xia, D Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the field of multi-modal language models, the majority of methods are built on an architecture similar to LLaVA. These models use a single-layer ViT feature as a visual prompt, directly feeding it into the language models alongside textual tokens \u2026"}, {"title": "DDK: Distilling Domain Knowledge for Efficient Large Language Models", "link": "https://arxiv.org/pdf/2407.16154", "details": "J Liu, C Zhang, J Guo, Y Zhang, H Que, K Deng, Z Bai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the advanced intelligence abilities of large language models (LLMs) in various applications, they still face significant computational and storage demands. Knowledge Distillation (KD) has emerged as an effective strategy to improve the \u2026"}, {"title": "UniMEL: A Unified Framework for Multimodal Entity Linking with Large Language Models", "link": "https://arxiv.org/pdf/2407.16160", "details": "L Qi, H Yongyi, L Defu, Z Zhi, X Tong, L Che, C Enhong - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal Entity Linking (MEL) is a crucial task that aims at linking ambiguous mentions within multimodal contexts to the referent entities in a multimodal knowledge base, such as Wikipedia. Existing methods focus heavily on using \u2026"}, {"title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models", "link": "https://arxiv.org/pdf/2407.04693", "details": "Y Gu, Z Ji, W Zhang, C Lyu, D Lin, K Chen - arXiv preprint arXiv:2407.04693, 2024", "abstract": "Large language models (LLMs) exhibit hallucinations in long-form question- answering tasks across various domains and wide applications. Current hallucination detection and mitigation datasets are limited in domains and sizes \u2026"}, {"title": "Universal Approximation Theory: The basic theory for large language models", "link": "https://arxiv.org/pdf/2407.00958", "details": "W Wang, Q Li - arXiv preprint arXiv:2407.00958, 2024", "abstract": "Language models have emerged as a critical area of focus in artificial intelligence, particularly with the introduction of groundbreaking innovations like ChatGPT. Large- scale Transformer networks have quickly become the leading approach for \u2026"}]
