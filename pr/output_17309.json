[{"title": "ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations", "link": "https://arxiv.org/pdf/2506.03763", "details": "QH Pham, TD Nguyen, T Pham, AT Luu, DQ Nguyen - arXiv preprint arXiv \u2026, 2025", "abstract": "The capabilities of large language models (LLMs) have been enhanced by training on data that reflects human thought processes, such as the Chain-of-Thought format. However, evidence suggests that the conventional scheme of next-word prediction \u2026", "entry_id": "http://arxiv.org/abs/2506.03763v1", "updated": "2025-06-04 09:27:21", "published": "2025-06-04 09:27:21", "authors": "Quang Hieu Pham;Thuy Duong Nguyen;Tung Pham;Anh Tuan Luu;Dat Quoc Nguyen", "summary": "The capabilities of large language models (LLMs) have been enhanced by\ntraining on data that reflects human thought processes, such as the\nChain-of-Thought format. However, evidence suggests that the conventional\nscheme of next-word prediction may not fully capture how humans learn to think.\nInspired by how humans generalize mathematical reasoning, we propose a new\napproach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our\nClozeMath involves a text-infilling task that predicts masked equations from a\ngiven solution, analogous to cloze exercises used in human learning.\nExperiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the\nstrong baseline Masked Thought in performance and robustness, with two\ntest-time scaling decoding algorithms, Beam Search and Chain-of-Thought\ndecoding. Additionally, we conduct an ablation study to analyze the effects of\nvarious architectural and implementation choices on our approach.", "comment": "Accepted to ACL 2025 Findings", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.03763v1;http://arxiv.org/pdf/2506.03763v1", "pdf_url": "http://arxiv.org/pdf/2506.03763v1"}, {"title": "Do Language Models Mirror Human Confidence? Exploring Psychological Insights to Address Overconfidence in LLMs", "link": "https://arxiv.org/pdf/2506.00582", "details": "C Xu, B Wen, B Han, R Wolfe, LL Wang, B Howe - arXiv preprint arXiv:2506.00582, 2025", "abstract": "Psychology research has shown that humans are poor at estimating their performance on tasks, tending towards underconfidence on easy tasks and overconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct \u2026", "entry_id": "http://arxiv.org/abs/2506.00582v1", "updated": "2025-05-31 14:37:18", "published": "2025-05-31 14:37:18", "authors": "Chenjun Xu;Bingbing Wen;Bin Han;Robert Wolfe;Lucy Lu Wang;Bill Howe", "summary": "Psychology research has shown that humans are poor at estimating their\nperformance on tasks, tending towards underconfidence on easy tasks and\noverconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct,\nClaude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and\nshow that models exhibit subtle differences from human patterns of\noverconfidence: less sensitive to task difficulty, and when prompted to answer\nbased on different personas -- e.g., expert vs layman, or different race,\ngender, and ages -- the models will respond with stereotypically biased\nconfidence estimations even though their underlying answer accuracy remains the\nsame. Based on these observations, we propose Answer-Free Confidence Estimation\n(AFCE) to improve confidence calibration and LLM interpretability in these\nsettings. AFCE is a self-assessment method that employs two stages of\nprompting, first eliciting only confidence scores on questions, then asking\nseparately for the answer. Experiments on the MMLU and GPQA datasets spanning\nsubjects and difficulty show that this separation of tasks significantly\nreduces overconfidence and delivers more human-like sensitivity to task\ndifficulty.", "comment": "Accepted by ACL 2025 Findings, 20 pages", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;I.2.7", "links": "http://arxiv.org/abs/2506.00582v1;http://arxiv.org/pdf/2506.00582v1", "pdf_url": "http://arxiv.org/pdf/2506.00582v1"}, {"title": "Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces", "link": "https://arxiv.org/pdf/2506.00123", "details": "G Luo, G Yang, Z Gong, G Chen, H Duan, E Cui\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The remarkable progress of Multimodal Large Language Models (MLLMs) has attracted increasing attention to extend them to physical entities like legged robot. This typically requires MLLMs to not only grasp multimodal understanding abilities \u2026", "entry_id": "http://arxiv.org/abs/2506.00123v1", "updated": "2025-05-30 18:00:34", "published": "2025-05-30 18:00:34", "authors": "Gen Luo;Ganlin Yang;Ziyang Gong;Guanzhou Chen;Haonan Duan;Erfei Cui;Ronglei Tong;Zhi Hou;Tianyi Zhang;Zhe Chen;Shenglong Ye;Lewei Lu;Jingbo Wang;Wenhai Wang;Jifeng Dai;Yu Qiao;Rongrong Ji;Xizhou Zhu", "summary": "The remarkable progress of Multimodal Large Language Models (MLLMs) has\nattracted increasing attention to extend them to physical entities like legged\nrobot. This typically requires MLLMs to not only grasp multimodal understanding\nabilities, but also integrate visual-spatial reasoning and physical interaction\ncapabilities. Nevertheless,existing methods struggle to unify these\ncapabilities due to their fundamental differences.In this paper, we present the\nVisual Embodied Brain (VeBrain), a unified framework for perception, reasoning,\nand control in real world. VeBrain reformulates robotic control into common\ntext-based MLLM tasks in the 2D visual space, thus unifying the objectives and\nmapping spaces of different tasks. Then, a novel robotic adapter is proposed to\nconvert textual control signals from MLLMs to motion policies of real robots.\nFrom the data perspective, we further introduce VeBrain-600k, a high-quality\ninstruction dataset encompassing various capabilities of VeBrain. In\nVeBrain-600k, we take hundreds of hours to collect, curate and annotate the\ndata, and adopt multimodal chain-of-thought(CoT) to mix the different\ncapabilities into a single conversation. Extensive experiments on 13 multimodal\nbenchmarks and 5 spatial intelligence benchmarks demonstrate the superior\nperformance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to\nlegged robots and robotic arms, VeBrain shows strong adaptability, flexibility,\nand compositional capabilities compared to existing methods. For example,\ncompared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by\n+5.6%, but also excels in legged robot tasks with +50% average gains.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.RO", "links": "http://arxiv.org/abs/2506.00123v1;http://arxiv.org/pdf/2506.00123v1", "pdf_url": "http://arxiv.org/pdf/2506.00123v1"}, {"title": "Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2505.24164", "details": "S Xu, Y Li, R Yang, T Zhang, Y Sun, W Chow, L Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent works on large language models (LLMs) have successfully demonstrated the emergence of reasoning capabilities via reinforcement learning (RL). Although recent efforts leverage group relative policy optimization (GRPO) for MLLMs post \u2026", "entry_id": "http://arxiv.org/abs/2505.24164v1", "updated": "2025-05-30 03:11:46", "published": "2025-05-30 03:11:46", "authors": "Shilin Xu;Yanwei Li;Rui Yang;Tao Zhang;Yueyi Sun;Wei Chow;Linfeng Li;Hang Song;Qi Xu;Yunhai Tong;Xiangtai Li;Hao Fei", "summary": "Recent works on large language models (LLMs) have successfully demonstrated\nthe emergence of reasoning capabilities via reinforcement learning (RL).\nAlthough recent efforts leverage group relative policy optimization (GRPO) for\nMLLMs post-training, they constantly explore one specific aspect, such as\ngrounding tasks, math problems, or chart analysis. There are no works that can\nleverage multi-source MLLM tasks for stable reinforcement learning. In this\nwork, we present a unified perspective to solve this problem. We present\nMixed-R1, a unified yet straightforward framework that contains a mixed reward\nfunction design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K).\nWe first design a data engine to select high-quality examples to build the\nMixed-45K post-training dataset. Then, we present a Mixed-Reward design, which\ncontains various reward functions for various MLLM tasks. In particular, it has\nfour different reward functions: matching reward for binary answer or\nmultiple-choice problems, chart reward for chart-aware datasets, IoU reward for\ngrounding problems, and open-ended reward for long-form text responses such as\ncaption datasets. To handle the various long-form text content, we propose a\nnew open-ended reward named Bidirectional Max-Average Similarity (BMAS) by\nleveraging tokenizer embedding matching between the generated response and the\nground truth. Extensive experiments show the effectiveness of our proposed\nmethod on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes.\nOur dataset and model are available at https://github.com/xushilin1/mixed-r1.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.CV", "links": "http://arxiv.org/abs/2505.24164v1;http://arxiv.org/pdf/2505.24164v1", "pdf_url": "http://arxiv.org/pdf/2505.24164v1"}, {"title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement", "link": "https://arxiv.org/pdf/2506.03541", "details": "X Zhou, H Huang, L Liao - arXiv preprint arXiv:2506.03541, 2025", "abstract": "Large Language Models (LLMs) continue to set new standards in knowledge- intensive and complex reasoning tasks, yet their high computational demands limit widespread adoption. While distilling large models into smaller ones offers a \u2026", "entry_id": "http://arxiv.org/abs/2506.03541v1", "updated": "2025-06-04 03:52:20", "published": "2025-06-04 03:52:20", "authors": "Xiaofeng Zhou;Heyan Huang;Lizi Liao", "summary": "Large Language Models (LLMs) continue to set new standards in\nknowledge-intensive and complex reasoning tasks, yet their high computational\ndemands limit widespread adoption. While distilling large models into smaller\nones offers a sustainable solution, current techniques--such as static\nknowledge distillation, resource-intensive reinforcement learning from human\nfeedback, or limited self-reflection--struggle to yield substantial and lasting\nperformance gains. In this paper, we present a novel Debate and Reflect (D&R)\nframework that orchestrates multi-turn debates between smaller models and\nstronger teacher models, eliciting actionable feedback (e.g., error analysis,\ncorrective strategies) to guide student models. Further, we introduce\nTree-structured Direct Preference Optimization (T-DPO) to efficiently leverage\nthese debate logs, organizing interactions into a hierarchical format for\neffective training. Empirical evaluations across diverse NLP benchmarks\ndemonstrate that our approach significantly improves smaller-model accuracy,\nrobustness, and generalization, outperforming conventional baselines by a large\nmargin.", "comment": "16 pages, 10 figures. The camera-ready paper for Findings of ACL 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.03541v1;http://arxiv.org/pdf/2506.03541v1", "pdf_url": "http://arxiv.org/pdf/2506.03541v1"}, {"title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models", "link": "https://arxiv.org/pdf/2505.07591", "details": "J Ye, C Huang, Z Chen, W Fu, C Yang, L Yang, Y Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of \u2026", "entry_id": "http://arxiv.org/abs/2505.07591v1", "updated": "2025-05-12 14:16:55", "published": "2025-05-12 14:16:55", "authors": "Junjie Ye;Caishuang Huang;Zhuohan Chen;Wenjie Fu;Chenyuan Yang;Leyi Yang;Yilong Wu;Peng Wang;Meng Zhou;Xiaolong Yang;Tao Gui;Qi Zhang;Zhongchao Shi;Jianping Fan;Xuanjing Huang", "summary": "Instruction following evaluates large language models (LLMs) on their ability\nto generate outputs that adhere to user-defined constraints. However, existing\nbenchmarks often rely on templated constraint prompts, which lack the diversity\nof real-world usage and limit fine-grained performance assessment. To fill this\ngap, we propose a multi-dimensional constraint framework encompassing three\nconstraint patterns, four constraint categories, and four difficulty levels.\nBuilding on this framework, we develop an automated instruction generation\npipeline that performs constraint expansion, conflict detection, and\ninstruction rewriting, yielding 1,200 code-verifiable instruction-following\ntest samples. We evaluate 19 LLMs across seven model families and uncover\nsubstantial variation in performance across constraint forms. For instance,\naverage performance drops from 77.67% at Level I to 32.96% at Level IV.\nFurthermore, we demonstrate the utility of our approach by using it to generate\ndata for reinforcement learning, achieving substantial gains in instruction\nfollowing without degrading general performance. In-depth analysis indicates\nthat these gains stem primarily from modifications in the model's attention\nmodules parameters, which enhance constraint recognition and adherence. Code\nand data are available in https://github.com/Junjie-Ye/MulDimIF.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.07591v1;http://arxiv.org/pdf/2505.07591v1", "pdf_url": "http://arxiv.org/pdf/2505.07591v1"}, {"title": "Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration", "link": "https://arxiv.org/pdf/2505.24688", "details": "Q Zhu, R Zhao, H Yan, Y He, Y Chen, L Gui - arXiv preprint arXiv:2505.24688, 2025", "abstract": "Large Language Models (LLMs) struggle with complex reasoning due to limited diversity and inefficient search. We propose Soft Reasoning, an embedding-based search framework that optimises the embedding of the first token to guide generation \u2026", "entry_id": "http://arxiv.org/abs/2505.24688v2", "updated": "2025-06-04 08:11:18", "published": "2025-05-30 15:11:52", "authors": "Qinglin Zhu;Runcong Zhao;Hanqi Yan;Yulan He;Yudong Chen;Lin Gui", "summary": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution.", "comment": "Accepted as a Spotlight at ICML 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.24688v2;http://arxiv.org/pdf/2505.24688v2", "pdf_url": "http://arxiv.org/pdf/2505.24688v2"}, {"title": "A Simple Linear Patch Revives Layer-Pruned Large Language Models", "link": "https://arxiv.org/pdf/2505.24680", "details": "X Chen, H Bai, T Yuan, R Liu, K Zhao, X Yu, L Hou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Layer pruning has become a popular technique for compressing large language models (LLMs) due to its simplicity. However, existing layer pruning methods often suffer from significant performance drops. We identify that this degradation stems \u2026", "entry_id": "http://arxiv.org/abs/2505.24680v1", "updated": "2025-05-30 15:06:08", "published": "2025-05-30 15:06:08", "authors": "Xinrui Chen;Haoli Bai;Tao Yuan;Ruikang Liu;Kang Zhao;Xianzhi Yu;Lu Hou;Tian Guan;Yonghong He;Chun Yuan", "summary": "Layer pruning has become a popular technique for compressing large language\nmodels (LLMs) due to its simplicity. However, existing layer pruning methods\noften suffer from significant performance drops. We identify that this\ndegradation stems from the mismatch of activation magnitudes across layers and\ntokens at the pruning interface. To address this, we propose LinearPatch, a\nsimple plug-and-play technique to revive the layer-pruned LLMs. The proposed\nmethod adopts Hadamard transformation to suppress massive outliers in\nparticular tokens, and channel-wise scaling to align the activation magnitudes.\nThese operations can be fused into a single matrix, which functions as a patch\nto bridge the pruning interface with negligible inference overhead. LinearPatch\nretains up to 94.15% performance of the original model when pruning 5 layers of\nLLaMA-3-8B on the question answering benchmark, surpassing existing\nstate-of-the-art methods by 4%. In addition, the patch matrix can be further\noptimized with memory efficient offline knowledge distillation. With only 5K\nsamples, the retained performance of LinearPatch can be further boosted to\n95.16% within 30 minutes on a single computing card.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.24680v1;http://arxiv.org/pdf/2505.24680v1", "pdf_url": "http://arxiv.org/pdf/2505.24680v1"}, {"title": "Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models", "link": "https://arxiv.org/pdf/2506.03781", "details": "S Park, J Bae, B Kwon, M Kim, B Kim, SJ Kwon, U Kang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "How can we quantize large language models while preserving accuracy? Quantization is essential for deploying large language models (LLMs) efficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are promising \u2026", "entry_id": "http://arxiv.org/abs/2506.03781v1", "updated": "2025-06-04 09:42:17", "published": "2025-06-04 09:42:17", "authors": "Seungcheol Park;Jeongin Bae;Beomseok Kwon;Minjun Kim;Byeongwook Kim;Se Jung Kwon;U Kang;Dongsoo Lee", "summary": "How can we quantize large language models while preserving accuracy?\nQuantization is essential for deploying large language models (LLMs)\nefficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are\npromising quantization schemes that have strong expressiveness and\noptimizability, respectively. However, neither scheme leverages both\nadvantages. In this paper, we propose UniQuanF (Unified Quantization with\nFlexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses\nboth strong expressiveness and optimizability by unifying the flexible mapping\ntechnique in UQ and non-uniform quantization levels of BCQ. We propose unified\ninitialization, and local and periodic mapping techniques to optimize the\nparameters in UniQuanF precisely. After optimization, our unification theorem\nremoves computational and memory overhead, allowing us to utilize the superior\naccuracy of UniQuanF without extra deployment costs induced by the unification.\nExperimental results demonstrate that UniQuanF outperforms existing UQ and BCQ\nmethods, achieving up to 4.60% higher accuracy on GSM8K benchmark.", "comment": "ACL 2025 Main Track", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;68T50;I.2.7", "links": "http://arxiv.org/abs/2506.03781v1;http://arxiv.org/pdf/2506.03781v1", "pdf_url": "http://arxiv.org/pdf/2506.03781v1"}]
