[{"title": "Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment", "link": "https://arxiv.org/pdf/2503.04647", "details": "W Yang, J Wu, C Wang, C Zong, J Zhang - arXiv preprint arXiv:2503.04647, 2025", "abstract": "Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences. While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is \u2026"}, {"title": "Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions", "link": "https://arxiv.org/pdf/2503.22353", "details": "Y Li, Y Miao, X Ding, R Krishnan, R Padman - arXiv preprint arXiv:2503.22353, 2025", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities across various tasks, but their deployment in high-stake domains requires consistent performance across multiple interaction rounds. This paper introduces a comprehensive \u2026"}, {"title": "Assessing Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks", "link": "https://openreview.net/pdf%3Fid%3D3YyyiyV4B6", "details": "F Lin, S Mao, E La Malfa, V Hofmann, A de Wynter\u2026 - Workshop on Reasoning and \u2026", "abstract": "Language is not monolithic. While benchmarks, including those designed for multiple languages, are often used as proxies to evaluate the performance of Large Language Models (LLMs), they tend to overlook the nuances of within-language \u2026"}, {"title": "Privacy Auditing for Large Language Models with Natural Identifiers", "link": "https://openreview.net/pdf%3Fid%3Djp4XlcpRIW", "details": "L Rossi, B Marek, F Boenisch, A Dziedzic - ICLR 2025 Workshop on Foundation Models in \u2026", "abstract": "The privacy auditing for large language models (LLMs) faces significant challenges. Membership inference attacks, once considered a practical privacy auditing tool, are unreliable for pretrained LLMs due to the lack of non-member data from the same \u2026"}, {"title": "Mitigating Social Bias in Large Language Models by Self-Correction", "link": "https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/Q2-22.pdf", "details": "PAM Kaneko, N Okazaki", "abstract": "Abstract Self-Correction enables Large Language Models (LLMs) to refine their responses during inference based on feedback. While prior research mainly examines the impact of Self-Correction on reasoning tasks such as arithmetic \u2026"}]
