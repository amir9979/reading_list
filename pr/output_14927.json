[{"title": "Stabilizing Modality Gap & Lowering Gradient Norms Improve Zero-Shot Adversarial Robustness of VLMs", "link": "https://dl.acm.org/doi/abs/10.1145/3690624.3709296", "details": "J Dong, P Koniusz, X Qu, YS Ong - Proceedings of the 31st ACM SIGKDD Conference \u2026, 2025", "abstract": "Contemporary Vision-Language Models (VLMs) such as CLIP offer an attractive zero- shot classification functionality facilitated by large-scale vision-language pre-training. However, they remain vulnerable to adversarial attacks, a critical security threat in \u2026"}, {"title": "Robust Dataset Distillation by Matching Adversarial Trajectories", "link": "https://arxiv.org/pdf/2503.12069", "details": "W Lai, T Ding, L Wang, J Huo, Y Gao, W Li - arXiv preprint arXiv:2503.12069, 2025", "abstract": "Dataset distillation synthesizes compact datasets that enable models to achieve performance comparable to training on the original large-scale datasets. However, existing distillation methods overlook the robustness of the model, resulting in \u2026"}, {"title": "Federated Koopman-Reservoir Learning for Large-Scale Multivariate Time-Series Anomaly Detection", "link": "https://arxiv.org/pdf/2503.11255", "details": "LT Le, TA Nguyen, H Shu, S Seneviratne, CS Hong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The proliferation of edge devices has dramatically increased the generation of multivariate time-series (MVTS) data, essential for applications from healthcare to smart cities. Such data streams, however, are vulnerable to anomalies that signal \u2026"}, {"title": "Hardware-Aware Iterative One-Shot Neural Architecture Search with Adaptable Knowledge Distillation for Efficient Edge Computing", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10938148.pdf", "details": "OTC Chen, YX Chang, CY Chung, YY Cheng, MH Ha - IEEE Access, 2025", "abstract": "The growing demand for edge applications calls for efficient and optimized deep neural network models. Neural Architecture Search (NAS) is instrumental in designing such models, but achieving optimal architectures quickly remains a key \u2026"}, {"title": "Fusionformer: A Novel Adversarial Transformer Utilizing Fusion Attention for Multivariate Anomaly Detection", "link": "https://ieeexplore.ieee.org/abstract/document/10922726/", "details": "C Wang, Z Wang, H Dong, S Lauria, W Liu, Y Wang\u2026 - IEEE Transactions on \u2026, 2025", "abstract": "Multivariate time series forecasting (MTSF) is of significant importance in the enhancement and optimization of real-world applications. The task of MTSF poses substantial challenges due to the unpredictability of temporal patterns and the \u2026"}, {"title": "Series clustering and dynamic periodic patching-based transformer for multivariate time series forecasting", "link": "https://www.sciencedirect.com/science/article/pii/S1568494625002911", "details": "Y Wang, X Wu, J Zhang, W Wang, L Zheng, J Shang - Applied Soft Computing, 2025", "abstract": "Multivariate time series forecasting (MTSF) is widely employed in research-intensive domains, such as weather forecasting. Recently, Transformer-based models have outstanding ability to achieve SOTA performance, benefiting from its self-attention \u2026"}, {"title": "Teacher privileged distillation: How to deal with imperfect teachers?", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125003855", "details": "M Mart\u00ednez-Garc\u00eda, I Inza, JA Lozano - Knowledge-Based Systems, 2025", "abstract": "The paradigm of learning using privileged information leverages privileged features present at training time, but not at prediction, as additional training information. The privileged learning process is addressed through a knowledge distillation \u2026"}, {"title": "TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification", "link": "https://arxiv.org/pdf/2503.12206", "details": "A Munir, FZ Qureshi, MH Khan, M Ali - arXiv preprint arXiv:2503.12206, 2025", "abstract": "Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot performance on image classification. However, state-of-the-art methods often rely on fine-tuning techniques like prompt learning and adapter-based tuning to optimize \u2026"}, {"title": "Preserving Angles Improves Feature Distillation of Foundation Models", "link": "https://www.researchgate.net/profile/Evelyn-Mannix/publication/386112516_Preserving_Angles_Improves_Feature_Distillation_of_Foundation_Models/links/67d4da32be849d39d679003d/Preserving-Angles-Improves-Feature-Distillation-of-Foundation-Models.pdf", "details": "EJ Mannix, L Hodgkinson, H Bondell", "abstract": "Abstract Knowledge distillation approaches compress models by training a student network using the classification outputs of a high quality teacher model, but can fail to effectively transfer the properties of computer vision foundation models from the \u2026"}]
