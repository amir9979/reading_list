[{"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers", "link": "https://openreview.net/pdf%3Fid%3D67tRrjgzsh", "details": "X Lu, Y Zhao, B Qin, L Huo, Q Yang, D Xu - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Using a natural language processing toolkit to classify electronic health records by psychiatric diagnosis", "link": "https://journals.sagepub.com/doi/pdf/10.1177/14604582241296411", "details": "A Hutto, TM Zikry, B Bohac, T Rose, J Staebler, J Slay\u2026 - Health Informatics Journal, 2024", "abstract": "Objective: We analyzed a natural language processing (NLP) toolkit's ability to classify unstructured EHR data by psychiatric diagnosis. Expertise can be a barrier to using NLP. We employed an NLP toolkit (CLARK) created to support studies led by \u2026"}, {"title": "Accelerating Blockwise Parallel Language Models with Draft Refinement", "link": "https://openreview.net/pdf%3Fid%3DKT6F5Sw0eg", "details": "T Kim, AT Suresh, KA Papineni, M Riley, S Kumar\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Autoregressive language models have achieved remarkable advancements, yet their potential is often limited by the slow inference speeds associated with sequential token generation. Blockwise parallel decoding (BPD) was proposed by Stern et \u2026"}, {"title": "Code-switching finetuning: Bridging multilingual pretrained language models for enhanced cross-lingual performance", "link": "https://www.sciencedirect.com/science/article/pii/S0952197624016907", "details": "C Zan, L Ding, L Shen, Y Cao, W Liu - Engineering Applications of Artificial \u2026, 2025", "abstract": "In recent years, the development of pre-trained models has significantly propelled advancements in natural language processing. However, multilingual sequence-to- sequence pretrained language models (Seq2Seq PLMs) are pretrained on a wide \u2026"}, {"title": "Mathematical Reasoning via Multi-step Self Questioning and Answering for Small Language Models", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9440-9_7", "details": "K Chen, J Wang, X Zhang - CCF International Conference on Natural Language \u2026, 2024", "abstract": "Mathematical reasoning is challenging for large language models (LLMs), while the scaling relationship concerning LLM capacity is under-explored. Existing works have tried to leverage the rationales of LLMs to train small language models (SLMs) for \u2026"}, {"title": "Optimizing Fine-Tuning in Quantized Language Models: An In-Depth Analysis of Key Variables", "link": "https://cdn.techscience.cn/files/cmc/2024/online/CMC1030/TSP_CMC_57491/TSP_CMC_57491.pdf", "details": "A Shen, Z Lai, D Li, X Hu - 2024", "abstract": "ABSTRACT Large-scale Language Models (LLMs) have achieved significant breakthroughs in Natural Language Processing (NLP), driven by the pre-training and fine-tuning paradigm. While this approach allows models to specialize in specific \u2026"}, {"title": "Large Language Models Know What is Key Visual Entity: An LLM-assisted Multimodal Retrieval for VQA", "link": "https://aclanthology.org/2024.emnlp-main.613.pdf", "details": "P Jian, D Yu, J Zhang - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Visual question answering (VQA) tasks, often performed by visual language model (VLM), face challenges with long-tail knowledge. Recent retrieval-augmented VQA (RA-VQA) systems address this by retrieving and integrating external knowledge \u2026"}, {"title": "Probing Social Bias in Labor Market Text Generation by ChatGPT: A Masked Language Model Approach", "link": "https://openreview.net/pdf%3Fid%3DMP7j58lbWO", "details": "L Ding, Y Hu, N Denier, E Shi, J Zhang, Q Hu\u2026 - The Thirty-eighth Annual \u2026", "abstract": "As generative large language models (LLMs) such as ChatGPT gain widespread adoption in various domains, their potential to propagate and amplify social biases, particularly in high-stakes areas such as the labor market, has become a pressing \u2026"}, {"title": "Fine-tuned Large Language Models (LLMs): Improved Prompt Injection Attacks Detection", "link": "https://arxiv.org/pdf/2410.21337", "details": "MA Rahman, F Wu, A Cuzzocrea, SI Ahamed - arXiv preprint arXiv:2410.21337, 2024", "abstract": "Large language models (LLMs) are becoming a popular tool as they have significantly advanced in their capability to tackle a wide range of language-based tasks. However, LLMs applications are highly vulnerable to prompt injection attacks \u2026"}]
