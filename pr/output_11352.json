[{"title": "Toward a privacy-preserving predictive foundation model of single-cell transcriptomics with federated learning and tabular modeling", "link": "https://www.biorxiv.org/content/10.1101/2025.01.06.631427.full.pdf", "details": "J Ding, J Lin, S Jiang, Y Wang, Z Mao, Z Fang, J Tang\u2026 - bioRxiv, 2025", "abstract": "The ability to pre-train on vast amounts of data to build foundation models (FMs) has achieved remarkable success in numerous domains, including natural language processing, computer vision, and, more recently, single-cell genomics-epitomized by \u2026"}, {"title": "Guiding Medical Vision-Language Models with Explicit Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations", "link": "https://arxiv.org/pdf/2501.02385", "details": "K Zhu, Z Qin, H Yi, Z Jiang, Q Lao, S Zhang, K Li - arXiv preprint arXiv:2501.02385, 2025", "abstract": "With the recent advancements in vision-language models (VLMs) driven by large language models (LLMs), many researchers have focused on models that comprised of an image encoder, an image-to-language projection layer, and a text decoder \u2026"}, {"title": "Efficient Architectures for High Resolution Vision-Language Models", "link": "https://arxiv.org/pdf/2501.02584", "details": "M Carvalho, B Martins - arXiv preprint arXiv:2501.02584, 2025", "abstract": "Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This \u2026"}, {"title": "Training Medical Large Vision-Language Models with Abnormal-Aware Feedback", "link": "https://arxiv.org/pdf/2501.01377", "details": "Y Zhou, L Song, J Shen - arXiv preprint arXiv:2501.01377, 2025", "abstract": "Existing Medical Large Vision-Language Models (Med-LVLMs), which encapsulate extensive medical knowledge, demonstrate excellent capabilities in understanding medical images and responding to human queries based on these images \u2026"}, {"title": "3VL: Using Trees to Improve Vision-Language Models' Interpretability", "link": "https://ieeexplore.ieee.org/abstract/document/10829542/", "details": "N Yellinek, L Karlinsky, R Giryes - IEEE Transactions on Image Processing, 2025", "abstract": "Vision-Language models (VLMs) have proven to be effective at aligning image and text representations, producing superior zero-shot results when transferred to many downstream tasks. However, these representations suffer from some key \u2026"}, {"title": "Interpretable LLM-based Table Question Answering", "link": "https://arxiv.org/pdf/2412.12386", "details": "I Brugere, S Sharma, S Kariyappa, AT Nguyen, F Lecue - arXiv preprint arXiv \u2026, 2024", "abstract": "Interpretability for Table Question Answering (Table QA) is critical, particularly in high- stakes industries like finance or healthcare. Although recent approaches using Large Language Models (LLMs) have significantly improved Table QA performance, their \u2026"}, {"title": "AD-LLM: Benchmarking Large Language Models for Anomaly Detection", "link": "https://arxiv.org/pdf/2412.11142", "details": "T Yang, Y Nian, S Li, R Xu, Y Li, J Li, Z Xiao, X Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Anomaly detection (AD) is an important machine learning task with many real-world uses, including fraud detection, medical diagnosis, and industrial monitoring. Within natural language processing (NLP), AD helps detect issues like spam \u2026"}, {"title": "Expert evaluation of large language models for clinical dialogue summarization", "link": "https://www.nature.com/articles/s41598-024-84850-x", "details": "D Fraile Navarro, E Coiera, TW Hambly, Z Triplett\u2026 - Scientific Reports, 2025", "abstract": "We assessed the performance of large language models' summarizing clinical dialogues using computational metrics and human evaluations. The comparison was done between automatically generated and human-produced summaries. We \u2026"}, {"title": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over Aligned Large Language Models", "link": "https://arxiv.org/pdf/2412.12865", "details": "Y Fan, Y Hong, Q Wang, J Bao, H Jiang, Y Song - arXiv preprint arXiv:2412.12865, 2024", "abstract": "Alignment, endowing a pre-trained Large language model (LLM) with the ability to follow instructions, is crucial for its real-world applications. Conventional supervised fine-tuning (SFT) methods formalize it as causal language modeling typically with a \u2026"}]
