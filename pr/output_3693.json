[{"title": "Inter-structure and intra-semantics graph contrastive learning for disease prediction", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124006932", "details": "Y Kang, J Zheng, M Yang, N An - Knowledge-Based Systems, 2024", "abstract": "Ever-evolving healthcare applications have witnessed a surge in the utilization of electronic health records (EHR) for predicting future patient diagnoses. While Graph Neural Networks have demonstrated that promise in modeling disease-patient \u2026"}, {"title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2406.13233", "details": "Z Zeng, Y Miao, H Gao, H Zhang, Z Deng - arXiv preprint arXiv:2406.13233, 2024", "abstract": "Mixture of experts (MoE) has become the standard for constructing production-level large language models (LLMs) due to its promise to boost model capacity without causing significant overheads. Nevertheless, existing MoE methods usually enforce \u2026"}, {"title": "BadCLM: Backdoor Attack in Clinical Language Models for Electronic Health Records", "link": "https://arxiv.org/pdf/2407.05213", "details": "W Lyu, Z Bi, F Wang, C Chen - arXiv preprint arXiv:2407.05213, 2024", "abstract": "The advent of clinical language models integrated into electronic health records (EHR) for clinical decision support has marked a significant advancement, leveraging the depth of clinical notes for improved decision-making. Despite their \u2026"}, {"title": "Timo: Towards Better Temporal Reasoning for Language Models", "link": "https://arxiv.org/pdf/2406.14192", "details": "Z Su, J Zhang, T Zhu, X Qu, J Li, M Zhang, Y Cheng - arXiv preprint arXiv:2406.14192, 2024", "abstract": "Reasoning about time is essential for Large Language Models (LLMs) to understand the world. Previous works focus on solving specific tasks, primarily on time-sensitive question answering. While these methods have proven effective, they cannot \u2026"}, {"title": "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "link": "https://arxiv.org/pdf/2406.12050", "details": "Z Zhang, Z Liang, W Yu, D Yu, M Jia, D Yu, M Jiang - arXiv preprint arXiv:2406.12050, 2024", "abstract": "Supervised fine-tuning enhances the problem-solving abilities of language models across various mathematical reasoning tasks. To maximize such benefits, existing research focuses on broadening the training set with various data augmentation \u2026"}, {"title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models", "link": "https://aclanthology.org/2024.findings-naacl.18.pdf", "details": "Z Su, Z Lin, B Baixue, H Chen, S Hu, W Zhou, G Ding\u2026 - Findings of the Association \u2026, 2024", "abstract": "Generative language models are usually pre-trained on large text corpus via predicting the next token (ie, sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language \u2026"}, {"title": "Fast and Slow Generating: An Empirical Study on Large and Small Language Models Collaborative Decoding", "link": "https://arxiv.org/pdf/2406.12295", "details": "K Zhang, J Wang, N Ding, B Qi, E Hua, X Lv, B Zhou - arXiv preprint arXiv:2406.12295, 2024", "abstract": "Large Language Models (LLMs) demonstrate impressive performance in diverse applications, yet they face significant drawbacks, including high inference latency, expensive training cost, and generation of hallucination. Collaborative decoding \u2026"}, {"title": "Abstraction-of-Thought Makes Language Models Better Reasoners", "link": "https://arxiv.org/pdf/2406.12442", "details": "R Hong, H Zhang, X Pan, D Yu, C Zhang - arXiv preprint arXiv:2406.12442, 2024", "abstract": "Abstract reasoning, the ability to reason from the abstract essence of a problem, serves as a key to generalization in human reasoning. However, eliciting language models to perform reasoning with abstraction remains unexplored. This paper seeks \u2026"}, {"title": "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models", "link": "https://arxiv.org/pdf/2407.05131", "details": "P Xia, K Zhu, H Li, H Zhu, Y Li, G Li, L Zhang, H Yao - arXiv preprint arXiv:2407.05131, 2024", "abstract": "The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical \u2026"}]
