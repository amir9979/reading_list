[{"title": "Why are Visually-Grounded Language Models Bad at Image Classification?", "link": "https://arxiv.org/pdf/2405.18415", "details": "Y Zhang, A Unell, X Wang, D Ghosh, Y Su, L Schmidt\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Image classification is one of the most fundamental capabilities of machine vision intelligence. In this work, we revisit the image classification task using visually- grounded language models (VLMs) such as GPT-4V and LLaVA. We find that \u2026"}, {"title": "Convergence Behavior of an Adversarial Weak Supervision Method", "link": "https://arxiv.org/pdf/2405.16013", "details": "S An, S Dasgupta - arXiv preprint arXiv:2405.16013, 2024", "abstract": "Labeling data via rules-of-thumb and minimal label supervision is central to Weak Supervision, a paradigm subsuming subareas of machine learning such as crowdsourced learning and semi-supervised ensemble learning. By using this \u2026"}, {"title": "FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2405.18218", "details": "Y Zhang, Y Li, X Wang, Q Shen, B Plank, B Bischl\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Overparametrized transformer networks are the state-of-the-art architecture for Large Language Models (LLMs). However, such models contain billions of parameters making large compute a necessity, while raising environmental concerns. To \u2026"}, {"title": "CF-OPT: Counterfactual Explanations for Structured Prediction", "link": "https://arxiv.org/pdf/2405.18293", "details": "A Forel, A Parmentier, T Vidal - arXiv preprint arXiv:2405.18293, 2024", "abstract": "Optimization layers in deep neural networks have enjoyed a growing popularity in structured learning, improving the state of the art on a variety of applications. Yet, these pipelines lack interpretability since they are made of two opaque layers: a \u2026"}, {"title": "Zero-shot LLM-guided Counterfactual Generation for Text", "link": "https://arxiv.org/pdf/2405.04793", "details": "A Bhattacharjee, R Moraffah, J Garland, H Liu - arXiv preprint arXiv:2405.04793, 2024", "abstract": "Counterfactual examples are frequently used for model development and evaluation in many natural language processing (NLP) tasks. Although methods for automated counterfactual generation have been explored, such methods depend on models \u2026"}, {"title": "HarmoDT: Harmony Multi-Task Decision Transformer for Offline Reinforcement Learning", "link": "https://arxiv.org/pdf/2405.18080", "details": "S Hu, Z Fan, L Shen, Y Zhang, Y Wang, D Tao - arXiv preprint arXiv:2405.18080, 2024", "abstract": "The purpose of offline multi-task reinforcement learning (MTRL) is to develop a unified policy applicable to diverse tasks without the need for online environmental interaction. Recent advancements approach this through sequence modeling \u2026"}, {"title": "SPO: Multi-Dimensional Preference Alignment With Implicit Reward Modeling", "link": "https://www.researchgate.net/profile/xingzhou_loulou/publication/380719409_SPO_Multi-Dimensional_Preference_Alignment_With_Implicit_Reward_Modeling/links/664ac304bc86444c72eebf4e/SPO-Multi-Dimensional-Preference-Alignment-With-Implicit-Reward-Modeling.pdf", "details": "X Lou, J Zhang, J Xie, L Liu, D Yan, K Huang", "abstract": "Human preference alignment is critical in building powerful and reliable large language models (LLMs). However, current methods either ignore the multidimensionality of human preferences (eg helpfulness and harmlessness) or \u2026"}, {"title": "Towards Human-AI Complementarity with Predictions Sets", "link": "https://arxiv.org/pdf/2405.17544", "details": "G De Toni, N Okati, S Thejaswi, E Straitouri\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Decision support systems based on prediction sets have proven to be effective at helping human experts solve classification tasks. Rather than providing single-label predictions, these systems provide sets of label predictions constructed using \u2026"}, {"title": "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "link": "https://arxiv.org/pdf/2404.19335", "details": "X Liu, C Liu, Z Zhang, C Li, L Wang, Y Lan, C Shen - arXiv preprint arXiv:2404.19335, 2024", "abstract": "Large language models have shown their ability to become effective few-shot learners with prompting, revoluting the paradigm of learning with data scarcity. However, this approach largely depends on the quality of prompt initialization, and \u2026"}]
