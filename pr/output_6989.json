[{"title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models", "link": "https://arxiv.org/pdf/2409.12294", "details": "A Jain, C Jermaine, V Unhelkar - arXiv preprint arXiv:2409.12294, 2024", "abstract": "Large language models (LLMs) have recently emerged as promising tools for solving challenging robotic tasks, even in the presence of action and observation uncertainties. Recent LLM-based decision-making methods (also referred to as LLM \u2026"}, {"title": "VLM's Eye Examination: Instruct and Inspect Visual Competency of Vision Language Models", "link": "https://arxiv.org/pdf/2409.14759", "details": "N Hyeon-Woo, M Ye-Bin, W Choi, L Hyun, TH Oh - arXiv preprint arXiv:2409.14759, 2024", "abstract": "Vision language models (VLMs) have shown promising reasoning capabilities across various benchmarks; however, our understanding of their visual perception remains limited. In this work, we propose an eye examination process to investigate \u2026"}, {"title": "Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models", "link": "https://arxiv.org/pdf/2409.12512", "details": "J Rao, X Liu, Z Lin, L Ding, J Li, D Tao - arXiv preprint arXiv:2409.12512, 2024", "abstract": "Knowledge distillation (KD) is a technique that compresses large teacher models by training smaller student models to mimic them. The success of KD in auto-regressive language models mainly relies on Reverse KL for mode-seeking and student \u2026"}, {"title": "Towards Efficient and Robust VQA-NLE Data Generation with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.14785", "details": "PA Irawan, GI Winata, S Cahyawijaya, A Purwarianti - arXiv preprint arXiv \u2026, 2024", "abstract": "Natural Language Explanation (NLE) aims to elucidate the decision-making process by providing detailed, human-friendly explanations in natural language. It helps demystify the decision-making processes of large vision-language models (LVLMs) \u2026"}, {"title": "Instruction Following without Instruction Tuning", "link": "https://arxiv.org/pdf/2409.14254", "details": "J Hewitt, NF Liu, P Liang, CD Manning - arXiv preprint arXiv:2409.14254, 2024", "abstract": "Instruction tuning commonly means finetuning a language model on instruction- response pairs. We discover two forms of adaptation (tuning) that are deficient compared to instruction tuning, yet still yield instruction following; we call this implicit \u2026"}, {"title": "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "link": "https://arxiv.org/pdf/2409.14037", "details": "P Bajpai, N Chatterjee, S Dutta, T Chakraborty - arXiv preprint arXiv:2409.14037, 2024", "abstract": "Large Language Models (LLMs) and AI assistants driven by these models are experiencing exponential growth in usage among both expert and amateur users. In this work, we focus on evaluating the reliability of current LLMs as science \u2026"}, {"title": "Improving the Efficiency of Visually Augmented Language Models", "link": "https://arxiv.org/pdf/2409.11148", "details": "P Ontalvilla, A Ormazabal, G Azkune - arXiv preprint arXiv:2409.11148, 2024", "abstract": "Despite the impressive performance of autoregressive Language Models (LM) it has been shown that due to reporting bias, LMs lack visual knowledge, ie they do not know much about the visual world and its properties. To augment LMs with visual \u2026"}, {"title": "Language Models Learn to Mislead Humans via RLHF", "link": "https://arxiv.org/pdf/2409.12822", "details": "J Wen, R Zhong, A Khan, E Perez, J Steinhardt\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models (LMs) can produce errors that are hard to detect for humans, especially when the task is complex. RLHF, the most popular post-training method, may exacerbate this problem: to achieve higher rewards, LMs might get better at \u2026"}, {"title": "Investigating Layer Importance in Large Language Models", "link": "https://arxiv.org/pdf/2409.14381", "details": "Y Zhang, Y Dong, K Kawaguchi - arXiv preprint arXiv:2409.14381, 2024", "abstract": "Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in \u2026"}]
