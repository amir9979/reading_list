[{"title": "GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization", "link": "https://arxiv.org/pdf/2410.04087", "details": "Y Ye, X Feng, X Feng, W Ma, L Qin, D Xu, Q Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "News summarization in today's global scene can be daunting with its flood of multilingual content and varied viewpoints from different sources. However, current studies often neglect such real-world scenarios as they tend to focus solely on either \u2026"}, {"title": "FM-ABS: Promptable Foundation Model Drives Active Barely Supervised Learning for 3D Medical Image Segmentation", "link": "https://papers.miccai.org/miccai-2024/paper/0050_paper.pdf", "details": "Z Xu, C Chen, D Lu, J Sun, D Wei, Y Zheng, Q Li\u2026 - International Conference on \u2026, 2024", "abstract": "Semi-supervised learning (SSL) has significantly advanced 3D medical image segmentation by effectively reducing the need for laborious dense labeling from radiologists. Traditionally focused on model-centric advancements, we anticipate that \u2026"}, {"title": "Task-Adaptive Pretrained Language Models via Clustered-Importance Sampling", "link": "https://arxiv.org/pdf/2410.03735", "details": "D Grangier, S Fan, S Seto, P Ablin - arXiv preprint arXiv:2410.03735, 2024", "abstract": "Specialist language models (LMs) focus on a specific task or domain on which they often outperform generalist LMs of the same size. However, the specialist data needed to pretrain these models is only available in limited amount for most tasks. In \u2026"}, {"title": "TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation Models", "link": "https://arxiv.org/pdf/2410.05239", "details": "R Adhikari, S Thapaliya, M Dhakal, B Khanal - arXiv preprint arXiv:2410.05239, 2024", "abstract": "Vision-Language Models (VLMs) have shown impressive performance in vision tasks, but adapting them to new domains often requires expensive fine-tuning. Prompt tuning techniques, including textual, visual, and multimodal prompting, offer \u2026"}, {"title": "Multi-scale Spatio-temporal Memory Network for Lightweight Video denoising", "link": "https://pubmed.ncbi.nlm.nih.gov/39378250/", "details": "L Sun, F Wu, W Ding, X Li, W Dong, G Shi - IEEE transactions on image processing: a \u2026", "abstract": "Deep learning-based video denoising methods have achieved great performance improvements in recent years. However, the expensive computational cost arising from sophisticated network design has severely limited their applications in real \u2026"}, {"title": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference", "link": "https://arxiv.org/pdf/2410.03090", "details": "J Xiong, J Shen, F Ye, C Tao, Z Wan, J Lu, X Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Deploying large language models (LLMs) is challenging due to their high memory and computational demands, especially during long-context inference. While key- value (KV) caching accelerates inference by reusing previously computed keys and \u2026"}, {"title": "Language Enhanced Model for Eye (LEME): An Open-Source Ophthalmology-Specific Large Language Model", "link": "https://arxiv.org/pdf/2410.03740", "details": "A Gilson, X Ai, Q Xie, S Srinivasan, K Pushpanathan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) are poised to revolutionize healthcare. Ophthalmology-specific LLMs remain scarce and underexplored. We introduced an open-source, specialized LLM for ophthalmology, termed Language Enhanced \u2026"}, {"title": "An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation", "link": "https://arxiv.org/pdf/2410.03334", "details": "A Abdulaal, H Fry, N Monta\u00f1a-Brown, A Ijishakin, J Gao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Radiological services are experiencing unprecedented demand, leading to increased interest in automating radiology report generation. Existing Vision- Language Models (VLMs) suffer from hallucinations, lack interpretability, and require \u2026"}, {"title": "HunFlair2 in a cross-corpus evaluation of biomedical named entity recognition and normalization tools", "link": "https://academic.oup.com/bioinformatics/article/40/10/btae564/7762634", "details": "M S\u00e4nger, S Garda, XD Wang, L Weber-Genzel\u2026 - Bioinformatics, 2024", "abstract": "Motivation With the exponential growth of the life sciences literature, biomedical text mining (BTM) has become an essential technology for accelerating the extraction of insights from publications. The identification of entities in texts, such as diseases or \u2026"}]
