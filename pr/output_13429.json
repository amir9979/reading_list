[{"title": "Improving Deep Assertion Generation via Fine-Tuning Retrieval-Augmented Pre-trained Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3721128", "details": "Q Zhang, C Fang, Y Zheng, Y Zhang, Y Zhao, R Huang\u2026 - ACM Transactions on \u2026, 2025", "abstract": "Unit testing validates the correctness of the units of the software system under test and serves as the cornerstone in improving software quality and reliability. To reduce manual efforts in writing unit tests, some techniques have been proposed to generate \u2026"}, {"title": "Language Models Use Trigonometry to Do Addition", "link": "https://arxiv.org/pdf/2502.00873%3F", "details": "S Kantamneni, M Tegmark - arXiv preprint arXiv:2502.00873, 2025", "abstract": "Mathematical reasoning is an increasingly important indicator of large language model (LLM) capabilities, yet we lack understanding of how LLMs process even simple mathematical tasks. To address this, we reverse engineer how three mid \u2026"}, {"title": "Language Models Largely Exhibit Human-like Constituent Ordering Preferences", "link": "https://arxiv.org/pdf/2502.05670", "details": "AD Tur, G Kamath, S Reddy - arXiv preprint arXiv:2502.05670, 2025", "abstract": "Though English sentences are typically inflexible vis-\\a-vis word order, constituents often show far more variability in ordering. One prominent theory presents the notion that constituent ordering is directly correlated with constituent weight: a measure of \u2026"}, {"title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach", "link": "https://arxiv.org/pdf/2502.05171", "details": "J Geiping, S McLeish, N Jain, J Kirchenbauer, S Singh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in \u2026"}, {"title": "The Relationship Between Reasoning and Performance in Large Language Models--o3 (mini) Thinks Harder, Not Longer", "link": "https://arxiv.org/pdf/2502.15631", "details": "M Ballon, A Algaba, V Ginis - arXiv preprint arXiv:2502.15631, 2025", "abstract": "Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token \u2026"}, {"title": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning", "link": "https://arxiv.org/pdf/2502.18439", "details": "C Park, S Han, X Guo, A Ozdaglar, K Zhang, JK Kim - arXiv preprint arXiv:2502.18439, 2025", "abstract": "Leveraging multiple large language models (LLMs) to build collaborative multi- agentic workflows has demonstrated significant potential. However, most previous studies focus on prompting the out-of-the-box LLMs, relying on their innate capability \u2026"}, {"title": "On the logical skills of large language models: evaluations using arbitrarily complex first-order logic problems", "link": "https://arxiv.org/pdf/2502.14180", "details": "S Ibragimov, A Jentzen, B Kuckuck - arXiv preprint arXiv:2502.14180, 2025", "abstract": "We present a method of generating first-order logic statements whose complexity can be controlled along multiple dimensions. We use this method to automatically create several datasets consisting of questions asking for the truth or falsity of first-order \u2026"}, {"title": "EAGER-LLM: Enhancing Large Language Models as Recommenders through Exogenous Behavior-Semantic Integration", "link": "https://arxiv.org/pdf/2502.14735", "details": "M Hong, Y Xia, Z Wang, J Zhu, Y Wang, S Cai, X Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) are increasingly leveraged as foundational backbones in the development of advanced recommender systems, offering enhanced capabilities through their extensive knowledge and reasoning. Existing llm \u2026"}, {"title": "Self-Training Elicits Concise Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2502.20122", "details": "T Munkhbat, N Ho, S Kim, Y Yang, Y Kim, SY Yun - arXiv preprint arXiv:2502.20122, 2025", "abstract": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens \u2026"}]
