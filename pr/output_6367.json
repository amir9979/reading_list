[{"title": "Large Language Models Prompting With Episodic Memory", "link": "https://arxiv.org/pdf/2408.07465", "details": "D Do, Q Tran, S Venkatesh, H Le - arXiv preprint arXiv:2408.07465, 2024", "abstract": "Prompt optimization is essential for enhancing the performance of Large Language Models (LLMs) in a range of Natural Language Processing (NLP) tasks, particularly in scenarios of few-shot learning where training examples are incorporated directly \u2026"}, {"title": "Understanding Defects in Generated Codes by Language Models", "link": "https://arxiv.org/pdf/2408.13372", "details": "AM Esfahani, N Kahani, SA Ajila - arXiv preprint arXiv:2408.13372, 2024", "abstract": "This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the \u2026"}, {"title": "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws", "link": "https://arxiv.org/pdf/2408.08310", "details": "R Li, Y Wei, M Zhang, N Yu, H Hu, H Peng - arXiv preprint arXiv:2408.08310, 2024", "abstract": "High-quality data is crucial for the pre-training performance of large language models. Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity. In \u2026"}, {"title": "MobileQuant: Mobile-friendly Quantization for On-device Language Models", "link": "https://arxiv.org/pdf/2408.13933", "details": "F Tan, R Lee, \u0141 Dudziak, SX Hu, S Bhattacharya\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have revolutionized language processing, delivering outstanding results across multiple applications. However, deploying LLMs on edge devices poses several challenges with respect to memory, energy, and compute \u2026"}, {"title": "KnowledgeFMath: A Knowledge-Intensive Math Reasoning Dataset in Finance Domains", "link": "https://aclanthology.org/2024.acl-long.693.pdf", "details": "Y Zhao, H Liu, Y Long, R Zhang, C Zhao, A Cohan - \u2026 of the 62nd Annual Meeting of \u2026, 2024", "abstract": "We introduce KnowledgeFMath, a novel benchmark designed to evaluate LLMs' capabilities in solving knowledge-intensive math reasoning problems. Compared to prior works, this study features three core advancements. First, KnowledgeFMath \u2026"}, {"title": "Self-Harmonized Chain of Thought", "link": "https://arxiv.org/pdf/2409.04057", "details": "Z Jin, W Lu - arXiv preprint arXiv:2409.04057, 2024", "abstract": "Chain-of-Thought (CoT) prompting reveals that large language models are capable of performing complex reasoning via intermediate steps. CoT prompting is primarily categorized into three approaches. The first approach utilizes straightforward \u2026"}, {"title": "Persona is a Double-edged Sword: Enhancing the Zero-shot Reasoning by Ensembling the Role-playing and Neutral Prompts", "link": "https://arxiv.org/pdf/2408.08631", "details": "J Kim, N Yang, K Jung - arXiv preprint arXiv:2408.08631, 2024", "abstract": "Recent studies demonstrate that prompting an appropriate role-playing persona to an LLM improves its reasoning capability. However, assigning a proper persona is difficult since an LLM's performance is extremely sensitive to assigned prompts; \u2026"}, {"title": "LLMEmbed: Rethinking Lightweight LLM's Genuine Function in Text Classification", "link": "https://aclanthology.org/2024.acl-long.433.pdf", "details": "CL ChunLiu, H Zhang, K Zhao, X Ju, L Yang - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "With the booming of Large Language Models (LLMs), prompt-learning has become a promising method mainly researched in various research areas. Recently, many attempts based on prompt-learning have been made to improve the performance of \u2026"}, {"title": "Importance Weighting Can Help Large Language Models Self-Improve", "link": "https://arxiv.org/pdf/2408.09849", "details": "C Jiang, C Chan, W Xue, Q Liu, Y Guo - arXiv preprint arXiv:2408.09849, 2024", "abstract": "Large language models (LLMs) have shown remarkable capability in numerous tasks and applications. However, fine-tuning LLMs using high-quality datasets under external supervision remains prohibitively expensive. In response, LLM self \u2026"}]
