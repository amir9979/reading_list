[{"title": "Training Compute-Optimal Protein Language Models", "link": "https://www.biorxiv.org/content/10.1101/2024.06.06.597716.full.pdf", "details": "X Cheng, B Chen, P Li, J Gong, J Tang, L Song - bioRxiv, 2024", "abstract": "We explore optimally training protein language models, an area of significant interest in biological research where guidance on best practices is limited. Most models are trained with extensive compute resources until performance gains plateau, focusing \u2026"}, {"title": "DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning", "link": "https://arxiv.org/pdf/2406.17659", "details": "X Zhang, Z Altaweel, Y Hayamizu, Y Ding, S Amiri\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language models (VLMs) have been applied to robot task planning problems, where the robot receives a task in natural language and generates plans based on visual inputs. While current VLMs have demonstrated strong vision-language \u2026"}, {"title": "Information Guided Regularization for Fine-tuning Language Models", "link": "https://arxiv.org/pdf/2406.14005", "details": "M Sharma, N Muralidhar, S Xu, RB Yosuf\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The pretraining-fine-tuning paradigm has been the de facto strategy for transfer learning in modern language modeling. With the understanding that task adaptation in LMs is often a function of parameters shared across tasks, we argue that a more \u2026"}, {"title": "ViGLUE: A Vietnamese General Language Understanding Benchmark and Analysis of Vietnamese Language Models", "link": "https://aclanthology.org/2024.findings-naacl.261.pdf", "details": "MN Tran, PV Nguyen, L Nguyen, D Dien - Findings of the Association for \u2026, 2024", "abstract": "As the number of language models has increased, various benchmarks have been suggested to assess the proficiency of the models in natural language understanding. However, there is a lack of such a benchmark in Vietnamese due to \u2026"}, {"title": "From Basic to Extra Features: Hypergraph Transformer Pretrain-then-Finetuning for Balanced Clinical Predictions on EHR", "link": "https://arxiv.org/pdf/2406.05682", "details": "R Xu, Y Lu, C Liu, Y Chen, Y Sun, X Hu, JC Ho, C Yang - arXiv preprint arXiv \u2026, 2024", "abstract": "Electronic Health Records (EHRs) contain rich patient information and are crucial for clinical research and practice. In recent years, deep learning models have been applied to EHRs, but they often rely on massive features, which may not be readily \u2026"}, {"title": "Towards a Personal Health Large Language Model", "link": "https://arxiv.org/pdf/2406.06474", "details": "J Cosentino, A Belyaeva, X Liu, NA Furlotte, Z Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In health, most large language model (LLM) research has focused on clinical tasks. However, mobile and wearable devices, which are rarely integrated into such tasks, provide rich, longitudinal data for personal health monitoring. Here we present \u2026"}, {"title": "Large Language Model as a Universal Clinical Multi-task Decoder", "link": "https://arxiv.org/pdf/2406.12738", "details": "Y Wu, H Song, J Zhang, X Wen, S Zheng, J Bian - arXiv preprint arXiv:2406.12738, 2024", "abstract": "The development of effective machine learning methodologies for enhancing the efficiency and accuracy of clinical systems is crucial. Despite significant research efforts, managing a plethora of diversified clinical tasks and adapting to emerging \u2026"}, {"title": "ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks", "link": "https://aclanthology.org/2024.findings-naacl.85.pdf", "details": "X Yu, H Cheng, X Liu, D Roth, J Gao - Findings of the Association for Computational \u2026, 2024", "abstract": "Despite remarkable advancements in mitigating hallucinations in large language models (LLMs) by retrieval augmentation, it remains challenging to measure the reliability of LLMs using static question-answering (QA) data. Specifically, given the \u2026"}, {"title": "Large Language Models are Interpretable Learners", "link": "https://arxiv.org/pdf/2406.17224", "details": "R Wang, S Si, F Yu, D Wiesmann, CJ Hsieh, I Dhillon - arXiv preprint arXiv \u2026, 2024", "abstract": "The trade-off between expressiveness and interpretability remains a core challenge when building human-centric predictive models for classification and decision- making. While symbolic rules offer interpretability, they often lack expressiveness \u2026"}]
