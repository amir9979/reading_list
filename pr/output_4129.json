[{"title": "MeshFeat: Multi-Resolution Features for Neural Fields on Meshes", "link": "https://arxiv.org/pdf/2407.13592", "details": "M Mahajan, F Hofherr, D Cremers - arXiv preprint arXiv:2407.13592, 2024", "abstract": "Parametric feature grid encodings have gained significant attention as an encoding approach for neural fields since they allow for much smaller MLPs, which significantly decreases the inference time of the models. In this work, we propose \u2026"}, {"title": "XeroPol: Emotion-Aware Contrastive Learning for Zero-Shot Cross-Lingual Politeness Identification in Dialogues", "link": "https://ieeexplore.ieee.org/abstract/document/10601659/", "details": "P Priya, M Firdaus, A Ekbal - IEEE Transactions on Computational Social Systems, 2024", "abstract": "Politeness is key to successful conversations. It depicts the behavior that is socially valued and is often accompanied by emotions. Previously, researchers have focused on detecting politeness in goal-oriented conversations in high-resource English \u2026"}, {"title": "Applicability of large language models and generative models for legal case judgement summarization", "link": "https://arxiv.org/pdf/2407.12848", "details": "A Deroy, K Ghosh, S Ghosh - Artificial Intelligence and Law, 2024", "abstract": "Automatic summarization of legal case judgements, which are known to be long and complex, has traditionally been tried via extractive summarization models. In recent years, generative models including abstractive summarization models and Large \u2026"}, {"title": "Unicoder: Scaling code large language model via universal code", "link": "https://arxiv.org/pdf/2406.16441", "details": "T Sun, L Chai, J Yang, Y Yin, H Guo, J Liu, B Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various downstream natural language processing (NLP) tasks. When applying LLMs for code generation, recent works mainly focus on \u2026"}, {"title": "AutoFlow: Automated Workflow Generation for Large Language Model Agents", "link": "https://arxiv.org/pdf/2407.12821", "details": "Z Li, S Xu, K Mei, W Hua, B Rama, O Raheja, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in Large Language Models (LLMs) have shown significant progress in understanding complex natural language. One important application of LLM is LLM-based AI Agent, which leverages the ability of LLM as well as external \u2026"}, {"title": "Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors", "link": "https://arxiv.org/pdf/2407.09136", "details": "N Daheim, J Macina, M Kapur, I Gurevych, M Sachan - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) present an opportunity to scale high-quality personalized education to all. A promising approach towards this means is to build dialog tutoring models that scaffold students' problem-solving. However, even though \u2026"}]
