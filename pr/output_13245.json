[{"title": "Chitrarth: Bridging Vision and Language for a Billion People", "link": "https://arxiv.org/pdf/2502.15392", "details": "S Khan, A Tarun, A Ravi, A Faraz, A Patidar, PK Pokala\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent multimodal foundation models are primarily trained on English or high resource European language data, which hinders their applicability to other medium and low-resource languages. To address this limitation, we introduce Chitrarth \u2026"}, {"title": "Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models", "link": "https://arxiv.org/pdf/2502.15639", "details": "A Sundar, S Williamson, K Metcalf, BJ Theobald\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally \u2026"}, {"title": "The Multilingual Mind: A Survey of Multilingual Reasoning in Language Models", "link": "https://arxiv.org/pdf/2502.09457", "details": "A Ghosh, D Datta, S Saha, C Agarwal - arXiv preprint arXiv:2502.09457, 2025", "abstract": "While reasoning and multilingual capabilities in Language Models (LMs) have achieved remarkable progress in recent years, their integration into a unified paradigm, multilingual reasoning, is at a nascent stage. Multilingual reasoning \u2026"}, {"title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback", "link": "https://arxiv.org/pdf/2502.15027", "details": "HH Zhao, W Pei, Y Tao, H Mei, MZ Shou - arXiv preprint arXiv:2502.15027, 2025", "abstract": "Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied \u2026"}, {"title": "Diversity-driven Data Selection for Language Model Tuning through Sparse Autoencoder", "link": "https://arxiv.org/pdf/2502.14050", "details": "X Yang, S Nie, L Liu, S Gururangan, U Karn, R Hou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Current pre-trained large language models typically need instruction tuning to align with human preferences. However, instruction tuning data is often quantity-saturated due to the large volume of data collection and fast model iteration, leaving coreset \u2026"}, {"title": "The Relationship Between Reasoning and Performance in Large Language Models--o3 (mini) Thinks Harder, Not Longer", "link": "https://arxiv.org/pdf/2502.15631", "details": "M Ballon, A Algaba, V Ginis - arXiv preprint arXiv:2502.15631, 2025", "abstract": "Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token \u2026"}, {"title": "Transferring Textual Preferences to Vision-Language Understanding through Model Merging", "link": "https://arxiv.org/pdf/2502.13487", "details": "CA Li, TH Lin, YN Chen, H Lee - arXiv preprint arXiv:2502.13487, 2025", "abstract": "Large vision-language models (LVLMs) perform outstandingly across various multimodal tasks. However, their ability to evaluate generated content remains limited, and training vision-language reward models (VLRMs) with preference data is \u2026"}, {"title": "Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models", "link": "https://arxiv.org/pdf/2502.15086", "details": "Y In, W Kim, K Yoon, S Kim, M Tanjim, K Kim, C Park - arXiv preprint arXiv \u2026, 2025", "abstract": "As the use of large language model (LLM) agents continues to grow, their safety vulnerabilities have become increasingly evident. Extensive benchmarks evaluate various aspects of LLM safety by defining the safety relying heavily on general \u2026"}, {"title": "SAFEERASER: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning", "link": "https://arxiv.org/pdf/2502.12520", "details": "J Chen, Z Deng, K Zheng, Y Yan, S Liu, PJ Wu, P Jiang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As Multimodal Large Language Models (MLLMs) develop, their potential security issues have become increasingly prominent. Machine Unlearning (MU), as an effective strategy for forgetting specific knowledge in training data, has been widely \u2026"}]
