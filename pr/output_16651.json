[{"title": "Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced Logical Recommendation", "link": "https://arxiv.org/pdf/2505.10940", "details": "Q Yu, X Wang, S Liu, Y Bai, X Yang, X Wang, C Meng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recommender systems filter contents/items valuable to users by inferring preferences from user features and historical behaviors. Mainstream approaches follow the learning-to-rank paradigm, which focus on discovering and modeling item \u2026", "entry_id": "http://arxiv.org/abs/2505.10940v2", "updated": "2025-05-20 06:58:19", "published": "2025-05-16 07:26:41", "authors": "Qing Yu;Xiaobei Wang;Shuchang Liu;Yandong Bai;Xiaoyu Yang;Xueliang Wang;Chang Meng;Shanshan Wu;Hailan Yang;Huihui Xiao;Xiang Li;Fan Yang;Xiaoqiang Feng;Lantao Hu;Han Li;Kun Gai;Lixin Zou", "summary": "Recommender systems filter contents/items valuable to users by inferring\npreferences from user features and historical behaviors. Mainstream approaches\nfollow the learning-to-rank paradigm, which focus on discovering and modeling\nitem topics (e.g., categories), and capturing user preferences on these topics\nbased on historical interactions. However, this paradigm often neglects the\nmodeling of user characteristics and their social roles, which are logical\nconfounders influencing the correlated interest and user preference transition.\nTo bridge this gap, we introduce the user role identification task and the\nbehavioral logic modeling task that aim to explicitly model user roles and\nlearn the logical relations between item topics and user social roles. We show\nthat it is possible to explicitly solve these tasks through an efficient\nintegration framework of Large Language Model (LLM) and recommendation systems,\nfor which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal)\nLLM's world knowledge and logic inference ability to extract realistic\ntag-based virtual logic graphs that reveal dynamic and expressive knowledge of\nusers, refining our understanding of user behaviors. On the other hand, TagCF\npresents empirically effective integration modules that take advantage of the\nextracted tag-logic information, augmenting the recommendation performance. We\nconduct both online experiments and offline experiments with industrial and\npublic datasets as verification of TagCF's effectiveness, and we empirically\nshow that the user role modeling strategy is potentially a better choice than\nthe modeling of item topics. Additionally, we provide evidence that the\nextracted logic graphs are empirically a general and transferable knowledge\nthat can benefit a wide range of recommendation tasks.", "comment": null, "journal_ref": null, "primary_category": "cs.IR", "categories": "cs.IR;cs.AI", "links": "http://arxiv.org/abs/2505.10940v2;http://arxiv.org/pdf/2505.10940v2", "pdf_url": "http://arxiv.org/pdf/2505.10940v2"}, {"title": "LLM-driven Sentiment Analysis for Predicting Customer Insights in Enterprise Systems: a Computational Design Science Approach", "link": "https://aisel.aisnet.org/ecis2025/ent_system/ent_system/3/", "details": "MSH Mukta, N Islam - 2025", "abstract": "In recent times, there has been a significant rethinking of enterprise management, which is increasingly based on customer insight. Customer insight management is crucial to facing today\u2019s business challenges. The adoption and application of \u2026"}, {"title": "Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports", "link": "https://arxiv.org/pdf/2505.10586", "details": "PA Nemkova, SO Polat, RI Jahan, SR Choudhury\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Timely and accurate situation awareness is vital for decision-making in humanitarian response, conflict monitoring, and early warning and early action. However, the manual analysis of vast and heterogeneous data sources often results in delays \u2026", "entry_id": "http://arxiv.org/abs/2505.10586v1", "updated": "2025-05-14 16:36:30", "published": "2025-05-14 16:36:30", "authors": "Poli A. Nemkova;Suleyman O. Polat;Rafid I. Jahan;Sagnik Ray Choudhury;Sun-joo Lee;Shouryadipta Sarkar;Mark V. Albert", "summary": "Timely and accurate situation awareness is vital for decision-making in\nhumanitarian response, conflict monitoring, and early warning and early action.\nHowever, the manual analysis of vast and heterogeneous data sources often\nresults in delays, limiting the effectiveness of interventions. This paper\nintroduces a dynamic Retrieval-Augmented Generation (RAG) system that\nautonomously generates situation awareness reports by integrating real-time\ndata from diverse sources, including news articles, conflict event databases,\nand economic indicators. Our system constructs query-specific knowledge bases\non demand, ensuring timely, relevant, and accurate insights.\n  To ensure the quality of generated reports, we propose a three-level\nevaluation framework that combines semantic similarity metrics, factual\nconsistency checks, and expert feedback. The first level employs automated NLP\nmetrics to assess coherence and factual accuracy. The second level involves\nhuman expert evaluation to verify the relevance and completeness of the\nreports. The third level utilizes LLM-as-a-Judge, where large language models\nprovide an additional layer of assessment to ensure robustness. The system is\ntested across multiple real-world scenarios, demonstrating its effectiveness in\nproducing coherent, insightful, and actionable reports. By automating report\ngeneration, our approach reduces the burden on human analysts and accelerates\ndecision-making processes. To promote reproducibility and further research, we\nopenly share our code and evaluation tools with the community via GitHub.", "comment": null, "journal_ref": null, "primary_category": "cs.CY", "categories": "cs.CY;cs.CL", "links": "http://arxiv.org/abs/2505.10586v1;http://arxiv.org/pdf/2505.10586v1", "pdf_url": "http://arxiv.org/pdf/2505.10586v1"}, {"title": "Evaluations at Work: Measuring the Capabilities of GenAI in Use", "link": "https://arxiv.org/pdf/2505.10742", "details": "B Lepine, G Weerantunga, J Kim, P Mishkin, M Beane - arXiv preprint arXiv \u2026, 2025", "abstract": "Current AI benchmarks miss the messy, multi-turn nature of human-AI collaboration. We present an evaluation framework that decomposes real-world tasks into interdependent subtasks, letting us track both LLM performance and users' \u2026", "entry_id": "http://arxiv.org/abs/2505.10742v1", "updated": "2025-05-15 23:06:23", "published": "2025-05-15 23:06:23", "authors": "Brandon Lepine;Gawesha Weerantunga;Juho Kim;Pamela Mishkin;Matthew Beane", "summary": "Current AI benchmarks miss the messy, multi-turn nature of human-AI\ncollaboration. We present an evaluation framework that decomposes real-world\ntasks into interdependent subtasks, letting us track both LLM performance and\nusers' strategies across a dialogue. Complementing this framework, we develop a\nsuite of metrics, including a composite usage derived from semantic similarity,\nword overlap, and numerical matches; structural coherence; intra-turn\ndiversity; and a novel measure of the \"information frontier\" reflecting the\nalignment between AI outputs and users' working knowledge. We demonstrate our\nmethodology in a financial valuation task that mirrors real-world complexity.\nOur empirical findings reveal that while greater integration of LLM-generated\ncontent generally enhances output quality, its benefits are moderated by\nfactors such as response incoherence, excessive subtask diversity, and the\ndistance of provided information from users' existing knowledge. These results\nsuggest that proactive dialogue strategies designed to inject novelty may\ninadvertently undermine task performance. Our work thus advances a more\nholistic evaluation of human-AI collaboration, offering both a robust\nmethodological framework and actionable insights for developing more effective\nAI-augmented work processes.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.HC", "links": "http://arxiv.org/abs/2505.10742v1;http://arxiv.org/pdf/2505.10742v1", "pdf_url": "http://arxiv.org/pdf/2505.10742v1"}, {"title": "MergeBench: A Benchmark for Merging Domain-Specialized LLMs", "link": "https://arxiv.org/pdf/2505.10833", "details": "Y He, S Zeng, Y Hu, R Yang, T Zhang, H Zhao - arXiv preprint arXiv:2505.10833, 2025", "abstract": "\u2026 The five categories of tasks are carefully selected with the following criteria: i) Broad coverage and relevance: The tasks should be widely adopted in **LLM** **evaluation** , and covers a wide range of skills obtained through training [12, 18]. ii) \u2026", "entry_id": "http://arxiv.org/abs/2505.10833v1", "updated": "2025-05-16 04:02:55", "published": "2025-05-16 04:02:55", "authors": "Yifei He;Siqi Zeng;Yuzheng Hu;Rui Yang;Tong Zhang;Han Zhao", "summary": "Model merging provides a scalable alternative to multi-task training by\ncombining specialized finetuned models through parameter arithmetic, enabling\nefficient deployment without the need for joint training or access to all task\ndata. While recent methods have shown promise, existing evaluations are limited\nin both model scale and task diversity, leaving open questions about their\napplicability to large, domain-specialized LLMs. To tackle the challenges, we\nintroduce MergeBench, a comprehensive evaluation suite designed to assess model\nmerging at scale. MergeBench builds on state-of-the-art open-source language\nmodels, including Llama and Gemma families at 2B to 9B scales, and covers five\nkey domains: instruction following, mathematics, multilingual understanding,\ncoding and safety. We standardize finetuning and evaluation protocols, and\nassess eight representative merging methods across multi-task performance,\nforgetting and runtime efficiency. Based on extensive experiments, we provide\npractical guidelines for algorithm selection and share insights showing that\nmodel merging tends to perform better on stronger base models, with techniques\nsuch as merging coefficient tuning and sparsification improving knowledge\nretention. However, several challenges remain, including the computational cost\non large models, the gap for in-domain performance compared to multi-task\nmodels, and the underexplored role of model merging in standard LLM training\npipelines. We hope MergeBench provides a foundation for future research to\nadvance the understanding and practical application of model merging. We open\nsource our code at\n\\href{https://github.com/uiuctml/MergeBench}{https://github.com/uiuctml/MergeBench}.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.10833v1;http://arxiv.org/pdf/2505.10833v1", "pdf_url": "http://arxiv.org/pdf/2505.10833v1"}, {"title": "MatTools: Benchmarking Large Language Models for Materials Science Tools", "link": "https://arxiv.org/pdf/2505.10852", "details": "S Liu, J Xu, B Ye, B Hu, DJ Srolovitz, T Wen - arXiv preprint arXiv:2505.10852, 2025", "abstract": "\u2026 Negative impacts While MatTools advances **LLM** **evaluation** for scientific tool usage, potential negative impacts include computational resource inequalities between institutions, risks of errors in automatically generated tasks and \u2026", "entry_id": "http://arxiv.org/abs/2505.10852v1", "updated": "2025-05-16 04:43:05", "published": "2025-05-16 04:43:05", "authors": "Siyu Liu;Jiamin Xu;Beilin Ye;Bo Hu;David J. Srolovitz;Tongqi Wen", "summary": "Large language models (LLMs) are increasingly applied to materials science\nquestions, including literature comprehension, property prediction, materials\ndiscovery and alloy design. At the same time, a wide range of physics-based\ncomputational approaches have been developed in which materials properties can\nbe calculated. Here, we propose a benchmark application to evaluate the\nproficiency of LLMs to answer materials science questions through the\ngeneration and safe execution of codes based on such physics-based\ncomputational materials science packages. MatTools is built on two\ncomplementary components: a materials simulation tool question-answer (QA)\nbenchmark and a real-world tool-usage benchmark. We designed an automated\nmethodology to efficiently collect real-world materials science tool-use\nexamples. The QA benchmark, derived from the pymatgen (Python Materials\nGenomics) codebase and documentation, comprises 69,225 QA pairs that assess the\nability of an LLM to understand materials science tools. The real-world\nbenchmark contains 49 tasks (138 subtasks) requiring the generation of\nfunctional Python code for materials property calculations. Our evaluation of\ndiverse LLMs yields three key insights: (1)Generalists outshine\nspecialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a\nstandardized framework for assessing and improving LLM capabilities for\nmaterials science tool applications, facilitating the development of more\neffective AI systems for materials science and general scientific research.", "comment": "27 pages, 23 figures", "journal_ref": null, "primary_category": "cond-mat.mtrl-sci", "categories": "cond-mat.mtrl-sci;cs.CL;cs.DB", "links": "http://arxiv.org/abs/2505.10852v1;http://arxiv.org/pdf/2505.10852v1", "pdf_url": "http://arxiv.org/pdf/2505.10852v1"}, {"title": "Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking", "link": "https://arxiv.org/pdf/2505.11065", "details": "C Li, Y Shi, C Wang, Q Duan, R Ruan, W Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have demonstrated notable capabilities across financial tasks, including financial report summarization, earnings call transcript analysis, and asset classification. However, their real-world effectiveness in \u2026", "entry_id": "http://arxiv.org/abs/2505.11065v1", "updated": "2025-05-16 10:00:56", "published": "2025-05-16 10:00:56", "authors": "Changlun Li;Yao Shi;Chen Wang;Qiqi Duan;Runke Ruan;Weijie Huang;Haonan Long;Lijun Huang;Yuyu Luo;Nan Tang", "summary": "Large Language Models (LLMs) have demonstrated notable capabilities across\nfinancial tasks, including financial report summarization, earnings call\ntranscript analysis, and asset classification. However, their real-world\neffectiveness in managing complex fund investment remains inadequately\nassessed. A fundamental limitation of existing benchmarks for evaluating\nLLM-driven trading strategies is their reliance on historical back-testing,\ninadvertently enabling LLMs to \"time travel\"-leveraging future information\nembedded in their training corpora, thus resulting in possible information\nleakage and overly optimistic performance estimates. To address this issue, we\nintroduce DeepFund, a live fund benchmark tool designed to rigorously evaluate\nLLM in real-time market conditions. Utilizing a multi-agent architecture,\nDeepFund connects directly with real-time stock market data-specifically data\npublished after each model pretraining cutoff-to ensure fair and leakage-free\nevaluations. Empirical tests on nine flagship LLMs from leading global\ninstitutions across multiple investment dimensions-including ticker-level\nanalysis, investment decision-making, portfolio management, and risk\ncontrol-reveal significant practical challenges. Notably, even cutting-edge\nmodels such as DeepSeek-V3 and Claude-3.7-Sonnet incur net trading losses\nwithin DeepFund real-time evaluation environment, underscoring the present\nlimitations of LLMs for active fund management. Our code is available at\nhttps://github.com/HKUSTDial/DeepFund.", "comment": "21 pages, 9 figures", "journal_ref": null, "primary_category": "cs.CE", "categories": "cs.CE;cs.AI;cs.MA", "links": "http://arxiv.org/abs/2505.11065v1;http://arxiv.org/pdf/2505.11065v1", "pdf_url": "http://arxiv.org/pdf/2505.11065v1"}, {"title": "Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models", "link": "https://arxiv.org/pdf/2505.11010", "details": "J Wu, C Wang, TH Su, J Yang, H Lin, C Zhang, M Peng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The effectiveness of large language models (LLMs) in conversational AI is hindered by their reliance on single-turn supervised fine-tuning (SFT) data, which limits contextual coherence in multi-turn dialogues. Existing methods for generating multi-turn \u2026", "entry_id": "http://arxiv.org/abs/2505.11010v1", "updated": "2025-05-16 08:59:07", "published": "2025-05-16 08:59:07", "authors": "Jiangxu Wu;Cong Wang;TianHuang Su;Jun Yang;Haozhi Lin;Chao Zhang;Ming Peng;Kai Shi;SongPan Yang;BinQing Pan;ZiXian Li;Ni Yang;ZhenYu Yang", "summary": "The effectiveness of large language models (LLMs) in conversational AI is\nhindered by their reliance on single-turn supervised fine-tuning (SFT) data,\nwhich limits contextual coherence in multi-turn dialogues. Existing methods for\ngenerating multi-turn dialogue data struggle to ensure both diversity and\nquality in instructions. To address this, we propose Review-Instruct, a novel\nframework that synthesizes multi-turn conversations through an iterative\n\"Ask-Respond-Review\" process involving three agent roles: a Candidate, multiple\nReviewers, and a Chairman. The framework iteratively refines instructions by\nincorporating Reviewer feedback, enhancing dialogue diversity and difficulty.\nWe construct a multi-turn dataset using the Alpaca dataset and fine-tune the\nLLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate\nsignificant improvements, achieving absolute gains of 2.9\\% on MMLU-Pro and 2\\%\non MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B.\nAblation studies confirm the critical role of the Review stage and the use of\nmultiple Reviewers in boosting instruction diversity and difficulty. Our work\nhighlights the potential of review-driven, multi-agent frameworks for\ngenerating high-quality conversational data at scale.", "comment": "ACL2025 Accepted", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.11010v1;http://arxiv.org/pdf/2505.11010v1", "pdf_url": "http://arxiv.org/pdf/2505.11010v1"}, {"title": "Bridging AI and Business: Are Large Language Models Good Management Consultants?", "link": "https://aisel.aisnet.org/ecis2025/ai_org/ai_org/3/", "details": "S Malberg, N Grigorev, VL Rein, B Borne Bass\u2026 - 2025", "abstract": "We present an evaluation of ten state-of-the-art large language models (LLMs) taking the role of management consultants. The assessment is performed in a simulated interview setting where the LLMs are tasked with solving case studies\u2013a \u2026"}]
