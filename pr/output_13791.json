[{"title": "MMTP: Meta-learning-based Multi-Textual Prompt Tuning for Visual-Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10888476/", "details": "F Sun, J Zhu, Z Fan, Y Li, Z Wang, K Yang - ICASSP 2025-2025 IEEE International \u2026, 2025", "abstract": "Pre-trained Visual-Language Models (VLMs) have demonstrated powerful performance on various downstream tasks. Recently, many prompt tuning methods represented by Context Optimization (CoOp) have effectively adapted VLMs to few \u2026"}, {"title": "Towards Zero-shot Cross-lingual SLU with Syntax-aware Multi-view Contrastive Learning", "link": "https://ieeexplore.ieee.org/iel8/10887540/10887541/10889521.pdf", "details": "Y Xie, Z Xiong, T Zhang, M Cui, Y Li, Z Huang, Z Zhu - ICASSP 2025-2025 IEEE \u2026, 2025", "abstract": "Recent state-of-the-art zero-shot cross-lingual spoken language understanding (SLU) models utilize contrastive learning to achieve multilingual semantics alignment between the original utterance and code-switched counterpart. Despite achieving \u2026"}, {"title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06788", "details": "H Diao, X Li, Y Cui, Y Wang, H Deng, T Pan, W Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient \u2026"}, {"title": "Contrastive Learning via Randomly Generated Deep Supervision", "link": "https://ieeexplore.ieee.org/abstract/document/10890867/", "details": "S Wang, Z Ma, KH Chan, Y Liu, T Tong, Q Gao, G Zhai\u2026 - ICASSP 2025-2025 IEEE \u2026, 2025", "abstract": "Unsupervised visual representation learning has gained significant attention in the computer vision community, driven by recent advancements in contrastive learning. Most existing contrastive learning frameworks rely on instance discrimination as a \u2026"}, {"title": "Noise is an Efficient Learner for Zero-Shot Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06019", "details": "R Imam, A Hanif, J Zhang, KW Dawoud\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, test-time adaptation has garnered attention as a method for tuning models without labeled data. The conventional modus operandi for adapting pre-trained vision-language models (VLMs) during test-time primarily focuses on tuning \u2026"}, {"title": "Large-scale benchmarking and boosting transfer learning for medical image analysis", "link": "https://www.sciencedirect.com/science/article/pii/S1361841525000350", "details": "MRH Taher, F Haghighi, MB Gotway, J Liang - Medical Image Analysis, 2025", "abstract": "Transfer learning, particularly fine-tuning models pretrained on photographic images to medical images, has proven indispensable for medical image analysis. There are numerous models with distinct architectures pretrained on various datasets using \u2026"}, {"title": "Enhancing Chest X-ray Classification through Knowledge Injection in Cross-Modality Learning", "link": "https://arxiv.org/pdf/2502.13447", "details": "Y Yan, B Yue, Q Li, M Huang, J Chen, Z Lan - ICASSP 2025-2025 IEEE International \u2026, 2025", "abstract": "The integration of artificial intelligence in medical imaging has shown tremendous potential, yet the relationship between pre-trained knowledge and performance in cross-modality learning remains unclear. This study investigates how explicitly \u2026"}, {"title": "Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding", "link": "https://arxiv.org/pdf/2502.11492", "details": "KH Huang, C Qin, H Qiu, P Laban, S Joty, C Xiong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision Language Models (VLMs) have achieved remarkable progress in multimodal tasks, yet they often struggle with visual arithmetic, seemingly simple capabilities like object counting or length comparison, which are essential for relevant complex tasks \u2026"}, {"title": "Anatomical grounding pre-training for medical phrase grounding", "link": "https://arxiv.org/pdf/2502.16585", "details": "W Zhang, S Chandra, A Nicolson - arXiv preprint arXiv:2502.16585, 2025", "abstract": "Medical Phrase Grounding (MPG) maps radiological findings described in medical reports to specific regions in medical images. The primary obstacle hindering progress in MPG is the scarcity of annotated data available for training and \u2026"}]
