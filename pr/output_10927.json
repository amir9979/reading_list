[{"title": "CPLLM: Clinical prediction with large language models", "link": "https://journals.plos.org/digitalhealth/article%3Fid%3D10.1371/journal.pdig.0000680", "details": "O Ben Shoham, N Rappoport - PLOS Digital Health, 2024", "abstract": "We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for predicting clinical disease and readmission. We utilized quantization and fine-tuned the LLM using \u2026"}, {"title": "Copyright-Protected Language Generation via Adaptive Model Fusion", "link": "https://arxiv.org/pdf/2412.06619%3F", "details": "J Abad, K Donhauser, F Pinto, F Yang - arXiv preprint arXiv:2412.06619, 2024", "abstract": "The risk of language models reproducing copyrighted material from their training data has led to the development of various protective measures. Among these, inference-time strategies that impose constraints via post-processing have shown \u2026"}, {"title": "Modeling and predicting single-cell multi-gene perturbation responses with scLAMBDA", "link": "https://www.biorxiv.org/content/biorxiv/early/2024/12/08/2024.12.04.626878.full.pdf", "details": "G Wang, T Liu, J Zhao, Y Cheng, H Zhao - bioRxiv, 2024", "abstract": "Understanding cellular responses to genetic perturbations is essential for understanding gene regulation and phenotype formation. While high-throughput single-cell RNA-sequencing has facilitated detailed profiling of heterogeneous \u2026"}, {"title": "Training large language models to reason in a continuous latent space", "link": "https://arxiv.org/pdf/2412.06769%3F", "details": "S Hao, S Sukhbaatar, DJ Su, X Li, Z Hu, J Weston\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) are restricted to reason in the\" language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may \u2026"}, {"title": "Minerva LLMs: The first family of Large Language Models trained from scratch on Italian data", "link": "https://ceur-ws.org/Vol-3878/76_main_long.pdf", "details": "R Orlando, L Moroni, PLH Cabot, E Barba, S Conia\u2026 - Proc. of CLiC-it, 2024", "abstract": "The growing interest in Large Language Models (LLMs) has accelerated research efforts to adapt these models for various languages. Despite this, pretraining LLMs from scratch for non-English languages remains underexplored. This is the case for \u2026"}, {"title": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models", "link": "https://arxiv.org/pdf/2412.02830", "details": "H Tran, Z Yao, J Wang, Y Zhang, Z Yang, H Yu - arXiv preprint arXiv:2412.02830, 2024", "abstract": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a versatile extension to the mutual reasoning framework (rStar), aimed at enhancing reasoning accuracy and factual integrity across large language models (LLMs) for \u2026"}, {"title": "QAPyramid: Fine-grained Evaluation of Content Selection for Text Summarization", "link": "https://arxiv.org/pdf/2412.07096", "details": "S Zhang, D Wan, A Cattan, A Klein, I Dagan, M Bansal - arXiv preprint arXiv \u2026, 2024", "abstract": "How to properly conduct human evaluations for text summarization is a longstanding challenge. The Pyramid human evaluation protocol, which assesses content selection by breaking the reference summary into sub-units and verifying their \u2026"}, {"title": "Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning", "link": "https://arxiv.org/pdf/2412.15797", "details": "S Park, X Liu, Y Gong, E Choi - arXiv preprint arXiv:2412.15797, 2024", "abstract": "Despite recent advances in large language models, open-source models often struggle to consistently perform well on complex reasoning tasks. Existing ensemble methods, whether applied at the token or output levels, fail to address these \u2026"}]
