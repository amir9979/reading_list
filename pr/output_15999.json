[{"title": "Parameter-efficient fine-tuning in large language models: a survey of methodologies", "link": "https://link.springer.com/article/10.1007/s10462-025-11236-4", "details": "L Wang, S Chen, L Jiang, S Pan, R Cai, S Yang\u2026 - Artificial Intelligence Review, 2025", "abstract": "The large language models, as predicted by scaling law forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the \u2026"}, {"title": "Leveraging Generative Pre-trained Transformer (GPT) Large Language Models (LLMs) For Interstitial Lung Diseases (ILD) Clinical Research", "link": "https://www.atsjournals.org/doi/abs/10.1164/ajrccm.2025.211.Abstracts.A2086", "details": "S Chen, MV Maddali, C Bluethgen, CP Langlotz, R Raj - American Journal of \u2026, 2025", "abstract": "Rationale: The majority of clinically relevant data is contained in unstructured text such as clinical notes. ILD notes are particularly prone to verbosity and imprecision, making structured data extraction a major bottleneck for clinical research and a costly \u2026"}]
