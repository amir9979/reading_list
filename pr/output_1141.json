'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Multi-hop Question Answering over Knowledge Graphs usi'
[{"title": "Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering", "link": "https://arxiv.org/pdf/2404.10193", "details": "Z Khan, Y Fu - arXiv preprint arXiv:2404.10193, 2024", "abstract": "The goal of selective prediction is to allow an a model to abstain when it may not be able to deliver a reliable prediction, which is important in safety-critical contexts. Existing approaches to selective prediction typically require access to the internals of \u2026"}, {"title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2404.05567", "details": "B Pan, Y Shen, H Liu, M Mishra, G Zhang, A Oliva\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) language models can reduce computational costs by 2- 4$\\times $ compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios. However, MoE models generally \u2026"}, {"title": "Can Language Models Solve Olympiad Programming?", "link": "https://arxiv.org/pdf/2404.10952", "details": "Q Shi, M Tang, K Narasimhan, S Yao - arXiv preprint arXiv:2404.10952, 2024", "abstract": "Computing olympiads contain some of the most challenging problems for humans, requiring complex algorithmic reasoning, puzzle solving, in addition to generating efficient code. However, it has been understudied as a domain to evaluate language \u2026"}, {"title": "FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback", "link": "https://arxiv.org/pdf/2404.05046", "details": "L Jing, X Du - arXiv preprint arXiv:2404.05046, 2024", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated proficiency in tackling a variety of visual-language tasks. However, current LVLMs suffer from misalignment between text and image modalities which causes three kinds of hallucination \u2026"}, {"title": "Is Table Retrieval a Solved Problem? Join-Aware Multi-Table Retrieval", "link": "https://arxiv.org/pdf/2404.09889", "details": "PB Chen, Y Zhang, D Roth - arXiv preprint arXiv:2404.09889, 2024", "abstract": "Retrieving relevant tables containing the necessary information to accurately answer a given question over tables is critical to open-domain question-answering (QA) systems. Previous methods assume the answer to such a question can be found \u2026"}, {"title": "FSRPCL: Privacy-preserve federated social relationship prediction with contrastive learning", "link": "https://ieeexplore.ieee.org/iel7/5971803/10367774/10520033.pdf", "details": "H Liu, N Li, H Kou, S Meng, Q Li - Tsinghua Science and Technology, 2024", "abstract": "Cross-Platform Social Relationship Prediction (CPSRP) aims to utilize users' data information on multiple platforms to enhance the performance of social relationship prediction, thereby promoting socioeconomic development. Due to the highly \u2026"}, {"title": "Self-playing Adversarial Language Game Enhances LLM Reasoning", "link": "https://arxiv.org/pdf/2404.10642", "details": "P Cheng, T Hu, H Xu, Z Zhang, Y Dai, L Han, N Du - arXiv preprint arXiv:2404.10642, 2024", "abstract": "We explore the self-play training procedure of large language models (LLMs) in a two-player adversarial language game called Adversarial Taboo. In this game, an attacker and a defender communicate with respect to a target word only visible to the \u2026"}, {"title": "FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations", "link": "https://arxiv.org/pdf/2404.06619", "details": "J Dwivedi-Yu, R Dwivedi, T Schick - arXiv preprint arXiv:2404.06619, 2024", "abstract": "The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes \u2026"}, {"title": "Measuring Cross-lingual Transfer in Bytes", "link": "https://arxiv.org/pdf/2404.08191", "details": "LR de Souza, TS Almeida, R Lotufo, R Nogueira - arXiv preprint arXiv:2404.08191, 2024", "abstract": "Multilingual pretraining has been a successful solution to the challenges posed by the lack of resources for languages. These models can transfer knowledge to target languages with minimal or no examples. Recent research suggests that monolingual \u2026"}]
