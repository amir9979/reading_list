[{"title": "EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models", "link": "https://arxiv.org/pdf/2502.06663", "details": "X Xing, Z Liu, S Xiao, B Gao, Y Liang, W Zhang, H Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Modern large language models (LLMs) driven by scaling laws, achieve intelligence emergency in large model sizes. Recently, the increasing concerns about cloud costs, latency, and privacy make it an urgent requirement to develop compact edge \u2026"}, {"title": "DeepThink: Aligning Language Models with Domain-Specific User Intents", "link": "https://arxiv.org/pdf/2502.05497", "details": "Y Li, M Luo, Y Gong, C Lin, J Jiao, Y Liu, K Huang - arXiv preprint arXiv:2502.05497, 2025", "abstract": "Supervised fine-tuning with synthesized instructions has been a common practice for adapting LLMs to domain-specific QA tasks. However, the synthesized instructions deviate from real user questions and expected answers. This study proposes a novel \u2026"}, {"title": "Scaling Embedding Layers in Language Models", "link": "https://arxiv.org/pdf/2502.01637%3F", "details": "D Yu, E Cohen, B Ghazi, Y Huang, P Kamath, R Kumar\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We propose SCONE ($\\textbf {S} $ calable, $\\textbf {C} $ ontextualized, $\\textbf {O} $ ffloaded, $\\textbf {N} $-gram $\\textbf {E} $ mbedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To \u2026"}, {"title": "Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models", "link": "https://arxiv.org/pdf/2502.08130", "details": "S Gupta, Y Nandwani, A Yehudai, D Khandelwal\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task \u2026"}, {"title": "HuDEx: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM responses", "link": "https://arxiv.org/pdf/2502.08109", "details": "S Lee, H Lee, S Heo, W Choi - arXiv preprint arXiv:2502.08109, 2025", "abstract": "Recent advances in large language models (LLMs) have shown promising improvements, often surpassing existing methods across a wide range of downstream tasks in natural language processing. However, these models still face \u2026"}, {"title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models", "link": "https://arxiv.org/pdf/2502.04404", "details": "XW Yang, XY Zhu, WD Wei, DC Zhang, JJ Shao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including \u2026"}, {"title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06788", "details": "H Diao, X Li, Y Cui, Y Wang, H Deng, T Pan, W Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient \u2026"}, {"title": "Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration", "link": "https://arxiv.org/pdf/2502.01969", "details": "Y Zhu, L Tao, M Dong, C Xu - arXiv preprint arXiv:2502.01969, 2025", "abstract": "Large Vision-Language Models (LVLMs) exhibit impressive multimodal reasoning capabilities but remain highly susceptible to object hallucination, where models generate responses that are not factually aligned with the visual content. Recent \u2026"}, {"title": "Mordal: Automated Pretrained Model Selection for Vision Language Models", "link": "https://arxiv.org/pdf/2502.00241", "details": "S He, I Jang, M Chowdhury - arXiv preprint arXiv:2502.00241, 2025", "abstract": "Incorporating multiple modalities into large language models (LLMs) is a powerful way to enhance their understanding of non-textual data, enabling them to perform multimodal tasks. Vision language models (VLMs) form the fastest growing category \u2026"}]
