[{"title": "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models", "link": "https://arxiv.org/pdf/2406.18510", "details": "L Jiang, K Rao, S Han, A Ettinger, F Brahman, S Kumar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce WildTeaming, an automatic LLM safety red-teaming framework that mines in-the-wild user-chatbot interactions to discover 5.7 K unique clusters of novel jailbreak tactics, and then composes multiple tactics for systematic exploration of \u2026"}, {"title": "MFC-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2406.11288", "details": "S Wang, H Lin, Z Luo, Z Ye, G Chen, J Ma - arXiv preprint arXiv:2406.11288, 2024", "abstract": "Large vision-language models (LVLMs) have significantly improved multimodal reasoning tasks, such as visual question answering and image captioning. These models embed multimodal facts within their parameters, rather than relying on \u2026"}, {"title": "LIONs: An Empirically Optimized Approach to Align Language Models", "link": "https://arxiv.org/pdf/2407.06542", "details": "X Yu, Q Wu, Y Li, Z Yu - arXiv preprint arXiv:2407.06542, 2024", "abstract": "Alignment is a crucial step to enhance the instruction-following and conversational abilities of language models. Despite many recent work proposing new algorithms, datasets, and training pipelines, there is a lack of comprehensive studies measuring \u2026"}, {"title": "Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?", "link": "https://arxiv.org/pdf/2406.13121", "details": "J Lee, A Chen, Z Dai, D Dua, DS Sachan, M Boratko\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. Leveraging LCLMs' ability to natively ingest and process entire corpora of \u2026"}, {"title": "ICLEval: Evaluating In-Context Learning Ability of Large Language Models", "link": "https://arxiv.org/pdf/2406.14955", "details": "W Chen, Y Lin, ZH Zhou, HY Huang, Y Jia, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) is a critical capability of Large Language Models (LLMs) as it empowers them to comprehend and reason across interconnected inputs. Evaluating the ICL ability of LLMs can enhance their utilization and deepen our \u2026"}, {"title": "Universal Approximation Theory: The basic theory for large language models", "link": "https://arxiv.org/pdf/2407.00958", "details": "W Wang, Q Li - arXiv preprint arXiv:2407.00958, 2024", "abstract": "Language models have emerged as a critical area of focus in artificial intelligence, particularly with the introduction of groundbreaking innovations like ChatGPT. Large- scale Transformer networks have quickly become the leading approach for \u2026"}, {"title": "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models", "link": "https://arxiv.org/pdf/2407.01009", "details": "J Pan, Y Zhang, C Zhang, Z Liu, H Wang, H Li - arXiv preprint arXiv:2407.01009, 2024", "abstract": "Large language models (LLMs) have demonstrated emergent capabilities across diverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However, such a simple and fast COT approach often encounters limitations in dealing with \u2026"}, {"title": "Protecting Privacy Through Approximating Optimal Parameters for Sequence Unlearning in Language Models", "link": "https://arxiv.org/pdf/2406.14091", "details": "D Lee, D Rim, M Choi, J Choo - arXiv preprint arXiv:2406.14091, 2024", "abstract": "Although language models (LMs) demonstrate exceptional capabilities on various tasks, they are potentially vulnerable to extraction attacks, which represent a significant privacy risk. To mitigate the privacy concerns of LMs, machine unlearning \u2026"}, {"title": "ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models", "link": "https://arxiv.org/pdf/2406.13342", "details": "H Jo, H Lee, T Park - arXiv preprint arXiv:2406.13342, 2024", "abstract": "The recent advancements in large language models (LLMs) have brought significant progress in solving NLP tasks. Notably, in-context learning (ICL) is the key enabling mechanism for LLMs to understand specific tasks and grasping nuances. In this \u2026"}]
