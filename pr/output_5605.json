[{"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "link": "https://arxiv.org/pdf/2408.00724", "details": "Y Wu, Z Sun, S Li, S Welleck, Y Yang - arXiv preprint arXiv:2408.00724, 2024", "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth \u2026"}, {"title": "CoDi: Conversational Distillation for Grounded Question Answering", "link": "https://arxiv.org/pdf/2408.11219", "details": "P Huber, A Einolghozati, R Conway, K Narang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Distilling conversational skills into Small Language Models (SLMs) with approximately 1 billion parameters presents significant challenges. Firstly, SLMs have limited capacity in their model parameters to learn extensive knowledge \u2026"}, {"title": "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "link": "https://arxiv.org/pdf/2408.02103", "details": "P Wang, X Wang, C Lou, S Mao, P Xie, Y Jiang - arXiv preprint arXiv:2408.02103, 2024", "abstract": "In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language \u2026"}, {"title": "Fine-tuning Language Models for Joint Rewriting and Completion of Code with Potential Bugs", "link": "https://aclanthology.org/2024.findings-acl.938.pdf", "details": "D Wang, J Zhao, H Pei, S Tan, S Zha - Findings of the Association for Computational \u2026, 2024", "abstract": "Handling drafty partial code remains a notable challenge in real-time code suggestion applications. Previous work has demonstrated shortcomings of large language models of code (CodeLLMs) in completing partial code with potential bugs \u2026"}, {"title": "Interactive-T2S: Multi-Turn Interactions for Text-to-SQL with Large Language Models", "link": "https://arxiv.org/pdf/2408.11062", "details": "G Xiong, J Bao, H Jiang, Y Song, W Zhao - arXiv preprint arXiv:2408.11062, 2024", "abstract": "This study explores text-to-SQL parsing by leveraging the powerful reasoning capabilities of large language models (LLMs). Despite recent advancements, existing LLM-based methods have not adequately addressed scalability, leading to \u2026"}, {"title": "Cognitive Assessment of Language Models", "link": "https://openreview.net/pdf%3Fid%3DpxRh1meUvN", "details": "D McDuff, D Munday, X Liu, I Galatzer-Levy - ICML 2024 Workshop on LLMs and Cognition", "abstract": "Large language models (LLMs) are a subclass of generative artificial intelligence that can interpret language inputs to generate novel responses. These capabilities are conceptualized as a significant step forward in artificial intelligence because the \u2026"}, {"title": "UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling", "link": "https://arxiv.org/pdf/2408.04810", "details": "H Al-Tahan, Q Garrido, R Balestriero, D Bouchacourt\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Significant research efforts have been made to scale and improve vision-language model (VLM) training approaches. Yet, with an ever-growing number of benchmarks, researchers are tasked with the heavy burden of implementing each protocol \u2026"}, {"title": "Chain of Condition: Construct, Verify and Solve Conditions for Conditional Question Answering", "link": "https://arxiv.org/pdf/2408.05442", "details": "J Lin, Y Lai, Y Feng - arXiv preprint arXiv:2408.05442, 2024", "abstract": "Conditional question answering (CQA) is an important task that aims to find probable answers and identify conditions that need to be satisfied to support the answer. Existing approaches struggle with CQA due to two main challenges:(1) precisely \u2026"}, {"title": "Boosting Large Language Models with Socratic Method for Conversational Mathematics Teaching", "link": "https://arxiv.org/pdf/2407.17349", "details": "Y Ding, H Hu, J Zhou, Q Chen, B Jiang, L He - arXiv preprint arXiv:2407.17349, 2024", "abstract": "With the introduction of large language models (LLMs), automatic math reasoning has seen tremendous success. However, current methods primarily focus on providing solutions or using techniques like Chain-of-Thought to enhance problem \u2026"}]
