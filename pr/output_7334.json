[{"title": "Evaluation of Large Language Model Performance on the Biomedical Language Understanding and Reasoning Benchmark: Comparative Study", "link": "https://www.medrxiv.org/content/10.1101/2024.05.17.24307411.pdf", "details": "H Feng, F Ronzano, J LaFleur, M Garber, R de Oliveira\u2026", "abstract": "Background: The availability of increasingly powerful large language models (LLMs) has attracted substantial interest in their potential for interpreting and generating human-like text for biomedical and clinical applications. However, there are often \u2026"}, {"title": "Rethinking Semantic Parsing for Large Language Models: Enhancing LLM Performance with Semantic Hints", "link": "https://arxiv.org/pdf/2409.14469", "details": "K An, S Si, H Hu, H Zhao, Y Wang, Q Guo, B Chang - arXiv preprint arXiv:2409.14469, 2024", "abstract": "Semantic Parsing aims to capture the meaning of a sentence and convert it into a logical, structured form. Previous studies show that semantic parsing enhances the performance of smaller models (eg, BERT) on downstream tasks. However, it \u2026"}, {"title": "Enhancing Text-to-SQL Capabilities of Large Language Models via Domain Database Knowledge Injection", "link": "https://arxiv.org/pdf/2409.15907", "details": "X Ma, X Tian, L Wu, X Wang, X Tang, J Wang - arXiv preprint arXiv:2409.15907, 2024", "abstract": "Text-to-SQL is a subtask in semantic parsing that has seen rapid progress with the evolution of Large Language Models (LLMs). However, LLMs face challenges due to hallucination issues and a lack of domain-specific database knowledge (such as \u2026"}, {"title": "Predicting and analyzing memorization within fine-tuned Large Language Models", "link": "https://arxiv.org/pdf/2409.18858", "details": "J Dentan, D Buscaldi, A Shabou, S Vanier - arXiv preprint arXiv:2409.18858, 2024", "abstract": "Large Language Models have received significant attention due to their abilities to solve a wide range of complex tasks. However these models memorize a significant proportion of their training data, posing a serious threat when disclosed at inference \u2026"}, {"title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models", "link": "https://arxiv.org/pdf/2409.18943", "details": "J Li, L Zhang, Y Li, Z Liu, R Luo, L Chen, M Yang - arXiv preprint arXiv:2409.18943, 2024", "abstract": "The instruction-following ability of large language models enables humans to interact with AI agents in a natural way. However, when required to generate responses of a specific length, large language models often struggle to meet users' needs due to \u2026"}]
