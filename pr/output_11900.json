[{"title": "Hierarchical Autoregressive Transformers: Combining Byte-~ and Word-Level Processing for Robust, Adaptable Language Models", "link": "https://arxiv.org/pdf/2501.10322", "details": "P Neitemeier, B Deiseroth, C Eichenberg, L Balles - arXiv preprint arXiv:2501.10322, 2025", "abstract": "Tokenization is a fundamental step in natural language processing, breaking text into units that computational models can process. While learned subword tokenizers have become the de-facto standard, they present challenges such as large \u2026"}, {"title": "HiMix: Reducing Computational Complexity in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2501.10318", "details": "X Zhang, D Li, B Liu, Z Bao, Y Zhou, B Yang, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Benefiting from recent advancements in large language models and modality alignment techniques, existing Large Vision-Language Models (LVLMs) have achieved prominent performance across a wide range of scenarios. However, the \u2026"}, {"title": "Clinical entity augmented retrieval for clinical information extraction", "link": "https://www.nature.com/articles/s41746-024-01377-1", "details": "I Lopez, A Swaminathan, K Vedula, S Narayanan\u2026 - npj Digital Medicine, 2025", "abstract": "Large language models (LLMs) with retrieval-augmented generation (RAG) have improved information extraction over previous methods, yet their reliance on embeddings often leads to inefficient retrieval. We introduce CLinical Entity \u2026"}]
