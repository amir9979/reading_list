[{"title": "Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination", "link": "https://arxiv.org/pdf/2409.12746", "details": "ES Salido, R Morante, J Gonzalo, G Marco\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this article we present UNED-ACCESS 2024, a bilingual dataset that consists of 1003 multiple-choice questions of university entrance level exams in Spanish and English. Questions are originally formulated in Spanish and translated manually into \u2026"}, {"title": "Egalitarian Language Representation in Language Models: It All Begins with Tokenizers", "link": "https://arxiv.org/pdf/2409.11501", "details": "M Velayuthan, K Sarveswaran - arXiv preprint arXiv:2409.11501, 2024", "abstract": "Tokenizers act as a bridge between human language and the latent space of language models, influencing how language is represented in these models. Due to the immense popularity of English-Centric Large Language Models (LLMs), efforts \u2026"}, {"title": "Probabilistic Active Few-Shot Learning in Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3DsSX9wLMSJT", "details": "A Baumann, M Klasson, R Li, A Solin, M Trapp - NeurIPS 2024 Workshop on Bayesian \u2026", "abstract": "Pre-trained vision-language models (VLMs) have shown to be an useful model class for zero-and few-shot learning tasks. In this work, we investigate probabilistic active few-shot learning in VLMs by leveraging post-hoc uncertainty estimation and \u2026"}, {"title": "Generative Chain-of-Thought for Zero-Shot Cognitive Reasoning", "link": "https://link.springer.com/chapter/10.1007/978-3-031-72344-5_22", "details": "L Liu, D Zhang, S Zhu, S Li - International Conference on Artificial Neural Networks, 2024", "abstract": "Cognitive reasoning holds a significant place within the field of Natural Language Processing (NLP). Yet, the exploration of zero-shot scenarios, which align more closely with real-life situations than supervised scenarios, has been relatively limited \u2026"}, {"title": "Investigating Layer Importance in Large Language Models", "link": "https://arxiv.org/pdf/2409.14381", "details": "Y Zhang, Y Dong, K Kawaguchi - arXiv preprint arXiv:2409.14381, 2024", "abstract": "Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in \u2026"}, {"title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "link": "https://arxiv.org/pdf/2409.13853", "details": "Z Wang, R Bao, Y Wu, J Taylor, C Xiao, F Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pretrained large language models (LLMs) have revolutionized natural language processing (NLP) tasks such as summarization, question answering, and translation. However, LLMs pose significant security risks due to their tendency to memorize \u2026"}, {"title": "From Linguistic Giants to Sensory Maestros: A Survey on Cross-Modal Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2409.18996", "details": "S Qian, Z Zhou, D Xue, B Wang, C Xu - arXiv preprint arXiv:2409.18996, 2024", "abstract": "Cross-modal reasoning (CMR), the intricate process of synthesizing and drawing inferences across divergent sensory modalities, is increasingly recognized as a crucial capability in the progression toward more sophisticated and anthropomorphic \u2026"}, {"title": "Search for Efficient Large Language Models", "link": "https://arxiv.org/pdf/2409.17372", "details": "X Shen, P Zhao, Y Gong, Z Kong, Z Zhan, Y Wu, M Lin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have long held sway in the realms of artificial intelligence research. Numerous efficient techniques, including weight pruning, quantization, and distillation, have been embraced to compress LLMs, targeting \u2026"}]
