[{"title": "Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination", "link": "https://arxiv.org/pdf/2409.12746", "details": "ES Salido, R Morante, J Gonzalo, G Marco\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this article we present UNED-ACCESS 2024, a bilingual dataset that consists of 1003 multiple-choice questions of university entrance level exams in Spanish and English. Questions are originally formulated in Spanish and translated manually into \u2026"}, {"title": "Understanding Defects in Generated Codes by Language Models", "link": "https://arxiv.org/pdf/2408.13372", "details": "AM Esfahani, N Kahani, SA Ajila - arXiv preprint arXiv:2408.13372, 2024", "abstract": "This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the \u2026"}, {"title": "Training Language Models to Self-Correct via Reinforcement Learning", "link": "https://arxiv.org/pdf/2409.12917", "details": "A Kumar, V Zhuang, R Agarwal, Y Su, JD Co-Reyes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Existing approaches for training self-correction either require multiple models or rely on a \u2026"}, {"title": "Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning", "link": "https://arxiv.org/pdf/2408.17150", "details": "X Qu, J Sun, W Wei, Y Cheng - arXiv preprint arXiv:2408.17150, 2024", "abstract": "Recently, Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities in multi-modal context comprehension. However, they still suffer from hallucination problems referring to generating inconsistent outputs with the image \u2026"}, {"title": "DP-MemArc: Differential Privacy Transfer Learning for Memory Efficient Language Models", "link": "https://www.researchgate.net/profile/Yanming-Liu-16/publication/383395255_DP-MemArc_Differential_Privacy_Transfer_Learning_for_Memory_Efficient_Language_Models/links/66ca3a35c2eaa5002314bfbf/DP-MemArc-Differential-Privacy-Transfer-Learning-for-Memory-Efficient-Language-Models.pdf", "details": "Y Liu, X Peng, Y Zhang, X Ke, S Deng, J Cao, C Ma\u2026", "abstract": "Large language models have repeatedly shown outstanding performance across diverse applications. However, deploying these models can inadvertently risk user privacy. The significant memory demands during training pose a major challenge in \u2026"}, {"title": "Small Language Models are Equation Reasoners", "link": "https://arxiv.org/pdf/2409.12393", "details": "B Kim, K Lee, J Kim, S Lee - arXiv preprint arXiv:2409.12393, 2024", "abstract": "Chain-of-Thought (CoT) reasoning has enabled Large Language Model (LLM) to achieve remarkable performance in various NLP tasks, including arithmetic problem- solving. However, this success does not generalize to small language model (sLM) \u2026"}, {"title": "On Robustness-Accuracy Characterization of Language Models using Synthetic Datasets", "link": "https://openreview.net/pdf%3Fid%3DC0j44uRPcl", "details": "CY Ko, PY Chen, P Das, YS Chuang, L Daniel - First Conference on Language \u2026, 2024", "abstract": "In recent years, language models (LMs) that were pretrained at scale on diverse data have proven to be a successful approach for solving different downstream tasks. However, new concerns about proper performance evaluation have been raised \u2026"}, {"title": "Enhancing Few-Shot Transfer Learning with Optimized Multi-Task Prompt Tuning through Modular Prompt Composition", "link": "https://arxiv.org/pdf/2408.13227", "details": "A Pouramini, H Faili - arXiv preprint arXiv:2408.13227, 2024", "abstract": "In recent years, multi-task prompt tuning has garnered considerable attention for its inherent modularity and potential to enhance parameter-efficient transfer learning across diverse tasks. This paper aims to analyze and improve the performance of \u2026"}, {"title": "Counterfactual Explanations for Clustering Models", "link": "https://arxiv.org/pdf/2409.12632", "details": "A Spagnol, K Sokol, P Barbiero, M Langheinrich\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Clustering algorithms rely on complex optimisation processes that may be difficult to comprehend, especially for individuals who lack technical expertise. While many explainable artificial intelligence techniques exist for supervised machine learning \u2026"}]
