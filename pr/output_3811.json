[{"title": "Media Bias Detection Across Families of Language Models", "link": "https://aclanthology.org/2024.naacl-long.227.pdf", "details": "I Maab, E Marrese-Taylor, S Pad\u00f3, Y Matsuo - Proceedings of the 2024 Conference of \u2026, 2024", "abstract": "Bias in reporting can influence the public's opinion on relevant societal issues. Examples include informational bias (selective presentation of content) and lexical bias (specific framing of content through linguistic choices). The recognition of media \u2026"}, {"title": "Developing a natural language processing system using transformer-based models for adverse drug event detection in electronic health records", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/07/10/2024.07.09.24310100.full.pdf", "details": "J Wu, X Ruan, E McNeer, KM Rossow, L Choi - medRxiv, 2024", "abstract": "Objective: To develop a transformer-based natural language processing (NLP) system for detecting adverse drug events (ADEs) from clinical notes in electronic health records (EHRs). Materials and Methods: We fine-tuned BERT Short-Formers \u2026"}, {"title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models", "link": "https://aclanthology.org/2024.findings-naacl.18.pdf", "details": "Z Su, Z Lin, B Baixue, H Chen, S Hu, W Zhou, G Ding\u2026 - Findings of the Association \u2026, 2024", "abstract": "Generative language models are usually pre-trained on large text corpus via predicting the next token (ie, sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language \u2026"}, {"title": "Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language Models", "link": "https://arxiv.org/pdf/2407.07263", "details": "J Parmar, S Satheesh, M Patwary, M Shoeybi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As language models have scaled both their number of parameters and pretraining dataset sizes, the computational cost for pretraining has become intractable except for the most well-resourced teams. This increasing cost makes it ever more important \u2026"}, {"title": "Fuzzy Multi-view Graph Learning on Sparse Electronic Health Records", "link": "https://ieeexplore.ieee.org/abstract/document/10572354/", "details": "T Tang, Z Han, S Yu, A Bagirov, Q Zhang - IEEE Transactions on Fuzzy Systems, 2024", "abstract": "Extracting latent disease patterns from electronic health records (EHRs) is a crucial solution for disease analysis, significantly facilitating healthcare decision-making. Multiview learning presents itself as a promising approach that offers a \u2026"}, {"title": "Benchmarking Children's ASR with Supervised and Self-supervised Speech Foundation Models", "link": "https://arxiv.org/pdf/2406.10507", "details": "R Fan, NB Shankar, A Alwan - arXiv preprint arXiv:2406.10507, 2024", "abstract": "Speech foundation models (SFMs) have achieved state-of-the-art results for various speech tasks in supervised (eg Whisper) or self-supervised systems (eg WavLM). However, the performance of SFMs for child ASR has not been systematically \u2026"}, {"title": "Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination", "link": "https://ieeecai.org/2024/wp-content/pdfs/540900a486/540900a486.pdf", "details": "X Zhong, K Batmanghelich, L Sun", "abstract": "Vision-language models pre-trained on large scale of unlabeled biomedical images and associated reports learn generalizable semantic representations. These multi- modal representations can benefit various downstream tasks in the biomedical \u2026"}, {"title": "Mental Modeling of Reinforcement Learning Agents by Language Models", "link": "https://arxiv.org/pdf/2406.18505", "details": "W Lu, X Zhao, J Spisak, JH Lee, S Wermter - arXiv preprint arXiv:2406.18505, 2024", "abstract": "Can emergent language models faithfully model the intelligence of decision-making agents? Though modern language models exhibit already some reasoning ability, and theoretically can potentially express any probable distribution over tokens, it \u2026"}, {"title": "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "link": "https://arxiv.org/pdf/2407.02118", "details": "W Zheng, W Pan, X Xu, L Qin, L Yue, M Zhou - arXiv preprint arXiv:2407.02118, 2024", "abstract": "In recent years, Large Language Models (LLMs) have made significant strides towards Artificial General Intelligence. However, training these models from scratch requires substantial computational resources and vast amounts of text data. In this \u2026"}]
