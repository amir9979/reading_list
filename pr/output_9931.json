[{"title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step", "link": "https://arxiv.org/pdf/2411.10440%3F", "details": "G Xu, P Jin, L Hao, Y Song, L Sun, L Yuan - arXiv preprint arXiv:2411.10440, 2024", "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to \u2026"}, {"title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information", "link": "https://arxiv.org/pdf/2412.00947", "details": "R Kamoi, Y Zhang, SSS Das, RH Zhang, R Zhang - arXiv preprint arXiv:2412.00947, 2024", "abstract": "Errors in understanding visual information in images (ie, visual perception errors) remain a major source of mistakes in Large Vision Language Models (LVLMs). While further analysis is essential, there is a deficiency in datasets for evaluating the visual \u2026"}, {"title": "Visual cot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought reasoning", "link": "https://openreview.net/pdf%3Fid%3DaXeiCbMFFJ", "details": "H Shao, S Qian, H Xiao, G Song, Z Zong, L Wang, Y Liu\u2026 - The Thirty-eight Conference \u2026, 2024", "abstract": "Multi-Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However, they often lack interpretability and struggle with complex visual inputs, especially when the resolution of the input image \u2026"}, {"title": "ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for Large Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3DPyTf2jj0SH", "details": "S Liu, K Ying, H Zhang, Y Yang, Y Lin, T Zhang, C Li\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Multi-turn visual conversation is an important ability of real-world AI assistants. However, the related evaluation benchmark is missed. This paper presents ConvBench, a multi-turn conversation benchmark with hierarchical capabilities \u2026"}, {"title": "Enhancing Instruction-Following Capability of Visual-Language Models by Reducing Image Redundancy", "link": "https://arxiv.org/pdf/2411.15453", "details": "T Yang, J Jia, X Zhu, W Zhao, B Wang, Y Cheng, Y Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have strong instruction-following capability to interpret and execute tasks as directed by human commands. Multimodal Large Language Models (MLLMs) have inferior instruction-following ability compared to \u2026"}, {"title": "Entity Matching with Large Language Models as Weak and Strong Labellers", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3D-78xEQAAQBAJ%26oi%3Dfnd%26pg%3DPA58%26ots%3DMaNrsDU6uI%26sig%3Ddq4VnqF7qXNKDWyvzY4O9G-Ki9c", "details": "H Du, N Hurley, A Lawlor - New Trends in Database and Information Systems \u2026", "abstract": "A number of recent studies have shown that pre-trained large language models (LLMs) display highly competitive performance on entity matching tasks while acting in a zero shot manner, thus reducing the need for labelled training data. However \u2026"}, {"title": "A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans", "link": "https://arxiv.org/pdf/2412.01131", "details": "Z Cao, H Yamada, S Teufel, T Tokunaga - arXiv preprint arXiv:2412.01131, 2024", "abstract": "Recently, much work has concerned itself with the enigma of what exactly PLMs (pretrained language models) learn about different aspects of language, and how they learn it. One stream of this type of research investigates the knowledge that \u2026"}, {"title": "Contrastive concept-phrase pre-training for generating clinically accurate and interpretable chest X-ray reports", "link": "https://link.springer.com/article/10.1007/s00521-024-10640-1", "details": "A Tubaishat, T Zia, D Windridge, M Nawaz, S Razzaq - Neural Computing and \u2026, 2024", "abstract": "Automated radiology report generation is an emerging field for improving patient care and alleviating radiologist workload. However, existing methods face a range of challenges such as limited data availability, clinical metric performance, and \u2026"}]
