[{"title": "Exploring Coding Spot: Understanding Parametric Contributions to LLM Coding Performance", "link": "https://arxiv.org/pdf/2412.07113", "details": "D Kim, M Kim, YC Chun, C Park, H Lim - arXiv preprint arXiv:2412.07113, 2024", "abstract": "Large Language Models (LLMs) have demonstrated notable proficiency in both code generation and comprehension across multiple programming languages. However, the mechanisms underlying this proficiency remain underexplored, particularly with \u2026"}, {"title": "Fact-Level Confidence Calibration and Self-Correction", "link": "https://arxiv.org/pdf/2411.13343", "details": "Y Yuan, B Xu, H Tan, F Sun, T Xiao, W Li, H Shen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Confidence calibration in LLMs, ie, aligning their self-assessed confidence with the actual accuracy of their responses, enabling them to self-evaluate the correctness of their outputs. However, current calibration methods for LLMs typically estimate two \u2026"}, {"title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases", "link": "https://arxiv.org/pdf/2412.04862", "details": "LG Research, S An, K Bae, E Choi, K Choi, SJ Choi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8 B, and 2.4 B. These models feature several \u2026"}, {"title": "ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models", "link": "https://arxiv.org/pdf/2412.07012", "details": "J Zhang, L Xue, L Song, J Wang, W Huang, M Shu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the rise of multimodal applications, instruction data has become critical for training multimodal language models capable of understanding complex image- based queries. Existing practices rely on powerful but costly large language models \u2026"}, {"title": "Free $^ 2$ Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.17041", "details": "J Kim, BS Kim, JC Ye - arXiv preprint arXiv:2411.17041, 2024", "abstract": "Diffusion models have achieved impressive results in generative tasks like text-to- image (T2I) and text-to-video (T2V) synthesis. However, achieving accurate text alignment in T2V generation remains challenging due to the complex temporal \u2026"}, {"title": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks", "link": "https://arxiv.org/pdf/2411.19325", "details": "MS Danish, MA Munir, SRA Shah, K Kuckreja, FS Khan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While numerous recent benchmarks focus on evaluating generic Vision-Language Models (VLMs), they fall short in addressing the unique demands of geospatial applications. Generic VLM benchmarks are not designed to handle the complexities \u2026"}, {"title": "ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models", "link": "https://arxiv.org/pdf/2412.00447", "details": "X Ye, Y Gan, Y Ge, XP Zhang, Y Tang - arXiv preprint arXiv:2412.00447, 2024", "abstract": "Large Vision Language Models (LVLMs) have achieved significant success across multi-modal tasks. However, the computational cost of processing long visual tokens can be prohibitively expensive on resource-limited devices. Previous methods have \u2026"}, {"title": "Evaluating Vision-Language Models as Evaluators in Path Planning", "link": "https://arxiv.org/pdf/2411.18711", "details": "M Aghzal, X Yue, E Plaku, Z Yao - arXiv preprint arXiv:2411.18711, 2024", "abstract": "Despite their promise to perform complex reasoning, large language models (LLMs) have been shown to have limited effectiveness in end-to-end planning. This has inspired an intriguing question: if these models cannot plan well, can they still \u2026"}, {"title": "AutoReason: Automatic Few-Shot Reasoning Decomposition", "link": "https://arxiv.org/pdf/2412.06975", "details": "A Sevinc, A Gumus - arXiv preprint arXiv:2412.06975, 2024", "abstract": "Chain of Thought (CoT) was introduced in recent research as a method for improving step-by-step reasoning in Large Language Models. However, CoT has limited applications such as its need for hand-crafted few-shot exemplar prompts and no \u2026"}]
