[{"title": "LM2: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.920.pdf", "details": "G Juneja, S Dutta, T Chakraborty - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}, {"title": "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.313.pdf", "details": "M Xu, Y Li, K Sun, T Qian - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Large language models (LLMs) have shown excellent capability for solving reasoning problems. Existing approaches do not differentiate the question difficulty when designing prompting methods for them. Clearly, a simple method cannot elicit \u2026"}, {"title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models", "link": "https://aclanthology.org/2024.emnlp-main.566.pdf", "details": "XH Feng, C Chen, Y Li, Z Lin - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Pre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for \u2026"}, {"title": "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "link": "https://aclanthology.org/2024.emnlp-main.204.pdf", "details": "J Gu, Z Wang, Y Zhang, Z Zhang, P Gong - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "With the proliferation of large language models, Parameter Efficient Fine-Tuning (PEFT) method, which freeze pre-trained parameters and only fine-tune a few task- specific parameters, are playing an increasingly important role. However, previous \u2026"}, {"title": "Learning to Plan by Updating Natural Language", "link": "https://aclanthology.org/2024.findings-emnlp.589.pdf", "details": "Y Guo, Y Liang, C Wu, W Wu, D Zhao, N Duan - Findings of the Association for \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have shown remarkable performance in various basic natural language tasks. For completing the complex task, we still need a plan for the task to guide LLMs to generate the specific solutions step by step \u2026"}, {"title": "Fox-1 Technical Report", "link": "https://arxiv.org/pdf/2411.05281", "details": "Z Hu, J Zhang, R Pan, Z Xu, S Avestimehr, C He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Fox-1, a series of small language models (SLMs) consisting of Fox-1-1.6 B and Fox-1-1.6 B-Instruct-v0. 1. These models are pre-trained on 3 trillion tokens of web-scraped document data and fine-tuned with 5 billion tokens of instruction \u2026"}, {"title": "Factuality of Large Language Models: A Survey", "link": "https://aclanthology.org/2024.emnlp-main.1088.pdf", "details": "Y Wang, M Wang, MA Manzoor, F Liu, G Georgiev\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a \u2026"}, {"title": "Knowledge-Centric Hallucination Detection", "link": "https://aclanthology.org/2024.emnlp-main.395.pdf", "details": "X Hu, D Ru, L Qiu, Q Guo, T Zhang, Y Xu, Y Luo, P Liu\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate. This paper presents RefChecker, a framework that introduces claim-triplets to represent claims in LLM responses \u2026"}, {"title": "Skills-in-Context: Unlocking Compositionality in Large Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.812.pdf", "details": "J Chen, X Pan, D Yu, K Song, X Wang, D Yu, J Chen - Findings of the Association for \u2026, 2024", "abstract": "We investigate how to elicit compositional generalization capabilities in large language models (LLMs). Compositional generalization empowers LLMs to solve complex problems by combining foundational skills, a critical reasoning ability akin to \u2026"}]
