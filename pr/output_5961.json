[{"title": "CogVLM2: Visual Language Models for Image and Video Understanding", "link": "https://arxiv.org/pdf/2408.16500", "details": "W Hong, W Wang, M Ding, W Yu, Q Lv, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Beginning with VisualGLM and CogVLM, we are continuously exploring VLMs in pursuit of enhanced vision-language fusion, efficient higher-resolution architecture, and broader modalities and applications. Here we propose the CogVLM2 family, a \u2026"}, {"title": "MedSyn: LLM-based Synthetic Medical Text Generation Framework", "link": "https://arxiv.org/pdf/2408.02056", "details": "G Kumichev, P Blinov, Y Kuzkina, V Goncharov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating synthetic text addresses the challenge of data availability in privacy- sensitive domains such as healthcare. This study explores the applicability of synthetic data in real-world medical settings. We introduce MedSyn, a novel medical \u2026"}]
