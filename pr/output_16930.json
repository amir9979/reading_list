[{"title": "Large Language Models for Predictive Analysis: How Far Are They?", "link": "https://arxiv.org/pdf/2505.17149", "details": "Q Chen, Y Ren, X Ma, Y Shi - arXiv preprint arXiv:2505.17149, 2025", "abstract": "\u2026 The responses from the LLMs are then **evaluated** by (i) data analysis experts and (ii) LLMs. The \u2026 Our **evaluation** on PredictiQ costs 900 human hours for response **evaluation** , 72.18 million \u2026 \u2022 We propose the PredictiQ benchmark\u2014a comprehensive \u2026", "entry_id": "http://arxiv.org/abs/2505.17149v1", "updated": "2025-05-22 09:02:15", "published": "2025-05-22 09:02:15", "authors": "Qin Chen;Yuanyi Ren;Xiaojun Ma;Yuyang Shi", "summary": "Predictive analysis is a cornerstone of modern decision-making, with\napplications in various domains. Large Language Models (LLMs) have emerged as\npowerful tools in enabling nuanced, knowledge-intensive conversations, thus\naiding in complex decision-making tasks. With the burgeoning expectation to\nharness LLMs for predictive analysis, there is an urgent need to systematically\nassess their capability in this domain. However, there is a lack of relevant\nevaluations in existing studies. To bridge this gap, we introduce the\n\\textbf{PredictiQ} benchmark, which integrates 1130 sophisticated predictive\nanalysis queries originating from 44 real-world datasets of 8 diverse fields.\nWe design an evaluation protocol considering text analysis, code generation,\nand their alignment. Twelve renowned LLMs are evaluated, offering insights into\ntheir practical use in predictive analysis. Generally, we believe that existing\nLLMs still face considerable challenges in conducting predictive analysis. See\n\\href{https://github.com/Cqkkkkkk/PredictiQ}{Github}.", "comment": "Accepted to ACL 2025 Findings", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.17149v1;http://arxiv.org/pdf/2505.17149v1", "pdf_url": "http://arxiv.org/pdf/2505.17149v1"}, {"title": "Annotation of biological samples data to standard ontologies with support from **large language models**", "link": "https://www.sciencedirect.com/science/article/pii/S2001037025001837", "details": "A Riquelme-Garc\u00eda, J Mulero-Hern\u00e1ndez\u2026 - Computational and \u2026, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) have demonstrated potential in automating complex language-\u2026 We **evaluated** model performance in annotating labels to four widely used ontologies: the \u2026 Nonetheless, our **evaluation** highlights persistent \u2026"}, {"title": "Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data", "link": "https://arxiv.org/pdf/2505.17116", "details": "A Dhruv, Y Xie, J Branham, T Mallick - arXiv preprint arXiv:2505.17116, 2025", "abstract": "\u2026 This paper presents a comparative study of **large** **language** **models** (LLMs) in interpreting gridstructured geospatial data. We **evaluate** the performance of a base model through structured prompting and contrast it with a fine-tuned variant trained \u2026", "entry_id": "http://arxiv.org/abs/2505.17116v1", "updated": "2025-05-21 16:27:51", "published": "2025-05-21 16:27:51", "authors": "Akash Dhruv;Yangxinyu Xie;Jordan Branham;Tanwi Mallick", "summary": "This paper presents a comparative study of large language models (LLMs) in\ninterpreting grid-structured geospatial data. We evaluate the performance of a\nbase model through structured prompting and contrast it with a fine-tuned\nvariant trained on a dataset of user-assistant interactions. Our results\nhighlight the strengths and limitations of zero-shot prompting and demonstrate\nthe benefits of fine-tuning for structured geospatial and temporal reasoning.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.ET", "links": "http://arxiv.org/abs/2505.17116v1;http://arxiv.org/pdf/2505.17116v1", "pdf_url": "http://arxiv.org/pdf/2505.17116v1"}, {"title": "Can Large Language Models Design Biological Weapons? Evaluating Moremi Bio", "link": "https://arxiv.org/pdf/2505.17154", "details": "G Hattoh, J Ayensu, NP Ofori, S Eshun, D Akogo - arXiv preprint arXiv:2505.17154, 2025", "abstract": "\u2026 The findings from this toxicity assessment challenge claims that **large** **language** **models** (LLMs) are incapable of designing bioweapons. This reinforces concerns about the potential misuse of LLMs in biodesign, posing a significant threat to \u2026", "entry_id": "http://arxiv.org/abs/2505.17154v1", "updated": "2025-05-22 11:27:50", "published": "2025-05-22 11:27:50", "authors": "Gertrude Hattoh;Jeremiah Ayensu;Nyarko Prince Ofori;Solomon Eshun;Darlington Akogo", "summary": "Advances in AI, particularly LLMs, have dramatically shortened drug discovery\ncycles by up to 40% and improved molecular target identification. However,\nthese innovations also raise dual-use concerns by enabling the design of toxic\ncompounds. Prompting Moremi Bio Agent without the safety guardrails to\nspecifically design novel toxic substances, our study generated 1020 novel\ntoxic proteins and 5,000 toxic small molecules. In-depth computational toxicity\nassessments revealed that all the proteins scored high in toxicity, with\nseveral closely matching known toxins such as ricin, diphtheria toxin, and\ndisintegrin-based snake venom proteins. Some of these novel agents showed\nsimilarities with other several known toxic agents including disintegrin\neristostatin, metalloproteinase, disintegrin triflavin, snake venom\nmetalloproteinase, corynebacterium ulcerans toxin. Through quantitative risk\nassessments and scenario analyses, we identify dual-use capabilities in current\nLLM-enabled biodesign pipelines and propose multi-layered mitigation\nstrategies. The findings from this toxicity assessment challenge claims that\nlarge language models (LLMs) are incapable of designing bioweapons. This\nreinforces concerns about the potential misuse of LLMs in biodesign, posing a\nsignificant threat to research and development (R&D). The accessibility of such\ntechnology to individuals with limited technical expertise raises serious\nbiosecurity risks. Our findings underscore the critical need for robust\ngovernance and technical safeguards to balance rapid biotechnological\ninnovation with biosecurity imperatives.", "comment": null, "journal_ref": null, "primary_category": "q-bio.QM", "categories": "q-bio.QM;cs.AI", "links": "http://arxiv.org/abs/2505.17154v1;http://arxiv.org/pdf/2505.17154v1", "pdf_url": "http://arxiv.org/pdf/2505.17154v1"}, {"title": "GIM: Improved Interpretability for Large Language Models", "link": "https://arxiv.org/pdf/2505.17630", "details": "J Edin, R Csord\u00e1s, T Ruotsalo, Z Wu, M Maistro\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 In Table 4, we **evaluate** the **large** **language** **models** \u2019 performance on the datasets. All models perform well across datasets except for LLAMA-3.2 1B, which always predicts positive sentiment on the Movie and Twitter datasets (high recall and low \u2026", "entry_id": "http://arxiv.org/abs/2505.17630v1", "updated": "2025-05-23 08:41:45", "published": "2025-05-23 08:41:45", "authors": "Joakim Edin;R\u00f3bert Csord\u00e1s;Tuukka Ruotsalo;Zhengxuan Wu;Maria Maistro;Jing Huang;Lars Maal\u00f8e", "summary": "Ensuring faithful interpretability in large language models is imperative for\ntrustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where\nnetworks compensate for reduced signal in one component by amplifying others,\nmasking the true importance of the ablated component. While prior work\nattributes self-repair to layer normalization and back-up components that\ncompensate for ablated components, we identify a novel form occurring within\nthe attention mechanism, where softmax redistribution conceals the influence of\nimportant attention scores. This leads traditional ablation and gradient-based\nmethods to underestimate the significance of all components contributing to\nthese attention scores. We introduce Gradient Interaction Modifications (GIM),\na technique that accounts for self-repair during backpropagation. Extensive\nexperiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,\nQwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves\nfaithfulness over existing circuit identification and feature attribution\nmethods. Our work is a significant step toward better understanding the inner\nmechanisms of LLMs, which is crucial for improving them and ensuring their\nsafety. Our code is available at https://github.com/JoakimEdin/gim.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG;68T07;I.2.0; I.2.7", "links": "http://arxiv.org/abs/2505.17630v1;http://arxiv.org/pdf/2505.17630v1", "pdf_url": "http://arxiv.org/pdf/2505.17630v1"}, {"title": " **Large Language Models** and Surgical Decision-Making: **Evaluation** of Generative Unimodal AI in Facial Traumatology Practice", "link": "https://link.springer.com/article/10.1007/s12663-025-02556-7", "details": "S Benedetti, A Frosolini, L Catarzi, LA Vaira, G Consorti\u2026 - Journal of Maxillofacial and \u2026, 2025", "abstract": "\u2026 It is worth noting that the recent update of Google Bard to Gemini could potentially reduce the relevance of this research, although a direct comparison of the two **large** **language** **models** \u2019 performance in the medical field has yet to be undertaken. The \u2026"}, {"title": "ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework", "link": "https://arxiv.org/pdf/2505.18105", "details": "L Huang, Y Liu, J Jiang, R Zhang, J Yan, J Li, WX Zhao - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 a rigorous **evaluation** of our system\u2019s deep search capabilities, we introduce ORION, a benchmark for Open-web Reasoning **evaluatION** \u2026 Our **evaluation** shows that even leading closed-source systems achieve under 30% accuracy on ORION \u2026", "entry_id": "http://arxiv.org/abs/2505.18105v1", "updated": "2025-05-23 17:02:02", "published": "2025-05-23 17:02:02", "authors": "Lisheng Huang;Yichen Liu;Jinhao Jiang;Rongxiang Zhang;Jiahao Yan;Junyi Li;Wayne Xin Zhao", "summary": "Recent advances in web-augmented large language models (LLMs) have exhibited\nstrong performance in complex reasoning tasks, yet these capabilities are\nmostly locked in proprietary systems with opaque architectures. In this work,\nwe propose \\textbf{ManuSearch}, a transparent and modular multi-agent framework\ndesigned to democratize deep search for LLMs. ManuSearch decomposes the search\nand reasoning process into three collaborative agents: (1) a solution planning\nagent that iteratively formulates sub-queries, (2) an Internet search agent\nthat retrieves relevant documents via real-time web search, and (3) a\nstructured webpage reading agent that extracts key evidence from raw web\ncontent. To rigorously evaluate deep reasoning abilities, we introduce\n\\textbf{ORION}, a challenging benchmark focused on open-web reasoning over\nlong-tail entities, covering both English and Chinese. Experimental results\nshow that ManuSearch substantially outperforms prior open-source baselines and\neven surpasses leading closed-source systems. Our work paves the way for\nreproducible, extensible research in open deep search systems. We release the\ndata and code in https://github.com/RUCAIBox/ManuSearch", "comment": "LLM, Complex Search Benchmark", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.18105v1;http://arxiv.org/pdf/2505.18105v1", "pdf_url": "http://arxiv.org/pdf/2505.18105v1"}, {"title": "Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations", "link": "https://arxiv.org/pdf/2505.17136", "details": "Y Ji, S Gao, Y Nie, I Maji\u0107, K Janowicz - arXiv preprint arXiv:2505.17136, 2025", "abstract": "\u2026 when the geospatial vector data are passed to **large** **language** **models** (LLMs) including GPT-3.5-\u2026 -based, and everyday language-based **evaluation**. Our experiment results demonstrate that both \u2026 **Evaluating** the capabilities of **large** \u2026", "entry_id": "http://arxiv.org/abs/2505.17136v1", "updated": "2025-05-22 05:21:31", "published": "2025-05-22 05:21:31", "authors": "Yuhan Ji;Song Gao;Ying Nie;Ivan Maji\u0107;Krzysztof Janowicz", "summary": "Applying AI foundation models directly to geospatial datasets remains\nchallenging due to their limited ability to represent and reason with\ngeographical entities, specifically vector-based geometries and natural\nlanguage descriptions of complex spatial relations. To address these issues, we\ninvestigate the extent to which a well-known-text (WKT) representation of\ngeometries and their spatial relations (e.g., topological predicates) are\npreserved during spatial reasoning when the geospatial vector data are passed\nto large language models (LLMs) including GPT-3.5-turbo, GPT-4, and\nDeepSeek-R1-14B. Our workflow employs three distinct approaches to complete the\nspatial reasoning tasks for comparison, i.e., geometry embedding-based, prompt\nengineering-based, and everyday language-based evaluation. Our experiment\nresults demonstrate that both the embedding-based and prompt engineering-based\napproaches to geospatial question-answering tasks with GPT models can achieve\nan accuracy of over 0.6 on average for the identification of topological\nspatial relations between two geometries. Among the evaluated models, GPT-4\nwith few-shot prompting achieved the highest performance with over 0.66\naccuracy on topological spatial relation inference. Additionally, GPT-based\nreasoner is capable of properly comprehending inverse topological spatial\nrelations and including an LLM-generated geometry can enhance the effectiveness\nfor geographic entity retrieval. GPT-4 also exhibits the ability to translate\ncertain vernacular descriptions about places into formal topological relations,\nand adding the geometry-type or place-type context in prompts may improve\ninference accuracy, but it varies by instance. The performance of these spatial\nreasoning tasks offers valuable insights for the refinement of LLMs with\ngeographical knowledge towards the development of geo-foundation models capable\nof geospatial reasoning.", "comment": "33 pages, 13 figures, IJGIS GeoFM Special Issue", "journal_ref": "International Journal of Geographical Information Science, 2025\n  International Journal of Geographical Information Science International\n  Journal of Geographical Information Science", "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;I.2", "links": "http://dx.doi.org/10.1080/13658816.2025.2511227;http://arxiv.org/abs/2505.17136v1;http://arxiv.org/pdf/2505.17136v1", "pdf_url": "http://arxiv.org/pdf/2505.17136v1"}, {"title": "Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models", "link": "https://arxiv.org/pdf/2505.17119", "details": "Z Shao, X Wang, Z Liu, C Wang, KP Subbalakshmi - arXiv preprint arXiv:2505.17119, 2025", "abstract": "\u2026 Our goal in this work is to systematically **evaluate** LLM reasoning and reveal potential statistical biases. To this end, we first provide a systematic **evaluation** of the reasoning over machine-generated detection and interpretation, thus revealing potential \u2026", "entry_id": "http://arxiv.org/abs/2505.17119v1", "updated": "2025-05-21 16:30:50", "published": "2025-05-21 16:30:50", "authors": "Zongru Shao;Xin Wang;Zhanyang Liu;Chenhan Wang;K. P. Subbalakshmi", "summary": "Recent research leverages large language models (LLMs) for early mental\nhealth detection, such as depression, often optimized with machine-generated\ndata. However, their detection may be subject to unknown weaknesses. Meanwhile,\nquality control has not been applied to these generated corpora besides limited\nhuman verifications. Our goal is to systematically evaluate LLM reasoning and\nreveal potential weaknesses. To this end, we first provide a systematic\nevaluation of the reasoning over machine-generated detection and\ninterpretation. Then we use the models' reasoning abilities to explore\nmitigation strategies for enhanced performance. Specifically, we do the\nfollowing: A. Design an LLM instruction strategy that allows for systematic\nanalysis of the detection by breaking down the task into several subtasks. B.\nDesign contrastive few-shot and chain-of-thought prompts by selecting typical\npositive and negative examples of detection reasoning. C. Perform human\nannotation for the subtasks identified in the first step and evaluate the\nperformance. D. Identify human-preferred detection with desired logical\nreasoning from the few-shot generation and use them to explore different\noptimization strategies. We conducted extensive comparisons on the DepTweet\ndataset across the following subtasks: 1. identifying whether the speaker is\ndescribing their own depression; 2. accurately detecting the presence of PHQ-9\nsymptoms, and 3. finally, detecting depression. Human verification of\nstatistical outliers shows that LLMs demonstrate greater accuracy in analyzing\nand detecting explicit language of depression as opposed to implicit\nexpressions of depression. Two optimization methods are used for performance\nenhancement and reduction of the statistic bias: supervised fine-tuning (SFT)\nand direct preference optimization (DPO). Notably, the DPO approach achieves\nsignificant performance improvement.", "comment": "8 pages without references", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.17119v1;http://arxiv.org/pdf/2505.17119v1", "pdf_url": "http://arxiv.org/pdf/2505.17119v1"}]
