[{"title": "Analyze and Improve Differentially Private Federated Learning: A Model Robustness Perspective", "link": "https://ieeexplore.ieee.org/abstract/document/10802990/", "details": "S Zhang, J Huang, P Li - IEEE Transactions on Information Forensics and \u2026, 2024", "abstract": "Differentially Private Federated learning (DPFL) applies differential privacy (DP) techniques to preserve clients' privacy in Federated Learning (FL). Existing methods based on Gaussian Mechanism require the operations of model updates clipping \u2026"}, {"title": "Multilingual and Explainable Text Detoxification with Parallel Corpora", "link": "https://arxiv.org/pdf/2412.11691", "details": "D Dementieva, N Babakov, A Ronen, AA Ayele\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Even with various regulations in place across countries and social media platforms (Government of India, 2021; European Parliament and Council of the European Union, 2022, digital abusive speech remains a significant issue. One potential \u2026"}, {"title": "SE-GCL: An Event-Based Simple and Effective Graph Contrastive Learning for Text Representation", "link": "https://arxiv.org/pdf/2412.11652", "details": "T Meng, W Ai, J Li, Z Wang, Y Shou, K Li - arXiv preprint arXiv:2412.11652, 2024", "abstract": "Text representation learning is significant as the cornerstone of natural language processing. In recent years, graph contrastive learning (GCL) has been widely used in text representation learning due to its ability to represent and capture complex text \u2026"}, {"title": "Smoothed Embeddings for Robust Language Models", "link": "https://www.merl.com/publications/docs/TR2024-170.pdf", "details": "H Ryo, MRU Rashid, A Lewis, J Liu, T Koike-Akino\u2026", "abstract": "Improving the safety and reliability of large language models (LLMs) is a crucial aspect of realizing trustworthy AI systems. Although alignment methods aim to suppress harmful content generation, LLMs are often still vulnerable to jail-breaking \u2026"}, {"title": "Efficient Federated Learning with Multi-Teacher Knowledge Distillation for COVID-19 Detection", "link": "https://dl.acm.org/doi/abs/10.1145/3698587.3701396", "details": "R Annan, H Qin, X Yuan, K Roy, R Newman, L Qingge - Proceedings of the 15th ACM \u2026, 2024", "abstract": "The growing availability of COVID-19 data and advancements in AI offer potential for improved pandemic prediction and prevention. Federated Learning (FL) frameworks support collaborative, privacy-preserving COVID-19 detection, but often neglect the \u2026"}, {"title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation", "link": "https://arxiv.org/pdf/2412.11919", "details": "X Li, J Jin, Y Zhou, Y Wu, Z Li, Q Ye, Z Dou - arXiv preprint arXiv:2412.11919, 2024", "abstract": "Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations. Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several \u2026"}, {"title": "The Superalignment of Superhuman Intelligence with Large Language Models", "link": "https://arxiv.org/pdf/2412.11145", "details": "M Huang, Y Wang, S Cui, P Ke, J Tang - arXiv preprint arXiv:2412.11145, 2024", "abstract": "We have witnessed superhuman intelligence thanks to the fast development of large language models and multimodal language models. As the application of such superhuman models becomes more and more common, a critical question rises \u2026"}, {"title": "Learning to Verify Summary Facts with Fine-Grained LLM Feedback", "link": "https://arxiv.org/pdf/2412.10689", "details": "J Oh, J Choi, NHY Kim, T Yun, H Song - arXiv preprint arXiv:2412.10689, 2024", "abstract": "Training automatic summary fact verifiers often faces the challenge of a lack of human-labeled data. In this paper, we explore alternative way of leveraging Large Language Model (LLM) generated feedback to address the inherent limitation of \u2026"}]
