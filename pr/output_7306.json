[{"title": "Assessing Electronic Health Records for Describing Racial and Ethnic Health Disparities: A Research Note", "link": "https://read.dukeupress.edu/demography/article-pdf/doi/10.1215/00703370-11582088/2147349/11582088.pdf", "details": "A Limburg, J Young, TS Carey, PR Chelminski\u2026 - Demography, 2024", "abstract": "The use of data derived from electronic health records (EHRs) to describe racial and ethnic health disparities is increasingly common, but there are challenges. While the number of patients covered by EHRs can be quite large, such patients may not be \u2026"}, {"title": "Comparative Analysis of Large Language Models in Chinese Medical Named Entity Recognition", "link": "https://www.mdpi.com/2306-5354/11/10/982", "details": "Z Zhu, Q Zhao, J Li, Y Ge, X Ding, T Gu, J Zou, S Lv\u2026 - Bioengineering, 2024", "abstract": "The emergence of large language models (LLMs) has provided robust support for application tasks across various domains, such as name entity recognition (NER) in the general domain. However, due to the particularity of the medical domain, the \u2026"}, {"title": "Large Language Models for Simplified Interventional Radiology Reports: A Comparative Analysis", "link": "https://www.sciencedirect.com/science/article/pii/S1076633224006901", "details": "E Can, W Uller, K Vogt, MC Doppler, F Busch, N Bayerl\u2026 - Academic Radiology, 2024", "abstract": "Purpose To quantitatively and qualitatively evaluate and compare the performance of leading large language models (LLMs), including proprietary models (GPT-4, GPT- 3.5 Turbo, Claude-3-Opus, and Gemini Ultra) and open-source models (Mistral-7b \u2026"}, {"title": "Entity Extraction from High-Level Corruption Schemes via Large Language Models", "link": "https://arxiv.org/pdf/2409.13704", "details": "P Koletsis, PK Gemos, C Chronis, I Varlamis\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rise of financial crime that has been observed in recent years has created an increasing concern around the topic and many people, organizations and governments are more and more frequently trying to combat it. Despite the increase \u2026"}, {"title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles", "link": "https://arxiv.org/pdf/2409.10502", "details": "K Shah, N Dikkala, X Wang, R Panigrahy - arXiv preprint arXiv:2409.10502, 2024", "abstract": "Causal language modeling using the Transformer architecture has yielded remarkable capabilities in Large Language Models (LLMs) over the last few years. However, the extent to which fundamental search and reasoning capabilities \u2026"}, {"title": "A Comprehensive Analysis of Memorization in Large Language Models", "link": "https://aclanthology.org/2024.inlg-main.45.pdf", "details": "H Kiyomaru, I Sugiura, D Kawahara, S Kurohashi - Proceedings of the 17th \u2026, 2024", "abstract": "This paper presents a comprehensive study that investigates memorization in large language models (LLMs) from multiple perspectives. Experiments are conducted with the Pythia and LLM-jp model suites, both of which offer LLMs with over 10B \u2026"}, {"title": "Decoding by Factual Prompts and Hallucination Prompts Improves Factuality in Large Language Models", "link": "https://www.preprints.org/manuscript/202409.2037/download/final_file", "details": "B Lv, A Feng, C Xie - 2024", "abstract": "Although large language models demonstrate impressive capabilities, they sometimes generate irrelevant or nonsensical text, or produce outputs that deviate from the provided source input\u2014an occurrence commonly referred to as \u2026"}, {"title": "The Use of Large Language Models Tuned with Socratic Methods on the Impact of Medical Students' Learning: A Randomised Controlled Trial", "link": "https://s3.ca-central-1.amazonaws.com/assets.jmir.org/assets/preprints/preprint-57995-submitted.pdf", "details": "CL Yong, MS Furqan, JWK Lee, A Makmur\u2026", "abstract": "Abstract Background: Large Language Models (LLM) are AI models that can generate conversational content based on a trained specified source of information (corpus). Objective: The aim is to use these corpus-trained LLMs to limit the content \u2026"}]
