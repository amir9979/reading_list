[{"title": "A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models", "link": "https://arxiv.org/pdf/2502.13942", "details": "H Huang, S Yuan, Y Hao, C Wen, Y Fang - arXiv preprint arXiv:2502.13942, 2025", "abstract": "A large-scale vision and language model that has been pretrained on massive data encodes visual and linguistic prior, which makes it easier to generate images and language that are more natural and realistic. Despite this, there is still a significant \u2026"}, {"title": "Scaling Embedding Layers in Language Models", "link": "https://arxiv.org/pdf/2502.01637%3F", "details": "D Yu, E Cohen, B Ghazi, Y Huang, P Kamath, R Kumar\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We propose SCONE ($\\textbf {S} $ calable, $\\textbf {C} $ ontextualized, $\\textbf {O} $ ffloaded, $\\textbf {N} $-gram $\\textbf {E} $ mbedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To \u2026"}, {"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}, {"title": "Language Models are Few-Shot Graders", "link": "https://arxiv.org/pdf/2502.13337", "details": "C Zhao, M Silva, S Poulsen - arXiv preprint arXiv:2502.13337, 2025", "abstract": "Providing evaluations to student work is a critical component of effective student learning, and automating its process can significantly reduce the workload on human graders. Automatic Short Answer Grading (ASAG) systems, enabled by \u2026"}, {"title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers", "link": "https://arxiv.org/pdf/2501.16961%3F", "details": "M Raza, N Milic-Frayling - arXiv preprint arXiv:2501.16961, 2025", "abstract": "Robustness of reasoning remains a significant challenge for large language models, and addressing it is essential for the practical applicability of AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a novel approach that \u2026"}, {"title": "Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement", "link": "https://arxiv.org/pdf/2502.02573%3F", "details": "S Abbasloo - arXiv preprint arXiv:2502.02573, 2025", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem- solving, a crucial, ubiquitous, and complex domain. This paper explores the \u2026"}, {"title": "HaluCheck: Explainable and verifiable automation for detecting hallucinations in LLM responses", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425003343", "details": "S Heo, S Son, H Park - Expert Systems with Applications, 2025", "abstract": "Large language models have become integral to various aspects of modern life, but a critical challenge persists: hallucinations. This work contributes to expert systems research by providing a systematic framework for enhancing AI reliability and \u2026"}, {"title": "Is Conversational XAI All You Need? Human-AI Decision Making With a Conversational XAI Assistant", "link": "https://arxiv.org/pdf/2501.17546", "details": "G He, N Aishwarya, U Gadiraju - arXiv preprint arXiv:2501.17546, 2025", "abstract": "Explainable artificial intelligence (XAI) methods are being proposed to help interpret and understand how AI systems reach specific predictions. Inspired by prior work on conversational user interfaces, we argue that augmenting existing XAI methods with \u2026"}, {"title": "How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild", "link": "https://arxiv.org/pdf/2502.12769", "details": "A Lauscher, G Glava\u0161 - arXiv preprint arXiv:2502.12769, 2025", "abstract": "In the age of misinformation, hallucination--the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses--represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority \u2026"}]
