[{"title": "EHR-SeqSQL: A Sequential Text-to-SQL Dataset For Interactively Exploring Electronic Health Records", "link": "https://arxiv.org/pdf/2406.00019", "details": "J Ryu, S Cho, G Lee, E Choi - arXiv preprint arXiv:2406.00019, 2024", "abstract": "In this paper, we introduce EHR-SeqSQL, a novel sequential text-to-SQL dataset for Electronic Health Record (EHR) databases. EHR-SeqSQL is designed to address critical yet underexplored aspects in text-to-SQL parsing: interactivity \u2026"}, {"title": "Amend to Alignment: Decoupled Prompt Tuning for Mitigating Spurious Correlation in Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3Df8G2KSCSdp", "details": "J Zhang, X Ma, S Guo, P Li, W Xu, X Tang, Z Hong - Forty-first International Conference on \u2026", "abstract": "Fine-tuning the learnable prompt for a pre-trained vision-language model (VLM), such as CLIP, has demonstrated exceptional efficiency in adapting to a broad range of downstream tasks. Existing prompt tuning methods for VLMs do not distinguish \u2026"}, {"title": "Applying generative AI with retrieval augmented generation to summarize and extract key clinical information from electronic health records", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424000807", "details": "M Alkhalaf, P Yu, M Yin, C Deng - Journal of Biomedical Informatics, 2024", "abstract": "Background Malnutrition is a prevalent issue in aged care facilities (RACFs), leading to adverse health outcomes. The ability to efficiently extract key clinical information from a large volume of data in electronic health records (EHR) can improve \u2026"}, {"title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions", "link": "https://arxiv.org/pdf/2405.19744", "details": "C Li, W Yang, J Zhang, J Lu, S Wang, C Zong - arXiv preprint arXiv:2405.19744, 2024", "abstract": "Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples \u2026"}, {"title": "Artificial intelligence approaches for phenotyping heart failure in US Veterans Health Administration electronic health record", "link": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/ehf2.14787", "details": "Y Shao, S Zhang, VK Raman, SS Patel, Y Cheng\u2026 - ESC Heart Failure, 2024", "abstract": "Aims Heart failure (HF) is a clinical syndrome with no definitive diagnostic tests. HF registries are often based on manual reviews of medical records of hospitalized HF patients identified using International Classification of Diseases (ICD) codes \u2026"}, {"title": "Continuous Predictive Modeling of Clinical Notes and ICD Codes in Patient Health Records", "link": "https://ui.adsabs.harvard.edu/abs/2024arXiv240511622H/abstract", "details": "M Hernandez Caralt, CBL Ng, M Rei - arXiv e-prints, 2024", "abstract": "Abstract Electronic Health Records (EHR) serve as a valuable source of patient information, offering insights into medical histories, treatments, and outcomes. Previous research has developed systems for detecting applicable ICD codes that \u2026"}, {"title": "Calibrating Reasoning in Language Models with Internal Consistency", "link": "https://arxiv.org/pdf/2405.18711", "details": "Z Xie, J Guo, T Yu, S Li - arXiv preprint arXiv:2405.18711, 2024", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought (CoT) prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious \u2026"}, {"title": "Exploring Activation Patterns of Parameters in Language Models", "link": "https://arxiv.org/pdf/2405.17799", "details": "Y Wang, D Dai, Z Sui - arXiv preprint arXiv:2405.17799, 2024", "abstract": "Most work treats large language models as black boxes without in-depth understanding of their internal working mechanism. In order to explain the internal representations of LLMs, we propose a gradient-based metric to assess the \u2026"}, {"title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective", "link": "https://arxiv.org/pdf/2405.16747", "details": "A Tomihari, I Sato - arXiv preprint arXiv:2405.16747, 2024", "abstract": "The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data. This success is largely \u2026"}]
