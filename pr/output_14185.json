[{"title": "Process-based self-rewarding language models", "link": "https://arxiv.org/pdf/2503.03746", "details": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' \u2026"}, {"title": "Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge", "link": "https://arxiv.org/pdf/2503.04036", "details": "X Cui, JTZ Wei, S Swayamdipta, R Jia - arXiv preprint arXiv:2503.04036, 2025", "abstract": "Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques \u2026"}, {"title": "TopoRefine: Iterative Refinement with Reasoning Topology as High-Level Feedback", "link": "https://ieeexplore.ieee.org/abstract/document/10888346/", "details": "H Liao, S Hu, Z Zhu, H He, Y Jin - ICASSP 2025-2025 IEEE International Conference \u2026, 2025", "abstract": "By leveraging effective signals to refine their outputs, large language models (LLMs) can achieve superior performance compared to single-pass outputs. However, internal signals often suffer from accumulated hallucinations and a lack of \u2026"}, {"title": "MMSciBench: Benchmarking Language Models on Multimodal Scientific Problems", "link": "https://arxiv.org/pdf/2503.01891", "details": "X Ye, C Li, S Chen, X Tang, W Wei - arXiv preprint arXiv:2503.01891, 2025", "abstract": "Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present \u2026"}, {"title": "FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models", "link": "https://arxiv.org/pdf/2502.17924", "details": "H Lin, Y Deng, Y Gu, W Zhang, J Ma, SK Ng, TS Chua - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have significantly advanced the fact-checking studies. However, existing automated fact-checking evaluation methods rely on static datasets and classification metrics, which fail to automatically evaluate the \u2026"}, {"title": "Probench: Benchmarking large language models in competitive programming", "link": "https://arxiv.org/pdf/2502.20868", "details": "L Yang, R Jin, L Shi, J Peng, Y Chen, D Xiong - arXiv preprint arXiv:2502.20868, 2025", "abstract": "With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging, large language models (LLMs) have entered a new phase of development. However, existing benchmarks for coding evaluation are gradually inadequate to assess the \u2026"}, {"title": "Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence", "link": "https://export-test.arxiv.org/abs/2503.14749%3Fcontext%3Dcs.LG", "details": "S Hager, D Mueller, K Duh, N Andrews - arXiv preprint arXiv:2503.14749, 2025", "abstract": "As large language models (LLMs) are increasingly used for factual question- answering, it becomes more important for LLMs to have the capability to communicate the likelihood that their answer is correct. For these verbalized \u2026"}, {"title": "Scalable best-of-n selection for large language models via self-certainty", "link": "https://arxiv.org/pdf/2502.18581", "details": "Z Kang, X Zhao, D Song - arXiv preprint arXiv:2502.18581, 2025", "abstract": "Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for \u2026"}, {"title": "When Debate Fails: Bias Reinforcement in Large Language Models", "link": "https://openreview.net/pdf%3Fid%3Dc5bjw7hqix", "details": "J Oh, M Jeong, J Ko, SY Yun - Workshop on Reasoning and Planning for Large \u2026", "abstract": "Large Language Models (LLMs) solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self \u2026"}]
