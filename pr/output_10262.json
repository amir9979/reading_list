[{"title": "Walking the Web of Concept-Class Relationships in Incrementally Trained Interpretable Models", "link": "https://susmit-a.github.io/images/AAAI_Concepts.pdf", "details": "S Agrawal, D Vemuri, SS Chakravarthy\u2026 - 2025", "abstract": "Abstract Concept-based methods have emerged as a promising direction to develop interpretable neural networks in standard supervised settings. However, most works that study them in incremental settings assume either a static concept set across all \u2026"}, {"title": "Sustainable and Explainable Neural Network for Real-Time Time Series Classification", "link": "https://link.springer.com/chapter/10.1007/978-3-031-78169-8_26", "details": "H Huang, T Shah, S Yoo, S Evans - International Conference on Pattern Recognition, 2024", "abstract": "The increasing demand for real-time time series classification, especially in high- stakes industries, underscores the need for predictive explanations and energy- efficient processing. To address these challenges, we propose a sustainable and \u2026"}, {"title": "Self\u2010supervised learning improves robustness of deep learning lung tumor segmentation models to CT imaging differences", "link": "https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.17541", "details": "J Jiang, A Rangnekar, H Veeraraghavan - Medical Physics", "abstract": "Background Self\u2010supervised learning (SSL) is an approach to extract useful feature representations from unlabeled data, and enable fine\u2010tuning on downstream tasks with limited labeled examples. Self\u2010pretraining is a SSL approach that uses curated \u2026"}]
