[{"title": "FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain", "link": "https://arxiv.org/pdf/2505.14826", "details": "R Deb, K Thekumparampil, K Kalantari, G Hiranandani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Supervised fine-tuning (SFT) is a standard approach to adapting large language models (LLMs) to new domains. In this work, we improve the statistical efficiency of SFT by selecting an informative subset of training examples. Specifically, for a fixed \u2026", "entry_id": "http://arxiv.org/abs/2505.14826v1", "updated": "2025-05-20 18:41:34", "published": "2025-05-20 18:41:34", "authors": "Rohan Deb;Kiran Thekumparampil;Kousha Kalantari;Gaurush Hiranandani;Shoham Sabach;Branislav Kveton", "summary": "Supervised fine-tuning (SFT) is a standard approach to adapting large\nlanguage models (LLMs) to new domains. In this work, we improve the statistical\nefficiency of SFT by selecting an informative subset of training examples.\nSpecifically, for a fixed budget of training examples, which determines the\ncomputational cost of fine-tuning, we determine the most informative ones. The\nkey idea in our method is to select examples that maximize information gain,\nmeasured by the Hessian of the log-likelihood of the LLM. We approximate it\nefficiently by linearizing the LLM at the last layer using multinomial logistic\nregression models. Our approach is computationally efficient, analyzable, and\nperforms well empirically. We demonstrate this on several problems, and back\nour claims with both quantitative results and an LLM evaluation.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CL;stat.ML", "links": "http://arxiv.org/abs/2505.14826v1;http://arxiv.org/pdf/2505.14826v1", "pdf_url": "http://arxiv.org/pdf/2505.14826v1"}, {"title": "Cross-Lingual Text Classification with Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3701716.3715567", "details": "B Han, ST Yang, C LuVogt - Companion Proceedings of the ACM on Web \u2026, 2025", "abstract": "Cross-lingual text classification involves using a model trained on data in one language to classify text in another, which is crucial in global web applications where labeled data is scarce in certain languages. Although multilingual language models \u2026"}, {"title": "When and How to Augment Your Input: Question Routing Helps Balance the Accuracy and Efficiency of Large Language Models", "link": "https://aclanthology.org/2025.findings-naacl.200.pdf", "details": "S Chen, H Zheng, L Cui - Findings of the Association for Computational \u2026, 2025", "abstract": "Although large language models rely on parametric knowledge to achieve exceptional performance across various question-answering tasks, they still face challenges when addressing knowledge-based long-tail questions. Augmented \u2026"}, {"title": "Rewriting Pre-Training Data Boosts LLM Performance in Math and Code", "link": "https://arxiv.org/pdf/2505.02881", "details": "K Fujii, Y Tajima, S Mizuki, H Shimada, T Shiotani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The performance of large language models (LLMs) in program synthesis and mathematical reasoning is fundamentally limited by the quality of their pre-training corpora. We introduce two openly licensed datasets, released under the Llama 3.3 \u2026", "entry_id": "http://arxiv.org/abs/2505.02881v2", "updated": "2025-05-10 14:45:30", "published": "2025-05-05 07:38:43", "authors": "Kazuki Fujii;Yukito Tajima;Sakae Mizuki;Hinari Shimada;Taihei Shiotani;Koshiro Saito;Masanari Ohi;Masaki Kawamura;Taishi Nakamura;Takumi Okamoto;Shigeki Ishida;Kakeru Hattori;Youmi Ma;Hiroya Takamura;Rio Yokota;Naoaki Okazaki", "summary": "The performance of large language models (LLMs) in program synthesis and\nmathematical reasoning is fundamentally limited by the quality of their\npre-training corpora. We introduce two openly licensed datasets, released under\nthe Llama 3.3 Community License, that significantly enhance LLM performance by\nsystematically rewriting public data. SwallowCode (approximately 16.1 billion\ntokens) refines Python snippets from The-Stack-v2 through a novel four-stage\npipeline: syntax validation, pylint-based style filtering, and a two-stage LLM\nrewriting process that enforces style conformity and transforms snippets into\nself-contained, algorithmically efficient examples. Unlike prior methods that\nrely on exclusionary filtering or limited transformations, our\ntransform-and-retain approach upgrades low-quality code, maximizing data\nutility. SwallowMath (approximately 2.3 billion tokens) enhances Finemath-4+ by\nremoving boilerplate, restoring context, and reformatting solutions into\nconcise, step-by-step explanations. Within a fixed 50 billion token training\nbudget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1\nby +17.0 on HumanEval and +17.7 on HumanEval+ compared to Stack-Edu, surpassing\nthe baseline model's code generation capabilities. Similarly, substituting\nSwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies\nconfirm that each pipeline stage contributes incrementally, with rewriting\ndelivering the largest gains. All datasets, prompts, and checkpoints are\npublicly available, enabling reproducible research and advancing LLM\npre-training for specialized domains.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.02881v2;http://arxiv.org/pdf/2505.02881v2", "pdf_url": "http://arxiv.org/pdf/2505.02881v2"}, {"title": "prompt4vis: prompting large language models with example mining for tabular data visualization", "link": "https://link.springer.com/article/10.1007/s00778-025-00912-0", "details": "S Li, X Chen, Y Song, Y Song, CJ Zhang, F Hao\u2026 - The VLDB Journal, 2025", "abstract": "We are currently in the epoch of Large Language Models (LLMs), which have transformed numerous technological domains within the database community. In this paper, we examine the application of LLMs in text-to-visualization (text-to-vis). The \u2026"}, {"title": "Atoxia: Red-teaming Large Language Models with Target Toxic Answers", "link": "https://aclanthology.org/2025.findings-naacl.179.pdf", "details": "Y Du, Z Li, P Cheng, X Wan, A Gao - Findings of the Association for Computational \u2026, 2025", "abstract": "Despite the substantial advancements in artificial intelligence, large language models (LLMs) remain being challenged by generation safety. With adversarial jailbreaking prompts, one can effortlessly induce LLMs to output harmful content \u2026"}, {"title": "A Framework for Domain-Specific Dataset Creation and Adaptation of Large Language Models", "link": "https://www.mdpi.com/2073-431X/14/5/172", "details": "G Balaskas, H Papadopoulos, D Pappa, Q Loisel\u2026 - Computers, 2025", "abstract": "This paper introduces a novel framework for addressing domain adaptation challenges in large language models (LLMs), emphasising privacy-preserving synthetic data generation and efficient fine-tuning. The proposed framework employs \u2026"}]
