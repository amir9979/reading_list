[{"title": "Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding", "link": "https://arxiv.org/pdf/2501.00712", "details": "J Zhu, P Wang, R Cai, JD Lee, P Li, Z Wang - arXiv preprint arXiv:2501.00712, 2025", "abstract": "Transformers rely on both content-based and position-based addressing mechanisms to make predictions, but existing positional encoding techniques often diminish the effectiveness of position-based addressing. Many current methods \u2026"}, {"title": "HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.08378", "details": "S Zhu, W Dong, J Song, Y Guo, B Zheng - arXiv preprint arXiv:2412.08378, 2024", "abstract": "Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into \u2026"}, {"title": "Reusing routine electronic health record data for nationwide COVID-19 surveillance in nursing homes: barriers, facilitators, and lessons learned", "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11674179/", "details": "Y Wieland-Jorna, RA Verheij, AL Francke, R Coppen\u2026 - BMC Medical Informatics \u2026, 2024", "abstract": "Background At the beginning of the COVID-19 pandemic in 2020, little was known about the spread of COVID-19 in Dutch nursing homes while older people were particularly at risk of severe symptoms. Therefore, attempts were made to develop a \u2026"}, {"title": "Information-Maximized Soft Variable Discretization for Self-Supervised Image Representation Learning", "link": "https://arxiv.org/pdf/2501.03469", "details": "C Niu, W Xia, H Shan, G Wang - arXiv preprint arXiv:2501.03469, 2025", "abstract": "Self-supervised learning (SSL) has emerged as a crucial technique in image processing, encoding, and understanding, especially for developing today's vision foundation models that utilize large-scale datasets without annotations to enhance \u2026"}, {"title": "A vision\u2013language foundation model for precision oncology", "link": "https://www.nature.com/articles/s41586-024-08378-w", "details": "J Xiang, X Wang, X Zhang, Y Xi, F Eweje, Y Chen, Y Li\u2026 - Nature, 2025", "abstract": "Clinical decision-making is driven by multimodal data, including clinical notes and pathological characteristics. Artificial intelligence approaches that can effectively integrate multimodal data hold significant promise in advancing clinical care 1, 2 \u2026"}, {"title": "Instruction-Following Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2501.02086", "details": "B Hou, Q Chen, J Wang, G Yin, C Wang, N Du, R Pang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from \u2026"}, {"title": "Core Context Aware Attention for Long Context Language Modeling", "link": "https://arxiv.org/pdf/2412.12465", "details": "Y Chen, Z You, S Zhang, H Li, Y Li, Y Wang, M Tan - arXiv preprint arXiv:2412.12465, 2024", "abstract": "Transformer-based Large Language Models (LLMs) have exhibited remarkable success in various natural language processing tasks primarily attributed to self- attention mechanism, which requires a token to consider all preceding tokens as its \u2026"}, {"title": "Evaluation and Enhancement of Large Language Models for In-Patient Diagnostic Support", "link": "https://www.researchsquare.com/article/rs-5599195/latest.pdf", "details": "Y Yuan - 2025", "abstract": "In-patient diagnosis demands complex clinical decision-making based on comprehensive patient information, posing critical challenges for clinicians. Despite advancements in large language models (LLMs) in medical applications, limited \u2026"}, {"title": "Sepllm: Accelerate large language models by compressing one segment into one separator", "link": "https://arxiv.org/pdf/2412.12094", "details": "G Chen, H Shi, J Li, Y Gao, X Ren, Y Chen, X Jiang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have exhibited exceptional performance across a spectrum of natural language processing tasks. However, their substantial sizes pose considerable challenges, particularly in computational demands and inference \u2026"}]
