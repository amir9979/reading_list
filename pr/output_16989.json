[{"title": "Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models", "link": "https://arxiv.org/pdf/2505.19037", "details": "KH Lu, CY Kuan, H Lee - arXiv preprint arXiv:2505.19037, 2025", "abstract": "We introduce Speech-IFeval, an evaluation framework designed to assess instruction-following capabilities and quantify catastrophic forgetting in speech- aware language models (SLMs). Recent SLMs integrate speech perception with \u2026", "entry_id": "http://arxiv.org/abs/2505.19037v1", "updated": "2025-05-25 08:37:55", "published": "2025-05-25 08:37:55", "authors": "Ke-Han Lu;Chun-Yi Kuan;Hung-yi Lee", "summary": "We introduce Speech-IFeval, an evaluation framework designed to assess\ninstruction-following capabilities and quantify catastrophic forgetting in\nspeech-aware language models (SLMs). Recent SLMs integrate speech perception\nwith large language models (LLMs), often degrading textual capabilities due to\nspeech-centric training. Existing benchmarks conflate speech perception with\ninstruction-following, hindering evaluation of these distinct skills. To\naddress this gap, we provide a benchmark for diagnosing the\ninstruction-following abilities of SLMs. Our findings show that most SLMs\nstruggle with even basic instructions, performing far worse than text-based\nLLMs. Additionally, these models are highly sensitive to prompt variations,\noften yielding inconsistent and unreliable outputs. We highlight core\nchallenges and provide insights to guide future research, emphasizing the need\nfor evaluation beyond task-level metrics.", "comment": "Accecpted by Interspeech 2025;\n  https://github.com/kehanlu/Speech-IFEval", "journal_ref": null, "primary_category": "eess.AS", "categories": "eess.AS;cs.CL", "links": "http://arxiv.org/abs/2505.19037v1;http://arxiv.org/pdf/2505.19037v1", "pdf_url": "http://arxiv.org/pdf/2505.19037v1"}, {"title": "Behavior Injection: Preparing Language Models for Reinforcement Learning", "link": "https://arxiv.org/pdf/2505.18917", "details": "Z Cen, Y Yao, W Han, Z Liu, D Zhao - arXiv preprint arXiv:2505.18917, 2025", "abstract": "Reinforcement fine-tuning (RFT) has emerged as a powerful post-training technique to incentivize the reasoning ability of large language models (LLMs). However, LLMs can respond very inconsistently to RFT: some show substantial performance gains \u2026", "entry_id": "http://arxiv.org/abs/2505.18917v1", "updated": "2025-05-25 00:54:50", "published": "2025-05-25 00:54:50", "authors": "Zhepeng Cen;Yihang Yao;William Han;Zuxin Liu;Ding Zhao", "summary": "Reinforcement fine-tuning (RFT) has emerged as a powerful post-training\ntechnique to incentivize the reasoning ability of large language models (LLMs).\nHowever, LLMs can respond very inconsistently to RFT: some show substantial\nperformance gains, while others plateau or even degrade. To understand this\ndivergence, we analyze the per-step influence of the RL objective and identify\ntwo key conditions for effective post-training: (1) RL-informative rollout\naccuracy, and (2) strong data co-influence, which quantifies how much the\ntraining data affects performance on other samples. Guided by these insights,\nwe propose behavior injection, a task-agnostic data-augmentation scheme applied\nprior to RL. Behavior injection enriches the supervised finetuning (SFT) data\nby seeding exploratory and exploitative behaviors, effectively making the model\nmore RL-ready. We evaluate our method across two reasoning benchmarks with\nmultiple base models. The results demonstrate that our theoretically motivated\naugmentation can significantly increases the performance gain from RFT over the\npre-RL model.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.18917v1;http://arxiv.org/pdf/2505.18917v1", "pdf_url": "http://arxiv.org/pdf/2505.18917v1"}, {"title": "Are Language Models Consequentialist or Deontological Moral Reasoners?", "link": "https://arxiv.org/pdf/2505.21479", "details": "K Samway, M Kleiman-Weiner, DG Piedrahita\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As AI systems increasingly navigate applications in healthcare, law, and governance, understanding how they handle ethically complex scenarios becomes critical. Previous work has mainly examined the moral judgments in large language models \u2026", "entry_id": "http://arxiv.org/abs/2505.21479v1", "updated": "2025-05-27 17:51:18", "published": "2025-05-27 17:51:18", "authors": "Keenan Samway;Max Kleiman-Weiner;David Guzman Piedrahita;Rada Mihalcea;Bernhard Sch\u00f6lkopf;Zhijing Jin", "summary": "As AI systems increasingly navigate applications in healthcare, law, and\ngovernance, understanding how they handle ethically complex scenarios becomes\ncritical. Previous work has mainly examined the moral judgments in large\nlanguage models (LLMs), rather than their underlying moral reasoning process.\nIn contrast, we focus on a large-scale analysis of the moral reasoning traces\nprovided by LLMs. Furthermore, unlike prior work that attempted to draw\ninferences from only a handful of moral dilemmas, our study leverages over 600\ndistinct trolley problems as probes for revealing the reasoning patterns that\nemerge within different LLMs. We introduce and test a taxonomy of moral\nrationales to systematically classify reasoning traces according to two main\nnormative ethical theories: consequentialism and deontology. Our analysis\nreveals that LLM chains-of-thought tend to favor deontological principles based\non moral obligations, while post-hoc explanations shift notably toward\nconsequentialist rationales that emphasize utility. Our framework provides a\nfoundation for understanding how LLMs process and articulate ethical\nconsiderations, an important step toward safe and interpretable deployment of\nLLMs in high-stakes decision-making environments. Our code is available at\nhttps://github.com/keenansamway/moral-lens .", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.21479v1;http://arxiv.org/pdf/2505.21479v1", "pdf_url": "http://arxiv.org/pdf/2505.21479v1"}, {"title": "The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants", "link": "https://arxiv.org/pdf/2505.19797", "details": "Y Zhang, H Li, C Wang, L Chen, Q Zhang, P Ye, S Feng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As proprietary giants increasingly dominate the race for ever-larger language models, a pressing question arises for the open-source community: can smaller models remain competitive across a broad range of tasks? In this paper, we present \u2026", "entry_id": "http://arxiv.org/abs/2505.19797v2", "updated": "2025-05-28 10:47:12", "published": "2025-05-26 10:29:42", "authors": "Yiqun Zhang;Hao Li;Chenxu Wang;Linyao Chen;Qiaosheng Zhang;Peng Ye;Shi Feng;Daling Wang;Zhen Wang;Xinrun Wang;Jia Xu;Lei Bai;Wanli Ouyang;Shuyue Hu", "summary": "As proprietary giants increasingly dominate the race for ever-larger language\nmodels, a pressing question arises for the open-source community: can smaller\nmodels remain competitive across a broad range of tasks? In this paper, we\npresent the Avengers--a simple recipe that effectively leverages the collective\nintelligence of open-source, smaller language models. Our framework is built\nupon four lightweight operations: (i) embedding: encode queries using a text\nembedding model; (ii) clustering: group queries based on their semantic\nsimilarity; (iii) scoring: scores each model's performance within each cluster;\nand (iv) voting: improve outputs via repeated sampling and voting. At inference\ntime, each query is embedded and assigned to its nearest cluster. The\ntop-performing model(s) within that cluster are selected to generate the\nresponse using the Self-Consistency or its multi-model variant. Remarkably,\nwith 10 open-source models (~7B parameters each), the Avengers collectively\noutperforms GPT-4.1 on nine out of 15 datasets (spanning mathematics, code,\nlogic, knowledge, and affective tasks). In particular, it surpasses GPT-4.1 on\nmathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the\nAvengers delivers superior out-of-distribution generalization, and remains\nrobust across various embedding models, clustering algorithms, ensemble\nstrategies, and values of its sole parameter--the number of clusters. We have\nopen-sourced the code on GitHub: https://github.com/ZhangYiqun018/Avengers", "comment": "9 pages, 3 figures, 6 tables, supplementary material (appendix)\n  included separately", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.19797v2;http://arxiv.org/pdf/2505.19797v2", "pdf_url": "http://arxiv.org/pdf/2505.19797v2"}, {"title": "Multilingual Pretraining for Pixel Language Models", "link": "https://arxiv.org/pdf/2505.21265", "details": "I Kesen, JF Lotz, I Ziegler, P Rust, D Elliott - arXiv preprint arXiv:2505.21265, 2025", "abstract": "Pixel language models operate directly on images of rendered text, eliminating the need for a fixed vocabulary. While these models have demonstrated strong capabilities for downstream cross-lingual transfer, multilingual pretraining remains \u2026", "entry_id": "http://arxiv.org/abs/2505.21265v1", "updated": "2025-05-27 14:40:47", "published": "2025-05-27 14:40:47", "authors": "Ilker Kesen;Jonas F. Lotz;Ingo Ziegler;Phillip Rust;Desmond Elliott", "summary": "Pixel language models operate directly on images of rendered text,\neliminating the need for a fixed vocabulary. While these models have\ndemonstrated strong capabilities for downstream cross-lingual transfer,\nmultilingual pretraining remains underexplored. We introduce PIXEL-M4, a model\npretrained on four visually and linguistically diverse languages: English,\nHindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic\nand syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart\non non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4\ncaptures rich linguistic features, even in languages not seen during\npretraining. Furthermore, an analysis of its hidden representations shows that\nmultilingual pretraining yields a semantic embedding space closely aligned\nacross the languages used for pretraining. This work demonstrates that\nmultilingual pretraining substantially enhances the capability of pixel\nlanguage models to effectively support a diverse set of languages.", "comment": "17 pages, 19 figures, 7 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.21265v1;http://arxiv.org/pdf/2505.21265v1", "pdf_url": "http://arxiv.org/pdf/2505.21265v1"}, {"title": "How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation", "link": "https://arxiv.org/pdf/2505.18522", "details": "X Lu, Y Zhao, S Wei, S Wang, B Qin, T Liu - arXiv preprint arXiv:2505.18522, 2025", "abstract": "Pre-trained language models represented by the Transformer have been proven to possess strong base capabilities, and the representative self-attention mechanism in the Transformer has become a classic in sequence modeling architectures. Different \u2026", "entry_id": "http://arxiv.org/abs/2505.18522v1", "updated": "2025-05-24 05:40:03", "published": "2025-05-24 05:40:03", "authors": "Xin Lu;Yanyan Zhao;Si Wei;Shijin Wang;Bing Qin;Ting Liu", "summary": "Pre-trained language models represented by the Transformer have been proven\nto possess strong base capabilities, and the representative self-attention\nmechanism in the Transformer has become a classic in sequence modeling\narchitectures. Different from the work of proposing sequence modeling\narchitecture to improve the efficiency of attention mechanism, this work\nfocuses on the impact of sequence modeling architectures on base capabilities.\nSpecifically, our concern is: How exactly do sequence modeling architectures\naffect the base capabilities of pre-trained language models? In this work, we\nfirst point out that the mixed domain pre-training setting commonly adopted in\nexisting architecture design works fails to adequately reveal the differences\nin base capabilities among various architectures. To address this, we propose a\nlimited domain pre-training setting with out-of-distribution testing, which\nsuccessfully uncovers significant differences in base capabilities among\narchitectures at an early stage. Next, we analyze the base capabilities of\nstateful sequence modeling architectures, and find that they exhibit\nsignificant degradation in base capabilities compared to the Transformer. Then,\nthrough a series of architecture component analysis, we summarize a key\narchitecture design principle: A sequence modeling architecture need possess\nfull-sequence arbitrary selection capability to avoid degradation in base\ncapabilities. Finally, we empirically validate this principle using an\nextremely simple Top-1 element selection architecture and further generalize it\nto a more practical Top-1 chunk selection architecture. Experimental results\ndemonstrate our proposed sequence modeling architecture design principle and\nsuggest that our work can serve as a valuable reference for future architecture\nimprovements and novel designs.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.18522v1;http://arxiv.org/pdf/2505.18522v1", "pdf_url": "http://arxiv.org/pdf/2505.18522v1"}, {"title": "Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models", "link": "https://arxiv.org/pdf/2505.21200", "details": "X Tan, Y Yang, P Ye, J Zheng, B Bai, X Wang, J Hao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm for general-purpose robot control through natural language instructions. However, their high inference cost-stemming from large-scale token computation and \u2026", "entry_id": "http://arxiv.org/abs/2505.21200v1", "updated": "2025-05-27 13:47:18", "published": "2025-05-27 13:47:18", "authors": "Xudong Tan;Yaoxin Yang;Peng Ye;Jialin Zheng;Bizhe Bai;Xinyi Wang;Jia Hao;Tao Chen", "summary": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm for\ngeneral-purpose robot control through natural language instructions. However,\ntheir high inference cost-stemming from large-scale token computation and\nautoregressive decoding-poses significant challenges for real-time deployment\nand edge applications. While prior work has primarily focused on architectural\noptimization, we take a different perspective by identifying a dual form of\nredundancy in VLA models: (i) high similarity across consecutive action steps,\nand (ii) substantial redundancy in visual tokens. Motivated by these\nobservations, we propose FlashVLA, the first training-free and plug-and-play\nacceleration framework that enables action reuse in VLA models. FlashVLA\nimproves inference efficiency through a token-aware action reuse mechanism that\navoids redundant decoding across stable action steps, and an information-guided\nvisual token selection strategy that prunes low-contribution tokens. Extensive\nexperiments on the LIBERO benchmark show that FlashVLA reduces FLOPs by 55.7%\nand latency by 36.0%, with only a 0.7% drop in task success rate. These results\ndemonstrate the effectiveness of FlashVLA in enabling lightweight, low-latency\nVLA inference without retraining.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.21200v1;http://arxiv.org/pdf/2505.21200v1", "pdf_url": "http://arxiv.org/pdf/2505.21200v1"}, {"title": "Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar", "link": "https://arxiv.org/pdf/2505.19599", "details": "A Gambardella, T Kojima, Y Iwasawa, Y Matsuo - arXiv preprint arXiv:2505.19599, 2025", "abstract": "Typical methods for evaluating the performance of language models evaluate their ability to answer questions accurately. These evaluation metrics are acceptable for determining the extent to which language models can understand and reason about \u2026", "entry_id": "http://arxiv.org/abs/2505.19599v1", "updated": "2025-05-26 07:08:47", "published": "2025-05-26 07:08:47", "authors": "Andrew Gambardella;Takeshi Kojima;Yusuke Iwasawa;Yutaka Matsuo", "summary": "Typical methods for evaluating the performance of language models evaluate\ntheir ability to answer questions accurately. These evaluation metrics are\nacceptable for determining the extent to which language models can understand\nand reason about text in a general sense, but fail to capture nuanced\ncapabilities, such as the ability of language models to recognize and obey rare\ngrammar points, particularly in languages other than English. We measure the\nperplexity of language models when confronted with the \"first person psych\npredicate restriction\" grammar point in Japanese. Weblab is the only tested\nopen source model in the 7-10B parameter range which consistently assigns\nhigher perplexity to ungrammatical psych predicate sentences than grammatical\nones. We give evidence that Weblab's uniformly bad tokenization is a possible\nroot cause for its good performance, and show that Llama 3's perplexity on\ngrammatical psych predicate sentences can be reduced by orders of magnitude\n(28x difference) by restricting test sentences to those with uniformly\nwell-behaved tokenizations. We show in further experiments on machine\ntranslation tasks that language models will use alternative grammar patterns in\norder to produce grammatical sentences when tokenization issues prevent the\nmost natural sentence from being output.", "comment": "In Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics, 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.19599v1;http://arxiv.org/pdf/2505.19599v1", "pdf_url": "http://arxiv.org/pdf/2505.19599v1"}, {"title": "Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models", "link": "https://arxiv.org/pdf/2505.18773", "details": "J Hayes, I Shumailov, CA Choquette-Choo, M Jagielski\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker \u2026", "entry_id": "http://arxiv.org/abs/2505.18773v1", "updated": "2025-05-24 16:23:43", "published": "2025-05-24 16:23:43", "authors": "Jamie Hayes;Ilia Shumailov;Christopher A. Choquette-Choo;Matthew Jagielski;George Kaissis;Katherine Lee;Milad Nasr;Sahra Ghalebikesabi;Niloofar Mireshghallah;Meenatchi Sundaram Mutu Selva Annamalai;Igor Shilov;Matthieu Meeus;Yves-Alexandre de Montjoye;Franziska Boenisch;Adam Dziedzic;A. Feder Cooper", "summary": "State-of-the-art membership inference attacks (MIAs) typically require\ntraining many reference models, making it difficult to scale these attacks to\nlarge pre-trained language models (LLMs). As a result, prior research has\neither relied on weaker attacks that avoid training reference models (e.g.,\nfine-tuning attacks), or on stronger attacks applied to small-scale models and\ndatasets. However, weaker attacks have been shown to be brittle - achieving\nclose-to-arbitrary success - and insights from strong attacks in simplified\nsettings do not translate to today's LLMs. These challenges have prompted an\nimportant question: are the limitations observed in prior work due to attack\ndesign choices, or are MIAs fundamentally ineffective on LLMs? We address this\nquestion by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures\nranging from 10M to 1B parameters, training reference models on over 20B tokens\nfrom the C4 dataset. Our results advance the understanding of MIAs on LLMs in\nthree key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their\neffectiveness, however, remains limited (e.g., AUC<0.7) in practical settings;\nand, (3) the relationship between MIA success and related privacy metrics is\nnot as straightforward as prior work has suggested.", "comment": null, "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.18773v1;http://arxiv.org/pdf/2505.18773v1", "pdf_url": "http://arxiv.org/pdf/2505.18773v1"}]
