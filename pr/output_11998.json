[{"title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2501.12370", "details": "S Abnar, H Shah, D Busbridge, AME Ali, J Susskind\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Scaling the capacity of language models has consistently proven to be a reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the \u2026"}, {"title": "Reasoning Language Models: A Blueprint", "link": "https://arxiv.org/pdf/2501.11223", "details": "M Besta, J Barth, E Schreiber, A Kubicek, A Catarino\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have redefined AI's problem-solving capabilities by extending large language models \u2026"}, {"title": "CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning", "link": "https://arxiv.org/pdf/2501.12226", "details": "Y Fang, G Chao, W Lei, S Li, D Chu - arXiv preprint arXiv:2501.12226, 2025", "abstract": "Large Language Models (LLMs) have recently achieved impressive results in complex reasoning tasks through Chain of Thought (CoT) prompting. However, most existing CoT methods rely on using the same prompts, whether manually designed \u2026"}, {"title": "A Collection of Question Answering Datasets for Norwegian", "link": "https://arxiv.org/pdf/2501.11128", "details": "V Mikhailov, P M\u00e6hlum, VOC Lang\u00f8, E Velldal\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper introduces a new suite of question answering datasets for Norwegian; NorOpenBookQA, NorCommonSenseQA, NorTruthfulQA, and NRK-Quiz-QA. The data covers a wide range of skills and knowledge domains, including world \u2026"}, {"title": "DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision Language Models in Real-World Scenarios with Driving Theory Tests", "link": "https://arxiv.org/pdf/2501.04671", "details": "C Corbi\u00e8re, S Roburin, S Montariol, A Bosselut, A Alahi - arXiv preprint arXiv \u2026, 2025", "abstract": "Large vision-language models (LVLMs) augment language models with visual understanding, enabling multimodal reasoning. However, due to the modality gap between textual and visual data, they often face significant challenges, such as over \u2026"}, {"title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models", "link": "https://arxiv.org/pdf/2501.02955", "details": "W Hong, Y Cheng, Z Yang, W Wang, L Wang, X Gu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In recent years, vision language models (VLMs) have made significant advancements in video understanding. However, a crucial capability-fine-grained motion comprehension-remains under-explored in current benchmarks. To address \u2026"}, {"title": "How Ambiguous Are the Rationales for Natural Language Reasoning? A Simple Approach to Handling Rationale Uncertainty", "link": "https://aclanthology.org/2025.coling-main.671.pdf", "details": "HH Kim - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "The quality of rationales is essential in the reasoning capabilities of language models. Rationales not only enhance reasoning performance in complex natural language tasks but also justify model decisions. However, obtaining impeccable \u2026"}, {"title": "Utility-inspired Reward Transformations Improve Reinforcement Learning Training of Language Models", "link": "https://arxiv.org/pdf/2501.06248", "details": "RR Maura-Rivero, C Nagpal, R Patel, F Visin - arXiv preprint arXiv:2501.06248, 2025", "abstract": "Current methods that train large language models (LLMs) with reinforcement learning feedback, often resort to averaging outputs of multiple rewards functions during training. This overlooks crucial aspects of individual reward dimensions and \u2026"}, {"title": "Guiding Medical Vision-Language Models with Explicit Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations", "link": "https://arxiv.org/pdf/2501.02385", "details": "K Zhu, Z Qin, H Yi, Z Jiang, Q Lao, S Zhang, K Li - arXiv preprint arXiv:2501.02385, 2025", "abstract": "With the recent advancements in vision-language models (VLMs) driven by large language models (LLMs), many researchers have focused on models that comprised of an image encoder, an image-to-language projection layer, and a text decoder \u2026"}]
