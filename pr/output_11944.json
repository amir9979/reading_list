[{"title": "CmEAA: Cross-modal Enhancement and Alignment Adapter for Radiology Report Generation", "link": "https://aclanthology.org/2025.coling-main.571.pdf", "details": "X Huang, Y Han, L Yx, R Li, P Wu, K Zhang - \u2026 of the 31st International Conference on \u2026, 2025", "abstract": "Automatic radiology report generation is pivotal in reducing the workload of radiologists, while simultaneously improving diagnostic accuracy and operational efficiency. Current methods face significant challenges, including the effective \u2026"}, {"title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining", "link": "https://arxiv.org/pdf/2501.00958", "details": "W Zhang, H Zhang, X Li, J Sun, Y Shen, W Lu, D Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge \u2026"}, {"title": "Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models", "link": "https://arxiv.org/pdf/2412.18609%3F", "details": "J Yi, ST Wasim, Y Luo, M Naseer, J Gall - arXiv preprint arXiv:2412.18609, 2024", "abstract": "We present an efficient encoder-free approach for video-language understanding that achieves competitive performance while significantly reducing computational overhead. Current video-language models typically rely on heavyweight image \u2026"}, {"title": "Unifying Specialized Visual Encoders for Video Language Models", "link": "https://arxiv.org/pdf/2501.01426", "details": "J Chung, T Zhu, MG Saez-Diez, JC Niebles, H Zhou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of \u2026"}, {"title": "Bridged Semantic Alignment for Zero-shot 3D Medical Image Diagnosis", "link": "https://arxiv.org/pdf/2501.03565", "details": "H Lai, Z Jiang, Q Yao, R Wang, Z He, X Tao, W Wei\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "3D medical images such as Computed tomography (CT) are widely used in clinical practice, offering a great potential for automatic diagnosis. Supervised learning- based approaches have achieved significant progress but rely heavily on extensive \u2026"}, {"title": "MMFactory: A Universal Solution Search Engine for Vision-Language Tasks", "link": "https://arxiv.org/pdf/2412.18072", "details": "WC Fan, T Rahman, L Sigal - arXiv preprint arXiv:2412.18072, 2024", "abstract": "With advances in foundational and vision-language models, and effective fine-tuning techniques, a large number of both general and special-purpose models have been developed for a variety of visual tasks. Despite the flexibility and accessibility of these \u2026"}, {"title": "ICONS: Influence Consensus for Vision-Language Data Selection", "link": "https://arxiv.org/pdf/2501.00654", "details": "X Wu, M Xia, R Shao, Z Deng, PW Koh, O Russakovsky - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual Instruction Tuning typically requires a large amount of vision-language training data. This data often containing redundant information that increases computational costs without proportional performance gains. In this work, we \u2026"}, {"title": "Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment", "link": "https://arxiv.org/pdf/2412.19326", "details": "Z Yan, Z Li, Y He, C Wang, K Li, X Li, X Zeng, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Current multimodal large language models (MLLMs) struggle with fine-grained or precise understanding of visuals though they give comprehensive perception and reasoning in a spectrum of vision applications. Recent studies either develop tool \u2026"}, {"title": "LLaVA-RE: Binary image-text relevancy evaluation with multimodal large language model", "link": "https://www.amazon.science/publications/llava-re-binary-image-text-relevancy-evaluation-with-multimodal-large-language-model", "details": "T Sun, O Liu, JJ Li, L Ma - 2025", "abstract": "Multimodal generative AI usually involves generating image or text responses given inputs in another modality. The evaluation of image-text relevancy is essential for measuring response quality or ranking candidate responses. In particular, binary \u2026"}]
