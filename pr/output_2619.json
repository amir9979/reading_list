[{"title": "Symmetric Dot-Product Attention for Efficient Training of BERT Language Models", "link": "https://arxiv.org/pdf/2406.06366", "details": "M Courtois, M Ostendorff, L Hennig, G Rehm - arXiv preprint arXiv:2406.06366, 2024", "abstract": "Initially introduced as a machine translation model, the Transformer architecture has now become the foundation for modern deep learning architecture, with applications in a wide range of fields, from computer vision to natural language processing \u2026"}, {"title": "Training Compute-Optimal Protein Language Models", "link": "https://www.biorxiv.org/content/10.1101/2024.06.06.597716.full.pdf", "details": "X Cheng, B Chen, P Li, J Gong, J Tang, L Song - bioRxiv, 2024", "abstract": "We explore optimally training protein language models, an area of significant interest in biological research where guidance on best practices is limited. Most models are trained with extensive compute resources until performance gains plateau, focusing \u2026"}, {"title": "Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads", "link": "https://arxiv.org/pdf/2405.20053", "details": "AA Hadji-Kyriacou, O Arandjelovic - arXiv preprint arXiv:2405.20053, 2024", "abstract": "Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context learning capabilities; however, their behaviors are often difficult to control. By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible to fine-tune \u2026"}, {"title": "SNOBERT: A Benchmark for clinical notes entity linking in the SNOMED CT clinical terminology", "link": "https://arxiv.org/pdf/2405.16115", "details": "M Kulyabin, G Sokolov, A Galaida, A Maier\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The extraction and analysis of insights from medical data, primarily stored in free-text formats by healthcare workers, presents significant challenges due to its unstructured nature. Medical coding, a crucial process in healthcare, remains minimally \u2026"}, {"title": "CPLIP: Zero-Shot Learning for Histopathology with Comprehensive Vision-Language Alignment", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Javed_CPLIP_Zero-Shot_Learning_for_Histopathology_with_Comprehensive_Vision-Language_Alignment_CVPR_2024_paper.pdf", "details": "S Javed, A Mahmood, II Ganapathi, FA Dharejo\u2026 - Proceedings of the IEEE \u2026, 2024", "abstract": "Abstract This paper proposes Comprehensive Pathology Language Image Pre- training (CPLIP) a new unsupervised technique designed to enhance the alignment of images and text in histopathology for tasks such as classification and \u2026"}, {"title": "Show Think and Tell: Thought-Augmented Fine-Tuning of Large Language Models for Video Captioning", "link": "https://openaccess.thecvf.com/content/CVPR2024W/MMFM/papers/Kim_Show_Think_and_Tell_Thought-Augmented_Fine-Tuning_of_Large_Language_Models_CVPRW_2024_paper.pdf", "details": "B Kim, D Hwang, S Cho, Y Jang, H Lee, M Lee - \u2026 of the IEEE/CVF Conference on \u2026, 2024", "abstract": "Large language models (LLMs) have achieved a great success in natural language processing and have a significant potential for multi-modal applications. Despite the surprising zero-shot or few-shot ability it is also required to effectively fine-tune pre \u2026"}, {"title": "TAIA: Large Language Models are Out-of-Distribution Data Learners", "link": "https://arxiv.org/pdf/2405.20192", "details": "S Jiang, Y Liao, Y Zhang, Y Wang, Y Wang - arXiv preprint arXiv:2405.20192, 2024", "abstract": "Fine-tuning on task-specific question-answer pairs is a predominant method for enhancing the performance of instruction-tuned large language models (LLMs) on downstream tasks. However, in certain specialized domains, such as healthcare or \u2026"}, {"title": "Assessing Large Language Models for Oncology Data Inference from Radiology Reports", "link": "https://www.medrxiv.org/content/10.1101/2024.05.23.24307579.full.pdf", "details": "LC Chen, T Zack, A Demirci, M Sushil, B Miao, C Kasap\u2026 - medRxiv, 2024", "abstract": "Key Points Purpose: We examined the effectiveness of proprietary and open Large Language Models (LLMs) in detecting disease presence, location, and treatment response in pancreatic cancer from radiology reports. Methods: We analyzed 203 \u2026"}]
