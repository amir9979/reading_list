'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Phi-3 technical report: A highly capable language mode'
[{"title": "Meta In-Context Learning Makes Large Language Models Better Zero and Few-Shot Relation Extractors", "link": "https://arxiv.org/pdf/2404.17807", "details": "G Li, P Wang, J Liu, Y Guo, K Ji, Z Shang, Z Xu - arXiv preprint arXiv:2404.17807, 2024", "abstract": "Relation extraction (RE) is an important task that aims to identify the relationships between entities in texts. While large language models (LLMs) have revealed remarkable in-context learning (ICL) capability for general zero and few-shot \u2026"}, {"title": "Tele-FLM Technical Report", "link": "https://arxiv.org/pdf/2404.16645", "details": "X Li, Y Yao, X Jiang, X Fang, C Wang, X Liu, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have showcased profound capabilities in language understanding and generation, facilitating a wide array of applications. However, there is a notable paucity of detailed, open-sourced methodologies on efficiently \u2026"}, {"title": "Temporal Scaling Law for Large Language Models", "link": "https://arxiv.org/pdf/2404.17785", "details": "Y Xiong, X Chen, X Ye, H Chen, Z Lin, H Lian, J Niu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, Large Language Models (LLMs) are widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed as Scaling Laws, have discovered \u2026"}, {"title": "Benchmarking Benchmark Leakage in Large Language Models", "link": "https://arxiv.org/pdf/2404.18824%3Ftrk%3Dpublic_post_comment-text", "details": "R Xu, Z Wang, RZ Fan, P Liu - arXiv preprint arXiv:2404.18824, 2024", "abstract": "Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary \u2026"}, {"title": "Text Quality-Based Pruning for Efficient Training of Language Models", "link": "https://arxiv.org/pdf/2405.01582", "details": "V Sharma, K Padthe, N Ardalani, K Tirumala, R Howes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent times training Language Models (LMs) have relied on computationally heavy training over massive datasets which makes this training process extremely laborious. In this paper we propose a novel method for numerically evaluating text \u2026"}, {"title": "AlpaPICO: Extraction of PICO Frames from Clinical Trial Documents Using LLMs", "link": "https://www.sciencedirect.com/science/article/pii/S1046202324000896", "details": "M Ghosh, S Mukherjee, A Ganguly, P Basuchowdhuri\u2026 - Methods, 2024", "abstract": "In recent years, there has been a surge in the publication of clinical trial reports, making it challenging to conduct systematic reviews. Automatically extracting Population, Intervention, Comparator, and Outcome (PICO) from clinical trial studies \u2026"}, {"title": "What Drives Performance in Multilingual Language Models?", "link": "https://arxiv.org/pdf/2404.19159", "details": "SB Nezhad, A Agrawal - arXiv preprint arXiv:2404.19159, 2024", "abstract": "This study investigates the factors influencing the performance of multilingual large language models (MLLMs) across diverse languages. We study 6 MLLMs, including masked language models, autoregressive models, and instruction-tuned LLMs, on \u2026"}, {"title": "KS-LLM: Knowledge Selection of Large Language Models with Evidence Document for Question Answering", "link": "https://arxiv.org/pdf/2404.15660", "details": "X Zheng, F Che, J Wu, S Zhang, S Nie, K Liu, J Tao - arXiv preprint arXiv:2404.15660, 2024", "abstract": "Large language models (LLMs) suffer from the hallucination problem and face significant challenges when applied to knowledge-intensive tasks. A promising approach is to leverage evidence documents as extra supporting knowledge, which \u2026"}, {"title": "Towards a Holistic Evaluation of LLMs on Factual Knowledge Recall", "link": "https://arxiv.org/pdf/2404.16164", "details": "J Yuan, L Pan, CW Hang, J Guo, J Jiang, B Min, P Ng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have shown remarkable performance on a variety of NLP tasks, and are being rapidly adopted in a wide range of use cases. It is therefore of vital importance to holistically evaluate the factuality of their generated outputs, as \u2026"}]
