[{"title": "Composable Interventions for Language Models", "link": "https://arxiv.org/pdf/2407.06483", "details": "A Kolbeinsson, K O'Brien, T Huang, S Gao, S Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Test-time interventions for language models can enhance factual accuracy, mitigate harmful outputs, and improve model efficiency without costly retraining. But despite a flood of new methods, different types of interventions are largely developing \u2026"}, {"title": "Deconstructing What Makes a Good Optimizer for Language Models", "link": "https://arxiv.org/pdf/2407.07972", "details": "R Zhao, D Morwani, D Brandfonbrener, N Vyas\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Training language models becomes increasingly expensive with scale, prompting numerous attempts to improve optimization efficiency. Despite these efforts, the Adam optimizer remains the most widely used, due to a prevailing view that it is the \u2026"}, {"title": "Cloud Atlas: Efficient Fault Localization for Cloud Systems using Language Models and Causal Insight", "link": "https://arxiv.org/pdf/2407.08694", "details": "Z Xie, Y Zheng, L Ottens, K Zhang, C Kozyrakis, J Mace - arXiv preprint arXiv \u2026, 2024", "abstract": "Runtime failure and performance degradation is commonplace in modern cloud systems. For cloud providers, automatically determining the root cause of incidents is paramount to ensuring high reliability and availability as prompt fault localization can \u2026"}, {"title": "Collaborative Performance Prediction for Large Language Models", "link": "https://arxiv.org/pdf/2407.01300", "details": "Q Zhang, F Lyu, X Liu, C Ma - arXiv preprint arXiv:2407.01300, 2024", "abstract": "Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works \u2026"}, {"title": "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models", "link": "https://arxiv.org/pdf/2406.19999", "details": "X Chen, B Liao, J Qi, P Eustratiadis, C Monz, A Bisazza\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Following multiple instructions is a crucial ability for large language models (LLMs). Evaluating this ability comes with significant challenges:(i) limited coherence between multiple instructions,(ii) positional bias where the order of instructions \u2026"}, {"title": "Universal Approximation Theory: The basic theory for large language models", "link": "https://arxiv.org/pdf/2407.00958", "details": "W Wang, Q Li - arXiv preprint arXiv:2407.00958, 2024", "abstract": "Language models have emerged as a critical area of focus in artificial intelligence, particularly with the introduction of groundbreaking innovations like ChatGPT. Large- scale Transformer networks have quickly become the leading approach for \u2026"}, {"title": "On the attribution of confidence to large language models", "link": "https://arxiv.org/pdf/2407.08388", "details": "G Keeling, W Street - arXiv preprint arXiv:2407.08388, 2024", "abstract": "Credences are mental states corresponding to degrees of confidence in propositions. Attribution of credences to Large Language Models (LLMs) is commonplace in the empirical literature on LLM evaluation. Yet the theoretical basis \u2026"}, {"title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs", "link": "https://arxiv.org/pdf/2407.00653", "details": "Y Zhang, X Wang, J Liang, S Xia, L Chen, Y Xiao - arXiv preprint arXiv:2407.00653, 2024", "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new \u2026"}, {"title": "Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement", "link": "https://arxiv.org/pdf/2407.01461", "details": "Z Huang, X Wang, F Zhang, Z Xu, C Zhang, X Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The capacity of large language models (LLMs) to generate honest, harmless, and helpful responses heavily relies on the quality of user prompts. However, these prompts often tend to be brief and vague, thereby significantly limiting the full \u2026"}]
