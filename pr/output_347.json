'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [A Comparison of Parameter-Efficient ASR Domain Adaptation Me'
[{"title": "Envisioning MedCLIP: A Deep Dive into Explainability for Medical Vision-Language Models", "link": "https://arxiv.org/html/2403.18996v1", "details": "AUR Hashmi, D Mahapatra, M Yaqub - arXiv preprint arXiv:2403.18996, 2024", "abstract": "Explaining Deep Learning models is becoming increasingly important in the face of daily emerging multimodal models, particularly in safety-critical domains like medical imaging. However, the lack of detailed investigations into the performance of \u2026"}, {"title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.07860", "details": "S Zhao, S Hao, B Zi, H Xu, KYK Wong - arXiv preprint arXiv:2403.07860, 2024", "abstract": "Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding \u2026"}, {"title": "$\\mathbf {(N, K)} $-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model", "link": "https://arxiv.org/html/2403.07191v1", "details": "Y Zhang, L Chen, B Liu, Y Yang, Q Cui, Y Tao, H Yang - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these \u2026"}, {"title": "Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous Driving", "link": "https://arxiv.org/html/2403.05907v1", "details": "J Cao, Z Li, N Wang, C Ma - arXiv preprint arXiv:2403.05907, 2024", "abstract": "Recent studies have highlighted the promising application of NeRF in autonomous driving contexts. However, the complexity of outdoor environments, combined with the restricted viewpoints in driving scenarios, complicates the task of precisely \u2026"}, {"title": "A Comprehensive Overhaul of Multimodal Assistant with Small Language Models", "link": "https://arxiv.org/pdf/2403.06199", "details": "M Zhu, Y Zhu, X Liu, N Liu, Z Xu, C Shen, Y Peng, Z Ou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal Large Language Models (MLLMs) have showcased impressive skills in tasks related to visual understanding and reasoning. Yet, their widespread application faces obstacles due to the high computational demands during both the \u2026"}, {"title": "RAFT: Adapting Language Model to Domain Specific RAG", "link": "https://arxiv.org/pdf/2403.10131", "details": "T Zhang, SG Patil, N Jain, S Shen, M Zaharia, I Stoica\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (eg, time-critical news, or private \u2026"}, {"title": "Algorithmic progress in language models", "link": "https://arxiv.org/html/2403.05812v1", "details": "A Ho, T Besiroglu, E Erdil, D Owen, R Rahman, ZC Guo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We investigate the rate at which algorithms for pre-training language models have improved since the advent of deep learning. Using a dataset of over 200 language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we find that \u2026"}, {"title": "Common 7B Language Models Already Possess Strong Math Capabilities", "link": "https://arxiv.org/html/2403.04706v1", "details": "C Li, W Wang, J Hu, Y Wei, N Zheng, H Hu, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits \u2026"}, {"title": "Accelerating convergence of score-based diffusion models, provably", "link": "https://arxiv.org/pdf/2403.03852", "details": "G Li, Y Huang, T Efimov, Y Wei, Y Chi, Y Chen - arXiv preprint arXiv:2403.03852, 2024", "abstract": "Score-based diffusion models, while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase. Despite a flurry of recent activities towards speeding up \u2026"}]
