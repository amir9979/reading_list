[{"title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding", "link": "https://arxiv.org/pdf/2412.16158%3F", "details": "C Tao, S Su, X Zhu, C Zhang, Z Chen, J Liu, W Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advance of Large Language Models (LLMs) has catalyzed the development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid modality-specific encoders, offer a promising alternative to the compositional ones \u2026"}, {"title": "Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models", "link": "https://arxiv.org/pdf/2412.18609%3F", "details": "J Yi, ST Wasim, Y Luo, M Naseer, J Gall - arXiv preprint arXiv:2412.18609, 2024", "abstract": "We present an efficient encoder-free approach for video-language understanding that achieves competitive performance while significantly reducing computational overhead. Current video-language models typically rely on heavyweight image \u2026"}, {"title": "Double Visual Defense: Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness", "link": "https://arxiv.org/pdf/2501.09446", "details": "Z Wang, C Xie, B Bartoldson, B Kailkhura - arXiv preprint arXiv:2501.09446, 2025", "abstract": "This paper investigates the robustness of vision-language models against adversarial visual perturbations and introduces a novel``double visual defense\" to enhance this robustness. Unlike previous approaches that resort to lightweight \u2026"}, {"title": "Assessing the factors militating against the effective implementation of electronic health records (EHR) in Nigeria", "link": "https://www.nature.com/articles/s41598-024-83009-y", "details": "AE Babatope, IP Adewumi, DO Ajisafe, KO Adepoju\u2026 - Scientific Reports, 2024", "abstract": "This study assessed the factors militating against the effective implementation of electronic health records (EHR) in Nigeria, the computerization of patients' health records with a lot of benefits including improved patients' satisfaction, improved care \u2026"}, {"title": "On Linear Representations and Pretraining Data Frequency in Language Models", "link": "https://openreview.net/pdf%3Fid%3D90tmmXyaaV", "details": "J Merullo, NA Smith, S Wiegreffe, Y Elazar", "abstract": "Pretraining data has a direct impact on the behaviors and quality of language models (LMs), but we only understand the most basic principles of this relationship. While most work focuses on pretraining data and downstream task behavior, we look at the \u2026"}, {"title": "Few-Shot Adaptation of Training-Free Foundation Model for 3D Medical Image Segmentation", "link": "https://arxiv.org/pdf/2501.09138", "details": "X He, Y Hu, Z Zhou, M Jarraya, F Liu - arXiv preprint arXiv:2501.09138, 2025", "abstract": "Vision foundation models have achieved remarkable progress across various image analysis tasks. In the image segmentation task, foundation models like the Segment Anything Model (SAM) enable generalizable zero-shot segmentation through user \u2026"}, {"title": "scReader: Prompting Large Language Models to Interpret scRNA-seq Data", "link": "https://arxiv.org/pdf/2412.18156", "details": "C Li, Q Long, Y Zhou, M Xiao - arXiv preprint arXiv:2412.18156, 2024", "abstract": "Large language models (LLMs) have demonstrated remarkable advancements, primarily due to their capabilities in modeling the hidden relationships within text sequences. This innovation presents a unique opportunity in the field of life sciences \u2026"}, {"title": "Foundations of Large Language Models", "link": "https://arxiv.org/pdf/2501.09223", "details": "T Xiao, J Zhu - arXiv preprint arXiv:2501.09223, 2025", "abstract": "This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting- edge technologies. The book is structured into four main chapters, each exploring a \u2026"}]
