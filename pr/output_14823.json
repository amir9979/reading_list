[{"title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models", "link": "https://arxiv.org/pdf/2503.18931", "details": "Y Chen, L Meng, W Peng, Z Wu, YG Jiang - arXiv preprint arXiv:2503.18931, 2025", "abstract": "Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of \u2026"}, {"title": "ViLBench: A Suite for Vision-Language Process Reward Modeling", "link": "https://arxiv.org/pdf/2503.20271", "details": "H Tu, W Feng, H Chen, H Liu, X Tang, C Xie - arXiv preprint arXiv:2503.20271, 2025", "abstract": "Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on \u2026"}, {"title": "GPBench: A Comprehensive and Fine-Grained Benchmark for Evaluating Large Language Models as General Practitioners", "link": "https://arxiv.org/pdf/2503.17599", "details": "Z Li, Y Yang, J Lang, W Jiang, Y Zhao, S Li, D Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "General practitioners (GPs) serve as the cornerstone of primary healthcare systems by providing continuous and comprehensive medical services. However, due to community-oriented nature of their practice, uneven training and resource gaps, the \u2026"}, {"title": "Can large language models independently complete tasks? A dynamic evaluation framework for multi-turn task planning and completion", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225008070", "details": "J Gao, J Cui, H Wu, L Xiang, H Zhao, X Li, M Fang\u2026 - Neurocomputing, 2025", "abstract": "Large language models (LLMs) are increasingly relied upon for multi-turn dialogue to conduct complex tasks. However, existing benchmarks mainly evaluate LLMs as agents, overlooking their potential as independent systems to accomplish complex \u2026"}, {"title": "CEFW: A Comprehensive Evaluation Framework for Watermark in Large Language Models", "link": "https://arxiv.org/pdf/2503.20802", "details": "S Zhang, B Cheng, J Han, Y Chen, Z Wu, C Li, P Gu - arXiv preprint arXiv:2503.20802, 2025", "abstract": "Text watermarking provides an effective solution for identifying synthetic text generated by large language models. However, existing techniques often focus on satisfying specific criteria while ignoring other key aspects, lacking a unified \u2026"}, {"title": "Knowledge-Centered Dual-Process Reasoning for Math Word Problems with Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10946242/", "details": "J Liu, Z Huang, Q Liu, Z Ma, C Zhai, E Chen - IEEE Transactions on Knowledge and \u2026, 2025", "abstract": "Math word problem (MWP) serves as a critical milestone for assessing the text mining ability and knowledge mastery level of models. Recent advancements have witnessed large language models (LLMs) showcasing remarkable performance on \u2026"}, {"title": "STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in Large Language Models", "link": "https://arxiv.org/pdf/2503.17932", "details": "X Wang, W Wang, Z Ji, Z Li, P Ma, D Wu, S Wang - arXiv preprint arXiv:2503.17932, 2025", "abstract": "Large Language Models (LLMs) have become increasingly vulnerable to jailbreak attacks that circumvent their safety mechanisms. While existing defense methods either suffer from adaptive attacks or require computationally expensive auxiliary \u2026"}, {"title": "Investigating Large Language Models in Diagnosing Students' Cognitive Skills in Math Problem-solving", "link": "https://arxiv.org/pdf/2504.00843%3F", "details": "H Jin, Y Kim, D Jung, S Kim, K Choi, J Son, J Kim - arXiv preprint arXiv:2504.00843, 2025", "abstract": "Mathematics learning entails mastery of both content knowledge and cognitive processing of knowing, applying, and reasoning with it. Automated math assessment primarily has focused on grading students' exhibition of content knowledge by finding \u2026"}, {"title": "Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning Large Language Models with Less Overthinking", "link": "https://arxiv.org/pdf/2503.19602%3F", "details": "Y Ge, S Liu, Y Wang, L Mei, L Chen, B Bi, X Cheng - arXiv preprint arXiv:2503.19602, 2025", "abstract": "Recent advances in Large Language Models (LLMs) have introduced Reasoning Large Language Models (RLLMs), which employ extended thinking processes with reflection and self-correction capabilities, demonstrating the effectiveness of test-time \u2026"}]
