'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Dual Memory Networks: A Versatile Adaptation Approach '
[{"title": "Federated Learning For Heterogeneous Electronic Health Records Utilising Augmented Temporal Graph Attention Networks", "link": "https://proceedings.mlr.press/v238/molaei24a/molaei24a.pdf", "details": "S Molaei, A Thakur, G Niknam, A Soltan, H Zare\u2026 - International Conference on \u2026, 2024", "abstract": "The proliferation of decentralised electronic healthcare records (EHRs) across medical institutions requires innovative federated learning strategies for collaborative data analysis and global model training, prioritising data privacy. A prevalent issue \u2026"}, {"title": "More Room for Language: Investigating the Effect of Retrieval on Language Models", "link": "https://arxiv.org/pdf/2404.10939", "details": "D Samuel, LGG Charpentier, S Wold - arXiv preprint arXiv:2404.10939, 2024", "abstract": "Retrieval-augmented language models pose a promising alternative to standard language modeling. During pretraining, these models search in a corpus of documents for contextually relevant information that could aid the language \u2026"}, {"title": "SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models on Natural Language Inference for Clinical Trials", "link": "https://arxiv.org/pdf/2404.03977", "details": "M Aguiar, P Zweigenbaum, N Naderi - arXiv preprint arXiv:2404.03977, 2024", "abstract": "This paper describes our submission to Task 2 of SemEval-2024: Safe Biomedical Natural Language Inference for Clinical Trials. The Multi-evidence Natural Language Inference for Clinical Trial Data (NLI4CT) consists of a Textual Entailment (TE) task \u2026"}, {"title": "Offset Unlearning for Large Language Models", "link": "https://arxiv.org/pdf/2404.11045", "details": "JY Huang, W Zhou, F Wang, F Morstatter, S Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, harmful, and private content has led to ethical and \u2026"}, {"title": "Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning of Large Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29879/31534", "details": "D Wang, L Dou, W Zhang, J Zeng, W Che - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Numerical reasoning is a vital capability for natural language processing models to understand and process numerical information in real-world scenarios. Most current methods first generate the Intermediate Meaning Representations (IMRs) of \u2026"}]
