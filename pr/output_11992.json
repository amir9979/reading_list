[{"title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2501.11175", "details": "Y Bendou, A Ouasfi, V Gripon, A Boukhayma - arXiv preprint arXiv:2501.11175, 2025", "abstract": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has led to its widespread application in various visual downstream tasks. To enhance CLIP's effectiveness and versatility, efficient few-shot adaptation techniques have been \u2026"}, {"title": "CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification", "link": "https://arxiv.org/pdf/2501.12266", "details": "C Patr\u00edcio, I Rio-Torto, JS Cardoso, LF Teixeira\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter \u2026"}, {"title": "Towards normalized clinical information extraction in Chinese radiology report with large language models", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425002076", "details": "Q Xu, X Xu, C Zhou, Z Liu, F Huang, S Li, L Zhu, Z Bai\u2026 - Expert Systems with \u2026, 2025", "abstract": "Radiology reports serve as a fundamental component within electronic medical records. Converting unstructured free-text reports into structured formats holds paramount importance for the management and utilization of radiology reports. In this \u2026"}, {"title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training", "link": "https://arxiv.org/pdf/2501.11425", "details": "S Yuan, Z Chen, Z Xi, J Ye, Z Du, J Chen - arXiv preprint arXiv:2501.11425, 2025", "abstract": "Large Language Models (LLMs) agents are increasingly pivotal for addressing complex tasks in interactive environments. Existing work mainly focuses on enhancing performance through behavior cloning from stronger experts, yet such \u2026"}]
