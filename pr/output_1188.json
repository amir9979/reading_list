'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [FairPair: A Robust Evaluation of Biases in Language Mo'
[{"title": "Applications of GPT in Political Science Research", "link": "https://kyuwon-lee.github.io/research/gpt_polisci.pdf", "details": "K Lee, S Paci, J Park, HY You, S Zheng", "abstract": "This paper explores the transformative role of GPT in political science research, demonstrating its potential to streamline data collection and analysis processes. By automating the extraction of information from diverse data sources\u2014such as \u2026"}, {"title": "Annot-Mix: Learning with Noisy Class Labels from Multiple Annotators via a Mixup Extension", "link": "https://arxiv.org/pdf/2405.03386", "details": "M Herde, L L\u00fchrs, D Huseljic, B Sick - arXiv preprint arXiv:2405.03386, 2024", "abstract": "Training with noisy class labels impairs neural networks' generalization performance. In this context, mixup is a popular regularization technique to improve training robustness by making memorizing false class labels more difficult. However, mixup \u2026"}, {"title": "Recall Them All: Retrieval-Augmented Language Models for Long Object List Extraction from Long Documents", "link": "https://arxiv.org/pdf/2405.02732", "details": "S Singhania, S Razniewski, G Weikum - arXiv preprint arXiv:2405.02732, 2024", "abstract": "Methods for relation extraction from text mostly focus on high precision, at the cost of limited recall. High recall is crucial, though, to populate long lists of object entities that stand in a specific relation with a given subject. Cues for relevant objects can be \u2026"}, {"title": "Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think", "link": "https://arxiv.org/pdf/2404.08382", "details": "X Wang, C Hu, B Ma, P R\u00f6ttger, B Plank - arXiv preprint arXiv:2404.08382, 2024", "abstract": "Multiple choice questions (MCQs) are commonly used to evaluate the capabilities of large language models (LLMs). One common way to evaluate the model response is to rank the candidate answers based on the log probability of the first token \u2026"}, {"title": "NGLUEni: Benchmarking and Adapting Pretrained Language Models for Nguni Languages", "link": "https://openreview.net/pdf%3Fid%3DDAirSCt8kO", "details": "F Meyer, H Song, A Chakrabarty, J Buys, R Dabre\u2026 - 5th Workshop on African Natural \u2026", "abstract": "The Nguni languages have over 20 million home language speakers in South Africa. There has been considerable growth in datasets for Nguni languages, but no analysis of performance of NLP models for these languages has been reported \u2026"}, {"title": "Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language Selection and Cross-lingual Self-Distillation", "link": "https://arxiv.org/pdf/2404.08491", "details": "H Zhao, Z Cai, S Si, L Chen, Y He, K An, B Chang - arXiv preprint arXiv:2404.08491, 2024", "abstract": "Large-scale multilingual Pretrained Language Models (mPLMs) yield impressive performance on cross-language tasks, yet significant performance disparities exist across different languages within the same mPLM. Previous studies endeavored to \u2026"}, {"title": "Rethinking Software Engineering in the Foundation Model Era: From Task-Driven AI Copilots to Goal-Driven AI Pair Programmers", "link": "https://arxiv.org/pdf/2404.10225", "details": "AE Hassan, GA Oliva, D Lin, B Chen, Z Ming - arXiv preprint arXiv:2404.10225, 2024", "abstract": "The advent of Foundation Models (FMs) and AI-powered copilots has transformed the landscape of software development, offering unprecedented code completion capabilities and enhancing developer productivity. However, the current task-driven \u2026"}, {"title": "A self-supervised text-vision framework for automated brain abnormality detection", "link": "https://arxiv.org/pdf/2405.02782", "details": "DA Wood, E Guilhem, S Kafiabadi, AA Busaidi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Artificial neural networks trained on large, expert-labelled datasets are considered state-of-the-art for a range of medical image recognition tasks. However, categorically labelled datasets are time-consuming to generate and constrain \u2026"}, {"title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training", "link": "https://arxiv.org/pdf/2405.03133", "details": "Z Zhong, M Xia, D Chen, M Lewis - arXiv preprint arXiv:2405.03133, 2024", "abstract": "Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture, SMEAR, was proposed \u2026"}]
