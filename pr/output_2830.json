[{"title": "DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling Language Models", "link": "https://openreview.net/pdf%3Fid%3Dpvg1OdUtDQ", "details": "S Lu, W Zhao, C Tao, A Gupta, S Wu, T Chung, N Peng - Forty-first International Conference \u2026", "abstract": "NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable generation with large language models. It is designed to avoid catastrophic forgetting while achieving guaranteed convergence to an entropy-maximized closed-form \u2026"}, {"title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective", "link": "https://arxiv.org/pdf/2405.16747", "details": "A Tomihari, I Sato - arXiv preprint arXiv:2405.16747, 2024", "abstract": "The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data. This success is largely \u2026"}, {"title": "Is On-Device AI Broken and Exploitable? Assessing the Trust and Ethics in Small Language Models", "link": "https://arxiv.org/pdf/2406.05364", "details": "K Nakka, J Dani, N Saxena - arXiv preprint arXiv:2406.05364, 2024", "abstract": "In this paper, we present a very first study to investigate trust and ethical implications of on-device artificial intelligence (AI), focusing on''small''language models (SLMs) amenable for personal devices like smartphones. While on-device SLMs promise \u2026"}, {"title": "llmNER:(Zero| Few)-Shot Named Entity Recognition, Exploiting the Power of Large Language Models", "link": "https://arxiv.org/pdf/2406.04528", "details": "F Villenaa, L Mirandab, C Aracenab - arXiv preprint arXiv:2406.04528, 2024", "abstract": "Large language models (LLMs) allow us to generate high-quality human-like text. One interesting task in natural language processing (NLP) is named entity recognition (NER), which seeks to detect mentions of relevant information in \u2026"}, {"title": "Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?", "link": "https://arxiv.org/pdf/2406.07546", "details": "X Fu, M He, Y Lu, WY Wang, D Roth - arXiv preprint arXiv:2406.07546, 2024", "abstract": "We present a novel task and benchmark for evaluating the ability of text-to-image (T2I) generation models to produce images that fit commonsense in real life, which we call Commonsense-T2I. Given two adversarial text prompts containing an \u2026"}, {"title": "Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation", "link": "https://arxiv.org/pdf/2405.19958", "details": "Y Liu, X Liu, X Zhu, W Hu - arXiv preprint arXiv:2405.19958, 2024", "abstract": "Multi-aspect controllable text generation aims to control the generated texts in attributes from multiple aspects (eg,\" positive\" from sentiment and\" sport\" from topic). For ease of obtaining training samples, existing works neglect attribute correlations \u2026"}, {"title": "Cost-Effective LLM Utilization for Machine Learning Tasks over Tabular Data", "link": "https://dl.acm.org/doi/pdf/10.1145/3665601.3669848", "details": "Y Einy, T Milo, S Novgorodov - Proceedings of the Conference on Governance \u2026, 2024", "abstract": "Classic machine learning (ML) models excel in modeling tabular datasets but lack broader world knowledge due to the absence of pre-training, an area where Large Language Models (LLMs) stand out. This paper presents an effective method that \u2026"}, {"title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents", "link": "https://openreview.net/pdf%3Fid%3DDwTgy1hXXo", "details": "K Zhu, J Wang, Q Zhao, R Xu, X Xie - Forty-first International Conference on Machine \u2026", "abstract": "Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily \u2026"}, {"title": "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "link": "https://arxiv.org/pdf/2405.16908", "details": "G Yona, R Aharoni, M Geva - arXiv preprint arXiv:2405.16908, 2024", "abstract": "We posit that large language models (LLMs) should be capable of expressing their intrinsic uncertainty in natural language. For example, if the LLM is equally likely to output two contradicting answers to the same question, then its generated response \u2026"}]
