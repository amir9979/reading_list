[{"title": "Masked Contrastive Representation Learning for Self-Supervised Visual Pre-Training", "link": "https://ieeexplore.ieee.org/abstract/document/10722789/", "details": "Y Yao, N Desai, M Palaniswami - 2024 IEEE 11th International Conference on Data \u2026, 2024", "abstract": "Self-supervised learning has achieved state-of-the-art performance in various tasks and applications. In computer vision, self-supervised learning often employs contrastive learning and masked image modeling, each with its limitations \u2026"}, {"title": "TLDR: Token-Level Detective Reward Model for Large Vision Language Models", "link": "https://arxiv.org/pdf/2410.04734", "details": "D Fu, T Xiao, R Wang, W Zhu, P Zhang, G Pang, R Jia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Although reward models have been successful in improving multimodal large language models, the reward models themselves remain brutal and contain minimal information. Notably, existing reward models only mimic human annotations by \u2026"}, {"title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures", "link": "https://arxiv.org/pdf/2410.07698", "details": "Y Chen, Y Zhang, L Cao, K Yuan, Z Wen - arXiv preprint arXiv:2410.07698, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs when adapting large language models (LLMs) for downstream applications. However, traditional first-order (FO) fine-tuning algorithms incur substantial memory overhead \u2026"}]
