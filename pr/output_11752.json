[{"title": "Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models", "link": "https://arxiv.org/pdf/2412.16545", "details": "Z Zhang, Y Wang, X Huang, T Fang, H Zhang, C Deng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models have shown remarkable performance across a wide range of language tasks, owing to their exceptional capabilities in context modeling. The most commonly used method of context modeling is full self-attention, as seen in \u2026"}, {"title": "Privacy preserving strategies for electronic health records in the era of large language models", "link": "https://www.nature.com/articles/s41746-025-01429-0", "details": "J Jonnagaddala, ZSY Wong - npj Digital Medicine, 2025", "abstract": "Electronic health records (EHRs) secondary usage with large language models (LLMs) raise privacy challenges. National regulations like GDPR and HIPAA offer protection frameworks, but specific strategies are needed to mitigate risk in \u2026"}, {"title": "Optimizing Fine-Tuning in Quantized Language Models: An In-Depth Analysis of Key Variables", "link": "https://www.sciencedirect.com/org/science/article/pii/S1546221825000645", "details": "A Shen, Z Lai, D Li, X Hu - Computers, Materials and Continua, 2025", "abstract": "Large-scale Language Models (LLMs) have achieved significant breakthroughs in Natural Language Processing (NLP), driven by the pre-training and fine-tuning paradigm. While this approach allows models to specialize in specific tasks with \u2026"}, {"title": "Vision Foundation Models for Computed Tomography", "link": "https://arxiv.org/pdf/2501.09001", "details": "S Pai, I Hadzic, D Bontempi, K Bressem, BH Kann\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Foundation models (FMs) have shown transformative potential in radiology by performing diverse, complex tasks across imaging modalities. Here, we developed CT-FM, a large-scale 3D image-based pre-trained model designed explicitly for \u2026"}, {"title": "Frequency Is What You Need: Word-frequency Masking Benefits Vision-Language Model Pre-training", "link": "https://arxiv.org/pdf/2412.16148", "details": "M Liang, M Larson - arXiv preprint arXiv:2412.16148, 2024", "abstract": "Vision Language Models (VLMs) can be trained more efficiently if training sets can be reduced in size. Recent work has shown the benefits of masking text during VLM training using a variety of approaches: truncation, random masking, block masking \u2026"}, {"title": "BenCzechMark: A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism", "link": "https://arxiv.org/pdf/2412.17933", "details": "M Fajcik, M Docekal, J Dolezal, K Ondrej, K Bene\u0161\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present BenCzechMark (BCM), the first comprehensive Czech language benchmark designed for large language models, offering diverse tasks, multiple task formats, and multiple evaluation metrics. Its scoring system is grounded in statistical \u2026"}, {"title": "Large Language Models and Large Multimodal Models in Medical Imaging: A Primer for Physicians", "link": "https://jnm.snmjournals.org/content/early/2025/01/16/jnumed.124.268072", "details": "TJ Bradshaw, X Tie, J Warner, J Hu, Q Li, X Li - Journal of Nuclear Medicine, 2025", "abstract": "Large language models (LLMs) are poised to have a disruptive impact on health care. Numerous studies have demonstrated promising applications of LLMs in medical imaging, and this number will grow as LLMs further evolve into large \u2026"}, {"title": "Exploring the Efficacy of Meta-Learning: Unveiling Superior Data Diversity Utilization of MAML Over Pre-training", "link": "https://arxiv.org/pdf/2501.08506", "details": "K Selva, S Vittayaareekul, B Miranda - arXiv preprint arXiv:2501.08506, 2025", "abstract": "Currently, data and model size dominate the narrative in the training of super-large, powerful models. However, there has been a lack of exploration on the effect of other attributes of the training dataset on model performance. We hypothesize that dataset \u2026"}, {"title": "YuLan-Mini: An Open Data-efficient Language Model", "link": "https://arxiv.org/pdf/2412.17743", "details": "Y Hu, H Song, J Deng, J Wang, J Chen, K Zhou, Y Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly \u2026"}]
