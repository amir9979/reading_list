[{"title": "Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More", "link": "https://arxiv.org/pdf/2502.11494", "details": "Z Wen, Y Gao, S Wang, J Zhang, Q Zhang, W Li, C He\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision tokens in multimodal large language models often dominate huge computational overhead due to their excessive length compared to linguistic modality. Abundant recent methods aim to solve this problem with token pruning \u2026"}, {"title": "Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding", "link": "https://arxiv.org/pdf/2502.09906", "details": "TD Truong, HQ Nguyen, XB Nguyen, A Dowling, X Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual \u2026"}, {"title": "Multidimensional Consistency Improves Reasoning in Language Models", "link": "https://arxiv.org/pdf/2503.02670", "details": "H Lai, X Zhang, M Nissim - arXiv preprint arXiv:2503.02670, 2025", "abstract": "While Large language models (LLMs) have proved able to address some complex reasoning tasks, we also know that they are highly sensitive to input variation, which can lead to different solution paths and final answers. Answer consistency across \u2026"}, {"title": "Rethinking Data: Towards Better Performing Domain-Specific Small Language Models", "link": "https://arxiv.org/pdf/2503.01464", "details": "B Nazarov, D Frolova, Y Lubarsky, A Gaissinski\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Fine-tuning of Large Language Models (LLMs) for downstream tasks, performed on domain-specific data has shown significant promise. However, commercial use of such LLMs is limited by the high computational cost required for their deployment at \u2026"}, {"title": "MedHEval: Benchmarking Hallucinations and Mitigation Strategies in Medical Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.02157", "details": "A Chang, L Huang, P Bhatia, T Kass-Hout, F Ma\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision Language Models (LVLMs) are becoming increasingly important in the medical domain, yet Medical LVLMs (Med-LVLMs) frequently generate hallucinations due to limited expertise and the complexity of medical applications \u2026"}, {"title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks", "link": "https://arxiv.org/pdf/2503.02656", "details": "P Suganthan, F Moiseev, L Yan, J Wu, J Ni, J Han\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Decoder-based transformers, while revolutionizing language modeling and scaling to immense sizes, have not completely overtaken encoder-heavy architectures in natural language processing. Specifically, encoder-only models remain dominant in \u2026"}, {"title": "Consistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models", "link": "https://arxiv.org/pdf/2502.20647", "details": "C Gilhuly, H Shahzad - arXiv preprint arXiv:2502.20647, 2025", "abstract": "Text summarizing is a critical Natural Language Processing (NLP) task with applications ranging from information retrieval to content generation. Large Language Models (LLMs) have shown remarkable promise in generating fluent \u2026"}, {"title": "Physics-Informed Implicit Neural Representations for Joint B0 Estimation and Echo Planar Imaging", "link": "https://arxiv.org/pdf/2503.00230", "details": "W Huang, N Wang, C Liao, Y Lin, M Gao, D Rueckert\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Echo Planar Imaging (EPI) is widely used for its rapid acquisition but suffers from severe geometric distortions due to B0 inhomogeneities, particularly along the phase encoding direction. Existing methods follow a two-step process: reconstructing blip \u2026"}, {"title": "Prompting, Decoding, Embedding: Leveraging Pretrained Language Models for High-quality and Diverse Open Rule Induction", "link": "https://ieeexplore.ieee.org/abstract/document/10906469/", "details": "W Sun, S He, J Zhao, K Liu - IEEE Transactions on Audio, Speech and Language \u2026, 2025", "abstract": "Open rule induction (OpenRI) is devoted to obtaining reasoning rules expressed in natural languages. Compared with traditional rules with predefined logical symbols and domain predicates, open rules are more expressive and easier to use in real \u2026"}]
