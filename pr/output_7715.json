[{"title": "Are Expert-Level Language Models Expert-Level Annotators?", "link": "https://arxiv.org/pdf/2410.03254", "details": "YM Tseng, WL Chen, CC Chen, HH Chen - arXiv preprint arXiv:2410.03254, 2024", "abstract": "Data annotation refers to the labeling or tagging of textual data with relevant information. A large body of works have reported positive results on leveraging LLMs as an alternative to human annotators. However, existing studies focus on classic \u2026"}, {"title": "Erasing Conceptual Knowledge from Language Models", "link": "https://arxiv.org/pdf/2410.02760", "details": "R Gandikota, S Feucht, S Marks, D Bau - arXiv preprint arXiv:2410.02760, 2024", "abstract": "Concept erasure in language models has traditionally lacked a comprehensive evaluation framework, leading to incomplete assessments of effectiveness of erasure methods. We propose an evaluation paradigm centered on three critical criteria \u2026"}, {"title": "Can Language Models Reason about Individualistic Human Values and Preferences?", "link": "https://arxiv.org/pdf/2410.03868", "details": "L Jiang, T Sorensen, S Levine, Y Choi - arXiv preprint arXiv:2410.03868, 2024", "abstract": "Recent calls for pluralistic alignment emphasize that AI systems should address the diverse needs of all people. Yet, efforts in this space often require sorting people into fixed buckets of pre-specified diversity-defining dimensions (eg, demographics \u2026"}, {"title": "General Preference Modeling with Preference Representations for Aligning Language Models", "link": "https://arxiv.org/pdf/2410.02197%3F", "details": "Y Zhang, G Zhang, Y Wu, K Xu, Q Gu - arXiv preprint arXiv:2410.02197, 2024", "abstract": "Modeling human preferences is crucial for aligning foundation models with human values. Traditional reward modeling methods, such as the Bradley-Terry (BT) reward model, fall short in expressiveness, particularly in addressing intransitive \u2026"}, {"title": "POSIX: A Prompt Sensitivity Index For Large Language Models", "link": "https://arxiv.org/pdf/2410.02185", "details": "A Chatterjee, HK Renduchintala, S Bhatia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are found to be surprisingly sensitive to minor variations in prompts, often generating significantly divergent outputs in response to minor variations in the prompts, such as spelling \u2026"}, {"title": "The Role of Deductive and Inductive Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2410.02892", "details": "C Cai, X Zhao, H Liu, Z Jiang, T Zhang, Z Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have achieved substantial progress in artificial intelligence, particularly in reasoning tasks. However, their reliance on static prompt structures, coupled with limited dynamic reasoning capabilities, often constrains their \u2026"}, {"title": "Enhancing Language Model Reasoning via Weighted Reasoning in Self-Consistency", "link": "https://arxiv.org/pdf/2410.07839", "details": "T Knappe, R Li, A Chauhan, K Chhua, K Zhu, S O'Brien - arXiv preprint arXiv \u2026, 2024", "abstract": "While large language models (LLMs) have rapidly improved their performance on a broad number of tasks, they still often fall short on reasoning tasks. As LLMs become more integrated in diverse real-world tasks, advancing their reasoning capabilities is \u2026"}, {"title": "MedQA-CS: Benchmarking Large Language Models Clinical Skills Using an AI-SCE Framework", "link": "https://arxiv.org/pdf/2410.01553%3F", "details": "Z Yao, Z Zhang, C Tang, X Bian, Y Zhao, Z Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Artificial intelligence (AI) and large language models (LLMs) in healthcare require advanced clinical skills (CS), yet current benchmarks fail to evaluate these comprehensively. We introduce MedQA-CS, an AI-SCE framework inspired by \u2026"}, {"title": "Large Language Models can be Strong Self-Detoxifiers", "link": "https://arxiv.org/pdf/2410.03818", "details": "CY Ko, PY Chen, P Das, Y Mroueh, S Dan, G Kollias\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Reducing the likelihood of generating harmful and toxic output is an essential task when aligning large language models (LLMs). Existing methods mainly rely on training an external reward model (ie, another language model) or fine-tuning the \u2026"}]
