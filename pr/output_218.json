'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Foresightâ€”a generative pretrained transformer for mod'
[{"title": "Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models", "link": "https://arxiv.org/html/2403.13590v1", "details": "A Liusie, Y Fathullah, MJF Gales - arXiv preprint arXiv:2403.13590, 2024", "abstract": "Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities and versatility in NLP tasks, however they sometimes fail to maintain crucial invariances for specific tasks. One example is permutation sensitivity, where \u2026"}, {"title": "Dynamic Survival Analysis for Early Event Prediction", "link": "https://arxiv.org/html/2403.12818v1", "details": "H Y\u00e8che, M Burger, D Veshchezerova, G R\u00e4tsch - arXiv preprint arXiv:2403.12818, 2024", "abstract": "This study advances Early Event Prediction (EEP) in healthcare through Dynamic Survival Analysis (DSA), offering a novel approach by integrating risk localization into alarm policies to enhance clinical event metrics. By adapting and evaluating \u2026"}, {"title": "Clinical decision support to improve CBC and differential ordering", "link": "https://academic.oup.com/ajcp/advance-article/doi/10.1093/ajcp/aqae024/7632655", "details": "GK Mahowald, KB Lewandrowski, AS Dighe - American Journal of Clinical Pathology, 2024", "abstract": "Objectives Complete blood count and differential (CBC diff) is a common laboratory test that may be overused or misordered, particularly in an inpatient setting. We assessed the ability of a clinical decision support (CDS) alert to decrease \u2026"}, {"title": "Advancing entity recognition in biomedicine via instruction tuning of large language models", "link": "https://academic.oup.com/bioinformatics/advance-article-pdf/doi/10.1093/bioinformatics/btae163/57056267/btae163.pdf", "details": "VK Keloth, Y Hu, Q Xie, X Peng, Y Wang, A Zheng\u2026 - Bioinformatics, 2024", "abstract": "Abstract Motivation Large Language Models (LLMs) have the potential to revolutionize the field of Natural Language Processing (NLP), excelling not only in text generation and reasoning tasks but also in their ability for zero/few-shot learning \u2026"}, {"title": "CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion", "link": "https://arxiv.org/html/2402.14551v1", "details": "Z Long, G Killick, L Zhuang, G Aragon-Camarasa\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "State-of-the-art pre-trained image models predominantly adopt a two-stage approach: initial unsupervised pre-training on large-scale datasets followed by task- specific fine-tuning using Cross-Entropy loss~(CE). However, it has been \u2026"}, {"title": "Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting", "link": "https://arxiv.org/pdf/2403.13369", "details": "P Richter-Pechanski, P Wiesenbach, DM Schwab\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent \u2026"}, {"title": "A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models", "link": "https://arxiv.org/pdf/2402.15422", "details": "S Hegselmann, SZ Shen, F Gierse, M Agrawal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries \u2026"}, {"title": "RelayAttention for Efficient Large Language Model Serving with Long System Prompts", "link": "https://arxiv.org/html/2402.14808v1", "details": "L Zhu, X Wang, W Zhang, RWH Lau - arXiv preprint arXiv:2402.14808, 2024", "abstract": "Practical large language model (LLM) services may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests. However, the long system prompt causes \u2026"}, {"title": "ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical Notes", "link": "https://arxiv.org/pdf/2403.05795", "details": "Z Yang, A Mitra, S Kwon, H Yu - arXiv preprint arXiv:2403.05795, 2024", "abstract": "The advancement of natural language processing (NLP) systems in healthcare hinges on language model ability to interpret the intricate information contained within clinical notes. This process often requires integrating information from various \u2026"}]
