'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Mitigating Social Biases of Pre-trained Language Models via '
[{"title": "Global Contrastive Training for Multimodal Electronic Health Records with Language Supervision", "link": "https://arxiv.org/pdf/2404.06723", "details": "Y Ma, S Kolla, Z Hu, D Kaliraman, V Nolan, Z Guan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern electronic health records (EHRs) hold immense promise in tracking personalized patient health trajectories through sequential deep learning, owing to their extensive breadth, scale, and temporal granularity. Nonetheless, how to \u2026"}, {"title": "SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models on Natural Language Inference for Clinical Trials", "link": "https://arxiv.org/pdf/2404.03977", "details": "M Aguiar, P Zweigenbaum, N Naderi - arXiv preprint arXiv:2404.03977, 2024", "abstract": "This paper describes our submission to Task 2 of SemEval-2024: Safe Biomedical Natural Language Inference for Clinical Trials. The Multi-evidence Natural Language Inference for Clinical Trial Data (NLI4CT) consists of a Textual Entailment (TE) task \u2026"}, {"title": "Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models", "link": "https://arxiv.org/pdf/2404.01992", "details": "S Linzbach, D Dimitrov, L Kallmeyer, K Evang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained Language Models (PLMs) are known to contain various kinds of knowledge. One method to infer relational knowledge is through the use of cloze- style prompts, where a model is tasked to predict missing subjects or objects \u2026"}, {"title": "Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models", "link": "https://arxiv.org/pdf/2404.10209", "details": "S Xue, D Qi, C Jiang, W Shi, F Cheng, K Chen, H Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The recent breakthroughs in large language models (LLMs) are positioned to transition many areas of software. The technologies of interacting with data particularly have an important entanglement with LLMs as efficient and intuitive data \u2026"}, {"title": "Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form on Mathematical Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2404.11500", "details": "Y Zhou, Y Zhu, D Antognini, Y Kim, Y Zhang - arXiv preprint arXiv:2404.11500, 2024", "abstract": "This paper studies the relationship between the surface form of a mathematical problem and its solvability by large language models. We find that subtle alterations in the surface form can significantly impact the answer distribution and the solve rate \u2026"}, {"title": "Systematic synthesis of design prompts for large language models in conceptual design", "link": "https://www.sciencedirect.com/science/article/pii/S000785062400074X", "details": "Y Tian, A Liu, Y Dai, K Nagato, M Nakao - CIRP Annals, 2024", "abstract": "Recent advancements in large language models (LLMs) demonstrate great potential in supporting engineering design, especially conceptual design. Prompt engineering plays an important role in facilitating designer-LLM collaboration in conceptual \u2026"}, {"title": "Leveraging Large Language Models for Multimodal Search", "link": "https://arxiv.org/pdf/2404.15790", "details": "O Barbany, M Huang, X Zhu, A Dhua - arXiv preprint arXiv:2404.15790, 2024", "abstract": "Multimodal search has become increasingly important in providing users with a natural and effective way to ex-press their search intentions. Images offer fine- grained details of the desired products, while text allows for easily incorporating \u2026"}, {"title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback", "link": "https://arxiv.org/pdf/2404.00934", "details": "Z Hou, Y Niu, Z Du, X Zhang, X Liu, A Zeng, Q Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline--a reinforcement learning from human feedback (RLHF) system--designed to enhance \u2026"}, {"title": "MedExpQA: Multilingual Benchmarking of Large Language Models for Medical Question Answering", "link": "https://arxiv.org/pdf/2404.05590", "details": "I Alonso, M Oronoz, R Agerri - arXiv preprint arXiv:2404.05590, 2024", "abstract": "Large Language Models (LLMs) have the potential of facilitating the development of Artificial Intelligence technology to assist medical experts for interactive decision support, which has been demonstrated by their competitive performances in Medical \u2026"}]
