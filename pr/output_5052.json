[{"title": "How Well Can Vision Language Models See Image Details?", "link": "https://arxiv.org/pdf/2408.03940", "details": "C Gou, A Felemban, FF Khan, D Zhu, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Model-based Vision-Language Models (LLM-based VLMs) have demonstrated impressive results in various vision-language understanding tasks. However, how well these VLMs can see image detail beyond the semantic level \u2026"}, {"title": "Gemma 2: Improving open language models at a practical size", "link": "https://arxiv.org/pdf/2408.00118", "details": "G Team, M Riviere, S Pathak, PG Sessa, C Hardin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to \u2026"}, {"title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models", "link": "https://arxiv.org/pdf/2408.03907", "details": "SH Kumar, S Sahay, S Mazumder, E Okur\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have excelled at language understanding and generating human-level text. However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users \u2026"}, {"title": "Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models", "link": "https://arxiv.org/pdf/2407.19474", "details": "N Bitton-Guetta, A Slobodkin, A Maimon, E Habba\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Imagine observing someone scratching their arm; to understand why, additional context would be necessary. However, spotting a mosquito nearby would immediately offer a likely explanation for the person's discomfort, thereby alleviating \u2026"}, {"title": "Leveraging Language Models and Automatic Summarization in Online Programming Learning Environments", "link": "https://dl.acm.org/doi/full/10.1145/3653323", "details": "C Areces, L Benotti, F Bulgarelli, E Echeveste, N Finzi - Communications of the ACM", "abstract": "Objective A. Enhance the interaction between tutors, the Mumuki platform, and the group of trainee programmers. By utilizing the stochastic language models of learners' errors in each programming language, training errors in the exercise are \u2026"}, {"title": "XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models", "link": "https://aclanthology.org/2024.bionlp-1.35/", "details": "OC Thawakar, AM Shaker, SS Mullappilly, H Cholakkal\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "The latest breakthroughs in large language models (LLMs) and vision-language models (VLMs) have showcased promising capabilities toward performing a wide range of tasks. Such models are typically trained on massive datasets comprising \u2026"}, {"title": "ULLME: A Unified Framework for Large Language Model Embeddings with Generation-Augmented Learning", "link": "https://arxiv.org/pdf/2408.03402", "details": "H Man, NT Ngo, F Dernoncourt, TH Nguyen - arXiv preprint arXiv:2408.03402, 2024", "abstract": "Large Language Models (LLMs) excel in various natural language processing tasks, but leveraging them for dense passage embedding remains challenging. This is due to their causal attention mechanism and the misalignment between their pre-training \u2026"}, {"title": "DiffCoder: Enhancing Large Language Model on API Invocation via Analogical Code Exercises", "link": "https://dl.acm.org/doi/pdf/10.1145/3643745", "details": "D Zan, A Yu, B Shen, B Chen, W Li, Y Gong, X Chen\u2026 - Proceedings of the ACM on \u2026, 2024", "abstract": "The task of code generation aims to generate code solutions based on given programming problems. Recently, code large language models (code LLMs) have shed new light on this task, owing to their formidable code generation capabilities \u2026"}]
