[{"title": "Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models", "link": "https://arxiv.org/pdf/2503.15888", "details": "B Bi, S Liu, Y Wang, Y Xu, J Fang, L Mei, X Cheng - arXiv preprint arXiv:2503.15888, 2025", "abstract": "Retrieval-Augmented Generation (RAG) mitigates hallucinations in Large Language Models (LLMs) by integrating external knowledge. However, conflicts between parametric knowledge and retrieved context pose challenges, particularly when \u2026"}, {"title": "Inference retrieval-augmented multi-modal chain-of-thoughts reasoning for language models", "link": "https://ieeexplore.ieee.org/abstract/document/10888701/", "details": "Q He, S Qian, J Zhang, C Wang - ICASSP 2025-2025 IEEE International Conference \u2026, 2025", "abstract": "Recent advancements in Large Language Models (LLMs) have catalyzed the exploration of Chain of Thought (CoT) approaches, particularly in extending their application to multimodal tasks to enhance reasoning capabilities. However, current \u2026"}, {"title": "Behaviour Discovery and Attribution for Explainable Reinforcement Learning", "link": "https://arxiv.org/pdf/2503.14973%3F", "details": "R Rishav, S Nath, V Michalski, SE Kahou - arXiv preprint arXiv:2503.14973, 2025", "abstract": "Explaining the decisions made by reinforcement learning (RL) agents is critical for building trust and ensuring reliability in real-world applications. Traditional approaches to explainability often rely on saliency analysis, which can be limited in \u2026"}, {"title": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection", "link": "https://arxiv.org/pdf/2504.00695", "details": "X Zhu, Z Gu, S Zheng, T Wang, T Li, H Feng, Y Xiao - arXiv preprint arXiv:2504.00695, 2025", "abstract": "Pre-training large language models (LLMs) necessitates enormous diverse textual corpora, making effective data selection a key challenge for balancing computational resources and model performance. Current methodologies primarily emphasize data \u2026"}, {"title": "Hybrid Fine-Tuning of Large Language Models Using LoRA: Enhancing Multi-Task Text Classification Through Knowledge Sharing", "link": "https://jecei.sru.ac.ir/article_2303.html", "details": "J Salimi Sartakhti, A Beirnvand, M Sarhadi - Journal of Electrical and Computer \u2026, 2025", "abstract": "Background and Objectives: Large Language Models have demonstrated\u200e exceptional performance across various NLP tasks, especially when fine-tuned for\u200e specific applications.\u200e\u200f\u200f Full fine-tuning of large language models requires extensive\u200e \u2026"}, {"title": "OUTLIER-AWARE PREFERENCE OPTIMIZATION FOR LARGE LANGUAGE MODELS", "link": "https://openreview.net/pdf%3Fid%3DYevRFGa9I7", "details": "P Srivastava, SS Nalli, A Deshpande, A Sharma - \u2026 in Foundation Models: The Next Frontier in \u2026", "abstract": "Aligning large language models (LLMs) to user preferences often relies on learning a reward model as a proxy from feedback. However, such reward models can fail on out-of-distribution examples and, if kept static, may reinforce incorrect preferences \u2026"}]
