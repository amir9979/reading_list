[{"title": "Towards Medical Vision-Language Contrastive Pre-training via Study-Oriented Semantic Exploration", "link": "https://openreview.net/pdf%3Fid%3DVIPZtona4Q", "details": "LIU BO, LU ZEXIN, Y Wang - ACM Multimedia 2024", "abstract": "Contrastive vision-language pre-training has shown great promise in representation transfer learning and cross-modality learning in the medical field. However, without fully exploiting the intrinsic properties and correlations of multimodal medical data \u2026"}, {"title": "Large language models as reliable knowledge bases?", "link": "https://arxiv.org/pdf/2407.13578", "details": "D Zheng, M Lapata, JZ Pan - arXiv preprint arXiv:2407.13578, 2024", "abstract": "The NLP community has recently shown a growing interest in leveraging Large Language Models (LLMs) for knowledge-intensive tasks, viewing LLMs as potential knowledge bases (KBs). However, the reliability and extent to which LLMs can \u2026"}, {"title": "Patch-Level Training for Large Language Models", "link": "https://arxiv.org/pdf/2407.12665", "details": "C Shao, F Meng, J Zhou - arXiv preprint arXiv:2407.12665, 2024", "abstract": "As Large Language Models (LLMs) achieve remarkable progress in language understanding and generation, their training efficiency has become a critical concern. Traditionally, LLMs are trained to predict the next token in a sequence \u2026"}, {"title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2407.11681", "details": "H Cheng, M Zhang, JQ Shi - arXiv preprint arXiv:2407.11681, 2024", "abstract": "As Large Language Models (LLMs) grow dramatically in size, there is an increasing trend in compressing and speeding up these models. Previous studies have highlighted the usefulness of gradients for importance scoring in neural network \u2026"}, {"title": "Not All Attention is Needed: Parameter and Computation Efficient Tuning for Multi-modal Large Language Models via Effective Attention Skipping", "link": "https://arxiv.org/pdf/2407.14093", "details": "Q Wu, Z Ke, Y Zhou, G Luo, X Sun, R Ji - arXiv preprint arXiv:2407.14093, 2024", "abstract": "Recently, mixture of experts (MoE) has become a popular paradigm for achieving the trade-off between modal capacity and efficiency of multi-modal large language models (MLLMs). Different from previous efforts, we are dedicated to exploring the \u2026"}]
