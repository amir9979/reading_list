[{"title": "Limits of transformer language models on learning to compose algorithms", "link": "https://openreview.net/pdf%3Fid%3Dx7AD0343Jz", "details": "J Thomm, G Camposampiero, A Terzic, M Hersche\u2026 - The Thirty-eighth Annual \u2026, 2024", "abstract": "We analyze the capabilities of Transformer language models in learning compositional discrete tasks. To this end, we evaluate training LLaMA models and prompting GPT-4 and Gemini on four tasks demanding to learn a composition of \u2026"}, {"title": "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "link": "https://aclanthology.org/2024.emnlp-main.565.pdf", "details": "J Li, H Zhang, F Zhang, TW Chang, K Kuang, L Chen\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Reinforcement learning from human feedback (RLHF) and AI-generated feedback (RLAIF) have become prominent techniques that significantly enhance the functionality of pre-trained language models (LMs). These methods harness \u2026"}, {"title": "Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding", "link": "https://arxiv.org/pdf/2411.04282", "details": "H Chen, Y Feng, Z Liu, W Yao, A Prabhakar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time \u2026"}, {"title": "DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing", "link": "https://aclanthology.org/2024.emnlp-main.1132.pdf", "details": "D Das, V Khetan - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine- tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data \u2026"}, {"title": "DA3: A Distribution-Aware Adversarial Attack against Language Models", "link": "https://aclanthology.org/2024.emnlp-main.107.pdf", "details": "Y Wang, X Dong, J Caverlee, SY Philip - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "Abstract Language models can be manipulated by adversarial attacks, which introduce subtle perturbations to input data. While recent attack methods can achieve a relatively high attack success rate (ASR), we've observed that the generated \u2026"}, {"title": "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "link": "https://aclanthology.org/2024.findings-emnlp.61.pdf", "details": "X Wei, H Chen, H Yu, H Fei, Q Liu - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have achieved notable success in commonsense reasoning tasks, benefiting from their extensive world knowledge acquired through extensive pretraining. While approaches like Chain-of-Thought \u2026"}, {"title": "Explaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective on Molecule Graphs", "link": "https://aclanthology.org/2024.findings-emnlp.415.pdf", "details": "Y He, Z Zheng, P Soga, Y Zhu, Y Dong, J Li - Findings of the Association for \u2026, 2024", "abstract": "Abstract In recent years, Graph Neural Networks (GNNs) have become successful in molecular property prediction tasks such as toxicity analysis. However, due to the black-box nature of GNNs, their outputs can be concerning in high-stakes decision \u2026"}, {"title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding", "link": "https://aclanthology.org/2024.emnlp-main.870.pdf", "details": "L Tu, S Yavuz, J Qu, J Xu, R Meng, C Xiong, Y Zhou - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired \u2026"}, {"title": "Summarization-Based Document IDs for Generative Retrieval with Language Models", "link": "https://aclanthology.org/2024.wikinlp-1.18.pdf", "details": "A Li, D Cheng, P Keung, J Kasai, NA Smith - Proceedings of the First Workshop on \u2026, 2024", "abstract": "Generative retrieval (Wang et al., 2022; Tay et al., 2022) is a popular approach for end-to-end document retrieval that directly generates document identifiers given an input query. We introduce summarization-based document IDs, in which each \u2026"}]
