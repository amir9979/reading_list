[{"title": "IQBench: How \"Smart'' Are Vision-Language Models? A Study with Human IQ Tests", "link": "https://arxiv.org/pdf/2505.12000", "details": "TH Pham, PV Nguyen, DT Hung, BT Duong, VN Thanh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Although large Vision-Language Models (VLMs) have demonstrated remarkable performance in a wide range of multimodal tasks, their true reasoning capabilities on human IQ tests remain underexplored. To advance research on the fluid intelligence \u2026", "entry_id": "http://arxiv.org/abs/2505.12000v1", "updated": "2025-05-17 13:24:08", "published": "2025-05-17 13:24:08", "authors": "Tan-Hanh Pham;Phu-Vinh Nguyen;Dang The Hung;Bui Trong Duong;Vu Nguyen Thanh;Chris Ngo;Tri Quang Truong;Truong-Son Hy", "summary": "Although large Vision-Language Models (VLMs) have demonstrated remarkable\nperformance in a wide range of multimodal tasks, their true reasoning\ncapabilities on human IQ tests remain underexplored. To advance research on the\nfluid intelligence of VLMs, we introduce **IQBench**, a new benchmark designed\nto evaluate VLMs on standardized visual IQ tests. We focus on evaluating the\nreasoning capabilities of VLMs, which we argue are more important than the\naccuracy of the final prediction. **Our benchmark is visually centric,\nminimizing the dependence on unnecessary textual content**, thus encouraging\nmodels to derive answers primarily from image-based information rather than\nlearned textual knowledge. To this end, we manually collected and annotated 500\nvisual IQ questions to **prevent unintentional data leakage during training**.\nUnlike prior work that focuses primarily on the accuracy of the final answer,\nwe evaluate the reasoning ability of the models by assessing their explanations\nand the patterns used to solve each problem, along with the accuracy of the\nfinal prediction and human evaluation. Our experiments show that there are\nsubstantial performance disparities between tasks, with models such as\n`o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet` achieving the highest\naverage accuracies of 0.615, 0.578, and 0.548, respectively. However, all\nmodels struggle with 3D spatial and anagram reasoning tasks, highlighting\nsignificant limitations in current VLMs' general reasoning abilities. In terms\nof reasoning scores, `o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet`\nachieved top averages of 0.696, 0.586, and 0.516, respectively. These results\nhighlight inconsistencies between the reasoning processes of the models and\ntheir final answers, emphasizing the importance of evaluating the accuracy of\nthe reasoning in addition to the final predictions.", "comment": "IQ Test for Multimodal Models", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.12000v1;http://arxiv.org/pdf/2505.12000v1", "pdf_url": "http://arxiv.org/pdf/2505.12000v1"}, {"title": "Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models", "link": "https://arxiv.org/pdf/2505.10446", "details": "Z Huang, Z Chen, Z Wang, T Li, GJ Qi - arXiv preprint arXiv:2505.10446, 2025", "abstract": "We introduce the\\emph {Diffusion Chain of Lateral Thought (DCoLT)}, a reasoning framework for diffusion language models. DCoLT treats each intermediate step in the reverse diffusion process as a latent\" thinking\" action and optimizes the entire \u2026", "entry_id": "http://arxiv.org/abs/2505.10446v2", "updated": "2025-05-21 01:44:47", "published": "2025-05-15 16:06:32", "authors": "Zemin Huang;Zhiyang Chen;Zijun Wang;Tiancheng Li;Guo-Jun Qi", "summary": "We introduce the Diffusion Chain of Lateral Thought (DCoLT), a reasoning\nframework for diffusion language models. DCoLT treats each intermediate step in\nthe reverse diffusion process as a latent \"thinking\" action and optimizes the\nentire reasoning trajectory to maximize the reward on the correctness of the\nfinal answer with outcome-based Reinforcement Learning (RL). Unlike traditional\nChain-of-Thought (CoT) methods that follow a causal, linear thinking process,\nDCoLT allows bidirectional, non-linear reasoning with no strict rule on\ngrammatical correctness amid its intermediate steps of thought. We implement\nDCoLT on two representative Diffusion Language Models (DLMs). First, we choose\nSEDD as a representative continuous-time discrete diffusion model, where its\nconcrete score derives a probabilistic policy to maximize the RL reward over\nthe entire sequence of intermediate diffusion steps. We further consider the\ndiscrete-time masked diffusion language model -- LLaDA, and find that the order\nto predict and unmask tokens plays an essential role to optimize its RL action\nresulting from the ranking-based Unmasking Policy Module (UPM) defined by the\nPlackett-Luce model. Experiments on both math and code generation tasks show\nthat using only public data and 16 H800 GPUs, DCoLT-reinforced DLMs outperform\nother DLMs trained by SFT or RL or even both. Notably, DCoLT-reinforced LLaDA\nboosts its reasoning accuracy by +9.8%, +5.7%, +11.4%, +19.5% on GSM8K, MATH,\nMBPP, and HumanEval.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.10446v2;http://arxiv.org/pdf/2505.10446v2", "pdf_url": "http://arxiv.org/pdf/2505.10446v2"}, {"title": "Dense Communication between Language Models", "link": "https://arxiv.org/pdf/2505.12741", "details": "S Wu, Y Wang, Q Yao - arXiv preprint arXiv:2505.12741, 2025", "abstract": "As higher-level intelligence emerges from the combination of modular components with lower-level intelligence, many works combines Large Language Models (LLMs) for collective intelligence. Such combination is achieved by building communications \u2026", "entry_id": "http://arxiv.org/abs/2505.12741v1", "updated": "2025-05-19 05:56:06", "published": "2025-05-19 05:56:06", "authors": "Shiguang Wu;Yaqing Wang;Quanming Yao", "summary": "As higher-level intelligence emerges from the combination of modular\ncomponents with lower-level intelligence, many works combines Large Language\nModels (LLMs) for collective intelligence. Such combination is achieved by\nbuilding communications among LLMs. While current systems primarily facilitate\nsuch communication through natural language, this paper proposes a novel\nparadigm of direct dense vector communication between LLMs. Our approach\neliminates the unnecessary embedding and de-embedding steps when LLM interact\nwith another, enabling more efficient information transfer, fully\ndifferentiable optimization pathways, and exploration of capabilities beyond\nhuman heuristics. We use such stripped LLMs as vertexes and optimizable seq2seq\nmodules as edges to construct LMNet, with similar structure as MLPs. By\nutilizing smaller pre-trained LLMs as vertexes, we train a LMNet that achieves\ncomparable performance with LLMs in similar size with only less than 0.1%\ntraining cost. This offers a new perspective on scaling for general\nintelligence rather than training a monolithic LLM from scratch. Besides, the\nproposed method can be used for other applications, like customizing LLM with\nlimited data, showing its versatility.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.12741v1;http://arxiv.org/pdf/2505.12741v1", "pdf_url": "http://arxiv.org/pdf/2505.12741v1"}, {"title": "SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning", "link": "https://arxiv.org/pdf/2505.12448", "details": "Y Liu, M Ma, X Yu, P Ding, H Zhao, M Sun, S Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite impressive advancements in Visual-Language Models (VLMs) for multi- modal tasks, their reliance on RGB inputs limits precise spatial understanding. Existing methods for integrating spatial cues, such as point clouds or depth, either \u2026", "entry_id": "http://arxiv.org/abs/2505.12448v2", "updated": "2025-05-21 03:34:31", "published": "2025-05-18 14:40:16", "authors": "Yang Liu;Ming Ma;Xiaomin Yu;Pengxiang Ding;Han Zhao;Mingyang Sun;Siteng Huang;Donglin Wang", "summary": "Despite impressive advancements in Visual-Language Models (VLMs) for\nmulti-modal tasks, their reliance on RGB inputs limits precise spatial\nunderstanding. Existing methods for integrating spatial cues, such as point\nclouds or depth, either require specialized sensors or fail to effectively\nexploit depth information for higher-order reasoning. To this end, we propose a\nnovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that\ntransforms raw depth data into structured, interpretable textual rationales.\nThese textual rationales serve as meaningful intermediate representations to\nsignificantly enhance spatial reasoning capabilities. Additionally, we leverage\nknowledge distillation to compress the generated rationales into compact latent\nembeddings, which facilitate resource-efficient and plug-and-play integration\ninto existing VLMs without retraining. To enable comprehensive evaluation, we\nintroduce a new dataset named SSR-CoT, a million-scale visual-language\nreasoning dataset enriched with intermediate spatial reasoning annotations, and\npresent SSRBench, a comprehensive multi-task benchmark. Extensive experiments\non multiple benchmarks demonstrate SSR substantially improves depth utilization\nand enhances spatial reasoning, thereby advancing VLMs toward more human-like\nmulti-modal understanding. Our project page is at\nhttps://yliu-cs.github.io/SSR.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.12448v2;http://arxiv.org/pdf/2505.12448v2", "pdf_url": "http://arxiv.org/pdf/2505.12448v2"}, {"title": "Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation", "link": "https://arxiv.org/pdf/2505.16763", "details": "H Yang, Y Zhou, W Han, J Shen - arXiv preprint arXiv:2505.16763, 2025", "abstract": "Text-to-image models are powerful for producing high-quality images based on given text prompts, but crafting these prompts often requires specialized vocabulary. To address this, existing methods train rewriting models with supervision from large \u2026", "entry_id": "http://arxiv.org/abs/2505.16763v1", "updated": "2025-05-22 15:05:07", "published": "2025-05-22 15:05:07", "authors": "Hongji Yang;Yucheng Zhou;Wencheng Han;Jianbing Shen", "summary": "Text-to-image models are powerful for producing high-quality images based on\ngiven text prompts, but crafting these prompts often requires specialized\nvocabulary. To address this, existing methods train rewriting models with\nsupervision from large amounts of manually annotated data and trained aesthetic\nassessment models. To alleviate the dependence on data scale for model training\nand the biases introduced by trained models, we propose a novel prompt\noptimization framework, designed to rephrase a simple user prompt into a\nsophisticated prompt to a text-to-image model. Specifically, we employ the\nlarge vision language models (LVLMs) as the solver to rewrite the user prompt,\nand concurrently, employ LVLMs as a reward model to score the aesthetics and\nalignment of the images generated by the optimized prompt. Instead of laborious\nhuman feedback, we exploit the prior knowledge of the LVLM to provide rewards,\ni.e., AI feedback. Simultaneously, the solver and the reward model are unified\ninto one model and iterated in reinforcement learning to achieve\nself-improvement by giving a solution and judging itself. Results on two\npopular datasets demonstrate that our method outperforms other strong\ncompetitors.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.16763v1;http://arxiv.org/pdf/2505.16763v1", "pdf_url": "http://arxiv.org/pdf/2505.16763v1"}, {"title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models", "link": "https://arxiv.org/pdf/2505.16854", "details": "J Wang, KQ Lin, J Cheng, MZ Shou - arXiv preprint arXiv:2505.16854, 2025", "abstract": "Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision-language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to \u2026", "entry_id": "http://arxiv.org/abs/2505.16854v2", "updated": "2025-05-23 16:09:00", "published": "2025-05-22 16:13:29", "authors": "Jiaqi Wang;Kevin Qinghong Lin;James Cheng;Mike Zheng Shou", "summary": "Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across diverse vision-language tasks-covering\na range of reasoning difficulties under both 3B and 7B models-consistently\nreveal that the model progressively learns to bypass unnecessary reasoning\nsteps as training advances. These findings shed light on the path toward\nhuman-like reasoning patterns in reinforcement learning approaches. Our code is\navailable at https://github.com/kokolerk/TON.", "comment": "update more examples in appendix", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CV", "links": "http://arxiv.org/abs/2505.16854v2;http://arxiv.org/pdf/2505.16854v2", "pdf_url": "http://arxiv.org/pdf/2505.16854v2"}, {"title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2505.13444", "details": "L Tang, G Kim, X Zhao, T Lake, W Ding, F Yin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these \u2026", "entry_id": "http://arxiv.org/abs/2505.13444v1", "updated": "2025-05-19 17:59:27", "published": "2025-05-19 17:59:27", "authors": "Liyan Tang;Grace Kim;Xinyu Zhao;Thom Lake;Wenxuan Ding;Fangcong Yin;Prasann Singhal;Manya Wadhwa;Zeyu Leo Liu;Zayne Sprague;Ramya Namuduri;Bodun Hu;Juan Diego Rodriguez;Puyuan Peng;Greg Durrett", "summary": "Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.CV", "links": "http://arxiv.org/abs/2505.13444v1;http://arxiv.org/pdf/2505.13444v1", "pdf_url": "http://arxiv.org/pdf/2505.13444v1"}, {"title": "ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2505.16517", "details": "Z Song, G Ouyang, M Li, Y Ji, C Wang, Z Xu, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision-Language Models (LVLMs) have recently advanced robotic manipulation by leveraging vision for scene perception and language for instruction following. However, existing methods rely heavily on costly human-annotated \u2026", "entry_id": "http://arxiv.org/abs/2505.16517v1", "updated": "2025-05-22 10:57:07", "published": "2025-05-22 10:57:07", "authors": "Zirui Song;Guangxian Ouyang;Mingzhe Li;Yuheng Ji;Chenxi Wang;Zixiang Xu;Zeyu Zhang;Xiaoqing Zhang;Qian Jiang;Zhenhao Chen;Zhongzhi Li;Rui Yan;Xiuying Chen", "summary": "Large Vision-Language Models (LVLMs) have recently advanced robotic\nmanipulation by leveraging vision for scene perception and language for\ninstruction following. However, existing methods rely heavily on costly\nhuman-annotated training datasets, which limits their generalization and causes\nthem to struggle in out-of-domain (OOD) scenarios, reducing real-world\nadaptability. To address these challenges, we propose ManipLVM-R1, a novel\nreinforcement learning framework that replaces traditional supervision with\nReinforcement Learning using Verifiable Rewards (RLVR). By directly optimizing\nfor task-aligned outcomes, our method enhances generalization and physical\nreasoning while removing the dependence on costly annotations. Specifically, we\ndesign two rule-based reward functions targeting key robotic manipulation\nsubtasks: an Affordance Perception Reward to enhance localization of\ninteraction regions, and a Trajectory Match Reward to ensure the physical\nplausibility of action paths. These rewards provide immediate feedback and\nimpose spatial-logical constraints, encouraging the model to go beyond shallow\npattern matching and instead learn deeper, more systematic reasoning about\nphysical interactions.", "comment": "13pages", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.CV", "links": "http://arxiv.org/abs/2505.16517v1;http://arxiv.org/pdf/2505.16517v1", "pdf_url": "http://arxiv.org/pdf/2505.16517v1"}, {"title": "Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports", "link": "https://arxiv.org/pdf/2505.16624", "details": "FD Serra, P Schrempf, C Wang, Z Meng, F Deligianni\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present a novel approach to Chest X-ray (CXR) Visual Question Answering (VQA), addressing both single-image image-difference questions. Single-image questions focus on abnormalities within a specific CXR (\" What abnormalities are \u2026", "entry_id": "http://arxiv.org/abs/2505.16624v1", "updated": "2025-05-22 12:57:35", "published": "2025-05-22 12:57:35", "authors": "Francesco Dalla Serra;Patrick Schrempf;Chaoyang Wang;Zaiqiao Meng;Fani Deligianni;Alison Q. O'Neil", "summary": "We present a novel approach to Chest X-ray (CXR) Visual Question Answering\n(VQA), addressing both single-image image-difference questions. Single-image\nquestions focus on abnormalities within a specific CXR (\"What abnormalities are\nseen in image X?\"), while image-difference questions compare two longitudinal\nCXRs acquired at different time points (\"What are the differences between image\nX and Y?\"). We further explore how the integration of radiology reports can\nenhance the performance of VQA models. While previous approaches have\ndemonstrated the utility of radiology reports during the pre-training phase, we\nextend this idea by showing that the reports can also be leveraged as\nadditional input to improve the VQA model's predicted answers. First, we\npropose a unified method that handles both types of questions and\nauto-regressively generates the answers. For single-image questions, the model\nis provided with a single CXR. For image-difference questions, the model is\nprovided with two CXRs from the same patient, captured at different time\npoints, enabling the model to detect and describe temporal changes. Taking\ninspiration from 'Chain-of-Thought reasoning', we demonstrate that performance\non the CXR VQA task can be improved by grounding the answer generator module\nwith a radiology report predicted for the same CXR. In our approach, the VQA\nmodel is divided into two steps: i) Report Generation (RG) and ii) Answer\nGeneration (AG). Our results demonstrate that incorporating predicted radiology\nreports as evidence to the AG model enhances performance on both single-image\nand image-difference questions, achieving state-of-the-art results on the\nMedical-Diff-VQA dataset.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.CL", "links": "http://arxiv.org/abs/2505.16624v1;http://arxiv.org/pdf/2505.16624v1", "pdf_url": "http://arxiv.org/pdf/2505.16624v1"}]
