[{"title": "Liquid: Language Models are Scalable Multi-modal Generators", "link": "https://arxiv.org/pdf/2412.04332", "details": "J Wu, Y Jiang, C Ma, Y Liu, H Zhao, Z Yuan, S Bai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Liquid, an auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared \u2026"}, {"title": "DiCoDe: Diffusion-Compressed Deep Tokens for Autoregressive Video Generation with Language Models", "link": "https://arxiv.org/pdf/2412.04446%3F", "details": "Y Li, Y Ge, Y Ge, P Luo, Y Shan - arXiv preprint arXiv:2412.04446, 2024", "abstract": "Videos are inherently temporal sequences by their very nature. In this work, we explore the potential of modeling videos in a chronological and scalable manner with autoregressive (AR) language models, inspired by their success in natural language \u2026"}, {"title": "C3oT: Generating Shorter Chain-of-Thought without Compromising Effectiveness", "link": "https://arxiv.org/pdf/2412.11664", "details": "Y Kang, X Sun, L Chen, W Zou - arXiv preprint arXiv:2412.11664, 2024", "abstract": "Generating Chain-of-Thought (CoT) before deriving the answer can effectively improve the reasoning capabilities of large language models (LLMs) and significantly improve the accuracy of the generated answer. However, in most cases \u2026"}, {"title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding", "link": "https://arxiv.org/pdf/2412.10302%3F", "details": "Z Wu, X Chen, Z Pan, X Liu, W Liu, D Dai, H Gao, Y Ma\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL, through two key major upgrades. For the vision component, we \u2026"}, {"title": "Lost in the Middle, and In-Between: Enhancing Language Models' Ability to Reason Over Long Contexts in Multi-Hop QA", "link": "https://arxiv.org/pdf/2412.10079", "details": "GA Baker, A Raut, S Shaier, LE Hunter\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Previous work finds that recent long-context language models fail to make equal use of information in the middle of their inputs, preferring pieces of information located at the tail ends which creates an undue bias in situations where we would like models \u2026"}, {"title": "NVILA: Efficient Frontier Visual Language Models", "link": "https://arxiv.org/pdf/2412.04468", "details": "Z Liu, L Zhu, B Shi, Z Zhang, Y Lou, S Yang, H Xi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and \u2026"}, {"title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models", "link": "https://arxiv.org/pdf/2412.04467", "details": "S Yang, Y Chen, Z Tian, C Wang, J Li, B Yu, J Jia - arXiv preprint arXiv:2412.04467, 2024", "abstract": "Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens \u2026"}, {"title": "GIRAFFE: Design Choices for Extending the Context Length of Visual Language Models", "link": "https://arxiv.org/pdf/2412.12735", "details": "M Li, L Li, S Gong, Q Liu - arXiv preprint arXiv:2412.12735, 2024", "abstract": "Visual Language Models (VLMs) demonstrate impressive capabilities in processing multimodal inputs, yet applications such as visual agents, which require handling multiple images and high-resolution videos, demand enhanced long-range \u2026"}, {"title": "Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning", "link": "https://arxiv.org/pdf/2412.08614", "details": "F Lu, W Wu, K Zheng, S Ma, B Gong, J Liu, W Zhai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating detailed captions comprehending text-rich visual content in images has received growing attention for Large Vision-Language Models (LVLMs). However, few studies have developed benchmarks specifically tailored for detailed captions to \u2026"}]
