[{"title": "LLM-Based Text-to-SQL for Real-World Databases", "link": "https://link.springer.com/article/10.1007/s42979-025-03662-6", "details": "ER Nascimento, G Garc\u00eda, YT Izquierdo, L Feij\u00f3\u2026 - SN Computer Science, 2025", "abstract": "Text-to-SQL refers to the task defined as \u201cgiven a relational database D and a natural language sentence S that describes a question on D, generate an SQL query Q over D that expresses S\u201d. Several LLM-based text-to-SQL tools, that is, text-to-SQL tools \u2026"}, {"title": "Guiding Medical Vision-Language Models with Explicit Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations", "link": "https://arxiv.org/pdf/2501.02385", "details": "K Zhu, Z Qin, H Yi, Z Jiang, Q Lao, S Zhang, K Li - arXiv preprint arXiv:2501.02385, 2025", "abstract": "With the recent advancements in vision-language models (VLMs) driven by large language models (LLMs), many researchers have focused on models that comprised of an image encoder, an image-to-language projection layer, and a text decoder \u2026"}, {"title": "DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision Language Models in Real-World Scenarios with Driving Theory Tests", "link": "https://arxiv.org/pdf/2501.04671", "details": "C Corbi\u00e8re, S Roburin, S Montariol, A Bosselut, A Alahi - arXiv preprint arXiv \u2026, 2025", "abstract": "Large vision-language models (LVLMs) augment language models with visual understanding, enabling multimodal reasoning. However, due to the modality gap between textual and visual data, they often face significant challenges, such as over \u2026"}, {"title": "Language Models Encode the Value of Numbers Linearly", "link": "https://aclanthology.org/2025.coling-main.47.pdf", "details": "F Zhu, D Dai, Z Sui - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Large language models (LLMs) have exhibited impressive competence in various tasks, but their internal mechanisms on mathematical problems are still under- explored. In this paper, we study a fundamental question: how language models \u2026"}, {"title": "Tinylvlm-ehub: Towards comprehensive and efficient evaluation for large vision-language models", "link": "https://ieeexplore.ieee.org/abstract/document/10858438/", "details": "W Shao, M Lei, Y Hu, P Gao, P Xu, K Zhang, F Meng\u2026 - IEEE Transactions on Big \u2026, 2025", "abstract": "Large Vision-Language Models (LVLMs) have made significant strides in various multimodal tasks. Notably, GPT4V, Claude, Gemini, and others showcase exceptional multimodal capabilities, marked by profound comprehension and \u2026"}, {"title": "Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models", "link": "https://arxiv.org/pdf/2501.14818", "details": "Z Li, G Chen, S Liu, S Wang, V VS, Y Ji, S Lan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, promising progress has been made by open-source vision-language models (VLMs) in bringing their capabilities closer to those of proprietary frontier models. However, most open-source models only publish their final model weights \u2026"}, {"title": "Efficient Architectures for High Resolution Vision-Language Models", "link": "https://arxiv.org/pdf/2501.02584", "details": "M Carvalho, B Martins - arXiv preprint arXiv:2501.02584, 2025", "abstract": "Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This \u2026"}, {"title": "SKIntern: Internalizing Symbolic Knowledge for Distilling Better CoT Capabilities into Small Language Models", "link": "https://aclanthology.org/2025.coling-main.215.pdf", "details": "H Liao, S He, Y Hao, X Li, Y Zhang, J Zhao, K Liu - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract Small Language Models (SLMs) are attracting attention due to the high computational demands and privacy concerns of Large Language Models (LLMs). Some studies fine-tune SLMs using Chains of Thought (CoT) data distilled from \u2026"}, {"title": "Exploring Primitive Visual Measurement Understanding and the Role of Output Format in Learning in Vision-Language Models", "link": "https://arxiv.org/pdf/2501.15144", "details": "A Yadav, L Liu, Y Qi - arXiv preprint arXiv:2501.15144, 2025", "abstract": "This work investigates the capabilities of current vision-language models (VLMs) in visual understanding and attribute measurement of primitive shapes using a benchmark focused on controlled 2D shape configurations with variations in spatial \u2026"}]
