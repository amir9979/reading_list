[{"title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation", "link": "https://arxiv.org/pdf/2505.12058", "details": "V Koc - arXiv preprint arXiv:2505.12058, 2025", "abstract": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual smoke-test suite designed to give large-language-model (LLM) pipelines a unit-test style safety net dataset that runs in seconds with minimal cost. Born out of the tight feedback-loop \u2026", "entry_id": "http://arxiv.org/abs/2505.12058v1", "updated": "2025-05-17 15:40:03", "published": "2025-05-17 15:40:03", "authors": "Vincent Koc", "summary": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual\nsmoke-test suite designed to give large-language-model (LLM) pipelines a\nunit-test style safety net dataset that runs in seconds with minimal cost. Born\nout of the tight feedback-loop demands building the Comet Opik\nprompt-optimization SDK, where waiting on heavyweight benchmarks breaks\ndeveloper flow. TQB++ couples a 52-item English gold set (less than 20 kB) with\na tiny synthetic-data generator pypi package built on provider-agnostic\nLiteLLM. The generator lets practitioners mint their own tiny packs in any\nlanguage, domain, or difficulty, while ten ready-made packs already cover\nArabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,\nSpanish, and Turkish. Every dataset ships with Croissant metadata and\nplug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so\nteams can drop deterministic micro-benchmarks directly into pull-request gates,\nprompt-engineering loops, and production dashboards without touching GPU\nbudgets. A complete TQB++ run adds only a few seconds to pipeline latency yet\nreliably flags prompt-template errors, tokenizer drift, and fine-tuning\nside-effects long before full-scale suites like MMLU or BIG-Bench would finish\nconfiguring. The entire framework is released to accelerate continuous,\nresource-efficient quality assurance across the generative-AI ecosystem.", "comment": "28 pages, 7 figures, 3 tables. Includes expanded appendix & full\n  score matrices. Dataset & code: HF Hub + GitHub + Pypi links in abstract.\n  Core data and code Apache-2.0; synthetic packs eval-only", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL;I.2.7; I.2.6; H.2.8", "links": "http://arxiv.org/abs/2505.12058v1;http://arxiv.org/pdf/2505.12058v1", "pdf_url": "http://arxiv.org/pdf/2505.12058v1"}, {"title": "Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches", "link": "https://arxiv.org/pdf/2505.12259", "details": "Y Zhou, X Chen, Y Cao, Y Ni, Y He, S Tian, X Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Current **LLM** **evaluation** methods can be classified into three categories. First, automatic evaluation using metrics is common, with benchmarks like MCQ-based datasets using accuracy, sentiment analysis datasets using F1 score, and text \u2026", "entry_id": "http://arxiv.org/abs/2505.12259v1", "updated": "2025-05-18 06:51:10", "published": "2025-05-18 06:51:10", "authors": "Yuhang Zhou;Xutian Chen;Yixin Cao;Yuchen Ni;Yu He;Siyu Tian;Xiang Liu;Jian Zhang;Chuanjun Ji;Guangnan Ye;Xipeng Qiu", "summary": "Recent progress in large language models (LLMs) has outpaced the development\nof effective evaluation methods. Traditional benchmarks rely on task-specific\nmetrics and static datasets, which often suffer from fairness issues, limited\nscalability, and contamination risks. In this paper, we introduce Teach2Eval,\nan indirect evaluation framework inspired by the Feynman Technique. Instead of\ndirectly testing LLMs on predefined tasks, our method evaluates a model's\nmultiple abilities to teach weaker student models to perform tasks effectively.\nBy converting open-ended tasks into standardized multiple-choice questions\n(MCQs) through teacher-generated feedback, Teach2Eval enables scalable,\nautomated, and multi-dimensional assessment. Our approach not only avoids data\nleakage and memorization but also captures a broad range of cognitive abilities\nthat are orthogonal to current benchmarks. Experimental results across 26\nleading LLMs show strong alignment with existing human and model-based dynamic\nrankings, while offering additional interpretability for training guidance.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.12259v1;http://arxiv.org/pdf/2505.12259v1", "pdf_url": "http://arxiv.org/pdf/2505.12259v1"}, {"title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs", "link": "https://arxiv.org/pdf/2505.13098", "details": "LP Meyer, J Frey, D Heim, F Brei, C Stadler\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Table 1: Comparison of some of the **LLM** **evaluation** approaches mentioned here. Best values are marked with bold font. Only the LLM-KG-Bench framework combines automatic evaluation with several Knowledge Graph Engineering (KGE) topics and \u2026", "entry_id": "http://arxiv.org/abs/2505.13098v1", "updated": "2025-05-19 13:29:27", "published": "2025-05-19 13:29:27", "authors": "Lars-Peter Meyer;Johannes Frey;Desiree Heim;Felix Brei;Claus Stadler;Kurt Junghanns;Michael Martin", "summary": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks.", "comment": "Peer reviewed publication at ESWC 2025 Resources Track", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL;cs.DB", "links": "http://arxiv.org/abs/2505.13098v1;http://arxiv.org/pdf/2505.13098v1", "pdf_url": "http://arxiv.org/pdf/2505.13098v1"}, {"title": "Think-J: Learning to Think for Generative LLM-as-a-Judge", "link": "https://arxiv.org/pdf/2505.14268", "details": "H Huang, Y He, H Zhou, R Zhang, W Liu, W Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses generated by Large Language Models (LLMs), which is of significant importance for both **LLM** **evaluation** and reward modeling. Although generative LLMs have made \u2026", "entry_id": "http://arxiv.org/abs/2505.14268v1", "updated": "2025-05-20 12:19:10", "published": "2025-05-20 12:19:10", "authors": "Hui Huang;Yancheng He;Hongli Zhou;Rui Zhang;Wei Liu;Weixun Wang;Wenbo Su;Bo Zheng;Jiaheng Liu", "summary": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses\ngenerated by Large Language Models (LLMs), which is of significant importance\nfor both LLM evaluation and reward modeling. Although generative LLMs have made\nsubstantial progress in various tasks, their performance as LLM-Judge still\nfalls short of expectations. In this work, we propose Think-J, which improves\ngenerative LLM-as-a-Judge by learning how to think. We first utilized a small\namount of curated data to develop the model with initial judgment thinking\ncapabilities. Subsequently, we optimize the judgment thinking traces based on\nreinforcement learning (RL). We propose two methods for judgment thinking\noptimization, based on offline and online RL, respectively. The offline RL\nrequires training a critic model to construct positive and negative examples\nfor learning. The online method defines rule-based reward as feedback for\noptimization. Experimental results showed that our approach can significantly\nenhance the evaluation capability of generative LLM-Judge, surpassing both\ngenerative and classifier-based LLM-Judge without requiring extra human\nannotations.", "comment": "16 pages, 14 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.14268v1;http://arxiv.org/pdf/2505.14268v1", "pdf_url": "http://arxiv.org/pdf/2505.14268v1"}, {"title": "MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations", "link": "https://arxiv.org/pdf/2505.14101", "details": "E Lavrinovics, R Biswas, K Hose, J Bjerva - arXiv preprint arXiv:2505.14101, 2025", "abstract": "\u2026 Multi-prompt evaluation has also been raised as an important component of **LLM** **evaluation** methodology [26, 23], which we do not cover, yet acknowledge as necessary, to test for LLM robustness and safety. Therefore, we propose expanding \u2026", "entry_id": "http://arxiv.org/abs/2505.14101v1", "updated": "2025-05-20 09:03:35", "published": "2025-05-20 09:03:35", "authors": "Ernests Lavrinovics;Russa Biswas;Katja Hose;Johannes Bjerva", "summary": "Large Language Models (LLMs) have inherent limitations of faithfulness and\nfactuality, commonly referred to as hallucinations. Several benchmarks have\nbeen developed that provide a test bed for factuality evaluation within the\ncontext of English-centric datasets, while relying on supplementary informative\ncontext like web links or text passages but ignoring the available structured\nfactual resources. To this end, Knowledge Graphs (KGs) have been identified as\na useful aid for hallucination mitigation, as they provide a structured way to\nrepresent the facts about entities and their relations with minimal linguistic\noverhead. We bridge the lack of KG paths and multilinguality for factual\nlanguage modeling within the existing hallucination evaluation benchmarks and\npropose a KG-based multilingual, multihop benchmark called \\textbf{MultiHal}\nframed for generative text evaluation. As part of our data collection pipeline,\nwe mined 140k KG-paths from open-domain KGs, from which we pruned noisy\nKG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation\nshows an absolute scale increase by approximately 0.12 to 0.36 points for the\nsemantic similarity score in KG-RAG over vanilla QA across multiple languages\nand multiple models, demonstrating the potential of KG integration. We\nanticipate MultiHal will foster future research towards several graph-based\nhallucination mitigation and fact-checking tasks.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.14101v1;http://arxiv.org/pdf/2505.14101v1", "pdf_url": "http://arxiv.org/pdf/2505.14101v1"}, {"title": "PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks", "link": "https://arxiv.org/pdf/2505.13862", "details": "G Shen, D Zhao, L Feng, X He, J Wang, S Shen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial prompts known as jailbreaks, which can bypass safety alignment and elicit harmful outputs. Despite growing efforts in LLM safety research \u2026", "entry_id": "http://arxiv.org/abs/2505.13862v1", "updated": "2025-05-20 03:14:57", "published": "2025-05-20 03:14:57", "authors": "Guobin Shen;Dongcheng Zhao;Linghao Feng;Xiang He;Jihang Wang;Sicheng Shen;Haibo Tong;Yiting Dong;Jindong Li;Xiang Zheng;Yi Zeng", "summary": "Large language models (LLMs) have achieved remarkable capabilities but remain\nvulnerable to adversarial prompts known as jailbreaks, which can bypass safety\nalignment and elicit harmful outputs. Despite growing efforts in LLM safety\nresearch, existing evaluations are often fragmented, focused on isolated attack\nor defense techniques, and lack systematic, reproducible analysis. In this\nwork, we introduce PandaGuard, a unified and modular framework that models LLM\njailbreak safety as a multi-agent system comprising attackers, defenders, and\njudges. Our framework implements 19 attack methods and 12 defense mechanisms,\nalong with multiple judgment strategies, all within a flexible plugin\narchitecture supporting diverse LLM interfaces, multiple interaction modes, and\nconfiguration-driven experimentation that enhances reproducibility and\npractical deployment. Built on this framework, we develop PandaBench, a\ncomprehensive benchmark that evaluates the interactions between these\nattack/defense methods across 49 LLMs and various judgment approaches,\nrequiring over 3 billion tokens to execute. Our extensive evaluation reveals\nkey insights into model vulnerabilities, defense cost-performance trade-offs,\nand judge consistency. We find that no single defense is optimal across all\ndimensions and that judge disagreement introduces nontrivial variance in safety\nassessments. We release the code, configurations, and evaluation results to\nsupport transparent and reproducible research in LLM safety.", "comment": null, "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR;cs.CL", "links": "http://arxiv.org/abs/2505.13862v1;http://arxiv.org/pdf/2505.13862v1", "pdf_url": "http://arxiv.org/pdf/2505.13862v1"}, {"title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "link": "https://arxiv.org/pdf/2505.14552", "details": "J Shi, J Yang, J Liu, X Bu, J Chen, J Zhou, K Ma, Z Wen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities. Existing benchmarks are often domain-specific and thus cannot fully \u2026", "entry_id": "http://arxiv.org/abs/2505.14552v2", "updated": "2025-05-21 07:43:57", "published": "2025-05-20 16:06:32", "authors": "Jiajun Shi;Jian Yang;Jiaheng Liu;Xingyuan Bu;Jiangjie Chen;Junting Zhou;Kaijing Ma;Zhoufutu Wen;Bingli Wang;Yancheng He;Liang Song;Hualei Zhu;Shilong Li;Xingjian Wang;Wei Zhang;Ruibin Yuan;Yifan Yao;Wenjun Yang;Yunli Wang;Siyuan Fang;Siyu Yuan;Qianyu He;Xiangru Tang;Yingshui Tan;Wangchunshu Zhou;Zhaoxiang Zhang;Zhoujun Li;Wenhao Huang;Ge Zhang", "summary": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.", "comment": "22 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.14552v2;http://arxiv.org/pdf/2505.14552v2", "pdf_url": "http://arxiv.org/pdf/2505.14552v2"}, {"title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering", "link": "https://arxiv.org/pdf/2505.14279", "details": "J D'Souza, HB Giglou, Q M\u00fcnch - arXiv preprint arXiv:2505.14279, 2025", "abstract": "\u2026 Optimism bias mitigation \u2013 We implement a reinforcement learning framework to align **LLM** **evaluation** behavior with real-world critical \u2026 Recent **LLM** **evaluation** trends emphasize human-aligned benchmarks for chat assistant alignment, such as \u2026", "entry_id": "http://arxiv.org/abs/2505.14279v1", "updated": "2025-05-20 12:30:46", "published": "2025-05-20 12:30:46", "authors": "Jennifer D'Souza;Hamed Babaei Giglou;Quentin M\u00fcnch", "summary": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence.", "comment": "8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.14279v1;http://arxiv.org/pdf/2505.14279v1", "pdf_url": "http://arxiv.org/pdf/2505.14279v1"}, {"title": "IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation", "link": "https://arxiv.org/pdf/2505.13498", "details": "KT Tran, B O'Sullivan, HD Nguyen - arXiv preprint arXiv:2505.13498, 2025", "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated promising knowledge and reasoning abilities, yet their performance in multilingual and low-resource settings remains underexplored. Existing benchmarks often exhibit cultural bias \u2026", "entry_id": "http://arxiv.org/abs/2505.13498v1", "updated": "2025-05-16 00:02:05", "published": "2025-05-16 00:02:05", "authors": "Khanh-Tung Tran;Barry O'Sullivan;Hoang D. Nguyen", "summary": "Recent advances in Large Language Models (LLMs) have demonstrated promising\nknowledge and reasoning abilities, yet their performance in multilingual and\nlow-resource settings remains underexplored. Existing benchmarks often exhibit\ncultural bias, restrict evaluation to text-only, rely on multiple-choice\nformats, and, more importantly, are limited for extremely low-resource\nlanguages. To address these gaps, we introduce IRLBench, presented in parallel\nEnglish and Irish, which is considered definitely endangered by UNESCO. Our\nbenchmark consists of 12 representative subjects developed from the 2024 Irish\nLeaving Certificate exams, enabling fine-grained analysis of model capabilities\nacross domains. By framing the task as long-form generation and leveraging the\nofficial marking scheme, it does not only support a comprehensive evaluation of\ncorrectness but also language fidelity. Our extensive experiments of leading\nclosed-source and open-source LLMs reveal a persistent performance gap between\nEnglish and Irish, in which models produce valid Irish responses less than 80\\%\nof the time, and answer correctly 55.8\\% of the time compared to 76.2\\% in\nEnglish for the best-performing model. We release IRLBench\n(https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying\nevaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future\nresearch on robust, culturally aware multilingual AI development.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.13498v1;http://arxiv.org/pdf/2505.13498v1", "pdf_url": "http://arxiv.org/pdf/2505.13498v1"}]
