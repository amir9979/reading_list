[{"title": "MEDFORM: A Foundation Model for Contrastive Learning of CT Imaging and Clinical Numeric Data in Multi-Cancer Analysis", "link": "https://arxiv.org/pdf/2501.13277", "details": "D Jung, J Jang, S Jang, YR Park - arXiv preprint arXiv:2501.13277, 2025", "abstract": "Computed tomography (CT) and clinical numeric data are essential modalities for cancer evaluation, but building large-scale multimodal training datasets for developing medical foundation models remains challenging due to the structural \u2026"}, {"title": "ConceptCLIP: Towards Trustworthy Medical AI via Concept-Enhanced Contrastive Langauge-Image Pre-training", "link": "https://arxiv.org/pdf/2501.15579", "details": "Y Nie, S He, Y Bie, Y Wang, Z Chen, S Yang, H Chen - arXiv preprint arXiv \u2026, 2025", "abstract": "Trustworthiness is essential for the precise and interpretable application of artificial intelligence (AI) in medical imaging. Traditionally, precision and interpretability have been addressed as separate tasks, namely medical image analysis and explainable \u2026"}, {"title": "Str-GCL: Structural Commonsense Driven Graph Contrastive Learning", "link": "https://openreview.net/pdf%3Fid%3DzefCoSncYR", "details": "D He, Y Huang, J Zhao, X Wang, Z Wang - THE WEB CONFERENCE 2025", "abstract": "Graph Contrastive Learning (GCL) is a widely adopted approach in unsupervised representation learning, utilizing representational constraints to derive effective embeddings. However, current GCL methods primarily focus on capturing implicit \u2026"}, {"title": "Dynamic graph based weakly supervised deep hashing for whole slide image classification and retrieval", "link": "https://www.sciencedirect.com/science/article/pii/S1361841525000167", "details": "H Jin, J Shen, L Cui, X Shi, K Li, X Zhu - Medical Image Analysis, 2025", "abstract": "Recently, a multi-scale representation attention based deep multiple instance learning method has proposed to directly extract patch-level image features from gigapixel whole slide images (WSIs), and achieved promising performance on \u2026"}, {"title": "scGPT-spatial: Continual Pretraining of Single-Cell Foundation Model for Spatial Transcriptomics", "link": "https://www.biorxiv.org/content/10.1101/2025.02.05.636714.full.pdf", "details": "CX Wang, H Cui, AH Zhang, R Xie, H Goodarzi\u2026 - bioRxiv, 2025", "abstract": "Spatial transcriptomics has emerged as a pivotal technology for profiling gene expression of cells within their spatial context. The rapid growth of publicly available spatial data presents an opportunity to further our understanding of \u2026"}, {"title": "SeLa-MIL: Developing an instance-level classifier via weakly-supervised self-training for whole slide image classification", "link": "https://www.sciencedirect.com/science/article/pii/S0169260725000318", "details": "Y Ma, M Yuan, A Shen, X Luo, B An, X Chen, M Wang - Computer Methods and \u2026, 2025", "abstract": "Abstract Background and Objective Pathology image classification is crucial in clinical cancer diagnosis and computer-aided diagnosis. Whole Slide Image (WSI) classification is often framed as a multiple instance learning (MIL) problem due to the \u2026"}, {"title": "Adapting Foundation Models for Few-Shot Medical Image Segmentation: Actively and Sequentially", "link": "https://arxiv.org/pdf/2502.01000", "details": "J Yang, G Zhang, J Wang, Y Li - arXiv preprint arXiv:2502.01000, 2025", "abstract": "Recent advances in foundation models have brought promising results in computer vision, including medical image segmentation. Fine-tuning foundation models on specific low-resource medical tasks has become a standard practice. However \u2026"}, {"title": "SKI Models: Skeleton Induced Vision-Language Embeddings for Understanding Activities of Daily Living", "link": "https://arxiv.org/pdf/2502.03459", "details": "A Sinha, D Reilly, F Bremond, P Wang, S Das - arXiv preprint arXiv:2502.03459, 2025", "abstract": "The introduction of vision-language models like CLIP has enabled the development of foundational video models capable of generalizing to unseen videos and human actions. However, these models are typically trained on web videos, which often fail \u2026"}, {"title": "Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink", "link": "https://arxiv.org/pdf/2501.15269", "details": "Y Wang, M Zhang, J Sun, C Wang, M Yang, H Xue\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Fusing visual understanding into language generation, Multi-modal Large Language Models (MLLMs) are revolutionizing visual-language applications. Yet, these models are often plagued by the hallucination problem, which involves generating \u2026"}]
