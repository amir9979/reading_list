[{"title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models", "link": "https://arxiv.org/pdf/2406.02224", "details": "T Fan, G Ma, Y Kang, H Gu, L Fan, Q Yang - arXiv preprint arXiv:2406.02224, 2024", "abstract": "Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small \u2026"}, {"title": "Multi-Prompting Decoder Helps Better Language Understanding", "link": "https://arxiv.org/pdf/2406.06279", "details": "Z Cheng, Z Chen, Z Jiang, Y Yin, S Ge, Y Liu, Q Gu - arXiv preprint arXiv:2406.06279, 2024", "abstract": "Recent Pre-trained Language Models (PLMs) usually only provide users with the inference APIs, namely the emerging Model-as-a-Service (MaaS) setting. To adapt MaaS PLMs to downstream tasks without accessing their parameters and gradients \u2026"}, {"title": "ReFiNe: Recursive Field Networks for Cross-modal Multi-scene Representation", "link": "https://arxiv.org/pdf/2406.04309", "details": "S Zakharov, K Liu, A Gaidon, R Ambrus - arXiv preprint arXiv:2406.04309, 2024", "abstract": "The common trade-offs of state-of-the-art methods for multi-shape representation (a single model\" packing\" multiple objects) involve trading modeling accuracy against memory and storage. We show how to encode multiple shapes represented as \u2026"}, {"title": "3D Snapshot: Invertible Embedding of 3D Neural Representations in a Single Image", "link": "https://ieeexplore.ieee.org/abstract/document/10552101/", "details": "Y Lu, B Deng, Z Zhong, T Zhang, Y Quan, H Cai, S He - IEEE Transactions on Pattern \u2026, 2024", "abstract": "3D neural rendering enables photo-realistic reconstruction of a specific scene by encoding discontinuous inputs into a neural representation. Despite the remarkable rendering results, the storage of network parameters is not transmission-friendly and \u2026"}, {"title": "Textual Inversion and Self-supervised Refinement for Radiology Report Generation", "link": "https://arxiv.org/pdf/2405.20607", "details": "Y Luo, H Li, X Wu, M Cao, X Huang, Z Zhu, P Liao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing mainstream approaches follow the encoder-decoder paradigm for generating radiology reports. They focus on improving the network structure of encoders and decoders, which leads to two shortcomings: overlooking the modality \u2026"}, {"title": "Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain", "link": "https://arxiv.org/pdf/2406.06435", "details": "B Hu, B Ray, A Leung, A Summerville, D Joy, C Funk\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In difficult decision-making scenarios, it is common to have conflicting opinions among expert human decision-makers as there may not be a single right answer. Such decisions may be guided by different attributes that can be used to characterize \u2026"}, {"title": "Mixture-of-Agents Enhances Large Language Model Capabilities", "link": "https://arxiv.org/pdf/2406.04692", "details": "J Wang, J Wang, B Athiwaratkun, C Zhang, J Zou - arXiv preprint arXiv:2406.04692, 2024", "abstract": "Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is \u2026"}, {"title": "Alphazero-like tree-search can guide large language model decoding and training", "link": "https://openreview.net/pdf%3Fid%3DC4OpREezgj", "details": "Z Wan, X Feng, M Wen, SM McAleer, Y Wen, W Zhang\u2026 - Forty-first International \u2026, 2024", "abstract": "Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the multi-step reasoning capabilities of LLMs by using tree-search algorithms. These methods rely on prompting a pre-trained model to serve as a value \u2026"}, {"title": "ASIMO: Agent-centric scene representation in multi-object manipulation", "link": "https://journals.sagepub.com/doi/abs/10.1177/02783649241257537", "details": "CH Min, YM Kim - The International Journal of Robotics Research, 2024", "abstract": "Vision-based reinforcement learning (RL) is a generalizable way to control an agent because it is agnostic of specific hardware configurations. As visual observations are highly entangled, attempts for vision-based RL rely on scene representation that \u2026"}]
