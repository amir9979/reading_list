[{"title": "Self-supervised analogical learning using language models", "link": "https://arxiv.org/pdf/2502.00996", "details": "B Zhou, S Jain, Y Zhang, Q Ning, S Wang, Y Benajiba\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can \u2026"}, {"title": "Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences", "link": "https://arxiv.org/pdf/2502.01126", "details": "V Shrivastava, A Kumar, P Liang - arXiv preprint arXiv:2502.01126, 2025", "abstract": "Language models (LMs) should provide reliable confidence estimates to help users detect mistakes in their outputs and defer to human experts when necessary. Asking a language model to assess its confidence (\" Score your confidence from 0-1.\") is a \u2026"}, {"title": "Denoising Multi-Level Cross-Attention and Contrastive Learning for Chest Radiology Report Generation", "link": "https://link.springer.com/article/10.1007/s10278-025-01422-9", "details": "D Zhu, L Liu, X Yang, L Liu, W Peng - Journal of Imaging Informatics in Medicine, 2025", "abstract": "Chest radiology report generation plays a vital role in supporting diagnosis, alleviating physician workload, and reducing the risk of misdiagnosis. However, significant challenges persist:(1) Data bias and background noise in chest images \u2026"}, {"title": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding", "link": "https://arxiv.org/pdf/2501.15747", "details": "S KJ, A Kumar, L Balaji, N Kotecha, V Jain, A Chadha\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Known by more than 1.5 billion people in the Indian subcontinent, Indic languages present unique challenges and opportunities for natural language processing (NLP) research due to their rich cultural heritage, linguistic diversity, and complex \u2026"}, {"title": "Advancing Vision-Language Models with Generative AI", "link": "https://www.preprints.org/frontend/manuscript/10b5ed95bd23954c58eef830d9d74bfa/download_pub", "details": "A Vats, R Raja - 2025", "abstract": "Generative AI within large vision-language models (LVLMs) has revolutionized multimodal learning, enabling machines to understand and generate visual content from textual descriptions with unprecedented accuracy. This paper explores state-of \u2026"}, {"title": "Self-Supervised Learning Using Nonlinear Dependence", "link": "https://arxiv.org/pdf/2501.18875", "details": "MH Sepanj, B Ghojogh, P Fieguth - arXiv preprint arXiv:2501.18875, 2025", "abstract": "Self-supervised learning has gained significant attention in contemporary applications, particularly due to the scarcity of labeled data. While existing SSL methodologies primarily address feature variance and linear correlations, they often \u2026"}, {"title": "COSDA: Covariance regularized semantic data augmentation for self-supervised visual representation learning", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125001273", "details": "H Chen, Y Ma, J Jiang, N Zheng - Knowledge-Based Systems, 2025", "abstract": "Recent contrastive learning-based self-supervised learning has seen significant improvements through employing an extensive data augmentation strategy, particularly focusing on the generation of positive pairs. However, the current \u2026"}, {"title": "Automated generation of chest X-ray imaging diagnostic reports by multimodal and multi granularity features fusion", "link": "https://www.sciencedirect.com/science/article/pii/S1746809425000734", "details": "J Fang, S Xing, K Li, Z Guo, G Li, C Yu - Biomedical Signal Processing and Control, 2025", "abstract": "The automatic generation of chest X-ray diagnostic reports can alleviate the workload of radiologists and reduce the probability of misdiagnosis and missed diagnosis. However, the subtle visual differences between diseases, imbalanced \u2026"}, {"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}]
