[{"title": "RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models", "link": "https://arxiv.org/pdf/2411.04097", "details": "M Varma, JB Delbrouck, Z Chen, A Chaudhari\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) \u2026"}, {"title": "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "link": "https://arxiv.org/pdf/2411.04118", "details": "DP Jeong, S Garg, ZC Lipton, M Oberst - arXiv preprint arXiv:2411.04118, 2024", "abstract": "Several recent works seek to develop foundation models specifically for medical applications, adapting general-purpose large language models (LLMs) and vision- language models (VLMs) via continued pretraining on publicly available biomedical \u2026"}, {"title": "Code-switching finetuning: Bridging multilingual pretrained language models for enhanced cross-lingual performance", "link": "https://www.sciencedirect.com/science/article/pii/S0952197624016907", "details": "C Zan, L Ding, L Shen, Y Cao, W Liu - Engineering Applications of Artificial \u2026, 2025", "abstract": "In recent years, the development of pre-trained models has significantly propelled advancements in natural language processing. However, multilingual sequence-to- sequence pretrained language models (Seq2Seq PLMs) are pretrained on a wide \u2026"}, {"title": "Classification Done Right for Vision-Language Pre-Training", "link": "https://openreview.net/pdf%3Fid%3DHd2EOwKItm", "details": "Z Huang, Q Ye, B Kang, J Feng, H Fan - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data. Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised \u2026"}, {"title": "Griffon-G: Bridging Vision-Language and Vision-Centric Tasks via Large Multimodal Models", "link": "https://arxiv.org/pdf/2410.16163", "details": "Y Zhan, H Zhao, Y Zhu, F Yang, M Tang, J Wang - arXiv preprint arXiv:2410.16163, 2024", "abstract": "Large Multimodal Models (LMMs) have achieved significant breakthroughs in various vision-language and vision-centric tasks based on auto-regressive modeling. However, these models typically focus on either vision-centric tasks, such as visual \u2026"}, {"title": "A Comparative Study of Recent Large Language Models on Generating Hospital Discharge Summaries for Lung Cancer Patients", "link": "https://arxiv.org/pdf/2411.03805", "details": "Y Li, F Li, K Roberts, L Cui, C Tao, H Xu - arXiv preprint arXiv:2411.03805, 2024", "abstract": "Generating discharge summaries is a crucial yet time-consuming task in clinical practice, essential for conveying pertinent patient information and facilitating continuity of care. Recent advancements in large language models (LLMs) have \u2026"}, {"title": "NSSC: a neuro-symbolic AI system for enhancing accuracy of named entity recognition and linking from oncologic clinical notes", "link": "https://link.springer.com/article/10.1007/s11517-024-03227-4", "details": "\u00c1 Garc\u00eda-Barrag\u00e1n, A Sakor, ME Vidal, E Menasalvas\u2026 - Medical & Biological \u2026, 2024", "abstract": "Accurate recognition and linking of oncologic entities in clinical notes is essential for extracting insights across cancer research, patient care, clinical decision-making, and treatment optimization. We present the Neuro-Symbolic System for Cancer \u2026"}, {"title": "Unified Generative and Discriminative Training for Multi-modal Large Language Models", "link": "https://arxiv.org/pdf/2411.00304", "details": "W Chow, J Li, Q Yu, K Pan, H Fei, Z Ge, S Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms. Generative training has enabled Multimodal Large Language Models (MLLMs) to tackle various complex tasks, yet issues such as \u2026"}, {"title": "Medical Information Extraction with Large Language Models", "link": "https://re.public.polimi.it/bitstream/11311/1275692/3/Medical_Event_Extraction_LLMs___ICNLSP_2024.pdf", "details": "R Fornasiere, N Brunello, V Scotti, MJ Carman - Proceedings of the 7th \u2026, 2024", "abstract": "The rapid increase in clinical text data due to the widespread adoption of electronic health records offers significant benefits for medical practice and introduces new challenges in automatic data extraction. Since manual extrac-tion is often inefficient \u2026"}]
