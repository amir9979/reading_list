[{"title": "An Empirical Study of Mamba-based Language Models", "link": "https://arxiv.org/pdf/2406.07887", "details": "R Waleffe, W Byeon, D Riach, B Norick, V Korthikanti\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value \u2026"}, {"title": "Detecting and Evaluating Medical Hallucinations in Large Vision Language Models", "link": "https://arxiv.org/pdf/2406.10185", "details": "J Chen, D Yang, T Wu, Y Jiang, X Hou, M Li, S Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision Language Models (LVLMs) are increasingly integral to healthcare applications, including medical visual question answering and imaging report generation. While these models inherit the robust capabilities of foundational Large \u2026"}, {"title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2406.13233", "details": "Z Zeng, Y Miao, H Gao, H Zhang, Z Deng - arXiv preprint arXiv:2406.13233, 2024", "abstract": "Mixture of experts (MoE) has become the standard for constructing production-level large language models (LLMs) due to its promise to boost model capacity without causing significant overheads. Nevertheless, existing MoE methods usually enforce \u2026"}, {"title": "Timo: Towards Better Temporal Reasoning for Language Models", "link": "https://arxiv.org/pdf/2406.14192", "details": "Z Su, J Zhang, T Zhu, X Qu, J Li, M Zhang, Y Cheng - arXiv preprint arXiv:2406.14192, 2024", "abstract": "Reasoning about time is essential for Large Language Models (LLMs) to understand the world. Previous works focus on solving specific tasks, primarily on time-sensitive question answering. While these methods have proven effective, they cannot \u2026"}, {"title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models", "link": "https://aclanthology.org/2024.findings-naacl.18.pdf", "details": "Z Su, Z Lin, B Baixue, H Chen, S Hu, W Zhou, G Ding\u2026 - Findings of the Association \u2026, 2024", "abstract": "Generative language models are usually pre-trained on large text corpus via predicting the next token (ie, sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language \u2026"}, {"title": "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "link": "https://arxiv.org/pdf/2406.12050", "details": "Z Zhang, Z Liang, W Yu, D Yu, M Jia, D Yu, M Jiang - arXiv preprint arXiv:2406.12050, 2024", "abstract": "Supervised fine-tuning enhances the problem-solving abilities of language models across various mathematical reasoning tasks. To maximize such benefits, existing research focuses on broadening the training set with various data augmentation \u2026"}, {"title": "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification", "link": "https://arxiv.org/pdf/2406.14496", "details": "G Geigle, R Timofte, G Glava\u0161 - arXiv preprint arXiv:2406.14496, 2024", "abstract": "Recent Large Vision-Language Models (LVLMs) demonstrate impressive abilities on numerous image understanding and reasoning tasks. The task of fine-grained object classification (eg, distinction between\\textit {animal species}), however, has been \u2026"}, {"title": "Sports Intelligence: Assessing the Sports Understanding Capabilities of Language Models through Question Answering from Text to Video", "link": "https://arxiv.org/pdf/2406.14877", "details": "Z Yang, H Xia, J Li, Z Chen, Z Zhu, W Shen - arXiv preprint arXiv:2406.14877, 2024", "abstract": "Understanding sports is crucial for the advancement of Natural Language Processing (NLP) due to its intricate and dynamic nature. Reasoning over complex sports scenarios has posed significant challenges to current NLP technologies which \u2026"}, {"title": "Improving Visual Commonsense in Language Models via Multiple Image Generation", "link": "https://arxiv.org/pdf/2406.13621", "details": "G Yariv, I Schwartz, Y Adi, S Benaim - arXiv preprint arXiv:2406.13621, 2024", "abstract": "Commonsense reasoning is fundamentally based on multimodal knowledge. However, existing large language models (LLMs) are primarily trained using textual data only, limiting their ability to incorporate essential visual information. In contrast \u2026"}]
