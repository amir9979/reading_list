[{"title": "Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition", "link": "https://arxiv.org/pdf/2505.22251", "details": "Y Tseng, T Parcollet, R van Dalen, S Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Abstract\u2014Recent work suggests that **large** **language** **models** (LLMs) can improve \u2026 However, this work finds that a substantial amount of the LibriSpeech and Common Voice **evaluation** \u2026 , highlighting the importance of **evaluating** LLM-based speech \u2026", "entry_id": "http://arxiv.org/abs/2505.22251v1", "updated": "2025-05-28 11:39:59", "published": "2025-05-28 11:39:59", "authors": "Yuan Tseng;Titouan Parcollet;Rogier van Dalen;Shucong Zhang;Sourav Bhattacharya", "summary": "Recent work suggests that large language models (LLMs) can improve\nperformance of speech tasks compared to existing systems. To support their\nclaims, results on LibriSpeech and Common Voice are often quoted. However, this\nwork finds that a substantial amount of the LibriSpeech and Common Voice\nevaluation sets appear in public LLM pretraining corpora. This calls into\nquestion the reliability of findings drawn from these two datasets. To measure\nthe impact of contamination, LLMs trained with or without contamination are\ncompared, showing that a contaminated LLM is more likely to generate test\nsentences it has seen during training. Speech recognisers using contaminated\nLLMs shows only subtle differences in error rates, but assigns significantly\nhigher probabilities to transcriptions seen during training. Results show that\nLLM outputs can be biased by tiny amounts of data contamination, highlighting\nthe importance of evaluating LLM-based speech systems with held-out data.", "comment": null, "journal_ref": null, "primary_category": "eess.AS", "categories": "eess.AS;cs.CL", "links": "http://arxiv.org/abs/2505.22251v1;http://arxiv.org/pdf/2505.22251v1", "pdf_url": "http://arxiv.org/pdf/2505.22251v1"}, {"title": "Exploring the occupational biases and stereotypes of Chinese **large language models**", "link": "https://www.nature.com/articles/s41598-025-03893-w", "details": "L Jiang, G Zhu, J Sun, J Cao, J Wu - Scientific Reports, 2025", "abstract": "\u2026 In this study, we proposed a structured approach to **evaluating** bias in Chinese **large** **language** **models** (C-LLMs) through the generation of personal profile texts. Our findings indicate that C-LLMs consistently exhibit traces of bias and social \u2026"}, {"title": "Evaluating the Retrieval Robustness of Large Language Models", "link": "https://arxiv.org/pdf/2505.21870", "details": "S Cao, K Radhakrishnan, D Rosenberg, S Lu, P Cheng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 ) generally enhances **large** **language** **models** \u2019 (LLMs) ability to solve knowledge-intensive tasks. But RAG may also lead to performance degradation due to imperfect retrieval and the model\u2019s limited ability to leverage retrieved content. In this work, we \u2026", "entry_id": "http://arxiv.org/abs/2505.21870v1", "updated": "2025-05-28 01:34:31", "published": "2025-05-28 01:34:31", "authors": "Shuyang Cao;Karthik Radhakrishnan;David Rosenberg;Steven Lu;Pengxiang Cheng;Lu Wang;Shiyue Zhang", "summary": "Retrieval-augmented generation (RAG) generally enhances large language\nmodels' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also\nlead to performance degradation due to imperfect retrieval and the model's\nlimited ability to leverage retrieved content. In this work, we evaluate the\nrobustness of LLMs in practical RAG setups (henceforth retrieval robustness).\nWe focus on three research questions: (1) whether RAG is always better than\nnon-RAG; (2) whether more retrieved documents always lead to better\nperformance; (3) and whether document orders impact results. To facilitate this\nstudy, we establish a benchmark of 1500 open-domain questions, each with\nretrieved documents from Wikipedia. We introduce three robustness metrics, each\ncorresponds to one research question. Our comprehensive experiments, involving\n11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit\nsurprisingly high retrieval robustness; nonetheless, different degrees of\nimperfect robustness hinders them from fully utilizing the benefits of RAG.", "comment": "19 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.21870v1;http://arxiv.org/pdf/2505.21870v1", "pdf_url": "http://arxiv.org/pdf/2505.21870v1"}, {"title": "Precise In-Parameter Concept Erasure in Large Language Models", "link": "https://arxiv.org/pdf/2505.22586", "details": "Y Gur-Arieh, C Suslik, Y Hong, F Barez, M Geva - arXiv preprint arXiv:2505.22586, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) often acquire knowledge during pretraining that is undesirable in downstream deployments, eg, sensitive information or \u2026 We **evaluate** PISCES against three other methods suitable for concept erasure. To do so \u2026", "entry_id": "http://arxiv.org/abs/2505.22586v1", "updated": "2025-05-28 16:58:23", "published": "2025-05-28 16:58:23", "authors": "Yoav Gur-Arieh;Clara Suslik;Yihuai Hong;Fazl Barez;Mor Geva", "summary": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.22586v1;http://arxiv.org/pdf/2505.22586v1", "pdf_url": "http://arxiv.org/pdf/2505.22586v1"}, {"title": "SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation", "link": "https://arxiv.org/pdf/2505.21514", "details": "M Jiang, A Jain, S Zorek, C Jermaine - arXiv preprint arXiv:2505.21514, 2025", "abstract": "\u2026 We introduce SimCopilot, a benchmark that simulates the role of **large** **language** **models** (\u2026 ), SimCopilot provides a comprehensive framework for **evaluating** LLM coding capabilities. The \u2026 Our key contributions include: (a) establishing a realistic \u2026", "entry_id": "http://arxiv.org/abs/2505.21514v1", "updated": "2025-05-21 04:59:44", "published": "2025-05-21 04:59:44", "authors": "Mingchao Jiang;Abhinav Jain;Sophia Zorek;Chris Jermaine", "summary": "We introduce SIMCOPILOT, a benchmark that simulates the role of large\nlanguage models (LLMs) as interactive, \"copilot\"-style coding assistants.\nTargeting both completion (finishing incomplete methods or code blocks) and\ninfill tasks (filling missing segments within existing code), SIMCOPILOT\nprovides a comprehensive framework for evaluating LLM coding capabilities. The\nbenchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python\n(SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our\nkey contributions include: (a) establishing a realistic, detailed evaluation\nenvironment to assess LLM utility in practical coding scenarios, and (b)\nproviding fine-grained analyses that address critical factors frequently\noverlooked by existing benchmarks, such as task-specific performance nuances,\ncontextual understanding across code segments, and sensitivity to variable\nscope. Evaluations conducted across domains-including algorithms, databases,\ncomputer vision, and neural networks-offer insights into model strengths and\nhighlight persistent challenges in maintaining logical consistency within\ncomplex dependency structures. Beyond benchmarking, our study sheds light on\nthe current limitations of LLM-driven code generation and underscores the\nongoing transition of LLMs from merely syntax-aware generators toward reliable,\nintelligent software development partners.", "comment": "Keywords: Benchmark Dataset, LLM Evaluation, Gen-AI, Program\n  Synthesis; TLDR: SimCoPilot is a benchmark for evaluating LLMs as\n  \"copilot\"-style interactive coding assistants, testing their ability to\n  integrate and complete code within complex real-world software environments", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.PL;cs.SE", "links": "http://arxiv.org/abs/2505.21514v1;http://arxiv.org/pdf/2505.21514v1", "pdf_url": "http://arxiv.org/pdf/2505.21514v1"}, {"title": "Valsci: an open-source, self-hostable literature review utility for automated large-batch scientific claim verification using **large language models**", "link": "https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-025-06159-4", "details": "B Edelman, J Skolnick - BMC bioinformatics, 2025", "abstract": "\u2026 key **evaluation** dimensions: hallucination reduction, processing speed and efficiency, and misclassifications. We used two popular base **large** **language** **models** , \u2026 Processing speed was additionally **evaluated** against a human undergraduate \u2026"}, {"title": "Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation", "link": "https://arxiv.org/pdf/2505.22222", "details": "Y Kim, J Wu, SH Kim, P Vasudev, J Shen, H Wu - arXiv preprint arXiv:2505.22222, 2025", "abstract": "\u2026 \u2022 Fewer Hallucinations and Errors: We show, through radiologist expert **evaluations** , that grounding fixation strengthens the alignment of generated text with the ground truth reports, mitigating one of the most pressing drawbacks of large-scale \u2026", "entry_id": "http://arxiv.org/abs/2505.22222v1", "updated": "2025-05-28 10:54:40", "published": "2025-05-28 10:54:40", "authors": "Yunsoo Kim;Jinge Wu;Su-Hwan Kim;Pardeep Vasudev;Jiashu Shen;Honghan Wu", "summary": "Recent advancements in multimodal Large Language Models (LLMs) have\nsignificantly enhanced the automation of medical image analysis, particularly\nin generating radiology reports from chest X-rays (CXR). However, these models\nstill suffer from hallucinations and clinically significant errors, limiting\ntheir reliability in real-world applications. In this study, we propose Look &\nMark (L&M), a novel grounding fixation strategy that integrates radiologist eye\nfixations (Look) and bounding box annotations (Mark) into the LLM prompting\nframework. Unlike conventional fine-tuning, L&M leverages in-context learning\nto achieve substantial performance gains without retraining. When evaluated\nacross multiple domain-specific and general-purpose models, L&M demonstrates\nsignificant gains, including a 1.2% improvement in overall metrics (A.AVG) for\nCXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for\nLLaVA-Med. General-purpose models also benefit from L&M combined with\nin-context learning, with LLaVA-OV achieving an 87.3% clinical average\nperformance (C.AVG)-the highest among all models, even surpassing those\nexplicitly trained for CXR report generation. Expert evaluations further\nconfirm that L&M reduces clinically significant errors (by 0.43 average errors\nper report), such as false predictions and omissions, enhancing both accuracy\nand reliability. These findings highlight L&M's potential as a scalable and\nefficient solution for AI-assisted radiology, paving the way for improved\ndiagnostic workflows in low-resource clinical settings.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.CL", "links": "http://arxiv.org/abs/2505.22222v1;http://arxiv.org/pdf/2505.22222v1", "pdf_url": "http://arxiv.org/pdf/2505.22222v1"}, {"title": "How valuable are the questions and answers generated by **large language models** in oral and maxillofacial surgery?", "link": "https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0322529", "details": "K Kim, SB Mun, YJ Kim, BC Kim, KG Kim - PLoS One, 2025", "abstract": "\u2026 In this study, we aim to **evaluate** the ability of **large** **language** **models** (LLM) to generate questions and answers in oral and maxillofacial \u2026 A flow diagram showing the process by which **large** **language** **models** generate and answer \u2026"}, {"title": "Exploring sentence-level revision capabilities of **large language models** in English for academic purposes writing assistance", "link": "https://link.springer.com/article/10.1186/s40862-025-00334-z", "details": "Z Du, K Hashimoto - Asian-Pacific Journal of Second and Foreign \u2026, 2025", "abstract": "\u2026 **evaluation** benchmarks and suggests that higher-level, discourse-based EAP text **evaluation** benchmarks merit deeper exploration. \u2026 We **evaluated** the performance of LLMs using infrequent English prompts combined with real data. Although there \u2026"}]
