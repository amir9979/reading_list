[{"title": "VocalAgent: Large Language Models for Vocal Health Diagnostics with Safety-Aware Evaluation", "link": "https://arxiv.org/pdf/2505.13577", "details": "Y Kim, T Kim, W Kang, E Park, J Yoon, D Lee, X Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vocal health plays a crucial role in peoples' lives, significantly impacting their communicative abilities and interactions. However, despite the global prevalence of voice disorders, many lack access to convenient diagnosis and treatment. This \u2026", "entry_id": "http://arxiv.org/abs/2505.13577v1", "updated": "2025-05-19 14:58:42", "published": "2025-05-19 14:58:42", "authors": "Yubin Kim;Taehan Kim;Wonjune Kang;Eugene Park;Joonsik Yoon;Dongjae Lee;Xin Liu;Daniel McDuff;Hyeonhoon Lee;Cynthia Breazeal;Hae Won Park", "summary": "Vocal health plays a crucial role in peoples' lives, significantly impacting\ntheir communicative abilities and interactions. However, despite the global\nprevalence of voice disorders, many lack access to convenient diagnosis and\ntreatment. This paper introduces VocalAgent, an audio large language model\n(LLM) to address these challenges through vocal health diagnosis. We leverage\nQwen-Audio-Chat fine-tuned on three datasets collected in-situ from hospital\npatients, and present a multifaceted evaluation framework encompassing a safety\nassessment to mitigate diagnostic biases, cross-lingual performance analysis,\nand modality ablation studies. VocalAgent demonstrates superior accuracy on\nvoice disorder classification compared to state-of-the-art baselines. Its\nLLM-based method offers a scalable solution for broader adoption of health\ndiagnostics, while underscoring the importance of ethical and technical\nvalidation.", "comment": null, "journal_ref": null, "primary_category": "cs.SD", "categories": "cs.SD;cs.AI;eess.AS", "links": "http://arxiv.org/abs/2505.13577v1;http://arxiv.org/pdf/2505.13577v1", "pdf_url": "http://arxiv.org/pdf/2505.13577v1"}, {"title": "Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?", "link": "https://arxiv.org/pdf/2505.11907", "details": "Z Dongfang, X Zheng, Z Weng, Y Lyu, DP Paudel\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 In this paper, we introduced OSR-Dataset and OSR-Bench, the first benchmarks specifically designed to **evaluate** the spatial reasoning capabilities of Multimodal **Large** **Language** **Models** (MLLMs) in 360 panoramic environments. Our benchmark \u2026", "entry_id": "http://arxiv.org/abs/2505.11907v1", "updated": "2025-05-17 08:48:40", "published": "2025-05-17 08:48:40", "authors": "Zihao Dongfang;Xu Zheng;Ziqiao Weng;Yuanhuiyi Lyu;Danda Pani Paudel;Luc Van Gool;Kailun Yang;Xuming Hu", "summary": "The 180x360 omnidirectional field of view captured by 360-degree cameras\nenables their use in a wide range of applications such as embodied AI and\nvirtual reality. Although recent advances in multimodal large language models\n(MLLMs) have shown promise in visual-spatial reasoning, most studies focus on\nstandard pinhole-view images, leaving omnidirectional perception largely\nunexplored. In this paper, we ask: Are MLLMs ready for omnidirectional spatial\nreasoning? To investigate this, we introduce OSR-Bench, the first benchmark\nspecifically designed for this setting. OSR-Bench includes over 153,000 diverse\nquestion-answer pairs grounded in high-fidelity panoramic indoor scene maps. It\ncovers key reasoning types including object counting, relative distance, and\ndirection. We also propose a negative sampling strategy that inserts\nnon-existent objects into prompts to evaluate hallucination and grounding\nrobustness. For fine-grained analysis, we design a two-stage evaluation\nframework assessing both cognitive map generation and QA accuracy using\nrotation-invariant matching and a combination of rule-based and LLM-based\nmetrics. We evaluate eight state-of-the-art MLLMs, including GPT-4o, Gemini 1.5\nPro, and leading open-source models under zero-shot settings. Results show that\ncurrent models struggle with spatial reasoning in panoramic contexts,\nhighlighting the need for more perceptually grounded MLLMs. OSR-Bench and code\nwill be released at: https://huggingface.co/datasets/UUUserna/OSR-Bench", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.11907v1;http://arxiv.org/pdf/2505.11907v1", "pdf_url": "http://arxiv.org/pdf/2505.11907v1"}, {"title": "Source framing triggers systematic evaluation bias in Large Language Models", "link": "https://arxiv.org/pdf/2505.13488", "details": "F Germani, G Spitale - arXiv preprint arXiv:2505.13488, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) are increasingly used not only to generate text but also to **evaluate** it, raising urgent questions about \u2026 , Deepseek Reasoner, xAI Grok 2, and Mistral \u2013 tasked with **evaluating** 4,800 narrative statements on 24 \u2026", "entry_id": "http://arxiv.org/abs/2505.13488v1", "updated": "2025-05-14 07:42:27", "published": "2025-05-14 07:42:27", "authors": "Federico Germani;Giovanni Spitale", "summary": "Large Language Models (LLMs) are increasingly used not only to generate text\nbut also to evaluate it, raising urgent questions about whether their judgments\nare consistent, unbiased, and robust to framing effects. In this study, we\nsystematically examine inter- and intra-model agreement across four\nstate-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and\nMistral) tasked with evaluating 4,800 narrative statements on 24 different\ntopics of social, political, and public health relevance, for a total of\n192,000 assessments. We manipulate the disclosed source of each statement to\nassess how attribution to either another LLM or a human author of specified\nnationality affects evaluation outcomes. We find that, in the blind condition,\ndifferent LLMs display a remarkably high degree of inter- and intra-model\nagreement across topics. However, this alignment breaks down when source\nframing is introduced. Here we show that attributing statements to Chinese\nindividuals systematically lowers agreement scores across all models, and in\nparticular for Deepseek Reasoner. Our findings reveal that framing effects can\ndeeply affect text evaluation, with significant implications for the integrity,\nneutrality, and fairness of LLM-mediated information systems.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.CY", "links": "http://arxiv.org/abs/2505.13488v1;http://arxiv.org/pdf/2505.13488v1", "pdf_url": "http://arxiv.org/pdf/2505.13488v1"}, {"title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models", "link": "https://arxiv.org/pdf/2505.14599", "details": "G Xiong, E Xie, C Williams, M Kim, AH Shariatmadari\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) have shown significant potential in scientific disciplines \u2026 However, a key challenge lies in **evaluating** the truthfulness of generated hypotheses, as \u2026 knowledge-based hallucination detector to **evaluate** \u2026", "entry_id": "http://arxiv.org/abs/2505.14599v1", "updated": "2025-05-20 16:49:40", "published": "2025-05-20 16:49:40", "authors": "Guangzhi Xiong;Eric Xie;Corey Williams;Myles Kim;Amir Hassan Shariatmadari;Sikun Guo;Stefan Bekiranov;Aidong Zhang", "summary": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.", "comment": "Accepted to IJCAI 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.14599v1;http://arxiv.org/pdf/2505.14599v1", "pdf_url": "http://arxiv.org/pdf/2505.14599v1"}, {"title": "MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models", "link": "https://arxiv.org/pdf/2505.11613", "details": "X Li, M Gao, Y Hao, T Li, G Wan, Z Wang, Y Wang - arXiv preprint arXiv:2505.11613, 2025", "abstract": "\u2026 However, it remains unclear whether **Large** **Language** **Models** (LLMs) can reliably follow such structured protocols. In this work, we introduce MedGUIDE, a new benchmark for **evaluating** LLMs on their ability to make guideline-consistent \u2026", "entry_id": "http://arxiv.org/abs/2505.11613v1", "updated": "2025-05-16 18:21:52", "published": "2025-05-16 18:21:52", "authors": "Xiaomin Li;Mingye Gao;Yuexing Hao;Taoran Li;Guangya Wan;Zihan Wang;Yijun Wang", "summary": "Clinical guidelines, typically structured as decision trees, are central to\nevidence-based medical practice and critical for ensuring safe and accurate\ndiagnostic decision-making. However, it remains unclear whether Large Language\nModels (LLMs) can reliably follow such structured protocols. In this work, we\nintroduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to\nmake guideline-consistent clinical decisions. MedGUIDE is constructed from 55\ncurated NCCN decision trees across 17 cancer types and uses clinical scenarios\ngenerated by LLMs to create a large pool of multiple-choice diagnostic\nquestions. We apply a two-stage quality selection process, combining\nexpert-labeled reward models and LLM-as-a-judge ensembles across ten clinical\nand linguistic criteria, to select 7,747 high-quality samples. We evaluate 25\nLLMs spanning general-purpose, open-source, and medically specialized models,\nand find that even domain-specific LLMs often underperform on tasks requiring\nstructured guideline adherence. We also test whether performance can be\nimproved via in-context guideline inclusion or continued pretraining. Our\nfindings underscore the importance of MedGUIDE in assessing whether LLMs can\noperate safely within the procedural frameworks expected in real-world clinical\nsettings.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.11613v1;http://arxiv.org/pdf/2505.11613v1", "pdf_url": "http://arxiv.org/pdf/2505.11613v1"}, {"title": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science", "link": "https://arxiv.org/pdf/2505.13220", "details": "J Ying, Z Chen, Z Wang, W Jiang, C Wang, Z Yuan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 We conduct **evaluations** on different **large** **language** **models** using opencompass as the primary tool. In order to ensure reproducibility and provide a reference for future research, the main hyperparameters used in our experiments, along with their \u2026", "entry_id": "http://arxiv.org/abs/2505.13220v1", "updated": "2025-05-19 15:02:59", "published": "2025-05-19 15:02:59", "authors": "Jie Ying;Zihong Chen;Zhefan Wang;Wanli Jiang;Chenyang Wang;Zhonghang Yuan;Haoyang Su;Huanjun Kong;Fan Yang;Nanqing Dong", "summary": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design.", "comment": "Accepted by ACL 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.13220v1;http://arxiv.org/pdf/2505.13220v1", "pdf_url": "http://arxiv.org/pdf/2505.13220v1"}, {"title": "Evaluating Large Language Models for Real-World Engineering Tasks", "link": "https://arxiv.org/pdf/2505.13484", "details": "R Heesch, S Eilermann, A Windmann, A Diedrich\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) are transformative not only for daily activities but also for engineering tasks. However, current evaluations \u2026 Using this dataset, we **evaluate** four state-of-the-art LLMs, including both cloud-based and locally hosted \u2026", "entry_id": "http://arxiv.org/abs/2505.13484v1", "updated": "2025-05-12 14:05:23", "published": "2025-05-12 14:05:23", "authors": "Rene Heesch;Sebastian Eilermann;Alexander Windmann;Alexander Diedrich;Philipp Rosenthal;Oliver Niggemann", "summary": "Large Language Models (LLMs) are transformative not only for daily activities\nbut also for engineering tasks. However, current evaluations of LLMs in\nengineering exhibit two critical shortcomings: (i) the reliance on simplified\nuse cases, often adapted from examination materials where correctness is easily\nverifiable, and (ii) the use of ad hoc scenarios that insufficiently capture\ncritical engineering competencies. Consequently, the assessment of LLMs on\ncomplex, real-world engineering problems remains largely unexplored. This paper\naddresses this gap by introducing a curated database comprising over 100\nquestions derived from authentic, production-oriented engineering scenarios,\nsystematically designed to cover core competencies such as product design,\nprognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art\nLLMs, including both cloud-based and locally hosted instances, to\nsystematically investigate their performance on complex engineering tasks. Our\nresults show that LLMs demonstrate strengths in basic temporal and structural\nreasoning but struggle significantly with abstract reasoning, formal modeling,\nand context-sensitive engineering logic.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL", "links": "http://arxiv.org/abs/2505.13484v1;http://arxiv.org/pdf/2505.13484v1", "pdf_url": "http://arxiv.org/pdf/2505.13484v1"}, {"title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models", "link": "https://arxiv.org/pdf/2505.12900", "details": "S Hou, Z Shen, H Wu, J Liang, H Jiao, Y Qing, X Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 for automatic **evaluation** in this domain. To address this gap, we propose AutoGEEval, the first multimodal, unit-level automated **evaluation** framework for geospatial code generation tasks on the Google Earth Engine (GEE) platform \u2026", "entry_id": "http://arxiv.org/abs/2505.12900v1", "updated": "2025-05-19 09:35:58", "published": "2025-05-19 09:35:58", "authors": "Shuyang Hou;Zhangxiao Shen;Huayi Wu;Jianyuan Liang;Haoyue Jiao;Yaxian Qing;Xiaopu Zhang;Xu Li;Zhipeng Gui;Xuefeng Guan;Longgang Xiang", "summary": "Geospatial code generation is emerging as a key direction in the integration\nof artificial intelligence and geoscientific analysis. However, there remains a\nlack of standardized tools for automatic evaluation in this domain. To address\nthis gap, we propose AutoGEEval, the first multimodal, unit-level automated\nevaluation framework for geospatial code generation tasks on the Google Earth\nEngine (GEE) platform powered by large language models (LLMs). Built upon the\nGEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)\ncomprising 1325 test cases that span 26 GEE data types. The framework\nintegrates both question generation and answer verification components to\nenable an end-to-end automated evaluation pipeline-from function invocation to\nexecution validation. AutoGEEval supports multidimensional quantitative\nanalysis of model outputs in terms of accuracy, resource consumption, execution\nefficiency, and error types. We evaluate 18 state-of-the-art LLMs-including\ngeneral-purpose, reasoning-augmented, code-centric, and geoscience-specialized\nmodels-revealing their performance characteristics and potential optimization\npathways in GEE code generation. This work provides a unified protocol and\nfoundational resource for the development and assessment of geospatial code\ngeneration models, advancing the frontier of automated natural language to\ndomain-specific code translation.", "comment": null, "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE;cs.AI;cs.CG;cs.CL;cs.DB", "links": "http://arxiv.org/abs/2505.12900v1;http://arxiv.org/pdf/2505.12900v1", "pdf_url": "http://arxiv.org/pdf/2505.12900v1"}, {"title": "EfficientLLM: Efficiency in Large Language Models", "link": "https://arxiv.org/pdf/2505.13840", "details": "Z Yuan, W Sun, Y Liu, H Zhou, R Zhou, Y Li, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 : (1) the substantial computational cost associated with training and **evaluating** numerous large models and techniques; (2) the lack of \u2026 To address this gap, as shown in Figure 1, we introduce EfficientLLM, the first large-scale empirical \u2026", "entry_id": "http://arxiv.org/abs/2505.13840v1", "updated": "2025-05-20 02:27:08", "published": "2025-05-20 02:27:08", "authors": "Zhengqing Yuan;Weixiang Sun;Yixin Liu;Huichi Zhou;Rong Zhou;Yiyang Li;Zheyuan Zhang;Wei Song;Yue Huang;Haolong Jia;Keerthiram Murugesan;Yu Wang;Lifang He;Jianfeng Gao;Lichao Sun;Yanfang Ye", "summary": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.13840v1;http://arxiv.org/pdf/2505.13840v1", "pdf_url": "http://arxiv.org/pdf/2505.13840v1"}]
