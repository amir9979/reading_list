'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [A Comparison of Parameter-Efficient ASR Domain Adaptation Me'
[{"title": "Understanding emergent abilities of language models from the loss perspective", "link": "https://arxiv.org/pdf/2403.15796", "details": "Z Du, A Zeng, Y Dong, J Tang - arXiv preprint arXiv:2403.15796, 2024", "abstract": "Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) \u2026"}, {"title": "Grounding and Enhancing Grid-based Models for Neural Fields", "link": "https://arxiv.org/pdf/2403.20002", "details": "Z Zhao, F Fan, W Liao, J Yan - arXiv preprint arXiv:2403.20002, 2024", "abstract": "Many contemporary studies utilize grid-based models for neural field representation, but a systematic analysis of grid-based models is still missing, hindering the improvement of those models. Therefore, this paper introduces a theoretical \u2026"}, {"title": "Generative Language Models for Personalized Information Understanding", "link": "https://scholarworks.umass.edu/cgi/viewcontent.cgi%3Farticle%3D4123%26context%3Ddissertations_2", "details": "P Cai - 2024", "abstract": "A major challenge in information understanding stems from the diverse nature of the audience, where individuals possess varying preferences, experiences, educational and cultural backgrounds. Consequently, adopting a one-size-fits-all approach to \u2026"}, {"title": "A Diffusion Model with State Estimation for Degradation-Blind Inverse Imaging", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/28023/28060", "details": "L Ji, Z Rao, SJ Pan, C Lei, Q Chen - Proceedings of the AAAI Conference on Artificial \u2026, 2024", "abstract": "Solving the task of inverse imaging problems can restore unknown clean images from input measurements that have incomplete information. Utilizing powerful generative models, such as denoising diffusion models, could better tackle the ill \u2026"}, {"title": "Invertible Diffusion Models for Compressed Sensing", "link": "https://arxiv.org/pdf/2403.17006", "details": "B Chen, Z Zhang, W Li, C Zhao, J Yu, S Zhao, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although \u2026"}, {"title": "LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2404.03118", "details": "GBM Stan, RY Rohekar, Y Gurwicz, ML Olson\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the rapidly evolving landscape of artificial intelligence, multi-modal large language models are emerging as a significant area of interest. These models, which combine various forms of data input, are becoming increasingly popular. However \u2026"}, {"title": "BRAVE: Broadening the visual encoding of vision-language models", "link": "https://arxiv.org/pdf/2404.07204", "details": "OF Kar, A Tonioni, P Poklukar, A Kulshrestha, A Zamir\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language models (VLMs) are typically composed of a vision encoder, eg CLIP, and a language model (LM) that interprets the encoded features to solve downstream tasks. Despite remarkable progress, VLMs are subject to several \u2026"}, {"title": "HDPNERF: Hybrid Depth Priors for Neural Radiance Fields from Sparse Input Views", "link": "https://ieeexplore.ieee.org/abstract/document/10446844/", "details": "W Xu, Q Wang, X Pan, R Wang - ICASSP 2024-2024 IEEE International Conference \u2026, 2024", "abstract": "Neural Radiance Field (NeRF) shows a high prospect in the task of novel view synthesis. However, performance degrades drastically under limited input views since NeRF heavily relies on a large number of images to fit the geometry in scenes \u2026"}, {"title": "Monotonic Paraphrasing Improves Generalization of Language Model Prompting", "link": "https://arxiv.org/pdf/2403.16038", "details": "Q Liu, F Wang, N Xu, T Yan, T Meng, M Chen - arXiv preprint arXiv:2403.16038, 2024", "abstract": "Performance of large language models (LLMs) may vary with different prompts or instructions of even the same task. One commonly recognized factor for this phenomenon is the model's familiarity with the given prompt or instruction, which is \u2026"}]
