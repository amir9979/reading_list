'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [A Quantitative Bias Analysis Approach to Informative Presenc'
[{"title": "MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering", "link": "https://arxiv.org/pdf/2403.19116", "details": "C Guan, M Huang, P Zhang - arXiv preprint arXiv:2403.19116, 2024", "abstract": "In today's fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested \u2026"}, {"title": "OTTER: Improving Zero-Shot Classification via Optimal Transport", "link": "https://arxiv.org/pdf/2404.08461", "details": "C Shin, J Zhao, S Cromp, H Vishwakarma, F Sala - arXiv preprint arXiv:2404.08461, 2024", "abstract": "Popular zero-shot models suffer due to artifacts inherited from pretraining. A particularly detrimental artifact, caused by unbalanced web-scale pretraining data, is mismatched label distribution. Existing approaches that seek to repair the label \u2026"}, {"title": "Foundational Challenges in Assuring Alignment and Safety of Large Language Models", "link": "https://arxiv.org/pdf/2404.09932", "details": "U Anwar, A Saparov, J Rando, D Paleka, M Turpin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment \u2026"}, {"title": "Jamba: A Hybrid Transformer-Mamba Language Model", "link": "https://arxiv.org/pdf/2403.19887.pdf%3Ftrk%3Dpublic_post_comment-text", "details": "O Lieber, B Lenz, H Bata, G Cohen, J Osin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both \u2026"}]
