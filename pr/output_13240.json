[{"title": "Chest X-ray Foundation Model with Global and Local Representations Integration", "link": "https://arxiv.org/pdf/2502.05142", "details": "Z Yang, X Xu, J Zhang, G Wang, MK Kalra, P Yan - arXiv preprint arXiv:2502.05142, 2025", "abstract": "Chest X-ray (CXR) is the most frequently ordered imaging test, supporting diverse clinical tasks from thoracic disease detection to postoperative monitoring. However, task-specific classification models are limited in scope, require costly labeled data \u2026"}, {"title": "Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement", "link": "https://arxiv.org/pdf/2502.02573%3F", "details": "S Abbasloo - arXiv preprint arXiv:2502.02573, 2025", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem- solving, a crucial, ubiquitous, and complex domain. This paper explores the \u2026"}, {"title": "Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?", "link": "https://arxiv.org/pdf/2502.11895", "details": "J Nielsen, P Schneider-Kamp, L Galke - arXiv preprint arXiv:2502.11895, 2025", "abstract": "Large language models (LLMs) require immense resources for training and inference. Quantization, a technique that reduces the precision of model parameters, offers a promising solution for improving LLM efficiency and sustainability. While post \u2026"}, {"title": "DuCo-Net: Dual-Contrastive Learning Network for Medical Report Retrieval Leveraging Enhanced Encoders and Augmentations", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10870249.pdf", "details": "ZU Rahman, JH Lee, DT Vu, I Murtza, JY Kim - IEEE Access, 2025", "abstract": "The conventional process of generating medical radiology reports is labor-intensive and time-consuming, requiring radiologists to describe findings meticulously from imaging studies. This manual approach often causes undesirable delays in patient \u2026"}, {"title": "Hierarchical Vision\u2013Language Pre-Training with Freezing Strategy for Multi-Level Semantic Alignment", "link": "https://www.mdpi.com/2079-9292/14/4/816", "details": "H Xie, Y Qin, S Ding - Electronics, 2025", "abstract": "Vision\u2013language pre-training (VLP) faces challenges in aligning hierarchical textual semantics (words/phrases/sentences) with multi-scale visual features (objects/relations/global context). We propose a hierarchical VLP model (HieVLP) \u2026"}, {"title": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment", "link": "https://arxiv.org/pdf/2502.04328%3F", "details": "Z Liu, Y Dong, J Wang, Z Liu, W Hu, J Lu, Y Rao - arXiv preprint arXiv:2502.04328, 2025", "abstract": "Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have \u2026"}, {"title": "Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective", "link": "https://arxiv.org/pdf/2502.01524%3F", "details": "X Ma, H Xie, SJ Qin - arXiv preprint arXiv:2502.01524, 2025", "abstract": "The integration of vision-language modalities has been a significant focus in multimodal learning, traditionally relying on Vision-Language Pretrained Models. However, with the advent of Large Language Models (LLMs), there has been a \u2026"}, {"title": "Aligning, Autoencoding and Prompting Large Language Models for Novel Disease Reporting", "link": "https://drive.google.com/file/d/1Q8y9iQA3aw_4u98YkgaTas7I9IGfkEJM/view", "details": "F Liu, X Wu, J Huang, B Yang, K Branson, P Schwab\u2026 - IEEE Transactions on \u2026, 2025", "abstract": "Given radiology images, automatic radiology report generation aims to produce informative text that reports diseases. It can benefit current clinical practice in diagnostic radiology. Existing methods typically rely on large-scale medical datasets \u2026"}, {"title": "Supervised contrastive pre-training models for mammography screening", "link": "https://link.springer.com/article/10.1186/s40537-025-01075-z", "details": "Z Cao, Z Deng, Z Yang, J Ma, L Ma - Journal of Big Data, 2025", "abstract": "Breast cancer is now the most deadly cancer worldwide. Mammography screening is the most effective method for early detection and diagnosis of breast cancer. Due to the lack of labeled mammograms, building an AI system for mammography \u2026"}]
