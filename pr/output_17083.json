[{"title": "Multilingual Pretraining for Pixel Language Models", "link": "https://arxiv.org/pdf/2505.21265", "details": "I Kesen, JF Lotz, I Ziegler, P Rust, D Elliott - arXiv preprint arXiv:2505.21265, 2025", "abstract": "Pixel language models operate directly on images of rendered text, eliminating the need for a fixed vocabulary. While these models have demonstrated strong capabilities for downstream cross-lingual transfer, multilingual pretraining remains \u2026", "entry_id": "http://arxiv.org/abs/2505.21265v1", "updated": "2025-05-27 14:40:47", "published": "2025-05-27 14:40:47", "authors": "Ilker Kesen;Jonas F. Lotz;Ingo Ziegler;Phillip Rust;Desmond Elliott", "summary": "Pixel language models operate directly on images of rendered text,\neliminating the need for a fixed vocabulary. While these models have\ndemonstrated strong capabilities for downstream cross-lingual transfer,\nmultilingual pretraining remains underexplored. We introduce PIXEL-M4, a model\npretrained on four visually and linguistically diverse languages: English,\nHindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic\nand syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart\non non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4\ncaptures rich linguistic features, even in languages not seen during\npretraining. Furthermore, an analysis of its hidden representations shows that\nmultilingual pretraining yields a semantic embedding space closely aligned\nacross the languages used for pretraining. This work demonstrates that\nmultilingual pretraining substantially enhances the capability of pixel\nlanguage models to effectively support a diverse set of languages.", "comment": "17 pages, 19 figures, 7 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.21265v1;http://arxiv.org/pdf/2505.21265v1", "pdf_url": "http://arxiv.org/pdf/2505.21265v1"}, {"title": "Understanding Refusal in Language Models with Sparse Autoencoders", "link": "https://arxiv.org/pdf/2505.23556", "details": "WJ Yeo, N Prakash, C Neo, RKW Lee, E Cambria\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Refusal is a key safety behavior in aligned language models, yet the internal mechanisms driving refusals remain opaque. In this work, we conduct a mechanistic study of refusal in instruction-tuned LLMs using sparse autoencoders to identify \u2026", "entry_id": "http://arxiv.org/abs/2505.23556v1", "updated": "2025-05-29 15:33:39", "published": "2025-05-29 15:33:39", "authors": "Wei Jie Yeo;Nirmalendu Prakash;Clement Neo;Roy Ka-Wei Lee;Erik Cambria;Ranjan Satapathy", "summary": "Refusal is a key safety behavior in aligned language models, yet the internal\nmechanisms driving refusals remain opaque. In this work, we conduct a\nmechanistic study of refusal in instruction-tuned LLMs using sparse\nautoencoders to identify latent features that causally mediate refusal\nbehaviors. We apply our method to two open-source chat models and intervene on\nrefusal-related features to assess their influence on generation, validating\ntheir behavioral impact across multiple harmful datasets. This enables a\nfine-grained inspection of how refusal manifests at the activation level and\naddresses key research questions such as investigating upstream-downstream\nlatent relationship and understanding the mechanisms of adversarial\njailbreaking techniques. We also establish the usefulness of refusal features\nin enhancing generalization for linear probes to out-of-distribution\nadversarial samples in classification tasks. We open source our code in\nhttps://github.com/wj210/refusal_sae.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.23556v1;http://arxiv.org/pdf/2505.23556v1", "pdf_url": "http://arxiv.org/pdf/2505.23556v1"}, {"title": "Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models", "link": "https://arxiv.org/pdf/2505.23667", "details": "L Cao, J Xu, H Liu, J Wang, M Zhou, H Dong, S Han\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they \u2026", "entry_id": "http://arxiv.org/abs/2505.23667v1", "updated": "2025-05-29 17:13:40", "published": "2025-05-29 17:13:40", "authors": "Lang Cao;Jingxian Xu;Hanbing Liu;Jinyu Wang;Mengyu Zhou;Haoyu Dong;Shi Han;Dongmei Zhang", "summary": "Tables are a fundamental structure for organizing and analyzing data, making\neffective table understanding a critical capability for intelligent systems.\nWhile large language models (LMs) demonstrate strong general reasoning\nabilities, they continue to struggle with accurate numerical or symbolic\nreasoning over tabular data, especially in complex scenarios. Spreadsheet\nformulas provide a powerful and expressive medium for representing executable\nsymbolic operations, encoding rich reasoning patterns that remain largely\nunderutilized. In this paper, we propose Formula Tuning (Fortune), a\nreinforcement learning (RL) framework that trains LMs to generate executable\nspreadsheet formulas for question answering over general tabular data. Formula\nTuning reduces the reliance on supervised formula annotations by using binary\nanswer correctness as a reward signal, guiding the model to learn formula\nderivation through reasoning. We provide a theoretical analysis of its\nadvantages and demonstrate its effectiveness through extensive experiments on\nseven table reasoning benchmarks. Formula Tuning substantially enhances LM\nperformance, particularly on multi-step numerical and symbolic reasoning tasks,\nenabling a 7B model to outperform O1 on table understanding. This highlights\nthe potential of formula-driven RL to advance symbolic table reasoning in LMs.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.23667v1;http://arxiv.org/pdf/2505.23667v1", "pdf_url": "http://arxiv.org/pdf/2505.23667v1"}, {"title": "ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions", "link": "https://arxiv.org/pdf/2505.23662", "details": "B Kwak, M Kim, D Lim, H Chae, D Kang, S Kim, D Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have demonstrated strong capabilities in using external tools to address user inquiries. However, most existing evaluations assume tool use in short contexts, offering limited insight into model behavior during realistic \u2026", "entry_id": "http://arxiv.org/abs/2505.23662v1", "updated": "2025-05-29 17:10:12", "published": "2025-05-29 17:10:12", "authors": "Beong-woo Kwak;Minju Kim;Dongha Lim;Hyungjoo Chae;Dongjin Kang;Sunghwan Kim;Dongil Yang;Jinyoung Yeo", "summary": "Large language models (LLMs) have demonstrated strong capabilities in using\nexternal tools to address user inquiries. However, most existing evaluations\nassume tool use in short contexts, offering limited insight into model behavior\nduring realistic long-term interactions. To fill this gap, we introduce\nToolHaystack, a benchmark for testing the tool use capabilities in long-term\ninteractions. Each test instance in ToolHaystack includes multiple tasks\nexecution contexts and realistic noise within a continuous conversation,\nenabling assessment of how well models maintain context and handle various\ndisruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find\nthat while current models perform well in standard multi-turn settings, they\noften significantly struggle in ToolHaystack, highlighting critical gaps in\ntheir long-term robustness not revealed by previous tool benchmarks.", "comment": "Our code and data are available at\n  https://github.com/bwookwak/ToolHaystack", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.23662v1;http://arxiv.org/pdf/2505.23662v1", "pdf_url": "http://arxiv.org/pdf/2505.23662v1"}, {"title": "Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models", "link": "https://arxiv.org/pdf/2505.17089", "details": "MRU Rashid, VA Dasu, Y Wang, G Tan, S Mehnaz - arXiv preprint arXiv:2505.17089, 2025", "abstract": "Large Language Models (LLMs) exhibit impressive capabilities, but remain susceptible to a growing spectrum of safety risks, including jailbreaks, toxic content, hallucinations, and bias. Existing defenses often address only a single threat type or \u2026", "entry_id": "http://arxiv.org/abs/2505.17089v1", "updated": "2025-05-20 21:22:40", "published": "2025-05-20 21:22:40", "authors": "Md Rafi Ur Rashid;Vishnu Asutosh Dasu;Ye Wang;Gang Tan;Shagufta Mehnaz", "summary": "Large Language Models (LLMs) exhibit impressive capabilities, but remain\nsusceptible to a growing spectrum of safety risks, including jailbreaks, toxic\ncontent, hallucinations, and bias. Existing defenses often address only a\nsingle threat type or resort to rigid outright rejection, sacrificing user\nexperience and failing to generalize across diverse and novel attacks. This\npaper introduces Adversarial Scenario Extrapolation (ASE), a novel\ninference-time computation framework that leverages Chain-of-Thought (CoT)\nreasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides\nthe LLM through a self-generative process of contemplating potential\nadversarial scenarios and formulating defensive strategies before generating a\nresponse to the user query. Comprehensive evaluation on four adversarial\nbenchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak\nattack success rates and minimal toxicity, while slashing outright rejections\nto <4%. ASE outperforms six state-of-the-art defenses in\nrobustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&A and\n4-10x lower bias scores. By transforming adversarial perception into an\nintrinsic cognitive process, ASE sets a new paradigm for secure and natural\nhuman-AI interaction.", "comment": "26 pages, 2 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.17089v1;http://arxiv.org/pdf/2505.17089v1", "pdf_url": "http://arxiv.org/pdf/2505.17089v1"}, {"title": "OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature", "link": "https://arxiv.org/pdf/2505.22945", "details": "A Srivastava, E Korukluoglu, MN Le, D Tran, CM Pham\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) are known to memorize and recall English text from their pretraining data. However, the extent to which this ability generalizes to non- English languages or transfers across languages remains unclear. This paper \u2026", "entry_id": "http://arxiv.org/abs/2505.22945v1", "updated": "2025-05-28 23:57:03", "published": "2025-05-28 23:57:03", "authors": "Alisha Srivastava;Emir Korukluoglu;Minh Nhat Le;Duyen Tran;Chau Minh Pham;Marzena Karpinska;Mohit Iyyer", "summary": "Large language models (LLMs) are known to memorize and recall English text\nfrom their pretraining data. However, the extent to which this ability\ngeneralizes to non-English languages or transfers across languages remains\nunclear. This paper investigates multilingual and cross-lingual memorization in\nLLMs, probing if memorized content in one language (e.g., English) can be\nrecalled when presented in translation. To do so, we introduce OWL, a dataset\nof 31.5K aligned excerpts from 20 books in ten languages, including English\noriginals, official translations (Vietnamese, Spanish, Turkish), and new\ntranslations in six low-resource languages (Sesotho, Yoruba, Maithili,\nMalagasy, Setswana, Tahitian). We evaluate memorization across model families\nand sizes through three tasks: (1) direct probing, which asks the model to\nidentify a book's title and author; (2) name cloze, which requires predicting\nmasked character names; and (3) prefix probing, which involves generating\ncontinuations. We find that LLMs consistently recall content across languages,\neven for texts without direct translation in pretraining data. GPT-4o, for\nexample, identifies authors and titles 69% of the time and masked entities 6%\nof the time in newly translated excerpts. Perturbations (e.g., masking\ncharacters, shuffling words) modestly reduce direct probing accuracy (7% drop\nfor shuffled official translations). Our results highlight the extent of\ncross-lingual memorization and provide insights on the differences between the\nmodels.", "comment": "preprint, 25 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.22945v1;http://arxiv.org/pdf/2505.22945v1", "pdf_url": "http://arxiv.org/pdf/2505.22945v1"}, {"title": "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models", "link": "https://arxiv.org/pdf/2505.19743", "details": "Y Zhang, Y Yu, B Tang, Y Zhu, C Sun, W Wei, J Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the rapid development of Large Language Models (LLMs), aligning these models with human preferences and values is critical to ensuring ethical and safe applications. However, existing alignment techniques such as RLHF or DPO often \u2026", "entry_id": "http://arxiv.org/abs/2505.19743v2", "updated": "2025-05-27 04:07:01", "published": "2025-05-26 09:24:36", "authors": "Yang Zhang;Yu Yu;Bo Tang;Yu Zhu;Chuxiong Sun;Wenqiang Wei;Jie Hu;Zipeng Xie;Zhiyu Li;Feiyu Xiong;Edward Chung", "summary": "With the rapid development of Large Language Models (LLMs), aligning these\nmodels with human preferences and values is critical to ensuring ethical and\nsafe applications. However, existing alignment techniques such as RLHF or DPO\noften require direct fine-tuning on LLMs with billions of parameters, resulting\nin substantial computational costs and inefficiencies. To address this, we\npropose Micro token-level Accept-Reject Aligning (MARA) approach designed to\noperate independently of the language models. MARA simplifies the alignment\nprocess by decomposing sentence-level preference learning into token-level\nbinary classification, where a compact three-layer fully-connected network\ndetermines whether candidate tokens are \"Accepted\" or \"Rejected\" as part of the\nresponse. Extensive experiments across seven different LLMs and three\nopen-source datasets show that MARA achieves significant improvements in\nalignment performance while reducing computational costs. The source code and\nimplementation details are publicly available at\nhttps://github.com/IAAR-Shanghai/MARA, and the trained models are released at\nhttps://huggingface.co/IAAR-Shanghai/MARA_AGENTS.", "comment": "Accepted to 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.19743v2;http://arxiv.org/pdf/2505.19743v2", "pdf_url": "http://arxiv.org/pdf/2505.19743v2"}, {"title": "Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models", "link": "https://arxiv.org/pdf/2505.23561", "details": "Z Yuan, Y Xu, J Shi, P Zhou, L Sun - arXiv preprint arXiv:2505.23561, 2025", "abstract": "Model merging for Large Language Models (LLMs) directly fuses the parameters of different models finetuned on various tasks, creating a unified model for multi-domain tasks. However, due to potential vulnerabilities in models available on open-source \u2026", "entry_id": "http://arxiv.org/abs/2505.23561v1", "updated": "2025-05-29 15:37:23", "published": "2025-05-29 15:37:23", "authors": "Zenghui Yuan;Yangming Xu;Jiawen Shi;Pan Zhou;Lichao Sun", "summary": "Model merging for Large Language Models (LLMs) directly fuses the parameters\nof different models finetuned on various tasks, creating a unified model for\nmulti-domain tasks. However, due to potential vulnerabilities in models\navailable on open-source platforms, model merging is susceptible to backdoor\nattacks. In this paper, we propose Merge Hijacking, the first backdoor attack\ntargeting model merging in LLMs. The attacker constructs a malicious upload\nmodel and releases it. Once a victim user merges it with any other models, the\nresulting merged model inherits the backdoor while maintaining utility across\ntasks. Merge Hijacking defines two main objectives-effectiveness and\nutility-and achieves them through four steps. Extensive experiments demonstrate\nthe effectiveness of our attack across different models, merging algorithms,\nand tasks. Additionally, we show that the attack remains effective even when\nmerging real-world models. Moreover, our attack demonstrates robustness against\ntwo inference-time defenses (Paraphrasing and CLEANGEN) and one training-time\ndefense (Fine-pruning).", "comment": "This paper is accepted by ACL 2025 main conference", "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR", "links": "http://arxiv.org/abs/2505.23561v1;http://arxiv.org/pdf/2505.23561v1", "pdf_url": "http://arxiv.org/pdf/2505.23561v1"}, {"title": "WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning", "link": "https://arxiv.org/pdf/2505.22942", "details": "Y Zhuang, D Jin, J Chen, W Shi, H Wang, C Zhang - arXiv preprint arXiv:2505.22942, 2025", "abstract": "Large language models (LLMs)-empowered web agents enables automating complex, real-time web navigation tasks in enterprise environments. However, existing web agents relying on supervised fine-tuning (SFT) often struggle with \u2026", "entry_id": "http://arxiv.org/abs/2505.22942v1", "updated": "2025-05-28 23:45:28", "published": "2025-05-28 23:45:28", "authors": "Yuchen Zhuang;Di Jin;Jiaao Chen;Wenqi Shi;Hanrui Wang;Chao Zhang", "summary": "Large language models (LLMs)-empowered web agents enables automating complex,\nreal-time web navigation tasks in enterprise environments. However, existing\nweb agents relying on supervised fine-tuning (SFT) often struggle with\ngeneralization and robustness due to insufficient reasoning capabilities when\nhandling the inherently dynamic nature of web interactions. In this study, we\nintroduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based\nR1-style reinforcement learning framework designed explicitly to enhance\nsingle-step reasoning and planning for business-oriented web navigation tasks.\nWe employ a structured reward function that evaluates both adherence to output\nformats and correctness of actions, enabling WorkForceAgent-R1 to implicitly\nlearn robust intermediate reasoning without explicit annotations or extensive\nexpert demonstrations. Extensive experiments on the WorkArena benchmark\ndemonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by\n10.26-16.59%, achieving competitive performance relative to proprietary\nLLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.", "comment": "Work in Progress", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.22942v1;http://arxiv.org/pdf/2505.22942v1", "pdf_url": "http://arxiv.org/pdf/2505.22942v1"}]
