[{"title": "PRIMAL: Prompting Multiple Language Models for Low-resource Diverse Response Generation", "link": "https://ieeexplore.ieee.org/abstract/document/10768975/", "details": "Z Wen, Z Tian, S Pan, K Zhu, X Meng, Y Song, D Li - IEEE/ACM Transactions on \u2026, 2024", "abstract": "Low-resource conversation models are becoming increasingly important. Existing conversation models tend to generate uninformative responses that lack diversity, especially when the training data are limited. Researchers address this issue by \u2026"}, {"title": "Training and Evaluating Language Models with Template-based Data Generation", "link": "https://arxiv.org/pdf/2411.18104", "details": "Y Zhang - arXiv preprint arXiv:2411.18104, 2024", "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, these \u2026"}, {"title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2411.14432", "details": "Y Dong, Z Liu, HL Sun, J Yang, W Hu, Y Rao, Z Liu - arXiv preprint arXiv:2411.14432, 2024", "abstract": "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high \u2026"}, {"title": "Evaluating and Advancing Multimodal Large Language Models in Ability Lens", "link": "https://arxiv.org/pdf/2411.14725", "details": "F Chen, C Gou, J Liu, Y Yang, Z Li, J Zhang, Z Sun\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As multimodal large language models (MLLMs) advance rapidly, rigorous evaluation has become essential, providing further guidance for their development. In this work, we focus on a unified and robust evaluation of\\textbf {vision perception} abilities, the \u2026"}, {"title": "Measuring Non-Adversarial Reproduction of Training Data in Large Language Models", "link": "https://arxiv.org/pdf/2411.10242%3F", "details": "M Aerni, J Rando, E Debenedetti, N Carlini, D Ippolito\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models memorize parts of their training data. Memorizing short snippets and facts is required to answer questions about the world and to be fluent in any language. But models have also been shown to reproduce long verbatim \u2026"}, {"title": "Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training", "link": "https://arxiv.org/pdf/2411.14318%3F", "details": "Z Luo, X Zhang, X Liu, H Li, Y Gong, C Qi, P Cheng - arXiv preprint arXiv:2411.14318, 2024", "abstract": "It is well-known that a diverse corpus is critical for training large language models, which are typically constructed from a mixture of various domains. In general, previous efforts resort to sampling training data from different domains with static \u2026"}, {"title": "A Real-World Benchmark for Evaluating Fine-Grained Issue Solving Capabilities of Large Language Models", "link": "https://arxiv.org/pdf/2411.18019", "details": "R Hu, C Peng, J Ren, B Jiang, X Meng, Q Wu, P Gao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Automatically resolving software issues is crucial for software development in practice, impacting the software quality and user experience. The process of resolving real-world issues encompasses tasks such as question-answering (QA) \u2026"}, {"title": "Text-to-SQL Systems in the Era of Advanced Large Language Models", "link": "https://era.library.ualberta.ca/items/3db9c207-9248-4760-8f82-0f6f308ff3ff/download/d817de66-5fe8-47fb-b065-1e5cf7644244", "details": "M Pourreza - 2024", "abstract": "Text-to-SQL conversion, the process of transforming natural language queries into executable SQL commands, stands at the forefront of bridging human linguistic capabilities with the structured logic of databases. This dissertation embarks on a \u2026"}, {"title": "MC-NEST--Enhancing Mathematical Reasoning in Large Language Models with a Monte Carlo Nash Equilibrium Self-Refine Tree", "link": "https://arxiv.org/pdf/2411.15645", "details": "G Rabby, F Keya, P Zamil, S Auer - arXiv preprint arXiv:2411.15645, 2024", "abstract": "Mathematical reasoning has proven to be a critical yet challenging task for large language models (LLMs), as they often struggle with complex multi-step problems. To address these limitations, we introduce the Monte Carlo Nash Equilibrium Self \u2026"}]
