[{"title": "From Deep Additive Kernel Learning to Last-Layer Bayesian Neural Networks via Induced Prior Approximation", "link": "https://arxiv.org/pdf/2502.10540", "details": "W Zhao, H Chen, T Liu, R Tuo, C Tian - arXiv preprint arXiv:2502.10540, 2025", "abstract": "With the strengths of both deep learning and kernel methods like Gaussian Processes (GPs), Deep Kernel Learning (DKL) has gained considerable attention in recent years. From the computational perspective, however, DKL becomes \u2026"}, {"title": "Mastering Reinforcement Learning: Foundations, Algorithms, and Real-World Applications", "link": "https://osf.io/bg79j_v2/download", "details": "X Song, K Chen, Z Bi, Q Niu, J Liu, B Peng, S Zhang\u2026 - 2025", "abstract": "Reinforcement Learning (RL) is a distinct branch of machine learning focused on how agents should take actions in an environment to maximize cumulative rewards [409]. Unlike supervised learning, which relies on labeled datasets, RL is driven by \u2026"}, {"title": "Causal-Informed Contrastive Learning: Towards Bias-Resilient Pre-training under Concept Drift", "link": "https://arxiv.org/pdf/2502.07620%3F", "details": "X Yang, J Lu, E Yu - arXiv preprint arXiv:2502.07620, 2025", "abstract": "The evolution of large-scale contrastive pre-training propelled by top-tier datasets has reached a transition point in the scaling law. Consequently, sustaining and enhancing a model's pre-training capabilities in drift environments have surfaced as \u2026"}, {"title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation", "link": "https://arxiv.org/pdf/2503.04872", "details": "L Sun, G Zhao, X Jian, Y Wu, W Lin, Y Zhu, L Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The challenge of reducing the size of Large Language Models (LLMs) while maintaining their performance has gained significant attention. However, existing methods, such as model distillation and transfer learning, often fail to achieve high \u2026"}, {"title": "Deep Out-of-Distribution Uncertainty Quantification via Weight Entropy Maximization", "link": "http://www.jmlr.org/papers/volume26/23-1359/23-1359.pdf", "details": "A de Mathelin, F Deheeger, M Mougeot, N Vayatis - Journal of Machine Learning \u2026, 2025", "abstract": "This paper deals with uncertainty quantification and out-of-distribution detection in deep learning using Bayesian and ensemble methods. It proposes a practical solution to the lack of prediction diversity observed recently for standard approaches \u2026"}, {"title": "[TINY] Vision language models can implicitly quantify aleatoric uncertainty", "link": "https://openreview.net/pdf%3Fid%3DBkWVcXevTs", "details": "X Wang, E Nalisnick - \u2026 Workshop: Quantify Uncertainty and Hallucination in \u2026", "abstract": "Recent advances in vision language models (VLMs), such as GPT-4o, have revolutionized visual reasoning by enabling zero-shot task completion through natural language instructions. In this paper, we study VLMs' ability to detect input \u2026"}, {"title": "Mapping representations in Reinforcement Learning via Semantic Alignment for Zero-Shot Stitching", "link": "https://arxiv.org/pdf/2503.01881", "details": "AP Ricciardi, V Maiorca, L Moschella, R Marin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Deep Reinforcement Learning (RL) models often fail to generalize when even small changes occur in the environment's observations or task requirements. Addressing these shifts typically requires costly retraining, limiting the reusability of learned \u2026"}, {"title": "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via Cross-Architecture Distillation", "link": "https://arxiv.org/pdf/2502.15016", "details": "J Ni, Z Liu, S Wang, M Jin, W Jin - arXiv preprint arXiv:2502.15016, 2025", "abstract": "Transformer-based and CNN-based methods demonstrate strong performance in long-term time series forecasting. However, their high computational and storage requirements can hinder large-scale deployment. To address this limitation, we \u2026"}, {"title": "Graph Structure Learning via Transfer Entropy for Multivariate Time Series Anomaly Detection", "link": "https://ieeexplore.ieee.org/abstract/document/10889333/", "details": "M Liu, Y Wang, X Zhou, Y Wang - ICASSP 2025-2025 IEEE International Conference \u2026, 2025", "abstract": "Multivariate time series anomaly detection (MTAD) poses a challenge due to temporal and feature dependencies. The critical aspects of enhancing the detection performance lie in accurately capturing the dependencies between variables within \u2026"}]
