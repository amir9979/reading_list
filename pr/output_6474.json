[{"title": "Large Language Models Prompting With Episodic Memory", "link": "https://arxiv.org/pdf/2408.07465", "details": "D Do, Q Tran, S Venkatesh, H Le - arXiv preprint arXiv:2408.07465, 2024", "abstract": "Prompt optimization is essential for enhancing the performance of Large Language Models (LLMs) in a range of Natural Language Processing (NLP) tasks, particularly in scenarios of few-shot learning where training examples are incorporated directly \u2026"}, {"title": "Understanding Defects in Generated Codes by Language Models", "link": "https://arxiv.org/pdf/2408.13372", "details": "AM Esfahani, N Kahani, SA Ajila - arXiv preprint arXiv:2408.13372, 2024", "abstract": "This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the \u2026"}, {"title": "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws", "link": "https://arxiv.org/pdf/2408.08310", "details": "R Li, Y Wei, M Zhang, N Yu, H Hu, H Peng - arXiv preprint arXiv:2408.08310, 2024", "abstract": "High-quality data is crucial for the pre-training performance of large language models. Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity. In \u2026"}, {"title": "MobileQuant: Mobile-friendly Quantization for On-device Language Models", "link": "https://arxiv.org/pdf/2408.13933", "details": "F Tan, R Lee, \u0141 Dudziak, SX Hu, S Bhattacharya\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have revolutionized language processing, delivering outstanding results across multiple applications. However, deploying LLMs on edge devices poses several challenges with respect to memory, energy, and compute \u2026"}, {"title": "Persona is a Double-edged Sword: Enhancing the Zero-shot Reasoning by Ensembling the Role-playing and Neutral Prompts", "link": "https://arxiv.org/pdf/2408.08631", "details": "J Kim, N Yang, K Jung - arXiv preprint arXiv:2408.08631, 2024", "abstract": "Recent studies demonstrate that prompting an appropriate role-playing persona to an LLM improves its reasoning capability. However, assigning a proper persona is difficult since an LLM's performance is extremely sensitive to assigned prompts; \u2026"}, {"title": "Importance Weighting Can Help Large Language Models Self-Improve", "link": "https://arxiv.org/pdf/2408.09849", "details": "C Jiang, C Chan, W Xue, Q Liu, Y Guo - arXiv preprint arXiv:2408.09849, 2024", "abstract": "Large language models (LLMs) have shown remarkable capability in numerous tasks and applications. However, fine-tuning LLMs using high-quality datasets under external supervision remains prohibitively expensive. In response, LLM self \u2026"}, {"title": "CogLM: Tracking Cognitive Development of Large Language Models", "link": "https://arxiv.org/pdf/2408.09150", "details": "X Wang, P Yuan, S Feng, Y Li, B Pan, H Wang, Y Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Piaget's Theory of Cognitive Development (PTC) posits that the development of cognitive levels forms the foundation for human learning across various abilities. As Large Language Models (LLMs) have recently shown remarkable abilities across a \u2026"}, {"title": "Designing Retrieval-Augmented Language Models for Clinical Decision Support", "link": "https://link.springer.com/chapter/10.1007/978-3-031-63592-2_13", "details": "K Quigley, T Koker, J Taylor, V Mancuso, L Brattain - AI for Health Equity and Fairness \u2026, 2024", "abstract": "Ever-increasing demands for physician expertise drive the need for trustworthy point- of-care tools that can help aid decision-making in all clinical settings. Retrieval- augmented language models carry potential to relieve the information burden on \u2026"}, {"title": "Benchmarking Large Language Models for Math Reasoning Tasks", "link": "https://arxiv.org/pdf/2408.10839", "details": "K Se\u00dfler, Y Rong, E G\u00f6zl\u00fckl\u00fc, E Kasneci - arXiv preprint arXiv:2408.10839, 2024", "abstract": "The use of Large Language Models (LLMs) in mathematical reasoning has become a cornerstone of related research, demonstrating the intelligence of these models and enabling potential practical applications through their advanced performance \u2026"}]
