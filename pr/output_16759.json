[{"title": "Likelihood Variance as Text Importance for Resampling Texts to Map Language Models", "link": "https://arxiv.org/pdf/2505.15428", "details": "M Oyama, R Kishino, H Yamagiwa, H Shimodaira - arXiv preprint arXiv:2505.15428, 2025", "abstract": "We address the computational cost of constructing a model map, which embeds diverse language models into a common space for comparison via KL divergence. The map relies on log-likelihoods over a large text set, making the cost proportional \u2026", "entry_id": "http://arxiv.org/abs/2505.15428v1", "updated": "2025-05-21 12:10:40", "published": "2025-05-21 12:10:40", "authors": "Momose Oyama;Ryo Kishino;Hiroaki Yamagiwa;Hidetoshi Shimodaira", "summary": "We address the computational cost of constructing a model map, which embeds\ndiverse language models into a common space for comparison via KL divergence.\nThe map relies on log-likelihoods over a large text set, making the cost\nproportional to the number of texts. To reduce this cost, we propose a\nresampling method that selects important texts with weights proportional to the\nvariance of log-likelihoods across models for each text. Our method\nsignificantly reduces the number of required texts while preserving the\naccuracy of KL divergence estimates. Experiments show that it achieves\ncomparable performance to uniform sampling with about half as many texts, and\nalso facilitates efficient incorporation of new models into an existing map.\nThese results enable scalable and efficient construction of language model\nmaps.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15428v1;http://arxiv.org/pdf/2505.15428v1", "pdf_url": "http://arxiv.org/pdf/2505.15428v1"}, {"title": "Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework", "link": "https://arxiv.org/pdf/2505.15245", "details": "Z Jiang, B Liu, M Peng, W Xu, Y Xiao, Z Shan, M Peng - arXiv preprint arXiv \u2026, 2025", "abstract": "While large language models (LLMs) show great potential in temporal reasoning, most existing work focuses heavily on enhancing performance, often neglecting the explainable reasoning processes underlying the results. To address this gap, we \u2026", "entry_id": "http://arxiv.org/abs/2505.15245v1", "updated": "2025-05-21 08:20:35", "published": "2025-05-21 08:20:35", "authors": "Zihao Jiang;Ben Liu;Miao Peng;Wenjie Xu;Yao Xiao;Zhenyan Shan;Min Peng", "summary": "While large language models (LLMs) show great potential in temporal\nreasoning, most existing work focuses heavily on enhancing performance, often\nneglecting the explainable reasoning processes underlying the results. To\naddress this gap, we introduce a comprehensive benchmark covering a wide range\nof temporal granularities, designed to systematically evaluate LLMs'\ncapabilities in explainable temporal reasoning. Furthermore, our findings\nreveal that LLMs struggle to deliver convincing explanations when relying\nsolely on textual information. To address challenge, we propose GETER, a novel\nstructure-aware generative framework that integrates Graph structures with text\nfor Explainable TEmporal Reasoning. Specifically, we first leverage temporal\nknowledge graphs to develop a temporal encoder that captures structural\ninformation for the query. Subsequently, we introduce a structure-text prefix\nadapter to map graph structure features into the text embedding space. Finally,\nLLMs generate explanation text by seamlessly integrating the soft graph token\nwith instruction-tuning prompt tokens. Experimental results indicate that GETER\nachieves state-of-the-art performance while also demonstrating its\neffectiveness as well as strong generalization capabilities. Our dataset and\ncode are available at https://github.com/carryTatum/GETER.", "comment": "In Findings of the Association for Computational Linguistics: ACL\n  2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.15245v1;http://arxiv.org/pdf/2505.15245v1", "pdf_url": "http://arxiv.org/pdf/2505.15245v1"}, {"title": "Computer Vision\u2013ECCV 2024 Workshops: Milan, Italy, September 29\u2013October 4, 2024, Proceedings, Part XVIII", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3D9ApfEQAAQBAJ%26oi%3Dfnd%26pg%3DPR1%26ots%3DP7971RMLR8%26sig%3DUI0OUBPP5LSpaW6i9pQeHgpviaw", "details": "A Del Bue", "abstract": "Welcome to the workshop proceedings of the 18th European Conference on Computer Vision (ECCV 2024). This year, the main ECCV event was accompanied by 73 workshops, scheduled on September 29 and 30, 2024. We received 131 \u2026"}, {"title": "Fractal Graph Contrastive Learning", "link": "https://arxiv.org/pdf/2505.11356", "details": "NZ Li, X Zhai, Z Shi, B Shi, X Jiang - arXiv preprint arXiv:2505.11356, 2025", "abstract": "While Graph Contrastive Learning (GCL) has attracted considerable attention in the field of graph self-supervised learning, its performance heavily relies on data augmentations that are expected to generate semantically consistent positive pairs \u2026", "entry_id": "http://arxiv.org/abs/2505.11356v2", "updated": "2025-05-22 14:40:09", "published": "2025-05-16 15:19:10", "authors": "Nero Z. Li;Xuehao Zhai;Zhichao Shi;Boshen Shi;Xuhui Jiang", "summary": "While Graph Contrastive Learning (GCL) has attracted considerable attention\nin the field of graph self-supervised learning, its performance heavily relies\non data augmentations that are expected to generate semantically consistent\npositive pairs. Existing strategies typically resort to random perturbations or\nlocal structure preservation, yet lack explicit control over global structural\nconsistency between augmented views. To address this limitation, we propose\nFractal Graph Contrastive Learning (FractalGCL), a theory-driven framework that\nleverages fractal self-similarity to enforce global topological coherence.\nFractalGCL introduces two key innovations: a renormalisation-based augmentation\nthat generates structurally aligned positive views via box coverings; and a\nfractal-dimension-aware contrastive loss that aligns graph embeddings according\nto their fractal dimensions. While combining the two innovations markedly\nboosts graph-representation quality, it also adds non-trivial computational\noverhead. To mitigate the computational overhead of fractal dimension\nestimation, we derive a one-shot estimator by proving that the dimension\ndiscrepancy between original and renormalised graphs converges weakly to a\ncentred Gaussian distribution. This theoretical insight enables a reduction in\ndimension computation cost by an order of magnitude, cutting overall training\ntime by approximately 61%. The experiments show that FractalGCL not only\ndelivers state-of-the-art results on standard benchmarks but also outperforms\ntraditional baselines on traffic networks by an average margin of about\nremarkably 7%. Codes are available at\n(https://anonymous.4open.science/r/FractalGCL-0511).", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.11356v2;http://arxiv.org/pdf/2505.11356v2", "pdf_url": "http://arxiv.org/pdf/2505.11356v2"}, {"title": "Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study", "link": "https://arxiv.org/pdf/2504.16414", "details": "M Khodadad, AS Kasmaee, M Astaraki, N Sherck\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a \u2026", "entry_id": "http://arxiv.org/abs/2504.16414v1", "updated": "2025-04-23 04:36:19", "published": "2025-04-23 04:36:19", "authors": "Mohammad Khodadad;Ali Shiraee Kasmaee;Mahdi Astaraki;Nicholas Sherck;Hamidreza Mahyar;Soheila Samiee", "summary": "In this study, we introduced a new benchmark consisting of a curated dataset\nand a defined evaluation process to assess the compositional reasoning\ncapabilities of large language models within the chemistry domain. We designed\nand validated a fully automated pipeline, verified by subject matter experts,\nto facilitate this task. Our approach integrates OpenAI reasoning models with\nnamed entity recognition (NER) systems to extract chemical entities from recent\nliterature, which are then augmented with external knowledge bases to form a\ncomprehensive knowledge graph. By generating multi-hop questions across these\ngraphs, we assess LLM performance in both context-augmented and non-context\naugmented settings. Our experiments reveal that even state-of-the-art models\nface significant challenges in multi-hop compositional reasoning. The results\nreflect the importance of augmenting LLMs with document retrieval, which can\nhave a substantial impact on improving their performance. However, even perfect\nretrieval accuracy with full context does not eliminate reasoning errors,\nunderscoring the complexity of compositional reasoning. This work not only\nbenchmarks and highlights the limitations of current LLMs but also presents a\nnovel data generation pipeline capable of producing challenging reasoning\ndatasets across various domains. Overall, this research advances our\nunderstanding of reasoning in computational linguistics.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2504.16414v1;http://arxiv.org/pdf/2504.16414v1", "pdf_url": "http://arxiv.org/pdf/2504.16414v1"}, {"title": "Domain-To-Text: improving Domain Generalization using Natural", "link": "https://thesis.dial.uclouvain.be/bitstreams/5511cd3a-36fc-4092-a96a-1522c3dae0ad/download", "details": "T TOMMASI, S BUCCI", "abstract": "A common assumption in machine learning is to suppose that the training and the testing distribution are the same. Unfortunately, this assumption is not always true, especially in computer vision: one of the biggest challenges that visual recognition \u2026"}, {"title": "CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs", "link": "https://arxiv.org/pdf/2505.11413", "details": "S Chen, X Li, M Zhang, EH Jiang, Q Zeng, CH Yu - arXiv preprint arXiv:2505.11413, 2025", "abstract": "Large language models (LLMs) are increasingly deployed in medical contexts, raising critical concerns about safety, alignment, and susceptibility to adversarial manipulation. While prior benchmarks assess model refusal capabilities for harmful \u2026", "entry_id": "http://arxiv.org/abs/2505.11413v1", "updated": "2025-05-16 16:25:51", "published": "2025-05-16 16:25:51", "authors": "Sijia Chen;Xiaomin Li;Mengxue Zhang;Eric Hanchen Jiang;Qingcheng Zeng;Chen-Hsiang Yu", "summary": "Large language models (LLMs) are increasingly deployed in medical contexts,\nraising critical concerns about safety, alignment, and susceptibility to\nadversarial manipulation. While prior benchmarks assess model refusal\ncapabilities for harmful prompts, they often lack clinical specificity, graded\nharmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES\n(Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for\nevaluating LLM safety in healthcare. CARES includes over 18,000 prompts\nspanning eight medical safety principles, four harm levels, and four prompting\nstyles: direct, indirect, obfuscated, and role-play, to simulate both malicious\nand benign use cases. We propose a three-way response evaluation protocol\n(Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess\nmodel behavior. Our analysis reveals that many state-of-the-art LLMs remain\nvulnerable to jailbreaks that subtly rephrase harmful prompts, while also\nover-refusing safe but atypically phrased queries. Finally, we propose a\nmitigation strategy using a lightweight classifier to detect jailbreak attempts\nand steer models toward safer behavior via reminder-based conditioning. CARES\nprovides a rigorous framework for testing and improving medical LLM safety\nunder adversarial and ambiguous conditions.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.11413v1;http://arxiv.org/pdf/2505.11413v1", "pdf_url": "http://arxiv.org/pdf/2505.11413v1"}, {"title": "Model Merging in Pre-training of Large Language Models", "link": "https://arxiv.org/pdf/2505.12082", "details": "Y Li, Y Ma, S Yan, C Zhang, J Liu, J Lu, Z Xu, M Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model \u2026", "entry_id": "http://arxiv.org/abs/2505.12082v3", "updated": "2025-05-22 09:35:43", "published": "2025-05-17 16:53:14", "authors": "Yunshui Li;Yiyuan Ma;Shen Yan;Chaoyi Zhang;Jing Liu;Jianqiao Lu;Ziwen Xu;Mengzhao Chen;Minrui Wang;Shiyi Zhan;Jin Ma;Xunhao Lai;Deyi Liu;Yao Luo;Xingyan Bin;Hongbin Ren;Mingji Han;Wenhao Hao;Bairen Yi;LingJun Liu;Bole Ma;Xiaoying Jia;Xun Zhou;Siyuan Qiao;Liang Xiang;Yonghui Wu", "summary": "Model merging has emerged as a promising technique for enhancing large\nlanguage models, though its application in large-scale pre-training remains\nrelatively unexplored. In this paper, we present a comprehensive investigation\nof model merging techniques during the pre-training process. Through extensive\nexperiments with both dense and Mixture-of-Experts (MoE) architectures ranging\nfrom millions to over 100 billion parameters, we demonstrate that merging\ncheckpoints trained with constant learning rates not only achieves significant\nperformance improvements but also enables accurate prediction of annealing\nbehavior. These improvements lead to both more efficient model development and\nsignificantly lower training costs. Our detailed ablation studies on merging\nstrategies and hyperparameters provide new insights into the underlying\nmechanisms while uncovering novel applications. Through comprehensive\nexperimental analysis, we offer the open-source community practical\npre-training guidelines for effective model merging.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.12082v3;http://arxiv.org/pdf/2505.12082v3", "pdf_url": "http://arxiv.org/pdf/2505.12082v3"}, {"title": "EfficientLLM: Efficiency in Large Language Models", "link": "https://arxiv.org/pdf/2505.13840", "details": "Z Yuan, W Sun, Y Liu, H Zhou, R Zhou, Y Li, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have driven significant progress, yet their growing parameter counts and context windows incur prohibitive compute, energy, and monetary costs. We introduce EfficientLLM, a novel benchmark and the first \u2026", "entry_id": "http://arxiv.org/abs/2505.13840v1", "updated": "2025-05-20 02:27:08", "published": "2025-05-20 02:27:08", "authors": "Zhengqing Yuan;Weixiang Sun;Yixin Liu;Huichi Zhou;Rong Zhou;Yiyang Li;Zheyuan Zhang;Wei Song;Yue Huang;Haolong Jia;Keerthiram Murugesan;Yu Wang;Lifang He;Jianfeng Gao;Lichao Sun;Yanfang Ye", "summary": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.13840v1;http://arxiv.org/pdf/2505.13840v1", "pdf_url": "http://arxiv.org/pdf/2505.13840v1"}]
