'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Few-Shot Recalibration of Language Models](https://sch'
[{"title": "Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models", "link": "https://arxiv.org/pdf/2403.17589", "details": "Y Zhang, W Zhu, H Tang, Z Ma, K Zhou, L Zhang - arXiv preprint arXiv:2403.17589, 2024", "abstract": "With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research. The adaptation strategies can be typically categorized into three \u2026"}, {"title": "DINGO: Towards Diverse and Fine-Grained Instruction-Following Evaluation", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29768/31322", "details": "Z Gu, X Sun, F Lian, Z Kang, C Xu, J Fan - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Instruction-following is particularly crucial for large language models (LLMs) to support diverse user requests. While existing work has made progress in aligning LLMs with human preferences, evaluating their capabilities on instruction-following \u2026"}, {"title": "Language Models for Text Classification: Is In-Context Learning Enough?", "link": "https://arxiv.org/pdf/2403.17661", "details": "A Edwards, J Camacho-Collados - arXiv preprint arXiv:2403.17661, 2024", "abstract": "Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero-and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand \u2026"}, {"title": "Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models", "link": "https://arxiv.org/html/2403.17359v1", "details": "Z Pan, H Luo, M Li, H Liu - arXiv preprint arXiv:2403.17359, 2024", "abstract": "We present a Chain-of-Action (CoA) framework for multimodal and retrieval- augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications:(i) unfaithful hallucination that is \u2026"}]
