[{"title": "HiMix: Reducing Computational Complexity in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2501.10318", "details": "X Zhang, D Li, B Liu, Z Bao, Y Zhou, B Yang, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Benefiting from recent advancements in large language models and modality alignment techniques, existing Large Vision-Language Models (LVLMs) have achieved prominent performance across a wide range of scenarios. However, the \u2026"}, {"title": "Metadata Conditioning Accelerates Language Model Pre-training", "link": "https://arxiv.org/pdf/2501.01956%3F", "details": "T Gao, A Wettig, L He, Y Dong, S Malladi, D Chen - arXiv preprint arXiv:2501.01956, 2025", "abstract": "The vast diversity of styles, domains, and quality levels present in language model pre-training corpora is essential in developing general model capabilities, but efficiently learning and deploying the correct behaviors exemplified in each of these \u2026"}]
