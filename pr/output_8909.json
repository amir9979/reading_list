[{"title": "Eliciting Critical Reasoning in Retrieval-Augmented Language Models via Contrastive Explanations", "link": "https://arxiv.org/pdf/2410.22874", "details": "L Ranaldi, M Valentino, A Freitas - arXiv preprint arXiv:2410.22874, 2024", "abstract": "Retrieval-augmented generation (RAG) has emerged as a critical mechanism in contemporary NLP to support Large Language Models (LLMs) in systematically accessing richer factual context. However, the integration of RAG mechanisms brings \u2026"}, {"title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models", "link": "https://arxiv.org/pdf/2411.00836", "details": "C Zou, X Guo, R Yang, J Zhang, B Hu, H Zhang - arXiv preprint arXiv:2411.00836, 2024", "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor \u2026"}, {"title": "Vision-Language Models Can Self-Improve Reasoning via Reflection", "link": "https://arxiv.org/pdf/2411.00855", "details": "K Cheng, Y Li, F Xu, J Zhang, H Zhou, Y Liu - arXiv preprint arXiv:2411.00855, 2024", "abstract": "Chain-of-thought (CoT) has proven to improve the reasoning capability of large language models (LLMs). However, due to the complexity of multimodal scenarios and the difficulty in collecting high-quality CoT data, CoT reasoning in multimodal \u2026"}, {"title": "Factuality of Large Language Models: A Survey", "link": "https://aclanthology.org/2024.emnlp-main.1088.pdf", "details": "Y Wang, M Wang, MA Manzoor, F Liu, G Georgiev\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a \u2026"}, {"title": "Language-Emphasized Cross-Lingual In-Context Learning for Multilingual LLM", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9437-9_26", "details": "J Li, X Wei, X Wang, N Zhuang, L Wang, J Dang - CCF International Conference on \u2026, 2024", "abstract": "With the recent rise of large language models (LLMs), in-context learning (ICL) has shown remarkable performance, eliminating the need for fine-tuning parameters and reducing the reliance on extensive labeled data. However, the intricacies of cross \u2026"}, {"title": "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "link": "https://arxiv.org/pdf/2410.20008", "details": "Z Zhao, Y Ziser, SB Cohen - arXiv preprint arXiv:2410.20008, 2024", "abstract": "Fine-tuning pre-trained large language models (LLMs) on a diverse array of tasks has become a common approach for building models that can solve various natural language processing (NLP) tasks. However, where and to what extent these models \u2026"}, {"title": "Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2410.21728", "details": "K Luo, Z Ding, Z Weng, L Qiao, M Zhao, X Li, D Yin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Chain of Thought (CoT) prompting approaches have significantly consolidated the reasoning capabilities of large language models (LLMs), they still face limitations that require extensive human effort or have performance needs to be improved \u2026"}, {"title": "Pandora's Box: Towards Building Universal Attackers against Real-World Large Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3DgDpWYpocE1", "details": "D Liu, M Yang, X Qu, P Zhou, X Fang, K Tang, Y Wan\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across a wide range of multimodal understanding tasks. Nevertheless, these models are susceptible to adversarial examples. In real-world applications, existing LVLM \u2026"}, {"title": "CLR-Bench: Evaluating Large Language Models in College-level Reasoning", "link": "https://arxiv.org/pdf/2410.17558", "details": "J Dong, Z Hong, Y Bei, F Huang, X Wang, X Huang - arXiv preprint arXiv:2410.17558, 2024", "abstract": "Large language models (LLMs) have demonstrated their remarkable performance across various language understanding tasks. While emerging benchmarks have been proposed to evaluate LLMs in various domains such as mathematics and \u2026"}]
