[{"title": "Designing Retrieval-Augmented Language Models for Clinical Decision", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DWcMbEQAAQBAJ%26oi%3Dfnd%26pg%3DPA159%26ots%3DtCwXv1SEbn%26sig%3DRSfAKP7MOdFPp7UIWp9MYYDh5-A", "details": "K Quigley, T Koker, J Taylor, V Mancuso - AI for Health Equity and Fairness: Leveraging AI to \u2026", "abstract": "Ever-increasing demands for physician expertise drive the need for trust-worthy point- of-care tools that can help aid decision-making in all clinical settings. Retrieval- augmented language models carry potential to relieve the information burden on \u2026"}, {"title": "Toward a Patient-Centered Care Supporting System: Integration of Multidisciplinary Health Records in Breast Cancer Care", "link": "https://pubmed.ncbi.nlm.nih.gov/39176542/", "details": "A Sugiyama, K Utsunomiya, H Okumiya, K Fujimoto\u2026 - Studies in health technology \u2026, 2024", "abstract": "An important paradigm shift within healthcare is the shift toward patient-centered care (PCC). Multidisciplinary team meetings (MDTM) are considered essential for PCC, despite being considered time-consuming and expensive. Patient-centered \u2026"}, {"title": "Language Models Pre-training", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-65647-7_2.pdf", "details": "U Kamath, K Keenan, G Somers, S Sorenson - Large Language Models: A Deep Dive \u2026, 2024", "abstract": "Pre-training forms the foundation for LLMs' capabilities. LLMs gain vital language comprehension and generative language skills by using large-scale datasets. The size and quality of these datasets are essential for maximizing LLMs' potential. It is \u2026"}, {"title": "NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks", "link": "https://arxiv.org/pdf/2408.13106", "details": "H Huang, T Park, K Dhawan, I Medennikov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-supervised learning has been proved to benefit a wide range of speech processing tasks, such as speech recognition/translation, speaker verification and diarization, etc. However, most of these approaches are computationally intensive \u2026"}, {"title": "Towards Harnessing Large Language Models as Autonomous Agents for Semantic Triple Extraction from Unstructured Text", "link": "https://ceur-ws.org/Vol-3747/text2kg_paper1.pdf", "details": "A Ananya, S Tiwari, N Mihindukulasooriya, T Soru\u2026 - 2024", "abstract": "Abstract The use of Large Language Models as autonomous agents interacting with tools has shown to improve the performance of several tasks from code generation to API calling and sequencing. This paper proposes a framework for using Large \u2026"}, {"title": "CEval: A Benchmark for Evaluating Counterfactual Text Generation", "link": "https://aclanthology.org/2024.inlg-main.6.pdf", "details": "C Seifert, J Schl\u00f6tterer - Proceedings of the 17th International Natural \u2026, 2024", "abstract": "Counterfactual text generation aims to minimally change a text, such that it is classified differently. Assessing progress in method development for counterfactual text generation is hindered by a non-uniform usage of data sets and metrics in \u2026"}, {"title": "Automated Mining of Structured Knowledge from Text in the Era of Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671469", "details": "Y Zhang, M Zhong, S Ouyang, Y Jiao, S Zhou, L Ding\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "Massive amount of unstructured text data are generated daily, ranging from news articles to scientific papers. How to mine structured knowledge from the text data remains a crucial research question. Recently, large language models (LLMs) have \u2026"}, {"title": "Focused Large Language Models are Stable Many-Shot Learners", "link": "https://arxiv.org/pdf/2408.13987", "details": "P Yuan, S Feng, Y Li, X Wang, Y Zhang, C Tan, B Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL \u2026"}, {"title": "Generating Synthetic Datasets for Few-shot Prompt Tuning", "link": "https://openreview.net/pdf%3Fid%3DVd0KvChLXr", "details": "X Guo, Z Du, B Li, C Miao - First Conference on Language Modeling", "abstract": "A major limitation of prompt tuning is its dependence on large labeled training datasets. Under few-shot learning settings, prompt tuning lags far behind full-model fine-tuning, limiting its scope of application. In this paper, we leverage the powerful \u2026"}]
