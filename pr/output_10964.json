[{"title": "KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view Knowledge Graph Contrastive Learning", "link": "https://arxiv.org/pdf/2412.04948%3F", "details": "P Yu, C Deng, B Dai, X Wang, Y Wen - arXiv preprint arXiv:2412.04948, 2024", "abstract": "Autoregressive large language models (LLMs) pre-trained by next token prediction are inherently proficient in generative tasks. However, their performance on knowledge-driven tasks such as factual knowledge querying remains unsatisfactory \u2026"}, {"title": "Leveraging Contrastive Learning for Semantic Segmentation with Consistent Labels Across Varying Appearances", "link": "https://arxiv.org/pdf/2412.16592", "details": "J Montalvo, R Alcover-Couso, P Carballeira\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces a novel synthetic dataset that captures urban scenes under a variety of weather conditions, providing pixel-perfect, ground-truth-aligned images to facilitate effective feature alignment across domains. Additionally, we propose a \u2026"}, {"title": "Multi-Scale Contrastive Learning for Video Temporal Grounding", "link": "https://arxiv.org/pdf/2412.07157", "details": "TT Nguyen, Y Bin, X Wu, Z Hu, CDT Nguyen, SK Ng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Temporal grounding, which localizes video moments related to a natural language query, is a core problem of vision-language learning and video understanding. To encode video moments of varying lengths, recent methods employ a multi-level \u2026"}, {"title": "CTPT: Continual Test-time Prompt Tuning for vision-language models", "link": "https://www.sciencedirect.com/science/article/pii/S0031320324010513", "details": "F Wang, Z Han, X Liu, Y Yin, X Gao - Pattern Recognition, 2024", "abstract": "Abstract Test-time Prompt Tuning (TPT) aims to further enhance the generalization capabilities of pre-trained vision-language models, eg, CLIP, on streaming test samples from a new distribution. Current TPT methods primarily utilize self-training \u2026"}, {"title": "Enhancing radiology report generation through pre-trained language models", "link": "https://link.springer.com/article/10.1007/s13748-024-00358-5", "details": "G Leonardi, L Portinale, A Santomauro - Progress in Artificial Intelligence, 2024", "abstract": "In the healthcare field, the ability to integrate and process data from various modalities, such as medical images, clinical notes, and patient records, plays a central role in enabling Artificial Intelligence models to provide more informed \u2026"}, {"title": "Masked Deformation Modeling for Volumetric Brain MRI Self-supervised Pre-training", "link": "https://ieeexplore.ieee.org/abstract/document/10777582/", "details": "J Lyu, PF Bartlett, FA Nasrallah, X Tang - IEEE Transactions on Medical Imaging, 2024", "abstract": "Self-supervised learning (SSL) has been proposed to alleviate neural networks' reliance on annotated data and to improve downstream tasks' performance, which has obtained substantial success in several volumetric medical image segmentation \u2026"}, {"title": "COSMOS: Cross-Modality Self-Distillation for Vision Language Pre-training", "link": "https://arxiv.org/pdf/2412.01814%3F", "details": "S Kim, R Xiao, MI Georgescu, S Alaniz, Z Akata - arXiv preprint arXiv:2412.01814, 2024", "abstract": "Vision-Language Models (VLMs) trained with contrastive loss have achieved significant advancements in various vision and language tasks. However, the global nature of contrastive loss makes VLMs focus predominantly on foreground objects \u2026"}, {"title": "ModelGrow: Continual Text-to-Video Pre-training with Model Expansion and Language Understanding Enhancement", "link": "https://arxiv.org/abs/2412.18966", "details": "Z Rao, L Ji, Y Xing, R Liu, Z Liu, J Xie, Z Peng, Y He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Text-to-video (T2V) generation has gained significant attention recently. However, the costs of training a T2V model from scratch remain persistently high, and there is considerable room for improving the generation performance, especially under \u2026"}, {"title": "HC-LLM: Historical-Constrained Large Language Models for Radiology Report Generation", "link": "https://arxiv.org/pdf/2412.11070", "details": "T Liu, J Wang, Y Hu, M Li, J Yi, X Chang, J Gao, B Yin - arXiv preprint arXiv \u2026, 2024", "abstract": "Radiology report generation (RRG) models typically focus on individual exams, often overlooking the integration of historical visual or textual data, which is crucial for patient follow-ups. Traditional methods usually struggle with long sequence \u2026"}]
