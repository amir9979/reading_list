[{"title": "DuCo-Net: Dual-Contrastive Learning Network for Medical Report Retrieval Leveraging Enhanced Encoders and Augmentations", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10870249.pdf", "details": "ZU Rahman, JH Lee, DT Vu, I Murtza, JY Kim - IEEE Access, 2025", "abstract": "The conventional process of generating medical radiology reports is labor-intensive and time-consuming, requiring radiologists to describe findings meticulously from imaging studies. This manual approach often causes undesirable delays in patient \u2026"}, {"title": "Valley2: Exploring Multimodal Models with Scalable Vision-Language Design", "link": "https://arxiv.org/pdf/2501.05901", "details": "Z Wu, Z Chen, R Luo, C Zhang, Y Gao, Z He, X Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, vision-language models have made remarkable progress, demonstrating outstanding capabilities in various tasks such as image captioning and video understanding. We introduce Valley2, a novel multimodal large language model \u2026"}, {"title": "EvidenceMap: Unleashing the Power of Small Language Models with Evidence Analysis for Biomedical Question Answering", "link": "https://arxiv.org/pdf/2501.12746", "details": "C Zong, J Wan, L Zhang - arXiv preprint arXiv:2501.12746, 2025", "abstract": "Current LLM-based approaches improve question answering performance by leveraging the internal reasoning abilities of models or incorporating external knowledge. However, when humans address professional problems, it is essential to \u2026"}, {"title": "LVPruning: An Effective yet Simple Language-Guided Vision Token Pruning Approach for Multi-modal Large Language Models", "link": "https://arxiv.org/pdf/2501.13652", "details": "Y Sun, Y Xin, H Li, J Sun, C Lin, R Batista-Navarro - arXiv preprint arXiv:2501.13652, 2025", "abstract": "Multi-modal Large Language Models (MLLMs) have achieved remarkable success by integrating visual and textual modalities. However, they incur significant computational overhead due to the large number of vision tokens processed, limiting \u2026"}, {"title": "Exploring the Role of Explicit Temporal Modeling in Multimodal Large Language Models for Video Understanding", "link": "https://arxiv.org/pdf/2501.16786", "details": "Y Li, Z Liu, Y Kong, G Li, J Zhang, C Bian, F Liu, L Yao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Applying Multimodal Large Language Models (MLLMs) to video understanding presents significant challenges due to the need to model temporal relations across frames. Existing approaches adopt either implicit temporal modeling, relying solely \u2026"}, {"title": "LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2501.06986", "details": "MN Azadani, J Riddell, S Sedwards, K Czarnecki - arXiv preprint arXiv:2501.06986, 2025", "abstract": "Enhanced visual understanding serves as a cornerstone for multimodal large language models (MLLMs). Recent hybrid MLLMs incorporate a mixture of vision experts to address the limitations of using a single vision encoder and excessively \u2026"}, {"title": "Semantic-aware contrastive learning via multi-prompt alignment", "link": "https://link.springer.com/article/10.1007/s10994-024-06665-1", "details": "Z Zhao, H Qin, M Kong, L Chen, D Xie, J Zhu, Q Zhu - Machine Learning, 2025", "abstract": "The role of the sample generation mechanism in contrastive learning is pivotal. It not only determines the pairings of positive and negative samples but also enriches the diversity of the sample pool, thereby substantially affecting the quality of the learned \u2026"}, {"title": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment", "link": "https://arxiv.org/pdf/2502.04328", "details": "Z Liu, Y Dong, J Wang, Z Liu, W Hu, J Lu, Y Rao - arXiv preprint arXiv:2502.04328, 2025", "abstract": "Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have \u2026"}, {"title": "FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers", "link": "https://arxiv.org/pdf/2501.16297%3F", "details": "R Zhang, R Shao, G Chen, K Zhou, W Guan, L Nie - arXiv preprint arXiv:2501.16297, 2025", "abstract": "The incorporation of high-resolution visual input equips multimodal large language models (MLLMs) with enhanced visual perception capabilities for real-world tasks. However, most existing high-resolution MLLMs rely on a cropping-based approach \u2026"}]
