[{"title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback", "link": "https://openreview.net/pdf%3Fid%3DBOorDpKHiJ", "details": "G Cui, L Yuan, N Ding, G Yao, B He, W Zhu, Y Ni, G Xie\u2026 - Forty-first International Conference \u2026", "abstract": "Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability \u2026"}, {"title": "Impact of high-quality, mixed-domain data on the performance of medical language models", "link": "https://academic.oup.com/jamia/advance-article-abstract/doi/10.1093/jamia/ocae120/7680487", "details": "M Griot, C Hemptinne, J Vanderdonckt, D Yuksel - Journal of the American Medical \u2026, 2024", "abstract": "Objective To optimize the training strategy of large language models for medical applications, focusing on creating clinically relevant systems that efficiently integrate into healthcare settings, while ensuring high standards of accuracy and reliability \u2026"}, {"title": "Conformal Counterfactual Inference under Hidden Confounding", "link": "https://arxiv.org/pdf/2405.12387", "details": "Z Chen, R Guo, JF Ton, Y Liu - arXiv preprint arXiv:2405.12387, 2024", "abstract": "Personalized decision making requires the knowledge of potential outcomes under different treatments, and confidence intervals about the potential outcomes further enrich this decision-making process and improve its reliability in high-stakes \u2026"}, {"title": "Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data", "link": "https://arxiv.org/pdf/2405.14212", "details": "H Li, X Zhao, D Guo, H Gu, Z Zeng, Y Han, Y Song\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As large language models (LLMs) demonstrate unparalleled performance and generalization ability, LLMs are widely used and integrated into various applications. When it comes to sensitive domains, as commonly described in federated learning \u2026"}, {"title": "Pathophysiological Features in Electronic Medical Records Sustain Model Performance under Temporal Dataset Shift", "link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11141811/", "details": "R Brosula, CK Corbin, JH Chen - AMIA Summits on Translational Science \u2026, 2024", "abstract": "Access to real-world data streams like electronic medical records (EMRs) has accelerated the development of supervised machine learning (ML) models for clinical applications. However, few studies investigate the differential impact of \u2026"}, {"title": "Learning the Meta Feature Transformer for Unsupervised Person Re-Identification", "link": "https://www.mdpi.com/2227-7390/12/12/1812", "details": "Q Li, C Yan, X Peng - Mathematics, 2024", "abstract": "Although unsupervised person re-identification (Re-ID) has drawn increasing research attention, it still faces the challenge of learning discriminative features in the absence of pairwise labels across disjoint camera views. To tackle the issue of label \u2026"}, {"title": "Nearly Tight Black-Box Auditing of Differentially Private Machine Learning", "link": "https://arxiv.org/pdf/2405.14106", "details": "MSMS Annamalai, E De Cristofaro - arXiv preprint arXiv:2405.14106, 2024", "abstract": "This paper presents a nearly tight audit of the Differentially Private Stochastic Gradient Descent (DP-SGD) algorithm in the black-box model. Our auditing procedure empirically estimates the privacy leakage from DP-SGD using \u2026"}, {"title": "FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2405.18218", "details": "Y Zhang, Y Li, X Wang, Q Shen, B Plank, B Bischl\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Overparametrized transformer networks are the state-of-the-art architecture for Large Language Models (LLMs). However, such models contain billions of parameters making large compute a necessity, while raising environmental concerns. To \u2026"}, {"title": "CSAMDT: Conditional Self Attention Memory-Driven Transformers for Radiology Report Generation from Chest X-Ray", "link": "https://link.springer.com/article/10.1007/s10278-024-01126-6", "details": "I Shahzadi, TM Madni, UI Janjua, G Batool, B Naz\u2026 - Journal of Imaging \u2026, 2024", "abstract": "A radiology report plays a crucial role in guiding patient treatment, but writing these reports is a time-consuming task that demands a radiologist's expertise. In response to this challenge, researchers in the subfields of artificial intelligence for healthcare \u2026"}]
