[{"title": "Style over Substance: Distilled Language Models Reason Via Stylistic Replication", "link": "https://arxiv.org/pdf/2504.01738", "details": "P Lippmann, J Yang - arXiv preprint arXiv:2504.01738, 2025", "abstract": "Specialized reasoning language models (RLMs) have demonstrated that scaling test- time computation through detailed reasoning traces significantly enhances performance. Although these traces effectively facilitate knowledge distillation into \u2026"}, {"title": "Explainable differential diagnosis with dual-inference large language models", "link": "https://www.nature.com/articles/s44401-025-00015-6", "details": "S Zhou, M Lin, S Ding, J Wang, C Chen, GB Melton\u2026 - npj Health Systems, 2025", "abstract": "Automatic differential diagnosis (DDx) involves identifying potential conditions that could explain a patient's symptoms and its accurate interpretation is of substantial significance. While large language models (LLMs) have demonstrated remarkable \u2026"}, {"title": "What, how, where, and how well? a survey on test-time scaling in large language models", "link": "https://arxiv.org/pdf/2503.24235%3F", "details": "Q Zhang, F Lyu, Z Sun, L Wang, W Zhang, Z Guo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as``test-time computing''has emerged as a prominent research focus. Recent studies demonstrate \u2026"}, {"title": "Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for Large Language Models", "link": "https://arxiv.org/pdf/2504.02273%3F", "details": "H Le, D Do, D Nguyen, S Venkatesh - arXiv preprint arXiv:2504.02273, 2025", "abstract": "Recent advances in fine-tuning large language models (LLMs) with reinforcement learning (RL) have shown promising improvements in complex reasoning tasks, particularly when paired with chain-of-thought (CoT) prompting. However, these \u2026"}, {"title": "Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study", "link": "https://arxiv.org/pdf/2504.16414", "details": "M Khodadad, AS Kasmaee, M Astaraki, N Sherck\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a \u2026"}, {"title": "DIMT25@ ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model", "link": "https://arxiv.org/pdf/2504.17315", "details": "Z Wu, T Song, N Xie, W Zhang, P Li, S Wu, C Li, J Zhu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper presents the technical solution proposed by Huawei Translation Service Center (HW-TSC) for the\" End-to-End Document Image Machine Translation for Complex Layouts\" competition at the 19th International Conference on Document \u2026"}, {"title": "When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars", "link": "https://arxiv.org/pdf/2504.17562", "details": "R Higuchi, R Kawata, N Nishikawa, K Oko\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The ability to acquire latent semantics is one of the key properties that determines the performance of language models. One convenient approach to invoke this ability is to prepend metadata (eg URLs, domains, and styles) at the beginning of texts in the \u2026"}, {"title": "Can large language models independently complete tasks? A dynamic evaluation framework for multi-turn task planning and completion", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225008070", "details": "J Gao, J Cui, H Wu, L Xiang, H Zhao, X Li, M Fang\u2026 - Neurocomputing, 2025", "abstract": "Large language models (LLMs) are increasingly relied upon for multi-turn dialogue to conduct complex tasks. However, existing benchmarks mainly evaluate LLMs as agents, overlooking their potential as independent systems to accomplish complex \u2026"}, {"title": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scale Test-Time Compute", "link": "https://arxiv.org/pdf/2504.00762%3F", "details": "J Chen, Z Xun, B Zhou, H Qi, Q Zhang, Y Chen, W Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper presents a simple, effective, and cost-efficient strategy to improve LLM performance by scaling test-time compute. Our strategy builds upon the repeated- sampling-then-voting framework, with a novel twist: incorporating multiple models \u2026"}]
