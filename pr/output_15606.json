[{"title": "$\\pi_ {0.5} $: a Vision-Language-Action Model with Open-World Generalization", "link": "https://arxiv.org/pdf/2504.16054", "details": "P Intelligence, K Black, N Brown, J Darpinian\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In order for robots to be useful, they must perform practically relevant tasks in the real world, outside of the lab. While vision-language-action (VLA) models have demonstrated impressive results for end-to-end robot control, it remains an open \u2026"}, {"title": "Steering CLIP's vision transformer with sparse autoencoders", "link": "https://arxiv.org/abs/2504.08729", "details": "S Joseph, P Suresh, E Goldfarb, L Hufe, Y Gandelsman\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While vision models are highly capable, their internal mechanisms remain poorly understood--a challenge which sparse autoencoders (SAEs) have helped address in language, but which remains underexplored in vision. We address this gap by \u2026"}]
