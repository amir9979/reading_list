[{"title": "Addressing Asynchronicity in Clinical Multimodal Fusion via Individualized Chest X-ray Generation", "link": "https://arxiv.org/pdf/2410.17918", "details": "W Yao, C Liu, K Yin, WK Cheung, J Qin - arXiv preprint arXiv:2410.17918, 2024", "abstract": "Integrating multi-modal clinical data, such as electronic health records (EHR) and chest X-ray images (CXR), is particularly beneficial for clinical prediction tasks. However, in a temporal setting, multi-modal data are often inherently asynchronous \u2026"}, {"title": "On the Comparison between Multi-modal and Single-modal Contrastive Learning", "link": "https://arxiv.org/pdf/2411.02837", "details": "W Huang, A Han, Y Chen, Y Cao, Z Xu, T Suzuki - arXiv preprint arXiv:2411.02837, 2024", "abstract": "Multi-modal contrastive learning with language supervision has presented a paradigm shift in modern machine learning. By pre-training on a web-scale dataset, multi-modal contrastive learning can learn high-quality representations that exhibit \u2026"}, {"title": "Evaluation of a task specific self-supervised learning framework in digital pathology relative to transfer learning approaches and existing foundation models", "link": "https://www.modernpathology.org/article/S0893-3952\\(24\\)00216-3/fulltext", "details": "T Rahman, AS Baras, R Chellappa - Modern Pathology, 2024", "abstract": "An integral stage in typical digital pathology workflows involves deriving specific features from tiles extracted from a tessellated whole slide image. Notably, various computer vision neural network architectures, particularly the ImageNet pre-trained \u2026"}, {"title": "SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology Image Understanding", "link": "https://arxiv.org/pdf/2410.11761", "details": "Y Chen, G Wang, Y Ji, Y Li, J Ye, T Li, B Zhang, N Pei\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the progress made by multimodal large language models (MLLMs) in computational pathology, they remain limited by a predominant focus on patch-level analysis, missing essential contextual information at the whole-slide level. The lack \u2026"}, {"title": "When Attention Sink Emerges in Language Models: An Empirical View", "link": "https://arxiv.org/pdf/2410.10781", "details": "X Gu, T Pang, C Du, Q Liu, F Zhang, C Du, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as attention sink. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV \u2026"}, {"title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "link": "https://arxiv.org/pdf/2410.11829%3F", "details": "Y Cao, Y Liu, Z Chen, G Shi, W Wang, D Zhao, T Lu - arXiv preprint arXiv:2410.11829, 2024", "abstract": "Despite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating \u2026"}, {"title": "ISImed: A Framework for Self-Supervised Learning using Intrinsic Spatial Information in Medical Images", "link": "https://arxiv.org/pdf/2410.16947", "details": "N Jabareen, D Yuan, S Lukassen - arXiv preprint arXiv:2410.16947, 2024", "abstract": "This paper demonstrates that spatial information can be used to learn interpretable representations in medical images using Self-Supervised Learning (SSL). Our proposed method, ISImed, is based on the observation that medical images exhibit a \u2026"}, {"title": "Beyond Labels: A Self-Supervised Framework with Masked Autoencoders and Random Cropping for Breast Cancer Subtype Classification", "link": "https://arxiv.org/pdf/2410.12006", "details": "A Chiocchetti, M Dossena, C Irwin, L Portinale - arXiv preprint arXiv:2410.12006, 2024", "abstract": "This work contributes to breast cancer sub-type classification using histopathological images. We utilize masked autoencoders (MAEs) to learn a self-supervised embedding tailored for computer vision tasks in this domain. This embedding \u2026"}, {"title": "Exploring the Impact of Backbone Architecture on Explainable CNNs' Interpretability", "link": "https://www.researchgate.net/profile/Zalan-Bodo/publication/384925487_Exploring_the_Impact_of_Backbone_Architecture_on_Explainable_CNNs%27_Interpretability/links/670e3ccf77bab74415a19534/Exploring-the-Impact-of-Backbone-Architecture-on-Explainable-CNNs-Interpretability.pdf", "details": "\u00c1 PORTIK, A BAJCSI, A SZENKOVITS, Z BOD\u00d3 - Acta Univ. Sapientiae, 2024", "abstract": "The growing demand for interpretable models in machine learning underscores the importance of transparency in decision-making processes for building trust and ensuring accountability in AI systems. Unlike complex black-box models \u2026"}]
