[{"title": "An Empirical Evaluation of the Zero-shot, Few-shot, and Traditional Fine-tuning Based Pretrained Language Models for Sentiment Analysis in Software Engineering", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10623654.pdf", "details": "M Shafikuzzaman, MR Islam, AC Rolli, S Akhter\u2026 - IEEE Access, 2024", "abstract": "Recent advances in natural language processing (NLP) have led to the development of revolutionized pretrained language models (PLMs) impacting various NLP tasks, including sentiment analysis in software engineering. Choosing the right PLMs is \u2026"}, {"title": "Mitigating Entity-Level Hallucination in Large Language Models", "link": "https://arxiv.org/pdf/2407.09417%3Ftrk%3Dpublic_post_comment-text", "details": "W Su, Y Tang, Q Ai, C Wang, Z Wu, Y Liu - arXiv preprint arXiv:2407.09417, 2024", "abstract": "The emergence of Large Language Models (LLMs) has revolutionized how users access information, shifting from traditional search engines to direct question-and- answer interactions with LLMs. However, the widespread adoption of LLMs has \u2026"}, {"title": "Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement", "link": "https://arxiv.org/pdf/2408.03092", "details": "L Yu, B Yu, H Yu, F Huang, Y Li - arXiv preprint arXiv:2408.03092, 2024", "abstract": "Merging Large Language Models (LLMs) aims to amalgamate multiple homologous LLMs into one with all the capabilities. Ideally, any LLMs sharing the same backbone should be mergeable, irrespective of whether they are Fine-Tuned (FT) with minor \u2026"}, {"title": "Interpretable Differential Diagnosis with Dual-Inference Large Language Models", "link": "https://arxiv.org/pdf/2407.07330", "details": "S Zhou, S Ding, J Wang, M Lin, GB Melton, R Zhang - arXiv preprint arXiv:2407.07330, 2024", "abstract": "Methodological advancements to automate the generation of differential diagnosis (DDx) to predict a list of potential diseases as differentials given patients' symptom descriptions are critical to clinical reasoning and applications such as decision \u2026"}, {"title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "link": "https://arxiv.org/pdf/2407.10817", "details": "T Vu, K Krishna, S Alzubi, C Tar, M Faruqui, YH Sung - arXiv preprint arXiv \u2026, 2024", "abstract": "As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their output due to the high costs of human evaluation. To make progress towards better LLM autoraters, we introduce FLAMe, a family of Foundational Large \u2026"}]
