[{"title": "MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization", "link": "https://arxiv.org/pdf/2412.06141", "details": "K Zhu, P Xia, Y Li, H Zhu, S Wang, H Yao - arXiv preprint arXiv:2412.06141, 2024", "abstract": "The advancement of Large Vision-Language Models (LVLMs) has propelled their application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter factuality challenges due to modality misalignment, where the models prioritize \u2026"}, {"title": "Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding", "link": "https://arxiv.org/pdf/2501.00712", "details": "J Zhu, P Wang, R Cai, JD Lee, P Li, Z Wang - arXiv preprint arXiv:2501.00712, 2025", "abstract": "Transformers rely on both content-based and position-based addressing mechanisms to make predictions, but existing positional encoding techniques often diminish the effectiveness of position-based addressing. Many current methods \u2026"}, {"title": "Distilling Large Language Models for Efficient Clinical Information Extraction", "link": "https://arxiv.org/pdf/2501.00031", "details": "KS Vedula, A Gupta, A Swaminathan, I Lopez, S Bedi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) excel at clinical information extraction but their computational demands limit practical deployment. Knowledge distillation--the process of transferring knowledge from larger to smaller models--offers a potential \u2026"}, {"title": "Enhancing LLMs for Physics Problem-Solving using Reinforcement Learning with Human-AI Feedback", "link": "https://arxiv.org/pdf/2412.06827", "details": "A Anand, K Prasad, C Kirtani, AR Nair, M Gupta\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in text-based tasks but struggle with the complex reasoning required for physics problems, particularly in advanced arithmetic and conceptual understanding. While some \u2026"}, {"title": "PETapter: Leveraging PET-style classification heads for modular few-shot parameter-efficient fine-tuning", "link": "https://arxiv.org/pdf/2412.04975%3F", "details": "J Rieger, M Ruckdeschel, G Wiedemann - arXiv preprint arXiv:2412.04975, 2024", "abstract": "Few-shot learning and parameter-efficient fine-tuning (PEFT) are crucial to overcome the challenges of data scarcity and ever growing language model sizes. This applies in particular to specialized scientific domains, where researchers might lack \u2026"}, {"title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases", "link": "https://arxiv.org/pdf/2412.04862", "details": "LG Research, S An, K Bae, E Choi, K Choi, SJ Choi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8 B, and 2.4 B. These models feature several \u2026"}, {"title": "AutoReason: Automatic Few-Shot Reasoning Decomposition", "link": "https://arxiv.org/pdf/2412.06975", "details": "A Sevinc, A Gumus - arXiv preprint arXiv:2412.06975, 2024", "abstract": "Chain of Thought (CoT) was introduced in recent research as a method for improving step-by-step reasoning in Large Language Models. However, CoT has limited applications such as its need for hand-crafted few-shot exemplar prompts and no \u2026"}, {"title": "Dynamic Ensemble Reasoning for LLM Experts", "link": "https://arxiv.org/pdf/2412.07448", "details": "J Hu, Y Wang, S Zhang, K Zhou, G Chen, Y Hu, B Xiao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Ensemble reasoning for the strengths of different LLM experts is critical to achieving consistent and satisfactory performance on diverse inputs across a wide range of tasks. However, existing LLM ensemble methods are either computationally \u2026"}, {"title": "LLM+ AL: Bridging Large Language Models and Action Languages for Complex Reasoning about Actions", "link": "https://arxiv.org/pdf/2501.00830", "details": "A Ishay, J Lee - arXiv preprint arXiv:2501.00830, 2025", "abstract": "Large Language Models (LLMs) have made significant strides in various intelligent tasks but still struggle with complex action reasoning tasks that require systematic search. To address this limitation, we propose a method that bridges the natural \u2026"}]
