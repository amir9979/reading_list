[{"title": "Building and better understanding vision-language models: insights and future directions", "link": "https://arxiv.org/pdf/2408.12637", "details": "H Lauren\u00e7on, A Marafioti, V Sanh, L Tronchon - arXiv preprint arXiv:2408.12637, 2024", "abstract": "The field of vision-language models (VLMs), which take images and texts as inputs and output texts, is rapidly evolving and has yet to reach consensus on several key aspects of the development pipeline, including data, architecture, and training \u2026"}, {"title": "Towards Holistic Disease Risk Prediction using Small Language Models", "link": "https://arxiv.org/pdf/2408.06943", "details": "L Bj\u00f6rkdahl, O Pauli, J \u00d6stman, C Ceccobello\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Data in the healthcare domain arise from a variety of sources and modalities, such as x-ray images, continuous measurements, and clinical notes. Medical practitioners integrate these diverse data types daily to make informed and accurate decisions \u2026"}, {"title": "Stacked Reflective Reasoning in Large Neural Language Models", "link": "https://ceur-ws.org/Vol-3740/paper-121.pdf", "details": "K Villarreal-Haro, F S\u00e1nchez-Vega, A Rosales-P\u00e9rez\u2026 - Working Notes of CLEF, 2024", "abstract": "Sexism, far from being merely a conceptual issue, is a concerning and pervasive social health problem that negatively impacts individuals' well-being and perception. In today's digital era, as sexism permeates online platforms, the creation of systems \u2026"}, {"title": "Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability", "link": "https://arxiv.org/pdf/2408.07852", "details": "J Hron, L Culp, G Elsayed, R Liu, B Adlam, M Bileschi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While many capabilities of language models (LMs) improve with increased training budget, the influence of scale on hallucinations is not yet fully understood. Hallucinations come in many forms, and there is no universally accepted definition \u2026"}, {"title": "Interactive dual-stream contrastive learning for radiology report generation", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001369", "details": "Z Zhang, A Jiang - Journal of Biomedical Informatics, 2024", "abstract": "Radiology report generation automates diagnostic narrative synthesis from medical imaging data. Current report generation methods primarily employ knowledge graphs for image enhancement, neglecting the interpretability and guiding function of \u2026"}, {"title": "Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models", "link": "https://arxiv.org/pdf/2408.13808", "details": "DK Pham, BQ Vo - arXiv preprint arXiv:2408.13808, 2024", "abstract": "The rapid advancement of large language models (LLMs) has significantly impacted various domains, including healthcare and biomedicine. However, the phenomenon of hallucination, where LLMs generate outputs that deviate from factual accuracy or \u2026"}, {"title": "Language Models Pre-training", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-65647-7_2.pdf", "details": "U Kamath, K Keenan, G Somers, S Sorenson - Large Language Models: A Deep Dive \u2026, 2024", "abstract": "Pre-training forms the foundation for LLMs' capabilities. LLMs gain vital language comprehension and generative language skills by using large-scale datasets. The size and quality of these datasets are essential for maximizing LLMs' potential. It is \u2026"}, {"title": "An Incomplete Loop: Instruction Inference, Instruction Following, and In-Context Learning in Language Models", "link": "https://openreview.net/pdf%3Fid%3DnUNbjMDBWC", "details": "E Liu, G Neubig, J Andreas - First Conference on Language Modeling", "abstract": "Modern language models (LMs) can learn to perform new tasks in different ways: in instruction following, the target task is described explicitly in natural language; in few- shot prompting, the task is specified implicitly with a small number of examples; in \u2026"}]
