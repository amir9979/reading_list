'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Counterfactual contrastive learning: robust representa'
[{"title": "Global Contrastive Training for Multimodal Electronic Health Records with Language Supervision", "link": "https://arxiv.org/pdf/2404.06723", "details": "Y Ma, S Kolla, Z Hu, D Kaliraman, V Nolan, Z Guan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern electronic health records (EHRs) hold immense promise in tracking personalized patient health trajectories through sequential deep learning, owing to their extensive breadth, scale, and temporal granularity. Nonetheless, how to \u2026"}, {"title": "A novel generative adversarial networks modelling for the class imbalance problem in high dimensional omics data", "link": "https://link.springer.com/article/10.1186/s12911-024-02487-2", "details": "S Cusworth, GV Gkoutos, A Acharjee - BMC Medical Informatics and Decision Making, 2024", "abstract": "Class imbalance remains a large problem in high-throughput omics analyses, causing bias towards the over-represented class when training machine learning- based classifiers. Oversampling is a common method used to balance classes \u2026"}, {"title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2404.05567", "details": "B Pan, Y Shen, H Liu, M Mishra, G Zhang, A Oliva\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) language models can reduce computational costs by 2- 4$\\times $ compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios. However, MoE models generally \u2026"}, {"title": "An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis", "link": "https://arxiv.org/pdf/2403.12402", "details": "Y Peng, I Kulikov, Y Yang, S Popuri, H Lu, C Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Speech language models (LMs) are promising for high-quality speech synthesis through in-context learning. A typical speech LM takes discrete semantic units as content and a short utterance as prompt, and synthesizes speech which preserves \u2026"}, {"title": "Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra", "link": "https://arxiv.org/pdf/2404.03647", "details": "D Kevian, U Syed, X Guo, A Havens, G Dullerud\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we explore the capabilities of state-of-the-art large language models (LLMs) such as GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra in solving undergraduate-level control problems. Controls provides an interesting case study \u2026"}, {"title": "InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions", "link": "https://arxiv.org/pdf/2403.11435", "details": "Y Wang, Y Liu, C Shi, H Li, C Chen, H Lu, Y Yang - arXiv preprint arXiv:2403.11435, 2024", "abstract": "Instruction tuning effectively optimizes Large Language Models (LLMs) for downstream tasks. Due to the changing environment in real-life applications, LLMs necessitate continual task-specific adaptation without catastrophic forgetting \u2026"}, {"title": "BioMedLM: A 2.7 B Parameter Language Model Trained On Biomedical Text", "link": "https://arxiv.org/pdf/2403.18421", "details": "E Bolton, A Venigalla, M Yasunaga, D Hall, B Xiong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance on a wide variety of biomedical NLP tasks. However, these models have hundreds of billions of parameters, are computationally expensive to run \u2026"}, {"title": "DinoBloom: A Foundation Model for Generalizable Cell Embeddings in Hematology", "link": "https://arxiv.org/pdf/2404.05022", "details": "V Koch, SJ Wagner, S Kazeminia, E Sancar, M Hehr\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In hematology, computational models offer significant potential to improve diagnostic accuracy, streamline workflows, and reduce the tedious work of analyzing single cells in peripheral blood or bone marrow smears. However, clinical adoption of \u2026"}, {"title": "DEE: Dual-stage Explainable Evaluation Method for Text Generation", "link": "https://arxiv.org/pdf/2403.11509", "details": "S Zhang, Y Li, R Wu, X Huang, Y Chen, W Xu, G Qi - arXiv preprint arXiv:2403.11509, 2024", "abstract": "Automatic methods for evaluating machine-generated texts hold significant importance due to the expanding applications of generative systems. Conventional methods tend to grapple with a lack of explainability, issuing a solitary numerical \u2026"}]
