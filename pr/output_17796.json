[{"title": "A multimodal visual\u2013language foundation model for computational ophthalmology", "link": "https://www.nature.com/articles/s41746-025-01772-2", "details": "D Shi, W Zhang, J Yang, S Huang, X Chen, P Xu, K Jin\u2026 - npj Digital Medicine, 2025", "abstract": "\u2026 Visual **Question** **Answering**. For visual **question** **answering** , we used the image encoder from EyeCLIP to extract image features, which were then concatenated with text features ( **questions** ). The combined feature was fed into the language model \u2026"}, {"title": "THE RISE OF MULTIMODAL AI: A QUICK REVIEW OF GPT-4V AND GEMINI", "link": "https://sesjournal.com/index.php/1/article/download/506/452", "details": "J Ahmed, G Nadeem, MK Majeed, R Ghaffar, AKK Baig\u2026 - Spectrum of Engineering \u2026, 2025", "abstract": "\u2026 Emphasis is placed upon their performance towards visual **question** **answering** , multimodal dialogue, instruction following, and other tasks \u2026 The invention of **large** **language** **models** (LLMs) was accompanied by transformational models like \u2026"}, {"title": "LMT++: Adaptively Collaborating LLMs with Multi-specialized Teachers for Continual VQA in Robotic Surgical Videos", "link": "https://ieeexplore.ieee.org/abstract/document/11045720/", "details": "Y Du, K Chen, Y Zhan, CH Low, M Islam, Z Guo, Y Jin\u2026 - \u2026 Transactions on **Medical** \u2026, 2025", "abstract": "\u2026 Abstract: Visual **question** **answering** (VQA) plays a vital role in advancing surgical education. However, due to the privacy concern of \u2026 sources, and ii) the data imbalance **problem** caused by the unequal occurrence of **medical** instruments or \u2026"}, {"title": "Limitations of Scientific Articles and Navigated Future Directions with LLM and RAG", "link": "https://search.proquest.com/openview/9fa08cf90c8a42aeb232e7ad729c7ee1/1%3Fpq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy", "details": "I Al Azher - 2025", "abstract": "\u2026 In this work, we focus on generating image descriptions based on some **questions** from charts and graphs using multi-modal **Large** **Language** **Models** (LLMs) such as QWen, Llama, Llava, Pali-GEMMA, and GPT4o. Using an LLM-as-a-judge \u2026"}, {"title": "Leveraging Attention Mechanism to Unlock Gene and Protein Attributes", "link": "https://search.proquest.com/openview/c8d984d26312ba0cb11ff225ca0be300/1%3Fpq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy", "details": "A Jararweh - 2025", "abstract": "\u2026 DISCUSSION In conclusion, DeepVul represents a tool for expanding the **clinical** \u2026 A notable advancement in foundation models is the development of **Large** **Language** **Models** (\u2026 is an audio-and-text model that extends BERT for spoken \u2026"}, {"title": "Embedded AI and Sensing for Wellness, Health, and Sustainability: Intelligent, Privacy-Aware Wearable Devices and AIoT Systems", "link": "https://search.proquest.com/openview/a5525888d8fb428a2c710df65395a514/1%3Fpq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy", "details": "J Nie - 2025", "abstract": "\u2026 CaiTI includes several novel technical contribu- tions: (i) a reinforcement learning framework that adapts conversation flow based on user history and therapist-defined priorities; (ii) integration of **large** **language** **models** (LLMs) with therapist- guided \u2026"}]
