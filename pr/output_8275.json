[{"title": "Neural Fields in Robotics: A Survey", "link": "https://arxiv.org/pdf/2410.20220", "details": "MZ Irshad, M Comi, YC Lin, N Heppert, A Valada\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Neural Fields have emerged as a transformative approach for 3D scene representation in computer vision and robotics, enabling accurate inference of geometry, 3D semantics, and dynamics from posed 2D data. Leveraging \u2026"}, {"title": "Masked Contrastive Representation Learning for Self-Supervised Visual Pre-Training", "link": "https://ieeexplore.ieee.org/abstract/document/10722789/", "details": "Y Yao, N Desai, M Palaniswami - 2024 IEEE 11th International Conference on Data \u2026, 2024", "abstract": "Self-supervised learning has achieved state-of-the-art performance in various tasks and applications. In computer vision, self-supervised learning often employs contrastive learning and masked image modeling, each with its limitations \u2026"}, {"title": "BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks", "link": "https://arxiv.org/pdf/2410.20971", "details": "Y Zhao, X Zheng, L Luo, Y Li, X Ma, YG Jiang - arXiv preprint arXiv:2410.20971, 2024", "abstract": "Despite their superb multimodal capabilities, Vision-Language Models (VLMs) have been shown to be vulnerable to jailbreak attacks, which are inference-time attacks that induce the model to output harmful responses with tricky prompts. It is thus \u2026"}, {"title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines", "link": "https://arxiv.org/pdf/2410.21220", "details": "Z Zhang, Y Zhang, X Ding, X Yue - arXiv preprint arXiv:2410.21220, 2024", "abstract": "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This \u2026"}, {"title": "Rephrasing natural text data with different languages and quality levels for Large Language Model pre-training", "link": "https://arxiv.org/pdf/2410.20796", "details": "M Pieler, M Bellagente, H Teufel, D Phung, N Cooper\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently published work on rephrasing natural text data for pre-training LLMs has shown promising results when combining the original dataset with the synthetically rephrased data. We build upon previous work by replicating existing results on C4 \u2026"}, {"title": "Imaging foundation model for universal enhancement of non-ideal measurement CT", "link": "https://arxiv.org/pdf/2410.01591%3F", "details": "Y Liu, R Ge, Y He, Z Wu, C You, S Li, Y Chen - arXiv preprint arXiv:2410.01591, 2024", "abstract": "Non-ideal measurement computed tomography (NICT), which sacrifices optimal imaging standards for new advantages in CT imaging, is expanding the clinical application scope of CT images. However, with the reduction of imaging standards \u2026"}, {"title": "BIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models", "link": "https://arxiv.org/pdf/2410.09079", "details": "A Chang, J Wang, H Liu, P Bhatia, C Xiao, T Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Parameter Efficient Fine-Tuning (PEFT) offers an efficient solution for fine-tuning large pretrained language models for downstream tasks. However, most PEFT strategies are manually designed, often resulting in suboptimal performance. Recent \u2026"}, {"title": "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents", "link": "https://arxiv.org/pdf/2410.21155", "details": "Q Zhang, Z Chen, H Pan, C Caragea, LJ Latecki\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scientific information extraction (SciIE) is critical for converting unstructured knowledge from scholarly articles into structured data (entities and relations). Several datasets have been proposed for training and validating SciIE models. However, due \u2026"}, {"title": "Fine-Tuning In-House Large Language Models to Infer Differential Diagnosis from Radiology Reports", "link": "https://arxiv.org/pdf/2410.09234", "details": "L Chen, R Teotia, A Verdone, A Cardall, L Tyagi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Radiology reports summarize key findings and differential diagnoses derived from medical imaging examinations. The extraction of differential diagnoses is crucial for downstream tasks, including patient management and treatment planning. However \u2026"}]
