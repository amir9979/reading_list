[{"title": "Does your data spark joy? Performance gains from domain upsampling at the end of training", "link": "https://arxiv.org/pdf/2406.03476", "details": "C Blakeney, M Paul, BW Larsen, S Owen, J Frankle - arXiv preprint arXiv:2406.03476, 2024", "abstract": "Pretraining datasets for large language models (LLMs) have grown to trillions of tokens composed of large amounts of CommonCrawl (CC) web scrape along with smaller, domain-specific datasets. It is expensive to understand the impact of these \u2026"}, {"title": "Probing Language Models for Pre-training Data Detection", "link": "https://arxiv.org/pdf/2406.01333", "details": "Z Liu, T Zhu, C Tan, H Lu, B Liu, W Chen - arXiv preprint arXiv:2406.01333, 2024", "abstract": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to \u2026"}, {"title": "Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration", "link": "https://openreview.net/pdf%3Fid%3DDLTjFFiuUJ", "details": "Z Yu, Z Wang, Y Fu, H Shi, K Shaikh, YC Lin - Forty-first International Conference on Machine \u2026", "abstract": "Attention is a fundamental component behind the remarkable achievements of large language models (LLMs). However, our current understanding of the attention mechanism, especially regarding how attention distributions are established \u2026"}, {"title": "An Independence-promoting Loss for Music Generation with Language Models", "link": "https://arxiv.org/pdf/2406.02315", "details": "JM Lemercier, S Rouard, J Copet, Y Adi, A D\u00e9ffosez - arXiv preprint arXiv:2406.02315, 2024", "abstract": "Music generation schemes using language modeling rely on a vocabulary of audio tokens, generally provided as codes in a discrete latent space learnt by an auto- encoder. Multi-stage quantizers are often employed to produce these tokens \u2026"}, {"title": "BLSP-Emo: Towards Empathetic Large Speech-Language Models", "link": "https://arxiv.org/pdf/2406.03872", "details": "C Wang, M Liao, Z Huang, J Wu, C Zong, J Zhang - arXiv preprint arXiv:2406.03872, 2024", "abstract": "The recent release of GPT-4o showcased the potential of end-to-end multimodal models, not just in terms of low latency but also in their ability to understand and generate expressive speech with rich emotions. While the details are unknown to the \u2026"}, {"title": "Position: Data Authenticity, Consent, & Provenance for AI are all broken: what will it take to fix them?", "link": "https://openreview.net/pdf%3Fid%3D3hSTecKy1b", "details": "S Longpre, R Mahari, N Obeng-Marnu, W Brannon\u2026", "abstract": "New capabilities in foundation models are owed in large part to massive, widely- sourced, and under-documented training data collections. Existing practices in data collection have led to challenges in tracing authenticity, verifying consent, preserving \u2026"}, {"title": "Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models", "link": "https://arxiv.org/pdf/2406.02061", "details": "M Nezhurina, L Cipolina-Kun, M Cherti, J Jitsev - arXiv preprint arXiv:2406.02061, 2024", "abstract": "Large Language Models (LLMs) are often described as being instances of foundation models-that is, models that transfer strongly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that \u2026"}, {"title": "Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large Language Models", "link": "https://arxiv.org/pdf/2406.03009", "details": "SL Wei, CK Wu, HH Huang, HH Chen - arXiv preprint arXiv:2406.03009, 2024", "abstract": "In this paper, we investigate the phenomena of\" selection biases\" in Large Language Models (LLMs), focusing on problems where models are tasked with choosing the optimal option from an ordered sequence. We delve into biases related to option \u2026"}, {"title": "Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models", "link": "https://openreview.net/pdf%3Fid%3D1tRLxQzdep", "details": "P Dong, L Li, Z Tang, X Liu, X Pan, Q Wang, X Chu - Forty-first International \u2026, 2024", "abstract": "Despite the remarkable capabilities, Large Language Models (LLMs) face deployment challenges due to their extensive size. Pruning methods drop a subset of weights to accelerate, but many of them require retraining, which is prohibitively \u2026"}]
