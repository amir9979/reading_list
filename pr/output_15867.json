[{"title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability", "link": "https://arxiv.org/pdf/2504.07416%3F", "details": "J Park, S Kim, B Yoon, K Choi - arXiv preprint arXiv:2504.07416, 2025", "abstract": "Recent advancements in multi-modal models have significantly improved vision- language alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning, rely on low-resolution \u2026"}, {"title": "Promoting transparency in AI for biomedical and behavioral research", "link": "https://www.nature.com/articles/s41591-025-03680-0", "details": "T Hernandez-Boussard, AY Lee, J Stoyanovich\u2026 - Nature Medicine, 2025", "abstract": "Recent advancements in artificial intelligence (AI) in healthcare have highlighted the need for transparency, including explainability, interpretability, and accountability across the AI lifecycle 1, 2. Transparency ensures stakeholders can make informed \u2026"}, {"title": "Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for Large Language Models", "link": "https://arxiv.org/pdf/2504.02273%3F", "details": "H Le, D Do, D Nguyen, S Venkatesh - arXiv preprint arXiv:2504.02273, 2025", "abstract": "Recent advances in fine-tuning large language models (LLMs) with reinforcement learning (RL) have shown promising improvements in complex reasoning tasks, particularly when paired with chain-of-thought (CoT) prompting. However, these \u2026"}, {"title": "ProtoECGNet: Case-Based Interpretable Deep Learning for Multi-Label ECG Classification with Contrastive Learning", "link": "https://arxiv.org/pdf/2504.08713", "details": "S Sethi, D Chen, T Statchen, MC Burkhart, N Bhandari\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Deep learning-based electrocardiogram (ECG) classification has shown impressive performance but clinical adoption has been slowed by the lack of transparent and faithful explanations. Post hoc methods such as saliency maps may fail to reflect a \u2026"}, {"title": "How to Detect and Defeat Molecular Mirage: A Metric-Driven Benchmark for Hallucination in LLM-based Molecular Comprehension", "link": "https://arxiv.org/pdf/2504.12314%3F", "details": "H Li, L Lv, H Cao, Z Liu, Z Yan, Y Wang, Y Tian, Y Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models are increasingly used in scientific domains, especially for molecular understanding and analysis. However, existing models are affected by hallucination issues, resulting in errors in drug design and utilization. In this paper \u2026"}, {"title": "Fast-Slow-Thinking: Complex Task Solving with Large Language Models", "link": "https://arxiv.org/pdf/2504.08690%3F", "details": "Y Sun, Y Zhang, Z Zhao, S Wan, D Tao, C Gong - arXiv preprint arXiv:2504.08690, 2025", "abstract": "Nowadays, Large Language Models (LLMs) have been gradually employed to solve complex tasks. To face the challenge, task decomposition has become an effective way, which proposes to divide a complex task into multiple simpler subtasks and \u2026"}, {"title": "Can large language models independently complete tasks? A dynamic evaluation framework for multi-turn task planning and completion", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225008070", "details": "J Gao, J Cui, H Wu, L Xiang, H Zhao, X Li, M Fang\u2026 - Neurocomputing, 2025", "abstract": "Large language models (LLMs) are increasingly relied upon for multi-turn dialogue to conduct complex tasks. However, existing benchmarks mainly evaluate LLMs as agents, overlooking their potential as independent systems to accomplish complex \u2026"}, {"title": "Cognitive memory in large language models", "link": "https://arxiv.org/pdf/2504.02441", "details": "L Shan, S Luo, Z Zhu, Y Yuan, Y Wu - arXiv preprint arXiv:2504.02441, 2025", "abstract": "This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long \u2026"}, {"title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs", "link": "https://arxiv.org/pdf/2504.07866%3F", "details": "Y Yin, W Huang, K Song, Y Tang, X Wu, W Guo, P Guo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing \u2026"}]
