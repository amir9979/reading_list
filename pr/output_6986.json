[{"title": "DARES: Depth Anything in Robotic Endoscopic Surgery with Self-supervised Vector-LoRA of the Foundation Model", "link": "https://arxiv.org/pdf/2408.17433", "details": "MS Zeinoddin, C Lena, J Qu, L Carlini, M Magro, S Kim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Robotic-assisted surgery (RAS) relies on accurate depth estimation for 3D reconstruction and visualization. While foundation models like Depth Anything Models (DAM) show promise, directly applying them to surgery often yields \u2026"}, {"title": "GroCo: Ground Constraint for Metric Self-Supervised Monocular Depth", "link": "https://arxiv.org/pdf/2409.14850", "details": "A Cecille, S Duffner, F Davoine, T Neveu, R Agier - arXiv preprint arXiv:2409.14850, 2024", "abstract": "Monocular depth estimation has greatly improved in the recent years but models predicting metric depth still struggle to generalize across diverse camera poses and datasets. While recent supervised methods mitigate this issue by leveraging ground \u2026"}, {"title": "Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs", "link": "https://arxiv.org/pdf/2409.14988", "details": "C Christophe, T Raha, S Maslenkova, MU Salman\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated significant potential in transforming clinical applications. In this study, we investigate the efficacy of four techniques in adapting LLMs for clinical use-cases: continuous pretraining, instruct \u2026"}, {"title": "SDSL: Spectral Distance Scaling Loss Pretraining SwinUNETR for 3D Medical Image Segmentation", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10654210.pdf", "details": "J Lee, DT Vu, GH Yu, JS Kim, KY Kim, JY Kim - IEEE Access, 2024", "abstract": "Recent approaches utilizing self-supervised learning with masked image modeling (MIM) have demonstrated great performance. However, applying MIM naively to small datasets results in poor generalization to downstream tasks. We hypothesize \u2026"}, {"title": "Cell comparative learning: A cervical cytopathology whole slide image classification method using normal and abnormal cells", "link": "https://www.sciencedirect.com/science/article/pii/S0895611124001046", "details": "J Qin, Y He, Y Liang, L Kang, J Zhao, B Ding - Computerized Medical Imaging and \u2026, 2024", "abstract": "Automated cervical cancer screening through computer-assisted diagnosis has shown considerable potential to improve screening accessibility and reduce associated costs and errors. However, classification performance on whole slide \u2026"}, {"title": "Audio xLSTMs: Learning Self-supervised audio representations with xLSTMs", "link": "https://arxiv.org/pdf/2408.16568", "details": "S Yadav, S Theodoridis, ZH Tan - arXiv preprint arXiv:2408.16568, 2024", "abstract": "While the transformer has emerged as the eminent neural architecture, several independent lines of research have emerged to address its limitations. Recurrent neural approaches have also observed a lot of renewed interest, including the \u2026"}]
