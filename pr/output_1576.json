'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [CAML: A Context-Aware Metric Learning approach for im'
[{"title": "Exploring and Mitigating Shortcut Learning for Generative Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.602.pdf", "details": "Z Sun, Y Xiao, J Li, Y Ji, W Chen, M Zhang - Proceedings of the 2024 Joint \u2026, 2024", "abstract": "Recent generative large language models (LLMs) have exhibited incredible instruction-following capabilities while keeping strong task completion ability, even without task-specific fine-tuning. Some works attribute this to the bonus of the new \u2026"}, {"title": "SoftMCL: Soft Momentum Contrastive Learning for Fine-grained Sentiment-aware Pre-training", "link": "https://arxiv.org/pdf/2405.01827", "details": "J Wang, LC Yu, X Zhang - arXiv preprint arXiv:2405.01827, 2024", "abstract": "The pre-training for language models captures general language understanding but fails to distinguish the affective impact of a particular context to a specific word. Recent works have sought to introduce contrastive learning (CL) for sentiment-aware \u2026"}, {"title": "PDAMeta: Meta-Learning Framework with Progressive Data Augmentation for Few-Shot Text Classification", "link": "https://aclanthology.org/2024.lrec-main.1109.pdf", "details": "X Li, K Song, T Lin, Y Kang, F Zhao, C Sun, X Liu - Proceedings of the 2024 Joint \u2026, 2024", "abstract": "Recently, we have witnessed the breakthroughs of meta-learning for few-shot learning scenario. Data augmentation is essential for meta-learning, particularly in situations where data is extremely scarce. However, existing text data augmentation \u2026"}, {"title": "Analyzing Chain-of-thought Prompting in Black-Box Large Language Models via Estimated V-information", "link": "https://aclanthology.org/2024.lrec-main.81.pdf", "details": "Z Wang, C Li, Z Yang, Q Liu, Y Hao, X Chen, D Chu\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract Chain-of-Thought (CoT) prompting combined with large language models (LLM) has shown great potential in improving performance on challenging reasoning tasks. While understanding why CoT prompting is effective is crucial for the \u2026"}, {"title": "Invariant Risk Minimization Is A Total Variation Model", "link": "https://arxiv.org/pdf/2405.01389", "details": "ZR Lai, WW Wang - arXiv preprint arXiv:2405.01389, 2024", "abstract": "Invariant risk minimization (IRM) is an arising approach to generalize invariant features to different environments in machine learning. While most related works focus on new IRM settings or new application scenarios, the mathematical essence \u2026"}]
