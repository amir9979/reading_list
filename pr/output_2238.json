[{"title": "NeuroGauss4D-PCI: 4D Neural Fields and Gaussian Deformation Fields for Point Cloud Interpolation", "link": "https://arxiv.org/pdf/2405.14241", "details": "C Jiang, D Du, J Liu, S Zhu, Z Liu, Z Ma, Z Liang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Point Cloud Interpolation confronts challenges from point sparsity, complex spatiotemporal dynamics, and the difficulty of deriving complete 3D point clouds from sparse temporal information. This paper presents NeuroGauss4D-PCI, which excels \u2026"}, {"title": "Unified Editing of Panorama, 3D Scenes, and Videos Through Disentangled Self-Attention Injection", "link": "https://arxiv.org/pdf/2405.16823", "details": "G Kwon, J Park, JC Ye - arXiv preprint arXiv:2405.16823, 2024", "abstract": "While text-to-image models have achieved impressive capabilities in image generation and editing, their application across various modalities often necessitates training separate models. Inspired by existing method of single image editing with \u2026"}, {"title": "Score-based generative models are provably robust: an uncertainty quantification perspective", "link": "https://arxiv.org/pdf/2405.15754", "details": "N Mimikos-Stamatopoulos, BJ Zhang, MA Katsoulakis - arXiv preprint arXiv \u2026, 2024", "abstract": "Through an uncertainty quantification (UQ) perspective, we show that score-based generative models (SGMs) are provably robust to the multiple sources of error in practical implementation. Our primary tool is the Wasserstein uncertainty propagation \u2026"}, {"title": "BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models", "link": "https://arxiv.org/pdf/2405.04756", "details": "CF Luo, A Ghawanmeh, X Zhu, FK Khattak - arXiv preprint arXiv:2405.04756, 2024", "abstract": "Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly. The language model can also \u2026"}, {"title": "Reducing the cost of posterior sampling in linear inverse problems via task-dependent score learning", "link": "https://arxiv.org/pdf/2405.15643", "details": "F Schneider, DL Duong, M Lassas, MV de Hoop\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Score-based diffusion models (SDMs) offer a flexible approach to sample from the posterior distribution in a variety of Bayesian inverse problems. In the literature, the prior score is utilized to sample from the posterior by different methods that require \u2026"}, {"title": "Fast Samplers for Inverse Problems in Iterative Refinement Models", "link": "https://arxiv.org/pdf/2405.17673", "details": "K Pandey, R Yang, S Mandt - arXiv preprint arXiv:2405.17673, 2024", "abstract": "Constructing fast samplers for unconditional diffusion and flow-matching models has received much attention recently; however, existing methods for solving inverse problems, such as super-resolution, inpainting, or deblurring, still require hundreds \u2026"}, {"title": "Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding", "link": "https://arxiv.org/pdf/2405.19763", "details": "K Liao, S Li, M Zhao, L Liu, M Xue, Z Hu, H Han, C Yin - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent strides in large language models (LLMs) have yielded remarkable performance, leveraging reinforcement learning from human feedback (RLHF) to significantly enhance generation and alignment capabilities. However, RLHF \u2026"}, {"title": "Multilingual Diversity Improves Vision-Language Representations", "link": "https://arxiv.org/pdf/2405.16915", "details": "T Nguyen, M Wallingford, S Santy, WC Ma, S Oh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Massive web-crawled image-text datasets lay the foundation for recent progress in multimodal learning. These datasets are designed with the goal of training a model to do well on standard computer vision benchmarks, many of which, however, have \u2026"}, {"title": "Effective Integration of Text Diffusion and Pre-Trained Language Models with Linguistic Easy-First Schedule", "link": "https://aclanthology.org/2024.lrec-main.493.pdf", "details": "Y Ou, P Jian - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Diffusion models have become a powerful generative modeling paradigm, achieving great success in continuous data patterns. However, the discrete nature of text data results in compatibility issues between continuous diffusion models (CDMs) and pre \u2026"}]
