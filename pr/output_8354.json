[{"title": "BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays", "link": "https://arxiv.org/pdf/2410.21969", "details": "Y Zhou, TLH Faith, Y Xu, S Leng, X Xu, Y Liu, RSM Goh - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream \u2026"}, {"title": "Natural Language Inference Improves Compositionality in Vision-Language Models", "link": "https://arxiv.org/pdf/2410.22315", "details": "P Cascante-Bonilla, Y Hou, YT Cao, H Daum\u00e9 III\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Compositional reasoning in Vision-Language Models (VLMs) remains challenging as these models often struggle to relate objects, attributes, and spatial relationships. Recent methods aim to address these limitations by relying on the semantics of the \u2026"}, {"title": "Large Language Model Benchmarks in Medical Tasks", "link": "https://arxiv.org/pdf/2410.21348", "details": "LKQ Yan, M Li, Y Zhang, CH Yin, C Fei, B Peng, Z Bi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the increasing application of large language models (LLMs) in the medical domain, evaluating these models' performance using benchmark datasets has become crucial. This paper presents a comprehensive survey of various benchmark \u2026"}, {"title": "Zero-shot Object Navigation with Vision-Language Models Reasoning", "link": "https://arxiv.org/pdf/2410.18570", "details": "C Wen, Y Huang, H Huang, Y Huang, S Yuan, Y Hao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Object navigation is crucial for robots, but traditional methods require substantial training data and cannot be generalized to unknown environments. Zero-shot object navigation (ZSON) aims to address this challenge, allowing robots to interact with \u2026"}, {"title": "Energy-Based Diffusion Language Models for Text Generation", "link": "https://arxiv.org/pdf/2410.21357", "details": "M Xu, T Geffner, K Kreis, W Nie, Y Xu, J Leskovec\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite remarkable progress in autoregressive language models, alternative generative paradigms beyond left-to-right generation are still being actively explored. Discrete diffusion models, with the capacity for parallel generation, have recently \u2026"}, {"title": "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?", "link": "https://arxiv.org/pdf/2410.10476", "details": "G Roccabruna, M Rizzoli, G Riccardi - arXiv preprint arXiv:2410.10476, 2024", "abstract": "The automatic detection of temporal relations among events has been mainly investigated with encoder-only models such as RoBERTa. Large Language Models (LLM) have recently shown promising performance in temporal reasoning tasks such \u2026"}, {"title": "Multifaceted Natural Language Processing Task\u2013Based Evaluation of Bidirectional Encoder Representations From Transformers Models for Bilingual (Korean and \u2026", "link": "https://medinform.jmir.org/2024/1/e52897/", "details": "K Kim, S Park, J Min, S Park, JY Kim, J Eun, K Jung\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background: The bidirectional encoder representations from transformers (BERT) model has attracted considerable attention in clinical applications, such as patient classification and disease prediction. However, current studies have typically \u2026"}, {"title": "Advancing Large Language Model Attribution through Self-Improving", "link": "https://arxiv.org/pdf/2410.13298", "details": "L Huang, X Feng, W Ma, L Zhao, Y Fan, W Zhong, D Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Teaching large language models (LLMs) to generate text with citations to evidence sources can mitigate hallucinations and enhance verifiability in information-seeking systems. However, improving this capability requires high-quality attribution data \u2026"}, {"title": "SPRIG: Improving Large Language Model Performance by System Prompt Optimization", "link": "https://arxiv.org/pdf/2410.14826", "details": "L Zhang, T Ergen, L Logeswaran, M Lee, D Jurgens - arXiv preprint arXiv:2410.14826, 2024", "abstract": "Large Language Models (LLMs) have shown impressive capabilities in many scenarios, but their performance depends, in part, on the choice of prompt. Past research has focused on optimizing prompts specific to a task. However, much less \u2026"}]
