[{"title": "EHRCon: Dataset for Checking Consistency between Unstructured Notes and Structured Tables in Electronic Health Records", "link": "https://arxiv.org/pdf/2406.16341", "details": "Y Kwon, J Kim, G Lee, S Bae, D Kyung, W Cha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Electronic Health Records (EHRs) are integral for storing comprehensive patient medical records, combining structured data (eg, medications) with detailed clinical notes (eg, physician notes). These elements are essential for straightforward data \u2026"}, {"title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions", "link": "https://arxiv.org/pdf/2405.19744", "details": "C Li, W Yang, J Zhang, J Lu, S Wang, C Zong - arXiv preprint arXiv:2405.19744, 2024", "abstract": "Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples \u2026"}, {"title": "DEM: Distribution Edited Model for Training with Mixed Data Distributions", "link": "https://arxiv.org/pdf/2406.15570", "details": "D Ram, A Rawal, M Hardalov, N Pappas, S Zha - arXiv preprint arXiv:2406.15570, 2024", "abstract": "Training with mixed data distributions is a common and important part of creating multi-task and instruction-following models. The diversity of the data distributions and cost of joint training makes the optimization procedure extremely challenging. Data \u2026"}, {"title": "A Scalable and Extensible Logical Data Model of Electronic Health Record Audit Logs for Temporal Data Mining (RNteract): Model Conceptualization and Formulation", "link": "https://nursing.jmir.org/2024/1/e55793", "details": "VL Tiase, KA Sward, JC Facelli - JMIR nursing, 2024", "abstract": "Background Increased workload, including workload related to electronic health record (EHR) documentation, is reported as a main contributor to nurse burnout and adversely affects patient safety and nurse satisfaction. Traditional methods for \u2026"}, {"title": "Exploring the full potential of the electronic health record: the application of natural language processing for clinical practice", "link": "https://academic.oup.com/eurjcn/advance-article/doi/10.1093/eurjcn/zvae091/7697888", "details": "L Van Bulck, M Reading Turchioe, M Topaz, J Song - European Journal of \u2026, 2024", "abstract": "The electronic health record contains valuable patient data and offers opportunities to administer and analyze patients' individual needs longitudinally. However, most information in the electronic health record is currently stored in unstructured text \u2026"}, {"title": "An Empirical Study of Mamba-based Language Models", "link": "https://arxiv.org/pdf/2406.07887", "details": "R Waleffe, W Byeon, D Riach, B Norick, V Korthikanti\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value \u2026"}, {"title": "Amend to Alignment: Decoupled Prompt Tuning for Mitigating Spurious Correlation in Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3Df8G2KSCSdp", "details": "J Zhang, X Ma, S Guo, P Li, W Xu, X Tang, Z Hong - Forty-first International Conference on \u2026", "abstract": "Fine-tuning the learnable prompt for a pre-trained vision-language model (VLM), such as CLIP, has demonstrated exceptional efficiency in adapting to a broad range of downstream tasks. Existing prompt tuning methods for VLMs do not distinguish \u2026"}, {"title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback", "link": "https://openreview.net/pdf%3Fid%3DBOorDpKHiJ", "details": "G Cui, L Yuan, N Ding, G Yao, B He, W Zhu, Y Ni, G Xie\u2026 - Forty-first International \u2026, 2024", "abstract": "Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability \u2026"}, {"title": "PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models", "link": "https://aclanthology.org/2024.naacl-long.336.pdf", "details": "HJ Kim, YJ Kim, JY Bak - Proceedings of the 2024 Conference of the North \u2026, 2024", "abstract": "Pre-trained language models (PLMs) show impressive performance in various downstream NLP tasks. However, pre-training large language models demands substantial memory and training compute. Furthermore, due to the substantial \u2026"}]
