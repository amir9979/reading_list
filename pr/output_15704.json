[{"title": "Never Start from Scratch: Expediting On-Device LLM Personalization via Explainable Model Selection", "link": "https://arxiv.org/pdf/2504.13938", "details": "H Wang, B Yang, X Yin, W Gao - arXiv preprint arXiv:2504.13938, 2025", "abstract": "Personalization of Large Language Models (LLMs) is important in practical applications to accommodate the individual needs of different mobile users. Due to data privacy concerns, LLM personalization often needs to be locally done at the \u2026"}, {"title": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection", "link": "https://arxiv.org/pdf/2504.00695", "details": "X Zhu, Z Gu, S Zheng, T Wang, T Li, H Feng, Y Xiao - arXiv preprint arXiv:2504.00695, 2025", "abstract": "Pre-training large language models (LLMs) necessitates enormous diverse textual corpora, making effective data selection a key challenge for balancing computational resources and model performance. Current methodologies primarily emphasize data \u2026"}, {"title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2504.00869%3F", "details": "X Huang, J Wu, H Liu, X Tang, Y Zhou - arXiv preprint arXiv:2504.00869, 2025", "abstract": "Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from \u2026"}, {"title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning", "link": "https://arxiv.org/pdf/2504.00406", "details": "J Han, W Buntine, E Shareghi - arXiv preprint arXiv:2504.00406, 2025", "abstract": "Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources \u2026"}, {"title": "Large language models are human-like annotators", "link": "https://link.springer.com/chapter/10.1007/978-3-031-88720-8_45", "details": "M Marreddy, SR Oota, M Gupta - European Conference on Information Retrieval, 2025", "abstract": "Large Language Models Are Human-Like Annotators | SpringerLink Skip to main content Advertisement Springer Nature Link Account Menu Find a journal Publish with us Track your research Search Cart 1.Home 2.Advances in Information \u2026"}, {"title": "From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models", "link": "https://arxiv.org/pdf/2504.06214", "details": "C Xu, W Ping, P Xu, Z Liu, B Wang, M Shoeybi, B Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Long-context capabilities are essential for a wide range of applications, including document and video understanding, in-context learning, and inference-time scaling, all of which require models to process and reason over long sequences of text and \u2026"}, {"title": "Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection", "link": "https://arxiv.org/pdf/2504.09440", "details": "MS Liu, S Bo, J Fang - arXiv preprint arXiv:2504.09440, 2025", "abstract": "Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical \u2026"}, {"title": "LLM $\\times $ MapReduce-V2: Entropy-Driven Convolutional Test-Time Scaling for Generating Long-Form Articles from Extremely Long Resources", "link": "https://arxiv.org/pdf/2504.05732", "details": "H Wang, Y Fu, Z Zhang, S Wang, Z Ren, X Wang, Z Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Long-form generation is crucial for a wide range of practical applications, typically categorized into short-to-long and long-to-long generation. While short-to-long generations have received considerable attention, generating long texts from \u2026"}]
