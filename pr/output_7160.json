[{"title": "Larger and more instructable language models become less reliable", "link": "https://www.nature.com/articles/s41586-024-07930-y", "details": "L Zhou, W Schellaert, F Mart\u00ednez-Plumed\u2026 - Nature, 2024", "abstract": "The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources) and bespoke shaping up (including post \u2026"}, {"title": "Gauging, enriching and applying geography knowledge in Pre-trained Language Models", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324002516", "details": "N Ramrakhiyani, V Varma, GK Palshikar, S Pawar - Information Processing & \u2026, 2025", "abstract": "Abstract To employ Pre-trained Language Models (PLMs) as knowledge containers in niche domains it is important to gauge the knowledge of these PLMs about facts in these domains. It is also an important pre-requisite to know how much enrichment \u2026"}, {"title": "Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models", "link": "https://arxiv.org/pdf/2409.12512", "details": "J Rao, X Liu, Z Lin, L Ding, J Li, D Tao - arXiv preprint arXiv:2409.12512, 2024", "abstract": "Knowledge distillation (KD) is a technique that compresses large teacher models by training smaller student models to mimic them. The success of KD in auto-regressive language models mainly relies on Reverse KL for mode-seeking and student \u2026"}, {"title": "Towards a Unified View of Preference Learning for Large Language Models: A Survey", "link": "https://arxiv.org/pdf/2409.02795", "details": "B Gao, F Song, Y Miao, Z Cai, Z Yang, L Chen, H Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of the crucial factors to achieve success is aligning the LLM's output with human preferences. This alignment process often requires only a small amount of data to \u2026"}, {"title": "CEval: A Benchmark for Evaluating Counterfactual Text Generation", "link": "https://aclanthology.org/2024.inlg-main.6.pdf", "details": "C Seifert, J Schl\u00f6tterer - Proceedings of the 17th International Natural \u2026, 2024", "abstract": "Counterfactual text generation aims to minimally change a text, such that it is classified differently. Assessing progress in method development for counterfactual text generation is hindered by a non-uniform usage of data sets and metrics in \u2026"}, {"title": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification", "link": "https://arxiv.org/pdf/2409.16718", "details": "M Li, J Zhong, C Li, L Li, N Lin, M Sugiyama - arXiv preprint arXiv:2409.16718, 2024", "abstract": "Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed the success of prompt tuning and adapter tuning, while the classic model fine-tuning on inherent parameters seems to be overlooked. It is believed that fine-tuning the \u2026"}, {"title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization", "link": "https://dl.acm.org/doi/pdf/10.1145/3670474.3685966", "details": "P Vijayaraghavan, A Nitsure, C Mackin, L Shi\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Large Language Models (LLMs) have become widely used across diverse NLP tasks and domains, demonstrating their adaptability and effectiveness. In the realm of Electronic Design Automation (EDA), LLMs show promise for tasks like Register \u2026"}, {"title": "Evaluating the Performance of Large Language Models in Competitive Programming: A Multi-Year, Multi-Grade Analysis", "link": "https://arxiv.org/pdf/2409.09054", "details": "AM Dumitran, AC Badea, SG Muscalu - \u2026 on INnovations in Intelligent SysTems and \u2026, 2024", "abstract": "This study explores the performance of large language models (LLMs) in solving competitive programming problems from the Romanian Informatics Olympiad at the county level. Romania, a leading nation in computer science competitions, provides \u2026"}]
