[{"title": "Capturing Nuanced Preferences: Preference-Aligned Distillation for Small Language Models", "link": "https://arxiv.org/pdf/2502.14272", "details": "Y Gu, J Li, S Huang, X Zou, Z Li, X Hu - arXiv preprint arXiv:2502.14272, 2025", "abstract": "Aligning small language models (SLMs) with human values typically involves distilling preference knowledge from large language models (LLMs). However, existing distillation methods model preference knowledge in teacher LLMs by \u2026"}, {"title": "Harnessing the Foundation Model for Exploration of Single-cell Expression Atlases in Plants", "link": "https://academic.oup.com/gpb/advance-article-pdf/doi/10.1093/gpbjnl/qzaf024/62429563/qzaf024.pdf", "details": "G Cao, H Chao, W Zheng, Y Lan, K Lu, Y Wang\u2026 - Genomics, Proteomics & \u2026, 2025", "abstract": "Single-cell RNA sequencing (scRNA-seq) provides unprecedented insights into plant cellular diversity by enabling high-resolution analyses of gene expression at the single-cell level. However, the complexity of scRNA-seq data, including \u2026"}, {"title": "Stackelberg Game Preference Optimization for Data-Efficient Alignment of Language Models", "link": "https://arxiv.org/pdf/2502.18099", "details": "X Chu, Z Zhang, T Jia, Y Jin - arXiv preprint arXiv:2502.18099, 2025", "abstract": "Aligning language models with human preferences is critical for real-world deployment, but existing methods often require large amounts of high-quality human annotations. Aiming at a data-efficient alignment method, we propose Stackelberg \u2026"}, {"title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?", "link": "https://arxiv.org/pdf/2503.02199", "details": "A Deng, T Cao, Z Chen, B Hooi - arXiv preprint arXiv:2503.02199, 2025", "abstract": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual \u2026"}, {"title": "MMSciBench: Benchmarking Language Models on Multimodal Scientific Problems", "link": "https://arxiv.org/pdf/2503.01891", "details": "X Ye, C Li, S Chen, X Tang, W Wei - arXiv preprint arXiv:2503.01891, 2025", "abstract": "Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present \u2026"}, {"title": "Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages", "link": "https://arxiv.org/pdf/2503.11609", "details": "M Farina, M Mancini, G Iacca, E Ricci - arXiv preprint arXiv:2503.11609, 2025", "abstract": "An old-school recipe for training a classifier is to (i) learn a good feature extractor and (ii) optimize a linear layer atop. When only a handful of samples are available per category, as in Few-Shot Adaptation (FSA), data are insufficient to fit a large number \u2026"}, {"title": "Palette of Language Models: A Solver for Controlled Text Generation", "link": "https://arxiv.org/pdf/2503.11182", "details": "Z Yang, Y Huang, Y Chen, X Wu, J Feng, C Deng - arXiv preprint arXiv:2503.11182, 2025", "abstract": "Recent advancements in large language models have revolutionized text generation with their remarkable capabilities. These models can produce controlled texts that closely adhere to specific requirements when prompted appropriately. However \u2026"}, {"title": "scNET: learning context-specific gene and cell embeddings by integrating single-cell gene expression data with protein\u2013protein interactions", "link": "https://www.nature.com/articles/s41592-025-02627-0", "details": "R Sheinin, R Sharan, A Madi - Nature Methods, 2025", "abstract": "Recent advances in single-cell RNA sequencing (scRNA-seq) techniques have provided unprecedented insights into the heterogeneity of various tissues. However, gene expression data alone often fails to capture and identify changes in cellular \u2026"}, {"title": "Detecting LLM Fact-conflicting Hallucinations Enhanced by Temporal-logic-based Reasoning", "link": "https://arxiv.org/pdf/2502.13416", "details": "N Li, Y Song, K Wang, Y Li, L Shi, Y Liu, H Wang - arXiv preprint arXiv:2502.13416, 2025", "abstract": "Large language models (LLMs) face the challenge of hallucinations--outputs that seem coherent but are actually incorrect. A particularly damaging type is fact- conflicting hallucination (FCH), where generated content contradicts established \u2026"}]
