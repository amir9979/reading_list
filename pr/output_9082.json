[{"title": "CriteriaMapper: establishing the automatic identification of clinical trial cohorts from electronic health records by matching normalized eligibility criteria and patient \u2026", "link": "https://www.nature.com/articles/s41598-024-77447-x", "details": "K Lee, Y Mai, Z Liu, K Raja, T Jun, M Ma, T Wang, L Ai\u2026 - Scientific Reports, 2024", "abstract": "The use of electronic health records (EHRs) holds the potential to enhance clinical trial activities. However, the identification of eligible patients within EHRs presents considerable challenges. We aimed to develop a CriteriaMapper system for \u2026"}, {"title": "Racial differences in laboratory testing as a potential mechanism for bias in AI: A matched cohort analysis in emergency department visits", "link": "https://journals.plos.org/globalpublichealth/article%3Fid%3D10.1371/journal.pgph.0003555", "details": "T Chang, M Nuppnau, Y He, KE Kocher, TS Valley\u2026 - PLOS Global Public Health, 2024", "abstract": "AI models are often trained using available laboratory test results. Racial differences in laboratory testing may bias AI models for clinical decision support, amplifying existing inequities. This study aims to measure the extent of racial differences in \u2026"}, {"title": "Information Extraction from Clinical Notes: Are We Ready to Switch to Large Language Models?", "link": "https://arxiv.org/pdf/2411.10020", "details": "Y Hu, X Zuo, Y Zhou, X Peng, J Huang, VK Keloth\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Backgrounds: Information extraction (IE) is critical in clinical natural language processing (NLP). While large language models (LLMs) excel on generative tasks, their performance on extractive tasks remains debated. Methods: We investigated \u2026"}, {"title": "Exploring the Benefits of Domain-Pretraining of Generative Large Language Models for Chemistry", "link": "https://arxiv.org/pdf/2411.03542", "details": "A Acharya, S Sharma, R Cosbey, M Subramanian\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "A proliferation of Large Language Models (the GPT series, BLOOM, LLaMA, and more) are driving forward novel development of multipurpose AI for a variety of tasks, particularly natural language processing (NLP) tasks. These models demonstrate \u2026"}, {"title": "Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language Models", "link": "https://openreview.net/pdf%3Fid%3Dnvn80cscVm", "details": "L Wei, Z Tan, C Li, J Wang, W Huang - The Thirty-eighth Annual Conference on \u2026, 2024", "abstract": "Large Language Models (LLMs) have transformed natural language processing and extended their powerful capabilities to multi-modal domains. As LLMs continue to advance, it is crucial to develop diverse and appropriate metrics for their evaluation \u2026"}, {"title": "Comparing Commercial and Open-Source Large Language Models for Labeling Chest Radiograph Reports", "link": "https://pubs.rsna.org/doi/abs/10.1148/radiol.241139", "details": "FJ Dorfner, L J\u00fcrgensen, L Donle, F Al Mohamad\u2026 - Radiology, 2024", "abstract": "Background Rapid advances in large language models (LLMs) have led to the development of numerous commercial and open-source models. While recent publications have explored OpenAI's GPT-4 to extract information of interest from \u2026"}, {"title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models", "link": "https://arxiv.org/pdf/2410.18252", "details": "M Noukhovitch, S Huang, S Xhonneux, A Hosseini\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this \u2026"}, {"title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step", "link": "https://arxiv.org/pdf/2411.10440", "details": "G Xu, P Jin, L Hao, Y Song, L Sun, L Yuan - arXiv preprint arXiv:2411.10440, 2024", "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to \u2026"}, {"title": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning", "link": "https://arxiv.org/pdf/2411.05193", "details": "J Hong, A Dragan, S Levine - arXiv preprint arXiv:2411.05193, 2024", "abstract": "Value-based reinforcement learning (RL) can in principle learn effective policies for a wide range of multi-turn problems, from games to dialogue to robotic control, including via offline RL from static previously collected datasets. However, despite \u2026"}]
