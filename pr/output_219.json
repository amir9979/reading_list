'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Foresightâ€”a generative pretrained transformer for mod'
[{"title": "An Integrated Data Processing Framework for Pretraining Foundation Models", "link": "https://arxiv.org/pdf/2402.16358", "details": "Y Sun, F Wang, Y Zhu, WX Zhao, J Mao - arXiv preprint arXiv:2402.16358, 2024", "abstract": "The ability of the foundation models heavily relies on large-scale, diverse, and high- quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and \u2026"}, {"title": "Do Not Worry if You Do Not Have Data: Building Pretrained Language Models Using Translationese", "link": "https://arxiv.org/pdf/2403.13638", "details": "M Doshi, R Dabre, P Bhattacharyya - arXiv preprint arXiv:2403.13638, 2024", "abstract": "In this paper, we explore the utility of\\textit {Translationese} as synthetic data created using machine translation for pre-training language models (LMs). Pre-training requires vast amounts of monolingual data, which is mostly unavailable for \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models", "link": "https://arxiv.org/pdf/2403.02436", "details": "X Lu, Y Zhao, B Qin - arXiv preprint arXiv:2403.02436, 2024", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models", "link": "https://arxiv.org/html/2402.18154v1", "details": "Z Jin, P Cao, H Yuan, Y Chen, J Xu, H Li, X Jiang, K Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, retrieval augmentation and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context. However, internal memory and external \u2026"}, {"title": "Ethos: Rectifying Language Models in Orthogonal Parameter Space", "link": "https://arxiv.org/pdf/2403.08994", "details": "L Gao, Y Niu, T Tang, S Avestimehr, M Annavaram - arXiv preprint arXiv:2403.08994, 2024", "abstract": "Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training \u2026"}, {"title": "Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting", "link": "https://arxiv.org/pdf/2403.13369", "details": "P Richter-Pechanski, P Wiesenbach, DM Schwab\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent \u2026"}, {"title": "Q-Probe: A Lightweight Approach to Reward Maximization for Language Models", "link": "https://arxiv.org/html/2402.14688v1", "details": "K Li, S Jelassi, H Zhang, S Kakade, M Wattenberg\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present an approach called Q-probing to adapt a pre-trained language model to maximize a task-specific reward function. At a high level, Q-probing sits between heavier approaches such as finetuning and lighter approaches such as few shot \u2026"}, {"title": "Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods", "link": "https://arxiv.org/pdf/2403.00998", "details": "P Tsvilodub, H Wang, S Grosch, M Franke - arXiv preprint arXiv:2403.00998, 2024", "abstract": "This paper systematically compares different methods of deriving item-level predictions of language models for multiple-choice tasks. It compares scoring methods for answer options based on free generation of responses, various \u2026"}, {"title": "Merino: Entropy-driven Design for Generative Language Models on IoT Devices", "link": "https://arxiv.org/html/2403.07921v1", "details": "Y Zhao, M Lin, H Tang, Q Wu, J Wang - arXiv preprint arXiv:2403.07921, 2024", "abstract": "Generative Large Language Models (LLMs) stand as a revolutionary advancement in the modern era of artificial intelligence (AI). However, directly deploying LLMs in resource-constrained hardware, such as Internet-of-Things (IoT) devices, is difficult \u2026"}]
