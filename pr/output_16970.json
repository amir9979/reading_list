[{"title": "Employ Artificial Intelligence to Advance **Diabetes** Cardiometabolic Care", "link": "https://journals.sagepub.com/doi/abs/10.1177/2633559X251321792", "details": "M Peeples, M Saunders, LA Scher, L Drago, J Macleod - ADCES in Practice", "abstract": "\u2026 In the rest of this article, we focus on the patient and provider experience and health equity in the use of AI in **diabetes** cardiometabolic \u2026 DCES and Use of **Large** **Language** **Models** Another example of the use of Al by DCESs is **large** **language** \u2026"}, {"title": "G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning", "link": "https://arxiv.org/pdf/2505.18499", "details": "X Guo, A Li, Y Wang, S Jegelka, Y Wang - arXiv preprint arXiv:2505.18499, 2025", "abstract": "\u2026 Chain-of-thought prompting elicits reasoning in **large** **language** **models**. Advances in neural information processing systems, 35:24824\u201324837, 2022. \u2026 Your answer should be chosen from: Type 1 **diabetes** Type 2 **diabetes** Experimentally induced **diabetes** \u2026", "entry_id": "http://arxiv.org/abs/2505.18499v1", "updated": "2025-05-24 04:33:41", "published": "2025-05-24 04:33:41", "authors": "Xiaojun Guo;Ang Li;Yifei Wang;Stefanie Jegelka;Yisen Wang", "summary": "Although Large Language Models (LLMs) have demonstrated remarkable progress,\ntheir proficiency in graph-related tasks remains notably limited, hindering the\ndevelopment of truly general-purpose models. Previous attempts, including\npretraining graph foundation models or employing supervised fine-tuning, often\nface challenges such as the scarcity of large-scale, universally represented\ngraph data. We introduce G1, a simple yet effective approach demonstrating that\nReinforcement Learning (RL) on synthetic graph-theoretic tasks can\nsignificantly scale LLMs' graph reasoning abilities. To enable RL training, we\ncurate Erd\\~os, the largest graph reasoning dataset to date comprising 50\ndiverse graph-theoretic tasks of varying difficulty levels, 100k training data\nand 5k test data, all drived from real-world graphs. With RL on Erd\\~os, G1\nobtains substantial improvements in graph reasoning, where our finetuned 3B\nmodel even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also\nshow strong zero-shot generalization to unseen tasks, domains, and graph\nencoding schemes, including other graph-theoretic benchmarks as well as\nreal-world node classification and link prediction tasks, without compromising\ngeneral reasoning abilities. Our findings offer an efficient, scalable path for\nbuilding strong graph reasoners by finetuning LLMs with RL on graph-theoretic\ntasks, which combines the strengths of pretrained LLM capabilities with\nabundant, automatically generated synthetic data, suggesting that LLMs possess\ngraph understanding abilities that RL can elicit successfully.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;stat.ML", "links": "http://arxiv.org/abs/2505.18499v1;http://arxiv.org/pdf/2505.18499v1", "pdf_url": "http://arxiv.org/pdf/2505.18499v1"}, {"title": "PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus", "link": "https://arxiv.org/pdf/2505.20323", "details": "S Noroozizadeh, S Kumar, GH Chen, JC Weiss - arXiv preprint arXiv:2505.20323, 2025", "abstract": "\u2026 We created a dataset of 100 reports across five diagnoses\u2014 **Diabetes** , Sepsis, Hypertension, COVID-19, and Atrial Fibrillation\u2014with 10 \u2026 of 124,699 temporally annotated clinical case reports generated using **large** **language** **models** (LLMs). Our \u2026", "entry_id": "http://arxiv.org/abs/2505.20323v1", "updated": "2025-05-23 18:01:09", "published": "2025-05-23 18:01:09", "authors": "Shahriar Noroozizadeh;Sayantan Kumar;George H. Chen;Jeremy C. Weiss", "summary": "Understanding temporal dynamics in clinical narratives is essential for\nmodeling patient trajectories, yet large-scale temporally annotated resources\nremain limited. We present PMOA-TTS, the first openly available dataset of\n124,699 PubMed Open Access (PMOA) case reports, each converted into structured\n(event, time) timelines via a scalable LLM-based pipeline. Our approach\ncombines heuristic filtering with Llama 3.3 to identify single-patient case\nreports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1,\nresulting in over 5.6 million timestamped clinical events. To assess timeline\nquality, we evaluate against a clinician-curated reference set using three\nmetrics: (i) event-level matching (80% match at a cosine similarity threshold\nof 0.1), (ii) temporal concordance (c-index > 0.90), and (iii) Area Under the\nLog-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide\ndiagnostic and demographic coverage. In a downstream survival prediction task,\nembeddings from extracted timelines achieve time-dependent concordance indices\nup to 0.82 $\\pm$ 0.01, demonstrating the predictive value of temporally\nstructured narratives. PMOA-TTS provides a scalable foundation for timeline\nextraction, temporal reasoning, and longitudinal modeling in biomedical NLP.\nThe dataset is available at: https://huggingface.co/datasets/snoroozi/pmoa-tts .", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.20323v1;http://arxiv.org/pdf/2505.20323v1", "pdf_url": "http://arxiv.org/pdf/2505.20323v1"}, {"title": "Understanding AI in Cybersecurity and Secure AI: Challenges, Strategies and Trends", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DvC1gEQAAQBAJ%26oi%3Dfnd%26pg%3DPR5%26dq%3Ddiabetes%2B%2B%2522large%2Blanguage%2Bmodels%2522%26ots%3DY1r71XwkYU%26sig%3Db5MnTNNPo_vlIBNBRbf3p8BOUW0", "details": "DP Sharma", "abstract": "\u2026 be the underground development of malicious **Large** **Language** **Models** (LLMs) and the \u2026 an output indicating the probability that the individual has **Diabetes**. There are two main types of \u2026 various symptoms into two categories of **diabetes** or \u2026"}]
