[{"title": "Scalable Multi-Domain Adaptation of Language Models using Modular Experts", "link": "https://arxiv.org/pdf/2410.10181", "details": "P Schafhalter, S Liao, Y Zhou, CK Yeh, A Kandoor\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Domain-specific adaptation is critical to maximizing the performance of pre-trained language models (PLMs) on one or multiple targeted tasks, especially under resource-constrained use cases, such as edge devices. However, existing methods \u2026"}, {"title": "Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models", "link": "https://arxiv.org/pdf/2410.09047%3F", "details": "Q Liu, C Shang, L Liu, N Pappas, J Ma, NA John\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be degraded by the integration of the vision module compared to its LLM backbone. We investigate this phenomenon, dubbed as''safety alignment degradation''in this paper \u2026"}, {"title": "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "link": "https://arxiv.org/pdf/2410.05077", "details": "FM Molfese, S Conia, R Orlando, R Navigli - arXiv preprint arXiv:2410.05077, 2024", "abstract": "Current Large Language Models (LLMs) have shown strong reasoning capabilities in commonsense question answering benchmarks, but the process underlying their success remains largely opaque. As a consequence, recent approaches have \u2026"}, {"title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ", "link": "https://arxiv.org/pdf/2410.09613", "details": "A Poulis, E Tsalapati, M Koubarakis - arXiv preprint arXiv:2410.09613, 2024", "abstract": "Recent advancements in transformer-based language models have sparked research into their logical reasoning capabilities. Most of the benchmarks used to evaluate these models are simple: generated from short (fragments of) first-order \u2026"}, {"title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models", "link": "https://arxiv.org/pdf/2410.06154", "details": "MJ Mirza, M Zhao, Z Mao, S Doveh, W Lin, P Gavrikov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task \u2026"}, {"title": "DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2410.11988", "details": "S Gao, CH Lin, T Hua, T Zheng, Y Shen, H Jin, YC Hsu - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have achieved remarkable success in various natural language processing tasks, including language modeling, understanding, and generation. However, the increased memory and computational costs \u2026"}, {"title": "CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning", "link": "https://arxiv.org/pdf/2410.11963", "details": "Q Cao, M Najibi, S Mehta - arXiv preprint arXiv:2410.11963, 2024", "abstract": "Pretraining robust vision or multimodal foundation models (eg, CLIP) relies on large- scale datasets that may be noisy, potentially misaligned, and have long-tail distributions. Previous works have shown promising results in augmenting datasets \u2026"}, {"title": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models", "link": "https://arxiv.org/pdf/2410.06741", "details": "Z Gong, H Yu, C Liao, B Liu, C Chen, J Li - arXiv preprint arXiv:2410.06741, 2024", "abstract": "Multi-task learning (MTL) benefits the fine-tuning of large language models (LLMs) by providing a single model with improved performance and generalization ability across tasks, presenting a resource-efficient alternative to developing separate \u2026"}, {"title": "MMAD: The First-Ever Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection", "link": "https://arxiv.org/pdf/2410.09453", "details": "X Jiang, J Li, H Deng, Y Liu, BB Gao, Y Zhou, J Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the field of industrial inspection, Multimodal Large Language Models (MLLMs) have a high potential to renew the paradigms in practical applications due to their robust language capabilities and generalization abilities. However, despite their \u2026"}]
