[{"title": "Aligning Language Models Using Follow-up Likelihood as Reward Signal", "link": "https://arxiv.org/pdf/2409.13948", "details": "C Zhang, D Chong, F Jiang, C Tang, A Gao, G Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In natural human-to-human conversations, participants often receive feedback signals from one another based on their follow-up reactions. These reactions can include verbal responses, facial expressions, changes in emotional state, and other \u2026"}, {"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "link": "https://arxiv.org/pdf/2410.05639", "details": "R Zhao, ZL Thai, Y Zhang, S Hu, Y Ba, J Zhou, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the \u2026"}, {"title": "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL", "link": "https://arxiv.org/pdf/2410.11371", "details": "Q Zhong, K Chen, L Ding, J Liu, B Du, D Tao - arXiv preprint arXiv:2410.11371, 2024", "abstract": "Large Language Models (LLMs) have shown promising performance in text-to-SQL, which involves translating natural language questions into SQL queries. However, current text-to-SQL LLMs are computationally expensive and challenging to deploy \u2026"}, {"title": "FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs", "link": "https://arxiv.org/pdf/2409.13612", "details": "B Yan, Z Zhang, L Jing, E Hossain, X Du - arXiv preprint arXiv:2409.13612, 2024", "abstract": "The rapid development of Large Vision-Language Models (LVLMs) often comes with widespread hallucination issues, making cost-effective and comprehensive assessments increasingly vital. Current approaches mainly rely on costly annotations \u2026"}, {"title": "Attention Prompting on Image for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.17143", "details": "R Yu, W Yu, X Wang - arXiv preprint arXiv:2409.17143, 2024", "abstract": "Compared with Large Language Models (LLMs), Large Vision-Language Models (LVLMs) can also accept images as input, thus showcasing more interesting emergent capabilities and demonstrating impressive performance on various vision \u2026"}, {"title": "Mixture of Experts Made Personalized: Federated Prompt Learning for Vision-Language Models", "link": "https://arxiv.org/pdf/2410.10114", "details": "J Luo, C Chen, S Wu - arXiv preprint arXiv:2410.10114, 2024", "abstract": "Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has demonstrated potent applicability across diverse downstream tasks. This lightweight approach has quickly gained traction from federated learning (FL) researchers who \u2026"}, {"title": "Do Vision-Language Models Really Understand Visual Language?", "link": "https://arxiv.org/pdf/2410.00193", "details": "B Giledereli, Y Hou, Y Tu, M Sachan - arXiv preprint arXiv:2410.00193, 2024", "abstract": "Visual language is a system of communication that conveys information through symbols, shapes, and spatial arrangements. Diagrams are a typical example of a visual language depicting complex concepts and their relationships in the form of an \u2026"}, {"title": "Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models", "link": "https://arxiv.org/pdf/2409.12512", "details": "J Rao, X Liu, Z Lin, L Ding, J Li, D Tao - arXiv preprint arXiv:2409.12512, 2024", "abstract": "Knowledge distillation (KD) is a technique that compresses large teacher models by training smaller student models to mimic them. The success of KD in auto-regressive language models mainly relies on Reverse KL for mode-seeking and student \u2026"}, {"title": "Neural-Symbolic Collaborative Distillation: Advancing Small Language Models for Complex Reasoning Tasks", "link": "https://arxiv.org/pdf/2409.13203", "details": "H Liao, S He, Y Xu, Y Zhang, K Liu, J Zhao - arXiv preprint arXiv:2409.13203, 2024", "abstract": "In this paper, we propose $\\textbf {Ne} $ ural-$\\textbf {Sy} $ mbolic $\\textbf {C} $ ollaborative $\\textbf {D} $ istillation ($\\textbf {NesyCD} $), a novel knowledge distillation method for learning the complex reasoning abilities of Large Language \u2026"}]
