[{"title": "Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs", "link": "https://arxiv.org/pdf/2407.00653", "details": "Y Zhang, X Wang, J Liang, S Xia, L Chen, Y Xiao - arXiv preprint arXiv:2407.00653, 2024", "abstract": "Large Language Models (LLMs) have exhibited impressive proficiency in various natural language processing (NLP) tasks, which involve increasingly complex reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving new \u2026"}, {"title": "Simple and Effective Masked Diffusion Language Models", "link": "https://arxiv.org/pdf/2406.07524", "details": "SS Sahoo, M Arriola, Y Schiff, A Gokaslan, E Marroquin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling. In this work, we show that simple masked discrete diffusion is \u2026"}, {"title": "Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models", "link": "https://arxiv.org/pdf/2406.09403", "details": "Y Hu, W Shi, X Fu, D Roth, M Ostendorf, L Zettlemoyer\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Humans draw to facilitate reasoning: we draw auxiliary lines when solving geometry problems; we mark and circle when reasoning on maps; we use sketches to amplify our ideas and relieve our limited-capacity working memory. However, such actions \u2026"}, {"title": "A large-scale dataset for korean document-level relation extraction from encyclopedia texts", "link": "https://link.springer.com/article/10.1007/s10489-024-05605-9", "details": "S Son, J Lim, S Koo, J Kim, Y Kim, Y Lim, D Hyun\u2026 - Applied Intelligence, 2024", "abstract": "Document-level relation extraction (RE) aims to predict the relational facts between two given entities from a document. Unlike widespread research on document-level RE in English, Korean document-level RE research is still at the very beginning due \u2026"}, {"title": "Finding Task-specific Subnetworks in Multi-task Spoken Language Understanding Model", "link": "https://arxiv.org/pdf/2406.12317", "details": "H Futami, S Arora, Y Kashiwagi, E Tsunoo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, multi-task spoken language understanding (SLU) models have emerged, designed to address various speech processing tasks. However, these models often rely on a large number of parameters. Also, they often encounter difficulties in \u2026"}, {"title": "Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions", "link": "https://arxiv.org/pdf/2406.05688", "details": "C Tan, D Lyu, S Li, Z Gao, J Wei, S Ma, Z Liu, SZ Li - arXiv preprint arXiv:2406.05688, 2024", "abstract": "Large Language Models (LLMs) have demonstrated wide-ranging applications across various fields and have shown significant potential in the academic peer- review process. However, existing applications are primarily limited to static review \u2026"}, {"title": "CGFTrans: Cross-Modal Global Feature Fusion Transformer for Medical Report Generation", "link": "https://ieeexplore.ieee.org/abstract/document/10557585/", "details": "L Xu, Q Tang, B Zheng, J Lv, W Li, X Zeng - IEEE Journal of Biomedical and Health \u2026, 2024", "abstract": "Medical report generation, as a cross-modal automatic text generation task, can be highly significant both in research and clinical fields. The core is to generate diagnosis reports in clinical language from medical images. However, several \u2026"}, {"title": "Self-Hint Prompting Improves Zero-shot Reasoning in Large Language Models via Reflective Cycle", "link": "https://escholarship.org/content/qt5ht3f0dt/qt5ht3f0dt_noSplash_508be8c9920e4bd796bec268a73a6b1a.pdf", "details": "J Chen, J Tian, Y Jin - Proceedings of the Annual Meeting of the Cognitive \u2026, 2024", "abstract": "Chain-of-Thought (CoT) has brought a fresh perspective to improve the reasoning ability of large language models (LLMs). To relieve the burden of manual design in CoT, Zero-shot CoT has pioneered a direct interaction with LLMs. Based on it \u2026"}, {"title": "Exploring Adversarial Robustness of Deep State Space Models", "link": "https://arxiv.org/pdf/2406.05532", "details": "B Qi, Y Luo, J Gao, P Li, K Tian, Z Ma, B Zhou - arXiv preprint arXiv:2406.05532, 2024", "abstract": "Deep State Space Models (SSMs) have proven effective in numerous task scenarios but face significant security challenges due to Adversarial Perturbations (APs) in real- world deployments. Adversarial Training (AT) is a mainstream approach to \u2026"}]
