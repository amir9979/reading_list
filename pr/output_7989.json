[{"title": "Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization", "link": "https://arxiv.org/pdf/2410.09302", "details": "G Liu, K Ji, R Zheng, Z Wu, C Dun, Q Gu, L Yan - arXiv preprint arXiv:2410.09302, 2024", "abstract": "Reinforcement Learning (RL) plays a crucial role in aligning large language models (LLMs) with human preferences and improving their ability to perform complex tasks. However, current approaches either require significant computational resources due \u2026"}, {"title": "LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks", "link": "https://arxiv.org/pdf/2410.13025", "details": "A Prabhakar, Y Li, K Narasimhan, S Kakade, E Malach\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Low-Rank Adaptation (LoRA) is a popular technique for parameter-efficient fine- tuning of Large Language Models (LLMs). We study how different LoRA modules can be merged to achieve skill composition--testing the performance of the merged \u2026"}, {"title": "SeRA: Self-Reviewing and Alignment of Large Language Models using Implicit Reward Margins", "link": "https://arxiv.org/pdf/2410.09362", "details": "J Ko, S Dingliwal, B Ganesh, S Sengupta, S Bodapati\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Direct alignment algorithms (DAAs), such as direct preference optimization (DPO), have become popular alternatives for Reinforcement Learning from Human Feedback (RLHF) due to their simplicity, efficiency, and stability. However, the \u2026"}, {"title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "link": "https://arxiv.org/pdf/2410.11829%3F", "details": "Y Cao, Y Liu, Z Chen, G Shi, W Wang, D Zhao, T Lu - arXiv preprint arXiv:2410.11829, 2024", "abstract": "Despite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating \u2026"}, {"title": "A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement", "link": "https://arxiv.org/pdf/2410.13828", "details": "H Yuan, Y Zeng, Y Wu, H Wang, M Wang, L Leqi - arXiv preprint arXiv:2410.13828, 2024", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for language model (LM) alignment. At its core, RLHF uses a margin-based loss for preference optimization, specifying ideal LM behavior only by \u2026"}, {"title": "Simultaneous Reward Distillation and Preference Learning: Get You a Language Model Who Can Do Both", "link": "https://arxiv.org/pdf/2410.08458", "details": "A Nath, C Jung, E Seefried, N Krishnaswamy - arXiv preprint arXiv:2410.08458, 2024", "abstract": "Reward modeling of human preferences is one of the cornerstones of building usable generative large language models (LLMs). While traditional RLHF-based alignment methods explicitly maximize the expected rewards from a separate reward \u2026"}, {"title": "Beyond Exact Match: Semantically Reassessing Event Extraction by Large Language Models", "link": "https://arxiv.org/pdf/2410.09418", "details": "YF Lu, XL Mao, T Lan, C Xu, H Huang - arXiv preprint arXiv:2410.09418, 2024", "abstract": "Event extraction has gained extensive research attention due to its broad range of applications. However, the current mainstream evaluation method for event extraction relies on token-level exact match, which misjudges numerous semantic \u2026"}, {"title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining", "link": "https://arxiv.org/pdf/2410.08102", "details": "T Bai, L Yang, ZH Wong, J Peng, X Zhuang, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these \u2026"}, {"title": "Measuring Human-AI Value Alignment in Large Language Models", "link": "https://ojs.aaai.org/index.php/AIES/article/download/31703/33870", "details": "H Norhashim, J Hahn - Proceedings of the AAAI/ACM Conference on AI, Ethics \u2026, 2024", "abstract": "This paper seeks to quantify the human-AI value alignment in large language models. Alignment between humans and AI has become a critical area of research to mitigate potential harm posed by AI. In tandem with this need, developers have \u2026"}]
