[{"title": "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks", "link": "https://openreview.net/pdf%3Fid%3Dtr0TcqitMH", "details": "S Xu, L Pang, H Shen, X Cheng, TS Chua - The Web Conference 2024, 2024", "abstract": "Making the contents generated by Large Language Model (LLM) such as ChatGPT, accurate, credible and traceable is crucial, especially in complex knowledge- intensive tasks that require multi-step reasoning and each step needs knowledge to \u2026"}, {"title": "IAD: In-Context Learning Ability Decoupler of Large Language Models in Meta-Training", "link": "https://aclanthology.org/2024.lrec-main.749.pdf", "details": "Y Liu, X Chen, G Xing, J Zhang, R Yan - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) exhibit remarkable In-Context Learning (ICL) ability, where the model learns tasks from prompts consisting of input-output examples. However, the pre-training objectives of LLMs often misalign with ICL \u2026"}, {"title": "BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models", "link": "https://arxiv.org/pdf/2405.04756", "details": "CF Luo, A Ghawanmeh, X Zhu, FK Khattak - arXiv preprint arXiv:2405.04756, 2024", "abstract": "Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly. The language model can also \u2026"}, {"title": "IT5: Text-to-text Pretraining for Italian Language Understanding and Generation", "link": "https://aclanthology.org/2024.lrec-main.823.pdf", "details": "G Sarti, M Nissim - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "We introduce IT5, the first family of encoder-decoder transformer models pretrained specifically on Italian. We document and perform a thorough cleaning procedure for a large Italian corpus and use it to pretrain four IT5 model sizes. We then introduce \u2026"}]
