[{"title": "ToCoAD: Two-Stage Contrastive Learning for Industrial Anomaly Detection", "link": "https://arxiv.org/pdf/2407.01312", "details": "Y Liang, Z Hu, J Huang, D Di, A Su, L Fan - arXiv preprint arXiv:2407.01312, 2024", "abstract": "Current unsupervised anomaly detection approaches perform well on public datasets but struggle with specific anomaly types due to the domain gap between pre- trained feature extractors and target-specific domains. To tackle this issue, this paper \u2026"}, {"title": "SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations", "link": "https://arxiv.org/pdf/2406.11171", "details": "SH Dumpala, A Jaiswal, C Sastry, E Milios, S Oore\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their remarkable successes, state-of-the-art large language models (LLMs), including vision-and-language models (VLMs) and unimodal language models (ULMs), fail to understand precise semantics. For example, semantically equivalent \u2026"}, {"title": "Mixing Natural and Synthetic Images for Robust Self-Supervised Representations", "link": "https://arxiv.org/pdf/2406.12368", "details": "RA Bafghi, N Harilal, C Monteleoni, M Raissi - arXiv preprint arXiv:2406.12368, 2024", "abstract": "This paper introduces DiffMix, a new self-supervised learning (SSL) pre-training framework that combines real and synthetic images. Unlike traditional SSL methods that predominantly use real images, DiffMix uses a variant of Stable Diffusion to \u2026"}]
