[{"title": "Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality", "link": "https://arxiv.org/pdf/2411.11531", "details": "V Chekalina, A Razzigaev, E Goncharova, A Kuznetsov - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper we present an approach to reduce hallucinations in Large Language Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional modality. Our method involves transforming input text into a set of KG embeddings and using \u2026"}, {"title": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2411.12580", "details": "L Ruis, M Mozes, J Bae, SR Kamalakara, D Talupuru\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The capabilities and limitations of Large Language Models have been sketched out in great detail in recent years, providing an intriguing yet conflicting picture. On the one hand, LLMs demonstrate a general ability to solve problems. On the other hand \u2026"}, {"title": "Training and Evaluating Language Models with Template-based Data Generation", "link": "https://arxiv.org/pdf/2411.18104", "details": "Y Zhang - arXiv preprint arXiv:2411.18104, 2024", "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, these \u2026"}]
