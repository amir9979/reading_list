[{"title": "Exploring the impact of text and tables in document **question answering** : a multimodal analysis with **large language models**", "link": "https://bibliographie.tu-ilmenau.de/servlets/DozBibEntryServlet%3Fmode%3Dshow%26id%3Dilm_mods_00044824", "details": "M Ali - 2025", "abstract": "\u2026 By leveraging **large** **language** **models** (LLMs), this work aims to enhance the extraction and synthesis of information from biomedical research articles, **answering** competency **questions** (CQs) with greater accuracy and depth. This research \u2026"}, {"title": "Assessing the System-Instruction Vulnerabilities of **Large Language Models** to Malicious Conversion Into Health Disinformation Chatbots", "link": "https://www.acpjournals.org/doi/abs/10.7326/ANNALS-24-03933", "details": "ND Modi, BD Menz, AA Awaty, CA Alex, JM Logan\u2026 - Annals of Internal **Medicine** , 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) offer substantial promise for improving **health** **care** ; however, some risks warrant evaluation and discussion. This study assessed the effectiveness of safeguards in foundational LLMs against malicious instruction into \u2026"}, {"title": "Effective **Medical** Visual **Question Answering** Using Dynamic Prompting and Decoding Knowledge Editing", "link": "https://link.springer.com/article/10.1007/s41019-025-00291-0", "details": "ZJ Zhou, YF Huang, XL Wang, X Hong - Data Science and Engineering, 2025", "abstract": "\u2026 capability of **large** **language** **models** , some studies have explored training or fine-tuning these models with **medical** corpora for **medical** VQA tasks[28]. \u2026 Taking the **medical** visual **question** **answering** task in this paper as an example, given a **medical** image \\\\(\\textbf \u2026"}, {"title": "Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives", "link": "https://arxiv.org/pdf/2506.18116", "details": "B Haider, A Gorti, A Chadha, M Gaur - arXiv preprint arXiv:2506.18116, 2025", "abstract": "\u2026 This research examines bias in **Large** **Language** **Models** within mental health contexts using \u2026 : Socio-demographic biases in **medical** decision-making by **large** **language** **models** : A large-\u2026 diagnosis and treatment: a qualitative comparison of \u2026", "entry_id": "http://arxiv.org/abs/2506.18116v1", "updated": "2025-06-22 18:00:16", "published": "2025-06-22 18:00:16", "authors": "Batool Haider;Atmika Gorti;Aman Chadha;Manas Gaur", "summary": "Large Language Models (LLMs) in mental healthcare risk propagating biases\nthat reinforce stigma and harm marginalized groups. While previous research\nidentified concerning trends, systematic methods for detecting intersectional\nbiases remain limited. This work introduces a multi-hop question answering\n(MHQA) framework to explore LLM response biases in mental health discourse. We\nanalyze content from the Interpretable Mental Health Instruction (IMHI) dataset\nacross symptom presentation, coping mechanisms, and treatment approaches. Using\nsystematic tagging across age, race, gender, and socioeconomic status, we\ninvestigate bias patterns at demographic intersections. We evaluate four LLMs:\nClaude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic\ndisparities across sentiment, demographics, and mental health conditions. Our\nMHQA approach demonstrates superior detection compared to conventional methods,\nidentifying amplification points where biases magnify through sequential\nreasoning. We implement two debiasing techniques: Roleplay Simulation and\nExplicit Bias Reduction, achieving 66-94% bias reductions through few-shot\nprompting with BBQ dataset examples. These findings highlight critical areas\nwhere LLMs reproduce mental healthcare biases, providing actionable insights\nfor equitable AI development.", "comment": "19 Pages, 7 Figures, 4 Tables (Note: Under Review)", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.CY", "links": "http://arxiv.org/abs/2506.18116v1;http://arxiv.org/pdf/2506.18116v1", "pdf_url": "http://arxiv.org/pdf/2506.18116v1"}, {"title": "Taming Vision-Language Models for Medical Image Analysis: A Comprehensive Review", "link": "https://arxiv.org/pdf/2506.18378", "details": "H Lin, C Xu, J Qin - arXiv preprint arXiv:2506.18378, 2025", "abstract": "\u2026 In genetic biomarker prediction [90], **large** **language** **models** (LLMs) are employed to generate **medical** \u2026 of **Medical** Visual **Question** **Answer** ing (VQA) is to develop AI systems capable of interpreting **medical** images and **answering** clinically relevant \u2026", "entry_id": "http://arxiv.org/abs/2506.18378v1", "updated": "2025-06-23 08:11:24", "published": "2025-06-23 08:11:24", "authors": "Haoneng Lin;Cheng Xu;Jing Qin", "summary": "Modern Vision-Language Models (VLMs) exhibit unprecedented capabilities in\ncross-modal semantic understanding between visual and textual modalities. Given\nthe intrinsic need for multi-modal integration in clinical applications, VLMs\nhave emerged as a promising solution for a wide range of medical image analysis\ntasks. However, adapting general-purpose VLMs to medical domain poses numerous\nchallenges, such as large domain gaps, complicated pathological variations, and\ndiversity and uniqueness of different tasks. The central purpose of this review\nis to systematically summarize recent advances in adapting VLMs for medical\nimage analysis, analyzing current challenges, and recommending promising yet\nurgent directions for further investigations. We begin by introducing core\nlearning strategies for medical VLMs, including pretraining, fine-tuning, and\nprompt learning. We then categorize five major VLM adaptation strategies for\nmedical image analysis. These strategies are further analyzed across eleven\nmedical imaging tasks to illustrate their current practical implementations.\nFurthermore, we analyze key challenges that impede the effective adaptation of\nVLMs to clinical applications and discuss potential directions for future\nresearch. We also provide an open-access repository of related literature to\nfacilitate further research, available at\nhttps://github.com/haonenglin/Awesome-VLM-for-MIA. It is anticipated that this\narticle can help researchers who are interested in harnessing VLMs in medical\nimage analysis tasks have a better understanding on their capabilities and\nlimitations, as well as current technical barriers, to promote their\ninnovative, robust, and safe application in clinical practice.", "comment": "34 pages", "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.CV", "links": "http://arxiv.org/abs/2506.18378v1;http://arxiv.org/pdf/2506.18378v1", "pdf_url": "http://arxiv.org/pdf/2506.18378v1"}, {"title": "Pareto-Optimized Open-Source LLMs for **Healthcare** via Context Retrieval", "link": "https://link.springer.com/chapter/10.1007/978-3-031-96235-6_27", "details": "J Bayarri-Planas, AK Gururajan, D Garcia-Gasulla - IFIP International Conference on \u2026, 2025", "abstract": "\u2026 This study leverages optimized context retrieval to enhance open-source **Large** **Language** **Models** (LLMs) for cost-effective, high performance **healthcare** AI. We demonstrate that this approach achieves state-of-the-art accuracy on **medical** \u2026"}, {"title": "Can Common VLMs Rival Medical VLMs? Evaluation and Strategic Insights", "link": "https://arxiv.org/pdf/2506.17337", "details": "Y Zhong, R Jin, X Li, Q Dou - arXiv preprint arXiv:2506.17337, 2025", "abstract": "\u2026 **med** ical imaging tasks? This study systematically evaluates common and **medical** VLMs across disease diagnosis and visual **question** **answering** (VQA)\u2026 LLaVA [15] is a VLM model that leverages CLIP vision encoders and **large** **language** **models** (LLMs) \u2026", "entry_id": "http://arxiv.org/abs/2506.17337v1", "updated": "2025-06-19 07:59:00", "published": "2025-06-19 07:59:00", "authors": "Yuan Zhong;Ruinan Jin;Xiaoxiao Li;Qi Dou", "summary": "Medical vision-language models (VLMs) leverage large-scale pretraining for\ndiverse imaging tasks but require substantial computational and data resources.\nMeanwhile, common or general-purpose VLMs (e.g., CLIP, LLaVA), though not\ntrained for medical use, show promise with fine-tuning. This raises a key\nquestion: Can efficient fine-tuned common VLMs rival generalist medical VLMs\nfor solving specific medical imaging tasks? This study systematically evaluates\ncommon and medical VLMs across disease diagnosis and visual question answering\n(VQA). Using CLIP-based and LLaVA-based models, we examine (1) off-the-shelf\nperformance gaps in in-domain (ID) settings, (2) whether fine-tuning bridges\nthese gaps, and (3) generalization to out-of-domain (OOD) tasks on unseen\nmedical modalities. While medical-specific pretraining provides advantages in\nID settings, common VLMs match or surpass medical-specific models after\nlightweight fine-tuning, with LoRA-based adaptation proving highly effective\namong different tasks. In OOD tasks, common VLMs demonstrate strong\nadaptability in some tasks, challenging the assumption that medical-specific\npre-training is essential. These findings suggest that leveraging common VLMs\nwith fine-tuning offers a scalable and cost-effective alternative to developing\nlarge-scale medical VLMs, providing crucial insights for future research in the\nmedical imaging field.", "comment": null, "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2506.17337v1;http://arxiv.org/pdf/2506.17337v1", "pdf_url": "http://arxiv.org/pdf/2506.17337v1"}, {"title": "Towards Global-level Mechanistic Interpretability: A Perspective of Modular Circuits of **Large Language Models**", "link": "https://openreview.net/pdf%3Fid%3Ddo5vVfKEXZ", "details": "Y He, W Zheng, Y Dong, Y Zhu, C Chen, J Li - Forty-second International Conference on \u2026", "abstract": "\u2026 2022) for **medical** multiple-choice **question** **answering** , and \u201cSymptom to Diagnosis\u201d (Gretel.ai\u2026 The proposed ModCirc framework, while demonstrating promising results in discovering modular circuits within **large** **language** **models** , has \u2026"}, {"title": "SHREC and PHEONA: Using Large Language Models to Advance Next-Generation Computational Phenotyping", "link": "https://arxiv.org/pdf/2506.16359", "details": "S Pungitore, S Yadav, M Douglas, J Mosier, V Subbian - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 One outstanding **question** for all biomedical tasks performed with LLMs is how to best incorporate specialized **medical** knowledge, including standardized vocabularies and ontologies. In this study, we injected **medical** information into the \u2026", "entry_id": "http://arxiv.org/abs/2506.16359v1", "updated": "2025-06-19 14:35:23", "published": "2025-06-19 14:35:23", "authors": "Sarah Pungitore;Shashank Yadav;Molly Douglas;Jarrod Mosier;Vignesh Subbian", "summary": "Objective: Computational phenotyping is a central informatics activity with\nresulting cohorts supporting a wide variety of applications. However, it is\ntime-intensive because of manual data review, limited automation, and\ndifficulties in adapting algorithms across sources. Since LLMs have\ndemonstrated promising capabilities for text classification, comprehension, and\ngeneration, we posit they will perform well at repetitive manual review tasks\ntraditionally performed by human experts. To support next-generation\ncomputational phenotyping methods, we developed SHREC, a framework for\ncomprehensive integration of LLMs into end-to-end phenotyping pipelines.\nMaterials and Methods: We applied and tested the ability of three lightweight\nLLMs (Gemma2 27 billion, Mistral Small 24 billion, and Phi-4 14 billion) to\nclassify concepts and phenotype patients using previously developed phenotypes\nfor ARF respiratory support therapies. Results: All models performed well on\nconcept classification, with the best model (Mistral) achieving an AUROC of\n0.896 across all relevant concepts. For phenotyping, models demonstrated\nnear-perfect specificity for all phenotypes, and the top-performing model\n(Mistral) reached an average AUROC of 0.853 for single-therapy phenotypes,\ndespite lower performance on multi-therapy phenotypes. Discussion: There are\nseveral advantages of LLMs that support their application to computational\nphenotyping, such as their ability to adapt to new tasks with prompt\nengineering alone and their ability to incorporate raw EHR data. Future steps\nto advance next-generation phenotyping methods include determining optimal\nstrategies for integrating biomedical data, exploring how LLMs reason, and\nadvancing generative model methods. Conclusion: Current lightweight LLMs can\nfeasibly assist researchers with resource-intensive phenotyping tasks such as\nmanual data review.", "comment": "Submitted to Journal of the American Medical Informatics Association", "journal_ref": null, "primary_category": "q-bio.QM", "categories": "q-bio.QM", "links": "http://arxiv.org/abs/2506.16359v1;http://arxiv.org/pdf/2506.16359v1", "pdf_url": "http://arxiv.org/pdf/2506.16359v1"}]
