[{"title": "Distilling vision-language pre-training models with modality-specific meta-learning", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125003478", "details": "X Ma, J Wang, X Zhang - Knowledge-Based Systems, 2025", "abstract": "Vision-language pre-training (VLP) models have exhibited excellent performance on diverse vision-language tasks, while the ensuing large-scale model parameters greatly limit their application. Knowledge distillation (KD) makes it possible to apply a \u2026"}, {"title": "Language Models Can Predict Their Own Behavior", "link": "https://arxiv.org/pdf/2502.13329", "details": "D Ashok, J May - arXiv preprint arXiv:2502.13329, 2025", "abstract": "Autoregressive Language Models output text by sequentially predicting the next token to generate, with modern methods like Chain-of-Thought (CoT) prompting achieving state-of-the-art reasoning capabilities by scaling the number of generated \u2026"}, {"title": "Is Self-Supervised Pre-training on Satellite Imagery Better than ImageNet? A Systematic Study with Sentinel-2", "link": "https://arxiv.org/pdf/2502.10669", "details": "S Lahrichi, Z Sheng, S Xia, K Bradbury, J Malof - arXiv preprint arXiv:2502.10669, 2025", "abstract": "Self-supervised learning (SSL) has demonstrated significant potential in pre-training robust models with limited labeled data, making it particularly valuable for remote sensing (RS) tasks. A common assumption is that pre-training on domain-aligned \u2026"}, {"title": "GPT-PPG: A GPT-based Foundation Model for Photoplethysmography Signals", "link": "https://arxiv.org/pdf/2503.08015", "details": "Z Chen, C Ding, S Kataria, R Yan, M Wang, R Lee\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This study introduces a novel application of a Generative Pre-trained Transformer (GPT) model tailored for photoplethysmography (PPG) signals, serving as a foundation model for various downstream tasks. Adapting the standard GPT \u2026"}, {"title": "Can Generative Geospatial Diffusion Models Excel as Discriminative Geospatial Foundation Models?", "link": "https://arxiv.org/pdf/2503.07890", "details": "Y Jia, V Marsocci, Z Gong, X Yang, M Vergauwen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Self-supervised learning (SSL) has revolutionized representation learning in Remote Sensing (RS), advancing Geospatial Foundation Models (GFMs) to leverage vast unlabeled satellite imagery for diverse downstream tasks. Currently, GFMs primarily \u2026"}, {"title": "Modeling Variants of Prompts for Vision-Language Models", "link": "https://arxiv.org/pdf/2503.08229", "details": "A Li, Z Liu, X Li, J Zhang, P Wang, H Wang - arXiv preprint arXiv:2503.08229, 2025", "abstract": "Large pre-trained vision-language models (VLMs) offer a promising approach to leveraging human language for enhancing downstream tasks. However, VLMs such as CLIP face significant limitation: its performance is highly sensitive to prompt \u2026"}, {"title": "Can Memory-Augmented Language Models Generalize on Reasoning-in-a-Haystack Tasks?", "link": "https://arxiv.org/pdf/2503.07903", "details": "P Das, CY Ko, S Dai, G Kollias, S Chaudhury\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models often expose their brittleness in reasoning tasks, especially while executing long chains of reasoning over context. We propose MemReasoner, a new and simple memory-augmented LLM architecture, in which the memory learns \u2026"}, {"title": "Continual Pre-training of MoEs: How robust is your router?", "link": "https://arxiv.org/pdf/2503.05029", "details": "B Th\u00e9rien, C\u00c9 Joseph, Z Sarwar, A Panda, A Das\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Sparsely-activated Mixture of Experts (MoE) transformers are promising architectures for foundation models. Compared to dense transformers that require the same amount of floating point operations (FLOPs) per forward pass, MoEs benefit from \u2026"}, {"title": "Pretraining GPT-style models in Hungarian", "link": "https://www.infocommunications.hu/documents/169298/4797540/InfocomJournal_2025_1_EA_1_vj.pdf", "details": "K Szentmih\u00e1lyi, DM Nemeskey, AM Szekeres\u2026", "abstract": "In this paper, we introduce two bilingual large lan-guage models, named OTP-1.5 B and OTP-13B, designed with a focus on both English and Hungarian languages. Both models utilize an 8k token context window and are trained on a dataset of 640 \u2026"}]
