[{"title": "Masked Contrastive Representation Learning for Self-Supervised Visual Pre-Training", "link": "https://ieeexplore.ieee.org/abstract/document/10722789/", "details": "Y Yao, N Desai, M Palaniswami - 2024 IEEE 11th International Conference on Data \u2026, 2024", "abstract": "Self-supervised learning has achieved state-of-the-art performance in various tasks and applications. In computer vision, self-supervised learning often employs contrastive learning and masked image modeling, each with its limitations \u2026"}, {"title": "Vicsgaze: a gaze estimation method using self-supervised contrastive learning", "link": "https://link.springer.com/article/10.1007/s00530-024-01458-x", "details": "D Gu, M Lv, J Liu - Multimedia Systems, 2024", "abstract": "Existing deep learning-based gaze estimation methods achieved high accuracy, and the prerequisite for ensuring their performance is large-scale datasets with gaze labels. However, collecting large-scale gaze datasets is time-consuming and \u2026"}, {"title": "Diffusion Models for Cross-Domain Image-to-Image Translation with Paired and Partially Paired Datasets", "link": "https://ieeexplore.ieee.org/abstract/document/10722775/", "details": "T Bell, D Li - 2024 IEEE 11th International Conference on Data \u2026, 2024", "abstract": "The line-art colorization problem is a task in generative modeling with the goal of generating colored artworks from an artist's hand-drawn line-arts. Machine learning models such as generative adversarial networks have been applied to this task. At \u2026"}, {"title": "SILC: Improving Vision Language Pretraining with Self-distillation", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-72664-4_3.pdf", "details": "L Hoyer, L Van Gool, F Tombari", "abstract": "Image-Text pretraining on web-scale image caption datasets has become the default recipe for open vocabulary classification and retrieval models thanks to the success of CLIP and its variants. Several works have also used CLIP features for dense \u2026"}]
