[{"title": "Neural-Symbolic Collaborative Distillation: Advancing Small Language Models for Complex Reasoning Tasks", "link": "https://arxiv.org/pdf/2409.13203", "details": "H Liao, S He, Y Xu, Y Zhang, K Liu, J Zhao - arXiv preprint arXiv:2409.13203, 2024", "abstract": "In this paper, we propose $\\textbf {Ne} $ ural-$\\textbf {Sy} $ mbolic $\\textbf {C} $ ollaborative $\\textbf {D} $ istillation ($\\textbf {NesyCD} $), a novel knowledge distillation method for learning the complex reasoning abilities of Large Language \u2026"}, {"title": "CEval: A Benchmark for Evaluating Counterfactual Text Generation", "link": "https://aclanthology.org/2024.inlg-main.6.pdf", "details": "C Seifert, J Schl\u00f6tterer - Proceedings of the 17th International Natural \u2026, 2024", "abstract": "Counterfactual text generation aims to minimally change a text, such that it is classified differently. Assessing progress in method development for counterfactual text generation is hindered by a non-uniform usage of data sets and metrics in \u2026"}, {"title": "Parameter Efficiency, Few-Shot, Zero-Shot, Prompting", "link": "https://jonmay.github.io/USC-CS662/assets/files/llm.pdf", "details": "J May - 2024", "abstract": "The models we've discussed so far follow the paradigm that, out of the box, they don't do too much, but when you expose them to some supervised data that is an exemplar of a task and fine-tune their parameters they can do the task when given \u2026"}, {"title": "LLM-CARD: Towards a Description and Landscape of Large Language Models", "link": "https://arxiv.org/pdf/2409.17011", "details": "S Tian, L Han, EM Guzman, G Nenadic - arXiv preprint arXiv:2409.17011, 2024", "abstract": "With the rapid growth of the Natural Language Processing (NLP) field, a vast variety of Large Language Models (LLMs) continue to emerge for diverse NLP tasks. As an increasing number of papers are presented, researchers and developers face the \u2026"}, {"title": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification", "link": "https://arxiv.org/pdf/2409.16718", "details": "M Li, J Zhong, C Li, L Li, N Lin, M Sugiyama - arXiv preprint arXiv:2409.16718, 2024", "abstract": "Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed the success of prompt tuning and adapter tuning, while the classic model fine-tuning on inherent parameters seems to be overlooked. It is believed that fine-tuning the \u2026"}, {"title": "DiPT: Enhancing LLM reasoning through diversified perspective-taking", "link": "https://arxiv.org/pdf/2409.06241", "details": "HA Just, M Dabas, L Huang, M Jin, R Jia - arXiv preprint arXiv:2409.06241, 2024", "abstract": "Existing work on improving language model reasoning typically explores a single solution path, which can be prone to errors. Inspired by perspective-taking in social studies, this paper introduces DiPT, a novel approach that complements current \u2026"}]
