[{"title": "PSSD: Making Large Language Models Self-denial via Human Psyche Structure", "link": "https://openreview.net/pdf%3Fid%3D80BpkRq6xe", "details": "J Liao, Z Liao, X Zhao - THE WEB CONFERENCE 2025", "abstract": "The enhance of accuracy in reasoning results of LLMs arouses the community's interests, wherein pioneering studies investigate post-hoc strategies to rectify potential mistakes. Despite extensive efforts, they are all stuck in a state of resource \u2026"}, {"title": "When Large Vision Language Models Meet Multimodal Sequential Recommendation: An Empirical Study", "link": "https://openreview.net/pdf%3Fid%3DE8bjWloEvU", "details": "P Zhou, C Liu, J Ren, X Zhou, XIE Yueqi, M Cao, Z Rao\u2026 - THE WEB CONFERENCE 2025", "abstract": "As multimedia content continues to grow on the Web, the integration of visual and textual data has become a crucial challenge for Web applications, particularly in recommendation systems. Large Vision Language Models (LVLMs) have \u2026"}, {"title": "Large Vision-Language Models for Knowledge-Grounded Data Annotation of Memes", "link": "https://arxiv.org/pdf/2501.13851%3F", "details": "S Deng, S Belongie, PE Christensen - arXiv preprint arXiv:2501.13851, 2025", "abstract": "Memes have emerged as a powerful form of communication, integrating visual and textual elements to convey humor, satire, and cultural messages. Existing research has focused primarily on aspects such as emotion classification, meme generation \u2026"}, {"title": "Assessing and Post-Processing Black Box Large Language Models for Knowledge Editing", "link": "https://openreview.net/pdf%3Fid%3DaGhk1VNcRJ", "details": "X Song, Z Wang, K He, G Dong, Y Mou, J Zhao, W Xu - THE WEB CONFERENCE 2025", "abstract": "The rapid evolution of the Web as a key platform for information dissemination has led to the growing integration of large language models (LLMs) in Web-based applications. However, the swift changes in web content present challenges in \u2026"}, {"title": "StaICC: Standardized Evaluation for Classification Task in In-context Learning", "link": "https://arxiv.org/pdf/2501.15708", "details": "H Cho, N Inoue - arXiv preprint arXiv:2501.15708, 2025", "abstract": "Classification tasks are widely investigated in the In-Context Learning (ICL) paradigm. However, current efforts are evaluated on disjoint benchmarks and settings, while their performances are significantly influenced by some trivial \u2026"}, {"title": "CASE-Bench: Context-Aware Safety Evaluation Benchmark for Large Language Models", "link": "https://arxiv.org/pdf/2501.14940", "details": "G Sun, X Zhan, S Feng, PC Woodland, J Such - arXiv preprint arXiv:2501.14940, 2025", "abstract": "Aligning large language models (LLMs) with human values is essential for their safe deployment and widespread adoption. Current LLM safety benchmarks often focus solely on the refusal of individual problematic queries, which overlooks the \u2026"}, {"title": "Evaluating and Improving Graph to Text Generation with Large Language Models", "link": "https://arxiv.org/pdf/2501.14497", "details": "J He, Y Yang, W Long, D Xiong, VG Basulto, JZ Pan - arXiv preprint arXiv:2501.14497, 2025", "abstract": "Large language models (LLMs) have demonstrated immense potential across various tasks. However, research for exploring and improving the capabilities of LLMs in interpreting graph structures remains limited. To address this gap, we \u2026"}, {"title": "Towards Safer Social Media Platforms: Scalable and Performant Few-Shot Harmful Content Moderation Using Large Language Models", "link": "https://arxiv.org/pdf/2501.13976", "details": "A Bonagiri, L Li, R Oak, Z Babar, M Wojcieszak\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The prevalence of harmful content on social media platforms poses significant risks to users and society, necessitating more effective and scalable content moderation strategies. Current approaches rely on human moderators, supervised classifiers \u2026"}, {"title": "Synthetic Data Generation Using Large Language Models for Financial Question Answering", "link": "https://aclanthology.org/2025.finnlp-1.7.pdf", "details": "C Harsha, KS Phogat, S Dasaratha, SA Puranam\u2026 - \u2026 of the Joint Workshop of the \u2026, 2025", "abstract": "Recent research has shown excellent performance of large language models (LLMs) for answering questions requiring multi-step financial reasoning. While the larger models have been used with zero-shot or few-shot prompting, the smaller variants \u2026"}]
