[{"title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability", "link": "https://arxiv.org/pdf/2504.06260", "details": "N Mudur, H Cui, S Venugopalan, P Raccuglia\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science. We present FEABench, a benchmark to evaluate the ability of large \u2026"}, {"title": "Argumentative Large Language Models for Explainable and Contestable Claim Verification", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/33637/35792", "details": "G Freedman, A Dejl, D Gorur, X Yin, A Rago, F Toni - Proceedings of the AAAI \u2026, 2025", "abstract": "The profusion of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them promising candidates for use in decision-making. However, they are currently limited by their \u2026"}, {"title": "Overcoming Heterogeneous Data in Federated Medical Vision-Language Pre-training: A Triple-Embedding Model Selector Approach", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/32807/34962", "details": "A Wang, Z Zhang, D Wang, F Wang, H Hu, J Guo\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "The scarcity data of medical field brings the collaborative training in medical vision- language pre-training (VLP) cross different clients. Therefore, the collaborative training in medical VLP faces two challenges: First, the medical data requires \u2026"}, {"title": "Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization", "link": "https://arxiv.org/pdf/2504.21063", "details": "S Gong, C Cui, X Dong, X Nie, L Zhu, X Chang - arXiv preprint arXiv:2504.21063, 2025", "abstract": "Federated domain generalization (FedDG) aims to learn a globally generalizable model from decentralized clients with heterogeneous data while preserving privacy. Recent studies have introduced prompt learning to adapt vision-language models \u2026"}, {"title": "HalluShift: Measuring Distribution Shifts towards Hallucination Detection in LLMs", "link": "https://arxiv.org/pdf/2504.09482", "details": "S Dasgupta, S Nath, A Basu, P Shamsolmoali, S Das - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have recently garnered widespread attention due to their adeptness at generating innovative responses to the given prompts across a multitude of domains. However, LLMs often suffer from the inherent limitation of \u2026"}, {"title": "Improving Sequential Recommenders through Counterfactual Augmentation of System Exposure", "link": "https://arxiv.org/pdf/2504.13482", "details": "Z Zhao, Z Ren, J Yang, Z Yan, Z Wang, L Yang, P Ren\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In sequential recommendation (SR), system exposure refers to items that are exposed to the user. Typically, only a few of the exposed items would be interacted with by the user. Although SR has achieved great success in predicting future user \u2026"}, {"title": "From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models", "link": "https://arxiv.org/pdf/2504.06214", "details": "C Xu, W Ping, P Xu, Z Liu, B Wang, M Shoeybi, B Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Long-context capabilities are essential for a wide range of applications, including document and video understanding, in-context learning, and inference-time scaling, all of which require models to process and reason over long sequences of text and \u2026"}, {"title": "Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection", "link": "https://arxiv.org/pdf/2504.09440", "details": "MS Liu, S Bo, J Fang - arXiv preprint arXiv:2504.09440, 2025", "abstract": "Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical \u2026"}, {"title": "LLM $\\times $ MapReduce-V2: Entropy-Driven Convolutional Test-Time Scaling for Generating Long-Form Articles from Extremely Long Resources", "link": "https://arxiv.org/pdf/2504.05732", "details": "H Wang, Y Fu, Z Zhang, S Wang, Z Ren, X Wang, Z Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Long-form generation is crucial for a wide range of practical applications, typically categorized into short-to-long and long-to-long generation. While short-to-long generations have received considerable attention, generating long texts from \u2026"}]
