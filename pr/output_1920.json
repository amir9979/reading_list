[{"title": "More RLHF, More Trust? On The Impact of Human Preference Alignment On Language Model Trustworthiness", "link": "https://arxiv.org/pdf/2404.18870", "details": "AJ Li, S Krishna, H Lakkaraju - arXiv preprint arXiv:2404.18870, 2024", "abstract": "The surge in Large Language Models (LLMs) development has led to improved performance on cognitive tasks as well as an urgent need to align these models with human values in order to safely exploit their power. Despite the effectiveness of \u2026"}, {"title": "A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks", "link": "https://arxiv.org/pdf/2405.10251", "details": "X Ni, P Li - arXiv preprint arXiv:2405.10251, 2024", "abstract": "Recent efforts have evaluated large language models (LLMs) in areas such as commonsense reasoning, mathematical reasoning, and code generation. However, to the best of our knowledge, no work has specifically investigated the performance \u2026"}, {"title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers", "link": "https://arxiv.org/pdf/2404.18443", "details": "R Xu, W Shi, Y Yu, Y Zhuang, Y Zhu, MD Wang, JC Ho\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Developing effective biomedical retrieval models is important for excelling at knowledge-intensive biomedical tasks but still challenging due to the deficiency of sufficient publicly annotated biomedical data and computational resources. We \u2026"}]
