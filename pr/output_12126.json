[{"title": "LLM-MedQA: Enhancing Medical Question Answering through Case Studies in Large Language Models", "link": "https://arxiv.org/pdf/2501.05464", "details": "H Yang, H Chen, H Guo, Y Chen, CS Lin, S Hu, J Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Accurate and efficient question-answering systems are essential for delivering high- quality patient care in the medical field. While Large Language Models (LLMs) have made remarkable strides across various domains, they continue to face significant \u2026"}, {"title": "Understanding Before Reasoning: Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting", "link": "https://arxiv.org/pdf/2501.04341%3F", "details": "DH Zhu, YJ Xiong, JC Zhang, XJ Xie, CM Xia - arXiv preprint arXiv:2501.04341, 2025", "abstract": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language Models (LLMs) to enhance complex reasoning. It guides LLMs to present multi-step reasoning, rather than generating the final answer directly. However, CoT \u2026"}]
