[{"title": "Frozen Language Models Are Gradient Coherence Rectifiers in Vision Transformers", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/32176/34331", "details": "L Bai, Z Xiong, H Lin, G Xu, X Xie, R Guo, Z Kang\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "Large language models (LLMs) have demonstrated remarkable performance in multimodal tasks even with frozen LLM Block and only a few trainable parameters. However, the underlying mechanisms of how LLMs enhance multimodal \u2026"}, {"title": "Summarizing Online Patient Conversations Using Generative Language Models: Experimental and Comparative Study", "link": "https://medinform.jmir.org/2025/1/e62909/", "details": "RAS Nair, M Hartung, P Heinisch, J Jaskolski\u2026 - JMIR Medical Informatics, 2025", "abstract": "Background: Social media is acknowledged by regulatory bodies (eg, the Food and Drug Administration) as an important source of patient experience data to learn about patients' unmet needs, priorities, and preferences. However, current methods \u2026"}, {"title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training", "link": "https://arxiv.org/pdf/2504.13161", "details": "S Diao, Y Yang, Y Fu, X Dong, D Su, M Kliegl, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Pre-training datasets are typically collected from web content and lack inherent domain divisions. For instance, widely used datasets like Common Crawl do not include explicit domain labels, while manually curating labeled datasets such as The \u2026"}, {"title": "ChatEXAONEPath: An Expert-level Multimodal Large Language Model for Histopathology Using Whole Slide Images", "link": "https://arxiv.org/pdf/2504.13023", "details": "S Kim, S Lee, J Jang - arXiv preprint arXiv:2504.13023, 2025", "abstract": "Recent studies have made significant progress in developing large language models (LLMs) in the medical domain, which can answer expert-level questions and demonstrate the potential to assist clinicians in real-world clinical scenarios. Studies \u2026"}, {"title": "SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data", "link": "https://arxiv.org/pdf/2504.12185", "details": "S Bae, H Kim, YS Choi, JH Lee - arXiv preprint arXiv:2504.12185, 2025", "abstract": "In various natural language processing (NLP) tasks, fine-tuning Pre-trained Language Models (PLMs) often leads to the issue of spurious correlations, which negatively impacts performance, particularly when dealing with out-of-distribution \u2026"}, {"title": "Retrieval augmented generation for 10 large language models and its generalizability in assessing medical", "link": "https://search.proquest.com/openview/3e51f6a2e99fb6ea56e15d9ad7c3e25c/1%3Fpq-origsite%3Dgscholar%26cbl%3D5061815", "details": "YH Ke, L Jin, K Elangovan, HR Abdullah, N Liu\u2026", "abstract": "Abstract Large Language Models (LLMs) hold promise for medical applications but often lack domain-specific expertise. Retrieval Augmented Generation (RAG) enables customization by integrating specialized knowledge. This study assessed \u2026"}]
