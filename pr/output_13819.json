[{"title": "Large language models are less effective at clinical prediction tasks than locally trained machine learning models", "link": "https://par.nsf.gov/biblio/10575953", "details": "KE Brown, C Yan, Z Li, X Zhang, BX Collins, Y Chen\u2026 - Journal of the American \u2026, 2025", "abstract": "ObjectivesTo determine the extent to which current large language models (LLMs) can serve as substitutes for traditional machine learning (ML) as clinical predictors using data from electronic health records (EHRs), we investigated various factors that \u2026"}, {"title": "Autoregressive Language Model with Historical Context Re-encoding", "link": "https://ieeexplore.ieee.org/iel8/10887540/10887541/10890165.pdf", "details": "Y Zhuang - ICASSP 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "The foundation of current large language model applications lies in the generative language model, which typically employs an autoregressive token generation approach. However, this model faces two key limitations: its unidirectional causal \u2026"}, {"title": "Assessing and alleviating state anxiety in large language models", "link": "https://www.nature.com/articles/s41746-025-01512-6", "details": "Z Ben-Zion, K Witte, AK Jagadish, O Duek\u2026 - npj Digital Medicine, 2025", "abstract": "Abstract The use of Large Language Models (LLMs) in mental health highlights the need to understand their responses to emotional content. Previous research shows that emotion-inducing prompts can elevate \u201canxiety\u201d in LLMs, affecting behavior and \u2026"}, {"title": "OPPA: OPtimizing PArallelism for Language Model Training", "link": "https://openreview.net/pdf%3Fid%3Dqsx5vPJlOx", "details": "A Hemachandra, Y Han, SK Ng, BKH Low - First Workshop on Scalable Optimization for \u2026", "abstract": "Training of modern large neural networks (NNs) is often done in parallel across multiple GPUs. While there are existing parallel training frameworks which easily allow NN training using multi-dimensional parallelism, the challenge remains in \u2026"}]
