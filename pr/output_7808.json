[{"title": "GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization", "link": "https://arxiv.org/pdf/2410.04087", "details": "Y Ye, X Feng, X Feng, W Ma, L Qin, D Xu, Q Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "News summarization in today's global scene can be daunting with its flood of multilingual content and varied viewpoints from different sources. However, current studies often neglect such real-world scenarios as they tend to focus solely on either \u2026"}, {"title": "Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures", "link": "https://arxiv.org/pdf/2410.06672", "details": "J Wang, X Ge, W Shu, Q Tang, Y Zhou, Z He, X Qiu - arXiv preprint arXiv:2410.06672, 2024", "abstract": "The hypothesis of Universality in interpretability suggests that different neural networks may converge to implement similar algorithms on similar tasks. In this work, we investigate two mainstream architectures for language modeling, namely \u2026"}, {"title": "Large Language Models for Clinical Text Cleansing Enhance Medical Concept Normalization", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10703053.pdf", "details": "A Abdulnazar, R Roller, S Schulz, M Kreuzthaler - IEEE Access, 2024", "abstract": "Most clinical information is only available as free text. Large language models (LLMs) are increasingly applied to clinical data to streamline communication, enhance the accuracy of clinical documentation, and ultimately improve healthcare \u2026"}, {"title": "Large Language Models for Simplified Interventional Radiology Reports: A Comparative Analysis", "link": "https://www.sciencedirect.com/science/article/pii/S1076633224006901", "details": "E Can, W Uller, K Vogt, MC Doppler, F Busch, N Bayerl\u2026 - Academic Radiology, 2024", "abstract": "Purpose To quantitatively and qualitatively evaluate and compare the performance of leading large language models (LLMs), including proprietary models (GPT-4, GPT- 3.5 Turbo, Claude-3-Opus, and Gemini Ultra) and open-source models (Mistral-7b \u2026"}, {"title": "Fine-Tuning In-House Large Language Models to Infer Differential Diagnosis from Radiology Reports", "link": "https://arxiv.org/pdf/2410.09234", "details": "L Chen, R Teotia, A Verdone, A Cardall, L Tyagi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Radiology reports summarize key findings and differential diagnoses derived from medical imaging examinations. The extraction of differential diagnoses is crucial for downstream tasks, including patient management and treatment planning. However \u2026"}, {"title": "Clinical Notes as Narratives: Implications for Large Language Models in Healthcare", "link": "https://link.springer.com/article/10.1007/s11606-024-09093-y", "details": "TD Brender, LA Celi, JM Cobert - Journal of General Internal Medicine, 2024", "abstract": "OpenAI's ChatGPT sparked tremendous excitement regarding potential healthcare applications of large language models (LLM). LLMs trained on electronic health record (EHR) notes could enrich the feature space for many tasks including risk \u2026"}, {"title": "Leveraging Coarse-to-Fine Grained Representations in Contrastive Learning for Differential Medical Visual Question Answering", "link": "https://papers.miccai.org/miccai-2024/paper/1957_paper.pdf", "details": "X Liang, Y Wang, D Wang, Z Jiao, H Zhong, M Yang\u2026 - International Conference on \u2026, 2024", "abstract": "Abstract Chest X-ray Differential Medical Visual Question Answering (Diff-MedVQA) is a novel multi-modal task designed to answer questions about diseases, especially their differences, based on a main image and a reference image. Compared to the \u2026"}, {"title": "How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not", "link": "https://arxiv.org/pdf/2409.17044", "details": "F Verdini, P Melucci, S Perna, F Cariaggi, M Gaido\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The remarkable performance achieved by Large Language Models (LLM) has driven research efforts to leverage them for a wide range of tasks and input modalities. In speech-to-text (S2T) tasks, the emerging solution consists of projecting the output of \u2026"}]
