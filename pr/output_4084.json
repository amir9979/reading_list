[{"title": "Scaling Laws for Linear Complexity Language Models", "link": "https://arxiv.org/pdf/2406.16690", "details": "X Shen, D Li, R Leng, Z Qin, W Sun, Y Zhong - arXiv preprint arXiv:2406.16690, 2024", "abstract": "The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their \u2026"}, {"title": "ICLEval: Evaluating In-Context Learning Ability of Large Language Models", "link": "https://arxiv.org/pdf/2406.14955", "details": "W Chen, Y Lin, ZH Zhou, HY Huang, Y Jia, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) is a critical capability of Large Language Models (LLMs) as it empowers them to comprehend and reason across interconnected inputs. Evaluating the ICL ability of LLMs can enhance their utilization and deepen our \u2026"}, {"title": "Towards Open-World Grasping with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2406.18722", "details": "G Tziafas, H Kasaei - arXiv preprint arXiv:2406.18722, 2024", "abstract": "The ability to grasp objects in-the-wild from open-ended language instructions constitutes a fundamental challenge in robotics. An open-world grasping system should be able to combine high-level contextual with low-level physical-geometric \u2026"}, {"title": "Universal Approximation Theory: The basic theory for large language models", "link": "https://arxiv.org/pdf/2407.00958", "details": "W Wang, Q Li - arXiv preprint arXiv:2407.00958, 2024", "abstract": "Language models have emerged as a critical area of focus in artificial intelligence, particularly with the introduction of groundbreaking innovations like ChatGPT. Large- scale Transformer networks have quickly become the leading approach for \u2026"}, {"title": "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models", "link": "https://arxiv.org/pdf/2407.01009", "details": "J Pan, Y Zhang, C Zhang, Z Liu, H Wang, H Li - arXiv preprint arXiv:2407.01009, 2024", "abstract": "Large language models (LLMs) have demonstrated emergent capabilities across diverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However, such a simple and fast COT approach often encounters limitations in dealing with \u2026"}, {"title": "PORT: Preference Optimization on Reasoning Traces", "link": "https://arxiv.org/pdf/2406.16061", "details": "S Lahlou, A Abubaker, H Hacid - arXiv preprint arXiv:2406.16061, 2024", "abstract": "Preference optimization methods have been successfully applied to improve not only the alignment of large language models (LLMs) with human values, but also specific natural language tasks such as summarization and stylistic continuations. This paper \u2026"}, {"title": "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training", "link": "https://arxiv.org/pdf/2406.16554", "details": "T Zhu, X Qu, D Dong, J Ruan, J Tong, C He, Y Cheng - arXiv preprint arXiv \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, training MoE from scratch in a large-scale setting still suffers from data-hungry and instability problems. Motivated \u2026"}, {"title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models", "link": "https://arxiv.org/pdf/2406.16714", "details": "J Cheng, Y Lu, X Gu, P Ke, X Liu, Y Dong, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Although Large Language Models (LLMs) are becoming increasingly powerful, they still exhibit significant but subtle weaknesses, such as mistakes in instruction- following or coding tasks. As these unexpected errors could lead to severe \u2026"}, {"title": "Concise and Precise Context Compression for Tool-Using Language Models", "link": "https://arxiv.org/pdf/2407.02043", "details": "Y Xu, Y Feng, H Mu, Y Hou, Y Li, X Wang, W Zhong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Through reading the documentation in the context, tool-using language models can dynamically extend their capability using external tools. The cost is that we have to input lengthy documentation every time the model needs to use the tool, occupying \u2026"}]
