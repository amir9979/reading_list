[{"title": "Deliberative alignment: Reasoning enables safer language models", "link": "https://arxiv.org/pdf/2412.16339", "details": "MY Guan, M Joglekar, E Wallace, S Jain, B Barak\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As large-scale language models increasingly impact safety-critical domains, ensuring their reliable adherence to well-defined principles remains a fundamental challenge. We introduce Deliberative Alignment, a new paradigm that directly \u2026"}, {"title": "Unifying Specialized Visual Encoders for Video Language Models", "link": "https://arxiv.org/pdf/2501.01426", "details": "J Chung, T Zhu, MG Saez-Diez, JC Niebles, H Zhou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of \u2026"}, {"title": "CLIP-UP: CLIP-Based Unanswerable Problem Detection for Visual Question Answering", "link": "https://arxiv.org/pdf/2501.01371", "details": "B Vardi, O Nir, A Shamir - arXiv preprint arXiv:2501.01371, 2025", "abstract": "Recent Vision-Language Models (VLMs) have demonstrated remarkable capabilities in visual understanding and reasoning, and in particular on multiple-choice Visual Question Answering (VQA). Still, these models can make distinctly unnatural errors \u2026"}, {"title": "Generalizing Trust: Weak-to-Strong Trustworthiness in Language Models", "link": "https://arxiv.org/pdf/2501.00418", "details": "M Pawelczyk, L Sun, Z Qi, A Kumar, H Lakkaraju - arXiv preprint arXiv:2501.00418, 2024", "abstract": "The rapid proliferation of generative AI, especially large language models, has led to their integration into a variety of applications. A key phenomenon known as weak-to- strong generalization-where a strong model trained on a weak model's outputs \u2026"}, {"title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining", "link": "https://arxiv.org/pdf/2501.00958", "details": "W Zhang, H Zhang, X Li, J Sun, Y Shen, W Lu, D Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge \u2026"}, {"title": "LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models", "link": "https://arxiv.org/pdf/2501.00055", "details": "M Yu, J Fang, Y Zhou, X Fan, K Wang, S Pan, Q Wen - arXiv preprint arXiv \u2026, 2024", "abstract": "While safety-aligned large language models (LLMs) are increasingly used as the cornerstone for powerful systems such as multi-agent frameworks to solve complex real-world problems, they still suffer from potential adversarial queries, such as \u2026"}, {"title": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over Aligned Large Language Models", "link": "https://arxiv.org/pdf/2412.12865", "details": "Y Fan, Y Hong, Q Wang, J Bao, H Jiang, Y Song - arXiv preprint arXiv:2412.12865, 2024", "abstract": "Alignment, endowing a pre-trained Large language model (LLM) with the ability to follow instructions, is crucial for its real-world applications. Conventional supervised fine-tuning (SFT) methods formalize it as causal language modeling typically with a \u2026"}, {"title": "Grounding Deliberate Reasoning in Multimodal Large Language Models", "link": "https://link.springer.com/chapter/10.1007/978-981-96-2061-6_2", "details": "J Chen, Y Liu, D Li, X An, W Deng, Z Feng, Y Zhao\u2026 - International Conference on \u2026, 2024", "abstract": "Abstract The rise of Multimodal Large Language Models, renowned for their advanced instruction-following and reasoning capabilities, has significantly propelled the field of visual reasoning. However, due to limitations in their image \u2026"}, {"title": "Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage Policy Optimization", "link": "https://arxiv.org/pdf/2412.18279%3F", "details": "J Liu, C Wang, CY Liu, L Zeng, R Yan, Y Sun, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The role of reinforcement learning (RL) in enhancing the reasoning of large language models (LLMs) is becoming increasingly significant. Despite the success of RL in many scenarios, there are still many challenges in improving the reasoning of \u2026"}]
