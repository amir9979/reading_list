[{"title": "HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/32171/34326", "details": "KHI Arif, JY Yoon, DS Nikolopoulos, H Vandierendonck\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "Abstract High-resolution Vision-Language Models (VLMs) are widely used in multimodal tasks to enhance accuracy by preserving detailed image information. However, these models often generate an excessive number of visual tokens due to \u2026"}, {"title": "ALGOPUZZLEVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Algorithmic Multimodal Puzzles", "link": "https://aclanthology.org/2025.naacl-long.486.pdf", "details": "D Ghosal, V Toh, YK Chia, S Poria - Proceedings of the 2025 Conference of the \u2026, 2025", "abstract": "This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models \u2026"}, {"title": "Transferable Adversarial Attacks on Black-Box Vision-Language Models", "link": "https://arxiv.org/pdf/2505.01050", "details": "K Hu, W Yu, L Zhang, A Robey, A Zou, C Xu, H Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision Large Language Models (VLLMs) are increasingly deployed to offer advanced capabilities on inputs comprising both text and images. While prior research has shown that adversarial attacks can transfer from open-source to \u2026"}, {"title": "QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models", "link": "https://arxiv.org/pdf/2504.11038", "details": "Y Zhang, R Xie, J Chen, X Sun, Z Kang, Y Wang - arXiv preprint arXiv:2504.11038, 2025", "abstract": "In typical multimodal tasks, such as Visual Question Answering (VQA), adversarial attacks targeting a specific image and question can lead large vision-language models (LVLMs) to provide incorrect answers. However, it is common for a single \u2026"}, {"title": "Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin", "link": "https://arxiv.org/pdf/2505.02056", "details": "Y Wang, X Bai, X Li, W Guan, L Nie, X Chen - arXiv preprint arXiv:2505.02056, 2025", "abstract": "Adapting vision-language models (VLMs) to downstream tasks with pseudolabels has gained increasing attention. A major obstacle is that the pseudolabels generated by VLMs tend to be imbalanced, leading to inferior performance. While existing \u2026"}, {"title": "GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models", "link": "https://arxiv.org/pdf/2504.09696%3F", "details": "J Zhang, C Zuo - arXiv preprint arXiv:2504.09696, 2025", "abstract": "Recent advances in R1-like reasoning models leveraging Group Relative Policy Optimization (GRPO) have significantly improved the performance of language models on mathematical reasoning tasks. However, current GRPO implementations \u2026"}, {"title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math", "link": "https://arxiv.org/pdf/2504.21233", "details": "H Xu, B Peng, H Awadalla, D Chen, YC Chen, M Gao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving \u2026"}, {"title": "Multi-Modal Language Models as Text-to-Image Model Evaluators", "link": "https://arxiv.org/pdf/2505.00759", "details": "J Chen, C Ross, R Askari-Hemmat, K Sinha, M Hall\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The steady improvements of text-to-image (T2I) generative models lead to slow deprecation of automatic evaluation benchmarks that rely on static datasets, motivating researchers to seek alternative ways to evaluate the T2I progress. In this \u2026"}, {"title": "Localizing Before Answering: A Benchmark for Grounded Medical Visual Question Answering", "link": "https://arxiv.org/pdf/2505.00744", "details": "D Nguyen, MK Ho, H Ta, TT Nguyen, Q Chen, K Rav\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical Large Multi-modal Models (LMMs) have demonstrated remarkable capabilities in medical data interpretation. However, these models frequently generate hallucinations contradicting source evidence, particularly due to \u2026"}]
