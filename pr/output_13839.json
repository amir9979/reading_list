[{"title": "Distill Not Only Data but Also Rewards: Can Smaller Language Models Surpass Larger Ones?", "link": "https://arxiv.org/pdf/2502.19557", "details": "Y Zhang, L Wang, M Fang, Y Du, C Huang, J Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Distilling large language models (LLMs) typically involves transferring the teacher model's responses through supervised fine-tuning (SFT). However, this approach neglects the potential to distill both data (output content) and reward signals (quality \u2026"}, {"title": "Mitigating Hallucinations in Large Vision-Language Models by Adaptively Constraining Information Flow", "link": "https://arxiv.org/pdf/2502.20750", "details": "J Bai, H Guo, Z Peng, J Yang, Z Li, M Li, Z Tian - arXiv preprint arXiv:2502.20750, 2025", "abstract": "Large vision-language models show tremendous potential in understanding visual information through human languages. However, they are prone to suffer from object hallucination, ie, the generated image descriptions contain objects that do not exist in \u2026"}, {"title": "Towards Statistical Factuality Guarantee for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2502.20560", "details": "Z Li, C Yan, NJ Jackson, W Cui, B Li, J Zhang, BA Malin - arXiv preprint arXiv \u2026, 2025", "abstract": "Advancements in Large Vision-Language Models (LVLMs) have demonstrated promising performance in a variety of vision-language tasks involving image- conditioned free-form text generation. However, growing concerns about \u2026"}, {"title": "Process-based Self-Rewarding Language Models", "link": "https://arxiv.org/pdf/2503.03746", "details": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' \u2026"}, {"title": "Balcony: A Lightweight Approach to Dynamic Inference of Generative Language Models", "link": "https://arxiv.org/pdf/2503.05005", "details": "B Jamialahmadi, P Kavehzadeh, M Rezagholizadeh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Deploying large language models (LLMs) in real-world applications is often hindered by strict computational and latency constraints. While dynamic inference offers the flexibility to adjust model behavior based on varying resource budgets, existing \u2026"}, {"title": "Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI", "link": "https://arxiv.org/pdf/2502.16691", "details": "E Noh, J Baek - arXiv preprint arXiv:2502.16691, 2025", "abstract": "Recent research has increasingly focused on training large language models (LLMs) using federated learning, known as FedLLM. However, responsible AI (RAI), which aims to ensure safe responses, remains underexplored in the context of FedLLM. In \u2026"}, {"title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation", "link": "https://arxiv.org/pdf/2502.09838", "details": "T Lin, W Zhang, S Li, Y Yuan, B Yu, H Li, W He, H Jiang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present HealthGPT, a powerful Medical Large Vision-Language Model (Med- LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to \u2026"}, {"title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning", "link": "https://arxiv.org/pdf/2503.04812", "details": "Z Lan, L Niu, F Meng, J Zhou, J Su - arXiv preprint arXiv:2503.04812, 2025", "abstract": "Universal multimodal embedding models play a critical role in tasks such as interleaved image-text retrieval, multimodal RAG, and multimodal clustering. However, our empirical results indicate that existing LMM-based embedding models \u2026"}, {"title": "Full-Step-DPO: Self-Supervised Preference Optimization with Step-wise Rewards for Mathematical Reasoning", "link": "https://arxiv.org/pdf/2502.14356", "details": "H Xu, X Mao, FL Li, X Wu, W Chen, W Zhang, AT Luu - arXiv preprint arXiv \u2026, 2025", "abstract": "Direct Preference Optimization (DPO) often struggles with long-chain mathematical reasoning. Existing approaches, such as Step-DPO, typically improve this by focusing on the first erroneous step in the reasoning chain. However, they overlook \u2026"}]
