[{"title": "ImageNet-RIB Benchmark: Large Pre-Training Datasets Don't Guarantee Robustness after Fine-Tuning", "link": "https://arxiv.org/pdf/2410.21582", "details": "J Hwang, B Cheung, ZW Hong, A Boopathy, P Agrawal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Highly performant large-scale pre-trained models promise to also provide a valuable foundation for learning specialized tasks, by fine-tuning the model to the desired task. By starting from a good general-purpose model, the goal is to achieve both \u2026"}, {"title": "SeRA: Self-Reviewing and Alignment of Large Language Models using Implicit Reward Margins", "link": "https://arxiv.org/pdf/2410.09362", "details": "J Ko, S Dingliwal, B Ganesh, S Sengupta, S Bodapati\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Direct alignment algorithms (DAAs), such as direct preference optimization (DPO), have become popular alternatives for Reinforcement Learning from Human Feedback (RLHF) due to their simplicity, efficiency, and stability. However, the \u2026"}, {"title": "Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered Clues", "link": "https://arxiv.org/pdf/2410.10700", "details": "Q Ren, H Li, D Liu, Z Xie, X Lu, Y Qiao, L Sha, J Yan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study exposes the safety vulnerabilities of Large Language Models (LLMs) in multi-turn interactions, where malicious users can obscure harmful intents across several queries. We introduce ActorAttack, a novel multi-turn attack method inspired \u2026"}, {"title": "Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy", "link": "https://arxiv.org/pdf/2410.17234", "details": "BA Tjandra, M Razzak, J Kossen, K Handa, Y Gal - arXiv preprint arXiv:2410.17234, 2024", "abstract": "Large Language Models (LLMs) are known to hallucinate, whereby they generate plausible but inaccurate text. This phenomenon poses significant risks in critical applications, such as medicine or law, necessitating robust hallucination mitigation \u2026"}, {"title": "WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models", "link": "https://arxiv.org/pdf/2410.17509", "details": "J Jia, J Liu, Y Zhang, P Ram, N Baracaldo, S Liu - arXiv preprint arXiv:2410.17509, 2024", "abstract": "The need for effective unlearning mechanisms in large language models (LLMs) is increasingly urgent, driven by the necessity to adhere to data regulations and foster ethical generative AI practices. Despite growing interest of LLM unlearning, much of \u2026"}, {"title": "Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs", "link": "https://arxiv.org/pdf/2410.10739", "details": "I Jindal, C Badrinath, P Bharti, L Vinay, SD Sharma - arXiv preprint arXiv:2410.10739, 2024", "abstract": "Large Language Models (LLMs) for public use require continuous pre-training to remain up-to-date with the latest data. The models also need to be fine-tuned with specific instructions to maintain their ability to follow instructions accurately. Typically \u2026"}]
