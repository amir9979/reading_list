[{"title": "UDKAG: Augmenting Large Vision-Language Models with Up-to-Date Knowledge", "link": "https://arxiv.org/pdf/2405.14554", "details": "C Li, Z Li, C Jing, S Liu, W Shao, Y Wu, P Luo, Y Qiao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision-language models (LVLMs) are ignorant of the up-to-date knowledge, such as LLaVA series, because they cannot be updated frequently due to the large amount of resources required, and therefore fail in many cases. For example, if a \u2026"}, {"title": "No Filter: Cultural and Socioeconomic Diversityin Contrastive Vision-Language Models", "link": "https://arxiv.org/pdf/2405.13777", "details": "A Pouget, L Beyer, E Bugliarello, X Wang, AP Steiner\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We study cultural and socioeconomic diversity in contrastive vision-language models (VLMs). Using a broad range of benchmark datasets and evaluation metrics, we bring to attention several important findings. First, the common filtering of training \u2026"}, {"title": "Calibrated Self-Rewarding Vision Language Models", "link": "https://arxiv.org/pdf/2405.14622", "details": "Y Zhou, Z Fan, D Cheng, S Yang, Z Chen, C Cui\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the \u2026"}, {"title": "Bridging Operator Learning and Conditioned Neural Fields: A Unifying Perspective", "link": "https://arxiv.org/pdf/2405.13998", "details": "S Wang, JH Seidman, S Sankaran, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Operator learning is an emerging area of machine learning which aims to learn mappings between infinite dimensional function spaces. Here we uncover a connection between operator learning architectures and conditioned neural fields \u2026"}, {"title": "Object Registration in Neural Fields", "link": "https://arxiv.org/pdf/2404.18381", "details": "D Hall, S Hausler, S Mahendren, P Moghadam - arXiv preprint arXiv:2404.18381, 2024", "abstract": "Neural fields provide a continuous scene representation of 3D geometry and appearance in a way which has great promise for robotics applications. One functionality that unlocks unique use-cases for neural fields in robotics is object 6 \u2026"}, {"title": "NeuroGauss4D-PCI: 4D Neural Fields and Gaussian Deformation Fields for Point Cloud Interpolation", "link": "https://arxiv.org/pdf/2405.14241", "details": "C Jiang, D Du, J Liu, S Zhu, Z Liu, Z Ma, Z Liang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Point Cloud Interpolation confronts challenges from point sparsity, complex spatiotemporal dynamics, and the difficulty of deriving complete 3D point clouds from sparse temporal information. This paper presents NeuroGauss4D-PCI, which excels \u2026"}, {"title": "MetaEarth: A Generative Foundation Model for Global-Scale Remote Sensing Image Generation", "link": "https://arxiv.org/pdf/2405.13570", "details": "Z Yu, C Liu, L Liu, Z Shi, Z Zou - arXiv preprint arXiv:2405.13570, 2024", "abstract": "The recent advancement of generative foundational models has ushered in a new era of image generation in the realm of natural images, revolutionizing art design, entertainment, environment simulation, and beyond. Despite producing high-quality \u2026"}, {"title": "Impact of high-quality, mixed-domain data on the performance of medical language models", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae120/7680487", "details": "M Griot, C Hemptinne, J Vanderdonckt, D Yuksel - Journal of the American Medical \u2026, 2024", "abstract": "Objective To optimize the training strategy of large language models for medical applications, focusing on creating clinically relevant systems that efficiently integrate into healthcare settings, while ensuring high standards of accuracy and reliability \u2026"}, {"title": "Super Tiny Language Models", "link": "https://arxiv.org/pdf/2405.14159", "details": "D Hillier, L Guertler, C Tan, P Agrawal, C Ruirui\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancement of large language models (LLMs) has led to significant improvements in natural language processing but also poses challenges due to their high computational and energy demands. This paper introduces a series of research \u2026"}]
