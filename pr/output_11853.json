[{"title": "Unifying Specialized Visual Encoders for Video Language Models", "link": "https://arxiv.org/pdf/2501.01426", "details": "J Chung, T Zhu, MG Saez-Diez, JC Niebles, H Zhou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of \u2026"}, {"title": "GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models", "link": "https://arxiv.org/pdf/2501.01428%3F", "details": "Z Qi, Z Zhang, Y Fang, J Wang, H Zhao - arXiv preprint arXiv:2501.01428, 2025", "abstract": "In recent years, 2D Vision-Language Models (VLMs) have made significant strides in image-text understanding tasks. However, their performance in 3D spatial comprehension, which is critical for embodied intelligence, remains limited. Recent \u2026"}, {"title": "CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries", "link": "https://arxiv.org/pdf/2501.01282", "details": "S Liu, Y Jin, C Li, DF Wong, Q Wen, L Sun, H Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) have advanced human-AI interaction but struggle with cultural understanding, often misinterpreting symbols, gestures, and artifacts due to biases in predominantly Western-centric training data. In this paper, we \u2026"}, {"title": "Multi-P $^ 2$ A: A Multi-perspective Benchmark on Privacy Assessment for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.19496", "details": "J Zhang, X Cao, Z Han, S Shan, X Chen - arXiv preprint arXiv:2412.19496, 2024", "abstract": "Large Vision-Language Models (LVLMs) exhibit impressive potential across various tasks but also face significant privacy risks, limiting their practical applications. Current researches on privacy assessment for LVLMs is limited in scope, with gaps \u2026"}, {"title": "MBQ: Modality-Balanced Quantization for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.19509", "details": "S Li, Y Hu, X Ning, X Liu, K Hong, X Jia, X Li, Y Yan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-Language Models (VLMs) have enabled a variety of real-world applications. The large parameter size of VLMs brings large memory and computation overhead which poses significant challenges for deployment. Post-Training Quantization (PTQ) \u2026"}, {"title": "IllusionBench: A Large-scale and Comprehensive Benchmark for Visual Illusion Understanding in Vision-Language Models", "link": "https://arxiv.org/pdf/2501.00848", "details": "Y Zhang, Z Zhang, X Wei, X Liu, G Zhai, X Min - arXiv preprint arXiv:2501.00848, 2025", "abstract": "Current Visual Language Models (VLMs) show impressive image understanding but struggle with visual illusions, especially in real-world scenarios. Existing benchmarks focus on classical cognitive illusions, which have been learned by state-of-the-art \u2026"}, {"title": "Are Vision-Language Models Truly Understanding Multi-vision Sensor?", "link": "https://arxiv.org/pdf/2412.20750", "details": "S Chung, Y Yu, Y Chee, SY Kim, BK Lee, YM Ro - arXiv preprint arXiv:2412.20750, 2024", "abstract": "Large-scale Vision-Language Models (VLMs) have advanced by aligning vision inputs with text, significantly improving performance in computer vision tasks. Moreover, for VLMs to be effectively utilized in real-world applications, an \u2026"}, {"title": "HALLUCINOGEN: A Benchmark for Evaluating Object Hallucination in Large Visual-Language Models", "link": "https://arxiv.org/pdf/2412.20622", "details": "A Seth, D Manocha, C Agarwal - arXiv preprint arXiv:2412.20622, 2024", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance in performing complex multimodal tasks. However, they are still plagued by object hallucination: the misidentification or misclassification of objects \u2026"}, {"title": "Exploring Information Processing in Large Language Models: Insights from Information Bottleneck Theory", "link": "https://arxiv.org/pdf/2501.00999", "details": "Z Yang, Z Qi, Z Ren, Z Jia, H Sun, X Zhu, X Liao - arXiv preprint arXiv:2501.00999, 2025", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks by understanding input information and predicting corresponding outputs. However, the internal mechanisms by which LLMs \u2026"}]
