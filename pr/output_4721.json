[{"title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "link": "https://arxiv.org/pdf/2407.21417", "details": "Z Wu, Y Zhang, P Qi, Y Xu, R Han, Y Zhang, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (ie, follow open-ended instructions) and \u2026"}, {"title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT", "link": "https://arxiv.org/pdf/2407.21058", "details": "M Ali, S Panda, Q Shen, M Wick, A Kobren - arXiv preprint arXiv:2407.21058, 2024", "abstract": "In the current landscape of language model research, larger models, larger datasets and more compute seems to be the only way to advance towards intelligence. While there have been extensive studies of scaling laws and models' scaling behaviors, the \u2026"}, {"title": "Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering", "link": "https://arxiv.org/pdf/2407.21368", "details": "D Guo, D Terzopoulos - arXiv preprint arXiv:2407.21368, 2024", "abstract": "Large Vision-Language Models (LVLMs) have achieved significant success in recent years, and they have been extended to the medical domain. Although demonstrating satisfactory performance on medical Visual Question Answering (VQA) tasks \u2026"}, {"title": "Strong Copyright Protection for Language Models via Adaptive Model Fusion", "link": "https://arxiv.org/pdf/2407.20105", "details": "J Abad, K Donhauser, F Pinto, F Yang - arXiv preprint arXiv:2407.20105, 2024", "abstract": "The risk of language models unintentionally reproducing copyrighted material from their training data has led to the development of various protective measures. In this paper, we propose model fusion as an effective solution to safeguard against \u2026"}, {"title": "Early diagnosis of HIV cases by means of text mining and machine learning models on clinical notes", "link": "https://www.sciencedirect.com/science/article/pii/S0010482524009156", "details": "R Morales-S\u00e1nchez, S Montalvo, A Ria\u00f1o, R Mart\u00ednez\u2026 - Computers in Biology and \u2026, 2024", "abstract": "Undiagnosed and untreated human immunodeficiency virus (HIV) infection increases morbidity in the HIV-positive person and allows onward transmission of the virus. Minimizing missed opportunities for HIV diagnosis when a patient visits a \u2026"}, {"title": "Igea: a Decoder-Only Language Model for Biomedical Text Generation in Italian", "link": "https://arxiv.org/pdf/2407.06011", "details": "TM Buonocore, S Rancati, E Parimbelli - arXiv preprint arXiv:2407.06011, 2024", "abstract": "The development of domain-specific language models has significantly advanced natural language processing applications in various specialized fields, particularly in biomedicine. However, the focus has largely been on English-language models \u2026"}, {"title": "Can Large Language Models Automatically Jailbreak GPT-4V?", "link": "https://arxiv.org/pdf/2407.16686", "details": "Y Wu, Y Huang, Y Liu, X Li, P Zhou, L Sun - arXiv preprint arXiv:2407.16686, 2024", "abstract": "GPT-4V has attracted considerable attention due to its extraordinary capacity for integrating and processing multimodal information. At the same time, its ability of face recognition raises new safety concerns of privacy leakage. Despite researchers' \u2026"}, {"title": "Modular Sentence Encoders: Separating Language Specialization from Cross-Lingual Alignment", "link": "https://arxiv.org/pdf/2407.14878", "details": "Y Huang, K Wang, G Glava\u0161, I Gurevych - arXiv preprint arXiv:2407.14878, 2024", "abstract": "Multilingual sentence encoders are commonly obtained by training multilingual language models to map sentences from different languages into a shared semantic space. As such, they are subject to curse of multilinguality, a loss of monolingual \u2026"}, {"title": "Evaluating Large Language Models with fmeval", "link": "https://arxiv.org/pdf/2407.12872", "details": "P Schw\u00f6bel, L Franceschi, MB Zafar, K Vasist\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "fmeval is an open source library to evaluate large language models (LLMs) in a range of tasks. It helps practitioners evaluate their model for task performance and along multiple responsible AI dimensions. This paper presents the library and \u2026"}]
