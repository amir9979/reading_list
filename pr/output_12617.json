[{"title": "Efficient Few-Shot Continual Learning in Vision-Language Models", "link": "https://arxiv.org/pdf/2502.04098", "details": "A Panos, R Aljundi, DO Reino, RE Turner - arXiv preprint arXiv:2502.04098, 2025", "abstract": "Vision-language models (VLMs) excel in tasks such as visual question answering and image captioning. However, VLMs are often limited by their use of pretrained image encoders, like CLIP, leading to image understanding errors that hinder overall \u2026"}, {"title": "Omni-DNA: A Unified Genomic Foundation Model for Cross-Modal and Multi-Task Learning", "link": "https://arxiv.org/pdf/2502.03499", "details": "Z Li, V Subasri, Y Shen, D Li, Y Zhao, GB Stan, C Shan - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) demonstrate remarkable generalizability across diverse tasks, yet genomic foundation models (GFMs) still require separate finetuning for each downstream application, creating significant overhead as model \u2026"}]
