[{"title": "SKILLWRAPPER: Autonomously Learning Interpretable Skill Abstractions with Foundation Models", "link": "https://dyalab.mines.edu/2025/icra-workshop/16.pdf", "details": "Z Yang, SS Raman, B Hedegaard, H Fu, L Zhao\u2026", "abstract": "We envision a future where robots are equipped \u201cout of the box\u201d with a library of general-purpose skills. To effectively compose these skills into long-horizon plans, a robot must understand each skill's preconditions and effects in a form that supports \u2026"}, {"title": "The impact of concept explanations and interventions on human-machine collaboration", "link": "https://orca.cardiff.ac.uk/id/eprint/177429/1/The_Impact_of_Concept_Explanations_and_Interventions_on_Human_machine_collaboration.pdf", "details": "J Furby, D Cunnington, D Braines, A Preece - 2025", "abstract": "Deep Neural Networks (DNNs) are often considered black boxes due to their opaque decision-making processes. To reduce their opacity Concept Models (CMs), such as Concept Bottleneck Models (CBMs), were introduced to predict human-defined \u2026"}]
