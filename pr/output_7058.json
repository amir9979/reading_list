[{"title": "PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning", "link": "https://arxiv.org/pdf/2408.16769", "details": "N Hussein, F Shamshad, M Naseer, K Nandakumar - arXiv preprint arXiv:2408.16769, 2024", "abstract": "Medical vision-language models (Med-VLMs) trained on large datasets of medical image-text pairs and later fine-tuned for specific tasks have emerged as a mainstream paradigm in medical image analysis. However, recent studies have \u2026"}, {"title": "LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning", "link": "https://arxiv.org/pdf/2409.01145", "details": "H Yang, X Zhao, S Huang, Q Li, G Xu - arXiv preprint arXiv:2409.01145, 2024", "abstract": "Graph Contrastive Learning (GCL) is a potent paradigm for self-supervised graph learning that has attracted attention across various application scenarios. However, GCL for learning on Text-Attributed Graphs (TAGs) has yet to be explored. Because \u2026"}, {"title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "link": "https://arxiv.org/pdf/2409.13853", "details": "Z Wang, R Bao, Y Wu, J Taylor, C Xiao, F Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pretrained large language models (LLMs) have revolutionized natural language processing (NLP) tasks such as summarization, question answering, and translation. However, LLMs pose significant security risks due to their tendency to memorize \u2026"}, {"title": "Judgment of Thoughts: Courtroom of the Binary Logical Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2409.16635", "details": "S Park, D Choi - arXiv preprint arXiv:2409.16635, 2024", "abstract": "This paper proposes a novel prompt engineering technique called Judgment of Thought (JoT) that is specifically tailored for binary logical reasoning tasks. JoT employs three roles $\\unicode {x2014} $ lawyer, prosecutor, and judge $\\unicode \u2026"}, {"title": "Target-Aware Language Modeling via Granular Data Sampling", "link": "https://arxiv.org/pdf/2409.14705", "details": "E Chang, PJ Lin, Y Li, C Zhao, D Kim, R Rabatin, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language model pretraining generally targets a broad range of use cases and incorporates data from diverse sources. However, there are instances where we desire a model that excels in specific areas without markedly compromising \u2026"}, {"title": "ZeroST: Zero-Shot Speech Translation", "link": "https://univ-lemans.hal.science/hal-04692601/document", "details": "S Khurana, C Hori, A Laurent, G Wichern, J Le Roux - Interspeech 2024, 2024", "abstract": "Our work introduces the Zero-Shot Speech Translation (ZeroST) framework, leveraging the synergistic potential of pre trained multilingual speech and text foundation models. Inspired by recent advances in multimodal foundation models \u2026"}, {"title": "The Use of Large Language Models Tuned with Socratic Methods on the Impact of Medical Students' Learning: A Randomised Controlled Trial", "link": "https://s3.ca-central-1.amazonaws.com/assets.jmir.org/assets/preprints/preprint-57995-submitted.pdf", "details": "CL Yong, MS Furqan, JWK Lee, A Makmur\u2026", "abstract": "Abstract Background: Large Language Models (LLM) are AI models that can generate conversational content based on a trained specified source of information (corpus). Objective: The aim is to use these corpus-trained LLMs to limit the content \u2026"}]
