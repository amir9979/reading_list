[{"title": "CapeLLM: Support-Free Category-Agnostic Pose Estimation with Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2411.06869", "details": "J Kim, H Chung, BH Kim - arXiv preprint arXiv:2411.06869, 2024", "abstract": "Category-agnostic pose estimation (CAPE) has traditionally relied on support images with annotated keypoints, a process that is often cumbersome and may fail to fully capture the necessary correspondences across diverse object categories. Recent \u2026"}, {"title": "Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training", "link": "https://arxiv.org/pdf/2411.15207", "details": "A Bawazir, K Wu, W Li - arXiv preprint arXiv:2411.15207, 2024", "abstract": "Recent advancements in vision-language pre-training via contrastive learning have significantly improved performance across computer vision tasks. However, in the medical domain, obtaining multimodal data is often costly and challenging due to \u2026"}, {"title": "LHRS-Bot-Nova: Improved Multimodal Large Language Model for Remote Sensing Vision-Language Interpretation", "link": "https://arxiv.org/pdf/2411.09301", "details": "Z Li, D Muhtar, F Gu, X Zhang, P Xiao, G He, X Zhu - arXiv preprint arXiv:2411.09301, 2024", "abstract": "Automatically and rapidly understanding Earth's surface is fundamental to our grasp of the living environment and informed decision-making. This underscores the need for a unified system with comprehensive capabilities in analyzing Earth's surface to \u2026"}, {"title": "What's in the Image? A Deep-Dive into the Vision of Vision Language Models", "link": "https://arxiv.org/abs/2411.17491", "details": "O Kaduri, S Bagon, T Dekel - arXiv preprint arXiv:2411.17491, 2024", "abstract": "Vision-Language Models (VLMs) have recently demonstrated remarkable capabilities in comprehending complex visual content. However, the mechanisms underlying how VLMs process visual information remain largely unexplored. In this \u2026"}, {"title": "Med-2E3: A 2D-Enhanced 3D Medical Multimodal Large Language Model", "link": "https://arxiv.org/pdf/2411.12783", "details": "Y Shi, X Zhu, Y Hu, C Guo, M Li, J Wu - arXiv preprint arXiv:2411.12783, 2024", "abstract": "The analysis of 3D medical images is crucial for modern healthcare, yet traditional task-specific models are becoming increasingly inadequate due to limited generalizability across diverse clinical scenarios. Multimodal large language models \u2026"}, {"title": "DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization", "link": "https://arxiv.org/pdf/2411.14055", "details": "H Deng, W Jiao, X Liu, M Zhang, Z Tu - arXiv preprint arXiv:2411.14055, 2024", "abstract": "Large language models (LLMs) deliver impressive results but face challenges from increasing model sizes and computational costs. Structured pruning reduces model size and speeds up inference but often causes uneven degradation across domains \u2026"}, {"title": "Training and Evaluating Language Models with Template-based Data Generation", "link": "https://arxiv.org/pdf/2411.18104", "details": "Y Zhang - arXiv preprint arXiv:2411.18104, 2024", "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, these \u2026"}, {"title": "Efficient Transfer Learning for Video-language Foundation Models", "link": "https://arxiv.org/pdf/2411.11223", "details": "H Chen, Z Huang, Y Hong, Y Wang, Z Lyu, Z Xu, J Lan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained vision-language models provide a robust foundation for efficient transfer learning across various downstream tasks. In the field of video action recognition, mainstream approaches often introduce additional parameter modules to capture \u2026"}]
