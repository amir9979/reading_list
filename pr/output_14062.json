[{"title": "PLM: Efficient Peripheral Language Models Hardware-Co-Designed for Ubiquitous Computing", "link": "https://arxiv.org/pdf/2503.12167", "details": "C Deng, L Sun, J Jiang, Y Zeng, X Wu, W Zhao, Q Xiao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While scaling laws have been continuously validated in large language models (LLMs) with increasing model parameters, the inherent tension between the inference demands of LLMs and the limited resources of edge devices poses a \u2026"}, {"title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information", "link": "https://arxiv.org/pdf/2502.14258", "details": "Y Park, C Yoon, J Park, M Jeong, J Kang - arXiv preprint arXiv:2502.14258, 2025", "abstract": "While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads primarily responsible for processing \u2026"}, {"title": "ASIDE: Architectural Separation of Instructions and Data in Language Models", "link": "https://arxiv.org/pdf/2503.10566", "details": "E Zverev, E Kortukov, A Panfilov, S Tabesh, A Volkova\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their remarkable performance, large language models lack elementary safety features, and this makes them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation \u2026"}, {"title": "Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models", "link": "https://arxiv.org/pdf/2503.12072", "details": "A Ravichander, J Fisher, T Sorensen, X Lu, Y Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-quality training data has proven crucial for developing performant large language models (LLMs). However, commercial LLM providers disclose few, if any, details about the data used for training. This lack of transparency creates multiple \u2026"}, {"title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2503.06749", "details": "W Huang, B Jia, Z Zhai, S Cao, Z Ye, F Zhao, Y Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning \u2026"}, {"title": "QA-Calibration of language model confidence scores", "link": "https://www.amazon.science/publications/qa-calibration-of-language-model-confidence-scores", "details": "A Mastakouri, E Kirschbaum, S Kasiviswanathan\u2026 - 2025", "abstract": "To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim \u2026"}, {"title": "R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization", "link": "https://arxiv.org/pdf/2503.12937", "details": "J Zhang, J Huang, H Yao, S Liu, X Zhang, S Lu, D Tao - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised fine- tuning on high-quality chain-of-thought reasoning data, which often leads models to merely imitate successful reasoning paths without understanding what the wrong \u2026"}, {"title": "XIFBench: Evaluating Large Language Models on Multilingual Instruction Following", "link": "https://arxiv.org/pdf/2503.07539%3F", "details": "Z Li, K Chen, Y Long, X Bai, Y Zhang, X Wei, J Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have demonstrated remarkable instruction-following capabilities across various applications. However, their performance in multilingual settings remains poorly understood, as existing evaluations lack fine-grained \u2026"}, {"title": "DAST: Difficulty-Aware Self-Training on Large Language Models", "link": "https://arxiv.org/pdf/2503.09029", "details": "B Xue, Q Zhu, H Wang, R Wang, S Wang, H Xu, F Mi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Present Large Language Models (LLM) self-training methods always under-sample on challenging queries, leading to inadequate learning on difficult problems which limits LLMs' ability. Therefore, this work proposes a difficulty-aware self-training \u2026"}]
