[{"title": "MegaCOIN: Enhancing Medium-Grained Color Perception for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.03927", "details": "MC Chiu, S Wen, PY Chen, X Ma - arXiv preprint arXiv:2412.03927, 2024", "abstract": "In vision-language models (VLMs), the ability to perceive and interpret color and physical environment is crucial for achieving contextually accurate understanding and interaction. However, despite advances in multimodal modeling, there remains a \u2026"}, {"title": "HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.08378", "details": "S Zhu, W Dong, J Song, Y Guo, B Zheng - arXiv preprint arXiv:2412.08378, 2024", "abstract": "Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into \u2026"}, {"title": "Deeper evaluation of a single-cell foundation model", "link": "https://www.nature.com/articles/s42256-024-00949-w", "details": "R Boiarsky, NM Singh, A Buendia, AP Amini, G Getz\u2026 - Nature Machine Intelligence, 2024", "abstract": "Large-scale foundation models, which are pre-trained on massive, unlabelled datasets and subsequently fine-tuned on specific tasks, have recently achieved unparalleled success on a wide array of applications, including in healthcare and \u2026"}, {"title": "Training Agents with Weakly Supervised Feedback from Large Language Models", "link": "https://arxiv.org/pdf/2411.19547", "details": "D Gong, P Lu, Z Wang, M Zhou, X He - arXiv preprint arXiv:2411.19547, 2024", "abstract": "Large Language Models (LLMs) offer a promising basis for creating agents that can tackle complex tasks through iterative environmental interaction. Existing methods either require these agents to mimic expert-provided trajectories or rely on definitive \u2026"}, {"title": "Efficient Fine-Tuning of Single-Cell Foundation Models Enables Zero-Shot Molecular Perturbation Prediction", "link": "https://arxiv.org/pdf/2412.13478", "details": "S Maleki, JC Huetter, KV Chuang, G Scalia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Predicting transcriptional responses to novel drugs provides a unique opportunity to accelerate biomedical research and advance drug discovery efforts. However, the inherent complexity and high dimensionality of cellular responses, combined with the \u2026"}, {"title": "Reliability in AI-Assisted Critical Care: Assessing Large Language Model Robustness and Instruction Following for Cardiac Arrest Identification", "link": "https://openreview.net/pdf%3Fid%3DpsOWQZbI6Z", "details": "U Vurgun, S Hwang, DL Mowery - Advancements In Medical Foundation Models \u2026", "abstract": "This study systematically evaluates the performance, robustness, and instruction- following capabilities of large language models (LLMs) in identifying in-hospital cardiac arrest (IHCA) events. We assessed 51 open-source LLMs\u2014comprising 36 \u2026"}, {"title": "RedStone: Curating general, code, math, and QA data for large language models", "link": "https://arxiv.org/pdf/2412.03398", "details": "Y Chang, L Cui, L Dong, S Huang, Y Huang, Y Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-training Large Language Models (LLMs) on high-quality, meticulously curated datasets is widely recognized as critical for enhancing their performance and generalization capabilities. This study explores the untapped potential of Common \u2026"}, {"title": "scReader: Prompting Large Language Models to Interpret scRNA-seq Data", "link": "https://arxiv.org/pdf/2412.18156", "details": "C Li, Q Long, Y Zhou, M Xiao - arXiv preprint arXiv:2412.18156, 2024", "abstract": "Large language models (LLMs) have demonstrated remarkable advancements, primarily due to their capabilities in modeling the hidden relationships within text sequences. This innovation presents a unique opportunity in the field of life sciences \u2026"}, {"title": "Breaking the Stage Barrier: A Novel Single-Stage Approach to Long Context Extension for Large Language Models", "link": "https://arxiv.org/pdf/2412.07171", "details": "H Lian, J Chen, W Huang, Y Xiong, W Hu, G Ding\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, Large language models (LLMs) have revolutionized Natural Language Processing (NLP). Pretrained LLMs, due to limited training context size, struggle with handling long token sequences, limiting their performance on various downstream \u2026"}]
