[{"title": "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.221.pdf", "details": "S Jiang, T Zheng, Y Zhang, Y Jin, L Yuan, Z Liu - Findings of the Association for \u2026, 2024", "abstract": "Recent advancements in general-purpose or domain-specific multimodal large language models (LLMs) have witnessed remarkable progress for medical decision- making. However, they are designated for specific classification or generative tasks \u2026"}, {"title": "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.1249.pdf", "details": "X Lin, M Li, R Zemel, H Ji, SF Chang - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "Recently, enabling pretrained language models (PLMs) to perform zero-shot crossmodal tasks such as video question answering has been extensively studied. A popular approach is to learn a projection network that projects visual features into the \u2026"}, {"title": "LM2: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.920.pdf", "details": "G Juneja, S Dutta, T Chakraborty - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}, {"title": "Mixed Distillation Helps Smaller Language Models Reason Better", "link": "https://aclanthology.org/2024.findings-emnlp.91.pdf", "details": "L Chenglin, Q Chen, L Li, C Wang, F Tao, Y Li, Z Chen\u2026 - Findings of the Association \u2026, 2024", "abstract": "As large language models (LLMs) have demonstrated impressive multiple step-by- step reasoning capabilities in recent natural language processing (NLP) reasoning tasks, many studies are interested in distilling reasoning abilities into smaller \u2026"}, {"title": "SearchLVLMs: A Plug-and-Play Framework for Augmenting Large Vision-Language Models by Searching Up-to-Date Internet Knowledge", "link": "https://openreview.net/pdf%3Fid%3Dleeosk2RAM", "details": "C Li, Z Li, C Jing, S Liu, W Shao, Y Wu, P Luo, Y Qiao\u2026 - The Thirty-eighth Annual \u2026, 2024", "abstract": "Large vision-language models (LVLMs) are ignorant of the up-to-date knowledge, such as LLaVA series, because they cannot be updated frequently due to the large amount of resources required, and therefore fail in many cases. For example, if a \u2026"}, {"title": "Improving Adversarial Robustness in Vision-Language Models with Architecture and Prompt Design", "link": "https://aclanthology.org/2024.findings-emnlp.990.pdf", "details": "R Bhagwatkar, S Nayak, P Bashivan, I Rish - Findings of the Association for \u2026, 2024", "abstract": "Abstract Vision-Language Models (VLMs) have seen a significant increase in both research interest and real-world applications across various domains, including healthcare, autonomous systems, and security. However, their growing prevalence \u2026"}, {"title": "Limits of transformer language models on learning to compose algorithms", "link": "https://openreview.net/pdf%3Fid%3Dx7AD0343Jz", "details": "J Thomm, G Camposampiero, A Terzic, M Hersche\u2026 - The Thirty-eighth Annual \u2026, 2024", "abstract": "We analyze the capabilities of Transformer language models in learning compositional discrete tasks. To this end, we evaluate training LLaMA models and prompting GPT-4 and Gemini on four tasks demanding to learn a composition of \u2026"}, {"title": "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.110.pdf", "details": "D Hao, Q Wang, L Guo, J Jiang, J Liu - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "While large pre-trained visual-language models have shown promising results on traditional visual question answering benchmarks, it is still challenging for them to answer complex VQA problems which requires diverse world knowledge. Motivated \u2026"}, {"title": "DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing", "link": "https://aclanthology.org/2024.emnlp-main.1132.pdf", "details": "D Das, V Khetan - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine- tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data \u2026"}]
