[{"title": "Are MLMs Trapped in the Visual Room?", "link": "https://arxiv.org/pdf/2505.23272", "details": "Y Zhang, C Zou, Q Liu, L Rong, B Yao, Z Lian, Q Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Can multi-modal large models (MLMs) that can``see''an image be said to``understand''it? Drawing inspiration from Searle's Chinese Room, we propose the\\textbf {Visual Room} argument: a system may process and describe every detail \u2026", "entry_id": "http://arxiv.org/abs/2505.23272v2", "updated": "2025-05-30 14:14:00", "published": "2025-05-29 09:20:12", "authors": "Yazhou Zhang;Chunwang Zou;Qimeng Liu;Lu Rong;Ben Yao;Zheng Lian;Qiuchi Li;Peng Zhang;Jing Qin", "summary": "Can multi-modal large models (MLMs) that can ``see'' an image be said to\n``understand'' it? Drawing inspiration from Searle's Chinese Room, we propose\nthe \\textbf{Visual Room} argument: a system may process and describe every\ndetail of visual inputs by following algorithmic rules, without genuinely\ncomprehending the underlying intention. This dilemma challenges the prevailing\nassumption that perceptual mastery implies genuine understanding. In\nimplementation, we introduce a two-tier evaluation framework spanning\nperception and cognition. The perception component evaluates whether MLMs can\naccurately capture the surface-level details of visual contents, where the\ncognitive component examines their ability to infer sarcasm polarity. To\nsupport this framework, We further introduce a high-quality multi-modal sarcasm\ndataset comprising both 924 static images and 100 dynamic videos. All sarcasm\nlabels are annotated by the original authors and verified by independent\nreviewers to ensure clarity and consistency. We evaluate eight state-of-the-art\n(SoTA) MLMs. Our results highlight three key findings: (1) MLMs demonstrate\nhigh accuracy in visual perception; (2) even with correct perception, MLMs\nexhibit an average error rate of ~17.1\\% in sarcasm understanding, revealing a\nsignificant gap between seeing and understanding; (3) this gap stems from\nweaknesses in context integration, emotional reasoning, and pragmatic\ninference. This work provides empirical grounding for the proposed Visual Room\nargument and offers a new evaluation paradigm for MLMs.", "comment": "19 pages", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.23272v2;http://arxiv.org/pdf/2505.23272v2", "pdf_url": "http://arxiv.org/pdf/2505.23272v2"}, {"title": "Sparsity-Driven Parallel Imaging Consistency for Improved Self-Supervised MRI Reconstruction", "link": "https://arxiv.org/pdf/2505.24136", "details": "YU Al\u00e7alar, M Ak\u00e7akaya - arXiv preprint arXiv:2505.24136, 2025", "abstract": "Physics-driven deep learning (PD-DL) models have proven to be a powerful approach for improved reconstruction of rapid MRI scans. In order to train these models in scenarios where fully-sampled reference data is unavailable, self \u2026", "entry_id": "http://arxiv.org/abs/2505.24136v1", "updated": "2025-05-30 02:11:25", "published": "2025-05-30 02:11:25", "authors": "Ya\u015far Utku Al\u00e7alar;Mehmet Ak\u00e7akaya", "summary": "Physics-driven deep learning (PD-DL) models have proven to be a powerful\napproach for improved reconstruction of rapid MRI scans. In order to train\nthese models in scenarios where fully-sampled reference data is unavailable,\nself-supervised learning has gained prominence. However, its application at\nhigh acceleration rates frequently introduces artifacts, compromising image\nfidelity. To mitigate this shortcoming, we propose a novel way to train PD-DL\nnetworks via carefully-designed perturbations. In particular, we enhance the\nk-space masking idea of conventional self-supervised learning with a novel\nconsistency term that assesses the model's ability to accurately predict the\nadded perturbations in a sparse domain, leading to more reliable and\nartifact-free reconstructions. The results obtained from the fastMRI knee and\nbrain datasets show that the proposed training strategy effectively reduces\naliasing artifacts and mitigates noise amplification at high acceleration\nrates, outperforming state-of-the-art self-supervised methods both visually and\nquantitatively.", "comment": "IEEE International Conference on Image Processing (ICIP), 2025", "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.AI;cs.CV;cs.LG;physics.med-ph", "links": "http://arxiv.org/abs/2505.24136v1;http://arxiv.org/pdf/2505.24136v1", "pdf_url": "http://arxiv.org/pdf/2505.24136v1"}]
