[{"title": "Personalizing Low-Rank Bayesian Neural Networks Via Federated Learning", "link": "https://arxiv.org/pdf/2410.14390", "details": "B Zhang, D Liu, O Simeone, G Wang, D Pezaros, G Zhu - arXiv preprint arXiv \u2026, 2024", "abstract": "To support real-world decision-making, it is crucial for models to be well-calibrated, ie, to assign reliable confidence estimates to their predictions. Uncertainty quantification is particularly important in personalized federated learning (PFL), as \u2026"}, {"title": "Enhancing Few-Shot Out-of-Distribution Detection with Pre-Trained Model Features", "link": "https://ieeexplore.ieee.org/abstract/document/10735106/", "details": "J Dong, Y Yao, W Jin, H Zhou, Y Gao, Z Fang - IEEE Transactions on Image \u2026, 2024", "abstract": "Ensuring the reliability of open-world intelligent systems heavily relies on effective out-of-distribution (OOD) detection. Despite notable successes in existing OOD detection methods, their performance in scenarios with limited training samples is \u2026"}, {"title": "Bi-Branching Feature Interaction Representation Learning for Multivariate Time Series", "link": "https://www.sciencedirect.com/science/article/pii/S1568494624011578", "details": "W Wang, E Zuo, R Wang, J Zhong, C Chen, C Chen\u2026 - Applied Soft Computing, 2024", "abstract": "Representational learning of time series plays a crucial role in various fields. However, existing time-series models do not perform well in representation learning. These models usually focus only on the relationship between variables at the same \u2026"}, {"title": "A personalized federated cloud-edge collaboration framework via cross-client knowledge distillation", "link": "https://www.sciencedirect.com/science/article/pii/S0167739X24005582", "details": "S Zhang, X Wang, R Zeng, C Zeng, Y Li, M Huang - Future Generation Computer \u2026, 2024", "abstract": "As an emerging distributed machine learning paradigm, federated learning has been extensively used in the domain of cloud\u2013edge computing to collaboratively train models without uploading their raw data. However, the existing federated learning \u2026"}, {"title": "ELBOing Stein: Variational Bayes with Stein Mixture Inference", "link": "https://arxiv.org/pdf/2410.22948%3F", "details": "O R\u00f8nning, E Nalisnick, C Ley, P Smyth, T Hamelryck - arXiv preprint arXiv \u2026, 2024", "abstract": "Stein variational gradient descent (SVGD)[Liu and Wang, 2016] performs approximate Bayesian inference by representing the posterior with a set of particles. However, SVGD suffers from variance collapse, ie poor predictions due to \u2026"}, {"title": "Improving Out-of-Distribution Detection with Disentangled Foreground and Background Features", "link": "https://dl.acm.org/doi/pdf/10.1145/3664647.3681614", "details": "C Ding, G Pang - Proceedings of the 32nd ACM International \u2026, 2024", "abstract": "Detecting out-of-distribution (OOD) inputs is a principal task for ensuring the safety of deploying deep-neural-network classifiers in open-set scenarios. OOD samples can be drawn from arbitrary distributions and exhibit deviations from in-distribution (ID) \u2026"}, {"title": "Dynamic Contrastive Learning for Time Series Representation", "link": "https://arxiv.org/pdf/2410.15416", "details": "AK Shamba, K Bach, G Taylor - arXiv preprint arXiv:2410.15416, 2024", "abstract": "Understanding events in time series is an important task in a variety of contexts. However, human analysis and labeling are expensive and time-consuming. Therefore, it is advantageous to learn embeddings for moments in time series in an \u2026"}, {"title": "FedGKD: personalized federated learning through grouping and distillation", "link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13404/134040E/FedGKD-personalized-federated-learning-through-grouping-and-distillation/10.1117/12.3050605.short", "details": "T Li, S Lin, P Zhao, Z Li, J Wang - Fifth International Conference on Control, Robotics \u2026, 2024", "abstract": "As is well known, the objective of traditional Federated Learning (FL) is to train a global model collaboratively across multiple clients without directly accessing client data. However, traditional federated learning is frequently impeded by the \u2026"}]
