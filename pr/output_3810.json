[{"title": "Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization", "link": "https://arxiv.org/pdf/2407.07880", "details": "J Wu, Y Xie, Z Yang, J Wu, J Chen, J Gao, B Ding\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study addresses the challenge of noise in training datasets for Direct Preference Optimization (DPO), a method for aligning Large Language Models (LLMs) with human preferences. We categorize noise into pointwise noise, which includes low \u2026"}, {"title": "Concise and Precise Context Compression for Tool-Using Language Models", "link": "https://arxiv.org/pdf/2407.02043", "details": "Y Xu, Y Feng, H Mu, Y Hou, Y Li, X Wang, W Zhong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Through reading the documentation in the context, tool-using language models can dynamically extend their capability using external tools. The cost is that we have to input lengthy documentation every time the model needs to use the tool, occupying \u2026"}, {"title": "Aligning Language Models with the Human World", "link": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi%3Farticle%3D1241%26context%3Ddissertations", "details": "R LIU - 2024", "abstract": "Abstract The field of Natural Language Processing (NLP) has undergone a significant transformation with the emergence of large language models (LMs). These models have enabled the development of human-like conversational \u2026"}, {"title": "Training on the Test Task Confounds Evaluation and Emergence", "link": "https://arxiv.org/pdf/2407.07890", "details": "R Dominguez-Olmedo, FE Dorner, M Hardt - arXiv preprint arXiv:2407.07890, 2024", "abstract": "We study a fundamental problem in the evaluation of large language models that we call training on the test task. Unlike wrongful practices like training on the test data, leakage, or data contamination, training on the test task is not a malpractice. Rather \u2026"}, {"title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models", "link": "https://arxiv.org/pdf/2407.02408", "details": "S Wang, P Wang, T Zhou, Y Dong, Z Tan, J Li - arXiv preprint arXiv:2407.02408, 2024", "abstract": "As Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases \u2026"}, {"title": "Igea: a Decoder-Only Language Model for Biomedical Text Generation in Italian", "link": "https://arxiv.org/pdf/2407.06011", "details": "TM Buonocore, S Rancati, E Parimbelli - arXiv preprint arXiv:2407.06011, 2024", "abstract": "The development of domain-specific language models has significantly advanced natural language processing applications in various specialized fields, particularly in biomedicine. However, the focus has largely been on English-language models \u2026"}, {"title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application", "link": "https://arxiv.org/pdf/2407.01885", "details": "C Yang, W Lu, Y Zhu, Y Wang, Q Chen, C Gao, B Yan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have showcased exceptional capabilities in various domains, attracting significant interest from both academia and industry. Despite their impressive performance, the substantial size and computational demands of LLMs \u2026"}, {"title": "CFinBench: A Comprehensive Chinese Financial Benchmark for Large Language Models", "link": "https://arxiv.org/pdf/2407.02301", "details": "Y Nie, B Yan, T Guo, H Liu, H Wang, W He, B Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have achieved remarkable performance on various NLP tasks, yet their potential in more challenging and domain-specific task, such as finance, has not been fully explored. In this paper, we present CFinBench: a \u2026"}, {"title": "Advancing Faithfulness of Large Language Models in Goal-Oriented Dialogue Question Answering", "link": "https://dl.acm.org/doi/abs/10.1145/3640794.3665573", "details": "A Sticha, N Braunschweiler, RS Doddipatla, KM Knill - \u2026 of the 6th ACM Conference on \u2026, 2024", "abstract": "Goal-oriented dialogue systems, such as assistant chatbots and conversational AI systems, have gained prominence for their question-answering capabilities, often utilizing large language models (LLMs) as knowledge bases. However, these \u2026"}]
