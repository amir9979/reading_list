'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Dense Training, Sparse Inference: Rethinking Training '
[{"title": "Self-Training Based Few-Shot Node Classification by Knowledge Distillation", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29530/30881", "details": "Z Wu, Y Mo, P Zhou, S Yuan, X Zhu - Proceedings of the AAAI Conference on Artificial \u2026, 2024", "abstract": "Self-training based few-shot node classification (FSNC) methods have shown excellent performance in real applications, but they cannot make the full use of the information in the base set and are easily affected by the quality of pseudo-labels. To \u2026"}, {"title": "SPContrastNet: A Self-Paced Contrastive Learning Model for Few-Shot Text Classification", "link": "https://dl.acm.org/doi/pdf/10.1145/3652600", "details": "J Chen, R Zhang, X Jiang, C Hu - ACM Transactions on Information Systems, 2024", "abstract": "Meta-learning has recently promoted few-shot text classification, which identifies target classes based on information transferred from source classes through a series of small tasks or episodes. Existing works constructing their meta-learner on \u2026"}, {"title": "ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models", "link": "https://arxiv.org/pdf/2403.16187", "details": "Z Liu, J Lyn, W Zhu, X Tian, Y Graham - arXiv preprint arXiv:2403.16187, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models. Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method \u2026"}, {"title": "Analyzing the Performance of Large Language Models on Code Summarization", "link": "https://arxiv.org/pdf/2404.08018", "details": "R Haldar, J Hockenmaier - arXiv preprint arXiv:2404.08018, 2024", "abstract": "Large language models (LLMs) such as Llama 2 perform very well on tasks that involve both natural language and source code, particularly code summarization and code generation. We show that for the task of code summarization, the performance \u2026"}]
