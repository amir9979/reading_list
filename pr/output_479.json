'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [MFORT-QA: Multi-hop Few-shot Open Rich Table Question '
[{"title": "Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models", "link": "https://arxiv.org/pdf/2403.12966", "details": "Z Liu, Y Dong, Y Rao, J Zhou, J Lu - arXiv preprint arXiv:2403.12966, 2024", "abstract": "In the realm of vision-language understanding, the proficiency of models in interpreting and reasoning over visual content has become a cornerstone for numerous applications. However, it is challenging for the visual encoder in Large \u2026"}, {"title": "Language models scale reliably with over-training and on downstream tasks", "link": "https://arxiv.org/pdf/2403.08540", "details": "SY Gadre, G Smyrnis, V Shankar, S Gururangan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute \u2026"}, {"title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models", "link": "https://arxiv.org/pdf/2403.18814", "details": "Y Li, Y Zhang, C Wang, Z Zhong, Y Chen, R Chu, S Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared \u2026"}, {"title": "Learn\" No\" to Say\" Yes\" Better: Improving Vision-Language Models via Negations", "link": "https://arxiv.org/html/2403.20312v1", "details": "J Singh, I Shrivastava, M Vatsa, R Singh, A Bharati - arXiv preprint arXiv:2403.20312, 2024", "abstract": "Existing vision-language models (VLMs) treat text descriptions as a unit, confusing individual concepts in a prompt and impairing visual semantic matching and reasoning. An important aspect of reasoning in logic and language is negations. This \u2026"}, {"title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models", "link": "https://arxiv.org/pdf/2403.20262", "details": "T Thonet, J Rozen, L Besacier - arXiv preprint arXiv:2403.20262, 2024", "abstract": "Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities \u2026"}, {"title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models", "link": "https://arxiv.org/pdf/2403.08281", "details": "N Ding, Y Chen, G Cui, X Lv, R Xie, B Zhou, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three \u2026"}, {"title": "Conceptual and Unbiased Reasoning in Language Models", "link": "https://arxiv.org/pdf/2404.00205", "details": "B Zhou, H Zhang, S Chen, D Yu, H Wang, B Peng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Conceptual reasoning, the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on large language models' capability to perform conceptual reasoning. In this work, we \u2026"}, {"title": "Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity", "link": "https://arxiv.org/html/2403.09428v1", "details": "Z Zhi, Z Liu, M Elbadawi, A Daneshmend, M Orlu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal machine learning with missing modalities is an increasingly relevant challenge arising in various applications such as healthcare. This paper extends the current research into missing modalities to the low-data regime, ie, a downstream \u2026"}, {"title": "DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.08857", "details": "M Huang, Y Long, X Deng, R Chu, J Xiong, X Liang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Text-to-image (T2I) generation models have significantly advanced in recent years. However, effective interaction with these models is challenging for average users due to the need for specialized prompt engineering knowledge and the inability to \u2026"}]
