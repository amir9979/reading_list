[{"title": "Towards Robustness Prompt Tuning with Fully Test-Time Adaptation for CLIP's Zero-Shot Generalization", "link": "https://openreview.net/pdf%3Fid%3DBVFAVis7ui", "details": "R Wang, H Zuo, Z Fang, J Lu - ACM Multimedia 2024", "abstract": "In the field of Vision-Language Models (VLM), the Contrastive Language-Image Pretraining (CLIP) model has yielded outstanding performance on many downstream tasks through prompt tuning. By integrating image and text representations, CLIP \u2026"}, {"title": "Multi-view Self-Supervised Contrastive Learning for Multivariate Time Series", "link": "https://openreview.net/pdf%3Fid%3DBgjLy3chju", "details": "Y Wu, X Meng, Y He, J Zhang, H Zhang, Y Dong, D Lu - ACM Multimedia 2024", "abstract": "Learning semantic-rich representations from unlabeled time series data with intricate dynamics is a notable challenge. Traditional contrastive learning techniques predominantly focus on segment-level augmentations through time slicing, a practice \u2026"}, {"title": "Learning by imitating the classics: Mitigating class imbalance in federated learning via simulated centralized learning", "link": "https://www.sciencedirect.com/science/article/pii/S0957417424016221", "details": "G Zhu, X Liu, J Niu, Y Wei, S Tang, J Zhang - Expert Systems with Applications, 2024", "abstract": "Federated learning (FL) is a distributed machine learning framework in which multiple clients update their local models in parallel and then aggregate them to generate a global model. However, when local data on different clients are class \u2026"}, {"title": "Towards Zero-Shot Generalization in Offline Reinforcement Learning", "link": "https://openreview.net/pdf%3Fid%3DnHcw4Z5VDd", "details": "Z Wang, C Yang, JCS Lui, D Zhou - ICML 2024 Workshop: Aligning Reinforcement \u2026", "abstract": "In this work, we study offline reinforcement learning (RL) with zero-shot generalization property (ZSG), where the agent has access to an offline dataset including experiences from different environments, and the goal of the agent is to \u2026"}, {"title": "FMamba: Mamba based on Fast-attention for Multivariate Time-series Forecasting", "link": "https://arxiv.org/pdf/2407.14814", "details": "S Ma, Y Kang, P Bai, YB Zhao - arXiv preprint arXiv:2407.14814, 2024", "abstract": "In multivariate time-series forecasting (MTSF), extracting the temporal correlations of the input sequences is crucial. While popular Transformer-based predictive models can perform well, their quadratic computational complexity results in inefficiency and \u2026"}]
