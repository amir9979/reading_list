[{"title": "Conducting patch contrastive learning with mixture of experts on mixed datasets for medical image segmentation", "link": "https://link.springer.com/article/10.1007/s00521-025-11234-1", "details": "J Wang, O Yoshie, Y Ieiri - Neural Computing and Applications, 2025", "abstract": "Medical image segmentation is critical for accurate diagnosis, treatment planning, and surgical navigation. In recent years, large multitask segmentation models have often struggled due to the limited size of datasets and significant variability in target \u2026"}, {"title": "Automated Grading Through Contrastive Learning: A Gradient Analysis and Feature Ablation Approach", "link": "https://www.mdpi.com/2504-4990/7/2/41", "details": "M Soka\u010d, M Fabijani\u0107, I Mekterovi\u0107, L Mr\u0161i\u0107 - Machine Learning and Knowledge \u2026, 2025", "abstract": "As programming education becomes increasingly complex, grading student code has become a challenging task. Traditional methods, such as dynamic and static analysis, offer foundational approaches but often fail to provide granular insights \u2026"}, {"title": "Teaching Large Language Models to Reason through Learning and Forgetting", "link": "https://arxiv.org/pdf/2504.11364%3F", "details": "T Ni, A Nie, S Chaudhary, Y Liu, H Rangwala, R Fakoor - arXiv preprint arXiv \u2026, 2025", "abstract": "Leveraging inference-time search in large language models has proven effective in further enhancing a trained model's capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational \u2026"}, {"title": "Unveiling Hidden Collaboration within Mixture-of-Experts in Large Language Models", "link": "https://arxiv.org/pdf/2504.12359", "details": "Y Tang, Y Tang, N Zhang, M Chen, Y Li - arXiv preprint arXiv:2504.12359, 2025", "abstract": "Mixture-of-Experts based large language models (MoE LLMs) have shown significant promise in multitask adaptability by dynamically routing inputs to specialized experts. Despite their success, the collaborative mechanisms among \u2026"}, {"title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models", "link": "https://arxiv.org/pdf/2504.10449%3F", "details": "J Wang, WD Li, D Paliotta, D Ritter, AM Rush, T Dao - arXiv preprint arXiv:2504.10449, 2025", "abstract": "Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based \u2026"}]
