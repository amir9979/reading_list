[{"title": "Language Models Encode the Value of Numbers Linearly", "link": "https://aclanthology.org/2025.coling-main.47.pdf", "details": "F Zhu, D Dai, Z Sui - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Large language models (LLMs) have exhibited impressive competence in various tasks, but their internal mechanisms on mathematical problems are still under- explored. In this paper, we study a fundamental question: how language models \u2026"}, {"title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models", "link": "https://arxiv.org/pdf/2501.05752", "details": "S Lee, H Park, J Kim, J Ok - arXiv preprint arXiv:2501.05752, 2025", "abstract": "Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer \u2026"}, {"title": "Factual Knowledge Assessment of Language Models Using Distractors", "link": "https://aclanthology.org/2025.coling-main.537.pdf", "details": "HA Khodja, F Bechet, Q Brabant, A Nasr, G Lecorv\u00e9 - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract Language models encode extensive factual knowledge within their parameters. The accurate assessment of this knowledge is crucial for understanding and improving these models. In the literature, factual knowledge assessment often \u2026"}, {"title": "Evaluating the effectiveness of XAI techniques for encoder-based language models", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125000899", "details": "MA Mersha, MG Yigezu, J Kalita - Knowledge-Based Systems, 2025", "abstract": "The black-box nature of large language models (LLMs) necessitates the development of eXplainable AI (XAI) techniques for transparency and trustworthiness. However, evaluating these techniques remains a challenge. This \u2026"}, {"title": "Foundations of Large Language Models", "link": "https://arxiv.org/pdf/2501.09223", "details": "T Xiao, J Zhu - arXiv preprint arXiv:2501.09223, 2025", "abstract": "This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting- edge technologies. The book is structured into four main chapters, each exploring a \u2026"}, {"title": "Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective", "link": "https://aclanthology.org/2025.coling-main.212.pdf", "details": "T Li, Z Wang, W Liu, M Wu, S Dou, C Lv, X Wang\u2026 - Proceedings of the 31st \u2026, 2025", "abstract": "The recent surge in jailbreaking attacks has revealed significant vulnerabilities in Large Language Models (LLMs) when exposed to malicious inputs. While various defense strategies have been proposed to mitigate these threats, there has been \u2026"}, {"title": "Cascaded Self-Evaluation Augmented Training for Efficient Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2501.05662", "details": "Z Lv, W Wang, J Wang, S Zhang, F Wu - arXiv preprint arXiv:2501.05662, 2025", "abstract": "Efficient Multimodal Large Language Models (EMLLMs) have rapidly advanced recently. Incorporating Chain-of-Thought (CoT) reasoning and step-by-step self- evaluation has improved their performance. However, limited parameters often \u2026"}, {"title": "Topology-of-Question-Decomposition: Enhancing Large Language Models with Information Retrieval for Knowledge-Intensive Tasks", "link": "https://aclanthology.org/2025.coling-main.191.pdf", "details": "W Li, J Wang, LC Yu, X Zhang - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Large language models (LLMs) are increasingly deployed for general problem- solving across various domains yet remain constrained to chaining immediate reasoning steps and depending solely on parametric knowledge. Integrating an \u2026"}, {"title": "Chain-of-Specificity: Enhancing Task-Specific Constraint Adherence in Large Language Models", "link": "https://aclanthology.org/2025.coling-main.164.pdf", "details": "K Wei, J Zhong, H Zhang, F Zhang, D Zhang, L Jin\u2026 - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract Large Language Models (LLMs) exhibit remarkable generative capabilities, enabling the generation of valuable information. Despite these advancements, previous research found that LLMs sometimes struggle with adhering to specific \u2026"}]
