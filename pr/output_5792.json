[{"title": "CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning", "link": "https://arxiv.org/pdf/2407.21011", "details": "Y Du, B Chang, NC Dvornek - arXiv preprint arXiv:2407.21011, 2024", "abstract": "Recent advancements in Contrastive Language-Image Pre-training (CLIP) have demonstrated notable success in self-supervised representation learning across various tasks. However, the existing CLIP-like approaches often demand extensive \u2026"}, {"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "link": "https://arxiv.org/pdf/2408.00724", "details": "Y Wu, Z Sun, S Li, S Welleck, Y Yang - arXiv preprint arXiv:2408.00724, 2024", "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth \u2026"}, {"title": "Fine-Tuning Pre-Trained Language Models with Gaze Supervision", "link": "https://aclanthology.org/2024.acl-short.21.pdf", "details": "S Deng, P Prasse, D Reich, T Scheffer, L J\u00e4ger - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Human gaze data provide cognitive information that reflect human language comprehension and has been effectively integrated into a variety of natural language processing (NLP) tasks, demonstrating improved performance over corresponding \u2026"}, {"title": "MMDG-DTI: Drug target interaction prediction via multimodal feature fusion and domain generalization", "link": "https://www.sciencedirect.com/science/article/pii/S0031320324006381", "details": "Y Hua, Z Feng, X Song, XJ Wu, J Kittler - Pattern Recognition, 2024", "abstract": "Recently, deep learning has become the essential methodology for Drug-Target Interaction (DTI) prediction. However, the existing learning-based representation methods embed the prior knowledge encapsulated by the training data and, as such \u2026"}, {"title": "Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?", "link": "https://arxiv.org/pdf/2408.02651", "details": "MB Karkevandi, N Vishwamitra, P Najafirad - arXiv preprint arXiv:2408.02651, 2024", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in natural language tasks, but their safety and morality remain contentious due to their training on internet text corpora. To address these concerns, alignment techniques \u2026"}, {"title": "Race, Gender, and Donor Heart Acceptance\u2014Reply", "link": "https://jamanetwork.com/journals/jama/article-abstract/2822179", "details": "K Breathett, SM Knapp - JAMA", "abstract": "In Reply Ms Majeed and colleagues pose important questions regarding our study. 1 First, the COVID-19 pandemic had several possible implications for transplant care. To explore these implications, we reexamined the number of donor heart offers until \u2026"}, {"title": "MixPrompt: Enhancing Generalizability and Adversarial Robustness for Vision-Language Models via Prompt Fusion", "link": "https://link.springer.com/chapter/10.1007/978-981-97-5606-3_28", "details": "H Fan, Z Ma, Y Li, R Tian, Y Chen, C Gao - International Conference on Intelligent \u2026, 2024", "abstract": "Abstract Pretrained Vision-Language Models (VLMs) like CLIP have exhibited remarkable capacities across downstream tasks, while their image encoders are vulnerable to adversarial examples. A recently introduced lightweight approach \u2026"}, {"title": "Coalitions of Large Language Models Increase the Robustness of AI Agents", "link": "https://arxiv.org/pdf/2408.01380", "details": "P Mangal, C Mak, T Kanakis, T Donovan, D Braines\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence of Large Language Models (LLMs) have fundamentally altered the way we interact with digital systems and have led to the pursuit of LLM powered AI agents to assist in daily workflows. LLMs, whilst powerful and capable of \u2026"}, {"title": "Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment", "link": "https://arxiv.org/pdf/2408.00137", "details": "S Yu, J Song, B Hwang, H Kang, S Cho, J Choi, S Joe\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "A binary decision task, like yes-no questions or answer verification, reflects a significant real-world scenario such as where users look for confirmation about the correctness of their decisions on specific issues. In this work, we observe that \u2026"}]
