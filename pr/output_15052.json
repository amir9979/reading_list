[{"title": "NoveltyBench: Evaluating Creativity and Diversity in Language Models", "link": "https://arxiv.org/pdf/2504.05228", "details": "Y Zhang, H Diddee, S Holm, H Liu, X Liu, V Samuel\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Language models have demonstrated remarkable capabilities on standard benchmarks, yet they struggle increasingly from mode collapse, the inability to generate diverse and novel outputs. Our work introduces NoveltyBench, a \u2026"}, {"title": "Cultural Learning-Based Culture Adaptation of Language Models", "link": "https://arxiv.org/pdf/2504.02953", "details": "CC Liu, A Korhonen, I Gurevych - arXiv preprint arXiv:2504.02953, 2025", "abstract": "Adapting large language models (LLMs) to diverse cultural values is a challenging task, as existing LLMs often reflect the values of specific groups by default, and potentially causing harm to others. In this paper, we present CLCA, a novel \u2026"}, {"title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing", "link": "https://arxiv.org/pdf/2503.15815%3F", "details": "VA Dasu, V Gupta, S Tizpaz-Niari, G Tan - arXiv preprint arXiv:2503.15815, 2025", "abstract": "This paper explores pruning attention heads as a post-processing bias mitigation method for large language models (LLMs). Modern AI systems such as LLMs are expanding into sensitive social contexts where fairness concerns become especially \u2026"}, {"title": "Subkv: Quantizing Long Context KV Cache for Sub\u2010Billion Parameter Language Models on Edge Devices", "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3422", "details": "Z Zeng, T Zhang, Z Lu, W Li, H Zhuang, H Shao\u2026 - Software: Practice and \u2026, 2025", "abstract": "ABSTRACT Background Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their substantial computational and memory requirements present significant challenges for \u2026"}]
