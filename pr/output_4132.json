[{"title": "SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers", "link": "https://arxiv.org/pdf/2407.09413", "details": "S Pramanick, R Chellappa, S Venugopalan - arXiv preprint arXiv:2407.09413, 2024", "abstract": "Seeking answers to questions within long scientific research articles is a crucial area of study that aids readers in quickly addressing their inquiries. However, existing question-answering (QA) datasets based on scientific papers are limited in scale and \u2026"}, {"title": "LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models", "link": "https://arxiv.org/pdf/2406.19486", "details": "S Guo, S Damani, K Chang - arXiv preprint arXiv:2406.19486, 2024", "abstract": "In prompt tuning, a prefix or suffix text is added to the prompt, and the embeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix are optimized to gain more control over language models for specific tasks. This approach eliminates the \u2026"}, {"title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment", "link": "https://arxiv.org/pdf/2407.10804", "details": "J Jiang, J Li, WX Zhao, Y Song, T Zhang, JR Wen - arXiv preprint arXiv:2407.10804, 2024", "abstract": "Adapting general large language models (LLMs) to specialized domains presents great challenges due to varied data distributions. This adaptation typically requires continual pre-training on massive domain-specific corpora to facilitate knowledge \u2026"}, {"title": "What to do if language models disagree? Black-box model ensembling for textual and visual question answering", "link": "https://arxiv.org/pdf/2407.12841", "details": "Y Xia, K Zaporojets, B Roth - arXiv preprint arXiv:2407.12841, 2024", "abstract": "A diverse range of large language models (LLMs), eg, ChatGPT, and visual question answering (VQA) models, eg, BLIP, have been developed for solving textual and visual question answering tasks. However, both LLMs and VQA models encounter \u2026"}, {"title": "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation", "link": "https://arxiv.org/pdf/2407.02056", "details": "X Wang, Y Li, S Feng, P Yuan, B Pan, H Wang, Y Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-consistency (SC), leveraging multiple samples from LLMs, shows significant gains on various reasoning tasks but struggles with free-form generation due to the difficulty of aggregating answers. Its variants, UCS and USC, rely on sample \u2026"}, {"title": "Adapting Knowledge for Few-shot Table-to-Text Generation", "link": "https://ieeexplore.ieee.org/abstract/document/10602781/", "details": "Z Guo, M Yan, J Qi, J Zhou, Z He, G Zheng, X Wang\u2026 - IEEE/ACM Transactions on \u2026, 2024", "abstract": "Pretrained language models (PLMs) have made remarkable progress in table-to-text generation tasks. However, the lack of domain-specific knowledge makes it challenging to bridge the topological gap between tabular data and text, especially in \u2026"}, {"title": "Fine-tuning Diffusion Models for Enhancing Face Quality in Text-to-image Generation", "link": "https://arxiv.org/pdf/2406.17100", "details": "Z Liao, Q Xie, C Chen, H Lu, Z Deng - arXiv preprint arXiv:2406.17100, 2024", "abstract": "Diffusion models (DMs) have achieved significant success in generating imaginative images given textual descriptions. However, they are likely to fall short when it comes to real-life scenarios with intricate details. The low-quality, unrealistic human faces in \u2026"}, {"title": "Out-of-Distribution Detection through Soft Clustering with Non-Negative Kernel Regression", "link": "https://arxiv.org/pdf/2407.13141", "details": "A Gulati, X Dong, C Hurtado, S Shekkizhar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As language models become more general purpose, increased attention needs to be paid to detecting out-of-distribution (OOD) instances, ie, those not belonging to any of the distributions seen during training. Existing methods for detecting OOD data are \u2026"}, {"title": "Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination", "link": "https://ieeecai.org/2024/wp-content/pdfs/540900a486/540900a486.pdf", "details": "X Zhong, K Batmanghelich, L Sun", "abstract": "Vision-language models pre-trained on large scale of unlabeled biomedical images and associated reports learn generalizable semantic representations. These multi- modal representations can benefit various downstream tasks in the biomedical \u2026"}]
