[{"title": "Belief in the Machine: Investigating Epistemological Blind Spots of Language Models", "link": "https://arxiv.org/pdf/2410.21195", "details": "M Suzgun, T Gur, F Bianchi, DE Ho, T Icard, D Jurafsky\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As language models (LMs) become integral to fields like healthcare, law, and journalism, their ability to differentiate between fact, belief, and knowledge is essential for reliable decision-making. Failure to grasp these distinctions can lead to \u2026"}, {"title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style", "link": "https://arxiv.org/pdf/2410.16184%3F", "details": "Y Liu, Z Yao, R Min, Y Cao, L Hou, J Li - arXiv preprint arXiv:2410.16184, 2024", "abstract": "Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward \u2026"}, {"title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models", "link": "https://arxiv.org/pdf/2411.00836", "details": "C Zou, X Guo, R Yang, J Zhang, B Hu, H Zhang - arXiv preprint arXiv:2411.00836, 2024", "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor \u2026"}, {"title": "Moe++: Accelerating mixture-of-experts methods with zero-computation experts", "link": "https://arxiv.org/pdf/2410.07348", "details": "P Jin, B Zhu, L Yuan, S Yan - arXiv preprint arXiv:2410.07348, 2024", "abstract": "In this work, we aim to simultaneously enhance the effectiveness and efficiency of Mixture-of-Experts (MoE) methods. To achieve this, we propose MoE++, a general and heterogeneous MoE framework that integrates both Feed-Forward \u2026"}, {"title": "Analysing the Residual Stream of Language Models Under Knowledge Conflicts", "link": "https://arxiv.org/pdf/2410.16090", "details": "Y Zhao, X Du, G Hong, AP Gema, A Devoto, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model \u2026"}, {"title": "PortLLM: Personalizing Evolving Large Language Models with Training-Free and Portable Model Patches", "link": "https://arxiv.org/pdf/2410.10870", "details": "RMS Khan, P Li, S Yun, Z Wang, S Nirjon, CW Wong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As large language models (LLMs) increasingly shape the AI landscape, fine-tuning pretrained models has become more popular than in the pre-LLM era for achieving optimal performance in domain-specific tasks. However, pretrained LLMs such as \u2026"}, {"title": "Eliciting Critical Reasoning in Retrieval-Augmented Language Models via Contrastive Explanations", "link": "https://arxiv.org/pdf/2410.22874", "details": "L Ranaldi, M Valentino, A Freitas - arXiv preprint arXiv:2410.22874, 2024", "abstract": "Retrieval-augmented generation (RAG) has emerged as a critical mechanism in contemporary NLP to support Large Language Models (LLMs) in systematically accessing richer factual context. However, the integration of RAG mechanisms brings \u2026"}, {"title": "A Survey of Hallucination in Large Visual Language Models", "link": "https://arxiv.org/pdf/2410.15359", "details": "W Lan, W Chen, Q Chen, S Pan, H Zhou, Y Pan - arXiv preprint arXiv:2410.15359, 2024", "abstract": "The Large Visual Language Models (LVLMs) enhances user interaction and enriches user experience by integrating visual modality on the basis of the Large Language Models (LLMs). It has demonstrated their powerful information processing \u2026"}, {"title": "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models", "link": "https://arxiv.org/pdf/2410.17389", "details": "M Lin, S Shi, Y Guo, B Chalaki, V Tadiparthi, EM Pari\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The correct specification of reward models is a well-known challenge in reinforcement learning. Hand-crafted reward functions often lead to inefficient or suboptimal policies and may not be aligned with user values. Reinforcement \u2026"}]
