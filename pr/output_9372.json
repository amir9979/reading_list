[{"title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2411.14432", "details": "Y Dong, Z Liu, HL Sun, J Yang, W Hu, Y Rao, Z Liu - arXiv preprint arXiv:2411.14432, 2024", "abstract": "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high \u2026"}, {"title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization", "link": "https://arxiv.org/pdf/2411.10442", "details": "W Wang, Z Chen, W Wang, Y Cao, Y Liu, Z Gao, J Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing open-source multimodal large language models (MLLMs) generally follow a training process involving pre-training and supervised fine-tuning. However, these models suffer from distribution shifts, which limit their multimodal reasoning \u2026"}, {"title": "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.285.pdf", "details": "Z Zha, X Zhu, Y Xu, C Huang, J Liu, Z Li, X Wang\u2026 - Findings of the Association \u2026, 2024", "abstract": "Abstract Multimodal Large Language Models (MLLMs) have shown promising results in various tasks, but their ability to perceive the visual world with deep, hierarchical understanding similar to humans remains uncertain. To address this gap, we \u2026"}, {"title": "Large Language Models Can Self-Improve in Long-context Reasoning", "link": "https://arxiv.org/pdf/2411.08147", "details": "S Li, C Yang, Z Cheng, L Liu, M Yu, Y Yang, W Lam - arXiv preprint arXiv:2411.08147, 2024", "abstract": "Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations \u2026"}, {"title": "Measuring Non-Adversarial Reproduction of Training Data in Large Language Models", "link": "https://arxiv.org/pdf/2411.10242%3F", "details": "M Aerni, J Rando, E Debenedetti, N Carlini, D Ippolito\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models memorize parts of their training data. Memorizing short snippets and facts is required to answer questions about the world and to be fluent in any language. But models have also been shown to reproduce long verbatim \u2026"}, {"title": "Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training", "link": "https://arxiv.org/pdf/2411.14318", "details": "Z Luo, X Zhang, X Liu, H Li, Y Gong, C Qi, P Cheng - arXiv preprint arXiv:2411.14318, 2024", "abstract": "It is well-known that a diverse corpus is critical for training large language models, which are typically constructed from a mixture of various domains. In general, previous efforts resort to sampling training data from different domains with static \u2026"}, {"title": "The First Prompt Counts the Most! An Evaluation of Large Language Models on Iterative Example-based Code Generation", "link": "https://arxiv.org/pdf/2411.06774", "details": "Y Fu, B Li, L Li, W Zhang, T Xie - arXiv preprint arXiv:2411.06774, 2024", "abstract": "The capabilities of Large Language Models (LLMs) in code generation, particularly for implementing target functionalities from natural language descriptions, have been extensively studied. As an alternative form of natural language, input-output \u2026"}, {"title": "Synthetic Data Generation with Large Language Models for Personalized Community Question Answering", "link": "https://arxiv.org/pdf/2410.22182", "details": "M Braga, P Kasela, A Raganato, G Pasi - arXiv preprint arXiv:2410.22182, 2024", "abstract": "Personalization in Information Retrieval (IR) is a topic studied by the research community since a long time. However, there is still a lack of datasets to conduct large-scale evaluations of personalized IR; this is mainly due to the fact that \u2026"}, {"title": "Text-to-SQL Systems in the Era of Advanced Large Language Models", "link": "https://era.library.ualberta.ca/items/3db9c207-9248-4760-8f82-0f6f308ff3ff/download/d817de66-5fe8-47fb-b065-1e5cf7644244", "details": "M Pourreza - 2024", "abstract": "Text-to-SQL conversion, the process of transforming natural language queries into executable SQL commands, stands at the forefront of bridging human linguistic capabilities with the structured logic of databases. This dissertation embarks on a \u2026"}]
