[{"title": "PEFT-as-an-Attack! Jailbreaking Language Models during Federated Parameter-Efficient Fine-Tuning", "link": "https://arxiv.org/pdf/2411.19335", "details": "S Li, ECH Ngai, F Ye, T Voigt - arXiv preprint arXiv:2411.19335, 2024", "abstract": "Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising paradigm for privacy-preserving and efficient adaptation of Pre-trained Language Models (PLMs) in Federated Learning (FL) settings. It preserves data privacy by \u2026"}, {"title": "Sneaking Syntax into Transformer Language Models with Tree Regularization", "link": "https://arxiv.org/pdf/2411.18885", "details": "A Nandi, CD Manning, S Murty - arXiv preprint arXiv:2411.18885, 2024", "abstract": "While compositional accounts of human language understanding are based on a hierarchical tree-like process, neural models like transformers lack a direct inductive bias for such tree structures. Introducing syntactic inductive biases could unlock more \u2026"}, {"title": "Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting", "link": "https://aclanthology.org/2024.findings-emnlp.400.pdf", "details": "M Naguib, X Tannier, A Neveol - Findings of the Association for Computational \u2026, 2024", "abstract": "Large language models (LLMs) have become the preferred solution for many natural language processing tasks. In low-resource environments such as specialized domains, their few-shot capabilities are expected to deliver high performance \u2026"}, {"title": "Rephrasing Electronic Health Records for Pretraining Clinical Language Models", "link": "https://arxiv.org/pdf/2411.18940", "details": "J Liu, A Nguyen - arXiv preprint arXiv:2411.18940, 2024", "abstract": "Clinical language models are important for many applications in healthcare, but their development depends on access to extensive clinical text for pretraining. However, obtaining clinical notes from electronic health records (EHRs) at scale is challenging \u2026"}, {"title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices", "link": "https://arxiv.org/pdf/2411.10640", "details": "X Lu, Y Chen, C Chen, H Tan, B Chen, Y Xie, R Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile \u2026"}, {"title": "GraphVis: Boosting LLMs with Visual Knowledge Graph Integration", "link": "https://openreview.net/pdf%3Fid%3DhaVPmN8UGi", "details": "Y Deng, C Ye, Z Huang, MD Ma, Y Kou, W Wang - The Thirty-eighth Annual Conference on \u2026", "abstract": "The rapid evolution of large language models (LLMs) has expanded their capabilities across various data modalities, extending from well-established image data to increasingly popular graph data. Given the limitation of LLMs in \u2026"}, {"title": "Group Robust Best-of-K Decoding of Language Models for Pluralistic Alignment", "link": "https://openreview.net/pdf%3Fid%3DJI6j4NUGHv", "details": "S Yoon, W Bankes, S Son, A Petrovic, SS Ramesh\u2026 - Pluralistic Alignment Workshop at \u2026", "abstract": "The desirable behaviour of a chat agent can be described with multiple criteria, such as harmlessness, helpfulness, and conciseness, each of which can be scored by a reward model. While each user, or a group of users, may perceive each criterion with \u2026"}, {"title": "METEOR: Evolutionary Journey of Large Language Models from Guidance to Self-Growth", "link": "https://arxiv.org/pdf/2411.11933", "details": "J Li, C Feng, Y Gao - arXiv preprint arXiv:2411.11933, 2024", "abstract": "Model evolution enables learning from feedback to refine experiences and update skills, transforming models from having no domain knowledge to becoming domain experts. However, there is currently no unified and effective method for guiding this \u2026"}, {"title": "KnowGPT: Knowledge Graph based Prompting for Large Language Models", "link": "https://openreview.net/pdf%3Fid%3DPacBluO5m7", "details": "Q Zhang, J Dong, H Chen, D Zha, Z Yu, X Huang - The Thirty-eighth Annual \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in many real-world applications. Nonetheless, LLMs are often criticized for their tendency to produce hallucinations, wherein the models fabricate incorrect statements on tasks \u2026"}]
