[{"title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models", "link": "https://arxiv.org/pdf/2407.03181", "details": "H Puerto, T Chubakov, X Zhu, HT Madabushi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model \u2026"}, {"title": "Factors Influencing Data Quality in Electronic Health Record Systems in 50 Health Facilities in Rwanda and the Role of Clinical Alerts: Cross-Sectional Observational \u2026", "link": "https://publichealth.jmir.org/2024/1/e49127/", "details": "HSF Fraser, M Mugisha, I Bacher, JL Ngenzi\u2026 - JMIR Public Health and \u2026, 2024", "abstract": "Background: Electronic health records (EHRs) play an increasingly important role in delivering HIV care in low-and middle-income countries. The data collected are used for direct clinical care, quality improvement, program monitoring, public health \u2026"}, {"title": "Speculative Speech Recognition by Audio-Prefixed Low-Rank Adaptation of Language Models", "link": "https://arxiv.org/pdf/2407.04641", "details": "B Yusuf, MK Baskar, A Rosenberg, B Ramabhadran - arXiv preprint arXiv:2407.04641, 2024", "abstract": "This paper explores speculative speech recognition (SSR), where we empower conventional automatic speech recognition (ASR) with speculation capabilities, allowing the recognizer to run ahead of audio. We introduce a metric for measuring \u2026"}, {"title": "Extracting and Encoding: Leveraging Large Language Models and Medical Knowledge to Enhance Radiological Text Representation", "link": "https://arxiv.org/pdf/2407.01948", "details": "P Messina, R Vidal, D Parra, \u00c1 Soto, V Araujo - arXiv preprint arXiv:2407.01948, 2024", "abstract": "Advancing representation learning in specialized fields like medicine remains challenging due to the scarcity of expert annotations for text and images. To tackle this issue, we present a novel two-stage framework designed to extract high-quality \u2026"}, {"title": "Towards Effective and Efficient Continual Pre-training of Large Language Models", "link": "https://arxiv.org/pdf/2407.18743", "details": "J Chen, Z Chen, J Wang, K Zhou, Y Zhu, J Jiang, Y Min\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Continual pre-training (CPT) has been an important approach for adapting language models to specific domains or tasks. To make the CPT approach more traceable, this paper presents a technical report for continually pre-training Llama-3 (8B), which \u2026"}, {"title": "Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning", "link": "https://arxiv.org/pdf/2407.06112", "details": "Y Zhang, S Mao, W Wu, Y Xia, T Ge, M Lan, F Wei - arXiv preprint arXiv:2407.06112, 2024", "abstract": "This paper introduces BI-Directional DEliberation Reasoning (BIDDER), a novel reasoning approach to enhance the decision rationality of language models. Traditional reasoning methods typically rely on historical information and employ uni \u2026"}, {"title": "CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents", "link": "https://arxiv.org/pdf/2407.01511%3Ftrk%3Dpublic_post_comment-text", "details": "T Xu, L Chen, DJ Wu, Y Chen, Z Zhang, X Yao, Z Xie\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The development of autonomous agents increasingly relies on Multimodal Language Models (MLMs) to perform tasks described in natural language with GUI environments, such as websites, desktop computers, or mobile phones. Existing \u2026"}, {"title": "Learning to Reduce: Towards Improving Performance of Large Language Models on Structured Data", "link": "https://arxiv.org/pdf/2407.02750", "details": "Y Lee, S Kim, RA Rossi, T Yu, X Chen - arXiv preprint arXiv:2407.02750, 2024", "abstract": "Large Language Models (LLMs) have been achieving competent performance on a wide range of downstream tasks, yet existing work shows that inference on structured data is challenging for LLMs. This is because LLMs need to either understand long \u2026"}]
