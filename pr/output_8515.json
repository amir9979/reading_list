[{"title": "CoPESD: A Multi-Level Surgical Motion Dataset for Training Large Vision-Language Models to Co-Pilot Endoscopic Submucosal Dissection", "link": "https://arxiv.org/pdf/2410.07540", "details": "G Wang, H Xiao, H Gao, R Zhang, L Bai, X Yang, Z Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "submucosal dissection (ESD) enables rapid resection of large lesions, minimizing recurrence rates and improving long-term overall survival. Despite these advantages, ESD is technically challenging and carries high risks of complications \u2026"}, {"title": "A foundation model for generalizable disease diagnosis in chest X-ray images", "link": "https://arxiv.org/pdf/2410.08861", "details": "L Xu, Z Ni, H Sun, H Li, S Zhang - arXiv preprint arXiv:2410.08861, 2024", "abstract": "Medical artificial intelligence (AI) is revolutionizing the interpretation of chest X-ray (CXR) images by providing robust tools for disease diagnosis. However, the effectiveness of these AI models is often limited by their reliance on large amounts of \u2026"}, {"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "link": "https://arxiv.org/pdf/2410.05639", "details": "R Zhao, ZL Thai, Y Zhang, S Hu, Y Ba, J Zhou, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the \u2026"}, {"title": "VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis", "link": "https://arxiv.org/pdf/2410.08397", "details": "A Hoopes, VI Butoi, JV Guttag, AV Dalca - arXiv preprint arXiv:2410.08397, 2024", "abstract": "We present VoxelPrompt, an agent-driven vision-language framework that tackles diverse radiological tasks through joint modeling of natural language, image volumes, and analytical metrics. VoxelPrompt is multi-modal and versatile \u2026"}, {"title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures", "link": "https://arxiv.org/pdf/2410.07698", "details": "Y Chen, Y Zhang, L Cao, K Yuan, Z Wen - arXiv preprint arXiv:2410.07698, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs when adapting large language models (LLMs) for downstream applications. However, traditional first-order (FO) fine-tuning algorithms incur substantial memory overhead \u2026"}, {"title": "Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures", "link": "https://arxiv.org/pdf/2410.06672", "details": "J Wang, X Ge, W Shu, Q Tang, Y Zhou, Z He, X Qiu - arXiv preprint arXiv:2410.06672, 2024", "abstract": "The hypothesis of Universality in interpretability suggests that different neural networks may converge to implement similar algorithms on similar tasks. In this work, we investigate two mainstream architectures for language modeling, namely \u2026"}, {"title": "Deformable Vertebra 3D/2D Registration from Biplanar X-Rays Using Particle-Based Shape Modelling", "link": "https://link.springer.com/chapter/10.1007/978-3-031-75291-9_3", "details": "B Aubert, N Khan, F Toupin, M Pacheco, A Morris\u2026 - International Workshop on \u2026, 2024", "abstract": "Patient-specific 3D vertebra models are essential for accurately assessing the spinal deformities quantitatively in 3D and for surgical planning, including determining the optimal implant size and 3D positioning. Calibrated biplanar X-rays serve as an \u2026"}]
