[{"title": "Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs", "link": "https://arxiv.org/pdf/2506.10769", "details": "A Testoni, I Calixto - arXiv preprint arXiv:2506.10769, 2025", "abstract": "\u2026 We present a fine-grained evaluation of uncertainty estimation methods for **clinical** multiple-choice **question** **answering** , covering ten open-source \u2026 Adapted **large** **language** **models** can outperform **medical** experts in **clinical** text summarization \u2026", "entry_id": "http://arxiv.org/abs/2506.10769v1", "updated": "2025-06-12 14:48:25", "published": "2025-06-12 14:48:25", "authors": "Alberto Testoni;Iacer Calixto", "summary": "Accurate and well-calibrated uncertainty estimates are essential for\ndeploying large language models (LLMs) in high-stakes domains such as clinical\ndecision support. We present a fine-grained evaluation of uncertainty\nestimation methods for clinical multiple-choice question answering, covering\nten open-source LLMs (general-purpose, biomedical, and reasoning models) across\ntwo datasets, eleven medical specialties, and six question types. We compare\nstandard single-generation and sampling-based methods, and present a case study\nexploring simple, single-pass estimators based on behavioral signals in\nreasoning traces. These lightweight methods approach the performance of\nSemantic Entropy while requiring only one generation. Our results reveal\nsubstantial variation across specialties and question types, underscoring the\nimportance of selecting models based on both the nature of the question and\nmodel-specific strengths.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.10769v1;http://arxiv.org/pdf/2506.10769v1", "pdf_url": "http://arxiv.org/pdf/2506.10769v1"}, {"title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy", "link": "https://arxiv.org/pdf/2506.09958", "details": "S Gautam, MA Riegler, P Halvorsen - arXiv preprint arXiv:2506.09958, 2025", "abstract": "\u2026 **Medical** Visual **Question** **Answering** (MedVQA) is a promising field for developing **clinical** decision support \u2026 **question** - **answer** pairs that are designed to test deeper **clinical** reasoning. We developed a systematic method using **large** **language** **models** \u2026", "entry_id": "http://arxiv.org/abs/2506.09958v1", "updated": "2025-06-11 17:31:38", "published": "2025-06-11 17:31:38", "authors": "Sushant Gautam;Michael A. Riegler;P\u00e5l Halvorsen", "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.LG;68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing) 68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing);I.2.10; I.2.6; J.3", "links": "http://arxiv.org/abs/2506.09958v1;http://arxiv.org/pdf/2506.09958v1", "pdf_url": "http://arxiv.org/pdf/2506.09958v1"}, {"title": "Applying Language Models To Patient Health Records: Acronym Expansion, Long Document Classification and Explainable Predictions", "link": "https://repository.upenn.edu/bitstreams/3ae58c00-9a3d-49fc-afbb-e9f6f2212d78/download", "details": "A Kashyap - 2025", "abstract": "\u2026 Our methods leverage recent advances in **large** **language** **models** while maintaining interpretability and **clinical** relevance. Figure 1.1 provides an overview of the \u2026 An example could include providing the model with a few sentence pairs for \u2026"}, {"title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning", "link": "https://arxiv.org/pdf/2506.09513", "details": "Y Sun, X Qian, W Xu, H Zhang, C Xiao, L Li, Y Rong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Though reasoning-based **large** **language** **models** (LLMs) have excelled in mathematics and programming, their capabilities in knowledge-intensive **medical** **question** **answering** remain underexplored. To address this, we introduce \u2026", "entry_id": "http://arxiv.org/abs/2506.09513v1", "updated": "2025-06-11 08:36:55", "published": "2025-06-11 08:36:55", "authors": "Yu Sun;Xingyu Qian;Weiwen Xu;Hao Zhang;Chenghao Xiao;Long Li;Yu Rong;Wenbing Huang;Qifeng Bai;Tingyang Xu", "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a \\textit{multi-agent\nverification and refinement process}, where we design an \\textit{Error Refiner}\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.", "comment": "24 pages, 6 figures, 7 tables", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.MA", "links": "http://arxiv.org/abs/2506.09513v1;http://arxiv.org/pdf/2506.09513v1", "pdf_url": "http://arxiv.org/pdf/2506.09513v1"}, {"title": "HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding", "link": "https://arxiv.org/pdf/2506.09634", "details": "Y Shi, X Zhang, J Ji, H Jiang, C Zheng, Y Wang, L Qu - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Recently, multi-modal **large** **language** **models** (MLLMs) have emerged as a powerful tool in **medical** image analysis, including diagnostic tasks such as **medical** report generation (MRG) and visual **question** **answering** (VQA). Current works \u2026", "entry_id": "http://arxiv.org/abs/2506.09634v1", "updated": "2025-06-11 11:46:57", "published": "2025-06-11 11:46:57", "authors": "Yanzhao Shi;Xiaodan Zhang;Junzhong Ji;Haoning Jiang;Chengxin Zheng;Yinong Wang;Liangqiong Qu", "summary": "Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based\ndecisions by enhancing diagnostic accuracy and workflow efficiency. While\nmultimodal large language models (MLLMs) exhibit promising performance in\nvisual-language understanding, existing methods mainly focus on 2D medical\nimages, which fundamentally limits their ability to capture complex 3D\nanatomical structures. This limitation often leads to misinterpretation of\nsubtle pathologies and causes diagnostic hallucinations. In this paper, we\npresent Hybrid Spatial Encoding Network (HSENet), a framework that exploits\nenriched 3D medical visual cues by effective visual perception and projection\nfor accurate and robust vision-language understanding. Specifically, HSENet\nemploys dual-3D vision encoders to perceive both global volumetric contexts and\nfine-grained anatomical details, which are pre-trained by dual-stage alignment\nwith diagnostic reports. Furthermore, we propose Spatial Packer, an efficient\nmultimodal projector that condenses high-resolution 3D spatial regions into a\ncompact set of informative visual tokens via centroid-based compression. By\nassigning spatial packers with dual-3D vision encoders, HSENet can seamlessly\nperceive and transfer hybrid visual representations to LLM's semantic space,\nfacilitating accurate diagnostic text generation. Experimental results\ndemonstrate that our method achieves state-of-the-art performance in 3D\nlanguage-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report\ngeneration (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering\n(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.\nOur code is available at https://github.com/YanzhaoShi/HSENet.", "comment": "27 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2410.14200 by other authors", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2506.09634v1;http://arxiv.org/pdf/2506.09634v1", "pdf_url": "http://arxiv.org/pdf/2506.09634v1"}, {"title": "One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence", "link": "https://arxiv.org/pdf/2506.10157", "details": "MM Li, BY Reis, A Rodman, T Cai, N Dagan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 strategies for adapting models to real-time **clinical** variation39. Sharing **large** **language** **models** trained on patient data is often not possible \u2026 trained to perform a distinct function (eg, information retrieval, **question** **answering** , decision support, or \u2026", "entry_id": "http://arxiv.org/abs/2506.10157v1", "updated": "2025-06-11 20:23:57", "published": "2025-06-11 20:23:57", "authors": "Michelle M. Li;Ben Y. Reis;Adam Rodman;Tianxi Cai;Noa Dagan;Ran D. Balicer;Joseph Loscalzo;Isaac S. Kohane;Marinka Zitnik", "summary": "Medical foundation models, including language models trained on clinical\nnotes, vision-language models on medical images, and multimodal models on\nelectronic health records, can summarize clinical notes, answer medical\nquestions, and assist in decision-making. Adapting these models to new\npopulations, specialties, or settings typically requires fine-tuning, careful\nprompting, or retrieval from knowledge bases. This can be impractical, and\nlimits their ability to interpret unfamiliar inputs and adjust to clinical\nsituations not represented during training. As a result, models are prone to\ncontextual errors, where predictions appear reasonable but fail to account for\ncritical patient-specific or contextual information. These errors stem from a\nfundamental limitation that current models struggle with: dynamically adjusting\ntheir behavior across evolving contexts of medical care. In this Perspective,\nwe outline a vision for context-switching in medical AI: models that\ndynamically adapt their reasoning without retraining to new specialties,\npopulations, workflows, and clinical roles. We envision context-switching AI to\ndiagnose, manage, and treat a wide range of diseases across specialties and\nregions, and expand access to medical care.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL", "links": "http://arxiv.org/abs/2506.10157v1;http://arxiv.org/pdf/2506.10157v1", "pdf_url": "http://arxiv.org/pdf/2506.10157v1"}, {"title": "Can We Infer Confidential Properties of Training Data from LLMs?", "link": "https://arxiv.org/pdf/2506.10364", "details": "C Yadav, R Wu, K Chaudhuri - arXiv preprint arXiv:2506.10364, 2025", "abstract": "\u2026 Contrary to previous works which either focus on discriminative models or pure generative models, we consider property inference attack for **large** **language** **models**. Since the model architecture, training paradigm and data type for LLMs are very \u2026", "entry_id": "http://arxiv.org/abs/2506.10364v1", "updated": "2025-06-12 05:42:06", "published": "2025-06-12 05:42:06", "authors": "Penguin Huang;Chhavi Yadav;Ruihan Wu;Kamalika Chaudhuri", "summary": "Large language models (LLMs) are increasingly fine-tuned on domain-specific\ndatasets to support applications in fields such as healthcare, finance, and\nlaw. These fine-tuning datasets often have sensitive and confidential\ndataset-level properties -- such as patient demographics or disease prevalence\n-- that are not intended to be revealed. While prior work has studied property\ninference attacks on discriminative models (e.g., image classification models)\nand generative models (e.g., GANs for image data), it remains unclear if such\nattacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark\ntask for evaluating property inference in LLMs under two fine-tuning paradigms:\nquestion-answering and chat-completion. Built on the ChatDoctor dataset, our\nbenchmark includes a range of property types and task configurations. We\nfurther propose two tailored attacks: a prompt-based generation attack and a\nshadow-model attack leveraging word frequency signals. Empirical evaluations\nacross multiple pretrained LLMs show the success of our attacks, revealing a\npreviously unrecognized vulnerability in LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CL;cs.CR", "links": "http://arxiv.org/abs/2506.10364v1;http://arxiv.org/pdf/2506.10364v1", "pdf_url": "http://arxiv.org/pdf/2506.10364v1"}, {"title": "Foundation Models in Medical Imaging -- A Review and Outlook", "link": "https://arxiv.org/pdf/2506.09095", "details": "V van Veldhuizen, V Botha, C Lu, ME Cesur\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 , such as using vision-language alignment to enhance whole-slide FMs and developing visual **question** **answering** FMs for pathology. \u2026 adaptation by using specialized adapters to align visual encoders with **large** **language** **models** , reducing \u2026", "entry_id": "http://arxiv.org/abs/2506.09095v1", "updated": "2025-06-10 12:14:05", "published": "2025-06-10 12:14:05", "authors": "Vivien van Veldhuizen;Vanessa Botha;Chunyao Lu;Melis Erdal Cesur;Kevin Groot Lipman;Edwin D. de Jong;Hugo Horlings;Cl\u00e1risa Sanchez;Cees Snoek;Ritse Mann;Eric Marcus;Jonas Teuwen", "summary": "Foundation models (FMs) are changing the way medical images are analyzed by\nlearning from large collections of unlabeled data. Instead of relying on\nmanually annotated examples, FMs are pre-trained to learn general-purpose\nvisual features that can later be adapted to specific clinical tasks with\nlittle additional supervision. In this review, we examine how FMs are being\ndeveloped and applied in pathology, radiology, and ophthalmology, drawing on\nevidence from over 150 studies. We explain the core components of FM pipelines,\nincluding model architectures, self-supervised learning methods, and strategies\nfor downstream adaptation. We also review how FMs are being used in each\nimaging domain and compare design choices across applications. Finally, we\ndiscuss key challenges and open questions to guide future research.", "comment": null, "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2506.09095v1;http://arxiv.org/pdf/2506.09095v1", "pdf_url": "http://arxiv.org/pdf/2506.09095v1"}, {"title": "Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives", "link": "https://arxiv.org/pdf/2506.09656", "details": "W Zeng, H Zhu, C Qin, H Wu, Y Cheng, S Zhang, X Jin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 As **Large** **Language** **Models** (LLMs) advance, their applications become more diverse and complex, leading to increasingly situational and systemic risks. This has brought significant attention to value \u2026 \u2022 Online shopping **question** **answering** \u2022 \u2026", "entry_id": "http://arxiv.org/abs/2506.09656v1", "updated": "2025-06-11 12:25:38", "published": "2025-06-11 12:25:38", "authors": "Wei Zeng;Hengshu Zhu;Chuan Qin;Han Wu;Yihang Cheng;Sirui Zhang;Xiaowei Jin;Yinuo Shen;Zhenxing Wang;Feimin Zhong;Hui Xiong", "summary": "The ongoing evolution of AI paradigms has propelled AI research into the\nAgentic AI stage. Consequently, the focus of research has shifted from single\nagents and simple applications towards multi-agent autonomous decision-making\nand task collaboration in complex environments. As Large Language Models (LLMs)\nadvance, their applications become more diverse and complex, leading to\nincreasingly situational and systemic risks. This has brought significant\nattention to value alignment for AI agents, which aims to ensure that an\nagent's goals, preferences, and behaviors align with human values and societal\nnorms. This paper reviews value alignment in agent systems within specific\napplication scenarios. It integrates the advancements in AI driven by large\nmodels with the demands of social governance. Our review covers value\nprinciples, agent system application scenarios, and agent value alignment\nevaluation. Specifically, value principles are organized hierarchically from a\ntop-down perspective, encompassing macro, meso, and micro levels. Agent system\napplication scenarios are categorized and reviewed from a general-to-specific\nviewpoint. Agent value alignment evaluation systematically examines datasets\nfor value alignment assessment and relevant value alignment methods.\nAdditionally, we delve into value coordination among multiple agents within\nagent systems. Finally, we propose several potential research directions in\nthis field.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2506.09656v1;http://arxiv.org/pdf/2506.09656v1", "pdf_url": "http://arxiv.org/pdf/2506.09656v1"}]
