[{"title": "Regression for the Mean: Auto-Evaluation and Inference with Few Labels through Post-hoc Regression", "link": "https://openreview.net/pdf%3Fid%3DkRKsUOJdp5", "details": "B Eyre, D Madras - Forty-second International Conference on Machine \u2026", "abstract": "\u2026 Through experiments estimating feature generation rates in the context of data analysis and **LLM** **evaluation** , we demonstrate that our approaches produce lower variance estimates than both classical estimation as well as PPI in the low-label \u2026"}, {"title": "QuEst: Enhancing Estimates of Quantile-Based Distributional Measures Using Model Predictions", "link": "https://openreview.net/pdf%3Fid%3DJwZVPTTEwO", "details": "Z Deng, TP Zollo, B Eyre, A Inamdar, D Madras\u2026 - Forty-second International \u2026", "abstract": "\u2026 Our experiments in Section 4.2.2 illustrate these multidimensional analyses in the context of **LLM** **evaluation**. \u2026 point estimates and tighter confidence intervals across applications from genomics to **LLM** **evaluation**. Future work might consider a more \u2026"}, {"title": "MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanation", "link": "https://arxiv.org/pdf/2506.19073", "details": "J Trager, F Vargas, D Alves, M Guida, MK Ngueajio\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is a growing concern as these systems are used in socially sensitive tasks. Nevertheless, current evaluation benchmarks present two major shortcomings: a lack of \u2026", "entry_id": "http://arxiv.org/abs/2506.19073v1", "updated": "2025-06-23 19:44:21", "published": "2025-06-23 19:44:21", "authors": "Jackson Trager;Francielle Vargas;Diego Alves;Matteo Guida;Mikel K. Ngueajio;Ameeta Agrawal;Flor Plaza-del-Arco;Yalda Daryanai;Farzan Karimi-Malekabadi", "summary": "Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is\na growing concern as these systems are used in socially sensitive tasks.\nNevertheless, current evaluation benchmarks present two major shortcomings: a\nlack of annotations that justify moral classifications, which limits\ntransparency and interpretability; and a predominant focus on English, which\nconstrains the assessment of moral reasoning across diverse cultural settings.\nIn this paper, we introduce MFTCXplain, a multilingual benchmark dataset for\nevaluating the moral reasoning of LLMs via hate speech multi-hop explanation\nusing Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across\nPortuguese, Italian, Persian, and English, annotated with binary hate speech\nlabels, moral categories, and text span-level rationales. Empirical results\nhighlight a misalignment between LLM outputs and human annotations in moral\nreasoning tasks. While LLMs perform well in hate speech detection (F1 up to\n0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35).\nFurthermore, rationale alignment remains limited mainly in underrepresented\nlanguages. These findings show the limited capacity of current LLMs to\ninternalize and reflect human moral reasoning.", "comment": "Under Review", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.19073v1;http://arxiv.org/pdf/2506.19073v1", "pdf_url": "http://arxiv.org/pdf/2506.19073v1"}, {"title": "From Documents to Disclosures: Automating Sustainability Reporting with Large Language Models", "link": "https://lup.lub.lu.se/luur/download%3Ffunc%3DdownloadFile%26recordOId%3D9204394%26fileOId%3D9204401", "details": "A Nystedt, O Wiksten - 2025", "abstract": "This study explores the use of Large Language Models (LLMs) to automate sustainability reporting in accordance with the European Sustainability Reporting Standards (ESRS). Given the increasing complexity and regulatory demands of \u2026"}]
