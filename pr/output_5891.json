[{"title": "Evaluating Perturbation-based Post-hoc Explainer Fidelity Using Inherently Interpretable Models", "link": "https://adamsunn.github.io/assets/other/fidelity.pdf", "details": "A Sun, A Sudjianto, W Fargo, A Zhang", "abstract": "Post-hoc explainability methods like LIME and SHAP are frequently used to explain predictions of complex blackbox machine learning models like neural networks and model ensembles. However, these explainers are only approximations, which is an \u2026"}]
