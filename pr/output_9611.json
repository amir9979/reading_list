[{"title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization", "link": "https://arxiv.org/pdf/2411.10442", "details": "W Wang, Z Chen, W Wang, Y Cao, Y Liu, Z Gao, J Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing open-source multimodal large language models (MLLMs) generally follow a training process involving pre-training and supervised fine-tuning. However, these models suffer from distribution shifts, which limit their multimodal reasoning \u2026"}, {"title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices", "link": "https://arxiv.org/pdf/2411.10640", "details": "X Lu, Y Chen, C Chen, H Tan, B Chen, Y Xie, R Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile \u2026"}, {"title": "Group Robust Best-of-K Decoding of Language Models for Pluralistic Alignment", "link": "https://openreview.net/pdf%3Fid%3DJI6j4NUGHv", "details": "S Yoon, W Bankes, S Son, A Petrovic, SS Ramesh\u2026 - Pluralistic Alignment Workshop at \u2026", "abstract": "The desirable behaviour of a chat agent can be described with multiple criteria, such as harmlessness, helpfulness, and conciseness, each of which can be scored by a reward model. While each user, or a group of users, may perceive each criterion with \u2026"}, {"title": "METEOR: Evolutionary Journey of Large Language Models from Guidance to Self-Growth", "link": "https://arxiv.org/pdf/2411.11933", "details": "J Li, C Feng, Y Gao - arXiv preprint arXiv:2411.11933, 2024", "abstract": "Model evolution enables learning from feedback to refine experiences and update skills, transforming models from having no domain knowledge to becoming domain experts. However, there is currently no unified and effective method for guiding this \u2026"}, {"title": "The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on Large Language Models", "link": "https://arxiv.org/pdf/2411.11407", "details": "X Yang, X Tang, J Han, S Hu - arXiv preprint arXiv:2411.11407, 2024", "abstract": "The widespread deployment of large language models (LLMs) across various domains has showcased their immense potential while exposing significant safety vulnerabilities. A major concern is ensuring that LLM-generated content aligns with \u2026"}, {"title": "The performance of the LSTM-based code generated by Large Language Models (LLMs) in forecasting time series data", "link": "https://www.sciencedirect.com/science/article/pii/S2949719124000682", "details": "S Gopali, S Siami-Namini, F Abri, AS Namin - Natural Language Processing Journal, 2024", "abstract": "Generative AI, and in particular Large Language Models (LLMs), have gained substantial momentum due to their wide applications in various disciplines. While the use of these game changing technologies in generating textual information has \u2026"}, {"title": "Mercury: A code efficiency benchmark for code large language models", "link": "https://openreview.net/pdf%3Fid%3DvyraA7xt4c", "details": "M Du, AT Luu, B Ji, Q Liu, SK Ng - The Thirty-eight Conference on Neural Information \u2026, 2024", "abstract": "Amidst the recent strides in evaluating Large Language Models for Code (Code LLMs), existing benchmarks have mainly focused on the functional correctness of generated code, neglecting the importance of their computational efficiency. To fill \u2026"}, {"title": "DecoPrompt: Decoding Prompts Reduces Hallucinations when Large Language Models Meet False Premises", "link": "https://arxiv.org/pdf/2411.07457", "details": "N Xu, X Ma - arXiv preprint arXiv:2411.07457, 2024", "abstract": "While large language models (LLMs) have demonstrated increasing power, they have also called upon studies on their hallucinated outputs that deviate from factually correct statements. In this paper, we focus on one important scenario of false \u2026"}, {"title": "DART-LLM: Dependency-Aware Multi-Robot Task Decomposition and Execution using Large Language Models", "link": "https://arxiv.org/pdf/2411.09022", "details": "Y Wang, R Xiao, JYL Kasahara, R Yajima, K Nagatani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated significant reasoning capabilities in robotic systems. However, their deployment in multi-robot systems remains fragmented and struggles to handle complex task dependencies and \u2026"}]
