[{"title": "VLM's Eye Examination: Instruct and Inspect Visual Competency of Vision Language Models", "link": "https://arxiv.org/pdf/2409.14759", "details": "N Hyeon-Woo, M Ye-Bin, W Choi, L Hyun, TH Oh - arXiv preprint arXiv:2409.14759, 2024", "abstract": "Vision language models (VLMs) have shown promising reasoning capabilities across various benchmarks; however, our understanding of their visual perception remains limited. In this work, we propose an eye examination process to investigate \u2026"}, {"title": "Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models", "link": "https://arxiv.org/pdf/2409.12512", "details": "J Rao, X Liu, Z Lin, L Ding, J Li, D Tao - arXiv preprint arXiv:2409.12512, 2024", "abstract": "Knowledge distillation (KD) is a technique that compresses large teacher models by training smaller student models to mimic them. The success of KD in auto-regressive language models mainly relies on Reverse KL for mode-seeking and student \u2026"}, {"title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models", "link": "https://arxiv.org/pdf/2409.18943", "details": "J Li, L Zhang, Y Li, Z Liu, R Luo, L Chen, M Yang - arXiv preprint arXiv:2409.18943, 2024", "abstract": "The instruction-following ability of large language models enables humans to interact with AI agents in a natural way. However, when required to generate responses of a specific length, large language models often struggle to meet users' needs due to \u2026"}, {"title": "Improving the Efficiency of Visually Augmented Language Models", "link": "https://arxiv.org/pdf/2409.11148", "details": "P Ontalvilla, A Ormazabal, G Azkune - arXiv preprint arXiv:2409.11148, 2024", "abstract": "Despite the impressive performance of autoregressive Language Models (LM) it has been shown that due to reporting bias, LMs lack visual knowledge, ie they do not know much about the visual world and its properties. To augment LMs with visual \u2026"}, {"title": "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models", "link": "https://arxiv.org/pdf/2409.13592", "details": "A Nandy, Y Agarwal, A Patwa, MM Das, A Bansal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Understanding satire and humor is a challenging task for even current Vision- Language models. In this paper, we propose the challenging tasks of Satirical Image Detection (detecting whether an image is satirical), Understanding (generating the \u2026"}, {"title": "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation", "link": "https://arxiv.org/pdf/2409.13928", "details": "S Lee, S Kim, J Jang, H Chon, D Lee, H Yu - arXiv preprint arXiv:2409.13928, 2024", "abstract": "We study the code generation behavior of instruction-tuned models built on top of code pre-trained language models when they could access an auxiliary function to implement a function. We design several ways to provide auxiliary functions to the \u2026"}, {"title": "Aligning Language Models Using Follow-up Likelihood as Reward Signal", "link": "https://arxiv.org/pdf/2409.13948", "details": "C Zhang, D Chong, F Jiang, C Tang, A Gao, G Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In natural human-to-human conversations, participants often receive feedback signals from one another based on their follow-up reactions. These reactions can include verbal responses, facial expressions, changes in emotional state, and other \u2026"}, {"title": "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "link": "https://arxiv.org/pdf/2409.14083", "details": "J Sun, J Zhang, Y Zhou, Z Su, X Qu, Y Cheng - arXiv preprint arXiv:2409.14083, 2024", "abstract": "Large Vision-Language Models (LVLMs) have become pivotal at the intersection of computer vision and natural language processing. However, the full potential of LVLMs Retrieval-Augmented Generation (RAG) capabilities remains underutilized \u2026"}, {"title": "FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs", "link": "https://arxiv.org/pdf/2409.13612", "details": "B Yan, Z Zhang, L Jing, E Hossain, X Du - arXiv preprint arXiv:2409.13612, 2024", "abstract": "The rapid development of Large Vision-Language Models (LVLMs) often comes with widespread hallucination issues, making cost-effective and comprehensive assessments increasingly vital. Current approaches mainly rely on costly annotations \u2026"}]
