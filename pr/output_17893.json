[{"title": "Multimodal Medical Image Binding via Shared Text Embeddings", "link": "https://arxiv.org/pdf/2506.18072", "details": "Y Liu, S Xi, S Liu, H Ding, C Jin, C Yang, J He, Y Shen - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical image analysis increasingly relies on the integration of multiple imaging modalities to capture complementary anatomical and functional information, enabling more accurate diagnosis and treatment planning. Achieving aligned feature \u2026", "entry_id": "http://arxiv.org/abs/2506.18072v1", "updated": "2025-06-22 15:39:25", "published": "2025-06-22 15:39:25", "authors": "Yunhao Liu;Suyang Xi;Shiqi Liu;Hong Ding;Chicheng Jin;Chenxi Yang;Junjun He;Yiqing Shen", "summary": "Medical image analysis increasingly relies on the integration of multiple\nimaging modalities to capture complementary anatomical and functional\ninformation, enabling more accurate diagnosis and treatment planning. Achieving\naligned feature representations across these diverse modalities is therefore\nimportant for effective multimodal analysis. While contrastive language-image\npre-training (CLIP) and its variant have enabled image-text alignments, they\nrequire explicitly paired data between arbitrary two modalities, which is\ndifficult to acquire in medical contexts. To address the gap, we present\nMultimodal Medical Image Binding with Text (M\\textsuperscript{3}Bind), a novel\npre-training framework that enables seamless alignment of multiple medical\nimaging modalities through a shared text representation space without requiring\nexplicit paired data between any two medical image modalities. Specifically,\nbased on the insight that different images can naturally bind with text,\nM\\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text\nmodels to align their modality-specific text embedding space while preserving\ntheir original image-text alignments. Subsequently, we distill these\nmodality-specific text encoders into a unified model, creating a shared text\nembedding space. Experiments on X-ray, CT, retina, ECG, and pathological images\non multiple downstream tasks demonstrate that M\\textsuperscript{3}Bind achieves\nstate-of-the-art performance in zero-shot, few-shot classification and\ncross-modal retrieval tasks compared to its CLIP-like counterparts. These\nresults validate M\\textsuperscript{3}Bind's effectiveness in achieving\ncross-image-modal alignment for medical analysis.", "comment": "10 pages, 3 figures", "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2506.18072v1;http://arxiv.org/pdf/2506.18072v1", "pdf_url": "http://arxiv.org/pdf/2506.18072v1"}, {"title": "Adaptive Mask-guided K-space Diffusion for Accelerated MRI Reconstruction", "link": "https://arxiv.org/pdf/2506.18270", "details": "Q Cai, Y Guan, Z Chen, D Liang, Q Fan, Q Liu - arXiv preprint arXiv:2506.18270, 2025", "abstract": "As the deep learning revolution marches on, masked modeling has emerged as a distinctive approach that involves predicting parts of the original data that are proportionally masked during training, and has demonstrated exceptional \u2026", "entry_id": "http://arxiv.org/abs/2506.18270v1", "updated": "2025-06-23 03:54:53", "published": "2025-06-23 03:54:53", "authors": "Qinrong Cai;Yu Guan;Zhibo Chen;Dong Liang;Qiuyun Fan;Qiegen Liu", "summary": "As the deep learning revolution marches on, masked modeling has emerged as a\ndistinctive approach that involves predicting parts of the original data that\nare proportionally masked during training, and has demonstrated exceptional\nperformance in multiple fields. Magnetic Resonance Imaging (MRI) reconstruction\nis a critical task in medical imaging that seeks to recover high-quality images\nfrom under-sampled k-space data. However, previous MRI reconstruction\nstrategies usually optimized the entire image domain or k-space, without\nconsidering the importance of different frequency regions in the k-space This\nwork introduces a diffusion model based on adaptive masks (AMDM), which\nutilizes the adaptive adjustment of frequency distribution based on k-space\ndata to develop a hybrid masks mechanism that adapts to different k-space\ninputs. This enables the effective separation of high-frequency and\nlow-frequency components, producing diverse frequency-specific representations.\nAdditionally, the k-space frequency distribution informs the generation of\nadaptive masks, which, in turn, guide a closed-loop diffusion process.\nExperimental results verified the ability of this method to learn specific\nfrequency information and thereby improved the quality of MRI reconstruction,\nproviding a flexible framework for optimizing k-space data using masks in the\nfuture.", "comment": "10 pages, 9 figures", "journal_ref": null, "primary_category": "eess.IV", "categories": "eess.IV;cs.CV", "links": "http://arxiv.org/abs/2506.18270v1;http://arxiv.org/pdf/2506.18270v1", "pdf_url": "http://arxiv.org/pdf/2506.18270v1"}, {"title": "Optimized attention U-Net for enhanced lung and area of infection segmentation in chest X-Rays and CT scans", "link": "https://www.sciencedirect.com/science/article/pii/S1687850725003620", "details": "B Mustapha, Y Zhou, B Nawel, S Chunyan, X Zhitao - Journal of Radiation Research \u2026, 2025", "abstract": "Medical image segmentation is crucial for accurate diagnosis, effective treatment planning, and thorough disease monitoring. However, segmentation inaccuracies can have serious clinical consequences clinically significant errors occur in up to \u2026"}, {"title": "Deep Learning-based Alignment Measurement in Knee Radiographs", "link": "https://arxiv.org/pdf/2506.18209", "details": "Z Hu, D Cullen, P Thompson, D Johnson, C Bian\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Radiographic knee alignment (KA) measurement is important for predicting joint health and surgical outcomes after total knee replacement. Traditional methods for KA measurements are manual, time-consuming and require long-leg radiographs \u2026", "entry_id": "http://arxiv.org/abs/2506.18209v1", "updated": "2025-06-22 23:57:46", "published": "2025-06-22 23:57:46", "authors": "Zhisen Hu;Dominic Cullen;Peter Thompson;David Johnson;Chang Bian;Aleksei Tiulpin;Timothy Cootes;Claudia Lindner", "summary": "Radiographic knee alignment (KA) measurement is important for predicting\njoint health and surgical outcomes after total knee replacement. Traditional\nmethods for KA measurements are manual, time-consuming and require long-leg\nradiographs. This study proposes a deep learning-based method to measure KA in\nanteroposterior knee radiographs via automatically localized knee anatomical\nlandmarks. Our method builds on hourglass networks and incorporates an\nattention gate structure to enhance robustness and focus on key anatomical\nfeatures. To our knowledge, this is the first deep learning-based method to\nlocalize over 100 knee anatomical landmarks to fully outline the knee shape\nwhile integrating KA measurements on both pre-operative and post-operative\nimages. It provides highly accurate and reliable anatomical varus/valgus KA\nmeasurements using the anatomical tibiofemoral angle, achieving mean absolute\ndifferences ~1{\\deg} when compared to clinical ground truth measurements.\nAgreement between automated and clinical measurements was excellent\npre-operatively (intra-class correlation coefficient (ICC) = 0.97) and good\npost-operatively (ICC = 0.86). Our findings demonstrate that KA assessment can\nbe automated with high accuracy, creating opportunities for digitally enhanced\nclinical workflows.", "comment": "Accepted to MICCAI 2025", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2506.18209v1;http://arxiv.org/pdf/2506.18209v1", "pdf_url": "http://arxiv.org/pdf/2506.18209v1"}, {"title": "Boosting Few-shot Semantic Segmentation of 3D Medical Images via Collaborative Slice Alignment", "link": "https://pubmed.ncbi.nlm.nih.gov/40549529/", "details": "R Duan, J Pei, Z Wang, R Zhang, Q Li, PA Heng - IEEE journal of biomedical and health \u2026", "abstract": "Few-shot semantic segmentation (FSS) of 3D medical images requires finding a 2D slice from the labeled volume as support to'query'slices of the unlabeled one. Accurately determining support slices is crucial for learning representative \u2026"}, {"title": "Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation", "link": "https://arxiv.org/pdf/2506.17500", "details": "J Silva-Rodr\u00edguez, F Shakeri, H Bahig, J Dolz, IB Ayed - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) are gaining attention in medical image analysis. These are pre-trained on large, heterogeneous data sources, yielding rich and transferable representations. Notably, the combination of modality-specialized VLMs \u2026", "entry_id": "http://arxiv.org/abs/2506.17500v1", "updated": "2025-06-20 22:35:00", "published": "2025-06-20 22:35:00", "authors": "Julio Silva-Rodr\u00edguez;Fereshteh Shakeri;Houda Bahig;Jose Dolz;Ismail Ben Ayed", "summary": "Vision-language models (VLMs) are gaining attention in medical image\nanalysis. These are pre-trained on large, heterogeneous data sources, yielding\nrich and transferable representations. Notably, the combination of\nmodality-specialized VLMs with few-shot adaptation has provided fruitful\nresults, enabling the efficient deployment of high-performing solutions.\nHowever, previous works on this topic make strong assumptions about the\ndistribution of adaptation data, which are unrealistic in the medical domain.\nFirst, prior art assumes access to a balanced support set, a condition that\nbreaks the natural imbalance in disease prevalence found in real-world\nscenarios. Second, these works typically assume the presence of an additional\nvalidation set to fix critical hyper-parameters, which is highly\ndata-inefficient. This work challenges these favorable deployment scenarios and\nintroduces a realistic, imbalanced, validation-free adaptation setting. Our\nextensive benchmark across various modalities and downstream tasks demonstrates\nthat current methods systematically compromise their performance when operating\nunder realistic conditions, occasionally even performing worse than zero-shot\ninference. Also, we introduce a training-free linear probe that adaptively\nblends visual and textual supervision. Detailed studies demonstrate that the\nproposed solver is a strong, efficient baseline, enabling robust adaptation in\nchallenging scenarios.", "comment": "MICCAI 2025. Code: https://github.com/jusiro/SS-Text", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.17500v1;http://arxiv.org/pdf/2506.17500v1", "pdf_url": "http://arxiv.org/pdf/2506.17500v1"}, {"title": "Trustworthy Few-Shot Transfer of Medical VLMs through Split Conformal Prediction", "link": "https://arxiv.org/pdf/2506.17503", "details": "J Silva-Rodr\u00edguez, IB Ayed, J Dolz - arXiv preprint arXiv:2506.17503, 2025", "abstract": "Medical vision-language models (VLMs) have demonstrated unprecedented transfer capabilities and are being increasingly adopted for data-efficient image classification. Despite its growing popularity, its reliability aspect remains largely \u2026", "entry_id": "http://arxiv.org/abs/2506.17503v1", "updated": "2025-06-20 22:48:07", "published": "2025-06-20 22:48:07", "authors": "Julio Silva-Rodr\u00edguez;Ismail Ben Ayed;Jose Dolz", "summary": "Medical vision-language models (VLMs) have demonstrated unprecedented\ntransfer capabilities and are being increasingly adopted for data-efficient\nimage classification. Despite its growing popularity, its reliability aspect\nremains largely unexplored. This work explores the split conformal prediction\n(SCP) framework to provide trustworthiness guarantees when transferring such\nmodels based on a small labeled calibration set. Despite its potential, the\ngeneralist nature of the VLMs' pre-training could negatively affect the\nproperties of the predicted conformal sets for specific tasks. While common\npractice in transfer learning for discriminative purposes involves an\nadaptation stage, we observe that deploying such a solution for conformal\npurposes is suboptimal since adapting the model using the available calibration\ndata breaks the rigid exchangeability assumptions for test data in SCP. To\naddress this issue, we propose transductive split conformal adaptation (SCA-T),\na novel pipeline for transfer learning on conformal scenarios, which performs\nan unsupervised transductive adaptation jointly on calibration and test data.\nWe present comprehensive experiments utilizing medical VLMs across various\nimage modalities, transfer tasks, and non-conformity scores. Our framework\noffers consistent gains in efficiency and conditional coverage compared to SCP,\nmaintaining the same empirical guarantees.", "comment": "MICCAI 2025. Code: https://github.com/jusiro/SCA-T", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.17503v1;http://arxiv.org/pdf/2506.17503v1", "pdf_url": "http://arxiv.org/pdf/2506.17503v1"}]
