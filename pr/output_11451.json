[{"title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining", "link": "https://arxiv.org/pdf/2501.00958", "details": "W Zhang, H Zhang, X Li, J Sun, Y Shen, W Lu, D Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge \u2026"}, {"title": "Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception", "link": "https://arxiv.org/pdf/2412.14233", "details": "Y Sun, J Hao, K Zhu, JJ Liu, Y Zhao, X Li, G Zhang, Z Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Training Large Multimodality Models (LMMs) relies on descriptive image caption that connects image and language. Existing methods either distill the caption from the LMM models or construct the captions from the internet images or by human. We \u2026"}, {"title": "On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena", "link": "https://arxiv.org/pdf/2501.04662", "details": "T Naous, W Xu - arXiv preprint arXiv:2501.04662, 2025", "abstract": "Language Models (LMs) have been shown to exhibit a strong preference towards entities associated with Western culture when operating in non-Western languages. In this paper, we aim to uncover the origins of entity-related cultural biases in LMs by \u2026"}, {"title": "Multimodal Preference Data Synthetic Alignment with Reward Model", "link": "https://arxiv.org/pdf/2412.17417", "details": "R Wijaya, NB Nguyen, NM Cheung - arXiv preprint arXiv:2412.17417, 2024", "abstract": "Multimodal large language models (MLLMs) have significantly advanced tasks like caption generation and visual question answering by integrating visual and textual data. However, they sometimes produce misleading or hallucinate content due to \u2026"}, {"title": "Natural Language Processing (NLP) in Analyzing Electronic Health Records for Better Decision Making", "link": "https://al-kindipublishers.org/index.php/jcsts/article/download/8456/7160", "details": "MR Hossain, S Mahabub, A Al Masum, I Jahan - Journal of Computer Science and \u2026, 2024", "abstract": "Natural Language Processing (NLP) is transforming healthcare decision-making by extracting valuable insights from Electronic Health Records (EHR). This paper explores the integration of NLP with EHR systems, focusing on its potential to \u2026"}, {"title": "A High-Quality Text-Rich Image Instruction Tuning Dataset via Hybrid Instruction Generation", "link": "https://arxiv.org/pdf/2412.16364", "details": "S Zhou, R Zhang, Y Zhou, C Chen - arXiv preprint arXiv:2412.16364, 2024", "abstract": "Large multimodal models still struggle with text-rich images because of inadequate training data. Self-Instruct provides an annotation-free way for generating instruction data, but its quality is poor, as multimodal alignment remains a hurdle even for the \u2026"}, {"title": "A Knowledge-enhanced Pathology Vision-language Foundation Model for Cancer Diagnosis", "link": "https://arxiv.org/pdf/2412.13126", "details": "X Zhou, L Sun, D He, W Guan, R Wang, L Wang, X Sun\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Deep learning has enabled the development of highly robust foundation models for various pathological tasks across diverse diseases and patient cohorts. Among these models, vision-language pre-training, which leverages large-scale paired data to \u2026"}, {"title": "A vision\u2013language foundation model for precision oncology", "link": "https://www.nature.com/articles/s41586-024-08378-w", "details": "J Xiang, X Wang, X Zhang, Y Xi, F Eweje, Y Chen, Y Li\u2026 - Nature, 2025", "abstract": "Clinical decision-making is driven by multimodal data, including clinical notes and pathological characteristics. Artificial intelligence approaches that can effectively integrate multimodal data hold significant promise in advancing clinical care \u2026"}, {"title": "GeoX: Geometric Problem Solving Through Unified Formalized Vision-Language Pre-training", "link": "https://arxiv.org/pdf/2412.11863", "details": "R Xia, M Li, H Ye, W Wu, H Zhou, J Yuan, T Peng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their proficiency in general tasks, Multi-modal Large Language Models (MLLMs) struggle with automatic Geometry Problem Solving (GPS), which demands understanding diagrams, interpreting symbols, and performing complex reasoning \u2026"}]
