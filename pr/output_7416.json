[{"title": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models", "link": "https://arxiv.org/pdf/2410.02355", "details": "J Fang, H Jiang, K Wang, Y Ma, X Wang, X He, T Chua - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) often exhibit hallucinations due to incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating \u2026"}, {"title": "Do Vision and Language Models Share Concepts? A Vector Space Alignment Study", "link": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00698/124631", "details": "J Li, Y Kementchedjhieva, C Fierro, A S\u00f8gaard - Transactions of the Association for \u2026, 2024", "abstract": "Large-scale pretrained language models (LMs) are said to \u201clack the ability to connect utterances to the world\u201d(Bender and Koller,), because they do not have \u201cmental models of the world\u201d(Mitchell and Krakauer,). If so, one would expect LM \u2026"}, {"title": "Preserving Generalization of Language models in Few-shot Continual Relation Extraction", "link": "https://arxiv.org/pdf/2410.00334", "details": "Q Tran, NX Thanh, NH Anh, NL Hai, T Le, L Van Ngo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Few-shot Continual Relations Extraction (FCRE) is an emerging and dynamic area of study where models can sequentially integrate knowledge from new relations with limited labeled data while circumventing catastrophic forgetting and preserving prior \u2026"}, {"title": "General Preference Modeling with Preference Representations for Aligning Language Models", "link": "https://arxiv.org/pdf/2410.02197", "details": "Y Zhang, G Zhang, Y Wu, K Xu, Q Gu - arXiv preprint arXiv:2410.02197, 2024", "abstract": "Modeling human preferences is crucial for aligning foundation models with human values. Traditional reward modeling methods, such as the Bradley-Terry (BT) reward model, fall short in expressiveness, particularly in addressing intransitive \u2026"}, {"title": "Exploring the Learning Capabilities of Language Models using LEVERWORLDS", "link": "https://arxiv.org/pdf/2410.00519", "details": "E Wagner, A Feder, O Abend - arXiv preprint arXiv:2410.00519, 2024", "abstract": "Learning a model of a stochastic setting often involves learning both general structure rules and specific properties of the instance. This paper investigates the interplay between learning the general and the specific in various learning methods \u2026"}, {"title": "The Perfect Blend: Redefining RLHF with Mixture of Judges", "link": "https://arxiv.org/pdf/2409.20370", "details": "T Xu, E Helenowski, KA Sankararaman, D Jin, K Peng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Reinforcement learning from human feedback (RLHF) has become the leading approach for fine-tuning large language models (LLM). However, RLHF has limitations in multi-task learning (MTL) due to challenges of reward hacking and \u2026"}, {"title": "POSIX: A Prompt Sensitivity Index For Large Language Models", "link": "https://arxiv.org/pdf/2410.02185", "details": "A Chatterjee, HK Renduchintala, S Bhatia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are found to be surprisingly sensitive to minor variations in prompts, often generating significantly divergent outputs in response to minor variations in the prompts, such as spelling \u2026"}, {"title": "Can Language Models Take A Hint? Prompting for Controllable Contextualized Commonsense Inference", "link": "https://arxiv.org/pdf/2410.02202", "details": "P Colon-Hernandez, N Liu, C Joe, P Chin, C Yin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating commonsense assertions within a given story context remains a difficult task for modern language models. Previous research has addressed this problem by aligning commonsense inferences with stories and training language generation \u2026"}, {"title": "Law of the Weakest Link: Cross Capabilities of Large Language Models", "link": "https://arxiv.org/pdf/2409.19951", "details": "M Zhong, A Zhang, X Wang, R Hou, W Xiong, C Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real \u2026"}]
