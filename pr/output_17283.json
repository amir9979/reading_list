[{"title": "Retrieval-Augmented Generation with **Large Language Models** in Radiology: From Theory to Practice", "link": "https://pubs.rsna.org/doi/abs/10.1148/ryai.240790", "details": "A Fink, A Rau, M Reisert, F Bamberg, MF Russe - Radiology: Artificial Intelligence, 2025", "abstract": "\u2026 **large** **language** **models** to function as complementary tools for radiologists, helping optimize patient care and meet growing **health** **care** \u2026 One promising approach is the use of **large** **language** **models** (LLMs) such as ChatGPT (2), Claude (3) \u2026"}, {"title": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation", "link": "https://arxiv.org/pdf/2506.04078", "details": "M Zhang, Y Shen, Z Li, H Sha, B Hu, Y Wang, C Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **large** **language** **models** (LLMs) in **medicine** is crucial because **medical** applications require high accuracy with little room for error. Current **medical** benchmarks have three main types: **medical** exam-based, comprehensive **medical** , \u2026 have limitations \u2026", "entry_id": "http://arxiv.org/abs/2506.04078v1", "updated": "2025-06-04 15:43:14", "published": "2025-06-04 15:43:14", "authors": "Ming Zhang;Yujiong Shen;Zelin Li;Huayu Sha;Binze Hu;Yuhui Wang;Chenhao Huang;Shichun Liu;Jingqi Tong;Changhao Jiang;Mingxu Chai;Zhiheng Xi;Shihan Dou;Tao Gui;Qi Zhang;Xuanjing Huang", "summary": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.04078v1;http://arxiv.org/pdf/2506.04078v1", "pdf_url": "http://arxiv.org/pdf/2506.04078v1"}, {"title": "Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing", "link": "https://arxiv.org/pdf/2506.03490", "details": "S Chen, L Luo, Z Qiu, Y Cao, C Yang, S Pan - arXiv preprint arXiv:2506.03490, 2025", "abstract": "\u2026 in **Large** **Language** **Models** (LLMs) without the need for full retraining. Despite the effectiveness in general-domain benchmarks, their applicability to complex **medical** domain \u2026 Medmcqa: A large-scale multi-subject multi-choice dataset for **medical** domain \u2026", "entry_id": "http://arxiv.org/abs/2506.03490v2", "updated": "2025-06-05 03:20:15", "published": "2025-06-04 02:14:43", "authors": "Shigeng Chen;Linhao Luo;Zhangchi Qiu;Yanan Cao;Carl Yang;Shirui Pan", "summary": "Recently, knowledge editing (KE) has emerged as a promising approach to\nupdate specific facts in Large Language Models (LLMs) without the need for full\nretraining. Despite the effectiveness in general-domain benchmarks, their\napplicability to complex medical domain remains largely unexplored. Medical\nknowledge editing is particularly challenging, as it requires LLMs to\ninternalize the knowledge and generalize to unseen scenarios for effective and\ninterpretable decision-making. In this work, we propose a novel framework\ncalled MedEditBench to rigorously evaluate the effectiveness of existing KE\nmethods in the medical domain. In MedEditBench, we introduce a new medical\nknowledge editing benchmark as well as three different knowledge editing\nparadigms, which are designed to assess the impact of different knowledge\nsources for editing. Our findings indicate that current KE methods result in\nonly superficial memorization of the injected information, failing to\ngeneralize to new scenarios. To overcome this limitation, we present\nSelf-Generated Rationale Editing (SGR-Edit), which utilizes model-derived\nrationales as the target knowledge for editing, thereby uncovering the\nunderlying reasoning process and demonstrating significant improvements over\nexisting KE approaches. Additionally, we offer deeper insights into medical\nknowledge editing, including the localization of medical knowledge in LLMs and\nthe impact of sequential editing on evolving knowledge. This could provide\npractical guidance for implementing KE methods in real-world medical\napplications.", "comment": "Under Review", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.03490v2;http://arxiv.org/pdf/2506.03490v2", "pdf_url": "http://arxiv.org/pdf/2506.03490v2"}, {"title": "High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning", "link": "https://arxiv.org/pdf/2506.04051", "details": "T Franzmeyer, A Sravankumar, L Liu, Y Mao, R Hou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **large** **language** **models**. arXiv preprint arXiv:2404.01295, 2024. \u2026 will **answer** the same few-shot prompt with slightly different numbers of fragments, as shown in Table 4, we choose nall according to the pre-trained model\u2019s response statistics in \u2026", "entry_id": "http://arxiv.org/abs/2506.04051v1", "updated": "2025-06-04 15:16:21", "published": "2025-06-04 15:16:21", "authors": "Tim Franzmeyer;Archie Sravankumar;Lijuan Liu;Yuning Mao;Rui Hou;Sinong Wang;Jakob N. Foerster;Luke Zettlemoyer;Madian Khabsa", "summary": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.04051v1;http://arxiv.org/pdf/2506.04051v1", "pdf_url": "http://arxiv.org/pdf/2506.04051v1"}, {"title": "A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization", "link": "https://arxiv.org/pdf/2506.04156", "details": "S Soni, D Demner-Fushman - arXiv preprint arXiv:2506.04156, 2025", "abstract": "\u2026 **question** **answering** (QA), we evaluated three openweight **large** **language** **models** (LLMs)\u2014Llama 4, Llama 3, and Mixtral\u2014using three prompting strategies: (1) generating **answers** with citations to clinical note sentences, (2) generating **answers** \u2026", "entry_id": "http://arxiv.org/abs/2506.04156v1", "updated": "2025-06-04 16:55:08", "published": "2025-06-04 16:55:08", "authors": "Sarvesh Soni;Dina Demner-Fushman", "summary": "Patients have distinct information needs about their hospitalization that can\nbe addressed using clinical evidence from electronic health records (EHRs).\nWhile artificial intelligence (AI) systems show promise in meeting these needs,\nrobust datasets are needed to evaluate the factual accuracy and relevance of\nAI-generated responses. To our knowledge, no existing dataset captures patient\ninformation needs in the context of their EHRs. We introduce ArchEHR-QA, an\nexpert-annotated dataset based on real-world patient cases from intensive care\nunit and emergency department settings. The cases comprise questions posed by\npatients to public health forums, clinician-interpreted counterparts, relevant\nclinical note excerpts with sentence-level relevance annotations, and\nclinician-authored answers. To establish benchmarks for grounded EHR question\nanswering (QA), we evaluated three open-weight large language models\n(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:\ngenerating (1) answers with citations to clinical note sentences, (2) answers\nbefore citations, and (3) answers from filtered citations. We assessed\nperformance on two dimensions: Factuality (overlap between cited note sentences\nand ground truth) and Relevance (textual and semantic similarity between system\nand reference answers). The final dataset contains 134 patient cases. The\nanswer-first prompting approach consistently performed best, with Llama 4\nachieving the highest scores. Manual error analysis supported these findings\nand revealed common issues such as omitted key clinical evidence and\ncontradictory or hallucinated content. Overall, ArchEHR-QA provides a strong\nbenchmark for developing and evaluating patient-centered EHR QA systems,\nunderscoring the need for further progress toward generating factual and\nrelevant responses in clinical contexts.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.04156v1;http://arxiv.org/pdf/2506.04156v1", "pdf_url": "http://arxiv.org/pdf/2506.04156v1"}, {"title": "A review of ChatGPT in **medical** education: exploring advantages and limitations", "link": "https://journals.lww.com/international-journal-of-surgery/_layouts/15/oaks.journals/downloadpdf.aspx%3Fan%3D01279778-990000000-02424", "details": "Y Cheng, L Zhu - International Journal of Surgery, 2025", "abstract": "\u2026 Recent advancements show that researchers are leveraging ChatGPT\u2019s architecture to develop specialized **medical** **large** **language** **models** (LLMs) with enhanced capabilities, such as **Med** -PaLM and MedAlpaca. Additionally, new \u2026"}, {"title": "Design and Development of an Educational Platform for Psychiatry", "link": "https://dspace.cvut.cz/bitstream/handle/10467/122449/F3-BP-2025-Petrishchev-Iaroslav-Bakalarska%2520prace%2520Petrishchev%2520Iaroslav.pdf%3Fsequence%3D-1", "details": "P Iaroslav - 2025", "abstract": "\u2026 , leveraging **large** **language** **models** (LLMs) to simulate realistic patient interactions. The platform aims to provide **medical** students with a safe \u2026 The publication of this work led to a boom in the development of **large** **language** **models** \u2026"}]
