[{"title": "Visual cot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought reasoning", "link": "https://openreview.net/pdf%3Fid%3DaXeiCbMFFJ", "details": "H Shao, S Qian, H Xiao, G Song, Z Zong, L Wang, Y Liu\u2026 - The Thirty-eight Conference \u2026, 2024", "abstract": "Multi-Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However, they often lack interpretability and struggle with complex visual inputs, especially when the resolution of the input image \u2026"}, {"title": "Domain-Invariant Few-Shot Contrastive Learning for Hyperspectral Image Classification", "link": "https://www.mdpi.com/2076-3417/14/23/11053", "details": "W Chen, Y Zhang, J Chu, X Wang - Applied Sciences, 2024", "abstract": "In Hyperspectral Image (HSI) classification, acquiring large quantities of high-quality labeled samples is typically costly and impractical. Traditional deep learning methods are limited in such scenarios due to their dependence on sample quantities \u2026"}]
