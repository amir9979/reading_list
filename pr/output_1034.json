'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Investigating Regularization of Self-Play Language Mod'
[{"title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2404.05567", "details": "B Pan, Y Shen, H Liu, M Mishra, G Zhang, A Oliva\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) language models can reduce computational costs by 2- 4$\\times $ compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios. However, MoE models generally \u2026"}, {"title": "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models", "link": "https://arxiv.org/pdf/2404.02575", "details": "H Chae, Y Kim, S Kim, KT Ong, B Kwak, M Kim, S Kim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution. Such nature of algorithmic reasoning makes it a challenge for large \u2026"}, {"title": "FeDeRA: Efficient Fine-tuning of Language Models in Federated Learning Leveraging Weight Decomposition", "link": "https://arxiv.org/pdf/2404.18848", "details": "Y Yan, S Tang, Z Shi, Q Yang - arXiv preprint arXiv:2404.18848, 2024", "abstract": "Pre-trained Language Models (PLMs) have shown excellent performance on various downstream tasks after fine-tuning. Nevertheless, the escalating concerns surrounding user privacy have posed significant challenges to centralized training \u2026"}, {"title": "Understanding Multimodal Contrastive Learning Through Pointwise Mutual Information", "link": "https://arxiv.org/pdf/2404.19228", "details": "T Uesaka, T Suzuki, Y Takida, CH Lai, N Murata\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal representation learning to integrate different modalities, such as text, vision, and audio is important for real-world applications. The symmetric InfoNCE loss proposed in CLIP is a key concept in multimodal representation learning. In this \u2026"}, {"title": "FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations", "link": "https://arxiv.org/pdf/2404.06619", "details": "J Dwivedi-Yu, R Dwivedi, T Schick - arXiv preprint arXiv:2404.06619, 2024", "abstract": "The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes \u2026"}, {"title": "Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning", "link": "https://arxiv.org/pdf/2404.03891", "details": "G Choi, H Ahn - arXiv preprint arXiv:2404.03891, 2024", "abstract": "In robotics, the use of Large Language Models (LLMs) is becoming prevalent, especially for understanding human commands. In particular, LLMs are utilized as domain-agnostic task planners for high-level human commands. LLMs are capable \u2026"}, {"title": "Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form on Mathematical Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2404.11500", "details": "Y Zhou, Y Zhu, D Antognini, Y Kim, Y Zhang - arXiv preprint arXiv:2404.11500, 2024", "abstract": "This paper studies the relationship between the surface form of a mathematical problem and its solvability by large language models. We find that subtle alterations in the surface form can significantly impact the answer distribution and the solve rate \u2026"}, {"title": "Conversational Disease Diagnosis via External Planner-Controlled Large Language Models", "link": "https://arxiv.org/pdf/2404.04292", "details": "Z Sun, C Luo, Z Huang - arXiv preprint arXiv:2404.04292, 2024", "abstract": "The advancement of medical artificial intelligence (AI) has set the stage for the realization of conversational diagnosis, where AI systems mimic human doctors by engaging in dialogue with patients to deduce diagnoses. This study introduces an \u2026"}, {"title": "On the Surprising Efficacy of Distillation as an Alternative to Pre-Training Small Models", "link": "https://arxiv.org/pdf/2404.03263", "details": "S Farhat, D Chen - arXiv preprint arXiv:2404.03263, 2024", "abstract": "In this paper, we propose that small models may not need to absorb the cost of pre- training to reap its benefits. Instead, they can capitalize on the astonishing results achieved by modern, enormous models to a surprising degree. We observe that \u2026"}]
