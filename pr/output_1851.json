[{"title": "Uncertainty-aware Evaluation of Auxiliary Anomalies with the Expected Anomaly Posterior", "link": "https://arxiv.org/pdf/2405.13699", "details": "L Perini, M Rudolph, S Schmedding, C Qiu - arXiv preprint arXiv:2405.13699, 2024", "abstract": "Anomaly detection is the task of identifying examples that do not behave as expected. Because anomalies are rare and unexpected events, collecting real anomalous examples is often challenging in several applications. In addition \u2026"}, {"title": "LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent Diffusion Models", "link": "https://arxiv.org/pdf/2405.14477", "details": "S Sadat, J Buhmann, D Bradley, O Hilliges, RM Weber - arXiv preprint arXiv \u2026, 2024", "abstract": "Advances in latent diffusion models (LDMs) have revolutionized high-resolution image generation, but the design space of the autoencoder that is central to these systems remains underexplored. In this paper, we introduce LiteVAE, a family of \u2026"}, {"title": "LucidPPN: Unambiguous Prototypical Parts Network for User-centric Interpretable Computer Vision", "link": "https://arxiv.org/pdf/2405.14331", "details": "M Pach, D Rymarczyk, K Lewandowska, J Tabor\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Prototypical parts networks combine the power of deep learning with the explainability of case-based reasoning to make accurate, interpretable decisions. They follow the this looks like that reasoning, representing each prototypical part with \u2026"}, {"title": "PhiNets: Brain-inspired Non-contrastive Learning Based on Temporal Prediction Hypothesis", "link": "https://arxiv.org/pdf/2405.14650", "details": "S Ishikawa, M Yamada, H Bao, Y Takezawa - arXiv preprint arXiv:2405.14650, 2024", "abstract": "SimSiam is a prominent self-supervised learning method that achieves impressive results in various vision tasks under static environments. However, it has two critical issues: high sensitivity to hyperparameters, especially weight decay, and \u2026"}, {"title": "Towards Imperceptible Backdoor Attack in Self-supervised Learning", "link": "https://arxiv.org/pdf/2405.14672", "details": "H Zhang, Z Wang, T Han, M Jin, C Zhan, M Du, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-supervised learning models are vulnerable to backdoor attacks. Existing backdoor attacks that are effective in self-supervised learning often involve noticeable triggers, like colored patches, which are vulnerable to human inspection \u2026"}]
