[{"title": "Reconstructing Missing Variables for Multivariate Time Series Forecasting via Conditional Generative Flows", "link": "https://www.ijcai.org/proceedings/2024/0228.pdf", "details": "X Hu, W Fan, H Chen, P Wang, Y Fu", "abstract": "Abstract The Variable Subset Forecasting (VSF) problem, where the majority of variables are unavailable in the inference stage of multivariate forecasting, has been an important but under-explored task with broad impacts in many real-world \u2026"}, {"title": "Contrastive learning enhanced by graph neural networks for Universal Multivariate Time Series Representation", "link": "https://www.sciencedirect.com/science/article/pii/S0306437924000875", "details": "X Wang, Q Xing, H Xiao, M Ye - Information Systems, 2024", "abstract": "Analyzing multivariate time series data is crucial for many real-world issues, such as power forecasting, traffic flow forecasting, industrial anomaly detection, and more. Recently, universal frameworks for time series representation based on \u2026"}, {"title": "Agreement between Apple Watch and Actical step counts in a community setting: The Framingham Heart Study", "link": "https://s3.ca-central-1.amazonaws.com/assets.jmir.org/assets/preprints/preprint-54631-accepted.pdf", "details": "D McManus, JM Murabito", "abstract": "Background: Step counting is comparable among many research-grade and consumer-grade accelerometers in laboratory settings, but few studies have compared step count measurement among devices outside of the laboratory, in a \u2026"}, {"title": "AutoCTS++: zero-shot joint neural architecture and hyperparameter search for correlated time series forecasting", "link": "https://vbn.aau.dk/files/730426496/AutoCTS_.pdf", "details": "X Wu, X Wu, B Yang, L Zhou, C Guo, X Qiu, J Hu\u2026 - The VLDB Journal, 2024", "abstract": "Sensors in cyber-physical systems often capture interconnected processes and thus emit correlated time series (CTS), the forecasting of which enables important applications. Recent deep learning based forecasting methods show strong \u2026"}, {"title": "Relation-Preserving Masked Modeling for Semi-Supervised Time-Series Classification", "link": "https://www.sciencedirect.com/science/article/pii/S0020025524011277", "details": "S Lee, C Choi, Y Son - Information Sciences, 2024", "abstract": "In this study, we address the challenge of label sparsity in time-series classification using semi-supervised learning that effectively leverages numerous unlabeled instances. Our approach introduces a pioneering framework for semi-supervised \u2026"}, {"title": "Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks", "link": "https://arxiv.org/pdf/2407.20657", "details": "H Yang, J Jeong, KJ Yoon - arXiv preprint arXiv:2407.20657, 2024", "abstract": "Recent vision-language foundation models, such as CLIP, have demonstrated superior capabilities in learning representations that can be transferable across diverse range of downstream tasks and domains. With the emergence of such \u2026"}, {"title": "General Time Transformer: an Encoder-only Foundation Model for Zero-Shot Multivariate Time Series Forecasting", "link": "https://cfeng783.github.io/pubs/CIKM24_GTT.pdf", "details": "C Feng, L Huang, D Krompass - 2024", "abstract": "Abstract We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse \u2026"}, {"title": "Model Debiasing by Learnable Data Augmentation", "link": "https://arxiv.org/pdf/2408.04955", "details": "P Morerio, R Ragonesi, V Murino - arXiv preprint arXiv:2408.04955, 2024", "abstract": "Deep Neural Networks are well known for efficiently fitting training data, yet experiencing poor generalization capabilities whenever some kind of bias dominates over the actual task labels, resulting in models learning\" shortcuts\". In essence, such \u2026"}, {"title": "Revisiting Attention for Multivariate Time Series Forecasting", "link": "https://arxiv.org/pdf/2407.13806", "details": "H Wu - arXiv preprint arXiv:2407.13806, 2024", "abstract": "Current Transformer methods for Multivariate Time-Series Forecasting (MTSF) are all based on the conventional attention mechanism. They involve sequence embedding and performing a linear projection of Q, K, and V, and then computing attention within \u2026"}]
