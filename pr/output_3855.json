[{"title": "MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models", "link": "https://arxiv.org/pdf/2407.02775", "details": "Y Zhang, Z Yang, S Ji - arXiv preprint arXiv:2407.02775, 2024", "abstract": "Knowledge distillation is an effective technique for pre-trained language model compression. Although existing knowledge distillation methods perform well for the most typical model BERT, they could be further improved in two aspects: the relation \u2026"}, {"title": "DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs", "link": "https://arxiv.org/pdf/2406.17158", "details": "VVD Prabhu, A Anand - arXiv preprint arXiv:2406.17158, 2024", "abstract": "Open-domain complex Question Answering (QA) is a difficult task with challenges in evidence retrieval and reasoning. The complexity of such questions could stem from questions being compositional, hybrid evidence, or ambiguity in questions. While \u2026"}, {"title": "Information Guided Regularization for Fine-tuning Language Models", "link": "https://arxiv.org/pdf/2406.14005", "details": "M Sharma, N Muralidhar, S Xu, RB Yosuf\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The pretraining-fine-tuning paradigm has been the de facto strategy for transfer learning in modern language modeling. With the understanding that task adaptation in LMs is often a function of parameters shared across tasks, we argue that a more \u2026"}, {"title": "ViGLUE: A Vietnamese General Language Understanding Benchmark and Analysis of Vietnamese Language Models", "link": "https://aclanthology.org/2024.findings-naacl.261.pdf", "details": "MN Tran, PV Nguyen, L Nguyen, D Dien - Findings of the Association for \u2026, 2024", "abstract": "As the number of language models has increased, various benchmarks have been suggested to assess the proficiency of the models in natural language understanding. However, there is a lack of such a benchmark in Vietnamese due to \u2026"}, {"title": "LTRC-IIITH at MEDIQA-M3G 2024: Medical Visual Question Answering with Vision-Language Models", "link": "https://aclanthology.org/2024.clinicalnlp-1.67.pdf", "details": "J Thomas, S Marimuthu, P Krishnamurthy - Proceedings of the 6th Clinical Natural \u2026, 2024", "abstract": "In this paper, we present our work to the MEDIQA-M3G 2024 shared task, which tackles multilingual and multimodal medical answer generation. Our system consists of a lightweight Vision-and-Language Transformer (ViLT) model which is fine-tuned \u2026"}, {"title": "Language Models as SPARQL Query Filtering for Improving the Quality of Multilingual Question Answering over Knowledge Graphs", "link": "https://link.springer.com/chapter/10.1007/978-3-031-62362-2_1", "details": "A Perevalov, A Gashkov, M Eltsova, A Both - International Conference on Web \u2026, 2024", "abstract": "Question Answering systems working over Knowledge Graphs (KGQA) generate a ranked list of SPARQL query candidates for a given natural-language question. In this paper, we follow our long-term research agenda of providing trustworthy KGQA \u2026"}, {"title": "Large Language Model as a Universal Clinical Multi-task Decoder", "link": "https://arxiv.org/pdf/2406.12738", "details": "Y Wu, H Song, J Zhang, X Wen, S Zheng, J Bian - arXiv preprint arXiv:2406.12738, 2024", "abstract": "The development of effective machine learning methodologies for enhancing the efficiency and accuracy of clinical systems is crucial. Despite significant research efforts, managing a plethora of diversified clinical tasks and adapting to emerging \u2026"}, {"title": "Enhancing text-based knowledge graph completion with zero-shot large language models: A focus on semantic enhancement", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124007895", "details": "R Yang, J Zhu, J Man, L Fang, Y Zhou - Knowledge-Based Systems, 2024", "abstract": "The design and development of text-based knowledge graph completion (KGC) methods leveraging textual entity descriptions are at the forefront of research. These methods involve advanced optimization techniques such as soft prompts and \u2026"}, {"title": "Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation", "link": "https://arxiv.org/pdf/2407.03056", "details": "M Mistretta, A Baldrati, M Bertini, AD Bagdanov - arXiv preprint arXiv:2407.03056, 2024", "abstract": "Vision-Language Models (VLMs) demonstrate remarkable zero-shot generalization to unseen tasks, but fall short of the performance of supervised methods in generalizing to downstream tasks with limited data. Prompt learning is emerging as a \u2026"}]
