[{"title": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning", "link": "https://arxiv.org/pdf/2411.05193", "details": "J Hong, A Dragan, S Levine - arXiv preprint arXiv:2411.05193, 2024", "abstract": "Value-based reinforcement learning (RL) can in principle learn effective policies for a wide range of multi-turn problems, from games to dialogue to robotic control, including via offline RL from static previously collected datasets. However, despite \u2026"}, {"title": "Frozen Large-scale Pretrained Vision-Language Models are the Effective Foundational Backbone for Multimodal Breast Cancer Prediction", "link": "https://ieeexplore.ieee.org/iel8/6221020/6363502/10769012.pdf", "details": "HQ Vo, L Wang, KK Wong, CF Ezeana, X Yu, W Yang\u2026 - IEEE Journal of Biomedical \u2026, 2024", "abstract": "Breast cancer is a pervasive global health concern among women. Leveraging multimodal data from enterprise patient databases-including Picture Archiving and Communication Systems (PACS) and Electronic Health Records (EHRs)-holds \u2026"}, {"title": "Accelerating Blockwise Parallel Language Models with Draft Refinement", "link": "https://openreview.net/pdf%3Fid%3DKT6F5Sw0eg", "details": "T Kim, AT Suresh, KA Papineni, M Riley, S Kumar\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Autoregressive language models have achieved remarkable advancements, yet their potential is often limited by the slow inference speeds associated with sequential token generation. Blockwise parallel decoding (BPD) was proposed by Stern et \u2026"}, {"title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "link": "https://openreview.net/pdf%3Fid%3DIRXyPm9IPW", "details": "X Wu, S Huang, G Wang, J Xiong, F Wei - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts. Despite these advances \u2026"}, {"title": "Training and Evaluating Language Models with Template-based Data Generation", "link": "https://arxiv.org/pdf/2411.18104", "details": "Y Zhang - arXiv preprint arXiv:2411.18104, 2024", "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, these \u2026"}, {"title": "Constraint Back-translation Improves Complex Instruction Following of Large Language Models", "link": "https://arxiv.org/pdf/2410.24175", "details": "Y Qi, H Peng, X Wang, B Xu, L Hou, J Li - arXiv preprint arXiv:2410.24175, 2024", "abstract": "Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs \u2026"}, {"title": "DoubleCCA: Improving Foundation Model Group Robustness with Random Sentence Embeddings", "link": "https://arxiv.org/pdf/2411.16236", "details": "H Liu, Y Lu - arXiv preprint arXiv:2411.16236, 2024", "abstract": "This paper presents a novel method to improve the robustness of foundation models to group-based biases. We propose a simple yet effective method, called DoubleCCA, that leverages random sentences and Canonical Correlation Analysis \u2026"}]
