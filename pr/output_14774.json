[{"title": "Revealing the Impact of Pre-training Data on Medical Foundation Models", "link": "https://www.researchsquare.com/article/rs-6080254/latest", "details": "Y Zhou, Z Wang, Y Wu, AY Ong, S Wagner, E Ruffell\u2026 - 2025", "abstract": "Medical foundation models (FM), pre-trained on large-scale unlabelled data, have demonstrated robust performance and high efficiency when fine-tuned to various clinically relevant applications. However, the impact of pre-training data on medical \u2026"}, {"title": "Generative Evaluation of Complex Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2504.02810", "details": "H Lin, X Wang, R Yan, B Huang, H Ye, J Zhu, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly \u2026"}, {"title": "Semantic Retrieval Augmented Contrastive Learning for Sequential Recommendation", "link": "https://arxiv.org/pdf/2503.04162", "details": "Z Cui, Y Weng, X Tang, X Zhang, D Liu, S Li, P Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Sequential recommendation aims to model user preferences based on historical behavior sequences, which is crucial for various online platforms. Data sparsity remains a significant challenge in this area as most users have limited interactions \u2026"}]
