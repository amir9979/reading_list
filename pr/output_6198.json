[{"title": "iHealth-Chile-3&2 at RRG24: Template Based Report Generation", "link": "https://aclanthology.org/2024.bionlp-1.53.pdf", "details": "O Loch, P Messina, R Elberg, D Campanini, \u00c1 Soto\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "This paper presents the approaches of the iHealth-Chile-3 and iHealth-Chile-2 teams for the shared task of Large-Scale Radiology Report Generation at the BioNLP workshop. Inspired by prior work on template-based report generation, both teams \u2026"}, {"title": "Masked autoencoder: influence of self-supervised pretraining on object segmentation in industrial images", "link": "https://link.springer.com/article/10.1007/s44244-024-00020-y", "details": "A Witte, S Lange, C Lins - Industrial Artificial Intelligence, 2024", "abstract": "The amount of labelled data in industrial use cases is limited because the annotation process is time-consuming and costly. As in research, self-supervised pretraining such as MAE resulted in training segmentation models with fewer labels, this is also \u2026"}, {"title": "Geometric View of Soft Decorrelation in Self-Supervised Learning", "link": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671914", "details": "Y Zhang, H Zhu, Z Song, Y Chen, X Fu, Z Meng\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "Contrastive learning, a form of Self-Supervised Learning (SSL), typically consists of an alignment term and a regularization term. The alignment term minimizes the distance between the embeddings of a positive pair, while the regularization term \u2026"}]
