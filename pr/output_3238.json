[{"title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models", "link": "https://arxiv.org/pdf/2406.02224", "details": "T Fan, G Ma, Y Kang, H Gu, L Fan, Q Yang - arXiv preprint arXiv:2406.02224, 2024", "abstract": "Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small \u2026"}, {"title": "DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features", "link": "https://arxiv.org/pdf/2406.12095", "details": "L Wang, SW Kim, J Yang, C Yu, B Ivanovic\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We propose DistillNeRF, a self-supervised learning framework addressing the challenge of understanding 3D environments from limited 2D observations in autonomous driving. Our method is a generalizable feedforward model that predicts \u2026"}, {"title": "ASIMO: Agent-centric scene representation in multi-object manipulation", "link": "https://journals.sagepub.com/doi/abs/10.1177/02783649241257537", "details": "CH Min, YM Kim - The International Journal of Robotics Research, 2024", "abstract": "Vision-based reinforcement learning (RL) is a generalizable way to control an agent because it is agnostic of specific hardware configurations. As visual observations are highly entangled, attempts for vision-based RL rely on scene representation that \u2026"}, {"title": "LTRC-IIITH at MEDIQA-M3G 2024: Medical Visual Question Answering with Vision-Language Models", "link": "https://aclanthology.org/2024.clinicalnlp-1.67.pdf", "details": "J Thomas, S Marimuthu, P Krishnamurthy - Proceedings of the 6th Clinical Natural \u2026, 2024", "abstract": "In this paper, we present our work to the MEDIQA-M3G 2024 shared task, which tackles multilingual and multimodal medical answer generation. Our system consists of a lightweight Vision-and-Language Transformer (ViLT) model which is fine-tuned \u2026"}, {"title": "Unicoder: Scaling code large language model via universal code", "link": "https://arxiv.org/pdf/2406.16441", "details": "T Sun, L Chai, J Yang, Y Yin, H Guo, J Liu, B Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various downstream natural language processing (NLP) tasks. When applying LLMs for code generation, recent works mainly focus on \u2026"}, {"title": "CPLIP: Zero-Shot Learning for Histopathology with Comprehensive Vision-Language Alignment", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Javed_CPLIP_Zero-Shot_Learning_for_Histopathology_with_Comprehensive_Vision-Language_Alignment_CVPR_2024_paper.pdf", "details": "S Javed, A Mahmood, II Ganapathi, FA Dharejo\u2026 - Proceedings of the IEEE \u2026, 2024", "abstract": "Abstract This paper proposes Comprehensive Pathology Language Image Pre- training (CPLIP) a new unsupervised technique designed to enhance the alignment of images and text in histopathology for tasks such as classification and \u2026"}]
