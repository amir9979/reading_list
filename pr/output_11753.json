[{"title": "Sentiment works in small-cap stocks: Japanese stock's sentiment with language models", "link": "https://www.sciencedirect.com/science/article/pii/S2667096824001071", "details": "M Suzuki, Y Ishikawa, M Teraguchi, H Sakaji - International Journal of Information \u2026, 2025", "abstract": "We calculate sentiment from the Japanese Company Handbook, which contains a compact overview of Japanese companies' business situation and financial data, using multiple methods, including large language models. Language models such \u2026"}, {"title": "A Two-Stage Pretraining-Finetuning Framework for Treatment Effect Estimation with Unmeasured Confounding", "link": "https://arxiv.org/pdf/2501.08888", "details": "C Zhou, Y Li, C Zheng, H Zhang, M Zhang, H Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Estimating the conditional average treatment effect (CATE) from observational data plays a crucial role in areas such as e-commerce, healthcare, and economics. Existing studies mainly rely on the strong ignorability assumption that there are no \u2026"}, {"title": "INSNER: A generative instruction-based prompting method for boosting performance in few-shot NER", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324003996", "details": "P Zhao, C Feng, P Li, G Dong, S Wang - Information Processing & Management, 2025", "abstract": "Abstract Most existing Named Entity Recognition (NER) methods require a large scale of labeled data and exhibit poor performance in low-resource scenarios. Thus in this paper, we propose INSNER, a generative INStruction-based prompting \u2026"}, {"title": "LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models", "link": "https://arxiv.org/pdf/2501.00055", "details": "M Yu, J Fang, Y Zhou, X Fan, K Wang, S Pan, Q Wen - arXiv preprint arXiv \u2026, 2024", "abstract": "While safety-aligned large language models (LLMs) are increasingly used as the cornerstone for powerful systems such as multi-agent frameworks to solve complex real-world problems, they still suffer from potential adversarial queries, such as \u2026"}, {"title": "Boosting LLM via Learning from Data Iteratively and Selectively", "link": "https://arxiv.org/pdf/2412.17365%3F", "details": "Q Jia, S Ren, Z Qin, F Xue, J Ni, Y You - arXiv preprint arXiv:2412.17365, 2024", "abstract": "Datasets nowadays are generally constructed from multiple sources and using different synthetic techniques, making data de-noising and de-duplication crucial before being used for post-training. In this work, we propose to perform instruction \u2026"}, {"title": "CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis", "link": "https://arxiv.org/pdf/2501.01668", "details": "B Zhang, X Zhang, J Zhang, J Yu, S Luo, J Tang - arXiv preprint arXiv:2501.01668, 2025", "abstract": "Current inference scaling methods, such as Self-consistency and Best-of-N, have proven effective in improving the accuracy of LLMs on complex reasoning tasks. However, these methods rely heavily on the quality of candidate responses and are \u2026"}]
