[{"title": "CARE: Aligning Language Models for Regional Cultural Awareness", "link": "https://arxiv.org/pdf/2504.05154", "details": "G Guo, T Naous, H Wakaki, Y Nishimura, Y Mitsufuji\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing language models (LMs) often exhibit a Western-centric bias and struggle to represent diverse cultural knowledge. Previous attempts to address this rely on synthetic data and express cultural knowledge only in English. In this work, we study \u2026"}, {"title": "Optimizing generative AI by backpropagating language model feedback", "link": "https://www.nature.com/articles/s41586-025-08661-4", "details": "M Yuksekgonul, F Bianchi, J Boen, S Liu, P Lu\u2026 - Nature, 2025", "abstract": "Recent breakthroughs in artificial intelligence (AI) are increasingly driven by systems orchestrating multiple large language models (LLMs) and other specialized tools, such as search engines and simulators. So far, these systems are primarily \u2026"}, {"title": "Superbpe: Space travel for language models", "link": "https://arxiv.org/pdf/2503.13423", "details": "A Liu, J Hayase, V Hofmann, S Oh, NA Smith, Y Choi - arXiv preprint arXiv \u2026, 2025", "abstract": "The assumption across nearly all language model (LM) tokenization schemes is that tokens should be subwords, ie, contained within word boundaries. While providing a seemingly reasonable inductive bias, is this common practice limiting the potential of \u2026"}, {"title": "Benchmarking Failures in Tool-Augmented Language Models", "link": "https://arxiv.org/pdf/2503.14227", "details": "E Trevi\u00f1o, H Contant, J Ngai, G Neubig, ZZ Wang - arXiv preprint arXiv:2503.14227, 2025", "abstract": "The integration of tools has extended the capabilities of language models (LMs) beyond vanilla text generation to versatile scenarios. However, tool-augmented language models (TaLMs) often assume'perfect'information access and tool \u2026"}, {"title": "Scaling Laws of Synthetic Data for Language Models", "link": "https://arxiv.org/pdf/2503.19551", "details": "Z Qin, Q Dong, X Zhang, L Dong, X Huang, Z Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) achieve strong performance across diverse tasks, largely driven by high-quality web data used in pre-training. However, recent studies indicate this data source is rapidly depleting. Synthetic data emerges as a promising \u2026"}, {"title": "Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation", "link": "https://arxiv.org/pdf/2504.02438", "details": "C Cheng, J Guan, W Wu, R Yan - arXiv preprint arXiv:2504.02438, 2025", "abstract": "Long-form video processing fundamentally challenges vision-language models (VLMs) due to the high computational costs of handling extended temporal sequences. Existing token pruning and feature merging methods often sacrifice \u2026"}, {"title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models", "link": "https://arxiv.org/pdf/2504.05258", "details": "A Bazaga, R Blloshmi, B Byrne, A de Gispert - arXiv preprint arXiv:2504.05258, 2025", "abstract": "Large Language Models (LLMs) have emerged as powerful tools for generating coherent text, understanding context, and performing reasoning tasks. However, they struggle with temporal reasoning, which requires processing time-related information \u2026"}, {"title": "ReMoGPT: Part-Level Retrieval-Augmented Motion-Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/33044/35199", "details": "Q Yu, M Tanaka, K Fujiwara - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Generation of 3D human motion holds significant importance in the creative industry. While recent notable advances have been made in generating common motions, existing methods struggle to generate diverse and rare motions due to the complexity \u2026"}, {"title": "The Deployment of End-to-End Audio Language Models Should Take into Account the Principle of Least Privilege", "link": "https://arxiv.org/pdf/2503.16833%3F", "details": "L He, X Qi, M Liao, I Cheong, P Mittal, D Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We are at a turning point for language models that accept audio input. The latest end- to-end audio language models (Audio LMs) process speech directly instead of relying on a separate transcription step. This shift preserves detailed information \u2026"}]
