[{"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}, {"title": "A Distributional Perspective on Word Learning in Neural Language Models", "link": "https://arxiv.org/pdf/2502.05892", "details": "F Ficarra, R Cotterell, A Warstadt - arXiv preprint arXiv:2502.05892, 2025", "abstract": "Language models (LMs) are increasingly being studied as models of human language learners. Due to the nascency of the field, it is not well-established whether LMs exhibit similar learning dynamics to humans, and there are few direct \u2026"}, {"title": "Self-supervised analogical learning using language models", "link": "https://arxiv.org/pdf/2502.00996", "details": "B Zhou, S Jain, Y Zhang, Q Ning, S Wang, Y Benajiba\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can \u2026"}, {"title": "EHealth: A Chinese Biomedical Language Model Built via Multi-Level Text Discrimination", "link": "https://ieeexplore.ieee.org/abstract/document/10857372/", "details": "Q Wang, S Dai, B Xu, Y Lyu, H Wu, H Wang - IEEE Transactions on Audio, Speech \u2026, 2025", "abstract": "Pre-trained language models (PLMs) have recently revolutionized the field of natural language processing, impacting not only the general domain but also the biomedical domain. Most previous studies on constructing biomedical PLMs relied simply on \u2026"}, {"title": "How Much Do Code Language Models Remember? An Investigation on Data Extraction Attacks before and after Fine-tuning", "link": "https://arxiv.org/pdf/2501.17501", "details": "F Salerno, A Al-Kaswan, M Izadi - arXiv preprint arXiv:2501.17501, 2025", "abstract": "Code language models, while widely popular, are often trained on unsanitized source code gathered from across the Internet. Previous work revealed that pre- trained models can remember the content of their training data and regurgitate them \u2026"}, {"title": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models", "link": "https://arxiv.org/pdf/2501.17420", "details": "Y Li, H Shirado, S Das - arXiv preprint arXiv:2501.17420, 2025", "abstract": "While advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating \u2026"}, {"title": "Engaging Preference Optimization Alignment in Large Language Model for Continual Radiology Report Generation: A Hybrid Approach", "link": "https://link.springer.com/article/10.1007/s12559-025-10404-6", "details": "A Izhar, N Idris, N Japar - Cognitive Computation, 2025", "abstract": "Large language models (LLMs) remain relatively underutilized in medical imaging, particularly in radiology, which is essential for disease diagnosis and management. Nonetheless, radiology report generation (RRG) is a time-consuming task that can \u2026"}, {"title": "VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic Health Records", "link": "https://arxiv.org/pdf/2501.16672%3F", "details": "P Chung, A Swaminathan, AJ Goodell, Y Kim\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Methods to ensure factual accuracy of text generated by large language models (LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence system that combines retrieval-augmented generation and LLM-as-a-Judge to verify whether \u2026"}, {"title": "Multilingual Language Model Pretraining using Machine-translated Data", "link": "https://arxiv.org/pdf/2502.13252", "details": "J Wang, Y Lu, M Weber, M Ryabinin, D Adelani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-resource languages such as English, enables the pretraining of high-quality large language models (LLMs). The same can not be said for most other languages as LLMs still underperform for non-English languages, likely due to a gap in the \u2026"}]
