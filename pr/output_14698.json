[{"title": "Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving", "link": "https://arxiv.org/pdf/2503.21505", "details": "Y Li, M Tian, Z Lin, J Zhu, D Zhu, H Liu, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing benchmarks for Vision-Language Model (VLM) on autonomous driving (AD) primarily assess interpretability through open-form visual question answering (QA) within coarse-grained tasks, which remain insufficient to assess capabilities in \u2026"}, {"title": "Multi-Cue Adaptive Visual Token Pruning for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.08019%3F", "details": "B Luan, W Zhou, H Feng, Z Wang, X Li, H Li - arXiv preprint arXiv:2503.08019, 2025", "abstract": "As the computational needs of Large Vision-Language Models (LVLMs) increase, visual token pruning has proven effective in improving inference speed and memory efficiency. Traditional pruning methods in LVLMs predominantly focus on attention \u2026"}, {"title": "Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic Evaluation of Language Models", "link": "https://arxiv.org/pdf/2504.01001", "details": "J Pombal, NM Guerreiro, R Rei, AFT Martins - arXiv preprint arXiv:2504.01001, 2025", "abstract": "As language models improve and become capable of performing more complex tasks across modalities, evaluating them automatically becomes increasingly challenging. Developing strong and robust task-specific automatic metrics gets \u2026"}, {"title": "Can Vision-Language Models Answer Face to Face Questions in the Real-World?", "link": "https://arxiv.org/pdf/2503.19356", "details": "R Pourreza, R Dagli, A Bhattacharyya, S Panchal\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "AI models have made significant strides in recent years in their ability to describe and answer questions about real-world images. They have also made progress in the ability to converse with users in real-time using audio input. This raises the question \u2026"}, {"title": "Beyond Standard MoE: Mixture of Latent Experts for Resource-Efficient Language Models", "link": "https://arxiv.org/pdf/2503.23100", "details": "Z Liu, H Wu, R She, X Fu, X Han, T Zhong, M Yuan - arXiv preprint arXiv:2503.23100, 2025", "abstract": "Mixture of Experts (MoE) has emerged as a pivotal architectural paradigm for efficient scaling of Large Language Models (LLMs), operating through selective activation of parameter subsets for each input token. Nevertheless, conventional MoE \u2026"}, {"title": "Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models", "link": "https://media.roboflow.com/rf100vl/rf100vl.pdf", "details": "P Robicheaux, M Popov, A Madan, I Robinson\u2026", "abstract": "Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to outof-distribution tasks \u2026"}, {"title": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL", "link": "https://arxiv.org/pdf/2503.23157", "details": "M Pourreza, S Talaei, R Sun, X Wan, H Li, A Mirhoseini\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted \u2026"}, {"title": "Towards Interpretable Counterfactual Generation via Multimodal Autoregression", "link": "https://arxiv.org/pdf/2503.23149", "details": "C Ma, Y Ji, J Ye, L Zhang, Y Chen, T Li, M Li, J He\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Counterfactual medical image generation enables clinicians to explore clinical hypotheses, such as predicting disease progression, facilitating their decision- making. While existing methods can generate visually plausible images from disease \u2026"}, {"title": "Federated Continual Instruction Tuning", "link": "https://arxiv.org/pdf/2503.12897", "details": "H Guo, F Zeng, F Zhu, W Liu, DH Wang, J Xu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "A vast amount of instruction tuning data is crucial for the impressive performance of Large Multimodal Models (LMMs), but the associated computational costs and data collection demands during supervised fine-tuning make it impractical for most \u2026"}]
