[{"title": "MOMA: Contrastive Learning Distills Better Masked Autoencoders", "link": "https://link.springer.com/chapter/10.1007/978-3-031-78398-2_5", "details": "Y Yao, N Desai, M Palaniswami - International Conference on Pattern Recognition, 2024", "abstract": "Self-supervised learning has achieved remarkable performance in computer vision, utilizing two key paradigms: contrastive learning and masked image modeling. Contrastive learning focuses on global representations by learning similarities and \u2026"}, {"title": "Pre-Training Graph Contrastive Masked Autoencoders are Strong Distillers for EEG", "link": "https://arxiv.org/pdf/2411.19230", "details": "X Wei, K Zhao, Y Jiao, NB Carlisle, H Xie, Y Zhang - arXiv preprint arXiv:2411.19230, 2024", "abstract": "Effectively utilizing extensive unlabeled high-density EEG data to improve performance in scenarios with limited labeled low-density EEG data presents a significant challenge. In this paper, we address this by framing it as a graph transfer \u2026"}, {"title": "Label-Expanded Feature Debiasing for Single Domain Generalization", "link": "https://link.springer.com/chapter/10.1007/978-3-031-78128-5_13", "details": "J Yang, L Jing, Y Xu, S Wu, S Drew, X Niu - International Conference on Pattern \u2026, 2024", "abstract": "In single domain generalization, the model is trained on a single source domain and requires being generalized to multiple unseen target domains. However, the presence of domain discrepancies poses a significant threat to this goal. A \u2026"}, {"title": "EdgeConvFormer: An Unsupervised Anomaly Detection Method for Multivariate Time Series", "link": "https://link.springer.com/chapter/10.1007/978-3-031-78128-5_24", "details": "J Liu, Q Li, S An, B Ezard, L Li - International Conference on Pattern Recognition, 2024", "abstract": "In this paper, EdgeConvFormer is introduced as a novel approach to unsupervised anomaly detection in multivariate time series, combining the strengths of graph convolutions and Transformers within a hierarchical structure. The model utilizes \u2026"}, {"title": "Exploring the Trade-Off in the Variational Information Bottleneck for Regression with a Single Training Run", "link": "https://www.mdpi.com/1099-4300/26/12/1043", "details": "S Kudo, N Ono, S Kanaya, M Huang - Entropy, 2024", "abstract": "An information bottleneck (IB) enables the acquisition of useful representations from data by retaining necessary information while reducing unnecessary information. In its objective function, the Lagrange multiplier \u03b2 controls the trade-off between \u2026"}, {"title": "You Don't Need Domain-Specific Data Augmentations When Scaling Self-Supervised Learning", "link": "https://openreview.net/pdf%3Fid%3D7RwKMRMNrc", "details": "T Moutakanni, M Oquab, M Szafraniec\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Self-Supervised learning (SSL) with Joint-Embedding Architectures (JEA) has led to outstanding performances. All instantiations of this paradigm were trained using strong and well-established hand-crafted data augmentations, leading to the general \u2026"}]
