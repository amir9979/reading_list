[{"title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?", "link": "https://arxiv.org/pdf/2410.23856", "details": "Z Zhou, R Tao, J Zhu, Y Luo, Z Wang, B Han - arXiv preprint arXiv:2410.23856, 2024", "abstract": "This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We \u2026"}, {"title": "Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies", "link": "https://arxiv.org/pdf/2410.22886", "details": "S Salhan, RD Martinez, Z Goriely, P Buttery - arXiv preprint arXiv:2410.22886, 2024", "abstract": "Curriculum Learning has been a popular strategy to improve the cognitive plausibility of Small-Scale Language Models (SSLMs) in the BabyLM Challenge. However, it has not led to considerable improvements over non-curriculum models. We assess \u2026"}, {"title": "Identifying stigmatizing and positive/preferred language in obstetric clinical notes using natural language processing", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae290/7906099", "details": "JK Scroggins, II Hulchafo, S Harkins, D Scharp\u2026 - Journal of the American \u2026, 2024", "abstract": "Objective To identify stigmatizing language in obstetric clinical notes using natural language processing (NLP). Materials and Methods We analyzed electronic health records from birth admissions in the Northeast United States in 2017. We annotated \u2026"}, {"title": "Identifying and Characterizing Bias at Scale in Clinical Notes Using Large Language Models", "link": "https://www.medrxiv.org/content/10.1101/2024.10.24.24316073.full.pdf", "details": "DU Apakama, KAN Nguyen, D Hyppolite, S Soffer\u2026 - medRxiv, 2024", "abstract": "Importance. Discriminatory language in clinical documentation impacts patient care and reinforces systemic biases. Scalable tools to detect and mitigate this are needed. Objective. Determine utility of a frontier large language model (GPT-4) in identifying \u2026"}, {"title": "Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text", "link": "https://aclanthology.org/2024.conll-1.27.pdf", "details": "S Adak, D Agrawal, A Mukherjee, S Aditya - Proceedings of the 28th Conference on \u2026, 2024", "abstract": "We investigate the knowledge of object affordances in pre-trained language models (LMs) and pre-trained Vision-Language models (VLMs). A growing body of literature shows that PTLMs fail inconsistently and non-intuitively, demonstrating a lack of \u2026"}, {"title": "VILA-M3: Enhancing Vision-Language Models with Medical Expert Knowledge", "link": "https://arxiv.org/pdf/2411.12915", "details": "V Nath, W Li, D Yang, A Myronenko, M Zheng, Y Lu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generalist vision language models (VLMs) have made significant strides in computer vision, but they fall short in specialized fields like healthcare, where expert knowledge is essential. In traditional computer vision tasks, creative or approximate \u2026"}, {"title": "A Survey of Small Language Models", "link": "https://arxiv.org/abs/2410.20011", "details": "C Van Nguyen, X Shen, R Aponte, Y Xia, S Basu, Z Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Small Language Models (SLMs) have become increasingly important due to their efficiency and performance to perform various language tasks with minimal computational resources, making them ideal for various settings including on-device \u2026"}, {"title": "Human-level information extraction from clinical reports with fine-tuned language models", "link": "https://www.medrxiv.org/content/10.1101/2024.11.18.24317466.full.pdf", "details": "L Liu, L Lian, Y Hao, A Pace, E Kim, N Homsi\u2026 - medRxiv, 2024", "abstract": "Background: Extracting structured data from clinical notes is a key bottleneck in developing AI tools for radiology and pathology. Manual annotation is labor-intensive and unscalable. An efficient, automated method for clinical information extraction \u2026"}, {"title": "Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings", "link": "https://arxiv.org/pdf/2410.22685", "details": "YS Grewal, EV Bonilla, TD Bui - arXiv preprint arXiv:2410.22685, 2024", "abstract": "Accurately quantifying uncertainty in large language models (LLMs) is crucial for their reliable deployment, especially in high-stakes applications. Current state-of-the- art methods for measuring semantic uncertainty in LLMs rely on strict bidirectional \u2026"}]
