'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Demographic bias in misdiagnosis by computational pathology '
[{"title": "Comprehensive Study on German Language Models for Clinical and Biomedical Text Understanding", "link": "https://arxiv.org/pdf/2404.05694", "details": "A Idrissi-Yaghir, A Dada, H Sch\u00e4fer, K Arzideh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in natural language processing (NLP) can be largely attributed to the advent of pre-trained language models such as BERT and RoBERTa. While these models demonstrate remarkable performance on general datasets, they can \u2026"}, {"title": "PMC-LLaMA: toward building open-source language models for medicine", "link": "https://academic.oup.com/jamia/advance-article-abstract/doi/10.1093/jamia/ocae045/7645318", "details": "C Wu, W Lin, X Zhang, Y Zhang, W Xie, Y Wang - Journal of the American Medical \u2026, 2024", "abstract": "Objective Recently, large language models (LLMs) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering (QA) situations, these models \u2026"}, {"title": "Predicting Future Disorders via Temporal Knowledge Graphs and Medical Ontologies", "link": "https://ieeexplore.ieee.org/iel7/6221020/6363502/10504898.pdf", "details": "M Postiglione, D Bean, Z Kraljevic, RJB Dobson\u2026 - IEEE Journal of Biomedical \u2026, 2024", "abstract": "Despite the vast potential for insights and value present in Electronic Health Records (EHRs), it is challenging to fully leverage all the available information, particularly that contained in the free-text data written by clinicians describing the health status of \u2026"}, {"title": "Effects of Different Prompts on the Quality of GPT-4 Responses to Dementia Care Questions", "link": "https://arxiv.org/pdf/2404.08674", "details": "Z Li, B Xie, R Hilsabeck, A Aguirre, N Zou, Z Luo, D He - arXiv preprint arXiv \u2026, 2024", "abstract": "Evidence suggests that different prompts lead large language models (LLMs) to generate responses with varying quality. Yet, little is known about prompts' effects on response quality in healthcare domains. In this exploratory study, we address this \u2026"}]
