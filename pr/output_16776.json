[{"title": "Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework", "link": "https://arxiv.org/pdf/2505.15245", "details": "Z Jiang, B Liu, M Peng, W Xu, Y Xiao, Z Shan, M Peng - arXiv preprint arXiv \u2026, 2025", "abstract": "While large language models (LLMs) show great potential in temporal reasoning, most existing work focuses heavily on enhancing performance, often neglecting the explainable reasoning processes underlying the results. To address this gap, we \u2026", "entry_id": "http://arxiv.org/abs/2505.15245v1", "updated": "2025-05-21 08:20:35", "published": "2025-05-21 08:20:35", "authors": "Zihao Jiang;Ben Liu;Miao Peng;Wenjie Xu;Yao Xiao;Zhenyan Shan;Min Peng", "summary": "While large language models (LLMs) show great potential in temporal\nreasoning, most existing work focuses heavily on enhancing performance, often\nneglecting the explainable reasoning processes underlying the results. To\naddress this gap, we introduce a comprehensive benchmark covering a wide range\nof temporal granularities, designed to systematically evaluate LLMs'\ncapabilities in explainable temporal reasoning. Furthermore, our findings\nreveal that LLMs struggle to deliver convincing explanations when relying\nsolely on textual information. To address challenge, we propose GETER, a novel\nstructure-aware generative framework that integrates Graph structures with text\nfor Explainable TEmporal Reasoning. Specifically, we first leverage temporal\nknowledge graphs to develop a temporal encoder that captures structural\ninformation for the query. Subsequently, we introduce a structure-text prefix\nadapter to map graph structure features into the text embedding space. Finally,\nLLMs generate explanation text by seamlessly integrating the soft graph token\nwith instruction-tuning prompt tokens. Experimental results indicate that GETER\nachieves state-of-the-art performance while also demonstrating its\neffectiveness as well as strong generalization capabilities. Our dataset and\ncode are available at https://github.com/carryTatum/GETER.", "comment": "In Findings of the Association for Computational Linguistics: ACL\n  2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.15245v1;http://arxiv.org/pdf/2505.15245v1", "pdf_url": "http://arxiv.org/pdf/2505.15245v1"}, {"title": "Model Merging in Pre-training of Large Language Models", "link": "https://arxiv.org/pdf/2505.12082", "details": "Y Li, Y Ma, S Yan, C Zhang, J Liu, J Lu, Z Xu, M Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model \u2026", "entry_id": "http://arxiv.org/abs/2505.12082v3", "updated": "2025-05-22 09:35:43", "published": "2025-05-17 16:53:14", "authors": "Yunshui Li;Yiyuan Ma;Shen Yan;Chaoyi Zhang;Jing Liu;Jianqiao Lu;Ziwen Xu;Mengzhao Chen;Minrui Wang;Shiyi Zhan;Jin Ma;Xunhao Lai;Deyi Liu;Yao Luo;Xingyan Bin;Hongbin Ren;Mingji Han;Wenhao Hao;Bairen Yi;LingJun Liu;Bole Ma;Xiaoying Jia;Xun Zhou;Siyuan Qiao;Liang Xiang;Yonghui Wu", "summary": "Model merging has emerged as a promising technique for enhancing large\nlanguage models, though its application in large-scale pre-training remains\nrelatively unexplored. In this paper, we present a comprehensive investigation\nof model merging techniques during the pre-training process. Through extensive\nexperiments with both dense and Mixture-of-Experts (MoE) architectures ranging\nfrom millions to over 100 billion parameters, we demonstrate that merging\ncheckpoints trained with constant learning rates not only achieves significant\nperformance improvements but also enables accurate prediction of annealing\nbehavior. These improvements lead to both more efficient model development and\nsignificantly lower training costs. Our detailed ablation studies on merging\nstrategies and hyperparameters provide new insights into the underlying\nmechanisms while uncovering novel applications. Through comprehensive\nexperimental analysis, we offer the open-source community practical\npre-training guidelines for effective model merging.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.12082v3;http://arxiv.org/pdf/2505.12082v3", "pdf_url": "http://arxiv.org/pdf/2505.12082v3"}, {"title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models", "link": "https://arxiv.org/pdf/2505.15293", "details": "Q Hao, Y Song, Q Liao, J Yuan, Y Li - arXiv preprint arXiv:2505.15293, 2025", "abstract": "Policy exploration is critical in reinforcement learning (RL), where existing approaches include greedy, Gaussian process, etc. However, these approaches utilize preset stochastic processes and are indiscriminately applied in all kinds of RL \u2026", "entry_id": "http://arxiv.org/abs/2505.15293v1", "updated": "2025-05-21 09:24:23", "published": "2025-05-21 09:24:23", "authors": "Qianyue Hao;Yiwen Song;Qingmin Liao;Jian Yuan;Yong Li", "summary": "Policy exploration is critical in reinforcement learning (RL), where existing\napproaches include greedy, Gaussian process, etc. However, these approaches\nutilize preset stochastic processes and are indiscriminately applied in all\nkinds of RL tasks without considering task-specific features that influence\npolicy exploration. Moreover, during RL training, the evolution of such\nstochastic processes is rigid, which typically only incorporates a decay in the\nvariance, failing to adjust flexibly according to the agent's real-time\nlearning status. Inspired by the analyzing and reasoning capability of large\nlanguage models (LLMs), we design LLM-Explorer to adaptively generate\ntask-specific exploration strategies with LLMs, enhancing the policy\nexploration in RL. In our design, we sample the learning trajectory of the\nagent during the RL training in a given task and prompt the LLM to analyze the\nagent's current policy learning status and then generate a probability\ndistribution for future policy exploration. Updating the probability\ndistribution periodically, we derive a stochastic process specialized for the\nparticular task and dynamically adjusted to adapt to the learning process. Our\ndesign is a plug-in module compatible with various widely applied RL\nalgorithms, including the DQN series, DDPG, TD3, and any possible variants\ndeveloped based on them. Through extensive experiments on the Atari and MuJoCo\nbenchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy\nexploration, achieving an average performance improvement up to 37.27%. Our\ncode is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for\nreproducibility.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.15293v1;http://arxiv.org/pdf/2505.15293v1", "pdf_url": "http://arxiv.org/pdf/2505.15293v1"}, {"title": "Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation", "link": "https://arxiv.org/pdf/2505.11995", "details": "Y Wang, R Ren, Y Wang, WX Zhao, J Liu, H Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Considering the inherent limitations of parametric knowledge in large language models (LLMs), retrieval-augmented generation (RAG) is widely employed to expand their knowledge scope. Since RAG has shown promise in knowledge-intensive tasks \u2026", "entry_id": "http://arxiv.org/abs/2505.11995v1", "updated": "2025-05-17 13:13:13", "published": "2025-05-17 13:13:13", "authors": "Yuhao Wang;Ruiyang Ren;Yucheng Wang;Wayne Xin Zhao;Jing Liu;Hua Wu;Haifeng Wang", "summary": "Considering the inherent limitations of parametric knowledge in large\nlanguage models (LLMs), retrieval-augmented generation (RAG) is widely employed\nto expand their knowledge scope. Since RAG has shown promise in\nknowledge-intensive tasks like open-domain question answering, its broader\napplication to complex tasks and intelligent assistants has further advanced\nits utility. Despite this progress, the underlying knowledge utilization\nmechanisms of LLM-based RAG remain underexplored. In this paper, we present a\nsystematic investigation of the intrinsic mechanisms by which LLMs integrate\ninternal (parametric) and external (retrieved) knowledge in RAG scenarios.\nSpecially, we employ knowledge stream analysis at the macroscopic level, and\ninvestigate the function of individual modules at the microscopic level.\nDrawing on knowledge streaming analyses, we decompose the knowledge utilization\nprocess into four distinct stages within LLM layers: knowledge refinement,\nknowledge elicitation, knowledge expression, and knowledge contestation. We\nfurther demonstrate that the relevance of passages guides the streaming of\nknowledge through these stages. At the module level, we introduce a new method,\nknowledge activation probability entropy (KAPE) for neuron identification\nassociated with either internal or external knowledge. By selectively\ndeactivating these neurons, we achieve targeted shifts in the LLM's reliance on\none knowledge source over the other. Moreover, we discern complementary roles\nfor multi-head attention and multi-layer perceptron layers during knowledge\nformation. These insights offer a foundation for improving interpretability and\nreliability in retrieval-augmented LLMs, paving the way for more robust and\ntransparent generative solutions in knowledge-intensive domains.", "comment": "SIGIR 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.11995v1;http://arxiv.org/pdf/2505.11995v1", "pdf_url": "http://arxiv.org/pdf/2505.11995v1"}, {"title": "Ranked Voting based Self-Consistency of Large Language Models", "link": "https://arxiv.org/pdf/2505.10772", "details": "W Wang, Y Wang, H Huang - arXiv preprint arXiv:2505.10772, 2025", "abstract": "Majority voting is considered an effective method to enhance chain-of-thought reasoning, as it selects the answer with the highest\" self-consistency\" among different reasoning paths (Wang et al., 2023). However, previous chain-of-thought \u2026", "entry_id": "http://arxiv.org/abs/2505.10772v1", "updated": "2025-05-16 01:09:43", "published": "2025-05-16 01:09:43", "authors": "Weiqin Wang;Yile Wang;Hui Huang", "summary": "Majority voting is considered an effective method to enhance chain-of-thought\nreasoning, as it selects the answer with the highest \"self-consistency\" among\ndifferent reasoning paths (Wang et al., 2023). However, previous\nchain-of-thought reasoning methods typically generate only a single answer in\neach trial, thereby ignoring the possibility of other potential answers. As a\nresult, these alternative answers are often overlooked in subsequent voting\nprocesses. In this work, we propose to generate ranked answers in each\nreasoning process and conduct ranked voting among multiple ranked answers from\ndifferent responses, thereby making the overall self-consistency more reliable.\nSpecifically, we use three ranked voting methods: Instant-runoff voting, Borda\ncount voting, and mean reciprocal rank voting. We validate our methods on six\ndatasets, including three multiple-choice and three open-ended\nquestion-answering tasks, using both advanced open-source and closed-source\nlarge language models. Extensive experimental results indicate that our\nproposed method outperforms the baselines, showcasing the potential of\nleveraging the information of ranked answers and using ranked voting to improve\nreasoning performance. The code is available at\nhttps://github.com/szu-tera/RankedVotingSC.", "comment": "ACL 2025 Findings", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.10772v1;http://arxiv.org/pdf/2505.10772v1", "pdf_url": "http://arxiv.org/pdf/2505.10772v1"}]
