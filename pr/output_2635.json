[{"title": "Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models", "link": "https://arxiv.org/pdf/2405.16282", "details": "A Kumar, R Morabito, S Umbet, J Kabbara, A Emami - arXiv preprint arXiv \u2026, 2024", "abstract": "As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models \u2026"}, {"title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning", "link": "https://arxiv.org/pdf/2406.00645", "details": "Y Fu, H Zhang, D Wu, W Xu, B Boulet - arXiv preprint arXiv:2406.00645, 2024", "abstract": "In this work, we investigate how to leverage pre-trained visual-language models (VLM) for online Reinforcement Learning (RL). In particular, we focus on sparse reward tasks with pre-defined textual task descriptions. We first identify the problem \u2026"}, {"title": "Verbalized Machine Learning: Revisiting Machine Learning with Language Models", "link": "https://arxiv.org/pdf/2406.04344", "details": "TZ Xiao, R Bamler, B Sch\u00f6lkopf, W Liu - arXiv preprint arXiv:2406.04344, 2024", "abstract": "Motivated by the large progress made by large language models (LLMs), we introduce the framework of verbalized machine learning (VML). In contrast to conventional machine learning models that are typically optimized over a continuous \u2026"}, {"title": "Supplementary Material VILA: On Pre-training for Visual Language Models", "link": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Lin_VILA_On_Pre-training_CVPR_2024_supplemental.pdf", "details": "J Lin, H Yin, W Ping, P Molchanov, M Shoeybi, S Han", "abstract": "We used an in-house data blend for supervised finetuning/instruction tuning during the ablation study. We followed [5] to build the FLAN-style instructions from the training set of 18 visual language datasets, as shown in Table 1. We may see that \u2026"}, {"title": "Residual-based Language Models are Free Boosters for Biomedical Imaging Tasks", "link": "https://openaccess.thecvf.com/content/CVPR2024W/DEF-AI-MIA/papers/Lai_Residual-based_Language_Models_are_Free_Boosters_for_Biomedical_Imaging_Tasks_CVPRW_2024_paper.pdf", "details": "Z Lai, J Wu, S Chen, Y Zhou, N Hovakimyan - \u2026 of the IEEE/CVF Conference on \u2026, 2024", "abstract": "In this study we uncover the unexpected efficacy of residual-based large language models (LLMs) as part of encoders for biomedical imaging tasks a domain traditionally devoid of language or textual data. The approach diverges from \u2026"}, {"title": "The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models", "link": "https://arxiv.org/abs/2406.05761", "details": "S Kim, J Suk, JY Cho, S Longpre, C Kim, D Yoon\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As language models (LMs) become capable of handling a wide range of tasks, their evaluation is becoming as challenging as their development. Most generation benchmarks currently assess LMs using abstract evaluation criteria like helpfulness \u2026"}, {"title": "Accelerating Iterative Retrieval-augmented Language Model Serving with Speculation", "link": "https://openreview.net/pdf%3Fid%3DCDnv4vg02f", "details": "Z Zhang, A Zhu, L Yang, Y Xu, L Li, PM Phothilimthana\u2026 - Forty-first International Conference \u2026", "abstract": "This paper introduces RaLMSpec, a framework that accelerates iterative retrieval- augmented language model (RaLM) with* speculative retrieval* and* batched verification*. RaLMSpec further introduces several important systems optimizations \u2026"}, {"title": "Automatic Instruction Evolving for Large Language Models", "link": "https://arxiv.org/pdf/2406.00770", "details": "W Zeng, C Xu, Y Zhao, JG Lou, W Chen - arXiv preprint arXiv:2406.00770, 2024", "abstract": "Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This \u2026"}, {"title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models", "link": "https://arxiv.org/pdf/2406.00548", "details": "T Liu, H Wang, S Wang, Y Cheng, J Gao - arXiv preprint arXiv:2406.00548, 2024", "abstract": "Large language models (LLMs) have achieved impressive performance on various natural language generation tasks. Nonetheless, they suffer from generating negative and harmful contents that are biased against certain demographic groups \u2026"}]
