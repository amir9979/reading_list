'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [MFORT-QA: Multi-hop Few-shot Open Rich Table Question '
[{"title": "Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models", "link": "https://arxiv.org/pdf/2403.12966", "details": "Z Liu, Y Dong, Y Rao, J Zhou, J Lu - arXiv preprint arXiv:2403.12966, 2024", "abstract": "In the realm of vision-language understanding, the proficiency of models in interpreting and reasoning over visual content has become a cornerstone for numerous applications. However, it is challenging for the visual encoder in Large \u2026"}, {"title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models", "link": "https://arxiv.org/pdf/2403.18814", "details": "Y Li, Y Zhang, C Wang, Z Zhong, Y Chen, R Chu, S Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared \u2026"}, {"title": "Mechanisms of non-factual hallucinations in language models", "link": "https://arxiv.org/pdf/2403.18167", "details": "L Yu, M Cao, JCK Cheung, Y Dong - arXiv preprint arXiv:2403.18167, 2024", "abstract": "State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains \u2026"}, {"title": "Prototype Similarity Distillation for Communication-Efficient Federated Unsupervised Representation Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10495206/", "details": "C Zhang, Y Xie, T Chen, W Mao, B Yu - IEEE Transactions on Knowledge and Data \u2026, 2024", "abstract": "Federated unsupervised representation learning aims at leveraging unlabeled data from multiple parties to learn visual representations without compromising the data privacy and tackle the non-IID challenge by aligning diverse representation spaces \u2026"}, {"title": "Self-supervised Dataset Distillation: A Good Compression Is All You Need", "link": "https://arxiv.org/pdf/2404.07976", "details": "M Zhou, Z Yin, S Shao, Z Shen - arXiv preprint arXiv:2404.07976, 2024", "abstract": "Dataset distillation aims to compress information from a large-scale original dataset to a new compact dataset while striving to preserve the utmost degree of the original data informational essence. Previous studies have predominantly concentrated on \u2026"}, {"title": "OpenBias: Open-set Bias Detection in Text-to-Image Generative Models", "link": "https://arxiv.org/pdf/2404.07990", "details": "M D'Inc\u00e0, E Peruzzo, M Mancini, D Xu, V Goel, X Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Text-to-image generative models are becoming increasingly popular and accessible to the general public. As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any \u2026"}, {"title": "Adaptive Prompt Routing for Arbitrary Text Style Transfer with Pre-trained Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29832/31446", "details": "Q Liu, J Qin, W Ye, H Mou, Y He, K Wang - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Recently, arbitrary text style transfer (TST) has made significant progress with the paradigm of prompt learning. In this paradigm, researchers often design or search for a fixed prompt for any input. However, existing evidence shows that large language \u2026"}, {"title": "Best Practices and Lessons Learned on Synthetic Data for Language Models", "link": "https://arxiv.org/pdf/2404.07503", "details": "R Liu, J Wei, F Liu, C Si, Y Zhang, J Rao, S Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The success of AI models relies on the availability of large, diverse, and high-quality datasets, which can be challenging to obtain due to data scarcity, privacy concerns, and high costs. Synthetic data has emerged as a promising solution by generating \u2026"}, {"title": "Towards Multimodal In-Context Learning for Vision & Language Models", "link": "https://arxiv.org/pdf/2403.12736", "details": "S Doveh, S Perek, MJ Mirza, A Alfassy, A Arbelle\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Inspired by the emergence of Large Language Models (LLMs) that can truly understand human language, significant progress has been made in aligning other, non-language, modalities to beunderstandable'by an LLM, primarily via converting \u2026"}]
