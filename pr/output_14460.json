[{"title": "Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models", "link": "https://arxiv.org/pdf/2503.16724", "details": "Z Li, Z Xi-Jia, B Altundas, L Chen, R Paleja\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Semantic Interpretability in Reinforcement Learning (RL) enables transparency, accountability, and safer deployment by making the agent's decisions understandable and verifiable. Achieving this, however, requires a feature space \u2026"}, {"title": "Temporal Relation Extraction in Clinical Texts: A Span-based Graph Transformer Approach", "link": "https://arxiv.org/pdf/2503.18085", "details": "R Chaturvedi, P Baghershahi, S Medya, B Di Eugenio - arXiv preprint arXiv \u2026, 2025", "abstract": "Temporal information extraction from unstructured text is essential for contextualizing events and deriving actionable insights, particularly in the medical domain. We address the task of extracting clinical events and their temporal relations using the \u2026"}, {"title": "Enhancing Large Language Models with Retrieval-augmented Generation: A Radiology-specific Approach", "link": "https://pubs.rsna.org/doi/abs/10.1148/ryai.240313", "details": "DA Weinert, AM Rauschecker - Radiology: Artificial Intelligence, 2025", "abstract": "\u201cJust Accepted\u201d papers have undergone full peer review and have been accepted for publication in Radiology: Artificial Intelligence. This article will undergo copyediting, layout, and proof review before it is published in its final version. Please note that \u2026"}, {"title": "DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question Answering in Large Language Models", "link": "https://arxiv.org/pdf/2503.19426", "details": "S Bae, YS Choi, JH Lee - arXiv preprint arXiv:2503.19426, 2025", "abstract": "While Large Language Models (LLMs) excel in zero-shot Question Answering (QA), they tend to expose biases in their internal knowledge when faced with socially sensitive questions, leading to a degradation in performance. Existing zero-shot \u2026"}, {"title": "Deidentifying Medical Documents with Local, Privacy-Preserving Large Language Models: The LLM-Anonymizer", "link": "https://ai.nejm.org/doi/pdf/10.1056/AIdbp2400537", "details": "IC Wiest, ME Le\u00dfmann, F Wolf, D Ferber, MV Treeck\u2026 - NEJM AI, 2025", "abstract": "Background Medical research with real-world clinical data is challenging as a result of privacy requirements. Patient data should be anonymized before analysis in research studies. Anonymization procedures aim to reduce the reidentification risk \u2026"}, {"title": "Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models", "link": "https://arxiv.org/pdf/2503.18681%3F", "details": "Y Zhang, C Zou, B Wang, J Qin - arXiv preprint arXiv:2503.18681, 2025", "abstract": "Sarcasm detection, as a crucial research direction in the field of Natural Language Processing (NLP), has attracted widespread attention. Traditional sarcasm detection tasks have typically focused on single-modal approaches (eg, text), but due to the \u2026"}]
