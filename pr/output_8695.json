[{"title": "Vision-Language Models Can Self-Improve Reasoning via Reflection", "link": "https://arxiv.org/pdf/2411.00855", "details": "K Cheng, Y Li, F Xu, J Zhang, H Zhou, Y Liu - arXiv preprint arXiv:2411.00855, 2024", "abstract": "Chain-of-thought (CoT) has proven to improve the reasoning capability of large language models (LLMs). However, due to the complexity of multimodal scenarios and the difficulty in collecting high-quality CoT data, CoT reasoning in multimodal \u2026"}, {"title": "Language-Emphasized Cross-Lingual In-Context Learning for Multilingual LLM", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9437-9_26", "details": "J Li, X Wei, X Wang, N Zhuang, L Wang, J Dang - CCF International Conference on \u2026, 2024", "abstract": "With the recent rise of large language models (LLMs), in-context learning (ICL) has shown remarkable performance, eliminating the need for fine-tuning parameters and reducing the reliance on extensive labeled data. However, the intricacies of cross \u2026"}, {"title": "Membership Inference Attacks against Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.02902", "details": "Z Li, Y Wu, Y Chen, F Tonin, EA Rocamora, V Cevher - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of \u2026"}, {"title": "Continual LLaVA: Continual Instruction Tuning in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.02564", "details": "M Cao, Y Liu, Y Liu, T Wang, J Dong, H Ding, X Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Instruction tuning constitutes a prevalent technique for tailoring Large Vision Language Models (LVLMs) to meet individual task requirements. To date, most of the existing approaches are confined to single-task adaptation, whereas the \u2026"}, {"title": "Natural Language Inference Improves Compositionality in Vision-Language Models", "link": "https://arxiv.org/pdf/2410.22315%3F", "details": "P Cascante-Bonilla, Y Hou, YT Cao, H Daum\u00e9 III\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Compositional reasoning in Vision-Language Models (VLMs) remains challenging as these models often struggle to relate objects, attributes, and spatial relationships. Recent methods aim to address these limitations by relying on the semantics of the \u2026"}, {"title": "Retrieval-Enhanced Named Entity Recognition", "link": "https://arxiv.org/pdf/2410.13118", "details": "E Shiraishi, RY de Camargo, HLP Silva, RC Prati - arXiv preprint arXiv:2410.13118, 2024", "abstract": "When combined with In-Context Learning, a technique that enables models to adapt to new tasks by incorporating task-specific examples or demonstrations directly within the input prompt, autoregressive language models have achieved good \u2026"}, {"title": "FOX-1 TECHNICAL REPORT", "link": "https://www.researchgate.net/profile/Zijian-Hu-2/publication/385619515_Fox-1_Technical_Report/links/672d76f477f274616d626f77/Fox-1-Technical-Report.pdf", "details": "Z Hu, J Zhang, R Pan, Z Xu, S Avestimehr, C He\u2026", "abstract": "We present Fox-1, a series of small language models (SLMs) consisting of Fox-1-1.6 B and Fox-1-1.6 B-Instruct-v0. 1. These models are pre-trained on 3 trillion tokens of web-scraped document data and fine-tuned with 5 billion tokens of instruction \u2026"}, {"title": "Toxicity of the Commons: Curating Open-Source Pre-Training Data", "link": "https://arxiv.org/pdf/2410.22587", "details": "C Arnett, E Jones, IP Yamshchikov, PC Langlais - arXiv preprint arXiv:2410.22587, 2024", "abstract": "Open-source large language models are becoming increasingly available and popular among researchers and practitioners. While significant progress has been made on open-weight models, open training data is a practice yet to be adopted by \u2026"}, {"title": "The future of learning in the age of generative ai: Automated question generation and assessment with large language models", "link": "https://arxiv.org/pdf/2410.09576", "details": "S Maity, A Deroy - arXiv preprint arXiv:2410.09576, 2024", "abstract": "In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs \u2026"}]
