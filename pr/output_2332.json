[{"title": "Enhancing Self-Supervised Learning through Explainable Artificial Intelligence Mechanisms: A Computational Analysis", "link": "https://www.mdpi.com/2504-2289/8/6/58", "details": "E Neghawi, Y Liu - Big Data and Cognitive Computing, 2024", "abstract": "Self-supervised learning continues to drive advancements in machine learning. However, the absence of unified computational processes for benchmarking and evaluation remains a challenge. This study conducts a comprehensive analysis of \u2026"}, {"title": "Data Science Principles for Interpretable and Explainable AI", "link": "https://arxiv.org/pdf/2405.10552", "details": "K Sankaran - arXiv preprint arXiv:2405.10552, 2024", "abstract": "Society's capacity for algorithmic problem-solving has never been greater. Artificial Intelligence is now applied across more domains than ever, a consequence of powerful abstractions, abundant data, and accessible software. As capabilities have \u2026"}, {"title": "Enhancing Generative Molecular Design via Uncertainty-guided Fine-tuning of Variational Autoencoders", "link": "https://arxiv.org/pdf/2405.20573", "details": "ANM Abeer, S Jantre, NM Urban, BJ Yoon - arXiv preprint arXiv:2405.20573, 2024", "abstract": "In recent years, deep generative models have been successfully adopted for various molecular design tasks, particularly in the life and material sciences. A critical challenge for pre-trained generative molecular design (GMD) models is to fine-tune \u2026"}, {"title": "Quantifying Representation Reliability in Self-Supervised Learning Models", "link": "https://haowang94.github.io/files/uai24.pdf", "details": "YJ Park, H Wang, S Ardeshir, N Azizan", "abstract": "Self-supervised learning models extract generalpurpose representations from data. Quantifying the reliability of these representations is crucial, as many downstream models rely on them as input for their own tasks. To this end, we introduce a formal \u2026"}, {"title": "VOICE: Variance of Induced Contrastive Explanations to quantify Uncertainty in Neural Network Interpretability", "link": "https://arxiv.org/pdf/2406.00573", "details": "M Prabhushankar, G AlRegib - arXiv preprint arXiv:2406.00573, 2024", "abstract": "In this paper, we visualize and quantify the predictive uncertainty of gradient-based post hoc visual explanations for neural networks. Predictive uncertainty refers to the variability in the network predictions under perturbations to the input. Visual post hoc \u2026"}, {"title": "SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model", "link": "https://arxiv.org/pdf/2406.01584", "details": "AC Cheng, H Yin, Y Fu, Q Guo, R Yang, J Kautz\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision Language Models (VLMs) have demonstrated remarkable performance in 2D vision and language tasks. However, their ability to reason about spatial arrangements remains limited. In this work, we introduce Spatial Region GPT \u2026"}, {"title": "Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised Anomaly Detection", "link": "https://arxiv.org/pdf/2405.14325", "details": "J Guo, S Lu, W Zhang, H Li - arXiv preprint arXiv:2405.14325, 2024", "abstract": "Recent studies highlighted a practical setting of unsupervised anomaly detection (UAD) that builds a unified model for multi-class images, serving as an alternative to the conventional one-class-one-model setup. Despite various advancements \u2026"}]
