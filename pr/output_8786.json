[{"title": "Continual LLaVA: Continual Instruction Tuning in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.02564", "details": "M Cao, Y Liu, Y Liu, T Wang, J Dong, H Ding, X Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Instruction tuning constitutes a prevalent technique for tailoring Large Vision Language Models (LVLMs) to meet individual task requirements. To date, most of the existing approaches are confined to single-task adaptation, whereas the \u2026"}, {"title": "Which Client is Reliable?: A Reliable and Personalized Prompt-based Federated Learning for Medical Image Question Answering", "link": "https://arxiv.org/pdf/2410.17484", "details": "H Zhu, R Togo, T Ogawa, M Haseyama - arXiv preprint arXiv:2410.17484, 2024", "abstract": "Conventional medical artificial intelligence (AI) models face barriers in clinical application and ethical issues owing to their inability to handle the privacy-sensitive characteristics of medical data. We present a novel personalized federated learning \u2026"}, {"title": "MSc-SQL: Multi-Sample Critiquing Small Language Models For Text-To-SQL Translation", "link": "https://arxiv.org/pdf/2410.12916", "details": "SK Gorti, I Gofman, Z Liu, J Wu, N Vouitsis, G Yu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Text-to-SQL generation enables non-experts to interact with databases via natural language. Recent advances rely on large closed-source models like GPT-4 that present challenges in accessibility, privacy, and latency. To address these issues, we \u2026"}, {"title": "Multifaceted Natural Language Processing Task\u2013Based Evaluation of Bidirectional Encoder Representations From Transformers Models for Bilingual (Korean and \u2026", "link": "https://medinform.jmir.org/2024/1/e52897/", "details": "K Kim, S Park, J Min, S Park, JY Kim, J Eun, K Jung\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background: The bidirectional encoder representations from transformers (BERT) model has attracted considerable attention in clinical applications, such as patient classification and disease prediction. However, current studies have typically \u2026"}, {"title": "Natural Language Inference Improves Compositionality in Vision-Language Models", "link": "https://arxiv.org/pdf/2410.22315%3F", "details": "P Cascante-Bonilla, Y Hou, YT Cao, H Daum\u00e9 III\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Compositional reasoning in Vision-Language Models (VLMs) remains challenging as these models often struggle to relate objects, attributes, and spatial relationships. Recent methods aim to address these limitations by relying on the semantics of the \u2026"}, {"title": "Retrieval In Decoder benefits generative models for explainable complex question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007573", "details": "J Feng, Q Wang, H Qiu, L Liu - Neural Networks, 2024", "abstract": "Abstract Large-scale Language Models (LLMs) utilizing the Chain-of-Thought prompting demonstrate exceptional performance in a variety of tasks. However, the persistence of factual hallucinations remains a significant challenge in practical \u2026"}, {"title": "Language-based reasoning graph neural network for commonsense question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024007408", "details": "M Yang, Y Wang, Y Gu - Neural Networks, 2024", "abstract": "Abstract Language model (LM) has played an increasingly important role in the common-sense understanding and reasoning in the CSQA task (Common Sense Question Answering). However, due to the amount of model parameters, increasing \u2026"}, {"title": "Prompt Learning for Few-Shot Question Answering via Self-Context Data Augmentation", "link": "https://ieeexplore.ieee.org/abstract/document/10723112/", "details": "JQ Qiu, CY Zhang, CLP Chen - IEEE Transactions on Artificial Intelligence, 2024", "abstract": "Pre-trained language models (PLMs) have shown remarkable performance on question answering (QA) tasks, but they usually require fine-tuning that depends on a substantial quantity of QA pairs. Therefore, improving the performance of PLMs in \u2026"}, {"title": "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs", "link": "https://aclanthology.org/2024.findings-emnlp.527.pdf", "details": "M Kim, C Park, S Baek - Findings of the Association for Computational \u2026, 2024", "abstract": "Retrieval-augmented generation (RAG) has received much attention for Open- domain question-answering (ODQA) tasks as a means to compensate for the parametric knowledge of large language models (LLMs). While previous approaches \u2026"}]
