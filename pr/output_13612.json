[{"title": "Quantifying Memorization and Retriever Performance in Retrieval-Augmented Vision-Language Models", "link": "https://arxiv.org/pdf/2502.13836%3F", "details": "P Carragher, A Jha, R Raghav, KM Carley - arXiv preprint arXiv:2502.13836, 2025", "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in question answering (QA), but metrics for assessing their reliance on memorization versus retrieval remain underdeveloped. Moreover, while finetuned models are state-of-the \u2026"}, {"title": "Medical foundation large language models for comprehensive text analysis and beyond", "link": "https://www.nature.com/articles/s41746-025-01533-1", "details": "Q Xie, Q Chen, A Chen, C Peng, Y Hu, F Lin, X Peng\u2026 - npj Digital Medicine, 2025", "abstract": "Recent advancements in large language models (LLMs) show significant potential in medical applications but are hindered by limited specialized medical knowledge. We present Me-LLaMA, a family of open-source medical LLMs integrating extensive \u2026"}, {"title": "Reliability, Resiliency, and Responsibility: A Framework for Addressing Security Concerns in Large Language Models", "link": "https://link.springer.com/article/10.1007/s42979-025-03807-7", "details": "L Passarella, E Begoli, C Smith, A Sadovnik - SN Computer Science, 2025", "abstract": "The increase in popularity of Large Language Models (LLMs) has given rise to concerns about their vulnerabilities and security risks that make them open to exploitation and misuse. Their widespread adoption prompts the need for further \u2026"}]
