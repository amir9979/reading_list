[{"title": "Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning", "link": "https://arxiv.org/pdf/2505.18503", "details": "A Chang, L Huang, AJ Boyd, P Bhatia, T Kass-Hout\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal attention distribution on visual inputs, leading to hallucinated or inaccurate outputs. Existing mitigation methods primarily rely on inference-time interventions, which are \u2026", "entry_id": "http://arxiv.org/abs/2505.18503v1", "updated": "2025-05-24 04:45:45", "published": "2025-05-24 04:45:45", "authors": "Aofei Chang;Le Huang;Alex James Boyd;Parminder Bhatia;Taha Kass-Hout;Cao Xiao;Fenglong Ma", "summary": "Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal\nattention distribution on visual inputs, leading to hallucinated or inaccurate\noutputs. Existing mitigation methods primarily rely on inference-time\ninterventions, which are limited in attention adaptation or require additional\nsupervision. To address this, we propose A$^3$Tune, a novel fine-tuning\nframework for Automatic Attention Alignment Tuning. A$^3$Tune leverages\nzero-shot weak labels from SAM, refines them into prompt-aware labels using\nBioMedCLIP, and then selectively modifies visually-critical attention heads to\nimprove alignment while minimizing interference. Additionally, we introduce a\nA$^3$MoE module, enabling adaptive parameter selection for attention tuning\nacross diverse prompts and images. Extensive experiments on medical VQA and\nreport generation benchmarks show that A$^3$Tune outperforms state-of-the-art\nbaselines, achieving enhanced attention distributions and performance in\nMed-LVLMs.", "comment": "Accepted to ACL2025 (main)", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.18503v1;http://arxiv.org/pdf/2505.18503v1", "pdf_url": "http://arxiv.org/pdf/2505.18503v1"}, {"title": "Paying Alignment Tax with Contrastive Learning", "link": "https://arxiv.org/pdf/2505.19327", "details": "BS Korkmaz, R Nair, EM Daly, AR Chanona - arXiv preprint arXiv:2505.19327, 2025", "abstract": "Current debiasing approaches often result a degradation in model capabilities such as factual accuracy and knowledge retention. Through systematic evaluation across multiple benchmarks, we demonstrate that existing debiasing methods face \u2026", "entry_id": "http://arxiv.org/abs/2505.19327v1", "updated": "2025-05-25 21:26:18", "published": "2025-05-25 21:26:18", "authors": "Buse Sibel Korkmaz;Rahul Nair;Elizabeth M. Daly;Antonio del Rio Chanona", "summary": "Current debiasing approaches often result a degradation in model capabilities\nsuch as factual accuracy and knowledge retention. Through systematic evaluation\nacross multiple benchmarks, we demonstrate that existing debiasing methods face\nfundamental trade-offs, particularly in smaller models, leading to reduced\ntruthfulness, knowledge loss, or unintelligible outputs. To address these\nlimitations, we propose a contrastive learning framework that learns through\ncarefully constructed positive and negative examples. Our approach introduces\ncontrast computation and dynamic loss scaling to balance bias mitigation with\nfaithfulness preservation. Experimental results across multiple model scales\ndemonstrate that our method achieves substantial improvements in both toxicity\nreduction and faithfulness preservation. Most importantly, we show that our\nframework is the first to consistently improve both metrics simultaneously,\navoiding the capability degradation characteristic of existing approaches.\nThese results suggest that explicit modeling of both positive and negative\nexamples through contrastive learning could be a promising direction for\nreducing the alignment tax in language model debiasing.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.19327v1;http://arxiv.org/pdf/2505.19327v1", "pdf_url": "http://arxiv.org/pdf/2505.19327v1"}, {"title": "Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning", "link": "https://arxiv.org/pdf/2505.17509", "details": "S Zhao, Q Zhu, S Xiong, S Ruan, Y Fan, R Duan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large pre-trained Vision Language Models (VLMs) have excellent generalization capabilities but are highly susceptible to adversarial examples, presenting potential security risks. To improve the robustness of VLMs against adversarial examples \u2026", "entry_id": "http://arxiv.org/abs/2505.17509v1", "updated": "2025-05-23 06:04:15", "published": "2025-05-23 06:04:15", "authors": "Shiji Zhao;Qihui Zhu;Shukun Xiong;Shouwei Ruan;Yize Fan;Ranjie Duan;Qing Guo;Xingxing Wei", "summary": "Large pre-trained Vision Language Models (VLMs) have excellent generalization\ncapabilities but are highly susceptible to adversarial examples, presenting\npotential security risks. To improve the robustness of VLMs against adversarial\nexamples, adversarial prompt tuning methods are proposed to align the text\nfeature with the adversarial image feature without changing model parameters.\nHowever, when facing various adversarial attacks, a single learnable text\nprompt has insufficient generalization to align well with all adversarial image\nfeatures, which finally leads to the overfitting phenomenon. To address the\nabove challenge, in this paper, we empirically find that increasing the number\nof learned prompts can bring more robustness improvement than a longer prompt.\nThen we propose an adversarial tuning method named Adversarial Mixture Prompt\nTuning (AMPT) to enhance the generalization towards various adversarial attacks\nfor VLMs. AMPT aims to learn mixture text prompts to obtain more robust text\nfeatures. To further enhance the adaptability, we propose a conditional weight\nrouter based on the input adversarial image to predict the mixture weights of\nmultiple learned prompts, which helps obtain sample-specific aggregated text\nfeatures aligning with different adversarial image features. A series of\nexperiments show that our method can achieve better adversarial robustness than\nstate-of-the-art methods on 11 datasets under different experimental settings.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.17509v1;http://arxiv.org/pdf/2505.17509v1", "pdf_url": "http://arxiv.org/pdf/2505.17509v1"}, {"title": "PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus", "link": "https://arxiv.org/pdf/2505.20323", "details": "S Noroozizadeh, S Kumar, GH Chen, JC Weiss - arXiv preprint arXiv:2505.20323, 2025", "abstract": "Understanding temporal dynamics in clinical narratives is essential for modeling patient trajectories, yet large-scale temporally annotated resources remain limited. We present PMOA-TTS, the first openly available dataset of 124,699 PubMed Open \u2026", "entry_id": "http://arxiv.org/abs/2505.20323v1", "updated": "2025-05-23 18:01:09", "published": "2025-05-23 18:01:09", "authors": "Shahriar Noroozizadeh;Sayantan Kumar;George H. Chen;Jeremy C. Weiss", "summary": "Understanding temporal dynamics in clinical narratives is essential for\nmodeling patient trajectories, yet large-scale temporally annotated resources\nremain limited. We present PMOA-TTS, the first openly available dataset of\n124,699 PubMed Open Access (PMOA) case reports, each converted into structured\n(event, time) timelines via a scalable LLM-based pipeline. Our approach\ncombines heuristic filtering with Llama 3.3 to identify single-patient case\nreports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1,\nresulting in over 5.6 million timestamped clinical events. To assess timeline\nquality, we evaluate against a clinician-curated reference set using three\nmetrics: (i) event-level matching (80% match at a cosine similarity threshold\nof 0.1), (ii) temporal concordance (c-index > 0.90), and (iii) Area Under the\nLog-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide\ndiagnostic and demographic coverage. In a downstream survival prediction task,\nembeddings from extracted timelines achieve time-dependent concordance indices\nup to 0.82 $\\pm$ 0.01, demonstrating the predictive value of temporally\nstructured narratives. PMOA-TTS provides a scalable foundation for timeline\nextraction, temporal reasoning, and longitudinal modeling in biomedical NLP.\nThe dataset is available at: https://huggingface.co/datasets/snoroozi/pmoa-tts .", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.20323v1;http://arxiv.org/pdf/2505.20323v1", "pdf_url": "http://arxiv.org/pdf/2505.20323v1"}, {"title": "Fine-grained List-wise Alignment for Generative Medication Recommendation", "link": "https://arxiv.org/pdf/2505.20218", "details": "C Fan, C Gao, W Shi, Y Gong, Z Zhao, F Feng - arXiv preprint arXiv:2505.20218, 2025", "abstract": "Accurate and safe medication recommendations are critical for effective clinical decision-making, especially in multimorbidity cases. However, existing systems rely on point-wise prediction paradigms that overlook synergistic drug effects and \u2026", "entry_id": "http://arxiv.org/abs/2505.20218v1", "updated": "2025-05-26 16:59:23", "published": "2025-05-26 16:59:23", "authors": "Chenxiao Fan;Chongming Gao;Wentao Shi;Yaxin Gong;Zihao Zhao;Fuli Feng", "summary": "Accurate and safe medication recommendations are critical for effective\nclinical decision-making, especially in multimorbidity cases. However, existing\nsystems rely on point-wise prediction paradigms that overlook synergistic drug\neffects and potential adverse drug-drug interactions (DDIs). We propose FLAME,\na fine-grained list-wise alignment framework for large language models (LLMs),\nenabling drug-by-drug generation of drug lists. FLAME formulates recommendation\nas a sequential decision process, where each step adds or removes a single\ndrug. To provide fine-grained learning signals, we devise step-wise Group\nRelative Policy Optimization (GRPO) with potential-based reward shaping, which\nexplicitly models DDIs and optimizes the contribution of each drug to the\noverall prescription. Furthermore, FLAME enhances patient modeling by\nintegrating structured clinical knowledge and collaborative information into\nthe representation space of LLMs. Experiments on benchmark datasets demonstrate\nthat FLAME achieves state-of-the-art performance, delivering superior accuracy,\ncontrollable safety-accuracy trade-offs, and strong generalization across\ndiverse clinical scenarios. Our code is available at\nhttps://github.com/cxfann/Flame.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.20218v1;http://arxiv.org/pdf/2505.20218v1", "pdf_url": "http://arxiv.org/pdf/2505.20218v1"}, {"title": "Rethinking Gating Mechanism in Sparse MoE: Handling Arbitrary Modality Inputs with Confidence-Guided Gate", "link": "https://arxiv.org/pdf/2505.19525", "details": "LN Zheng, WE Zhang, M Guo, M Xu, O Maennel\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Effectively managing missing modalities is a fundamental challenge in real-world multimodal learning scenarios, where data incompleteness often results from systematic collection errors or sensor failures. Sparse Mixture-of-Experts (SMoE) \u2026", "entry_id": "http://arxiv.org/abs/2505.19525v1", "updated": "2025-05-26 05:18:55", "published": "2025-05-26 05:18:55", "authors": "Liangwei Nathan Zheng;Wei Emma Zhang;Mingyu Guo;Miao Xu;Olaf Maennel;Weitong Chen", "summary": "Effectively managing missing modalities is a fundamental challenge in\nreal-world multimodal learning scenarios, where data incompleteness often\nresults from systematic collection errors or sensor failures. Sparse\nMixture-of-Experts (SMoE) architectures have the potential to naturally handle\nmultimodal data, with individual experts specializing in different modalities.\nHowever, existing SMoE approach often lacks proper ability to handle missing\nmodality, leading to performance degradation and poor generalization in\nreal-world applications. We propose Conf-SMoE to introduce a two-stage\nimputation module to handle the missing modality problem for the SMoE\narchitecture and reveal the insight of expert collapse from theoretical\nanalysis with strong empirical evidence. Inspired by our theoretical analysis,\nConf-SMoE propose a novel expert gating mechanism by detaching the softmax\nrouting score to task confidence score w.r.t ground truth. This naturally\nrelieves expert collapse without introducing additional load balance loss\nfunction. We show that the insights of expert collapse aligns with other gating\nmechanism such as Gaussian and Laplacian gate. We also evaluate the proposed\nmethod on four different real world dataset with three different experiment\nsettings to conduct comprehensive the analysis of Conf-SMoE on modality fusion\nand resistance to missing modality.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.19525v1;http://arxiv.org/pdf/2505.19525v1", "pdf_url": "http://arxiv.org/pdf/2505.19525v1"}, {"title": "MBLSTM is a contextual interaction refined method for time series prediction", "link": "https://www.nature.com/articles/s41598-025-03243-w", "details": "W Qiu, F Zhu, T Hao, M Wang, R Huang - Scientific Reports, 2025", "abstract": "Time series prediction has been widely used in the medical field to predict patient recurrence or physiological fluctuations. However, the adequacy of the existing methods for contextual information interaction is still insufficient when dealing with a \u2026"}, {"title": "Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation", "link": "https://arxiv.org/pdf/2505.21190", "details": "JH Moon, G Choi, P Rabaey, MG Kim, HG Hong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Radiology reports convey detailed clinical observations and capture diagnostic reasoning that evolves over time. However, existing evaluation methods are limited to single-report settings and rely on coarse metrics that fail to capture fine-grained \u2026", "entry_id": "http://arxiv.org/abs/2505.21190v1", "updated": "2025-05-27 13:40:00", "published": "2025-05-27 13:40:00", "authors": "Jong Hak Moon;Geon Choi;Paloma Rabaey;Min Gwan Kim;Hyuk Gi Hong;Jung-Oh Lee;Hangyul Yoon;Eun Woo Doe;Jiyoun Kim;Harshita Sharma;Daniel C. Castro;Javier Alvarez-Valle;Edward Choi", "summary": "Radiology reports convey detailed clinical observations and capture\ndiagnostic reasoning that evolves over time. However, existing evaluation\nmethods are limited to single-report settings and rely on coarse metrics that\nfail to capture fine-grained clinical semantics and temporal dependencies. We\nintroduce LUNGUAGE,a benchmark dataset for structured radiology report\ngeneration that supports both single-report evaluation and longitudinal\npatient-level assessment across multiple studies. It contains 1,473 annotated\nchest X-ray reports, each reviewed by experts, and 80 of them contain\nlongitudinal annotations to capture disease progression and inter-study\nintervals, also reviewed by experts. Using this benchmark, we develop a\ntwo-stage framework that transforms generated reports into fine-grained,\nschema-aligned structured representations, enabling longitudinal\ninterpretation. We also propose LUNGUAGESCORE, an interpretable metric that\ncompares structured outputs at the entity, relation, and attribute level while\nmodeling temporal consistency across patient timelines. These contributions\nestablish the first benchmark dataset, structuring framework, and evaluation\nmetric for sequential radiology reporting, with empirical results demonstrating\nthat LUNGUAGESCORE effectively supports structured report evaluation. The code\nis available at: https://github.com/SuperSupermoon/Lunguage", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.21190v1;http://arxiv.org/pdf/2505.21190v1", "pdf_url": "http://arxiv.org/pdf/2505.21190v1"}, {"title": "Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies", "link": "https://arxiv.org/pdf/2505.19337", "details": "K Li, M Zitnik - arXiv preprint arXiv:2505.19337, 2025", "abstract": "Offline goal-conditioned reinforcement learning methods have shown promise for reach-avoid tasks, where an agent must reach a target state while avoiding undesirable regions of the state space. Existing approaches typically encode avoid \u2026", "entry_id": "http://arxiv.org/abs/2505.19337v2", "updated": "2025-05-27 02:56:11", "published": "2025-05-25 22:00:38", "authors": "Kevin Li;Marinka Zitnik", "summary": "Offline goal-conditioned reinforcement learning methods have shown promise\nfor reach-avoid tasks, where an agent must reach a target state while avoiding\nundesirable regions of the state space. Existing approaches typically encode\navoid-region information into an augmented state space and cost function, which\nprevents flexible, dynamic specification of novel avoid-region information at\nevaluation time. They also rely heavily on well-designed reward and cost\nfunctions, limiting scalability to complex or poorly structured environments.\nWe introduce RADT, a decision transformer model for offline, reward-free,\ngoal-conditioned, avoid region-conditioned RL. RADT encodes goals and avoid\nregions directly as prompt tokens, allowing any number of avoid regions of\narbitrary size to be specified at evaluation time. Using only suboptimal\noffline trajectories from a random policy, RADT learns reach-avoid behavior\nthrough a novel combination of goal and avoid-region hindsight relabeling. We\nbenchmark RADT against 3 existing offline goal-conditioned RL models across 11\ntasks, environments, and experimental settings. RADT generalizes in a zero-shot\nmanner to out-of-distribution avoid region sizes and counts, outperforming\nbaselines that require retraining. In one such zero-shot setting, RADT achieves\n35.7% improvement in normalized cost over the best retrained baseline while\nmaintaining high goal-reaching success. We apply RADT to cell reprogramming in\nbiology, where it reduces visits to undesirable intermediate gene expression\nstates during trajectories to desired target states, despite stochastic\ntransitions and discrete, structured state dynamics.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;q-bio.QM", "links": "http://arxiv.org/abs/2505.19337v2;http://arxiv.org/pdf/2505.19337v2", "pdf_url": "http://arxiv.org/pdf/2505.19337v2"}]
