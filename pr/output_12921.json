[{"title": "Initialization Matters: Unraveling the Impact of Pre-Training on Federated Learning", "link": "https://arxiv.org/pdf/2502.08024", "details": "D Jhunjhunwala, P Sharma, Z Xu, G Joshi - arXiv preprint arXiv:2502.08024, 2025", "abstract": "Initializing with pre-trained models when learning on downstream tasks is becoming standard practice in machine learning. Several recent works explore the benefits of pre-trained initialization in a federated learning (FL) setting, where the downstream \u2026"}, {"title": "Incorporating Review-missing Interactions for Generative Explainable Recommendation", "link": "https://aclanthology.org/2025.coling-main.527.pdf", "details": "X Li, X Bo, C Ma, X Chen - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Explainable recommendation has attracted much attention from the academic and industry communities. Traditional models usually leverage user reviews as ground truths for model training, and the interactions without reviews are totally ignored \u2026"}, {"title": "Conditional Diffusion Models are Robust and Explainable Medical Image Classifiers", "link": "https://openreview.net/forum%3Fid%3D3LySEy7MR3", "details": "GM Favero, P Saremi, E Kaczmarek, B Nichyporuk\u2026 - Medical Imaging with Deep \u2026", "abstract": "Discriminative classifiers have become a foundational tool in deep learning for medical imaging, excelling at learning separable features of complex data distributions. However, these models often need careful design, augmentation, and \u2026"}, {"title": "HuDEx: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM responses", "link": "https://arxiv.org/pdf/2502.08109", "details": "S Lee, H Lee, S Heo, W Choi - arXiv preprint arXiv:2502.08109, 2025", "abstract": "Recent advances in large language models (LLMs) have shown promising improvements, often surpassing existing methods across a wide range of downstream tasks in natural language processing. However, these models still face \u2026"}, {"title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement", "link": "https://arxiv.org/pdf/2501.12273%3F", "details": "M Cao, T Zhang, M Li, C Zhang, Y Liu, H Duan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The quality of Supervised Fine-Tuning (SFT) data plays a critical role in enhancing the conversational capabilities of Large Language Models (LLMs). However, as LLMs become more advanced, the availability of high-quality human-annotated SFT \u2026"}, {"title": "Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models", "link": "https://arxiv.org/pdf/2502.08130", "details": "S Gupta, Y Nandwani, A Yehudai, D Khandelwal\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task \u2026"}]
