[{"title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual Tokens in Ineffective Layers", "link": "https://arxiv.org/pdf/2504.00502", "details": "Q Yuan, Q Zhang, Y Liu, J Chen, Y Lu, H Lin, J Zheng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal Large Language Models (MLLMs) suffer from high computational costs due to their massive size and the large number of visual tokens. In this paper, we investigate layer-wise redundancy in MLLMs by introducing a novel metric, Layer \u2026"}, {"title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization", "link": "https://arxiv.org/pdf/2503.23733", "details": "Y Du, X Wang, C Chen, J Ye, Y Wang, P Li, M Yan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous \u2026"}, {"title": "Integrating Large Language Models with Human Expertise for Disease Detection in Electronic Health Records", "link": "https://arxiv.org/pdf/2504.00053", "details": "J Pan, S Lee, C Cheligeer, EA Martin, K Riazi, H Quan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Objective: Electronic health records (EHR) are widely available to complement administrative data-based disease surveillance and healthcare performance evaluation. Defining conditions from EHR is labour-intensive and requires extensive \u2026"}, {"title": "Agentic Large Language Models, a survey", "link": "https://arxiv.org/pdf/2503.23037", "details": "A Plaat, M van Duijn, N van Stein, M Preuss\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "There is great interest in agentic LLMs, large language models that act as agents. We review the growing body of work in this area and provide a research agenda. Agentic LLMs are LLMs that (1) reason,(2) act, and (3) interact. We organize the \u2026"}, {"title": "Process-based self-rewarding language models", "link": "https://arxiv.org/pdf/2503.03746", "details": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' \u2026"}, {"title": "Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic Evaluation of Language Models", "link": "https://arxiv.org/pdf/2504.01001", "details": "J Pombal, NM Guerreiro, R Rei, AFT Martins - arXiv preprint arXiv:2504.01001, 2025", "abstract": "As language models improve and become capable of performing more complex tasks across modalities, evaluating them automatically becomes increasingly challenging. Developing strong and robust task-specific automatic metrics gets \u2026"}, {"title": "Foundation Model for Predicting Prognosis and Adjuvant Therapy Benefit From Digital Pathology in GI Cancers", "link": "https://ascopubs.org/doi/abs/10.1200/JCO-24-01501", "details": "X Wang, Y Jiang, S Yang, F Wang, X Zhang, W Wang\u2026 - Journal of Clinical Oncology, 2025", "abstract": "PURPOSE Artificial intelligence (AI) holds significant promise for improving cancer diagnosis and treatment. Here, we present a foundation AI model for prognosis prediction on the basis of standard hematoxylin and eosin\u2013stained histopathology \u2026"}, {"title": "CodeIF-Bench: Evaluating Instruction-Following Capabilities of Large Language Models in Interactive Code Generation", "link": "https://arxiv.org/pdf/2503.22688", "details": "P Wang, L Zhang, F Liu, L Shi, M Li, B Shen, A Fu - arXiv preprint arXiv:2503.22688, 2025", "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance in code generation tasks and have become indispensable programming assistants for developers. However, existing code generation benchmarks primarily assess the \u2026"}, {"title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning", "link": "https://arxiv.org/pdf/2504.00406", "details": "J Han, W Buntine, E Shareghi - arXiv preprint arXiv:2504.00406, 2025", "abstract": "Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources \u2026"}]
