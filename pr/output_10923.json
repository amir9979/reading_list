[{"title": "Use of Attention Maps to Enrich Discriminability in Deep Learning Prediction Models Using Longitudinal Data from Electronic Health Records", "link": "https://www.mdpi.com/2076-3417/15/1/146", "details": "LA Carrasco-Ribelles, M Cabrera-Bean\u2026 - Applied Sciences, 2024", "abstract": "Featured Application A better discrimination in a prediction model does not imply a better interpretability. In healthcare, where transparency is crucial, both discriminability and interpretability should be checked before stating that a new \u2026"}, {"title": "Leveraging Weak Supervision for Cell Localization in Digital Pathology Using Multitask Learning and Consistency Loss", "link": "https://arxiv.org/pdf/2412.15392", "details": "BL Cesur, AHD Karasayar, P Bulutay, N Kapucuoglu\u2026", "abstract": "Cell detection and segmentation are integral parts of automated systems in digital pathology. Encoder-decoder networks have emerged as a promising solution for these tasks. However, training of these networks has typically required full boundary \u2026"}, {"title": "CLASP: Contrastive Language-Speech Pretraining for Multilingual Multimodal Information Retrieval", "link": "https://arxiv.org/pdf/2412.13071", "details": "MM Abootorabi, E Asgari - arXiv preprint arXiv:2412.13071, 2024", "abstract": "This study introduces CLASP (Contrastive Language-Speech Pretraining), a multilingual, multimodal representation tailored for audio-text information retrieval. CLASP leverages the synergy between spoken content and textual data. During \u2026"}, {"title": "Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models", "link": "https://arxiv.org/pdf/2412.16545", "details": "Z Zhang, Y Wang, X Huang, T Fang, H Zhang, C Deng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models have shown remarkable performance across a wide range of language tasks, owing to their exceptional capabilities in context modeling. The most commonly used method of context modeling is full self-attention, as seen in \u2026"}, {"title": "Mastering Board Games by External and Internal Planning with Language Models", "link": "https://arxiv.org/pdf/2412.12119%3F", "details": "J Schultz, J Adamek, M Jusup, M Lanctot, M Kaisers\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While large language models perform well on a range of complex tasks (eg, text generation, question answering, summarization), robust multi-step planning and reasoning remains a considerable challenge for them. In this paper we show that \u2026"}, {"title": "MegaCOIN: Enhancing Medium-Grained Color Perception for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.03927", "details": "MC Chiu, S Wen, PY Chen, X Ma - arXiv preprint arXiv:2412.03927, 2024", "abstract": "In vision-language models (VLMs), the ability to perceive and interpret color and physical environment is crucial for achieving contextually accurate understanding and interaction. However, despite advances in multimodal modeling, there remains a \u2026"}, {"title": "Core Context Aware Attention for Long Context Language Modeling", "link": "https://arxiv.org/pdf/2412.12465", "details": "Y Chen, Z You, S Zhang, H Li, Y Li, Y Wang, M Tan - arXiv preprint arXiv:2412.12465, 2024", "abstract": "Transformer-based Large Language Models (LLMs) have exhibited remarkable success in various natural language processing tasks primarily attributed to self- attention mechanism, which requires a token to consider all preceding tokens as its \u2026"}, {"title": "M $^ 3$ PC: Test-time Model Predictive Control for Pretrained Masked Trajectory Model", "link": "https://arxiv.org/pdf/2412.05675", "details": "K Wen, Y Hu, Y Mu, L Ke - arXiv preprint arXiv:2412.05675, 2024", "abstract": "Recent work in Offline Reinforcement Learning (RL) has shown that a unified Transformer trained under a masked auto-encoding objective can effectively capture the relationships between different modalities (eg, states, actions, rewards) within \u2026"}, {"title": "Breaking the Stage Barrier: A Novel Single-Stage Approach to Long Context Extension for Large Language Models", "link": "https://arxiv.org/pdf/2412.07171", "details": "H Lian, J Chen, W Huang, Y Xiong, W Hu, G Ding\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, Large language models (LLMs) have revolutionized Natural Language Processing (NLP). Pretrained LLMs, due to limited training context size, struggle with handling long token sequences, limiting their performance on various downstream \u2026"}]
