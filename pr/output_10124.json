[{"title": "A strategy for cost-effective large language model use at health system-scale", "link": "https://www.nature.com/articles/s41746-024-01315-1", "details": "E Klang, D Apakama, EE Abbott, A Vaid, J Lampert\u2026 - npj Digital Medicine, 2024", "abstract": "Large language models (LLMs) can optimize clinical workflows; however, the economic and computational challenges of their utilization at the health system scale are underexplored. We evaluated how concatenating queries with multiple clinical \u2026"}, {"title": "Self-improving generative foundation model for synthetic medical image generation and clinical applications", "link": "https://www.nature.com/articles/s41591-024-03359-y", "details": "J Wang, K Wang, Y Yu, Y Lu, W Xiao, Z Sun, F Liu\u2026 - Nature Medicine, 2024", "abstract": "In many clinical and research settings, the scarcity of high-quality medical imaging datasets has hampered the potential of artificial intelligence (AI) clinical applications. This issue is particularly pronounced in less common conditions, underrepresented \u2026"}, {"title": "BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities", "link": "https://arxiv.org/pdf/2412.07769", "details": "SS Mullappilly, MI Kurpath, S Pieri, SY Alseiari\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces BiMediX2, a bilingual (Arabic-English) Bio-Medical EXpert Large Multimodal Model (LMM) with a unified architecture that integrates text and visual modalities, enabling advanced image understanding and medical \u2026"}, {"title": "Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training", "link": "https://arxiv.org/pdf/2411.15207", "details": "A Bawazir, K Wu, W Li - arXiv preprint arXiv:2411.15207, 2024", "abstract": "Recent advancements in vision-language pre-training via contrastive learning have significantly improved performance across computer vision tasks. However, in the medical domain, obtaining multimodal data is often costly and challenging due to \u2026"}, {"title": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos", "link": "https://arxiv.org/pdf/2411.14901", "details": "T Hannan, MM Islam, J Gu, T Seidl, G Bertasius - arXiv preprint arXiv:2411.14901, 2024", "abstract": "Large language models (LLMs) excel at retrieving information from lengthy text, but their vision-language counterparts (VLMs) face difficulties with hour-long videos, especially for temporal grounding. Specifically, these VLMs are constrained by frame \u2026"}, {"title": "Med-2E3: A 2D-Enhanced 3D Medical Multimodal Large Language Model", "link": "https://arxiv.org/pdf/2411.12783", "details": "Y Shi, X Zhu, Y Hu, C Guo, M Li, J Wu - arXiv preprint arXiv:2411.12783, 2024", "abstract": "The analysis of 3D medical images is crucial for modern healthcare, yet traditional task-specific models are becoming increasingly inadequate due to limited generalizability across diverse clinical scenarios. Multimodal large language models \u2026"}, {"title": "SAT: Spatial Aptitude Training for Multimodal Language Models", "link": "https://arxiv.org/pdf/2412.07755", "details": "A Ray, J Duan, R Tan, D Bashkirova, R Hendrix\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Spatial perception is a fundamental component of intelligence. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only test for static spatial reasoning, such as categorizing the relative \u2026"}, {"title": "Qilin-Med-VL: Benchmarking Chinese Large Vision-Language Model for General Healthcare", "link": "https://www.researchgate.net/profile/Meng_Cao31/publication/386598823_Qilin-Med-VL_Benchmarking_Chinese_Large_Vision-Language_Model_for_General_Healthcare/links/6758096e3f7c7c7a83240bc7/Qilin-Med-VL-Benchmarking-Chinese-Large-Vision-Language-Model-for-General-Healthcare.pdf", "details": "J Liu, H Li, M Cao, Z Wang, Q Ye, D Chong, P Zhou\u2026", "abstract": "Abstract Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can \u2026"}, {"title": "Informed Augmentation Selection Improves Tabular Contrastive Learning", "link": "https://openreview.net/pdf%3Fid%3DGFu8qDtVQa", "details": "A Khoeini, S Peng, M Ester - NeurIPS 2024 Workshop: Self-Supervised Learning \u2026", "abstract": "While contrastive learning (CL) has demonstrated success in image data, its application to tabular data remains relatively unexplored. The effectiveness of CL heavily depends on data augmentations, yet the suitability of tabular augmentation \u2026"}]
