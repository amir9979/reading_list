[{"title": "CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification", "link": "https://arxiv.org/pdf/2501.12266", "details": "C Patr\u00edcio, I Rio-Torto, JS Cardoso, LF Teixeira\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter \u2026"}, {"title": "Learning with Enriched Inductive Biases for Vision-Language Models", "link": "https://ruyuanzhang.github.io/files/2501_indctbiasVisLangModel_IJCV.pdf", "details": "L Yang, RY Zhang, Q Chen, X Xie - International Journal of Computer Vision, 2025", "abstract": "Abstract Vision-Language Models, pre-trained on large-scale image-text pairs, serve as strong foundation models for transfer learning across a variety of downstream tasks. For few-shot generalization tasks, ie., when the model is trained on few-shot \u2026"}, {"title": "Efficient Few-Shot Continual Learning in Vision-Language Models", "link": "https://arxiv.org/pdf/2502.04098", "details": "A Panos, R Aljundi, DO Reino, RE Turner - arXiv preprint arXiv:2502.04098, 2025", "abstract": "Vision-language models (VLMs) excel in tasks such as visual question answering and image captioning. However, VLMs are often limited by their use of pretrained image encoders, like CLIP, leading to image understanding errors that hinder overall \u2026"}, {"title": "SQL Autograder: Web-based LLM-powered Autograder for Assessment of SQL Queries", "link": "https://link.springer.com/article/10.1007/s40593-025-00460-2", "details": "K Manikani, R Chapaneri, D Shetty, D Shah - International Journal of Artificial \u2026, 2025", "abstract": "Structured query language (SQL) queries are an important aspect of database concepts in the information technology (IT) domain. Evaluation of SQL queries ensures that the learners can understand and apply various SQL concepts correctly \u2026"}, {"title": "Test-time Loss Landscape Adaptation for Zero-Shot Generalization in Vision-Language Models", "link": "https://arxiv.org/pdf/2501.18864", "details": "A Li, L Zhuang, X Long, M Yao, S Wang - arXiv preprint arXiv:2501.18864, 2025", "abstract": "Test-time adaptation of pre-trained vision-language models has emerged as a technique for tackling distribution shifts during the test time. Although existing methods, especially those based on Test-time Prompt Tuning (TPT), have shown \u2026"}, {"title": "Mordal: Automated Pretrained Model Selection for Vision Language Models", "link": "https://arxiv.org/pdf/2502.00241", "details": "S He, I Jang, M Chowdhury - arXiv preprint arXiv:2502.00241, 2025", "abstract": "Incorporating multiple modalities into large language models (LLMs) is a powerful way to enhance their understanding of non-textual data, enabling them to perform multimodal tasks. Vision language models (VLMs) form the fastest growing category \u2026"}, {"title": "BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation", "link": "https://arxiv.org/pdf/2502.03860", "details": "B Pang, H Dong, J Xu, S Savarese, Y Zhou, C Xiong - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs), such as o1 from OpenAI, have demonstrated remarkable reasoning capabilities. o1 generates a long chain-of-thought (LongCoT) before answering a question. LongCoT allows LLMs to analyze problems, devise \u2026"}, {"title": "A Collection of Question Answering Datasets for Norwegian", "link": "https://arxiv.org/pdf/2501.11128", "details": "V Mikhailov, P M\u00e6hlum, VOC Lang\u00f8, E Velldal\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper introduces a new suite of question answering datasets for Norwegian; NorOpenBookQA, NorCommonSenseQA, NorTruthfulQA, and NRK-Quiz-QA. The data covers a wide range of skills and knowledge domains, including world \u2026"}, {"title": "Double Visual Defense: Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness", "link": "https://arxiv.org/pdf/2501.09446", "details": "Z Wang, C Xie, B Bartoldson, B Kailkhura - arXiv preprint arXiv:2501.09446, 2025", "abstract": "This paper investigates the robustness of vision-language models against adversarial visual perturbations and introduces a novel``double visual defense\" to enhance this robustness. Unlike previous approaches that resort to lightweight \u2026"}]
