[{"title": "LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models", "link": "https://arxiv.org/pdf/2406.19486", "details": "S Guo, S Damani, K Chang - arXiv preprint arXiv:2406.19486, 2024", "abstract": "In prompt tuning, a prefix or suffix text is added to the prompt, and the embeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix are optimized to gain more control over language models for specific tasks. This approach eliminates the \u2026"}, {"title": "{\\mu}-Bench: A Vision-Language Benchmark for Microscopy Understanding", "link": "https://arxiv.org/pdf/2407.01791", "details": "A Lozano, J Nirschl, J Burgess, SR Gupte, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in microscopy have enabled the rapid generation of terabytes of image data in cell biology and biomedical research. Vision-language models (VLMs) offer a promising solution for large-scale biological image analysis, enhancing \u2026"}, {"title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application", "link": "https://arxiv.org/pdf/2407.01885", "details": "C Yang, W Lu, Y Zhu, Y Wang, Q Chen, C Gao, B Yan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have showcased exceptional capabilities in various domains, attracting significant interest from both academia and industry. Despite their impressive performance, the substantial size and computational demands of LLMs \u2026"}, {"title": "Evaluating machine learning approaches for multi-label classification of unstructured electronic health records with a generative large language model", "link": "https://www.medrxiv.org/content/10.1101/2024.06.24.24309441.full.pdf", "details": "D Vithanage, C Deng, L Wang, M Yin, M Alkhalaf\u2026 - medRxiv, 2024", "abstract": "Multi-label classification of unstructured electronic health records (EHR) poses challenges due to the inherent semantic complexity in textual data. Advances in natural language processing (NLP) using large language models (LLMs) show \u2026"}, {"title": "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models", "link": "https://arxiv.org/pdf/2407.16470", "details": "K Benkirane, L Gongas, S Pelles, N Fuchs, J Darmon\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting \u2026"}, {"title": "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation", "link": "https://arxiv.org/pdf/2407.02056", "details": "X Wang, Y Li, S Feng, P Yuan, B Pan, H Wang, Y Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-consistency (SC), leveraging multiple samples from LLMs, shows significant gains on various reasoning tasks but struggles with free-form generation due to the difficulty of aggregating answers. Its variants, UCS and USC, rely on sample \u2026"}, {"title": "Continuous Identification of Sepsis-Associated Acute Heart Failure Patients: An Integrated LSTM-Based Algorithm", "link": "https://link.springer.com/chapter/10.1007/978-981-97-5128-0_40", "details": "J Zhuang, L Xie, C Peng, G Zeng, M Wu, X Yu - International Symposium on \u2026, 2024", "abstract": "Cardiovascular dysfunction often accompanies sepsis and increases the incidence of acute heart failure (AHF), which poses significant threats to patient survival and prognosis. Research applying machine learning to investigate AHF in this context is \u2026"}, {"title": "EMPL: A novel Efficient Meta Prompt Learning Framework for Few-shot Unsupervised Domain Adaptation", "link": "https://arxiv.org/pdf/2407.04066", "details": "W Yang, H Wang, L Wang, G Song, Y Gao - arXiv preprint arXiv:2407.04066, 2024", "abstract": "Few-shot unsupervised domain adaptation (FS-UDA) utilizes few-shot labeled source domain data to realize effective classification in unlabeled target domain. However, current FS-UDA methods are still suffer from two issues: 1) the data from \u2026"}, {"title": "Boosting Large Language Models with Socratic Method for Conversational Mathematics Teaching", "link": "https://arxiv.org/pdf/2407.17349", "details": "Y Ding, H Hu, J Zhou, Q Chen, B Jiang, L He - arXiv preprint arXiv:2407.17349, 2024", "abstract": "With the introduction of large language models (LLMs), automatic math reasoning has seen tremendous success. However, current methods primarily focus on providing solutions or using techniques like Chain-of-Thought to enhance problem \u2026"}]
