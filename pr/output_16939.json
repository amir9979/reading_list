[{"title": "Bayesian Deep Learning for Discrete Choice", "link": "https://arxiv.org/pdf/2505.18077", "details": "DF Villarraga, RA Daziano - arXiv preprint arXiv:2505.18077, 2025", "abstract": "Discrete choice models (DCMs) are used to analyze individual decision-making in contexts such as transportation choices, political elections, and consumer preferences. DCMs play a central role in applied econometrics by enabling inference \u2026", "entry_id": "http://arxiv.org/abs/2505.18077v1", "updated": "2025-05-23 16:33:47", "published": "2025-05-23 16:33:47", "authors": "Daniel F. Villarraga;Ricardo A. Daziano", "summary": "Discrete choice models (DCMs) are used to analyze individual decision-making\nin contexts such as transportation choices, political elections, and consumer\npreferences. DCMs play a central role in applied econometrics by enabling\ninference on key economic variables, such as marginal rates of substitution,\nrather than focusing solely on predicting choices on new unlabeled data.\nHowever, while traditional DCMs offer high interpretability and support for\npoint and interval estimation of economic quantities, these models often\nunderperform in predictive tasks compared to deep learning (DL) models. Despite\ntheir predictive advantages, DL models remain largely underutilized in discrete\nchoice due to concerns about their lack of interpretability, unstable parameter\nestimates, and the absence of established methods for uncertainty\nquantification. Here, we introduce a deep learning model architecture\nspecifically designed to integrate with approximate Bayesian inference methods,\nsuch as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model\ncollapses to behaviorally informed hypotheses when data is limited, mitigating\noverfitting and instability in underspecified settings while retaining the\nflexibility to capture complex nonlinear relationships when sufficient data is\navailable. We demonstrate our approach using SGLD through a Monte Carlo\nsimulation study, evaluating both predictive metrics--such as out-of-sample\nbalanced accuracy--and inferential metrics--such as empirical coverage for\nmarginal rates of substitution interval estimates. Additionally, we present\nresults from two empirical case studies: one using revealed mode choice data in\nNYC, and the other based on the widely used Swiss train choice stated\npreference data.", "comment": null, "journal_ref": null, "primary_category": "stat.ML", "categories": "stat.ML;cs.LG;econ.EM;stat.AP", "links": "http://arxiv.org/abs/2505.18077v1;http://arxiv.org/pdf/2505.18077v1", "pdf_url": "http://arxiv.org/pdf/2505.18077v1"}]
