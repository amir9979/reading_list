[{"title": "Generating Structured Outputs from Language Models: Benchmark and Studies", "link": "https://arxiv.org/pdf/2501.10868", "details": "S Geng, H Cooper, M Moskal, S Jenkins, J Berman\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reliably generating structured outputs has become a critical capability for modern language model (LM) applications. Constrained decoding has emerged as the dominant technology across sectors for enforcing structured outputs during \u2026"}, {"title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2501.11175", "details": "Y Bendou, A Ouasfi, V Gripon, A Boukhayma - arXiv preprint arXiv:2501.11175, 2025", "abstract": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has led to its widespread application in various visual downstream tasks. To enhance CLIP's effectiveness and versatility, efficient few-shot adaptation techniques have been \u2026"}, {"title": "Embedding-Driven Diversity Sampling to Improve Few-Shot Synthetic Data Generation", "link": "https://arxiv.org/pdf/2501.11199", "details": "I Lopez, FN Haredasht, K Caoili, JH Chen, A Chaudhari - arXiv preprint arXiv \u2026, 2025", "abstract": "Accurate classification of clinical text often requires fine-tuning pre-trained language models, a process that is costly and time-consuming due to the need for high-quality data and expert annotators. Synthetic data generation offers an alternative, though \u2026"}, {"title": "DiffuSETS: 12-lead ECG Generation Conditioned on Clinical Text Reports and Patient-Specific Information", "link": "https://arxiv.org/pdf/2501.05932", "details": "Y Lai, J Chen, D Zhang, Y Wang, S Geng, H Li, S Hong - arXiv preprint arXiv \u2026, 2025", "abstract": "Heart disease remains a significant threat to human health. As a non-invasive diagnostic tool, the electrocardiogram (ECG) is one of the most widely used methods for cardiac screening. However, the scarcity of high-quality ECG data, driven by \u2026"}, {"title": "Word-level Cross-lingual Structure in Large Language Models", "link": "https://aclanthology.org/2025.coling-main.138.pdf", "details": "Z Feng, H Cao, W Xu, T Zhao - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Abstract Large Language Models (LLMs) have demonstrated exceptional performance across a broad spectrum of cross-lingual Natural Language Processing (NLP) tasks. However, previous methods predominantly focus on leveraging parallel \u2026"}, {"title": "Benchmarking Large Language Models via Random Variables", "link": "https://arxiv.org/pdf/2501.11790", "details": "Z Hong, H Wu, S Dong, J Dong, Y Xiao, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the continuous advancement of large language models (LLMs) in mathematical reasoning, evaluating their performance in this domain has become a prominent research focus. Recent studies have raised concerns about the reliability of current \u2026"}, {"title": "Understanding Before Reasoning: Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting", "link": "https://arxiv.org/pdf/2501.04341%3F", "details": "DH Zhu, YJ Xiong, JC Zhang, XJ Xie, CM Xia - arXiv preprint arXiv:2501.04341, 2025", "abstract": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language Models (LLMs) to enhance complex reasoning. It guides LLMs to present multi-step reasoning, rather than generating the final answer directly. However, CoT \u2026"}]
