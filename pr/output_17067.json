[{"title": "ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency Room", "link": "https://arxiv.org/pdf/2505.22919", "details": "N Mehandru, N Golchini, D Bamman, T Zack\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 \u2022 **LLM** **evaluation** tasks aligned with key stages of the ER workflow, including triage intake, EHR review, initial assessment, diagnostic and treatment selection, disposition, and final diagnosis. Each task is grounded in realistic input derived from \u2026", "entry_id": "http://arxiv.org/abs/2505.22919v1", "updated": "2025-05-28 22:43:44", "published": "2025-05-28 22:43:44", "authors": "Nikita Mehandru;Niloufar Golchini;David Bamman;Travis Zack;Melanie F. Molina;Ahmed Alaa", "summary": "Large language models (LLMs) have been extensively evaluated on medical\nquestion answering tasks based on licensing exams. However, real-world\nevaluations often depend on costly human annotators, and existing benchmarks\ntend to focus on isolated tasks that rarely capture the clinical reasoning or\nfull workflow underlying medical decisions. In this paper, we introduce\nER-Reason, a benchmark designed to evaluate LLM-based clinical reasoning and\ndecision-making in the emergency room (ER)--a high-stakes setting where\nclinicians make rapid, consequential decisions across diverse patient\npresentations and medical specialties under time pressure. ER-Reason includes\ndata from 3,984 patients, encompassing 25,174 de-identified longitudinal\nclinical notes spanning discharge summaries, progress notes, history and\nphysical exams, consults, echocardiography reports, imaging notes, and ER\nprovider documentation. The benchmark includes evaluation tasks that span key\nstages of the ER workflow: triage intake, initial assessment, treatment\nselection, disposition planning, and final diagnosis--each structured to\nreflect core clinical reasoning processes such as differential diagnosis via\nrule-out reasoning. We also collected 72 full physician-authored rationales\nexplaining reasoning processes that mimic the teaching process used in\nresidency training, and are typically absent from ER documentation. Evaluations\nof state-of-the-art LLMs on ER-Reason reveal a gap between LLM-generated and\nclinician-authored clinical reasoning for ER decisions, highlighting the need\nfor future research to bridge this divide.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.22919v1;http://arxiv.org/pdf/2505.22919v1", "pdf_url": "http://arxiv.org/pdf/2505.22919v1"}, {"title": "Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective", "link": "https://arxiv.org/pdf/2505.23277", "details": "Y Zhang, Y Huang, N Cheng, Y Guo, Y Zhu, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external context, but retrieved passages are often lengthy, noisy, or exceed input limits. Existing compression methods typically require supervised training of \u2026", "entry_id": "http://arxiv.org/abs/2505.23277v1", "updated": "2025-05-29 09:24:12", "published": "2025-05-29 09:24:12", "authors": "Yong Zhang;Yanwen Huang;Ning Cheng;Yang Guo;Yun Zhu;Yanmeng Wang;Shaojun Wang;Jing Xiao", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external context, but retrieved passages are often lengthy, noisy, or\nexceed input limits. Existing compression methods typically require supervised\ntraining of dedicated compression models, increasing cost and reducing\nportability. We propose Sentinel, a lightweight sentence-level compression\nframework that reframes context filtering as an attention-based understanding\ntask. Rather than training a compression model, Sentinel probes decoder\nattention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier\nto identify sentence relevance. Empirically, we find that query-context\nrelevance estimation is consistent across model scales, with 0.5B proxies\nclosely matching the behaviors of larger models. On the LongBench benchmark,\nSentinel achieves up to 5$\\times$ compression while matching the QA performance\nof 7B-scale compression systems. Our results suggest that probing native\nattention signals enables fast, effective, and question-aware context\ncompression. Code available at: https://github.com/yzhangchuck/Sentinel.", "comment": "Preprint. 17 pages including appendix", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.23277v1;http://arxiv.org/pdf/2505.23277v1", "pdf_url": "http://arxiv.org/pdf/2505.23277v1"}, {"title": "Domain Specific Benchmarks for Evaluating Multimodal Large Language Models", "link": "https://www.preprints.org/frontend/manuscript/d56e264011a4a69f9edd5f4fdcfbb212/download_pub", "details": "K Anjum, MA Arshad, K Hayawi, E Polyzos, A Tariq\u2026 - 2025", "abstract": "\u2026 While several surveys address **LLM** **evaluation** and benchmarks, a domain-specific analysis remains underexplored in the literature. This paper introduces a taxonomy of seven key disciplines, encompassing various domains and application areas \u2026"}, {"title": "Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons", "link": "https://arxiv.org/pdf/2505.23477", "details": "K Vishwanath, A Alyakin, M Ghosh, JV Lee, DA Alber\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 KV designed and developed the **LLM** **evaluation** pipeline and the SANS with distractions benchmark. KV and MG wrote the initial draft and developed the figures of the manuscript. All authors revised and approved the manuscript. \u2026", "entry_id": "http://arxiv.org/abs/2505.23477v1", "updated": "2025-05-29 14:27:14", "published": "2025-05-29 14:27:14", "authors": "Krithik Vishwanath;Anton Alyakin;Mrigayu Ghosh;Jin Vivian Lee;Daniel Alexander Alber;Karl L. Sangwon;Douglas Kondziolka;Eric Karl Oermann", "summary": "The Congress of Neurological Surgeons Self-Assessment for Neurological\nSurgeons (CNS-SANS) questions are widely used by neurosurgical residents to\nprepare for written board examinations. Recently, these questions have also\nserved as benchmarks for evaluating large language models' (LLMs) neurosurgical\nknowledge. This study aims to assess the performance of state-of-the-art LLMs\non neurosurgery board-like questions and to evaluate their robustness to the\ninclusion of distractor statements. A comprehensive evaluation was conducted\nusing 28 large language models. These models were tested on 2,904 neurosurgery\nboard examination questions derived from the CNS-SANS. Additionally, the study\nintroduced a distraction framework to assess the fragility of these models. The\nframework incorporated simple, irrelevant distractor statements containing\npolysemous words with clinical meanings used in non-clinical contexts to\ndetermine the extent to which such distractions degrade model performance on\nstandard medical benchmarks. 6 of the 28 tested LLMs achieved board-passing\noutcomes, with the top-performing models scoring over 15.7% above the passing\nthreshold. When exposed to distractions, accuracy across various model\narchitectures was significantly reduced-by as much as 20.4%-with one model\nfailing that had previously passed. Both general-purpose and medical\nopen-source models experienced greater performance declines compared to\nproprietary variants when subjected to the added distractors. While current\nLLMs demonstrate an impressive ability to answer neurosurgery board-like exam\nquestions, their performance is markedly vulnerable to extraneous, distracting\ninformation. These findings underscore the critical need for developing novel\nmitigation strategies aimed at bolstering LLM resilience against in-text\ndistractions, particularly for safe and effective clinical deployment.", "comment": "22 pages, 3 main figures, 3 supplemental figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.23477v1;http://arxiv.org/pdf/2505.23477v1", "pdf_url": "http://arxiv.org/pdf/2505.23477v1"}, {"title": "Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking", "link": "https://arxiv.org/pdf/2505.23495", "details": "L Zhang, Z Jiang, H Chi, H Chen, M Elkoumy, F Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 This involves alternating between KG traversal and **LLM** **evaluation**. At each step, the subgraph is expanded by exploring neighboring entities and relations within the KG. After each expansion, an LLM is prompted to evaluate whether the subgraph \u2026", "entry_id": "http://arxiv.org/abs/2505.23495v1", "updated": "2025-05-29 14:44:52", "published": "2025-05-29 14:44:52", "authors": "Liangliang Zhang;Zhuorui Jiang;Hongliang Chi;Haoyang Chen;Mohammed Elkoumy;Fali Wang;Qiong Wu;Zhengyi Zhou;Shirui Pan;Suhang Wang;Yao Ma", "summary": "Knowledge Graph Question Answering (KGQA) systems rely on high-quality\nbenchmarks to evaluate complex multi-hop reasoning. However, despite their\nwidespread use, popular datasets such as WebQSP and CWQ suffer from critical\nquality issues, including inaccurate or incomplete ground-truth annotations,\npoorly constructed questions that are ambiguous, trivial, or unanswerable, and\noutdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA\ndatasets, including WebQSP and CWQ, we find that the average factual\ncorrectness rate is only 57 %. To address these issues, we introduce KGQAGen,\nan LLM-in-the-loop framework that systematically resolves these pitfalls.\nKGQAGen combines structured knowledge grounding, LLM-guided generation, and\nsymbolic verification to produce challenging and verifiable QA instances. Using\nKGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in\nWikidata, and evaluate a diverse set of KG-RAG models. Experimental results\ndemonstrate that even state-of-the-art systems struggle on this benchmark,\nhighlighting its ability to expose limitations of existing models. Our findings\nadvocate for more rigorous benchmark construction and position KGQAGen as a\nscalable framework for advancing KGQA evaluation.", "comment": "9 pages", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.23495v1;http://arxiv.org/pdf/2505.23495v1", "pdf_url": "http://arxiv.org/pdf/2505.23495v1"}, {"title": "SWE-bench Goes Live!", "link": "https://arxiv.org/pdf/2505.23419", "details": "L Zhang, S He, C Zhang, Y Kang, B Li, C Xie, J Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The issue-resolving task, where a model generates patches to fix real-world bugs, has emerged as a critical benchmark for evaluating the capabilities of large language models (LLMs). While SWE-bench and its variants have become standard \u2026", "entry_id": "http://arxiv.org/abs/2505.23419v1", "updated": "2025-05-29 13:09:44", "published": "2025-05-29 13:09:44", "authors": "Linghao Zhang;Shilin He;Chaoyun Zhang;Yu Kang;Bowen Li;Chengxing Xie;Junhao Wang;Maoquan Wang;Yufan Huang;Shengyu Fu;Elsie Nallipogu;Qingwei Lin;Yingnong Dang;Saravan Rajmohan;Dongmei Zhang", "summary": "The issue-resolving task, where a model generates patches to fix real-world\nbugs, has emerged as a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs). While SWE-bench and its variants have become\nstandard in this domain, they suffer from key limitations: they have not been\nupdated since their initial releases, cover a narrow set of repositories, and\ndepend heavily on manual effort for instance construction and environment\nsetup. These factors hinder scalability and introduce risks of overfitting and\ndata contamination. In this work, we present \\textbf{SWE-bench-Live}, a\n\\textit{live-updatable} benchmark designed to overcome these challenges. Our\ninitial release consists of 1,319 tasks derived from real GitHub issues created\nsince 2024, spanning 93 repositories. Each task is accompanied by a dedicated\nDocker image to ensure reproducible execution. Central to our benchmark is\n\\method, an automated curation pipeline that streamlines the entire process\nfrom instance creation to environment setup, removing manual bottlenecks and\nenabling scalability and continuous updates. We evaluate a range of\nstate-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a\nsubstantial performance gap compared to static benchmarks like SWE-bench, even\nunder controlled evaluation conditions. To better understand this discrepancy,\nwe perform detailed analyses across repository origin, issue recency, and task\ndifficulty. By providing a fresh, diverse, and executable benchmark grounded in\nlive repository activity, SWE-bench-Live facilitates rigorous,\ncontamination-resistant evaluation of LLMs and agents in dynamic, real-world\nsoftware development settings.", "comment": "Homepage: \\url{https://swe-bench-live.github.io/}, Code:\n  \\url{https://github.com/SWE-bench-Live}, Dataset:\n  \\url{https://huggingface.co/SWE-bench-Live}", "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE;cs.AI;cs.CL", "links": "http://arxiv.org/abs/2505.23419v1;http://arxiv.org/pdf/2505.23419v1", "pdf_url": "http://arxiv.org/pdf/2505.23419v1"}, {"title": "Can Large Language Models Match the Conclusions of Systematic Reviews?", "link": "https://arxiv.org/pdf/2505.22787", "details": "C Polzak, A Lozano, MW Sun, J Burgess, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **LLM** **evaluation** Model performance was evaluated using accuracy based on an exact match between the answer field and the ground truth. Model outputs were lower-cased and stripped of whitespace before comparison. If no \u2018answer\u2019 field was \u2026", "entry_id": "http://arxiv.org/abs/2505.22787v1", "updated": "2025-05-28 18:58:09", "published": "2025-05-28 18:58:09", "authors": "Christopher Polzak;Alejandro Lozano;Min Woo Sun;James Burgess;Yuhui Zhang;Kevin Wu;Serena Yeung-Levy", "summary": "Systematic reviews (SR), in which experts summarize and analyze evidence\nacross individual studies to provide insights on a specialized topic, are a\ncornerstone for evidence-based clinical decision-making, research, and policy.\nGiven the exponential growth of scientific articles, there is growing interest\nin using large language models (LLMs) to automate SR generation. However, the\nability of LLMs to critically assess evidence and reason across multiple\ndocuments to provide recommendations at the same proficiency as domain experts\nremains poorly characterized. We therefore ask: Can LLMs match the conclusions\nof systematic reviews written by clinical experts when given access to the same\nstudies? To explore this question, we present MedEvidence, a benchmark pairing\nfindings from 100 SRs with the studies they are based on. We benchmark 24 LLMs\non MedEvidence, including reasoning, non-reasoning, medical specialist, and\nmodels across varying sizes (from 7B-700B). Through our systematic evaluation,\nwe find that reasoning does not necessarily improve performance, larger models\ndo not consistently yield greater gains, and knowledge-based fine-tuning\ndegrades accuracy on MedEvidence. Instead, most models exhibit similar\nbehavior: performance tends to degrade as token length increases, their\nresponses show overconfidence, and, contrary to human experts, all models show\na lack of scientific skepticism toward low-quality findings. These results\nsuggest that more work is still required before LLMs can reliably match the\nobservations from expert-conducted SRs, even though these systems are already\ndeployed and being used by clinicians. We release our codebase and benchmark to\nthe broader research community to further investigate LLM-based SR systems.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.22787v1;http://arxiv.org/pdf/2505.22787v1", "pdf_url": "http://arxiv.org/pdf/2505.22787v1"}, {"title": "SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2505.23713", "details": "Z Xu, Y Wang, Y Huang, J Ye, H Zhuang, Z Song\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) are increasingly applied to socially grounded tasks, such as online community moderation, media content analysis, and social reasoning games. Success in these contexts depends on a model's social reasoning ability \u2026", "entry_id": "http://arxiv.org/abs/2505.23713v1", "updated": "2025-05-29 17:47:36", "published": "2025-05-29 17:47:36", "authors": "Zixiang Xu;Yanbo Wang;Yue Huang;Jiayi Ye;Haomin Zhuang;Zirui Song;Lang Gao;Chenxi Wang;Zhaorun Chen;Yujun Zhou;Sixian Li;Wang Pan;Yue Zhao;Jieyu Zhao;Xiangliang Zhang;Xiuying Chen", "summary": "Large language models (LLMs) are increasingly applied to socially grounded\ntasks, such as online community moderation, media content analysis, and social\nreasoning games. Success in these contexts depends on a model's social\nreasoning ability - the capacity to interpret social contexts, infer others'\nmental states, and assess the truthfulness of presented information. However,\nthere is currently no systematic evaluation framework that comprehensively\nassesses the social reasoning capabilities of LLMs. Existing efforts often\noversimplify real-world scenarios and consist of tasks that are too basic to\nchallenge advanced models. To address this gap, we introduce SocialMaze, a new\nbenchmark specifically designed to evaluate social reasoning. SocialMaze\nsystematically incorporates three core challenges: deep reasoning, dynamic\ninteraction, and information uncertainty. It provides six diverse tasks across\nthree key settings: social reasoning games, daily-life interactions, and\ndigital community platforms. Both automated and human validation are used to\nensure data quality. Our evaluation reveals several key insights: models vary\nsubstantially in their ability to handle dynamic interactions and integrate\ntemporally evolving information; models with strong chain-of-thought reasoning\nperform better on tasks requiring deeper inference beyond surface-level cues;\nand model reasoning degrades significantly under uncertainty. Furthermore, we\nshow that targeted fine-tuning on curated reasoning examples can greatly\nimprove model performance in complex social scenarios. The dataset is publicly\navailable at: https://huggingface.co/datasets/MBZUAI/SocialMaze", "comment": "Code available at https://github.com/xzx34/SocialMaze", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.23713v1;http://arxiv.org/pdf/2505.23713v1", "pdf_url": "http://arxiv.org/pdf/2505.23713v1"}, {"title": "Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness", "link": "https://arxiv.org/pdf/2505.22960", "details": "Y Yang, E Yi, J Ko, K Lee, Z Jin, SY Yun - arXiv preprint arXiv:2505.22960, 2025", "abstract": "\u2026 You are a helpful **LLM** **evaluation** assistant. You will receive a prompt and the AI assistant\u2019s response to the prompt. Please act as an impartial judge and evaluate the response of the LLM within the range from 0.0 to 1.0 with respect to one criterion \u2026", "entry_id": "http://arxiv.org/abs/2505.22960v1", "updated": "2025-05-29 01:02:55", "published": "2025-05-29 01:02:55", "authors": "Yongjin Yang;Euiin Yi;Jongwoo Ko;Kimin Lee;Zhijing Jin;Se-Young Yun", "summary": "The remarkable growth in large language model (LLM) capabilities has spurred\nexploration into multi-agent systems, with debate frameworks emerging as a\npromising avenue for enhanced problem-solving. These multi-agent debate (MAD)\napproaches, where agents collaboratively present, critique, and refine\narguments, potentially offer improved reasoning, robustness, and diverse\nperspectives over monolithic models. Despite prior studies leveraging MAD, a\nsystematic understanding of its effectiveness compared to self-agent methods,\nparticularly under varying conditions, remains elusive. This paper seeks to\nfill this gap by conceptualizing MAD as a test-time computational scaling\ntechnique, distinguished by collaborative refinement and diverse exploration\ncapabilities. We conduct a comprehensive empirical investigation comparing MAD\nwith strong self-agent test-time scaling baselines on mathematical reasoning\nand safety-related tasks. Our study systematically examines the influence of\ntask difficulty, model scale, and agent diversity on MAD's performance. Key\nfindings reveal that, for mathematical reasoning, MAD offers limited advantages\nover self-agent scaling but becomes more effective with increased problem\ndifficulty and decreased model capability, while agent diversity shows little\nbenefit. Conversely, for safety tasks, MAD's collaborative refinement can\nincrease vulnerability, but incorporating diverse agent configurations\nfacilitates a gradual reduction in attack success through the collaborative\nrefinement process. We believe our findings provide critical guidance for the\nfuture development of more effective and strategically deployed MAD systems.", "comment": "Preprint, under review", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.22960v1;http://arxiv.org/pdf/2505.22960v1", "pdf_url": "http://arxiv.org/pdf/2505.22960v1"}]
