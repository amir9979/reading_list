[{"title": "Interactive Post-Training for Vision-Language-Action Models", "link": "https://arxiv.org/pdf/2505.17016", "details": "S Tan, K Dou, Y Zhao, P Kr\u00e4henb\u00fchl - arXiv preprint arXiv:2505.17016, 2025", "abstract": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training \u2026", "entry_id": "http://arxiv.org/abs/2505.17016v1", "updated": "2025-05-22 17:59:45", "published": "2025-05-22 17:59:45", "authors": "Shuhan Tan;Kairan Dou;Yue Zhao;Philipp Kr\u00e4henb\u00fchl", "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.", "comment": "Project page: https://ariostgx.github.io/ript_vla/", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CV;cs.RO", "links": "http://arxiv.org/abs/2505.17016v1;http://arxiv.org/pdf/2505.17016v1", "pdf_url": "http://arxiv.org/pdf/2505.17016v1"}, {"title": "Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability", "link": "https://arxiv.org/pdf/2505.16789", "details": "PS Pandey, S Simko, K Pelrine, Z Jin - arXiv preprint arXiv:2505.16789, 2025", "abstract": "As large language models gain popularity, their vulnerability to adversarial attacks remains a primary concern. While fine-tuning models on domain-specific datasets is often employed to improve model performance, it can introduce vulnerabilities within \u2026", "entry_id": "http://arxiv.org/abs/2505.16789v1", "updated": "2025-05-22 15:30:00", "published": "2025-05-22 15:30:00", "authors": "Punya Syon Pandey;Samuel Simko;Kellin Pelrine;Zhijing Jin", "summary": "As large language models gain popularity, their vulnerability to adversarial\nattacks remains a primary concern. While fine-tuning models on domain-specific\ndatasets is often employed to improve model performance, it can introduce\nvulnerabilities within the underlying model. In this work, we investigate\nAccidental Misalignment, unexpected vulnerabilities arising from\ncharacteristics of fine-tuning data. We begin by identifying potential\ncorrelation factors such as linguistic features, semantic similarity, and\ntoxicity within our experimental datasets. We then evaluate the adversarial\nperformance of these fine-tuned models and assess how dataset factors correlate\nwith attack success rates. Lastly, we explore potential causal links, offering\nnew insights into adversarial defense strategies and highlighting the crucial\nrole of dataset design in preserving model alignment. Our code is available at\nhttps://github.com/psyonp/accidental_misalignment.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.16789v1;http://arxiv.org/pdf/2505.16789v1", "pdf_url": "http://arxiv.org/pdf/2505.16789v1"}, {"title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models", "link": "https://arxiv.org/pdf/2505.14103", "details": "G Chen, F Song, Z Zhao, X Jia, Y Liu, Y Qiao, W Zhang - arXiv preprint arXiv \u2026, 2025", "abstract": "Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first \u2026", "entry_id": "http://arxiv.org/abs/2505.14103v2", "updated": "2025-05-21 03:36:20", "published": "2025-05-20 09:10:45", "authors": "Guangke Chen;Fu Song;Zhe Zhao;Xiaojun Jia;Yang Liu;Yanchen Qiao;Weizhe Zhang", "summary": "Jailbreak attacks to Large audio-language models (LALMs) are studied\nrecently, but they achieve suboptimal effectiveness, applicability, and\npracticability, particularly, assuming that the adversary can fully manipulate\nuser prompts. In this work, we first conduct an extensive experiment showing\nthat advanced text jailbreak attacks cannot be easily ported to end-to-end\nLALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a\nnovel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio\ndoes not need to align with user prompts in the time axis by crafting suffixal\njailbreak audios; (2) universality: a single jailbreak perturbation is\neffective for different prompts by incorporating multiple prompts into\nperturbation generation; (3) stealthiness: the malicious intent of jailbreak\naudios will not raise the awareness of victims by proposing various intent\nconcealment strategies; and (4) over-the-air robustness: the jailbreak audios\nremain effective when being played over the air by incorporating the\nreverberation distortion effect with room impulse response into the generation\nof the perturbations. In contrast, all prior audio jailbreak attacks cannot\noffer asynchrony, universality, stealthiness, or over-the-air robustness.\nMoreover, AudioJailbreak is also applicable to the adversary who cannot fully\nmanipulate user prompts, thus has a much broader attack scenario. Extensive\nexperiments with thus far the most LALMs demonstrate the high effectiveness of\nAudioJailbreak. We highlight that our work peeks into the security implications\nof audio jailbreak attacks against LALMs, and realistically fosters improving\ntheir security robustness. The implementation and audio samples are available\nat our website https://audiojailbreak.github.io/AudioJailbreak.", "comment": null, "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR;cs.AI;cs.LG;cs.SD;eess.AS", "links": "http://arxiv.org/abs/2505.14103v2;http://arxiv.org/pdf/2505.14103v2", "pdf_url": "http://arxiv.org/pdf/2505.14103v2"}, {"title": "Continually Self-Improving Language Models for Bariatric Surgery Question--Answering", "link": "https://arxiv.org/pdf/2505.16102", "details": "YK Atri, TH Shin, T Hartvigsen - arXiv preprint arXiv:2505.16102, 2025", "abstract": "While bariatric and metabolic surgery (MBS) is considered the gold standard treatment for severe and morbid obesity, its therapeutic efficacy hinges upon active and longitudinal engagement with multidisciplinary providers, including surgeons \u2026", "entry_id": "http://arxiv.org/abs/2505.16102v1", "updated": "2025-05-22 01:02:51", "published": "2025-05-22 01:02:51", "authors": "Yash Kumar Atri;Thomas H Shin;Thomas Hartvigsen", "summary": "While bariatric and metabolic surgery (MBS) is considered the gold standard\ntreatment for severe and morbid obesity, its therapeutic efficacy hinges upon\nactive and longitudinal engagement with multidisciplinary providers, including\nsurgeons, dietitians/nutritionists, psychologists, and endocrinologists. This\nengagement spans the entire patient journey, from preoperative preparation to\nlong-term postoperative management. However, this process is often hindered by\nnumerous healthcare disparities, such as logistical and access barriers, which\nimpair easy patient access to timely, evidence-based, clinician-endorsed\ninformation. To address these gaps, we introduce bRAGgen, a novel adaptive\nretrieval-augmented generation (RAG)-based model that autonomously integrates\nreal-time medical evidence when response confidence dips below dynamic\nthresholds. This self-updating architecture ensures that responses remain\ncurrent and accurate, reducing the risk of misinformation. Additionally, we\npresent bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,\nvalidated by an expert bariatric surgeon. bRAGq constitutes the first\nlarge-scale, domain-specific benchmark for comprehensive MBS care. In a\ntwo-phase evaluation, bRAGgen is benchmarked against state-of-the-art models\nusing both large language model (LLM)--based metrics and expert surgeon review.\nAcross all evaluation dimensions, bRAGgen demonstrates substantially superior\nperformance in generating clinically accurate and relevant responses.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.16102v1;http://arxiv.org/pdf/2505.16102v1", "pdf_url": "http://arxiv.org/pdf/2505.16102v1"}, {"title": "GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data", "link": "https://arxiv.org/pdf/2505.03233", "details": "S Deng, M Yan, S Wei, H Ma, Y Yang, J Chen, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post- training. However, existing models rely heavily on real-world data, which is costly \u2026", "entry_id": "http://arxiv.org/abs/2505.03233v1", "updated": "2025-05-06 06:59:28", "published": "2025-05-06 06:59:28", "authors": "Shengliang Deng;Mi Yan;Songlin Wei;Haixin Ma;Yuxin Yang;Jiayi Chen;Zhiqi Zhang;Taoyu Yang;Xuheng Zhang;Heming Cui;Zhizheng Zhang;He Wang", "summary": "Embodied foundation models are gaining increasing attention for their\nzero-shot generalization, scalability, and adaptability to new tasks through\nfew-shot post-training. However, existing models rely heavily on real-world\ndata, which is costly and labor-intensive to collect. Synthetic data offers a\ncost-effective alternative, yet its potential remains largely underexplored. To\nbridge this gap, we explore the feasibility of training Vision-Language-Action\nmodels entirely with large-scale synthetic action data. We curate SynGrasp-1B,\na billion-frame robotic grasping dataset generated in simulation with\nphotorealistic rendering and extensive domain randomization. Building on this,\nwe present GraspVLA, a VLA model pretrained on large-scale synthetic action\ndata as a foundational model for grasping tasks. GraspVLA integrates\nautoregressive perception tasks and flow-matching-based action generation into\na unified Chain-of-Thought process, enabling joint training on synthetic action\ndata and Internet semantics data. This design helps mitigate sim-to-real gaps\nand facilitates the transfer of learned actions to a broader range of\nInternet-covered objects, achieving open-vocabulary generalization in grasping.\nExtensive evaluations across real-world and simulation benchmarks demonstrate\nGraspVLA's advanced zero-shot generalizability and few-shot adaptability to\nspecific human preferences. We will release SynGrasp-1B dataset and pre-trained\nweights to benefit the community.", "comment": null, "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO", "links": "http://arxiv.org/abs/2505.03233v1;http://arxiv.org/pdf/2505.03233v1", "pdf_url": "http://arxiv.org/pdf/2505.03233v1"}, {"title": "Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization", "link": "https://arxiv.org/pdf/2505.15660", "details": "J Zhou, K Ye, J Liu, T Ma, Z Wang, R Qiu, KY Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The generalization capabilities of vision-language-action (VLA) models to unseen tasks are crucial to achieving general-purpose robotic manipulation in open-world settings. However, the cross-task generalization capabilities of existing VLA models \u2026", "entry_id": "http://arxiv.org/abs/2505.15660v1", "updated": "2025-05-21 15:35:57", "published": "2025-05-21 15:35:57", "authors": "Jiaming Zhou;Ke Ye;Jiayi Liu;Teli Ma;Zifang Wang;Ronghe Qiu;Kun-Yu Lin;Zhilin Zhao;Junwei Liang", "summary": "The generalization capabilities of vision-language-action (VLA) models to\nunseen tasks are crucial to achieving general-purpose robotic manipulation in\nopen-world settings. However, the cross-task generalization capabilities of\nexisting VLA models remain significantly underexplored. To address this gap, we\nintroduce AGNOSTOS, a novel simulation benchmark designed to rigorously\nevaluate cross-task zero-shot generalization in manipulation. AGNOSTOS\ncomprises 23 unseen manipulation tasks for testing, distinct from common\ntraining task distributions, and incorporates two levels of generalization\ndifficulty to assess robustness. Our systematic evaluation reveals that current\nVLA models, despite being trained on diverse datasets, struggle to generalize\neffectively to these unseen tasks. To overcome this limitation, we propose\nCross-Task In-Context Manipulation (X-ICM), a method that conditions large\nlanguage models (LLMs) on in-context demonstrations from seen tasks to predict\naction sequences for unseen tasks. Additionally, we introduce a dynamics-guided\nsample selection strategy that identifies relevant demonstrations by capturing\ncross-task dynamics. On AGNOSTOS, X-ICM significantly improves cross-task\nzero-shot generalization performance over leading VLAs. We believe AGNOSTOS and\nX-ICM will serve as valuable tools for advancing general-purpose robotic\nmanipulation.", "comment": "Project Page: https://jiaming-zhou.github.io/AGNOSTOS", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.CV", "links": "http://arxiv.org/abs/2505.15660v1;http://arxiv.org/pdf/2505.15660v1", "pdf_url": "http://arxiv.org/pdf/2505.15660v1"}, {"title": "FLARE: Robot Learning with Implicit World Modeling", "link": "https://arxiv.org/pdf/2505.15659", "details": "R Zheng, J Wang, S Reed, J Bjorck, Y Fang, F Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce $\\textbf {F} $ uture $\\textbf {LA} $ tent $\\textbf {RE} $ presentation Alignment ($\\textbf {FLARE} $), a novel framework that integrates predictive latent world modeling into robot policy learning. By aligning features from a diffusion \u2026", "entry_id": "http://arxiv.org/abs/2505.15659v1", "updated": "2025-05-21 15:33:27", "published": "2025-05-21 15:33:27", "authors": "Ruijie Zheng;Jing Wang;Scott Reed;Johan Bjorck;Yu Fang;Fengyuan Hu;Joel Jang;Kaushil Kundalia;Zongyu Lin;Loic Magne;Avnish Narayan;You Liang Tan;Guanzhi Wang;Qi Wang;Jiannan Xiang;Yinzhen Xu;Seonghyeon Ye;Jan Kautz;Furong Huang;Yuke Zhu;Linxi Fan", "summary": "We introduce $\\textbf{F}$uture $\\textbf{LA}$tent $\\textbf{RE}$presentation\nAlignment ($\\textbf{FLARE}$), a novel framework that integrates predictive\nlatent world modeling into robot policy learning. By aligning features from a\ndiffusion transformer with latent embeddings of future observations,\n$\\textbf{FLARE}$ enables a diffusion transformer policy to anticipate latent\nrepresentations of future observations, allowing it to reason about long-term\nconsequences while generating actions. Remarkably lightweight, $\\textbf{FLARE}$\nrequires only minimal architectural modifications -- adding a few tokens to\nstandard vision-language-action (VLA) models -- yet delivers substantial\nperformance gains. Across two challenging multitask simulation imitation\nlearning benchmarks spanning single-arm and humanoid tabletop manipulation,\n$\\textbf{FLARE}$ achieves state-of-the-art performance, outperforming prior\npolicy learning baselines by up to 26%. Moreover, $\\textbf{FLARE}$ unlocks the\nability to co-train with human egocentric video demonstrations without action\nlabels, significantly boosting policy generalization to a novel object with\nunseen geometry with as few as a single robot demonstration. Our results\nestablish $\\textbf{FLARE}$ as a general and scalable approach for combining\nimplicit world modeling with high-frequency robotic control.", "comment": "Project Webpage / Blogpost:\n  https://research.nvidia.com/labs/gear/flare", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.LG", "links": "http://arxiv.org/abs/2505.15659v1;http://arxiv.org/pdf/2505.15659v1", "pdf_url": "http://arxiv.org/pdf/2505.15659v1"}, {"title": "Policy Contrastive Decoding for Robotic Foundation Models", "link": "https://arxiv.org/pdf/2505.13255", "details": "S Wu, J Zhang, X Luo, J Xie, J Song, HT Shen, L Gao - arXiv preprint arXiv \u2026, 2025", "abstract": "Robotic foundation models, or generalist robot policies, hold immense potential to enable flexible, general-purpose and dexterous robotic systems. Despite their advancements, our empirical experiments reveal that existing robot policies are \u2026", "entry_id": "http://arxiv.org/abs/2505.13255v2", "updated": "2025-05-22 07:53:04", "published": "2025-05-19 15:39:08", "authors": "Shihan Wu;Ji Zhang;Xu Luo;Junlin Xie;Jingkuan Song;Heng Tao Shen;Lianli Gao", "summary": "Robotic foundation models, or generalist robot policies, hold immense\npotential to enable flexible, general-purpose and dexterous robotic systems.\nDespite their advancements, our empirical experiments reveal that existing\nrobot policies are prone to learning spurious correlations from pre-training\ntrajectories, adversely affecting their generalization capabilities beyond the\ntraining data. To tackle this, we propose a novel Policy Contrastive Decoding\n(PCD) approach, which redirects the robot policy's focus toward object-relevant\nvisual clues by contrasting action probability distributions derived from\noriginal and object-masked visual inputs. As a training-free method, our PCD\ncan be used as a plugin to improve different types of robot policies without\nneeding to finetune or access model weights. We conduct extensive experiments\non top of three open-source robot policies, including the autoregressive policy\nOpenVLA and the diffusion-based policies Octo and $\\pi_0$. The obtained results\nin both simulation and real-world environments prove PCD's flexibility and\neffectiveness, e.g., PCD enhances the state-of-the-art policy $\\pi_0$ by 8% in\nthe simulation environment and by 108% in the real-world environment. Code and\ndemos are publicly available at: https://Koorye.github.io/proj/PCD.", "comment": null, "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO", "links": "http://arxiv.org/abs/2505.13255v2;http://arxiv.org/pdf/2505.13255v2", "pdf_url": "http://arxiv.org/pdf/2505.13255v2"}, {"title": "Latent Principle Discovery for Language Model Self-Improvement", "link": "https://arxiv.org/pdf/2505.16927", "details": "K Ramji, T Naseem, RF Astudillo - arXiv preprint arXiv:2505.16927, 2025", "abstract": "When language model (LM) users aim to improve the quality of its generations, it is crucial to specify concrete behavioral attributes that the model should strive to reflect. However, curating such principles across many domains, even non-exhaustively \u2026", "entry_id": "http://arxiv.org/abs/2505.16927v1", "updated": "2025-05-22 17:20:18", "published": "2025-05-22 17:20:18", "authors": "Keshav Ramji;Tahira Naseem;Ram\u00f3n Fernandez Astudillo", "summary": "When language model (LM) users aim to improve the quality of its generations,\nit is crucial to specify concrete behavioral attributes that the model should\nstrive to reflect. However, curating such principles across many domains, even\nnon-exhaustively, requires a labor-intensive annotation process. To automate\nthis process, we propose eliciting these latent attributes guiding model\nreasoning towards human-preferred responses by explicitly modeling them in a\nself-correction setting. Our approach mines new principles from the LM itself\nand compresses the discovered elements to an interpretable set via clustering.\nSpecifically, we employ an approximation of posterior-regularized Monte Carlo\nExpectation-Maximization to both identify a condensed set of the most effective\nlatent principles and teach the LM to strategically invoke them in order to\nintrinsically refine its responses. We demonstrate that bootstrapping our\nalgorithm over multiple iterations enables smaller language models (7-8B\nparameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an\naverage of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on\nIFEval. We also show that clustering the principles yields interpretable and\ndiverse model-generated constitutions while retaining model performance. The\ngains our method achieves highlight the potential of automated,\nprinciple-driven post-training recipes toward continual self-improvement.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.16927v1;http://arxiv.org/pdf/2505.16927v1", "pdf_url": "http://arxiv.org/pdf/2505.16927v1"}]
