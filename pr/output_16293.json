[{"title": "DICE: A Framework for Dimensional and Contextual Evaluation of Language Models", "link": "https://arxiv.org/pdf/2504.10359%3F", "details": "A Shrivastava, PA Aoyagui - arXiv preprint arXiv:2504.10359, 2025", "abstract": "Language models (LMs) are increasingly being integrated into a wide range of applications, yet the modern evaluation paradigm does not sufficiently reflect how they are actually being used. Current evaluations rely on benchmarks that often lack \u2026"}, {"title": "Atoxia: Red-teaming Large Language Models with Target Toxic Answers", "link": "https://aclanthology.org/2025.findings-naacl.179.pdf", "details": "Y Du, Z Li, P Cheng, X Wan, A Gao - Findings of the Association for Computational \u2026, 2025", "abstract": "Despite the substantial advancements in artificial intelligence, large language models (LLMs) remain being challenged by generation safety. With adversarial jailbreaking prompts, one can effortlessly induce LLMs to output harmful content \u2026"}, {"title": "Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction", "link": "https://arxiv.org/pdf/2505.00814", "details": "M S\u00e4nger, U Leser - arXiv preprint arXiv:2505.00814, 2025", "abstract": "Automatic relationship extraction (RE) from biomedical literature is critical for managing the vast amount of scientific knowledge produced each year. In recent years, utilizing pre-trained language models (PLMs) has become the prevalent \u2026"}, {"title": "RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning", "link": "https://arxiv.org/pdf/2505.03238", "details": "L Boyle, N Baumann, P Sivasothilingam, M Magno\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Future robotic systems operating in real-world environments will require on-board embodied intelligence without continuous cloud connection, balancing capabilities with constraints on computational power and memory. This work presents an \u2026"}, {"title": "Biases in Opinion Dynamics in Multi-Agent Systems of Large Language Models: A Case Study on Funding Allocation", "link": "https://aclanthology.org/2025.findings-naacl.101.pdf", "details": "P Cisneros-Velarde - Findings of the Association for Computational \u2026, 2025", "abstract": "We study the evolution of opinions inside a population of interacting large language models (LLMs). Every LLM needs to decide how much funding to allocate to an item with three initial possibilities: full, partial, or no funding. We identify biases that drive \u2026"}, {"title": "Analyzing and Improving Coherence of Large Language Models in Question Answering", "link": "https://aclanthology.org/2025.naacl-long.588.pdf", "details": "I Lauriola, S Campese, A Moschitti - Proceedings of the 2025 Conference of the \u2026, 2025", "abstract": "Large language models (LLMs) have recently revolutionized natural language processing. These models, however, often suffer from instability or lack of coherence, that is the ability of the models to generate semantically equivalent outputs when \u2026"}, {"title": "Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2504.21277", "details": "G Zhou, P Qiu, C Chen, J Wang, Z Yang, J Xu, M Qiu - arXiv preprint arXiv \u2026, 2025", "abstract": "The integration of reinforcement learning (RL) into the reasoning capabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as a transformative research direction. While MLLMs significantly extend Large Language \u2026"}, {"title": "MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance", "link": "https://arxiv.org/pdf/2505.03804", "details": "X Hu, Z Chen, D Yang, Z Xu, C Xu, Z Yuan, S Zhou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Mixture-of-Experts (MoE) large language models (LLMs), which leverage dynamic routing and sparse activation to enhance efficiency and scalability, have achieved higher performance while reducing computational costs. However, these models \u2026"}, {"title": "Forest for the Trees: Overarching Prompting Evokes High-Level Reasoning in Large Language Models", "link": "https://aclanthology.org/2025.naacl-long.66.pdf", "details": "H Liao, S Hu, Z Zhu, H He, Y Jin - Proceedings of the 2025 Conference of the Nations \u2026, 2025", "abstract": "Abstract Chain-of-thought (CoT) and subsequent methods adopted a deductive paradigm that decomposes the reasoning process, demonstrating remarkable performances across NLP tasks. However, such a paradigm faces the challenge of \u2026"}]
