[{"title": "Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More", "link": "https://arxiv.org/pdf/2502.11494", "details": "Z Wen, Y Gao, S Wang, J Zhang, Q Zhang, W Li, C He\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision tokens in multimodal large language models often dominate huge computational overhead due to their excessive length compared to linguistic modality. Abundant recent methods aim to solve this problem with token pruning \u2026"}, {"title": "Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge", "link": "https://arxiv.org/pdf/2503.04036", "details": "X Cui, JTZ Wei, S Swayamdipta, R Jia - arXiv preprint arXiv:2503.04036, 2025", "abstract": "Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques \u2026"}, {"title": "Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding", "link": "https://arxiv.org/pdf/2502.09906", "details": "TD Truong, HQ Nguyen, XB Nguyen, A Dowling, X Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual \u2026"}, {"title": "DCFormer: Efficient 3D Vision-Language Modeling with Decomposed Convolutions", "link": "https://arxiv.org/pdf/2502.05091%3F", "details": "GC Ates, K Gong, W Shao - arXiv preprint arXiv:2502.05091, 2025", "abstract": "Vision-language models (VLMs) align visual and textual representations, enabling high-performance zero-shot classification and image-text retrieval in 2D medical imaging. However, extending VLMs to 3D medical imaging remains computationally \u2026"}, {"title": "Hierarchical Vision\u2013Language Pre-Training with Freezing Strategy for Multi-Level Semantic Alignment", "link": "https://www.mdpi.com/2079-9292/14/4/816", "details": "H Xie, Y Qin, S Ding - Electronics, 2025", "abstract": "Vision\u2013language pre-training (VLP) faces challenges in aligning hierarchical textual semantics (words/phrases/sentences) with multi-scale visual features (objects/relations/global context). We propose a hierarchical VLP model (HieVLP) \u2026"}, {"title": "Semantic Alignment of Unimodal Medical Text and Vision Representations", "link": "https://arxiv.org/pdf/2503.04478", "details": "M Di Folco, E Chan, M Hasny, CI Bercea, JA Schnabel - arXiv preprint arXiv \u2026, 2025", "abstract": "General-purpose AI models, particularly those designed for text and vision, demonstrate impressive versatility across a wide range of deep-learning tasks. However, they often underperform in specialised domains like medical imaging \u2026"}, {"title": "Towards a holistic framework for multimodal LLM in 3D brain CT radiology report generation", "link": "https://www.nature.com/articles/s41467-025-57426-0", "details": "CY Li, KJ Chang, CF Yang, HY Wu, W Chen, H Bansal\u2026 - Nature Communications, 2025", "abstract": "Multi-modal large language models (MLLMs) have transformed the landscape of modern healthcare, with automated radiology report generation (RRG) emerging as a cutting-edge application. While 2D MLLM-based RRG has been well established \u2026"}, {"title": "Multilingual Language Model Pretraining using Machine-translated Data", "link": "https://arxiv.org/pdf/2502.13252", "details": "J Wang, Y Lu, M Weber, M Ryabinin, D Adelani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-resource languages such as English, enables the pretraining of high-quality large language models (LLMs). The same can not be said for most other languages as LLMs still underperform for non-English languages, likely due to a gap in the \u2026"}, {"title": "AIDE: Agentically Improve Visual Language Model with Domain Experts", "link": "https://arxiv.org/pdf/2502.09051", "details": "MC Chiu, F Liu, K Sapra, A Tao, Y Jacoob, X Ma, Z Yu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The enhancement of Visual Language Models (VLMs) has traditionally relied on knowledge distillation from larger, more capable models. This dependence creates a fundamental bottleneck for improving state-of-the-art systems, particularly when no \u2026"}]
