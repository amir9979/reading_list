[{"title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines", "link": "https://arxiv.org/pdf/2410.21220", "details": "Z Zhang, Y Zhang, X Ding, X Yue - arXiv preprint arXiv:2410.21220, 2024", "abstract": "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This \u2026"}, {"title": "No Need to Talk: Asynchronous Mixture of Language Models", "link": "https://arxiv.org/pdf/2410.03529%3F", "details": "A Filippova, A Katharopoulos, D Grangier, R Collobert - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce SmallTalk LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each model of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth \u2026"}, {"title": "When Attention Sink Emerges in Language Models: An Empirical View", "link": "https://arxiv.org/pdf/2410.10781", "details": "X Gu, T Pang, C Du, Q Liu, F Zhang, C Du, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as attention sink. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV \u2026"}, {"title": "A Survey of Small Language Models", "link": "https://arxiv.org/pdf/2410.20011", "details": "C Van Nguyen, X Shen, R Aponte, Y Xia, S Basu, Z Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Small Language Models (SLMs) have become increasingly important due to their efficiency and performance to perform various language tasks with minimal computational resources, making them ideal for various settings including on-device \u2026"}, {"title": "Scaling Parameter-Constrained Language Models with Quality Data", "link": "https://arxiv.org/pdf/2410.03083", "details": "E Chang, M Paltenghi, Y Li, PJ Lin, C Zhao, P Huber\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws in language modeling traditionally quantify training loss as a function of dataset size and model parameters, providing compute-optimal estimates but often neglecting the impact of data quality on model generalization. In this paper, we \u2026"}, {"title": "Ascle\u2014A Python Natural Language Processing Toolkit for Medical Text Generation: Development and Evaluation Study", "link": "https://www.jmir.org/2024/1/e60601/", "details": "R Yang, Q Zeng, K You, Y Qiao, L Huang, CC Hsieh\u2026 - Journal of Medical Internet \u2026, 2024", "abstract": "Background Medical texts present significant domain-specific challenges, and manually curating these texts is a time-consuming and labor-intensive process. To address this, natural language processing (NLP) algorithms have been developed to \u2026"}, {"title": "Contrastive random lead coding for channel-agnostic self-supervision of biosignals", "link": "https://arxiv.org/pdf/2410.19842", "details": "T Br\u00fcsch, MN Schmidt, TS Alstr\u00f8m - arXiv preprint arXiv:2410.19842, 2024", "abstract": "Contrastive learning yields impressive results for self-supervision in computer vision. The approach relies on the creation of positive pairs, something which is often achieved through augmentations. However, for multivariate time series effective \u2026"}, {"title": "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "link": "https://arxiv.org/pdf/2410.03122", "details": "Z Zhao, Y Yang, Y Li, Y Cao - arXiv preprint arXiv:2410.03122, 2024", "abstract": "The ripple effect poses a significant challenge in knowledge editing for large language models. Namely, when a single fact is edited, the model struggles to accurately update the related facts in a sequence, which is evaluated by multi-hop \u2026"}, {"title": "Mixture of Experts Made Personalized: Federated Prompt Learning for Vision-Language Models", "link": "https://arxiv.org/pdf/2410.10114", "details": "J Luo, C Chen, S Wu - arXiv preprint arXiv:2410.10114, 2024", "abstract": "Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has demonstrated potent applicability across diverse downstream tasks. This lightweight approach has quickly gained traction from federated learning (FL) researchers who \u2026"}]
