[{"title": "LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs", "link": "https://arxiv.org/pdf/2408.13467", "details": "C Park, J Jiang, F Wang, S Paul, J Tang, S Kim - arXiv preprint arXiv:2408.13467, 2024", "abstract": "The widespread adoption of cloud-based proprietary large language models (LLMs) has introduced significant challenges, including operational dependencies, privacy concerns, and the necessity of continuous internet connectivity. In this work, we \u2026"}, {"title": "MixIR: Mixing Input and Representations for Contrastive Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10636233/", "details": "T Zhao, X Guo, Y Lin, B Du - IEEE Transactions on Neural Networks and Learning \u2026, 2024", "abstract": "Recently, contrastive learning has shown significant progress in learning visual representations from unlabeled data. The core idea is training the backbone to be invariant to different augmentations of an instance. While most methods only \u2026"}, {"title": "SelfCP: Compressing over-limit prompt via the frozen large language model itself", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324002322", "details": "J Gao, Z Cao, W Li - Information Processing & Management, 2024", "abstract": "Long prompt leads to huge hardware costs when using transformer-based Large Language Models (LLMs). Unfortunately, many tasks, such as summarization, inevitably introduce long documents, and the wide application of in-context learning \u2026"}, {"title": "Contrastive Learning with Synthetic Positives", "link": "https://arxiv.org/pdf/2408.16965", "details": "D Zeng, Y Wu, X Hu, X Xu, Y Shi - arXiv preprint arXiv:2408.16965, 2024", "abstract": "Contrastive learning with the nearest neighbor has proved to be one of the most efficient self-supervised learning (SSL) techniques by utilizing the similarity of multiple instances within the same class. However, its efficacy is constrained as the \u2026"}, {"title": "Interpretable Biomedical Reasoning via Deep Fusion of Knowledge Graph and Pre-trained Language Models", "link": "https://search.proquest.com/openview/2974f00898e8421191bcce2b279deef7/1%3Fpq-origsite%3Dgscholar%26cbl%3D2048897", "details": "X Yinxin, Y Zongbao, L Yuchen, H Jinlong, D Shoubin - Beijing Da Xue Xue Bao, 2024", "abstract": "Joint inference based on pre-trained language model (LM) and knowledge graph (KG) has not achieved better results in the biomedical domain due to its diverse terminology representation, semantic ambiguity and the presence of large amount of \u2026"}, {"title": "When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?", "link": "https://arxiv.org/pdf/2408.11854", "details": "Y Gao, S Myers, S Chen, D Dligach, TA Miller\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The introduction of Large Language Models (LLMs) has advanced data representation and analysis, bringing significant progress in their use for medical questions and answering. Despite these advancements, integrating tabular data \u2026"}, {"title": "Multimodal Cross-lingual Summarization for Videos: A Revisit in Knowledge Distillation Induced Triple-stage Training Method", "link": "https://ieeexplore.ieee.org/abstract/document/10643687/", "details": "N Liu, K Wei, Y Yang, J Tao, X Sun, F Yao, H Yu, L Jin\u2026 - IEEE Transactions on \u2026, 2024", "abstract": "Multimodal summarization (MS) for videos aims to generate summaries from multi- source information (eg, video and text transcript), and this technique has made promising progress recently. However, existing works are limited to monolingual \u2026"}, {"title": "LM-PUB-QUIZ: A Comprehensive Framework for Zero-Shot Evaluation of Relational Knowledge in Language Models", "link": "https://arxiv.org/pdf/2408.15729", "details": "M Ploner, J Wiland, S Pohl, A Akbik - arXiv preprint arXiv:2408.15729, 2024", "abstract": "Knowledge probing evaluates the extent to which a language model (LM) has acquired relational knowledge during its pre-training phase. It provides a cost- effective means of comparing LMs of different sizes and training setups and is useful \u2026"}, {"title": "Generalized knowledge-enhanced framework for biomedical entity and relation extraction", "link": "https://arxiv.org/pdf/2408.06618", "details": "M Nguyen, P Le - arXiv preprint arXiv:2408.06618, 2024", "abstract": "In recent years, there has been an increasing number of frameworks developed for biomedical entity and relation extraction. This research effort aims to address the accelerating growth in biomedical publications and the intricate nature of biomedical \u2026"}]
