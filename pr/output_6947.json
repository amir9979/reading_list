[{"title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks", "link": "https://arxiv.org/pdf/2409.07353", "details": "MZ Hossain, A Imteaj - arXiv preprint arXiv:2409.07353, 2024", "abstract": "Large Vision-Language Models (LVLMs), trained on multimodal big datasets, have significantly advanced AI by excelling in vision-language tasks. However, these models remain vulnerable to adversarial attacks, particularly jailbreak attacks, which \u2026"}, {"title": "Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific Expert Pruning", "link": "https://arxiv.org/pdf/2409.01483", "details": "S Sarkar, L Lausen, V Cevher, S Zha, T Brox, G Karypis - arXiv preprint arXiv \u2026, 2024", "abstract": "Sparse Mixture of Expert (SMoE) models have emerged as a scalable alternative to dense models in language modeling. These models use conditionally activated feedforward subnetworks in transformer blocks, allowing for a separation between \u2026"}, {"title": "Pushing the Limits of Vision-Language Models in Remote Sensing without Human Annotations", "link": "https://arxiv.org/pdf/2409.07048", "details": "K Cha, D Yu, J Seo - arXiv preprint arXiv:2409.07048, 2024", "abstract": "The prominence of generalized foundation models in vision-language integration has witnessed a surge, given their multifarious applications. Within the natural domain, the procurement of vision-language datasets to construct these foundation \u2026"}, {"title": "Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language Models", "link": "https://arxiv.org/pdf/2409.01584", "details": "S Ozaki, K Hayashi, Y Sakai, H Kamigaito, K Hayashi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As the performance of Large-scale Vision Language Models (LVLMs) improves, they are increasingly capable of responding in multiple languages, and there is an expectation that the demand for explanations generated by LVLMs will grow \u2026"}, {"title": "In Defense of RAG in the Era of Long-Context Language Models", "link": "https://arxiv.org/pdf/2409.01666", "details": "T Yu, A Xu, R Akkiraju - arXiv preprint arXiv:2409.01666, 2024", "abstract": "Overcoming the limited context limitations in early-generation LLMs, retrieval- augmented generation (RAG) has been a reliable solution for context-based answer generation in the past. Recently, the emergence of long-context LLMs allows the \u2026"}, {"title": "On the Relationship between Truth and Political Bias in Language Models", "link": "https://arxiv.org/pdf/2409.05283", "details": "S Fulay, W Brannon, S Mohanty, C Overney\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language model alignment research often attempts to ensure that models are not only helpful and harmless, but also truthful and unbiased. However, optimizing these objectives simultaneously can obscure how improving one aspect might impact the \u2026"}, {"title": "PIP: Detecting Adversarial Examples in Large Vision-Language Models via Attention Patterns of Irrelevant Probe Questions", "link": "https://arxiv.org/pdf/2409.05076", "details": "Y Zhang, R Xie, J Chen, X Sun, Y Wang - arXiv preprint arXiv:2409.05076, 2024", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated their powerful multimodal capabilities. However, they also face serious safety problems, as adversaries can induce robustness issues in LVLMs through the use of well \u2026"}, {"title": "Factual and Tailored Recommendation Endorsements using Language Models and Reinforcement Learning", "link": "https://openreview.net/pdf%3Fid%3DxI8C7sfN1H", "details": "J Jeong, Y Chow, G Tennenholtz, CW Hsu\u2026 - First Conference on Language \u2026", "abstract": "Recommender systems (RSs) play a central role in matching candidate items to users based on their preferences. While traditional RSs rely on user feed-back signals, conversational RSs interact with users in natural language. In this work, we \u2026"}, {"title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding", "link": "https://arxiv.org/pdf/2409.03363", "details": "C Wang, Y Wang, B Hooi, Y Cai, N Peng, KW Chang - arXiv preprint arXiv \u2026, 2024", "abstract": "The training data in large language models is key to their success, but it also presents privacy and security risks, as it may contain sensitive information. Detecting pre-training data is crucial for mitigating these concerns. Existing methods typically \u2026"}]
