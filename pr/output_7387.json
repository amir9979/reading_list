[{"title": "Mitigating Hallucination in Visual-Language Models via Re-Balancing Contrastive Decoding", "link": "https://arxiv.org/pdf/2409.06485", "details": "X Liang, J Yu, L Mu, J Zhuang, J Hu, Y Yang, J Ye, L Lu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Although Visual-Language Models (VLMs) have shown impressive capabilities in tasks like visual question answering and image captioning, they still struggle with hallucinations. Analysis of attention distribution in these models shows that VLMs \u2026"}, {"title": "Top-down Activity Representation Learning for Video Question Answering", "link": "https://arxiv.org/pdf/2409.07748", "details": "Y Wang, S Haruta, D Zeng, J Vizcarra, M Kurokawa - arXiv preprint arXiv:2409.07748, 2024", "abstract": "Capturing complex hierarchical human activities, from atomic actions (eg, picking up one present, moving to the sofa, unwrapping the present) to contextual events (eg, celebrating Christmas) is crucial for achieving high-performance video question \u2026"}, {"title": "Intelligent maritime question-answering and recommendation system based on maritime vessel activity knowledge graph", "link": "https://www.sciencedirect.com/science/article/pii/S0029801824024533", "details": "C Xie, Z Zhong, L Zhang - Ocean Engineering, 2024", "abstract": "Traditional maritime traffic management typically relies on positioning data for data mining without incorporating other multi-source data to analyze the maritime vessel activity, which cannot conduct comprehensive maritime knowledge mining. Thus, this \u2026"}, {"title": "FedEAN: Entity-Aware Adversarial Negative Sampling for Federated Knowledge Graph Reasoning", "link": "https://ieeexplore.ieee.org/abstract/document/10694741/", "details": "L Meng, K Liang, H Yu, Y Liu, S Zhou, M Liu, X Liu - IEEE Transactions on Knowledge \u2026, 2024", "abstract": "Federated knowledge graph reasoning (FedKGR) aims to perform reasoning over different clients while protecting data privacy, drawing increasing attention to its high practical value. Previous works primarily focus on data heterogeneity, ignoring \u2026"}, {"title": "Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types", "link": "https://arxiv.org/pdf/2409.09269", "details": "N Sinha, V Jain, A Chadha - arXiv preprint arXiv:2409.09269, 2024", "abstract": "Visual Question-Answering (VQA) has become a key use-case in several applications to aid user experience, particularly after Vision-Language Models (VLMs) achieving good results in zero-shot inference. But evaluating different VLMs \u2026"}, {"title": "Parameter Efficiency, Few-Shot, Zero-Shot, Prompting", "link": "https://jonmay.github.io/USC-CS662/assets/files/llm.pdf", "details": "J May - 2024", "abstract": "The models we've discussed so far follow the paradigm that, out of the box, they don't do too much, but when you expose them to some supervised data that is an exemplar of a task and fine-tune their parameters they can do the task when given \u2026"}, {"title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks", "link": "https://arxiv.org/pdf/2409.06173", "details": "G Chochlakis, NM Pandiyan, K Lerman, S Narayanan - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the dominant technique for performing natural language tasks, as it does not require updating the model parameters with gradient-based methods. ICL promises to\" \u2026"}, {"title": "Exploring Hint Generation Approaches in Open-Domain Question Answering", "link": "https://arxiv.org/pdf/2409.16096", "details": "J Mozafari, A Abdallah, B Piryani, A Jatowt - arXiv preprint arXiv:2409.16096, 2024", "abstract": "Automatic Question Answering (QA) systems rely on contextual information to provide accurate answers. Commonly, contexts are prepared through either retrieval- based or generation-based methods. The former involves retrieving relevant \u2026"}, {"title": "Vision-Language Dual-Pattern Matching for Out-of-Distribution Detection", "link": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11399.pdf", "details": "Z Zhang, Z Xu, X Xiang", "abstract": "Out-of-distribution (OOD) detection is a significant challenge in deploying pattern recognition and machine learning models, as models often fail on data from novel distributions. Recent visionlanguage models (VLMs) such as CLIP have shown \u2026"}]
