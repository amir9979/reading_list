'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Prometheus 2: An Open Source Language Model Specialize'
[{"title": "StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation", "link": "https://arxiv.org/pdf/2405.01434", "details": "Y Zhou, D Zhou, MM Cheng, J Feng, Q Hou - arXiv preprint arXiv:2405.01434, 2024", "abstract": "For recent diffusion-based generative models, maintaining consistent content across a series of generated images, especially those containing subjects and complex details, presents a significant challenge. In this paper, we propose a new way of self \u2026"}, {"title": "DynaMo: Accelerating Language Model Inference with Dynamic Multi-Token Sampling", "link": "https://arxiv.org/pdf/2405.00888", "details": "S Tuli, CH Lin, YC Hsu, NK Jha, Y Shen, H Jin - arXiv preprint arXiv:2405.00888, 2024", "abstract": "Traditional language models operate autoregressively, ie, they predict one token at a time. Rapid explosion in model sizes has resulted in high inference times. In this work, we propose DynaMo, a suite of multi-token prediction language models that \u2026"}, {"title": "Decoding Medical Jargon: The Use of AI Language Models (ChatGPT-4, BARD, Microsoft Copilot) in Radiology Reports", "link": "https://www.sciencedirect.com/science/article/pii/S0738399124001745", "details": "M Tepe, E Emekli - Patient Education and Counseling, 2024", "abstract": "Abstract Objective Evaluate Artificial Intelligence (AI) language models (ChatGPT-4, BARD, Microsoft Copilot) in simplifying radiology reports, assessing readability, understandability, actionability, and urgency classification. Methods This study \u2026"}, {"title": "Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think", "link": "https://arxiv.org/pdf/2404.08382", "details": "X Wang, C Hu, B Ma, P R\u00f6ttger, B Plank - arXiv preprint arXiv:2404.08382, 2024", "abstract": "Multiple choice questions (MCQs) are commonly used to evaluate the capabilities of large language models (LLMs). One common way to evaluate the model response is to rank the candidate answers based on the log probability of the first token \u2026"}, {"title": "Improving the spatial resolution of solar images using super-resolution diffusion generative adversarial networks", "link": "https://www.aanda.org/articles/aa/pdf/forth/aa49100-23.pdf", "details": "W Song, Y Ma, H Sun, X Zhao, G Lin - 2024", "abstract": "Context. High-spatial-resolution solar images contribute to the study of small-scale structures on the Sun. The Helioseismic and Magnetic Imager (HMI) conducts continuous full-disk observations of the Sun at a fixed cadence, accumulating a \u2026"}, {"title": "Hallucinating for Diagnosing: One-Shot Medical Image Classification Leveraging Score-Based Generative Models", "link": "https://openreview.net/pdf%3Fid%3Dy5onNeda4Y", "details": "E Pachetti, S Colantonio - Medical Imaging with Deep Learning, 2024", "abstract": "Deep learning models in data-scarce domains, such as medical imaging, often suffer from poor performance due to the challenges of acquiring large amounts of labeled data. Few-shot learning offers a promising solution to this problem. This work \u2026"}, {"title": "CCSUM: A large-scale and high-quality dataset for abstractive news summarization", "link": "https://www.amazon.science/publications/ccsum-a-large-scale-and-high-quality-dataset-for-abstractive-news-summarization", "details": "X Jiang, M Dreyer - 2024", "abstract": "Training a supervised news summarization model requires large amounts of high- quality training data consisting of news articles paired with reference summaries. However, obtaining such data is costly, and existing datasets contain considerable \u2026"}, {"title": "Pre-training Small Base LMs with Fewer Tokens", "link": "https://arxiv.org/pdf/2404.08634", "details": "S Sanyal, S Sanghavi, AG Dimakis - arXiv preprint arXiv:2404.08634, 2024", "abstract": "We study the effectiveness of a simple approach to develop a small base language model (LM) starting from an existing large base LM: first inherit a few transformer blocks from the larger LM, and then train this smaller model on a very small subset \u2026"}, {"title": "Markovian Agents for Truthful Language Modeling", "link": "https://arxiv.org/pdf/2404.18988", "details": "S Viteri, M Lamparth, P Chatain, C Barrett - arXiv preprint arXiv:2404.18988, 2024", "abstract": "Chain-of-Thought (CoT) reasoning could in principle enable a deeper understanding of a language model's (LM) internal reasoning. However, prior work suggests that some LMs answer questions similarly despite changes in their CoT, suggesting that \u2026"}]
