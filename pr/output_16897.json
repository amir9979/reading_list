[{"title": "Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models", "link": "https://arxiv.org/pdf/2505.00150", "details": "MH Van, X Wu - arXiv preprint arXiv:2505.00150, 2025", "abstract": "The rapid evolution of social media has provided enhanced communication channels for individuals to create online content, enabling them to express their thoughts and opinions. Multimodal memes, often utilized for playful or humorous \u2026", "entry_id": "http://arxiv.org/abs/2505.00150v1", "updated": "2025-04-30 19:48:12", "published": "2025-04-30 19:48:12", "authors": "Minh-Hao Van;Xintao Wu", "summary": "The rapid evolution of social media has provided enhanced communication\nchannels for individuals to create online content, enabling them to express\ntheir thoughts and opinions. Multimodal memes, often utilized for playful or\nhumorous expressions with visual and textual elements, are sometimes misused to\ndisseminate hate speech against individuals or groups. While the detection of\nhateful memes is well-researched, developing effective methods to transform\nhateful content in memes remains a significant challenge. Leveraging the\npowerful generation and reasoning capabilities of Vision-Language Models\n(VLMs), we address the tasks of detecting and mitigating hateful content. This\npaper presents two key contributions: first, a definition-guided prompting\ntechnique for detecting hateful memes, and second, a unified framework for\nmitigating hateful content in memes, named UnHateMeme, which works by replacing\nhateful textual and/or visual components. With our definition-guided prompts,\nVLMs achieve impressive performance on hateful memes detection task.\nFurthermore, our UnHateMeme framework, integrated with VLMs, demonstrates a\nstrong capability to convert hateful memes into non-hateful forms that meet\nhuman-level criteria for hate speech and maintain multimodal coherence between\nimage and text. Through empirical experiments, we show the effectiveness of\nstate-of-the-art pretrained VLMs such as LLaVA, Gemini and GPT-4o on the\nproposed tasks, providing a comprehensive analysis of their respective\nstrengths and limitations for these tasks. This paper aims to shed light on\nimportant applications of VLMs for ensuring safe and respectful online\nenvironments.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI;cs.CL", "links": "http://arxiv.org/abs/2505.00150v1;http://arxiv.org/pdf/2505.00150v1", "pdf_url": "http://arxiv.org/pdf/2505.00150v1"}, {"title": "Function Calling in Large Language Models: Industrial Practices, Challenges, and Future Directions", "link": "https://openreview.net/pdf%3Fid%3DLNxVGPedFW", "details": "M WANG, Y ZHANG, C PENG, Y CHEN, WEI ZHOU\u2026 - 2025", "abstract": "1 INTRODUCTION Recent advancements in artificial intelligence have ushered in a transformative era with the development of large language models (LLMs) such as GPT series, LLama [164], ChatGLM [33, 212] and Qwen [10]. These models \u2026"}, {"title": "MIPS: Multilingual Inference Paradigm in Zero-Shot Cross-Lingual Stance Detection with Contrastive Learning", "link": "https://link.springer.com/chapter/10.1007/978-981-96-5123-8_4", "details": "Y Wang, S Zhang, H Zhang, Y Shu, Y Zhang, L Yang\u2026 - International Conference on \u2026, 2025", "abstract": "Stance detection aims to determine the stance of a text towards a specific target. Most research focuses on a single language with limited targets. Zero-shot cross- lingual stance detection addresses linguistic resource imbalances by training models \u2026"}, {"title": "Enhancing Zero-Shot Knowledge Graph Relation Prediction through Large Language Models and Contrastive Learning", "link": "https://dl.acm.org/doi/abs/10.1145/3701716.3715553", "details": "H Yu, H Yu - Companion Proceedings of the ACM on Web \u2026, 2025", "abstract": "Knowledge graph completion (KGC) aims to predict missing facts in a knowledge graph (KG), with zero-shot relation prediction remaining a significant challenge due to limited descriptive information. In this paper, we propose ROCKGC, a framework \u2026"}]
