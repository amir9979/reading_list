[{"title": "Optimizing Fine-Tuning in Quantized Language Models: An In-Depth Analysis of Key Variables", "link": "https://www.sciencedirect.com/org/science/article/pii/S1546221825000645", "details": "A Shen, Z Lai, D Li, X Hu - Computers, Materials and Continua, 2025", "abstract": "Large-scale Language Models (LLMs) have achieved significant breakthroughs in Natural Language Processing (NLP), driven by the pre-training and fine-tuning paradigm. While this approach allows models to specialize in specific tasks with \u2026"}, {"title": "Foundations of Large Language Models", "link": "https://arxiv.org/pdf/2501.09223", "details": "T Xiao, J Zhu - arXiv preprint arXiv:2501.09223, 2025", "abstract": "This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting- edge technologies. The book is structured into four main chapters, each exploring a \u2026"}, {"title": "Neural codec language models are zero-shot text to speech synthesizers", "link": "https://ieeexplore.ieee.org/abstract/document/10842513/", "details": "S Chen, C Wang, Y Wu, Z Zhang, L Zhou, S Liu\u2026 - IEEE Transactions on Audio \u2026, 2025", "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a \u2026"}, {"title": "On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena", "link": "https://arxiv.org/pdf/2501.04662", "details": "T Naous, W Xu - arXiv preprint arXiv:2501.04662, 2025", "abstract": "Language Models (LMs) have been shown to exhibit a strong preference towards entities associated with Western culture when operating in non-Western languages. In this paper, we aim to uncover the origins of entity-related cultural biases in LMs by \u2026"}, {"title": "Predicate Invention from Pixels via Pretrained Vision-Language Models", "link": "https://arxiv.org/pdf/2501.00296", "details": "A Athalye, N Kumar, T Silver, Y Liang, T Lozano-P\u00e9rez\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Our aim is to learn to solve long-horizon decision-making problems in highly- variable, combinatorially-complex robotics domains given raw sensor input in the form of images. Previous work has shown that one way to achieve this aim is to learn \u2026"}, {"title": "DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision Language Models in Real-World Scenarios with Driving Theory Tests", "link": "https://arxiv.org/pdf/2501.04671", "details": "C Corbi\u00e8re, S Roburin, S Montariol, A Bosselut, A Alahi - arXiv preprint arXiv \u2026, 2025", "abstract": "Large vision-language models (LVLMs) augment language models with visual understanding, enabling multimodal reasoning. However, due to the modality gap between textual and visual data, they often face significant challenges, such as over \u2026"}, {"title": "Adaptive Few-shot Prompting for Machine Translation with Pre-trained Language Models", "link": "https://arxiv.org/pdf/2501.01679%3F", "details": "L Tang, J Qin, W Ye, H Tan, Z Yang - arXiv preprint arXiv:2501.01679, 2025", "abstract": "Recently, Large language models (LLMs) with in-context learning have demonstrated remarkable potential in handling neural machine translation. However, existing evidence shows that LLMs are prompt-sensitive and it is sub-optimal to \u2026"}, {"title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models", "link": "https://arxiv.org/pdf/2501.02955", "details": "W Hong, Y Cheng, Z Yang, W Wang, L Wang, X Gu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In recent years, vision language models (VLMs) have made significant advancements in video understanding. However, a crucial capability-fine-grained motion comprehension-remains under-explored in current benchmarks. To address \u2026"}, {"title": "Eve: Efficient Multimodal Vision Language Models with Elastic Visual Experts", "link": "https://arxiv.org/pdf/2501.04322", "details": "M Rang, Z Bi, C Liu, Y Tang, K Han, Y Wang - arXiv preprint arXiv:2501.04322, 2025", "abstract": "Multimodal vision language models (VLMs) have made significant progress with the support of continuously increasing model sizes and data volumes. Running VLMs on edge devices has become a challenge for their widespread application. There are \u2026"}]
