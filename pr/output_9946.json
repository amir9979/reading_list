[{"title": "Efficient Federated Unlearning with Adaptive Differential Privacy Preservation", "link": "https://arxiv.org/pdf/2411.11044", "details": "Y Jiang, X Tong, Z Liu, H Ye, CW Tan, KY Lam - arXiv preprint arXiv:2411.11044, 2024", "abstract": "Federated unlearning (FU) offers a promising solution to effectively address the need to erase the impact of specific clients' data on the global model in federated learning (FL), thereby granting individuals the``Right to be Forgotten\". The most \u2026"}, {"title": "OPT-IML: Instruction Meta-Learning for Zero-Shot and Few-Shot Generalization", "link": "https://www.ijmrset.com/upload/67_Text.pdf", "details": "S Iyer, J Thorne, S Chen - Proceedings of the 61st Annual Meeting of the \u2026, 2024", "abstract": "Accessing and extracting insights from databases is often a challenge for non- technical users unfamiliar with Structured Query Language (SQL). This project proposes a novel solution by developing a system that converts plain language into \u2026"}, {"title": "Evaluating Language Models as Synthetic Data Generators", "link": "https://arxiv.org/pdf/2412.03679", "details": "S Kim, J Suk, X Yue, V Viswanathan, S Lee, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data \u2026"}, {"title": "Balrog: Benchmarking agentic llm and vlm reasoning on games", "link": "https://arxiv.org/pdf/2411.13543", "details": "D Paglieri, B Cupia\u0142, S Coward, U Piterbarg, M Wolczyk\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities; however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require \u2026"}]
