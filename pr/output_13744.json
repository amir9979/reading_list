[{"title": "FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge", "link": "https://arxiv.org/pdf/2502.19207", "details": "N Yang, M Kim, S Yoon, J Shin, K Jung - arXiv preprint arXiv:2502.19207, 2025", "abstract": "Various studies have attempted to remove sensitive or private knowledge from a language model to prevent its unauthorized exposure. However, prior studies have overlooked the complex and interconnected nature of knowledge, where related \u2026"}, {"title": "Tabby: Tabular Data Synthesis with Language Models", "link": "https://arxiv.org/pdf/2503.02152", "details": "S Cromp, SSSN GNVV, M Alkhudhayri, C Cao, S Guo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While advances in large language models (LLMs) have greatly improved the quality of synthetic text data in recent years, synthesizing tabular data has received relatively less attention. We address this disparity with Tabby, a simple but powerful post \u2026"}, {"title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "link": "https://arxiv.org/pdf/2502.21321", "details": "K Kumar, T Ashraf, O Thawakar, RM Anwer\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have transformed the natural language processing landscape and brought to life diverse applications. Pretraining on vast web-scale data has laid the foundation for these models, yet the research community is now \u2026"}, {"title": "KG-prompt: Interpretable knowledge graph prompt for pre-trained language models", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125001650", "details": "L Chen, J Liu, Y Duan, R Wang - Knowledge-Based Systems, 2025", "abstract": "Abstract Knowledge graphs (KGs) can provide rich factual knowledge for language models, enhancing reasoning ability and interpretability. However, existing knowledge injection methods usually ignore the structured information in KGs. Using \u2026"}, {"title": "MeMo: Towards Language Models with Associative Memory Mechanisms", "link": "https://arxiv.org/pdf/2502.12851", "details": "FM Zanzotto, ES Ruzzetti, GA Xompero, L Ranaldi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Memorization is a fundamental ability of Transformer-based Large Language Models, achieved through learning. In this paper, we propose a paradigm shift by designing an architecture to memorize text directly, bearing in mind the principle that \u2026"}, {"title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information", "link": "https://arxiv.org/pdf/2502.14258", "details": "Y Park, C Yoon, J Park, M Jeong, J Kang - arXiv preprint arXiv:2502.14258, 2025", "abstract": "While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads primarily responsible for processing \u2026"}, {"title": "Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models", "link": "https://arxiv.org/pdf/2502.15639", "details": "A Sundar, S Williamson, K Metcalf, BJ Theobald\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally \u2026"}, {"title": "Compressing Language Models for Specialized Domains", "link": "https://arxiv.org/pdf/2502.18424", "details": "M Williams, G Chrysostomou, V Jeronymo, N Aletras - arXiv preprint arXiv \u2026, 2025", "abstract": "Compression techniques such as pruning and quantization offer a solution for more efficient deployment of language models (LMs), albeit with small performance drops in benchmark performance. However, general-purpose LM compression methods \u2026"}, {"title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems", "link": "https://arxiv.org/pdf/2503.03686", "details": "R Ye, S Tang, R Ge, Y Du, Z Yin, S Chen, J Shao - arXiv preprint arXiv:2503.03686, 2025", "abstract": "LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability \u2026"}]
