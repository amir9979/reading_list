[{"title": "WHODUNIT: Evaluation benchmark for culprit detection in mystery stories", "link": "https://arxiv.org/pdf/2502.07747", "details": "K Gupta - arXiv preprint arXiv:2502.07747, 2025", "abstract": "We present a novel data set, WhoDunIt, to assess the deductive reasoning capabilities of large language models (LLM) within narrative contexts. Constructed from open domain mystery novels and short stories, the dataset challenges LLMs to \u2026"}, {"title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning", "link": "https://arxiv.org/pdf/2501.18858", "details": "H Zhong, Y Yin, S Zhang, X Xu, Y Liu, Y Zuo, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, yet generating reliable reasoning processes remains a significant challenge. We present a unified probabilistic framework that formalizes \u2026"}, {"title": "JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation", "link": "https://arxiv.org/pdf/2502.07557", "details": "S Zhang, Y Zhai, K Guo, H Hu, S Guo, Z Fang, L Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite the implementation of safety alignment strategies, large language models (LLMs) remain vulnerable to jailbreak attacks, which undermine these safety guardrails and pose significant security threats. Some defenses have been proposed \u2026"}, {"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models", "link": "https://arxiv.org/pdf/2502.03199%3F", "details": "J Wu, Y Shen, S Liu, Y Tang, S Song, X Wang, L Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the \u2026"}, {"title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models", "link": "https://arxiv.org/pdf/2502.07780", "details": "S Tang, O Sieberling, E Kurtic, Z Shen, D Alistarh - arXiv preprint arXiv:2502.07780, 2025", "abstract": "Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by \u2026"}, {"title": "Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models", "link": "https://arxiv.org/pdf/2502.02444", "details": "H Ye, T Zhang, Y Xie, L Zhang, Y Ren, X Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz's Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross \u2026"}, {"title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey", "link": "https://arxiv.org/pdf/2502.06872", "details": "B Ni, Z Liu, L Wang, Y Lei, Y Zhao, X Cheng, Q Zeng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to \u2026"}, {"title": "Comprehend Then Predict: Prompting Large Language Models for Recommendation with Semantic and Collaborative Data", "link": "https://dl.acm.org/doi/abs/10.1145/3716499", "details": "Z Dong, L Hu, J Chen, Z Wang, F Wu - ACM Transactions on Information Systems", "abstract": "Recommender systems primarily utilize user-item interactions (ie, collaborative information) and auxiliary textual information (ie, semantic information) to infer user preferences and provide recommendations. With the advancement in large language \u2026"}, {"title": "Normative Evaluation of Large Language Models with Everyday Moral Dilemmas", "link": "https://arxiv.org/pdf/2501.18081", "details": "PS Sachdeva, T van Nuenen - arXiv preprint arXiv:2501.18081, 2025", "abstract": "The rapid adoption of large language models (LLMs) has spurred extensive research into their encoded moral norms and decision-making processes. Much of this research relies on prompting LLMs with survey-style questions to assess how well \u2026"}]
