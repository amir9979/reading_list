[{"title": "Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement", "link": "https://arxiv.org/pdf/2502.02573", "details": "S Abbasloo - arXiv preprint arXiv:2502.02573, 2025", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem- solving, a crucial, ubiquitous, and complex domain. This paper explores the \u2026"}, {"title": "Advancing Math Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages", "link": "https://arxiv.org/pdf/2501.14002", "details": "Z Chen, T Liu, M Tian, Q Tong, W Luo, Z Liu - arXiv preprint arXiv:2501.14002, 2025", "abstract": "Advancements in LLMs have significantly expanded their capabilities across various domains. However, mathematical reasoning remains a challenging area, prompting the development of math-specific LLMs. These models typically follow a two-stage \u2026"}, {"title": "Leveraging language models for automated distribution of review notes in animated productions", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225002929", "details": "D Garc\u00e9s, M Santos, D Fern\u00e1ndez-Llorca - Neurocomputing, 2025", "abstract": "During the production of an animated film, professionals at the animation studio prepare thousands of notes. These notes describe improvements and corrections identified by supervisors and directors during daily meetings where the film's \u2026"}, {"title": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques", "link": "https://arxiv.org/pdf/2501.14492", "details": "Z Tang, Z Li, Z Xiao, T Ding, R Sun, B Wang, D Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Critiques are important for enhancing the performance of Large Language Models (LLMs), enabling both self-improvement and constructive feedback for others by identifying flaws and suggesting improvements. However, evaluating the critique \u2026"}, {"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models", "link": "https://arxiv.org/pdf/2502.03199", "details": "J Wu, Y Shen, S Liu, Y Tang, S Song, X Wang, L Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the \u2026"}, {"title": "PSSD: Making Large Language Models Self-denial via Human Psyche Structure", "link": "https://arxiv.org/pdf/2502.01344", "details": "J Liao, Z Liao, X Zhao - arXiv preprint arXiv:2502.01344, 2025", "abstract": "The enhance of accuracy in reasoning results of LLMs arouses the community's interests, wherein pioneering studies investigate post-hoc strategies to rectify potential mistakes. Despite extensive efforts, they are all stuck in a state of resource \u2026"}, {"title": "ESCARGOT: an AI agent leveraging large language models, dynamic graph of thoughts, and biomedical knowledge graphs for enhanced reasoning", "link": "https://academic.oup.com/bioinformatics/article/41/2/btaf031/7972741", "details": "N Matsumoto, H Choi, J Moran, ME Hernandez\u2026 - Bioinformatics, 2025", "abstract": "Motivation LLMs like GPT-4, despite their advancements, often produce hallucinations and struggle with integrating external knowledge effectively. While Retrieval-Augmented Generation (RAG) attempts to address this by incorporating \u2026"}, {"title": "Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models", "link": "https://arxiv.org/pdf/2502.02444", "details": "H Ye, T Zhang, Y Xie, L Zhang, Y Ren, X Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz's Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross \u2026"}, {"title": "Assessing and Post-Processing Black Box Large Language Models for Knowledge Editing", "link": "https://openreview.net/pdf%3Fid%3DaGhk1VNcRJ", "details": "X Song, Z Wang, K He, G Dong, Y Mou, J Zhao, W Xu - THE WEB CONFERENCE 2025", "abstract": "The rapid evolution of the Web as a key platform for information dissemination has led to the growing integration of large language models (LLMs) in Web-based applications. However, the swift changes in web content present challenges in \u2026"}]
