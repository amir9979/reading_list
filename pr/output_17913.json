[{"title": "MedErr-CT: A Visual Question Answering Benchmark for Identifying and Correcting Errors in CT Reports", "link": "https://arxiv.org/pdf/2506.19217", "details": "S Kyung, H Park, J Seo, J Sung, J Kim, D Kim, W Jo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 While Multimodal **Large** **Language** **Models** (MLLMs) demonstrate promising comprehension of **medical** knowledge, their tendency to \u2026 However, existing **medical** visual **question** **answering** (VQA) benchmarks primarily focus on simple \u2026", "entry_id": "http://arxiv.org/abs/2506.19217v1", "updated": "2025-06-24 00:51:03", "published": "2025-06-24 00:51:03", "authors": "Sunggu Kyung;Hyungbin Park;Jinyoung Seo;Jimin Sung;Jihyun Kim;Dongyeong Kim;Wooyoung Jo;Yoojin Nam;Sangah Park;Taehee Kwon;Sang Min Lee;Namkug Kim", "summary": "Computed Tomography (CT) plays a crucial role in clinical diagnosis, but the\ngrowing demand for CT examinations has raised concerns about diagnostic errors.\nWhile Multimodal Large Language Models (MLLMs) demonstrate promising\ncomprehension of medical knowledge, their tendency to produce inaccurate\ninformation highlights the need for rigorous validation. However, existing\nmedical visual question answering (VQA) benchmarks primarily focus on simple\nvisual recognition tasks, lacking clinical relevance and failing to assess\nexpert-level knowledge. We introduce MedErr-CT, a novel benchmark for\nevaluating medical MLLMs' ability to identify and correct errors in CT reports\nthrough a VQA framework. The benchmark includes six error categories - four\nvision-centric errors (Omission, Insertion, Direction, Size) and two lexical\nerror types (Unit, Typo) - and is organized into three task levels:\nclassification, detection, and correction. Using this benchmark, we\nquantitatively assess the performance of state-of-the-art 3D medical MLLMs,\nrevealing substantial variation in their capabilities across different error\ntypes. Our benchmark contributes to the development of more reliable and\nclinically applicable MLLMs, ultimately helping reduce diagnostic errors and\nimprove accuracy in clinical practice. The code and datasets are available at\nhttps://github.com/babbu3682/MedErr-CT.", "comment": "14 pages, 5 figures, submitted to CVPR 2025", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2506.19217v1;http://arxiv.org/pdf/2506.19217v1", "pdf_url": "http://arxiv.org/pdf/2506.19217v1"}, {"title": "Surgery-R1: Advancing Surgical-VQLA with Reasoning Multimodal Large Language Model via Reinforcement Learning", "link": "https://arxiv.org/pdf/2506.19469", "details": "P Hao, S Li, H Wang, Z Kou, J Zhang, G Yang, L Zhu - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 development in **clinical** applications. To address this issue, inspired by the development of Reasoning Multimodal **Large** **Language** **Models** (\u2026 datasets, demonstrating its effectiveness and advantages in surgical visual **question** \u2026", "entry_id": "http://arxiv.org/abs/2506.19469v1", "updated": "2025-06-24 09:53:10", "published": "2025-06-24 09:53:10", "authors": "Pengfei Hao;Shuaibo Li;Hongqiu Wang;Zhizhuo Kou;Junhang Zhang;Guang Yang;Lei Zhu", "summary": "In recent years, significant progress has been made in the field of surgical\nscene understanding, particularly in the task of Visual Question\nLocalized-Answering in robotic surgery (Surgical-VQLA). However, existing\nSurgical-VQLA models lack deep reasoning capabilities and interpretability in\nsurgical scenes, which limits their reliability and potential for development\nin clinical applications. To address this issue, inspired by the development of\nReasoning Multimodal Large Language Models (MLLMs), we first build the\nSurgery-R1-54k dataset, including paired data for Visual-QA, Grounding-QA, and\nChain-of-Thought (CoT). Then, we propose the first Reasoning MLLM for\nSurgical-VQLA (Surgery-R1). In our Surgery-R1, we design a two-stage\nfine-tuning mechanism to enable the basic MLLM with complex reasoning abilities\nby utilizing supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT).\nFurthermore, for an efficient and high-quality rule-based reward system in our\nRFT, we design a Multimodal Coherence reward mechanism to mitigate positional\nillusions that may arise in surgical scenarios. Experiment results demonstrate\nthat Surgery-R1 outperforms other existing state-of-the-art (SOTA) models in\nthe Surgical-VQLA task and widely-used MLLMs, while also validating its\nreasoning capabilities and the effectiveness of our approach. The code and\ndataset will be organized in https://github.com/FiFi-HAO467/Surgery-R1.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2506.19469v1;http://arxiv.org/pdf/2506.19469v1", "pdf_url": "http://arxiv.org/pdf/2506.19469v1"}, {"title": "MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration", "link": "https://arxiv.org/pdf/2506.19835", "details": "Y Zhou, L Song, J Shen - arXiv preprint arXiv:2506.19835, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) in multimodal medical diagnosis. Our experiments leverage a diverse collection of publicly available **medical** \u2026 For video-based **medical** **question** **answering** on MedVidQA (Table 6), MAM achieves leading \u2026", "entry_id": "http://arxiv.org/abs/2506.19835v1", "updated": "2025-06-24 17:52:43", "published": "2025-06-24 17:52:43", "authors": "Yucheng Zhou;Lingran Song;Jianbing Shen", "summary": "Recent advancements in medical Large Language Models (LLMs) have showcased\ntheir powerful reasoning and diagnostic capabilities. Despite their success,\ncurrent unified multimodal medical LLMs face limitations in knowledge update\ncosts, comprehensiveness, and flexibility. To address these challenges, we\nintroduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis\n(MAM). Inspired by our empirical findings highlighting the benefits of role\nassignment and diagnostic discernment in LLMs, MAM decomposes the medical\ndiagnostic process into specialized roles: a General Practitioner, Specialist\nTeam, Radiologist, Medical Assistant, and Director, each embodied by an\nLLM-based agent. This modular and collaborative framework enables efficient\nknowledge updates and leverages existing medical LLMs and knowledge bases.\nExtensive experimental evaluations conducted on a wide range of publicly\naccessible multimodal medical datasets, incorporating text, image, audio, and\nvideo modalities, demonstrate that MAM consistently surpasses the performance\nof modality-specific LLMs. Notably, MAM achieves significant performance\nimprovements ranging from 18% to 365% compared to baseline models. Our code is\nreleased at https://github.com/yczhou001/MAM.", "comment": "ACL 2025 Findings", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.19835v1;http://arxiv.org/pdf/2506.19835v1", "pdf_url": "http://arxiv.org/pdf/2506.19835v1"}, {"title": "Generative artificial intelligence applications in different industries", "link": "https://lutpub.lut.fi/bitstream/handle/10024/170107/Mastersthesis_Kunnap_Vivian.pdf%3Fsequence%3D1", "details": "V K\u00fcnnap - 2025", "abstract": "\u2026 2.5 **Large** **language** **models** **Large** **language** **models** (LLMs) are machine learning models specialised in tasks relating to language (Taulli 2023, 94). LLMs can be used for tasks like generating coherent and contextually relevant outputs \u2026"}]
