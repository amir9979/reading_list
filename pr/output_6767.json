[{"title": "Revisiting Prompt Pretraining of Vision-Language Models", "link": "https://arxiv.org/pdf/2409.06166", "details": "Z Chen, L Yang, S Chen, Z Chen, J Liang, X Li - arXiv preprint arXiv:2409.06166, 2024", "abstract": "Prompt learning is an effective method to customize Vision-Language Models (VLMs) for various downstream tasks, involving tuning very few parameters of input prompt tokens. Recently, prompt pretraining in large-scale dataset (eg, ImageNet \u2026"}, {"title": "Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning", "link": "https://arxiv.org/pdf/2408.17150", "details": "X Qu, J Sun, W Wei, Y Cheng - arXiv preprint arXiv:2408.17150, 2024", "abstract": "Recently, Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities in multi-modal context comprehension. However, they still suffer from hallucination problems referring to generating inconsistent outputs with the image \u2026"}, {"title": "Contrastive Learning and Inter-Speaker Distribution Alignment Based Unsupervised Domain Adaptation for Robust Speaker Verification", "link": "https://www.isca-archive.org/interspeech_2024/li24u_interspeech.pdf", "details": "Z Li, W Guo, B Gu, S Peng, J Zhang", "abstract": "Unsupervised domain adaptation (UDA) can tackle the mismatch between the source and target domains for real-world speaker verification applications. In this paper, we propose an UDA method by leveraging the target-domain data through a \u2026"}, {"title": "Scaling Pre-training Data and Language Models for African Languages", "link": "https://uwspace.uwaterloo.ca/bitstreams/0eb3cf2b-5660-492e-8be6-6f4d4e82f5e1/download", "details": "A Oladipo - 2024", "abstract": "Recent advancements in language models, particularly for high-resource languages, have not been paralleled in low-resource languages spoken across Africa. This thesis addresses this gap by scaling pre-training data and developing improved \u2026"}, {"title": "The more quality information the better: Hierarchical generation of multi-evidence alignment and fusion model for multimodal entity and relation extraction", "link": "https://www.sciencedirect.com/science/article/pii/S0306457324002346", "details": "X He, S Li, Y Zhang, B Li, S Xu, Y Zhou - Information Processing & Management, 2025", "abstract": "Abstract Multimodal Entity and Relation Extraction (MERE) encompasses tasks, including Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction (MRE), aiming to extract valuable information from environments rich in \u2026"}]
