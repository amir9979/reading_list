[{"title": "You Don't Need Domain-Specific Data Augmentations When Scaling Self-Supervised Learning", "link": "https://openreview.net/pdf%3Fid%3D7RwKMRMNrc", "details": "T Moutakanni, M Oquab, M Szafraniec\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Self-Supervised learning (SSL) with Joint-Embedding Architectures (JEA) has led to outstanding performances. All instantiations of this paradigm were trained using strong and well-established hand-crafted data augmentations, leading to the general \u2026"}, {"title": "Decoding Report Generators: A Cyclic Vision-Language Adapter for Counterfactual Explanations", "link": "https://arxiv.org/pdf/2411.05261", "details": "Y Fang, Z Jin, S Guo, J Liu, Y Gao, J Ning, Z Yue, Z Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite significant advancements in report generation methods, a critical limitation remains: the lack of interpretability in the generated text. This paper introduces an innovative approach to enhance the explainability of text generated by report \u2026"}]
