[{"title": "Analysing the Residual Stream of Language Models Under Knowledge Conflicts", "link": "https://arxiv.org/pdf/2410.16090", "details": "Y Zhao, X Du, G Hong, AP Gema, A Devoto, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model \u2026"}, {"title": "Exploring Pretraining via Active Forgetting for Improving Cross Lingual Transfer for Decoder Language Models", "link": "https://arxiv.org/pdf/2410.16168", "details": "D Aggarwal, A Sathe, S Sitaram - arXiv preprint arXiv:2410.16168, 2024", "abstract": "Large Language Models (LLMs) demonstrate exceptional capabilities in a multitude of NLP tasks. However, the efficacy of such models to languages other than English is often limited. Prior works have shown that encoder-only models such as BERT or \u2026"}, {"title": "Not All Votes Count! Programs as Verifiers Improve Self-Consistency of Language Models for Math Reasoning", "link": "https://arxiv.org/pdf/2410.12608%3F", "details": "VYH Toh, D Ghosal, S Poria - arXiv preprint arXiv:2410.12608, 2024", "abstract": "Large language models (LLMs) have shown increasing proficiency in solving mathematical reasoning problems. However, many current open-source LLMs often still make calculation and semantic understanding errors in their intermediate \u2026"}, {"title": "Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models", "link": "https://arxiv.org/pdf/2410.12011", "details": "K Tatariya, V Araujo, T Bauwens, M de Lhoneux - arXiv preprint arXiv:2410.12011, 2024", "abstract": "Pixel-based language models have emerged as a compelling alternative to subword- based language modelling, particularly because they can represent virtually any script. PIXEL, a canonical example of such a model, is a vision transformer that has \u2026"}, {"title": "Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation", "link": "https://arxiv.org/pdf/2410.14251", "details": "S Tang, X Pang, Z Liu, B Tang, R Ye, X Dong, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Post-training is essential for enabling large language models (LLMs) to follow human instructions. Inspired by the recent success of using LLMs to simulate human society, we leverage multi-agent simulation to automatically generate diverse text-based \u2026"}, {"title": "Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion", "link": "https://arxiv.org/pdf/2410.14405", "details": "D Saynova, L Hagstr\u00f6m, M Johansson, R Johansson\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Previous interpretations of language models (LMs) miss important distinctions in how these models process factual information. For example, given the query\" Astrid Lindgren was born in\" with the corresponding completion\" Sweden\", no difference is \u2026"}, {"title": "ComPO: Community Preferences for Language Model Personalization", "link": "https://arxiv.org/pdf/2410.16027", "details": "S Kumar, CY Park, Y Tsvetkov, NA Smith, H Hajishirzi - arXiv preprint arXiv \u2026, 2024", "abstract": "Conventional algorithms for training language models (LMs) with human feedback rely on preferences that are assumed to account for an\" average\" user, disregarding subjectivity and finer-grained variations. Recent studies have raised concerns that \u2026"}, {"title": "LLaVA-KD: A Framework of Distilling Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2410.16236", "details": "Y Cai, J Zhang, H He, X He, A Tong, Z Gan, C Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The success of Large Language Models (LLM) has led researchers to explore Multimodal Large Language Models (MLLM) for unified visual and linguistic understanding. However, the increasing model size and computational complexity of \u2026"}, {"title": "Table-LLM-Specialist: Language Model Specialists for Tables using Iterative Generator-Validator Fine-tuning", "link": "https://arxiv.org/pdf/2410.12164", "details": "J Xing, Y He, M Zhou, H Dong, S Han, D Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we propose Table-LLM-Specialist, or Table-Specialist for short, as a new self-trained fine-tuning paradigm specifically designed for table tasks. Our insight is that for each table task, there often exist two dual versions of the same task, one \u2026"}]
