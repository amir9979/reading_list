[{"title": "Beyond Outcomes: Transparent Assessment of LLM Reasoning in Games", "link": "https://arxiv.org/pdf/2412.13602", "details": "W Lin, J Roberts, Y Yang, S Albanie, Z Lu, K Han - arXiv preprint arXiv:2412.13602, 2024", "abstract": "Large Language Models (LLMs) are increasingly deployed in real-world applications that demand complex reasoning. To track progress, robust benchmarks are required to evaluate their capabilities beyond superficial pattern recognition. However, current \u2026"}, {"title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining", "link": "https://arxiv.org/pdf/2501.00958", "details": "W Zhang, H Zhang, X Li, J Sun, Y Shen, W Lu, D Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge \u2026"}, {"title": "A High-Quality Text-Rich Image Instruction Tuning Dataset via Hybrid Instruction Generation", "link": "https://arxiv.org/pdf/2412.16364", "details": "S Zhou, R Zhang, Y Zhou, C Chen - arXiv preprint arXiv:2412.16364, 2024", "abstract": "Large multimodal models still struggle with text-rich images because of inadequate training data. Self-Instruct provides an annotation-free way for generating instruction data, but its quality is poor, as multimodal alignment remains a hurdle even for the \u2026"}]
