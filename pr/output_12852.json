[{"title": "Evaluating Generalization Capability of Language Models across Abductive, Deductive and Inductive Logical Reasoning", "link": "https://aclanthology.org/2025.coling-main.330.pdf", "details": "Y Sheng, W Wen, L Li, D Zeng - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Transformer-based language models (LMs) have demonstrated remarkable performance on many natural language tasks, yet to what extent LMs possess the capability of generalizing to unseen logical rules remains not explored sufficiently. In \u2026"}, {"title": "Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models", "link": "https://arxiv.org/pdf/2501.18119%3F", "details": "Q Lin, T Zhao, K He, Z Peng, F Xu, L Huang, J Ma\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To \u2026"}, {"title": "ARIES: Stimulating Self-Refinement of Large Language Models by Iterative Preference Optimization", "link": "https://arxiv.org/pdf/2502.05605", "details": "Y Zeng, X Cui, X Jin, G Liu, Z Sun, Q He, D Li, N Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "A truly intelligent Large Language Model (LLM) should be capable of correcting errors in its responses through external interactions. However, even the most advanced models often face challenges in improving their outputs. In this paper, we \u2026"}, {"title": "Foundations of Large Language Models", "link": "https://arxiv.org/pdf/2501.09223", "details": "T Xiao, J Zhu - arXiv preprint arXiv:2501.09223, 2025", "abstract": "This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting- edge technologies. The book is structured into four main chapters, each exploring a \u2026"}, {"title": "ArithmeticGPT: empowering small-size large language models with advanced arithmetic skills", "link": "https://link.springer.com/article/10.1007/s10994-024-06681-1", "details": "Z Liu, Y Zheng, Z Yin, J Chen, T Liu, M Tian, W Luo - Machine Learning, 2025", "abstract": "Large language models (LLMs) have shown remarkable capabilities in understanding and generating language across a wide range of domains. However, their performance in advanced arithmetic calculation remains a significant challenge \u2026"}, {"title": "Benchmarking Large Language Models via Random Variables", "link": "https://arxiv.org/pdf/2501.11790", "details": "Z Hong, H Wu, S Dong, J Dong, Y Xiao, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the continuous advancement of large language models (LLMs) in mathematical reasoning, evaluating their performance in this domain has become a prominent research focus. Recent studies have raised concerns about the reliability of current \u2026"}, {"title": "DORA: Dynamic Optimization Prompt for Continuous Reflection of LLM-based Agent", "link": "https://aclanthology.org/2025.coling-main.504.pdf", "details": "K Li, T Zhao, W Zhou, S Hu - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Autonomous agents powered by large language models (LLMs) hold significant potential across various domains. The Reflection framework is designed to help agents learn from past mistakes in complex tasks. While previous research has \u2026"}]
