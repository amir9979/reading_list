[{"title": "Content-Style Learning from Unaligned Domains: Identifiability under Unknown Latent Dimensions", "link": "https://arxiv.org/pdf/2411.03755", "details": "S Shrestha, X Fu - arXiv preprint arXiv:2411.03755, 2024", "abstract": "Understanding identifiability of latent content and style variables from unaligned multi- domain data is essential for tasks such as domain translation and data generation. Existing works on content-style identification were often developed under somewhat \u2026"}, {"title": "GUIDE-VAE: Advancing Data Generation with User Information and Pattern Dictionaries", "link": "https://arxiv.org/pdf/2411.03936%3F", "details": "K B\u00f6lat, S Tindemans - arXiv preprint arXiv:2411.03936, 2024", "abstract": "Generative modelling of multi-user datasets has become prominent in science and engineering. Generating a data point for a given user requires employing user information, and conventional generative models, including variational autoencoders \u2026"}, {"title": "Simulating clinical features on chest radiographs for medical image exploration and CNN explainability using a style-based generative adversarial autoencoder", "link": "https://www.nature.com/articles/s41598-024-75886-0", "details": "KA Hasenstab, L Hahn, N Chao, A Hsiao - Scientific Reports, 2024", "abstract": "Explainability of convolutional neural networks (CNNs) is integral for their adoption into radiological practice. Commonly used attribution methods localize image areas important for CNN prediction but do not characterize relevant imaging features \u2026"}, {"title": "Generative Example-Based Explanations: Bridging the Gap between Generative Modeling and Explainability", "link": "https://arxiv.org/pdf/2410.20890", "details": "P Vaeth, AM Fruehwald, B Paassen, M Gregorova - arXiv preprint arXiv:2410.20890, 2024", "abstract": "Recently, several methods have leveraged deep generative modeling to produce example-based explanations of decision algorithms for high-dimensional input data. Despite promising results, a disconnect exists between these methods and the \u2026"}, {"title": "Decomposing The Dark Matter of Sparse Autoencoders", "link": "https://arxiv.org/pdf/2410.14670%3F", "details": "J Engels, L Riggs, M Tegmark - arXiv preprint arXiv:2410.14670, 2024", "abstract": "Sparse autoencoders (SAEs) are a promising technique for decomposing language model activations into interpretable linear features. However, current SAEs fall short of completely explaining model performance, resulting in\" dark matter\": unexplained \u2026"}, {"title": "Incremental Image Generation with Diffusion Models by Label Embedding Initialization and Fusion", "link": "https://dl.acm.org/doi/pdf/10.1145/3688859%23page%3D10", "details": "B Li, D Ren, H Liu, T Yu, C Peng, Y Gao - Proceedings of the 1st on Continual \u2026, 2024", "abstract": "Recent diffusion models have excelled in image generation, but adapting them incrementally to new, unseen classes remains difficult. This paper presents LeifDM, an incremental diffusion model that efficiently adapts pre-trained models to new \u2026"}, {"title": "Capacity Control is an Effective Memorization Mitigation Mechanism in Text-Conditional Diffusion Models", "link": "https://arxiv.org/pdf/2410.22149%3F", "details": "R Dutt, P Sanchez, O Bohdal, SA Tsaftaris\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we present compelling evidence that controlling model capacity during fine-tuning can effectively mitigate memorization in diffusion models. Specifically, we demonstrate that adopting Parameter-Efficient Fine-Tuning (PEFT) within the pre \u2026"}, {"title": "Debiasing Large Vision-Language Models by Ablating Protected Attribute Representations", "link": "https://arxiv.org/pdf/2410.13976", "details": "N Ratzlaff, ML Olson, M Hinck, SY Tseng, V Lal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision Language Models (LVLMs) such as LLaVA have demonstrated impressive capabilities as general-purpose chatbots that can engage in conversations about a provided input image. However, their responses are \u2026"}, {"title": "CB-RISE: Improving the RISE Interpretability Method Through Convergence Detection and Blurred", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DzfEoEQAAQBAJ%26oi%3Dfnd%26pg%3DPA45%26ots%3Dr7POashpCK%26sig%3DmK82sA56sKWXEeYGkpsm4ndXQMo", "details": "O Stanchi\u00b9, F Ronchetti, P Dal Bianco, F Quiroga - Cloud Computing, Big Data and Emerging \u2026", "abstract": "This paper presents significant advancements in the RISE (Randomized Input Sampling for Explanation) algorithm, a popular black-box interpretability method for image data. RISE's main weakness lies on the large number of model evaluations \u2026"}]
