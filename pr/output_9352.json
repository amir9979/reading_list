[{"title": "Metaaligner: Towards generalizable multi-objective alignment of language models", "link": "https://openreview.net/pdf%3Fid%3DdIVb5C0QFf", "details": "K Yang, Z Liu, Q Xie, J Huang, T Zhang, S Ananiadou - The Thirty-eighth Annual \u2026, 2024", "abstract": "Recent advancements in large language models (LLMs) focus on aligning to heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are dependent on the policy model \u2026"}, {"title": "Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models", "link": "https://arxiv.org/pdf/2411.14257", "details": "J Ferrando, O Obeso, S Rajamanoharan, N Nanda - arXiv preprint arXiv:2411.14257, 2024", "abstract": "Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability \u2026"}, {"title": "Multifaceted Natural Language Processing Task\u2013Based Evaluation of Bidirectional Encoder Representations From Transformers Models for Bilingual (Korean and \u2026", "link": "https://medinform.jmir.org/2024/1/e52897/", "details": "K Kim, S Park, J Min, S Park, JY Kim, J Eun, K Jung\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background: The bidirectional encoder representations from transformers (BERT) model has attracted considerable attention in clinical applications, such as patient classification and disease prediction. However, current studies have typically \u2026"}, {"title": "Structured Codes and Free-Text Notes: Measuring Information Complementarity in Electronic Health Records", "link": "https://www.medrxiv.org/content/10.1101/2024.10.28.24316294.full.pdf", "details": "TM Seinen, JA Kors, EM van mulligen, PR Rijnbeek - medRxiv, 2024", "abstract": "Background: Electronic health records (EHRs) consist of both structured data (eg, diagnostic codes) and unstructured data (eg, clinical notes). It's commonly believed that unstructured clinical narratives provide more comprehensive information \u2026"}, {"title": "A novel classical machine learning framework for early sepsis prediction using electronic health record data from ICU patients", "link": "https://www.sciencedirect.com/science/article/pii/S0010482524013696", "details": "J Prithula, KR Islam, J Kumar, TL Tan, MBI Reaz\u2026 - Computers in Biology and \u2026, 2025", "abstract": "Sepsis, a life-threatening condition triggered by the body's response to infection, remains a significant global health challenge, annually affecting millions in the United States alone with substantial mortality and healthcare costs. Early prediction \u2026"}, {"title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models", "link": "https://aclanthology.org/2024.emnlp-main.566.pdf", "details": "XH Feng, C Chen, Y Li, Z Lin - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Pre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for \u2026"}, {"title": "Reducing Distraction in Long-Context Language Models by Focused Learning", "link": "https://arxiv.org/pdf/2411.05928", "details": "Z Wu, B Liu, R Yan, L Chen, T Delteil - arXiv preprint arXiv:2411.05928, 2024", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced their capacity to process long contexts. However, effectively utilizing this long context remains a challenge due to the issue of distraction, where irrelevant \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers", "link": "https://openreview.net/pdf%3Fid%3D67tRrjgzsh", "details": "X Lu, Y Zhao, B Qin, L Huo, Q Yang, D Xu - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Optimizing Fine-Tuning in Quantized Language Models: An In-Depth Analysis of Key Variables", "link": "https://cdn.techscience.cn/files/cmc/2024/online/CMC1030/TSP_CMC_57491/TSP_CMC_57491.pdf", "details": "A Shen, Z Lai, D Li, X Hu - 2024", "abstract": "ABSTRACT Large-scale Language Models (LLMs) have achieved significant breakthroughs in Natural Language Processing (NLP), driven by the pre-training and fine-tuning paradigm. While this approach allows models to specialize in specific \u2026"}]
