[{"title": "Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation", "link": "https://arxiv.org/pdf/2505.16763", "details": "H Yang, Y Zhou, W Han, J Shen - arXiv preprint arXiv:2505.16763, 2025", "abstract": "Text-to-image models are powerful for producing high-quality images based on given text prompts, but crafting these prompts often requires specialized vocabulary. To address this, existing methods train rewriting models with supervision from large \u2026", "entry_id": "http://arxiv.org/abs/2505.16763v1", "updated": "2025-05-22 15:05:07", "published": "2025-05-22 15:05:07", "authors": "Hongji Yang;Yucheng Zhou;Wencheng Han;Jianbing Shen", "summary": "Text-to-image models are powerful for producing high-quality images based on\ngiven text prompts, but crafting these prompts often requires specialized\nvocabulary. To address this, existing methods train rewriting models with\nsupervision from large amounts of manually annotated data and trained aesthetic\nassessment models. To alleviate the dependence on data scale for model training\nand the biases introduced by trained models, we propose a novel prompt\noptimization framework, designed to rephrase a simple user prompt into a\nsophisticated prompt to a text-to-image model. Specifically, we employ the\nlarge vision language models (LVLMs) as the solver to rewrite the user prompt,\nand concurrently, employ LVLMs as a reward model to score the aesthetics and\nalignment of the images generated by the optimized prompt. Instead of laborious\nhuman feedback, we exploit the prior knowledge of the LVLM to provide rewards,\ni.e., AI feedback. Simultaneously, the solver and the reward model are unified\ninto one model and iterated in reinforcement learning to achieve\nself-improvement by giving a solution and judging itself. Results on two\npopular datasets demonstrate that our method outperforms other strong\ncompetitors.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.16763v1;http://arxiv.org/pdf/2505.16763v1", "pdf_url": "http://arxiv.org/pdf/2505.16763v1"}, {"title": "COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3717867.3717923", "details": "P Govil, H Jain, V Bonagiri, A Chadha, P Kumaraguru\u2026 - Proceedings of the 17th \u2026, 2025", "abstract": "Large Language Models (LLMs) often inherit biases from the web data they are trained on, which contains stereotypes and prejudices. Current methods for evaluating and mitigating these biases rely on bias-benchmark datasets. These \u2026"}, {"title": "Dense Communication between Language Models", "link": "https://arxiv.org/pdf/2505.12741", "details": "S Wu, Y Wang, Q Yao - arXiv preprint arXiv:2505.12741, 2025", "abstract": "As higher-level intelligence emerges from the combination of modular components with lower-level intelligence, many works combines Large Language Models (LLMs) for collective intelligence. Such combination is achieved by building communications \u2026", "entry_id": "http://arxiv.org/abs/2505.12741v1", "updated": "2025-05-19 05:56:06", "published": "2025-05-19 05:56:06", "authors": "Shiguang Wu;Yaqing Wang;Quanming Yao", "summary": "As higher-level intelligence emerges from the combination of modular\ncomponents with lower-level intelligence, many works combines Large Language\nModels (LLMs) for collective intelligence. Such combination is achieved by\nbuilding communications among LLMs. While current systems primarily facilitate\nsuch communication through natural language, this paper proposes a novel\nparadigm of direct dense vector communication between LLMs. Our approach\neliminates the unnecessary embedding and de-embedding steps when LLM interact\nwith another, enabling more efficient information transfer, fully\ndifferentiable optimization pathways, and exploration of capabilities beyond\nhuman heuristics. We use such stripped LLMs as vertexes and optimizable seq2seq\nmodules as edges to construct LMNet, with similar structure as MLPs. By\nutilizing smaller pre-trained LLMs as vertexes, we train a LMNet that achieves\ncomparable performance with LLMs in similar size with only less than 0.1%\ntraining cost. This offers a new perspective on scaling for general\nintelligence rather than training a monolithic LLM from scratch. Besides, the\nproposed method can be used for other applications, like customizing LLM with\nlimited data, showing its versatility.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.12741v1;http://arxiv.org/pdf/2505.12741v1", "pdf_url": "http://arxiv.org/pdf/2505.12741v1"}, {"title": "Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2505.16416", "details": "C Wang, J Guo, H Li, Y Tian, Y Nie, C Xu, K Han - arXiv preprint arXiv:2505.16416, 2025", "abstract": "Rotary Position Embedding (RoPE) is a widely adopted technique for encoding relative positional information in large language models (LLMs). However, when extended to large vision-language models (LVLMs), its variants introduce \u2026", "entry_id": "http://arxiv.org/abs/2505.16416v1", "updated": "2025-05-22 09:05:01", "published": "2025-05-22 09:05:01", "authors": "Chengcheng Wang;Jianyuan Guo;Hongguang Li;Yuchuan Tian;Ying Nie;Chang Xu;Kai Han", "summary": "Rotary Position Embedding (RoPE) is a widely adopted technique for encoding\nrelative positional information in large language models (LLMs). However, when\nextended to large vision-language models (LVLMs), its variants introduce\nunintended cross-modal positional biases. Specifically, they enforce relative\npositional dependencies between text token indices and image tokens, causing\nspurious alignments. This issue arises because image tokens representing the\nsame content but located at different spatial positions are assigned distinct\npositional biases, leading to inconsistent cross-modal associations. To address\nthis, we propose Per-Token Distance (PTD) - a simple yet effective metric for\nquantifying the independence of positional encodings across modalities.\nInformed by this analysis, we introduce Circle-RoPE, a novel encoding scheme\nthat maps image token indices onto a circular trajectory orthogonal to the\nlinear path of text token indices, forming a cone-like structure. This\nconfiguration ensures that each text token maintains an equal distance to all\nimage tokens, reducing artificial cross-modal biases while preserving\nintra-image spatial information. To further enhance performance, we propose a\nstaggered layer strategy that applies different RoPE variants across layers.\nThis design leverages the complementary strengths of each RoPE variant, thereby\nenhancing the model's overall performance. Our experimental results demonstrate\nthat our method effectively preserves spatial information from images while\nreducing relative positional bias, offering a more robust and flexible\npositional encoding framework for LVLMs. The code is available at\n[https://github.com/lose4578/CircleRoPE](https://github.com/lose4578/CircleRoPE).", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.16416v1;http://arxiv.org/pdf/2505.16416v1", "pdf_url": "http://arxiv.org/pdf/2505.16416v1"}, {"title": "ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2505.16517", "details": "Z Song, G Ouyang, M Li, Y Ji, C Wang, Z Xu, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision-Language Models (LVLMs) have recently advanced robotic manipulation by leveraging vision for scene perception and language for instruction following. However, existing methods rely heavily on costly human-annotated \u2026", "entry_id": "http://arxiv.org/abs/2505.16517v2", "updated": "2025-05-24 10:44:27", "published": "2025-05-22 10:57:07", "authors": "Zirui Song;Guangxian Ouyang;Mingzhe Li;Yuheng Ji;Chenxi Wang;Zixiang Xu;Zeyu Zhang;Xiaoqing Zhang;Qian Jiang;Zhenhao Chen;Zhongzhi Li;Rui Yan;Xiuying Chen", "summary": "Large Vision-Language Models (LVLMs) have recently advanced robotic\nmanipulation by leveraging vision for scene perception and language for\ninstruction following. However, existing methods rely heavily on costly\nhuman-annotated training datasets, which limits their generalization and causes\nthem to struggle in out-of-domain (OOD) scenarios, reducing real-world\nadaptability. To address these challenges, we propose ManipLVM-R1, a novel\nreinforcement learning framework that replaces traditional supervision with\nReinforcement Learning using Verifiable Rewards (RLVR). By directly optimizing\nfor task-aligned outcomes, our method enhances generalization and physical\nreasoning while removing the dependence on costly annotations. Specifically, we\ndesign two rule-based reward functions targeting key robotic manipulation\nsubtasks: an Affordance Perception Reward to enhance localization of\ninteraction regions, and a Trajectory Match Reward to ensure the physical\nplausibility of action paths. These rewards provide immediate feedback and\nimpose spatial-logical constraints, encouraging the model to go beyond shallow\npattern matching and instead learn deeper, more systematic reasoning about\nphysical interactions.", "comment": "14pages", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.CV", "links": "http://arxiv.org/abs/2505.16517v2;http://arxiv.org/pdf/2505.16517v2", "pdf_url": "http://arxiv.org/pdf/2505.16517v2"}, {"title": "AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation", "link": "https://arxiv.org/pdf/2505.11887", "details": "X Zhang, Z Ouyang, L Wang, G de Melo, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the proliferation of large language models (LLMs) in the medical domain, there is increasing demand for improved evaluation techniques to assess their capabilities. However, traditional metrics like F1 and ROUGE, which rely on token overlaps to \u2026", "entry_id": "http://arxiv.org/abs/2505.11887v1", "updated": "2025-05-17 07:44:54", "published": "2025-05-17 07:44:54", "authors": "Xiechi Zhang;Zetian Ouyang;Linlin Wang;Gerard de Melo;Zhu Cao;Xiaoling Wang;Ya Zhang;Yanfeng Wang;Liang He", "summary": "With the proliferation of large language models (LLMs) in the medical domain,\nthere is increasing demand for improved evaluation techniques to assess their\ncapabilities. However, traditional metrics like F1 and ROUGE, which rely on\ntoken overlaps to measure quality, significantly overlook the importance of\nmedical terminology. While human evaluation tends to be more reliable, it can\nbe very costly and may as well suffer from inaccuracies due to limits in human\nexpertise and motivation. Although there are some evaluation methods based on\nLLMs, their usability in the medical field is limited due to their proprietary\nnature or lack of expertise. To tackle these challenges, we present\nAutoMedEval, an open-sourced automatic evaluation model with 13B parameters\nspecifically engineered to measure the question-answering proficiency of\nmedical LLMs. The overarching objective of AutoMedEval is to assess the quality\nof responses produced by diverse models, aspiring to significantly reduce the\ndependence on human evaluation. Specifically, we propose a hierarchical\ntraining method involving curriculum instruction tuning and an iterative\nknowledge introspection mechanism, enabling AutoMedEval to acquire professional\nmedical assessment capabilities with limited instructional data. Human\nevaluations indicate that AutoMedEval surpasses other baselines in terms of\ncorrelation with human judgments.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.11887v1;http://arxiv.org/pdf/2505.11887v1", "pdf_url": "http://arxiv.org/pdf/2505.11887v1"}, {"title": "IQBench: How \"Smart'' Are Vision-Language Models? A Study with Human IQ Tests", "link": "https://arxiv.org/pdf/2505.12000", "details": "TH Pham, PV Nguyen, DT Hung, BT Duong, VN Thanh\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Although large Vision-Language Models (VLMs) have demonstrated remarkable performance in a wide range of multimodal tasks, their true reasoning capabilities on human IQ tests remain underexplored. To advance research on the fluid intelligence \u2026", "entry_id": "http://arxiv.org/abs/2505.12000v1", "updated": "2025-05-17 13:24:08", "published": "2025-05-17 13:24:08", "authors": "Tan-Hanh Pham;Phu-Vinh Nguyen;Dang The Hung;Bui Trong Duong;Vu Nguyen Thanh;Chris Ngo;Tri Quang Truong;Truong-Son Hy", "summary": "Although large Vision-Language Models (VLMs) have demonstrated remarkable\nperformance in a wide range of multimodal tasks, their true reasoning\ncapabilities on human IQ tests remain underexplored. To advance research on the\nfluid intelligence of VLMs, we introduce **IQBench**, a new benchmark designed\nto evaluate VLMs on standardized visual IQ tests. We focus on evaluating the\nreasoning capabilities of VLMs, which we argue are more important than the\naccuracy of the final prediction. **Our benchmark is visually centric,\nminimizing the dependence on unnecessary textual content**, thus encouraging\nmodels to derive answers primarily from image-based information rather than\nlearned textual knowledge. To this end, we manually collected and annotated 500\nvisual IQ questions to **prevent unintentional data leakage during training**.\nUnlike prior work that focuses primarily on the accuracy of the final answer,\nwe evaluate the reasoning ability of the models by assessing their explanations\nand the patterns used to solve each problem, along with the accuracy of the\nfinal prediction and human evaluation. Our experiments show that there are\nsubstantial performance disparities between tasks, with models such as\n`o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet` achieving the highest\naverage accuracies of 0.615, 0.578, and 0.548, respectively. However, all\nmodels struggle with 3D spatial and anagram reasoning tasks, highlighting\nsignificant limitations in current VLMs' general reasoning abilities. In terms\nof reasoning scores, `o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet`\nachieved top averages of 0.696, 0.586, and 0.516, respectively. These results\nhighlight inconsistencies between the reasoning processes of the models and\ntheir final answers, emphasizing the importance of evaluating the accuracy of\nthe reasoning in addition to the final predictions.", "comment": "IQ Test for Multimodal Models", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.12000v1;http://arxiv.org/pdf/2505.12000v1", "pdf_url": "http://arxiv.org/pdf/2505.12000v1"}, {"title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records", "link": "https://arxiv.org/pdf/2505.16941", "details": "C Pang, V Jeanselme, YS Choi, X Jiang, Z Jing\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Foundation models hold significant promise in healthcare, given their capacity to extract meaningful representations independent of downstream tasks. This property has enabled state-of-the-art performance across several clinical applications trained \u2026", "entry_id": "http://arxiv.org/abs/2505.16941v2", "updated": "2025-05-23 02:06:25", "published": "2025-05-22 17:29:52", "authors": "Chao Pang;Vincent Jeanselme;Young Sang Choi;Xinzhuo Jiang;Zilin Jing;Aparajita Kashyap;Yuta Kobayashi;Yanwei Li;Florent Pollet;Karthik Natarajan;Shalmali Joshi", "summary": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.16941v2;http://arxiv.org/pdf/2505.16941v2", "pdf_url": "http://arxiv.org/pdf/2505.16941v2"}, {"title": "Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator", "link": "https://arxiv.org/pdf/2505.16690", "details": "B Luo, S Wang, Y Li, H Wei - arXiv preprint arXiv:2505.16690, 2025", "abstract": "Post-training of large language models is essential for adapting pre-trained language models (PLMs) to align with human preferences and downstream tasks. While PLMs typically exhibit well-calibrated confidence, post-trained language \u2026", "entry_id": "http://arxiv.org/abs/2505.16690v1", "updated": "2025-05-22 13:55:39", "published": "2025-05-22 13:55:39", "authors": "Beier Luo;Shuoyuan Wang;Yixuan Li;Hongxin Wei", "summary": "Post-training of large language models is essential for adapting pre-trained\nlanguage models (PLMs) to align with human preferences and downstream tasks.\nWhile PLMs typically exhibit well-calibrated confidence, post-trained language\nmodels (PoLMs) often suffer from over-confidence, assigning high confidence to\nboth correct and incorrect outputs, which can undermine reliability in critical\napplications. A major obstacle in calibrating PoLMs is the scarcity of labeled\ndata for individual downstream tasks. To address this, we propose\nDisagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to\noptimize the parameters (e.g., temperature $\\tau$) in post-hoc confidence\ncalibration. Our method is motivated by the under-confidence issue caused by\nprediction disagreement between the PLM and PoLM while aligning their\nconfidence via temperature scaling. Theoretically, the PLM's confidence\nunderestimates PoLM's prediction accuracy on disagreement examples, causing a\nlarger $\\tau$ and producing under-confident predictions. DACA mitigates this by\nselectively using only agreement examples for calibration, effectively\ndecoupling the influence of disagreement. In this manner, our method avoids an\noverly large $\\tau$ in temperature scaling caused by disagreement examples,\nimproving calibration performance. Extensive experiments demonstrate the\neffectiveness of our method, improving the average ECE of open-sourced and\nAPI-based LLMs (e.g. GPT-4o) by up to 15.08$\\%$ on common benchmarks.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.16690v1;http://arxiv.org/pdf/2505.16690v1", "pdf_url": "http://arxiv.org/pdf/2505.16690v1"}]
