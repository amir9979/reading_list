[{"title": "Composable Interventions for Language Models", "link": "https://arxiv.org/pdf/2407.06483", "details": "A Kolbeinsson, K O'Brien, T Huang, S Gao, S Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Test-time interventions for language models can enhance factual accuracy, mitigate harmful outputs, and improve model efficiency without costly retraining. But despite a flood of new methods, different types of interventions are largely developing \u2026"}, {"title": "Supporting Cross-language Cross-project Bug Localization Using Pre-trained Language Models", "link": "https://arxiv.org/pdf/2407.02732", "details": "M Chandramohan, DQ Nguyen, P Krishnan, J Jancic - arXiv preprint arXiv \u2026, 2024", "abstract": "Automatically locating a bug within a large codebase remains a significant challenge for developers. Existing techniques often struggle with generalizability and deployment due to their reliance on application-specific data and large model sizes \u2026"}, {"title": "Collaborative Performance Prediction for Large Language Models", "link": "https://arxiv.org/pdf/2407.01300", "details": "Q Zhang, F Lyu, X Liu, C Ma - arXiv preprint arXiv:2407.01300, 2024", "abstract": "Comprehensively understanding and accurately predicting the performance of large language models across diverse downstream tasks has emerged as a pivotal challenge in NLP research. The pioneering scaling law on downstream works \u2026"}, {"title": "On the attribution of confidence to large language models", "link": "https://arxiv.org/pdf/2407.08388", "details": "G Keeling, W Street - arXiv preprint arXiv:2407.08388, 2024", "abstract": "Credences are mental states corresponding to degrees of confidence in propositions. Attribution of credences to Large Language Models (LLMs) is commonplace in the empirical literature on LLM evaluation. Yet the theoretical basis \u2026"}, {"title": "ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts", "link": "https://arxiv.org/pdf/2407.09447", "details": "AF Hardy, H Liu, B Lange, MJ Kochenderfer - arXiv preprint arXiv:2407.09447, 2024", "abstract": "Typical schemes for automated red-teaming large language models (LLMs) focus on discovering prompts that trigger a frozen language model (the defender) to generate toxic text. This often results in the prompting model (the adversary) producing text \u2026"}, {"title": "Beyond Instruction Following: Evaluating Rule Following of Large Language Models", "link": "https://arxiv.org/pdf/2407.08440", "details": "W Sun, C Zhang, X Zhang, Z Huang, H Xu, P Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Although Large Language Models (LLMs) have demonstrated strong instruction- following ability to be helpful, they are further supposed to be controlled and guided by rules in real-world scenarios to be safe, and accurate in responses. This demands \u2026"}, {"title": "DART: Deep Adversarial Automated Red Teaming for LLM Safety", "link": "https://arxiv.org/pdf/2407.03876", "details": "B Jiang, Y Jing, T Shen, Q Yang, D Xiong - arXiv preprint arXiv:2407.03876, 2024", "abstract": "Manual Red teaming is a commonly-used method to identify vulnerabilities in large language models (LLMs), which, is costly and unscalable. In contrast, automated red teaming uses a Red LLM to automatically generate adversarial prompts to the Target \u2026"}, {"title": "LANE: Logic Alignment of Non-tuning Large Language Models and Online Recommendation Systems for Explainable Reason Generation", "link": "https://arxiv.org/pdf/2407.02833", "details": "H Zhao, S Zheng, L Wu, B Yu, J Wang - arXiv preprint arXiv:2407.02833, 2024", "abstract": "The explainability of recommendation systems is crucial for enhancing user trust and satisfaction. Leveraging large language models (LLMs) offers new opportunities for comprehensive recommendation logic generation. However, in existing related \u2026"}, {"title": "Molecule Language Model with Augmented Pairs and Expertise Transfer", "link": "https://arxiv.org/pdf/2407.09043", "details": "N Lee, S Laghuvarapu, C Park, J Sun - arXiv preprint arXiv:2407.09043, 2024", "abstract": "Understanding the molecules and their textual descriptions via molecule language models (MoLM) recently got a surge of interest among researchers. However, unique challenges exist in the field of MoLM due to 1) a limited amount of molecule-text \u2026"}]
