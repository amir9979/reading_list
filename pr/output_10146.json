[{"title": "Liquid: Language Models are Scalable Multi-modal Generators", "link": "https://arxiv.org/pdf/2412.04332", "details": "J Wu, Y Jiang, C Ma, Y Liu, H Zhao, Z Yuan, S Bai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Liquid, an auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared \u2026"}, {"title": "DiCoDe: Diffusion-Compressed Deep Tokens for Autoregressive Video Generation with Language Models", "link": "https://arxiv.org/pdf/2412.04446%3F", "details": "Y Li, Y Ge, Y Ge, P Luo, Y Shan - arXiv preprint arXiv:2412.04446, 2024", "abstract": "Videos are inherently temporal sequences by their very nature. In this work, we explore the potential of modeling videos in a chronological and scalable manner with autoregressive (AR) language models, inspired by their success in natural language \u2026"}, {"title": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models", "link": "https://arxiv.org/pdf/2412.02830", "details": "H Tran, Z Yao, J Wang, Y Zhang, Z Yang, H Yu - arXiv preprint arXiv:2412.02830, 2024", "abstract": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a versatile extension to the mutual reasoning framework (rStar), aimed at enhancing reasoning accuracy and factual integrity across large language models (LLMs) for \u2026"}, {"title": "NVILA: Efficient Frontier Visual Language Models", "link": "https://arxiv.org/pdf/2412.04468", "details": "Z Liu, L Zhu, B Shi, Z Zhang, Y Lou, S Yang, H Xi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and \u2026"}, {"title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models", "link": "https://arxiv.org/pdf/2412.04467", "details": "S Yang, Y Chen, Z Tian, C Wang, J Li, B Yu, J Jia - arXiv preprint arXiv:2412.04467, 2024", "abstract": "Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens \u2026"}, {"title": "Deduce and Select Evidences with Language Models for Training-Free Video Goal Inference", "link": "https://basurafernando.github.io/papers/2025_WACV_VITTIFF.pdf", "details": "YK Ee, H Zhang, A Matyasko, B Fernando", "abstract": "Abstract We introduce ViDSE, a Video framework that Deduce and Selects visual Evidence for training-free video goal inference using language models. Unlike approaches that directly apply vision-language models (VLM) or combine VLM+ LLM \u2026"}, {"title": "Assessing and Learning Alignment of Unimodal Vision and Language Models", "link": "https://arxiv.org/pdf/2412.04616", "details": "L Zhang, Q Yang, A Agrawal - arXiv preprint arXiv:2412.04616, 2024", "abstract": "How well are unimodal vision and language models aligned? Although prior work have approached answering this question, their assessment methods do not directly translate to how these models are used in practical vision-language tasks. In this \u2026"}, {"title": "AdvDreamer Unveils: Are Vision-Language Models Truly Ready for Real-World 3D Variations?", "link": "https://arxiv.org/pdf/2412.03002", "details": "S Ruan, H Liu, Y Huang, X Wang, C Kang, H Su\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision Language Models (VLMs) have exhibited remarkable generalization capabilities, yet their robustness in dynamic real-world scenarios remains largely unexplored. To systematically evaluate VLMs' robustness to real-world 3D variations \u2026"}, {"title": "Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning", "link": "https://arxiv.org/pdf/2412.08614", "details": "F Lu, W Wu, K Zheng, S Ma, B Gong, J Liu, W Zhai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating detailed captions comprehending text-rich visual content in images has received growing attention for Large Vision-Language Models (LVLMs). However, few studies have developed benchmarks specifically tailored for detailed captions to \u2026"}]
