[{"title": "Tuple Perturbation-based Contrastive Learning Framework for Multimodal Remote Sensing Image Semantic Segmentation", "link": "https://ieeexplore.ieee.org/abstract/document/10896945/", "details": "Y Ye, J Dai, L Zhou, K Duan, R Tao, W Li, D Hong - IEEE Transactions on Geoscience \u2026, 2025", "abstract": "Deep learning models exhibit promising potential in multi-modal remote sensing image semantic segmentation (MRSISS). However, the constrained access to labeled samples for training deep learning networks significantly influences the \u2026"}, {"title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06788", "details": "H Diao, X Li, Y Cui, Y Wang, H Deng, T Pan, W Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient \u2026"}, {"title": "SDCluster: A clustering based self-supervised pre-training method for semantic segmentation of remote sensing images", "link": "https://www.sciencedirect.com/science/article/pii/S0924271625000796", "details": "H Xu, C Zhang, P Yue, K Wang - ISPRS Journal of Photogrammetry and Remote \u2026, 2025", "abstract": "Reducing the reliance of remote sensing semantic segmentation models on labeled training data is essential for practical model deployment. Self-supervised pre-training methods, which learn representations from unlabeled data by designing pretext \u2026"}, {"title": "Noise is an Efficient Learner for Zero-Shot Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06019", "details": "R Imam, A Hanif, J Zhang, KW Dawoud\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, test-time adaptation has garnered attention as a method for tuning models without labeled data. The conventional modus operandi for adapting pre-trained vision-language models (VLMs) during test-time primarily focuses on tuning \u2026"}, {"title": "Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding", "link": "https://arxiv.org/pdf/2502.11492", "details": "KH Huang, C Qin, H Qiu, P Laban, S Joty, C Xiong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision Language Models (VLMs) have achieved remarkable progress in multimodal tasks, yet they often struggle with visual arithmetic, seemingly simple capabilities like object counting or length comparison, which are essential for relevant complex tasks \u2026"}, {"title": "Towards a holistic framework for multimodal LLM in 3D brain CT radiology report generation", "link": "https://www.nature.com/articles/s41467-025-57426-0", "details": "CY Li, KJ Chang, CF Yang, HY Wu, W Chen, H Bansal\u2026 - Nature Communications, 2025", "abstract": "Multi-modal large language models (MLLMs) have transformed the landscape of modern healthcare, with automated radiology report generation (RRG) emerging as a cutting-edge application. While 2D MLLM-based RRG has been well established \u2026"}, {"title": "BPQA Dataset: Evaluating How Well Language Models Leverage Blood Pressures to Answer Biomedical Questions", "link": "https://arxiv.org/pdf/2503.04155", "details": "C Hang, R Deng, LY Jiang, Z Yang, A Alyakin, D Alber\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Clinical measurements such as blood pressures and respiration rates are critical in diagnosing and monitoring patient outcomes. It is an important component of biomedical data, which can be used to train transformer-based language models \u2026"}, {"title": "FetalCLIP: A Visual-Language Foundation Model for Fetal Ultrasound Image Analysis", "link": "https://arxiv.org/pdf/2502.14807", "details": "F Maani, N Saeed, T Saleem, Z Farooq, H Alasmawi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Foundation models are becoming increasingly effective in the medical domain, offering pre-trained models on large datasets that can be readily adapted for downstream tasks. Despite progress, fetal ultrasound images remain a challenging \u2026"}, {"title": "InsightVision: A Comprehensive, Multi-Level Chinese-based Benchmark for Evaluating Implicit Visual Semantics in Large Vision Language Models", "link": "https://arxiv.org/pdf/2502.15812", "details": "X Yin, Y Hong, Y Guo, Y Tu, W Wang, G Liu - arXiv preprint arXiv:2502.15812, 2025", "abstract": "In the evolving landscape of multimodal language models, understanding the nuanced meanings conveyed through visual cues-such as satire, insult, or critique- remains a significant challenge. Existing evaluation benchmarks primarily focus on \u2026"}]
