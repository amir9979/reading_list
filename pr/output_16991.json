[{"title": "Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning", "link": "https://arxiv.org/pdf/2505.18503", "details": "A Chang, L Huang, AJ Boyd, P Bhatia, T Kass-Hout\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal attention distribution on visual inputs, leading to hallucinated or inaccurate outputs. Existing mitigation methods primarily rely on inference-time interventions, which are \u2026", "entry_id": "http://arxiv.org/abs/2505.18503v1", "updated": "2025-05-24 04:45:45", "published": "2025-05-24 04:45:45", "authors": "Aofei Chang;Le Huang;Alex James Boyd;Parminder Bhatia;Taha Kass-Hout;Cao Xiao;Fenglong Ma", "summary": "Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal\nattention distribution on visual inputs, leading to hallucinated or inaccurate\noutputs. Existing mitigation methods primarily rely on inference-time\ninterventions, which are limited in attention adaptation or require additional\nsupervision. To address this, we propose A$^3$Tune, a novel fine-tuning\nframework for Automatic Attention Alignment Tuning. A$^3$Tune leverages\nzero-shot weak labels from SAM, refines them into prompt-aware labels using\nBioMedCLIP, and then selectively modifies visually-critical attention heads to\nimprove alignment while minimizing interference. Additionally, we introduce a\nA$^3$MoE module, enabling adaptive parameter selection for attention tuning\nacross diverse prompts and images. Extensive experiments on medical VQA and\nreport generation benchmarks show that A$^3$Tune outperforms state-of-the-art\nbaselines, achieving enhanced attention distributions and performance in\nMed-LVLMs.", "comment": "Accepted to ACL2025 (main)", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.18503v1;http://arxiv.org/pdf/2505.18503v1", "pdf_url": "http://arxiv.org/pdf/2505.18503v1"}, {"title": "Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation", "link": "https://arxiv.org/pdf/2505.01091", "details": "D Molino, F di Feola, L Shen, P Soda, V Guarrasi - arXiv preprint arXiv:2505.01091, 2025", "abstract": "Generative models have revolutionized Artificial Intelligence (AI), particularly in multimodal applications. However, adapting these models to the medical domain poses unique challenges due to the complexity of medical data and the stringent \u2026", "entry_id": "http://arxiv.org/abs/2505.01091v1", "updated": "2025-05-02 08:07:24", "published": "2025-05-02 08:07:24", "authors": "Daniele Molino;Francesco di Feola;Linlin Shen;Paolo Soda;Valerio Guarrasi", "summary": "Generative models have revolutionized Artificial Intelligence (AI),\nparticularly in multimodal applications. However, adapting these models to the\nmedical domain poses unique challenges due to the complexity of medical data\nand the stringent need for clinical accuracy. In this work, we introduce a\nframework specifically designed for multimodal medical data generation. By\nenabling the generation of multi-view chest X-rays and their associated\nclinical report, it bridges the gap between general-purpose vision-language\nmodels and the specialized requirements of healthcare. Leveraging the MIMIC-CXR\ndataset, the proposed framework shows superior performance in generating\nhigh-fidelity images and semantically coherent reports. Our quantitative\nevaluation reveals significant results in terms of FID and BLEU scores,\nshowcasing the quality of the generated data. Notably, our framework achieves\ncomparable or even superior performance compared to real data on downstream\ndisease classification tasks, underlining its potential as a tool for medical\nresearch and diagnostics. This study highlights the importance of\ndomain-specific adaptations in enhancing the relevance and utility of\ngenerative models for clinical applications, paving the way for future\nadvancements in synthetic multimodal medical data generation.", "comment": "arXiv admin note: substantial text overlap with arXiv:2501.04614", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.01091v1;http://arxiv.org/pdf/2505.01091v1", "pdf_url": "http://arxiv.org/pdf/2505.01091v1"}, {"title": "VLM-KG: Multimodal Radiology Knowledge Graph Generation", "link": "https://arxiv.org/pdf/2505.17042", "details": "A Abdullah, ST Kim - arXiv preprint arXiv:2505.17042, 2025", "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable success in natural language generation, excelling at instruction following and structured output generation. Knowledge graphs play a crucial role in radiology, serving as valuable \u2026", "entry_id": "http://arxiv.org/abs/2505.17042v1", "updated": "2025-05-13 06:11:10", "published": "2025-05-13 06:11:10", "authors": "Abdullah Abdullah;Seong Tae Kim", "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success in natural\nlanguage generation, excelling at instruction following and structured output\ngeneration. Knowledge graphs play a crucial role in radiology, serving as\nvaluable sources of factual information and enhancing various downstream tasks.\nHowever, generating radiology-specific knowledge graphs presents significant\nchallenges due to the specialized language of radiology reports and the limited\navailability of domain-specific data. Existing solutions are predominantly\nunimodal, meaning they generate knowledge graphs only from radiology reports\nwhile excluding radiographic images. Additionally, they struggle with long-form\nradiology data due to limited context length. To address these limitations, we\npropose a novel multimodal VLM-based framework for knowledge graph generation\nin radiology. Our approach outperforms previous methods and introduces the\nfirst multimodal solution for radiology knowledge graph generation.", "comment": "10 pages, 2 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.CV;cs.IR;cs.LG", "links": "http://arxiv.org/abs/2505.17042v1;http://arxiv.org/pdf/2505.17042v1", "pdf_url": "http://arxiv.org/pdf/2505.17042v1"}, {"title": "Multimodal Federated Learning With Missing Modalities through Feature Imputation Network", "link": "https://arxiv.org/pdf/2505.20232", "details": "P Poudel, A Chhetri, P Gyawali, G Leontidis\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal federated learning holds immense potential for collaboratively training models from multiple sources without sharing raw data, addressing both data scarcity and privacy concerns, two key challenges in healthcare. A major challenge in \u2026", "entry_id": "http://arxiv.org/abs/2505.20232v1", "updated": "2025-05-26 17:11:03", "published": "2025-05-26 17:11:03", "authors": "Pranav Poudel;Aavash Chhetri;Prashnna Gyawali;Georgios Leontidis;Binod Bhattarai", "summary": "Multimodal federated learning holds immense potential for collaboratively\ntraining models from multiple sources without sharing raw data, addressing both\ndata scarcity and privacy concerns, two key challenges in healthcare. A major\nchallenge in training multimodal federated models in healthcare is the presence\nof missing modalities due to multiple reasons, including variations in clinical\npractice, cost and accessibility constraints, retrospective data collection,\nprivacy concerns, and occasional technical or human errors. Previous methods\ntypically rely on publicly available real datasets or synthetic data to\ncompensate for missing modalities. However, obtaining real datasets for every\ndisease is impractical, and training generative models to synthesize missing\nmodalities is computationally expensive and prone to errors due to the high\ndimensionality of medical data. In this paper, we propose a novel, lightweight,\nlow-dimensional feature translator to reconstruct bottleneck features of the\nmissing modalities. Our experiments on three different datasets (MIMIC-CXR, NIH\nOpen-I, and CheXpert), in both homogeneous and heterogeneous settings\nconsistently improve the performance of competitive baselines. The code and\nimplementation details are available at:\nhttps://github.com/bhattarailab/FedFeatGen", "comment": "MIUA 2025", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CV", "links": "http://arxiv.org/abs/2505.20232v1;http://arxiv.org/pdf/2505.20232v1", "pdf_url": "http://arxiv.org/pdf/2505.20232v1"}, {"title": "Chest X-Ray Visual Saliency Modeling: Eye-Tracking Dataset and Saliency Prediction Model", "link": "https://ieeexplore.ieee.org/abstract/document/10993309/", "details": "J Lou, H Wang, X Wu, JCH Ng, R White, KA Thakoor\u2026 - IEEE Transactions on \u2026, 2025", "abstract": "Radiologists' eye movements during medical image interpretation reflect their perceptual-cognitive processes of diagnostic decisions. The eye movement data can be modeled to represent clinically relevant regions in a medical image and \u2026"}, {"title": "CLN: A multi-task deep neural network for chest X-ray image localisation and classification", "link": "https://durham-repository.worktribe.com/OutputFile/3955355", "details": "GI Okolo, S Katsigiannis, N Ramzan - Expert Systems with Applications, 2025", "abstract": "Chest X-ray (CXR) imaging is a widely used and cost-effective medical imaging technique for detecting various pathologies. However, accurate interpretation of CXR images is a challenging and time-consuming task that requires expert radiologists \u2026"}]
