[{"title": "Aligning Language Models Using Follow-up Likelihood as Reward Signal", "link": "https://arxiv.org/pdf/2409.13948", "details": "C Zhang, D Chong, F Jiang, C Tang, A Gao, G Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In natural human-to-human conversations, participants often receive feedback signals from one another based on their follow-up reactions. These reactions can include verbal responses, facial expressions, changes in emotional state, and other \u2026"}, {"title": "Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs", "link": "https://arxiv.org/pdf/2409.14988", "details": "C Christophe, T Raha, S Maslenkova, MU Salman\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated significant potential in transforming clinical applications. In this study, we investigate the efficacy of four techniques in adapting LLMs for clinical use-cases: continuous pretraining, instruct \u2026"}, {"title": "Evaluation of Large Language Model Performance on the Biomedical Language Understanding and Reasoning Benchmark: Comparative Study", "link": "https://www.medrxiv.org/content/10.1101/2024.05.17.24307411.pdf", "details": "H Feng, F Ronzano, J LaFleur, M Garber, R de Oliveira\u2026", "abstract": "Background: The availability of increasingly powerful large language models (LLMs) has attracted substantial interest in their potential for interpreting and generating human-like text for biomedical and clinical applications. However, there are often \u2026"}, {"title": "CALM: Context Augmentation with Large Language Model for Named Entity Recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-72437-4_16", "details": "T Luiggi, T Herserant, T Tran, L Soulier, V Guigue - \u2026 on Theory and Practice of Digital \u2026, 2024", "abstract": "In prior research on Named Entity Recognition (NER), the focus has been on addressing challenges arising from data scarcity and overfitting, particularly in the context of increasingly complex transformer-based architectures. A framework based \u2026"}, {"title": "An Efficient Contrastive Unimodal Pretraining Method for EHR Time Series Data", "link": "https://openreview.net/pdf%3Fid%3DZN5vbwMpgX", "details": "R King, S Kodali, C Krueger, T Yang, BJ Mortazavi - IEEE-EMBS International Conference on \u2026", "abstract": "Machine learning has revolutionized the modeling of clinical timeseries data. Using machine learning, a Deep Neural Network (DNN) can be automatically trained to learn a complex mapping of its input features for a desired task. This is particularly \u2026"}, {"title": "Assessments of Generative AI as Clinical Decision Support Ought to be Incorporated into Randomised Controlled Trials of Electronic Alerts for Acute Kidney Injury", "link": "https://www.mcpdigitalhealth.org/article/S2949-7612\\(24\\)00101-9/fulltext", "details": "DJ Sexton, C Judge - Mayo Clinic Proceedings: Digital Health", "abstract": "Acute Kidney Injury (AKI), characterised by an acute deterioration in kidney function occurs in approximately 25% of hospitalised individuals and is associated with prolonged stay, higher cost and increased morbidity and mortality. 1 Clinical \u2026"}, {"title": "Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination", "link": "https://arxiv.org/pdf/2409.12746", "details": "ES Salido, R Morante, J Gonzalo, G Marco\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this article we present UNED-ACCESS 2024, a bilingual dataset that consists of 1003 multiple-choice questions of university entrance level exams in Spanish and English. Questions are originally formulated in Spanish and translated manually into \u2026"}, {"title": "Larger and more instructable language models become less reliable", "link": "https://www.nature.com/articles/s41586-024-07930-y", "details": "L Zhou, W Schellaert, F Mart\u00ednez-Plumed\u2026 - Nature, 2024", "abstract": "The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources) and bespoke shaping up (including post \u2026"}, {"title": "Addition is All You Need for Energy-efficient Language Models", "link": "https://arxiv.org/pdf/2410.00907%3F", "details": "H Luo, W Sun - arXiv preprint arXiv:2410.00907, 2024", "abstract": "Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear \u2026"}]
