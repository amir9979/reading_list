[{"title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "link": "https://arxiv.org/pdf/2407.21417", "details": "Z Wu, Y Zhang, P Qi, Y Xu, R Han, Y Zhang, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (ie, follow open-ended instructions) and \u2026"}, {"title": "Harnessing the Intrinsic Knowledge of Pretrained Language Models for Challenging Text Classification Settings", "link": "https://arxiv.org/pdf/2408.15650", "details": "L Gao - arXiv preprint arXiv:2408.15650, 2024", "abstract": "Text classification is crucial for applications such as sentiment analysis and toxic text filtering, but it still faces challenges due to the complexity and ambiguity of natural language. Recent advancements in deep learning, particularly transformer \u2026"}, {"title": "Making Long-Context Language Models Better Multi-Hop Reasoners", "link": "https://arxiv.org/pdf/2408.03246", "details": "Y Li, S Liang, MR Lyu, L Wang - arXiv preprint arXiv:2408.03246, 2024", "abstract": "Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased \u2026"}, {"title": "Enhancing Audio-Language Models through Self-Supervised Post-Training with Text-Audio Pairs", "link": "https://arxiv.org/pdf/2408.09269", "details": "A Sinha, C Migozzi, A Rey, C Zhang - arXiv preprint arXiv:2408.09269, 2024", "abstract": "Research on multi-modal contrastive learning strategies for audio and text has rapidly gained interest. Contrastively trained Audio-Language Models (ALMs), such as CLAP, which establish a unified representation across audio and language \u2026"}, {"title": "A neural network approach to predict opioid misuse among previously hospitalized patients using electronic health records", "link": "https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0309424", "details": "L Vega, W Conneen, MA Veronin, RP Schumaker - PLOS ONE, 2024", "abstract": "Can Electronic Health Records (EHR) predict opioid misuse in general patient populations? This research trained three backpropagation neural networks to explore EHR predictors using existing patient data. Model 1 used patient diagnosis \u2026"}, {"title": "Exploring Vision Language Pretraining with Knowledge Enhancement via Large Language Model", "link": "https://link.springer.com/chapter/10.1007/978-3-031-67751-9_7", "details": "C Tung, Y Lin, J Yin, Q Ye, H Chen - International Workshop on Trustworthy Artificial \u2026, 2024", "abstract": "Abstract The integration of Vision-Language Pretraining (VLP) models in the medical field represents a significant advancement in the development of AI-driven diagnostic tools. These models, which learn to understand and generate descriptions of visual \u2026"}, {"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "link": "https://arxiv.org/pdf/2408.00724", "details": "Y Wu, Z Sun, S Li, S Welleck, Y Yang - arXiv preprint arXiv:2408.00724, 2024", "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth \u2026"}, {"title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios", "link": "https://aclanthology.org/2024.findings-acl.230.pdf", "details": "L Lin, J Fu, P Liu, Q Li, Y Gong, J Wan, F Zhang\u2026 - Findings of the Association \u2026, 2024", "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local \u2026"}, {"title": "FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt Optimization Across Tokenizers", "link": "https://arxiv.org/pdf/2408.04816", "details": "JN Williams, JZ Kolter - arXiv preprint arXiv:2408.04816, 2024", "abstract": "The widespread use of large language models has resulted in a multitude of tokenizers and embedding spaces, making knowledge transfer in prompt discovery tasks difficult. In this work, we propose FUSE (Flexible Unification of Semantic \u2026"}]
