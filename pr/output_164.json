'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Absorption Distribution Metabolism Excretion and Toxic'
[{"title": "Ehragent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records", "link": "https://openreview.net/pdf%3Fid%3DZjXEzFE0Qy", "details": "W Shi, R Xu, Y Zhuang, Y Yu, J Zhang, H Wu, Y Zhu\u2026 - ICLR 2024 Workshop on \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent, an LLM agent empowered with \u2026"}, {"title": "Approaching Human-Level Forecasting with Language Models", "link": "https://arxiv.org/pdf/2402.18563", "details": "D Halawi, F Zhang, C Yueh-Han, J Steinhardt - arXiv preprint arXiv:2402.18563, 2024", "abstract": "Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system \u2026"}, {"title": "Self-Training Language Models in Arithmetic Reasoning", "link": "https://openreview.net/pdf%3Fid%3DzBh79GuLNO", "details": "M Kadl\u010d\u00edk, M \u0160tef\u00e1nik, O Sotolar, V Martinek - ICLR 2024 Workshop on Large Language \u2026", "abstract": "Recent work shows impressive efficiency of methods for modeling human preferences but achieving further improvements with these methods requires costly human annotations of the quality of model outputs. In this work, we study the \u2026"}]
