[{"title": "Robust Image Classification in the Presence of Out-of-Distribution and Adversarial Samples Using Attractors in Neural Networks", "link": "https://arxiv.org/pdf/2406.10579", "details": "N Alipour, SA SeyyedSalehi - arXiv preprint arXiv:2406.10579, 2024", "abstract": "The proper handling of out-of-distribution (OOD) samples in deep classifiers is a critical concern for ensuring the suitability of deep neural networks in safety-critical systems. Existing approaches developed for robust OOD detection in the presence of \u2026"}, {"title": "Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation", "link": "https://arxiv.org/pdf/2406.06890", "details": "Y Zhai, K Lin, Z Yang, L Li, J Wang, CC Lin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Image diffusion distillation achieves high-fidelity generation with very few sampling steps. However, applying these techniques directly to video diffusion often results in unsatisfactory frame quality due to the limited visual quality in public video datasets \u2026"}, {"title": "LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices", "link": "https://arxiv.org/pdf/2406.08765", "details": "R Jin, Q Xu, M Wu, Y Xu, D Li, X Li, Z Chen - arXiv preprint arXiv:2406.08765, 2024", "abstract": "Limited by the scale and diversity of time series data, the neural networks trained on time series data often overfit and show unsatisfacotry performances. In comparison, large language models (LLMs) recently exhibit impressive generalization in diverse \u2026"}]
