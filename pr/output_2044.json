[{"title": "TableDC: Deep Clustering for Tabular Data", "link": "https://arxiv.org/pdf/2405.17723", "details": "HT Rauf, A Freitas, NW Paton - arXiv preprint arXiv:2405.17723, 2024", "abstract": "Deep clustering (DC), a fusion of deep representation learning and clustering, has recently demonstrated positive results in data science, particularly text processing and computer vision. However, joint optimization of feature learning and data \u2026"}, {"title": "SCE-MAE: Selective Correspondence Enhancement with Masked Autoencoder for Self-Supervised Landmark Estimation", "link": "https://arxiv.org/pdf/2405.18322", "details": "K Yin, VR Rao, R Jiang, X Liu, P Aarabi, DB Lindell - arXiv preprint arXiv:2405.18322, 2024", "abstract": "Self-supervised landmark estimation is a challenging task that demands the formation of locally distinct feature representations to identify sparse facial landmarks in the absence of annotated data. To tackle this task, existing state-of-the-art (SOTA) \u2026"}, {"title": "FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2405.18218", "details": "Y Zhang, Y Li, X Wang, Q Shen, B Plank, B Bischl\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Overparametrized transformer networks are the state-of-the-art architecture for Large Language Models (LLMs). However, such models contain billions of parameters making large compute a necessity, while raising environmental concerns. To \u2026"}, {"title": "Relational Self-supervised Distillation with Compact Descriptors for Image Copy Detection", "link": "https://arxiv.org/pdf/2405.17928", "details": "J Kim, S Woo, J Nang - arXiv preprint arXiv:2405.17928, 2024", "abstract": "This paper addresses image copy detection, a task in online sharing platforms for copyright protection. While previous approaches have performed exceptionally well, the large size of their networks and descriptors remains a significant disadvantage \u2026"}]
