[{"title": "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "link": "https://arxiv.org/pdf/2410.15096", "details": "OJ Kwon, DE Matsunaga, KE Kim - arXiv preprint arXiv:2410.15096, 2024", "abstract": "A critical component of the current generation of language models is preference alignment, which aims to precisely control the model's behavior to meet human needs and values. The most notable among such methods is Reinforcement \u2026"}, {"title": "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes", "link": "https://arxiv.org/pdf/2410.16930", "details": "BR Christ, Z Gottesman, J Kropko, T Hartvigsen - arXiv preprint arXiv:2410.16930, 2024", "abstract": "Math reasoning is a highly active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be \u2026"}, {"title": "Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop Question Answering", "link": "https://dl.acm.org/doi/pdf/10.1145/3627673.3679722", "details": "Y Shi, Q Tan, X Wu, S Zhong, K Zhou, N Liu - Proceedings of the 33rd ACM \u2026, 2024", "abstract": "Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging \u2026"}, {"title": "Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Models Alignment", "link": "https://arxiv.org/pdf/2410.16714", "details": "M Wang, C Ma, Q Chen, L Meng, Y Han, J Xiao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-play methods have demonstrated remarkable success in enhancing model capabilities across various domains. In the context of Reinforcement Learning from Human Feedback (RLHF), self-play not only boosts Large Language Model (LLM) \u2026"}, {"title": "Language Models-enhanced Semantic Topology Representation Learning For Temporal Knowledge Graph Extrapolation", "link": "https://dl.acm.org/doi/abs/10.1145/3627673.3679602", "details": "T Zhang, T Zheng, Z Xiao, Z Chen, L Li, Z Feng\u2026 - Proceedings of the 33rd \u2026, 2024", "abstract": "Temporal Knowledge Graph (TKG) extrapolation aims to predict future missing facts based on historical information, which has exhibited both semantics and topology of events. The mainstream methods have advanced the prediction performance by \u2026"}, {"title": "Vision Language Model is NOT All You Need: Augmentation Strategies for Molecule Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3627673.3679607", "details": "N Lee, S Laghuvarapu, C Park, J Sun - Proceedings of the 33rd ACM International \u2026, 2024", "abstract": "Recently, there has been a growing interest among researchers in understanding molecules and their textual descriptions through molecule language models (MoLM). However, despite some early promising developments, the advancement of MoLM \u2026"}, {"title": "MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time", "link": "https://arxiv.org/pdf/2410.14184", "details": "M Zhang, P Wang, C Tan, M Huang, D Zhang, Y Zhou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) acquire extensive knowledge and remarkable abilities from extensive text corpora, making them powerful tools for various applications. To make LLMs more usable, aligning them with human preferences is \u2026"}, {"title": "Towards Completeness-Oriented Tool Retrieval for Large Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3627673.3679847", "details": "C Qu, S Dai, X Wei, H Cai, S Wang, D Yin, J Xu\u2026 - Proceedings of the 33rd \u2026, 2024", "abstract": "Recently, integrating external tools with Large Language Models (LLMs) has gained significant attention as an effective strategy to mitigate the limitations inherent in their pre-training data. However, real-world systems often incorporate a wide array of \u2026"}, {"title": "Language Model Non-myopic Generation for Reasoning and Planning", "link": "https://arxiv.org/pdf/2410.17195", "details": "C Ma, H Zhao, J Zhang, J He, L Kong - arXiv preprint arXiv:2410.17195, 2024", "abstract": "Large Language Models have demonstrated remarkable abilities in reasoning and planning by breaking down complex problems into sequential steps. Despite their success in various domains like mathematical problem-solving and coding, LLMs \u2026"}]
