[{"title": "TAP-VL: Text Layout-Aware Pre-training for Enriched Vision-Language Models", "link": "https://arxiv.org/pdf/2411.04642", "details": "J Fhima, EB Avraham, O Nuriel, Y Kittenplon, R Ganz\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-Language (VL) models have garnered considerable research interest; however, they still face challenges in effectively handling text within images. To address this limitation, researchers have developed two approaches. The first \u2026"}, {"title": "BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks", "link": "https://arxiv.org/pdf/2410.20971", "details": "Y Zhao, X Zheng, L Luo, Y Li, X Ma, YG Jiang - arXiv preprint arXiv:2410.20971, 2024", "abstract": "Despite their superb multimodal capabilities, Vision-Language Models (VLMs) have been shown to be vulnerable to jailbreak attacks, which are inference-time attacks that induce the model to output harmful responses with tricky prompts. It is thus \u2026"}, {"title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines", "link": "https://arxiv.org/pdf/2410.21220", "details": "Z Zhang, Y Zhang, X Ding, X Yue - arXiv preprint arXiv:2410.21220, 2024", "abstract": "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This \u2026"}, {"title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "link": "https://arxiv.org/pdf/2410.11829%3F", "details": "Y Cao, Y Liu, Z Chen, G Shi, W Wang, D Zhao, T Lu - arXiv preprint arXiv:2410.11829, 2024", "abstract": "Despite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating \u2026"}, {"title": "Unified Representation of Genomic and Biomedical Concepts through Multi-Task, Multi-Source Contrastive Learning", "link": "https://arxiv.org/pdf/2410.10144", "details": "H Yuan, S Liu, K Cho, K Liao, A Pereira, T Cai - arXiv preprint arXiv:2410.10144, 2024", "abstract": "We introduce GENomic Encoding REpresentation with Language Model (GENEREL), a framework designed to bridge genetic and biomedical knowledge bases. What sets GENEREL apart is its ability to fine-tune language models to infuse \u2026"}, {"title": "BenchmarkCards: Large Language Model and Risk Reporting", "link": "https://arxiv.org/pdf/2410.12974", "details": "A Sokol, N Moniz, E Daly, M Hind, N Chawla - arXiv preprint arXiv:2410.12974, 2024", "abstract": "Large language models (LLMs) offer powerful capabilities but also introduce significant risks. One way to mitigate these risks is through comprehensive pre- deployment evaluations using benchmarks designed to test for specific \u2026"}, {"title": "PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation", "link": "https://arxiv.org/pdf/2411.05085", "details": "DC Castro, A Bustos, S Bannur, SL Hyland, K Bouzid\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Radiology report generation (RRG) aims to create free-text radiology reports from clinical imaging. Grounded radiology report generation (GRRG) extends RRG by including the localisation of individual findings on the image. Currently, there are no \u2026"}, {"title": "Griffon-G: Bridging Vision-Language and Vision-Centric Tasks via Large Multimodal Models", "link": "https://arxiv.org/pdf/2410.16163", "details": "Y Zhan, H Zhao, Y Zhu, F Yang, M Tang, J Wang - arXiv preprint arXiv:2410.16163, 2024", "abstract": "Large Multimodal Models (LMMs) have achieved significant breakthroughs in various vision-language and vision-centric tasks based on auto-regressive modeling. However, these models typically focus on either vision-centric tasks, such as visual \u2026"}, {"title": "A Novel Interpretability Metric for Explaining Bias in Language Models: Applications on Multilingual Models from Southeast Asia", "link": "https://arxiv.org/pdf/2410.15464", "details": "LCL Gamboa, M Lee - arXiv preprint arXiv:2410.15464, 2024", "abstract": "Work on bias in pretrained language models (PLMs) focuses on bias evaluation and mitigation and fails to tackle the question of bias attribution and explainability. We propose a novel metric, the $\\textit {bias attribution score} $, which draws from \u2026"}]
