[{"title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning", "link": "https://arxiv.org/pdf/2503.18013%3F", "details": "Y Zhan, Y Zhu, S Zheng, H Zhao, F Yang, M Tang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision-Language Models (LVLMs) typically follow a two-stage training paradigm-pretraining and supervised fine-tuning. Recently, preference optimization, derived from the language domain, has emerged as an effective post-training \u2026"}, {"title": "Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models", "link": "https://arxiv.org/pdf/2503.15888", "details": "B Bi, S Liu, Y Wang, Y Xu, J Fang, L Mei, X Cheng - arXiv preprint arXiv:2503.15888, 2025", "abstract": "Retrieval-Augmented Generation (RAG) mitigates hallucinations in Large Language Models (LLMs) by integrating external knowledge. However, conflicts between parametric knowledge and retrieved context pose challenges, particularly when \u2026"}, {"title": "Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization", "link": "https://arxiv.org/pdf/2504.12083", "details": "P Sarkar, A Etemad - arXiv preprint arXiv:2504.12083, 2025", "abstract": "Despite recent advances in Large Video Language Models (LVLMs), they still struggle with fine-grained temporal understanding, hallucinate, and often make simple mistakes on even simple video question-answering tasks, all of which pose \u2026"}, {"title": "ELM: Ensemble of Language Models for Predicting Tumor Group from Pathology Reports", "link": "https://arxiv.org/pdf/2503.21800", "details": "L Gondara, J Simkin, S Devji, G Arbour, R Ng - arXiv preprint arXiv:2503.21800, 2025", "abstract": "Population-based cancer registries (PBCRs) face a significant bottleneck in manually extracting data from unstructured pathology reports, a process crucial for tasks like tumor group assignment, which can consume 900 person-hours for approximately \u2026"}, {"title": "Generative Large Language Model\u2014Powered Conversational AI App for Personalized Risk Assessment: Case Study in COVID-19", "link": "https://ai.jmir.org/2025/1/e67363/", "details": "MA Roshani, X Zhou, Y Qiang, S Suresh, S Hicks\u2026 - JMIR AI, 2025", "abstract": "Background: Large language models (LLMs) have demonstrated powerful capabilities in natural language tasks and are increasingly being integrated into health care for tasks like disease risk assessment. Traditional machine learning \u2026"}, {"title": "Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models", "link": "https://arxiv.org/pdf/2504.12315", "details": "X Ji, J Wang, H Zhang, J Zhang, H Zhou, C Sun, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the development of Multimodal Large Language Models (MLLMs), numerous outstanding accomplishments have emerged within the open-source community. Due to the complexity of creating and training multimodal data pairs, it is still a \u2026"}, {"title": "UMIT: Unifying Medical Imaging Tasks via Vision-Language Models", "link": "https://arxiv.org/pdf/2503.15892", "details": "H Yu, S Yi, K Niu, M Zhuo, B Li - arXiv preprint arXiv:2503.15892, 2025", "abstract": "With the rapid advancement of deep learning, particularly in the field of medical image analysis, an increasing number of Vision-Language Models (VLMs) are being widely applied to solve complex health and biomedical challenges. However \u2026"}, {"title": "Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models", "link": "https://arxiv.org/pdf/2503.16779%3F", "details": "M Wu, T Zhu, H Han, X Zhang, W Shao, W Chen - arXiv preprint arXiv:2503.16779, 2025", "abstract": "Tool learning can further broaden the usage scenarios of large language models (LLMs). However most of the existing methods either need to finetune that the model can only use tools seen in the training data, or add tool demonstrations into the \u2026"}, {"title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training", "link": "https://arxiv.org/pdf/2504.13161", "details": "S Diao, Y Yang, Y Fu, X Dong, D Su, M Kliegl, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Pre-training datasets are typically collected from web content and lack inherent domain divisions. For instance, widely used datasets like Common Crawl do not include explicit domain labels, while manually curating labeled datasets such as The \u2026"}]
