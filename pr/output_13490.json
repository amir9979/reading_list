[{"title": "Semantic-aware contrastive learning via multi-prompt alignment", "link": "https://link.springer.com/article/10.1007/s10994-024-06665-1", "details": "Z Zhao, H Qin, M Kong, L Chen, D Xie, J Zhu, Q Zhu - Machine Learning, 2025", "abstract": "The role of the sample generation mechanism in contrastive learning is pivotal. It not only determines the pairings of positive and negative samples but also enriches the diversity of the sample pool, thereby substantially affecting the quality of the learned \u2026"}, {"title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models", "link": "https://arxiv.org/pdf/2502.17387", "details": "A Albalak, D Phung, N Lile, R Rafailov, K Gandhi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Increasing interest in reasoning models has led math to become a prominent testing ground for algorithmic and methodological improvements. However, existing open math datasets either contain a small collection of high-quality, human-written \u2026"}, {"title": "MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning", "link": "https://arxiv.org/pdf/2502.19634", "details": "J Pan, C Liu, J Wu, F Liu, J Zhu, HB Li, C Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show \u2026"}, {"title": "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training", "link": "https://arxiv.org/pdf/2502.11196", "details": "Y Ou, Y Yao, N Zhang, H Jin, J Sun, S Deng, Z Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural \u2026"}, {"title": "Clinical Decision Support for Patient Cases with Asymptomatic Carotid Artery Stenosis Using AI Models and Electronic Medical Records", "link": "https://www.mdpi.com/2308-3425/12/2/61", "details": "M Madison, X Luo, J Silvey, R Brenner\u2026 - Journal of Cardiovascular \u2026, 2025", "abstract": "An artificial intelligence (AI) analysis of electronic medical records (EMRs) was performed to analyze the differences between patients with carotid stenosis who developed symptomatic disease and those who remained asymptomatic. The EMRs \u2026"}, {"title": "HKRG: Hierarchical knowledge integration for radiology report generation", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425002441", "details": "B Wang, P Teng, H Zhang, F Yang, Z Wang, X Yi\u2026 - Expert Systems with \u2026, 2025", "abstract": "Radiology report generation aims to automatically produce coherent, free-text reports that describe the clinical observations of radiographs. Most existing approaches align visual features with full report features to bridge the gap between medical \u2026"}, {"title": "Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement", "link": "https://arxiv.org/pdf/2502.02573%3F", "details": "S Abbasloo - arXiv preprint arXiv:2502.02573, 2025", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem- solving, a crucial, ubiquitous, and complex domain. This paper explores the \u2026"}, {"title": "Omni-DNA: A Unified Genomic Foundation Model for Cross-Modal and Multi-Task Learning", "link": "https://arxiv.org/pdf/2502.03499", "details": "Z Li, V Subasri, Y Shen, D Li, Y Zhao, GB Stan, C Shan - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) demonstrate remarkable generalizability across diverse tasks, yet genomic foundation models (GFMs) still require separate finetuning for each downstream application, creating significant overhead as model \u2026"}, {"title": "Conformal Uncertainty Indicator for Continual Test-Time Adaptation", "link": "https://arxiv.org/pdf/2502.02998", "details": "F Lyu, H Zhao, Z Shi, Y Liu, F Hu, Z Zhang, L Wang - arXiv preprint arXiv:2502.02998, 2025", "abstract": "Continual Test-Time Adaptation (CTTA) aims to adapt models to sequentially changing domains during testing, relying on pseudo-labels for self-adaptation. However, incorrect pseudo-labels can accumulate, leading to performance \u2026"}]
