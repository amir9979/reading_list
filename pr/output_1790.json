[{"title": "A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks", "link": "https://arxiv.org/pdf/2405.10251", "details": "X Ni, P Li - arXiv preprint arXiv:2405.10251, 2024", "abstract": "Recent efforts have evaluated large language models (LLMs) in areas such as commonsense reasoning, mathematical reasoning, and code generation. However, to the best of our knowledge, no work has specifically investigated the performance \u2026"}, {"title": "A Philosophical Introduction to Language Models-Part II: The Way Forward", "link": "https://arxiv.org/pdf/2405.03207", "details": "R Milli\u00e8re, C Buckner - arXiv preprint arXiv:2405.03207, 2024", "abstract": "In this paper, the second of two companion pieces, we explore novel philosophical questions raised by recent progress in large language models (LLMs) that go beyond the classical debates covered in the first part. We focus particularly on issues \u2026"}, {"title": "Analyzing the Robustness of Vision & Language Models", "link": "https://ieeexplore.ieee.org/iel7/6570655/6633080/10526392.pdf", "details": "A Shirnin, N Andreev, S Potapova, E Artemova - IEEE/ACM Transactions on Audio \u2026, 2024", "abstract": "We present an approach to evaluate the robustness of pre-trained vision and language (V&L) models to noise in input data. Given a source image/text, we perturb it using standard computer vision (CV)/natural language processing (NLP) \u2026"}, {"title": "SecureLLM: Using Compositionality to Build Provably Secure Language Models for Private, Sensitive, and Secret Data", "link": "https://arxiv.org/pdf/2405.09805", "details": "A Alabdulakreem, CM Arnold, Y Lee, PM Feenstra\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Traditional security mechanisms isolate resources from users who should not access them. We reflect the compositional nature of such security mechanisms back into the structure of LLMs to build a provably secure LLM; that we term SecureLLM. Other \u2026"}, {"title": "Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization", "link": "https://arxiv.org/pdf/2405.10616", "details": "Y Ji, Y Xiang, J Li, W Chen, Z Liu, K Chen, M Zhang - arXiv preprint arXiv:2405.10616, 2024", "abstract": "In recent years, large language models (LLMs) have driven advances in natural language processing. Still, their growing scale has increased the computational burden, necessitating a balance between efficiency and performance. Low-rank \u2026"}, {"title": "UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language Models", "link": "https://arxiv.org/pdf/2405.10311", "details": "S Sharifymoghaddam, S Upadhyay, W Chen, J Lin - arXiv preprint arXiv:2405.10311, 2024", "abstract": "Recently, Multi-Modal (MM) Large Language Models (LLMs) have unlocked many complex use-cases that require MM understanding (eg, image captioning or visual question answering) and MM generation (eg, text-guided image generation or \u2026"}, {"title": "Layer-Condensed KV Cache for Efficient Inference of Large Language Models", "link": "https://arxiv.org/pdf/2405.10637", "details": "H Wu, K Tu - arXiv preprint arXiv:2405.10637, 2024", "abstract": "Huge memory consumption has been a major bottleneck for deploying high- throughput large language models in real-world applications. In addition to the large number of parameters, the key-value (KV) cache for the attention mechanism in the \u2026"}, {"title": "MBIAS: Mitigating Bias in Large Language Models While Retaining Context", "link": "https://arxiv.org/pdf/2405.11290", "details": "S Raza, A Raval, V Chatrath - arXiv preprint arXiv:2405.11290, 2024", "abstract": "In addressing the critical need for safety in Large Language Models (LLMs), it is crucial to ensure that the outputs are not only safe but also retain their contextual accuracy. Many existing LLMs are safe fine-tuned either with safety demonstrations \u2026"}, {"title": "SpeechVerse: A Large-scale Generalizable Audio Language Model", "link": "https://arxiv.org/pdf/2405.08295", "details": "N Das, S Dingliwal, S Ronanki, R Paturi, D Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have shown incredible proficiency in performing tasks that require semantic understanding of natural language instructions. Recently, many works have further expanded this capability to perceive multimodal audio and \u2026"}]
