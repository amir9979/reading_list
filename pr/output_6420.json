[{"title": "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models", "link": "https://arxiv.org/pdf/2408.09053", "details": "V Araujo, MF Moens, T Tuytelaars - arXiv preprint arXiv:2408.09053, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) methods are increasingly used with pre- trained language models (PLMs) for continual learning (CL). These methods involve training a PEFT module for each new task and using similarity-based selection to \u2026"}, {"title": "DP-FedEwc: Differentially private federated elastic weight consolidation for model personalization", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124010359", "details": "J Liang, S Su - Knowledge-Based Systems, 2024", "abstract": "Federated learning (FL) has become a prevalent paradigm for training a model collaboratively on multiple clients with the coordination of a central server. As traditional FL suffers from client-drift due to data heterogeneity across clients, many \u2026"}, {"title": "Latent 3D Brain MRI Counterfactual", "link": "https://arxiv.org/pdf/2409.05585", "details": "W Peng, T Xia, FDS Ribeiro, T Bosschieter, E Adeli\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The number of samples in structural brain MRI studies is often too small to properly train deep learning models. Generative models show promise in addressing this issue by effectively learning the data distribution and generating high-fidelity MRI \u2026"}, {"title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.06663", "details": "K Sun, M Dredze - arXiv preprint arXiv:2408.06663, 2024", "abstract": "The development of large language models leads to the formation of a pre-train-then- align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream \u2026"}, {"title": "KiL 2024: 4th International Workshop on Knowledge-infused Learning (Towards Consistent, Reliable, Explainable, and Safe LLMs)", "link": "https://dl.acm.org/doi/abs/10.1145/3637528.3671495", "details": "M Gaur, E Tsamoura, E Raff, N Vedula\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "The Knowledge-infused Learning Workshop is a recurring event in ACM's KDD Conference that gathers the research community on knowledge graphs and knowledge-enabled learning, grounded neurosymbolic AI, explainable and safe AI \u2026"}, {"title": "(Un) explainable Technology", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DeNUdEQAAQBAJ%26oi%3Dfnd%26pg%3DPR7%26ots%3Dk6CBDScH16%26sig%3Dm6k2PC0nWHWhQQo0sOBO7t5paLc", "details": "H Kempt", "abstract": "This book project started as a continuation and summarization of my PhD thesis in which I developed a pragmatic perspective on the role of explainability in medical decision-making. However, as I noticed how this topic cannot be fully appreciated \u2026"}, {"title": "Unraveling the Inner Workings of Massive Language Models: Architecture, Training, and Linguistic Capacities", "link": "https://www.igi-global.com/chapter/unraveling-the-inner-workings-of-massive-language-models/354398", "details": "CVS Babu, CSA Anniyappa - Challenges in Large Language Model Development \u2026, 2024", "abstract": "This study explores the evolution of language models, emphasizing the shift from traditional statistical methods to advanced neural networks, particularly the transformer architecture. It aims to understand the impact of these advancements on \u2026"}, {"title": "FactorLLM: Factorizing Knowledge via Mixture of Experts for Large Language Models", "link": "https://arxiv.org/pdf/2408.11855", "details": "Z Zhao, M Dong, R Zhang, W Zheng, Y Zhang, H Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has demonstrated that Feed-Forward Networks (FFNs) in Large Language Models (LLMs) play a pivotal role in storing diverse linguistic and factual knowledge. Conventional methods frequently face challenges due to knowledge \u2026"}, {"title": "Large Language Models Prompting With Episodic Memory", "link": "https://arxiv.org/pdf/2408.07465", "details": "D Do, Q Tran, S Venkatesh, H Le - arXiv preprint arXiv:2408.07465, 2024", "abstract": "Prompt optimization is essential for enhancing the performance of Large Language Models (LLMs) in a range of Natural Language Processing (NLP) tasks, particularly in scenarios of few-shot learning where training examples are incorporated directly \u2026"}]
