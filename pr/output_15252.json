[{"title": "VERO: Verification and Zero-Shot Feedback Acquisition for Few-Shot Multimodal Aspect-Level Sentiment Classification", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/34707/36862", "details": "K Sun, H Wu, B Shi, S Mensah, P Liu, B Dong - \u2026 of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Deep learning approaches for multimodal aspect-level sentiment classification (MALSC) often require extensive data, which is costly and time-consuming to obtain. To mitigate this, current methods typically fine-tune small-scale pretrained models \u2026"}, {"title": "Teacher privileged distillation: How to deal with imperfect teachers?", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125003855", "details": "M Mart\u00ednez-Garc\u00eda, I Inza, JA Lozano - Knowledge-Based Systems, 2025", "abstract": "The paradigm of learning using privileged information leverages privileged features present at training time, but not at prediction, as additional training information. The privileged learning process is addressed through a knowledge distillation \u2026"}, {"title": "Timecma: Towards llm-empowered multivariate time series forecasting via cross-modality alignment", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/34067/36222", "details": "C Liu, Q Xu, H Miao, S Yang, L Zhang, C Long, Z Li\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "Multivariate time series forecasting (MTSF) aims to learn temporal dynamics among variables to forecast future time series. Existing statistical and deep learning-based methods suffer from limited learnable parameters and small-scale training data \u2026"}, {"title": "High Quality Diffusion Distillation on a Single GPU with Relative and Absolute Position Matching", "link": "https://arxiv.org/pdf/2503.20744%3F", "details": "G Zhang, K Niwa, JP Lewis, C Mesnage, WB Kleijn - arXiv preprint arXiv:2503.20744, 2025", "abstract": "We introduce relative and absolute position matching (RAPM), a diffusion distillation method resulting in high quality generation that can be trained efficiently on a single GPU. Recent diffusion distillation research has achieved excellent results for high \u2026"}, {"title": "Hardware-Aware Iterative One-Shot Neural Architecture Search with Adaptable Knowledge Distillation for Efficient Edge Computing", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10938148.pdf", "details": "OTC Chen, YX Chang, CY Chung, YY Cheng, MH Ha - IEEE Access, 2025", "abstract": "The growing demand for edge applications calls for efficient and optimized deep neural network models. Neural Architecture Search (NAS) is instrumental in designing such models, but achieving optimal architectures quickly remains a key \u2026"}, {"title": "TiTAD: Time-Invariant Transformer for Multivariate Time Series Anomaly Detection", "link": "https://www.mdpi.com/2079-9292/14/7/1401", "details": "Y Liu, W Wang, Y Wu - Electronics, 2025", "abstract": "Anomaly detection in multivariate time series data is critical for industrial sectors such as manufacturing and aerospace. While existing methods have achieved notable success in specific scenarios, they often narrowly focus on either the temporal or \u2026"}, {"title": "Language Model Uncertainty Quantification with Attention Chain", "link": "https://arxiv.org/pdf/2503.19168", "details": "Y Li, R Qiang, L Moukheiber, C Zhang - arXiv preprint arXiv:2503.19168, 2025", "abstract": "Accurately quantifying a large language model's (LLM) predictive uncertainty is crucial for judging the reliability of its answers. While most existing research focuses on short, directly answerable questions with closed-form outputs (eg, multiple \u2026"}, {"title": "Scale-wise Distillation of Diffusion Models", "link": "https://arxiv.org/pdf/2503.16397%3F", "details": "N Starodubcev, D Kuznedelev, A Babenko\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present SwD, a scale-wise distillation framework for diffusion models (DMs), which effectively employs next-scale prediction ideas for diffusion-based few-step generators. In more detail, SwD is inspired by the recent insights relating diffusion \u2026"}, {"title": "Towards Efficient and General-Purpose Few-Shot Misclassification Detection for Vision-Language Models", "link": "https://arxiv.org/pdf/2503.20492", "details": "F Zeng, Z Cheng, F Zhu, XY Zhang - arXiv preprint arXiv:2503.20492, 2025", "abstract": "Reliable prediction by classifiers is crucial for their deployment in high security and dynamically changing situations. However, modern neural networks often exhibit overconfidence for misclassified predictions, highlighting the need for confidence \u2026"}]
