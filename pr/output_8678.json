[{"title": "MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models", "link": "https://arxiv.org/pdf/2410.13085", "details": "P Xia, K Zhu, H Li, T Wang, W Shi, S Wang, L Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for \u2026"}, {"title": "Mapping Bias in Vision Language Models: Signposts, Pitfalls, and the Road Ahead", "link": "https://arxiv.org/pdf/2410.13146", "details": "K Sasse, S Chen, J Pond, D Bitterman, J Osborne - arXiv preprint arXiv:2410.13146, 2024", "abstract": "As Vision Language Models (VLMs) gain widespread use, their fairness remains under-explored. In this paper, we analyze demographic biases across five models and six datasets. We find that portrait datasets like UTKFace and CelebA are the best \u2026"}, {"title": "Effective and Efficient Adversarial Detection for Vision-Language Models via A Single Vector", "link": "https://arxiv.org/pdf/2410.22888", "details": "Y Huang, F Zhu, J Tang, P Zhou, W Lei, J Lv, TS Chua - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual Language Models (VLMs) are vulnerable to adversarial attacks, especially those from adversarial images, which is however under-explored in literature. To facilitate research on this critical safety problem, we first construct a new laRge-scale \u2026"}, {"title": "Unsupervised SapBERT-based bi-encoders for medical concept annotation of clinical narratives with SNOMED CT", "link": "https://journals.sagepub.com/doi/pdf/10.1177/20552076241288681", "details": "A Abdulnazar, R Roller, S Schulz, M Kreuzthaler - Digital Health, 2024", "abstract": "Objective Clinical narratives provide comprehensive patient information. Achieving interoperability involves mapping relevant details to standardized medical vocabularies. Typically, natural language processing divides this task into named \u2026"}, {"title": "Drivedreamer4d: World models are effective data machines for 4d driving scene representation", "link": "https://arxiv.org/pdf/2410.13571", "details": "G Zhao, C Ni, X Wang, Z Zhu, G Huang, X Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Closed-loop simulation is essential for advancing end-to-end autonomous driving systems. Contemporary sensor simulation methods, such as NeRF and 3DGS, rely predominantly on conditions closely aligned with training data distributions, which \u2026"}, {"title": "Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?", "link": "https://arxiv.org/pdf/2410.13523", "details": "C Liu, Z Wan, H Wang, Y Chen, T Qaiser, C Jin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical Vision-Language Pre-training (MedVLP) has made significant progress in enabling zero-shot tasks for medical image understanding. However, training MedVLP models typically requires large-scale datasets with paired, high-quality \u2026"}, {"title": "MWVOS: Mask-Free Weakly Supervised Video Object Segmentation via promptable foundation model", "link": "https://www.sciencedirect.com/science/article/pii/S0031320324008513", "details": "Z Zhang, S Zhang, Z Dai, Z Dong, S Zhu - Pattern Recognition, 2024", "abstract": "The current state-of-the-art techniques for video object segmentation necessitate extensive training on video datasets with mask annotations, thereby constraining their ability to transfer zero-shot learning to new image distributions and tasks \u2026"}, {"title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "link": "https://arxiv.org/pdf/2410.11829%3F", "details": "Y Cao, Y Liu, Z Chen, G Shi, W Wang, D Zhao, T Lu - arXiv preprint arXiv:2410.11829, 2024", "abstract": "Despite significant advancements in Multimodal Large Language Models (MLLMs) for understanding complex human intentions through cross-modal interactions, capturing intricate image details remains challenging. Previous methods integrating \u2026"}, {"title": "Zero-shot Action Localization via the Confidence of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.14340%3F", "details": "J Aklilu, X Wang, S Yeung-Levy - arXiv preprint arXiv:2410.14340, 2024", "abstract": "Precise action localization in untrimmed video is vital for fields such as professional sports and minimally invasive surgery, where the delineation of particular motions in recordings can dramatically enhance analysis. But in many cases, large scale \u2026"}]
