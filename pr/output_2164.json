[{"title": "Enhancing Large Vision Language Models with Self-Training on Image Comprehension", "link": "https://arxiv.org/pdf/2405.19716", "details": "Y Deng, P Lu, F Yin, Z Hu, S Shen, J Zou, KW Chang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision language models (LVLMs) integrate large language models (LLMs) with pre-trained vision encoders, thereby activating the perception capability of the model to understand image inputs for different queries and conduct subsequent reasoning \u2026"}, {"title": "Language Models Need Inductive Biases to Count Inductively", "link": "https://arxiv.org/pdf/2405.20131", "details": "Y Chang, Y Bisk - arXiv preprint arXiv:2405.20131, 2024", "abstract": "Counting is a fundamental example of generalization, whether viewed through the mathematical lens of Peano's axioms defining the natural numbers or the cognitive science literature for children learning to count. The argument holds for both cases \u2026"}, {"title": "A Systematic Analysis on the Temporal Generalization of Language Models in Social Media", "link": "https://arxiv.org/pdf/2405.13017", "details": "A Ushio, J Camacho-Collados - arXiv preprint arXiv:2405.13017, 2024", "abstract": "In machine learning, temporal shifts occur when there are differences between training and test splits in terms of time. For streaming data such as news or social media, models are commonly trained on a fixed corpus from a certain period of time \u2026"}, {"title": "Xwin-LM: Strong and Scalable Alignment Practice for LLMs", "link": "https://arxiv.org/pdf/2405.20335", "details": "B Ni, JC Hu, Y Wei, H Peng, Z Zhang, G Meng, H Hu - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we present Xwin-LM, a comprehensive suite of alignment methodologies for large language models (LLMs). This suite encompasses several key techniques, including supervised finetuning (SFT), reward modeling (RM) \u2026"}, {"title": "MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning", "link": "https://arxiv.org/pdf/2405.05189", "details": "I Nair, L Wang - arXiv preprint arXiv:2405.05189, 2024", "abstract": "We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs). Previous approaches have explored various prompting schemes, yet they suffer from error \u2026"}, {"title": "Zero-shot LLM-guided Counterfactual Generation for Text", "link": "https://arxiv.org/pdf/2405.04793", "details": "A Bhattacharjee, R Moraffah, J Garland, H Liu - arXiv preprint arXiv:2405.04793, 2024", "abstract": "Counterfactual examples are frequently used for model development and evaluation in many natural language processing (NLP) tasks. Although methods for automated counterfactual generation have been explored, such methods depend on models \u2026"}, {"title": "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "link": "https://arxiv.org/pdf/2405.07551", "details": "S Yin, W You, Z Ji, G Zhong, J Bai - arXiv preprint arXiv:2405.07551, 2024", "abstract": "The tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs, while tool-free methods chose another track: augmenting math \u2026"}, {"title": "Backdoor Removal for Generative Large Language Models", "link": "https://arxiv.org/pdf/2405.07667", "details": "H Li, Y Chen, Z Zheng, Q Hu, C Chan, H Liu, Y Song - arXiv preprint arXiv \u2026, 2024", "abstract": "With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased \u2026"}, {"title": "Towards Better Vision-Inspired Vision-Language Models", "link": "https://www.lamda.nju.edu.cn/caoyh/files/VIVL.pdf", "details": "YH Cao, K Ji, Z Huang, C Zheng, J Liu, J Wang, J Chen\u2026", "abstract": "Vision-language (VL) models have achieved unprecedented success recently, in which the connection module is the key to bridge the modality gap. Nevertheless, the abundant visual clues are not sufficiently exploited in most existing methods. On the \u2026"}]
