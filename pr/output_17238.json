[{"title": "Hybrid Concept Bottleneck Models", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Hybrid_Concept_Bottleneck_Models_CVPR_2025_paper.pdf", "details": "Y Liu, T Zhang, S Gu - Proceedings of the Computer Vision and Pattern \u2026, 2025", "abstract": "Abstract Concept Bottleneck Models (CBMs) provide an interpretable framework for neural networks by mapping visual features to predefined, human-understandable concepts. However, the application of CBMs is often constrained by insufficient \u2026"}, {"title": "FSFM: A Generalizable Face Security Foundation Model via Self-Supervised Facial Representation Learning Supplementary Material", "link": "https://openaccess.thecvf.com/content/CVPR2025/supplemental/Wang_FSFM_A_Generalizable_CVPR_2025_supplemental.pdf", "details": "G Wang, F Lin, T Wu, Z Liu, Z Ba, K Ren", "abstract": "This supplementary material provides additional insights, details, and results to support our FSFM framework comprehensively, structured as follows:\u2022 Facial Masking Strategies in Masked Image Modeling (MIM)(Sec. B): We delve into the \u2026"}, {"title": "3D-MVP: 3D Multiview Pretraining for Manipulation", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Qian_3D-MVP_3D_Multiview_Pretraining_for_Manipulation_CVPR_2025_paper.pdf", "details": "S Qian, K Mo, V Blukis, DF Fouhey, D Fox, A Goyal - Proceedings of the Computer \u2026, 2025", "abstract": "Recent works have shown that visual pretraining on egocentric datasets using masked autoencoders (MAE) can improve generalization for downstream robotics tasks. However, these approaches pretrain only on 2D images, while many robotics \u2026"}, {"title": "Closest Neighbors are Harmful for Lightweight Masked Auto-encoders", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Meng_Closest_Neighbors_are_Harmful_for_Lightweight_Masked_Auto-encoders_CVPR_2025_paper.pdf", "details": "J Meng, A Hasssan, L Yang, D Fan, J Shin, J Seo - \u2026 of the Computer Vision and Pattern \u2026, 2025", "abstract": "Learning the visual representation via masked auto-encoder (MAE) training has been proven to be a powerful technique. Transferring the pre-trained vision transformer (ViT) to downstream tasks leads to superior performance compared to \u2026"}, {"title": "LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection", "link": "https://arxiv.org/pdf/2505.12507", "details": "X Zheng, Z Chen, E Schafir, S Chen, HA Salehi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The impressive ability of large language models to generate natural text across various tasks has led to critical challenges in authorship authentication. Although numerous detection methods have been developed to differentiate between machine \u2026", "entry_id": "http://arxiv.org/abs/2505.12507v1", "updated": "2025-05-18 17:55:45", "published": "2025-05-18 17:55:45", "authors": "Xu Zheng;Zhuomin Chen;Esteban Schafir;Sipeng Chen;Hojat Allah Salehi;Haifeng Chen;Farhad Shirani;Wei Cheng;Dongsheng Luo", "summary": "The impressive ability of large language models to generate natural text\nacross various tasks has led to critical challenges in authorship\nauthentication. Although numerous detection methods have been developed to\ndifferentiate between machine-generated texts (MGT) and human-generated texts\n(HGT), the explainability of these methods remains a significant gap.\nTraditional explainability techniques often fall short in capturing the complex\nword relationships that distinguish HGT from MGT. To address this limitation,\nwe present LM$^2$otifs, a novel explainable framework for MGT detection.\nInspired by probabilistic graphical models, we provide a theoretical rationale\nfor the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks\nto achieve both accurate detection and interpretability. The LM$^2$otifs\npipeline operates in three key stages: first, it transforms text into graphs\nbased on word co-occurrence to represent lexical dependencies; second, graph\nneural networks are used for prediction; and third, a post-hoc explainability\nmethod extracts interpretable motifs, offering multi-level explanations from\nindividual words to sentence structures. Extensive experiments on multiple\nbenchmark datasets demonstrate the comparable performance of LM$^2$otifs. The\nempirical evaluation of the extracted explainable motifs confirms their\neffectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis\nreveals distinct and visible linguistic fingerprints characteristic of MGT.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.CY", "links": "http://arxiv.org/abs/2505.12507v1;http://arxiv.org/pdf/2505.12507v1", "pdf_url": "http://arxiv.org/pdf/2505.12507v1"}, {"title": "Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models", "link": "https://arxiv.org/pdf/2506.01334", "details": "Y Jiang, D Mehta, W Feng, Z Ge - arXiv preprint arXiv:2506.01334, 2025", "abstract": "Concept Bottleneck Models (CBMs) decompose image classification into a process governed by interpretable, human-readable concepts. Recent advances in CBMs have used Large Language Models (LLMs) to generate candidate concepts \u2026", "entry_id": "http://arxiv.org/abs/2506.01334v1", "updated": "2025-06-02 05:25:52", "published": "2025-06-02 05:25:52", "authors": "Yiwen Jiang;Deval Mehta;Wei Feng;Zongyuan Ge", "summary": "Concept Bottleneck Models (CBMs) decompose image classification into a\nprocess governed by interpretable, human-readable concepts. Recent advances in\nCBMs have used Large Language Models (LLMs) to generate candidate concepts.\nHowever, a critical question remains: What is the optimal number of concepts to\nuse? Current concept banks suffer from redundancy or insufficient coverage. To\naddress this issue, we introduce a dynamic, agent-based approach that adjusts\nthe concept bank in response to environmental feedback, optimizing the number\nof concepts for sufficiency yet concise coverage. Moreover, we propose\nConditional Concept Bottleneck Models (CoCoBMs) to overcome the limitations in\ntraditional CBMs' concept scoring mechanisms. It enhances the accuracy of\nassessing each concept's contribution to classification tasks and feature an\neditable matrix that allows LLMs to correct concept scores that conflict with\ntheir internal knowledge. Our evaluations across 6 datasets show that our\nmethod not only improves classification accuracy by 6% but also enhances\ninterpretability assessments by 30%.", "comment": "Accepted at ACL 2025 (Main)", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.01334v1;http://arxiv.org/pdf/2506.01334v1", "pdf_url": "http://arxiv.org/pdf/2506.01334v1"}, {"title": "Language-Assisted Debiasing and Smoothing for Foundation Model-Based Semi-Supervised Learning", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Zheng_Language-Assisted_Debiasing_and_Smoothing_for_Foundation_Model-Based_Semi-Supervised_Learning_CVPR_2025_paper.pdf", "details": "N Zheng, X Song, X Dong, AN Ghosh, L Nie\u2026 - Proceedings of the \u2026, 2025", "abstract": "Recent studies have focused on introducing pre-trained foundation models into semi- supervised learning (SSL) tasks. Nevertheless, these foundation models can exhibit biases toward different classes and tend to generate imbalanced pseudo-labels for \u2026"}, {"title": "A Unified Latent Schrodinger Bridge Diffusion Model for Unsupervised Anomaly Detection and Localization", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Akshay_A_Unified_Latent_Schrodinger_Bridge_Diffusion_Model_for_Unsupervised_Anomaly_CVPR_2025_paper.pdf", "details": "S Akshay, NL Narasimhan, J George\u2026 - Proceedings of the \u2026, 2025", "abstract": "Anomaly detection and localization remain pivotal challenges in computer vision, with applications ranging from industrial inspection to medical diagnostics. While current supervised methods offer high precision, they are often impractical due to the \u2026"}, {"title": "Fine-Grained ECG-Text Contrastive Learning via Waveform Understanding Enhancement", "link": "https://arxiv.org/pdf/2505.11939", "details": "H Li, C Liu, Z Ding, Z Liu, Z Huang - arXiv preprint arXiv:2505.11939, 2025", "abstract": "Electrocardiograms (ECGs) are essential for diagnosing cardiovascular diseases. While previous ECG-text contrastive learning methods have shown promising results, they often overlook the incompleteness of the reports. Given an ECG, the \u2026", "entry_id": "http://arxiv.org/abs/2505.11939v1", "updated": "2025-05-17 10:03:06", "published": "2025-05-17 10:03:06", "authors": "Haitao Li;Che Liu;Zhengyao Ding;Ziyi Liu;Zhengxing Huang", "summary": "Electrocardiograms (ECGs) are essential for diagnosing cardiovascular\ndiseases. While previous ECG-text contrastive learning methods have shown\npromising results, they often overlook the incompleteness of the reports. Given\nan ECG, the report is generated by first identifying key waveform features and\nthen inferring the final diagnosis through these features. Despite their\nimportance, these waveform features are often not recorded in the report as\nintermediate results. Aligning ECGs with such incomplete reports impedes the\nmodel's ability to capture the ECG's waveform features and limits its\nunderstanding of diagnostic reasoning based on those features. To address this,\nwe propose FG-CLEP (Fine-Grained Contrastive Language ECG Pre-training), which\naims to recover these waveform features from incomplete reports with the help\nof large language models (LLMs), under the challenges of hallucinations and the\nnon-bijective relationship between waveform features and diagnoses.\nAdditionally, considering the frequent false negatives due to the prevalence of\ncommon diagnoses in ECGs, we introduce a semantic similarity matrix to guide\ncontrastive learning. Furthermore, we adopt a sigmoid-based loss function to\naccommodate the multi-label nature of ECG-related tasks. Experiments on six\ndatasets demonstrate that FG-CLEP outperforms state-of-the-art methods in both\nzero-shot prediction and linear probing across these datasets.", "comment": null, "journal_ref": null, "primary_category": "eess.SP", "categories": "eess.SP;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.11939v1;http://arxiv.org/pdf/2505.11939v1", "pdf_url": "http://arxiv.org/pdf/2505.11939v1"}]
