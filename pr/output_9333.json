[{"title": "Teaching Small Language Models Reasoning through Counterfactual Distillation", "link": "https://aclanthology.org/2024.emnlp-main.333.pdf", "details": "T Feng, Y Li, L Chenglin, H Chen, F Yu, Y Zhang - Proceedings of the 2024 \u2026, 2024", "abstract": "With the rise of large language models (LLMs), many studies are interested in transferring the reasoning capabilities of LLMs to small language models (SLMs). Previous distillation methods usually utilize the capabilities of LLMs to generate \u2026"}]
