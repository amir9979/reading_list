[{"title": "Cognitive Network Evaluation Toolkit for Medical Domains", "link": "https://link.springer.com/chapter/10.1007/978-3-031-90174-4_11", "details": "DP Panagoulias, GA Tsihrintzis, M Virvou - Artificial Intelligence-Empowered Bio \u2026, 2025", "abstract": "\u2026 To address the lack of or the limited availability of independent, free to use and ready to assess datasets for **LLM** **evaluation** in the medical domain [12], we have constructed Cognitive Network Evaluation Toolkit for Medical Domains (COGNET-MD) \u2026"}, {"title": "Enterprise Large Language Model Evaluation Benchmark", "link": "https://arxiv.org/pdf/2506.20274", "details": "L Wang, D Yi, D Jose, J Passarelli, J Gao, J Leventis\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) ) have demonstrated promise in boosting productivity across AI-powered tools, yet existing benchmarks like Massive Multitask Language Understanding (MMLU) inadequately assess enterprise-specific task \u2026", "entry_id": "http://arxiv.org/abs/2506.20274v1", "updated": "2025-06-25 09:34:25", "published": "2025-06-25 09:34:25", "authors": "Liya Wang;David Yi;Damien Jose;John Passarelli;James Gao;Jordan Leventis;Kang Li", "summary": "Large Language Models (LLMs) ) have demonstrated promise in boosting\nproductivity across AI-powered tools, yet existing benchmarks like Massive\nMultitask Language Understanding (MMLU) inadequately assess enterprise-specific\ntask complexities. We propose a 14-task framework grounded in Bloom's Taxonomy\nto holistically evaluate LLM capabilities in enterprise contexts. To address\nchallenges of noisy data and costly annotation, we develop a scalable pipeline\ncombining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented\ngeneration (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six\nleading models shows open-source contenders like DeepSeek R1 rival proprietary\nmodels in reasoning tasks but lag in judgment-based scenarios, likely due to\noverthinking. Our benchmark reveals critical enterprise performance gaps and\noffers actionable insights for model optimization. This work provides\nenterprises a blueprint for tailored evaluations and advances practical LLM\ndeployment.", "comment": "Submitted to MLNLP 2025 at https://csity2025.org/mlnlp/index", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2506.20274v1;http://arxiv.org/pdf/2506.20274v1", "pdf_url": "http://arxiv.org/pdf/2506.20274v1"}, {"title": "Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios", "link": "https://arxiv.org/pdf/2506.20384", "details": "D Ivry, O Nahum - arXiv preprint arXiv:2506.20384, 2025", "abstract": "This paper introduces two significant contributions to address the issue of grounding claims in a given context. Grounding means that given a context (document) and a claim, there's at least one supportive evidence for the claim in the document. We will \u2026", "entry_id": "http://arxiv.org/abs/2506.20384v1", "updated": "2025-06-25 12:50:28", "published": "2025-06-25 12:50:28", "authors": "Dror Ivry;Oran Nahum", "summary": "This paper introduces two significant contributions to address the issue of\ngrounding claims in a given context. Grounding means that given a context\n(document) and a claim, there's at least one supportive evidence for the claim\nin the document. We will introduce Paladin-mini, a compact (3.8B parameters)\nopen-source classifier model (used for labeling data as grounded or ungrounded)\nengineered for robust performance in real-world scenarios, and the\ngrounding-benchmark, a new evaluation dataset designed to assess performance on\ncritical reasoning tasks. We'll also demonstrate the results of Paladin-mini\nwith benchmarks against the current State-of-the-art and share clear and\nreproducible results.", "comment": "6 pages, 2 figures", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2506.20384v1;http://arxiv.org/pdf/2506.20384v1", "pdf_url": "http://arxiv.org/pdf/2506.20384v1"}, {"title": "QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges", "link": "https://arxiv.org/pdf/2506.20008", "details": "A Basit, M Shao, H Asif, N Innan, M Kashif, A Marchisio\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated strong potential in code generation, yet their effectiveness in quantum computing remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum code \u2026", "entry_id": "http://arxiv.org/abs/2506.20008v1", "updated": "2025-06-24 20:54:56", "published": "2025-06-24 20:54:56", "authors": "Abdul Basit;Minghao Shao;Haider Asif;Nouhaila Innan;Muhammad Kashif;Alberto Marchisio;Muhammad Shafique", "summary": "Recent advances in Large Language Models (LLMs) have demonstrated strong\npotential in code generation, yet their effectiveness in quantum computing\nremains underexplored. This paper benchmarks LLMs for PennyLane-based quantum\ncode generation using real-world challenges from the Quantum Hackathon (QHack).\nWe introduce QHackBench, a novel benchmark dataset derived from QHack\ncompetitions, and evaluate model performance under vanilla prompting and\nRetrieval-Augmented Generation (RAG). Our structured evaluation framework\nassesses functional correctness, syntactic validity, and execution success\nacross varying challenge difficulties. Results indicate that RAG-enhanced\nmodels, supplemented with an augmented PennyLane dataset, approximately\ngenerate similar results as the standard prompting, particularly in complex\nquantum algorithms. Additionally, we introduce a multi-agent evaluation\npipeline that iteratively refines incorrect solutions, further enhancing\nexecution success rates. To foster further research, we commit to publicly\nreleasing QHackBench, along with our evaluation framework and experimental\nresults, enabling continued advancements in AI-assisted quantum programming.", "comment": "8 pages, 6 figures, 3 tables, submitted to QAI 2025", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.PL;cs.SE;68T50, 81P68, 68T07, 68T20;I.2.7; I.2.2", "links": "http://arxiv.org/abs/2506.20008v1;http://arxiv.org/pdf/2506.20008v1", "pdf_url": "http://arxiv.org/pdf/2506.20008v1"}, {"title": "Benchmarking LLMs for Environmental Review and Permitting", "link": "https://www.pnnl.gov/sites/default/files/media/file/Meyur_MAPLE_and_NEPAQuAD_Paper_Published.pdf", "details": "R Meyur, H Phan, K Hayashi, I Stewart, S Sharma\u2026 - 2025", "abstract": "\u2026 More recently, complementary approaches to tackle different aspects of **LLM** **evaluation** have emerged [22, 42]. Efforts to standardize evaluation processes have produced several notable toolkits, including Evalverse [23], FMEval [11], and \u2026"}, {"title": "Can LLMs Replace Humans During Code Chunking?", "link": "https://arxiv.org/pdf/2506.19897", "details": "C Glasz, E Escamilla, EO Scott, A Patel, J Zimmer\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 In general, **LLM** **evaluation** studies show that these models are able to recall and leverage diverse informational content in very long documents [12]. For particular operational tasks, however, it is not clear that simply feeding LLMs as much context \u2026", "entry_id": "http://arxiv.org/abs/2506.19897v1", "updated": "2025-06-24 13:02:35", "published": "2025-06-24 13:02:35", "authors": "Christopher Glasz;Emily Escamilla;Eric O. Scott;Anand Patel;Jacob Zimmer;Colin Diggs;Michael Doyle;Scott Rosen;Nitin Naik;Justin F. Brunelle;Samruddhi Thaker;Parthav Poudel;Arun Sridharan;Amit Madan;Doug Wendt;William Macke;Thomas Schill", "summary": "Large language models (LLMs) have become essential tools in computer science,\nespecially for tasks involving code understanding and generation. However,\nexisting work does not address many of the unique challenges presented by code\nwritten for government applications. In particular, government enterprise\nsoftware is often written in legacy languages like MUMPS or assembly language\ncode (ALC) and the overall token lengths of these systems exceed the context\nwindow size for current commercially available LLMs. Additionally, LLMs are\nprimarily trained on modern software languages and have undergone limited\ntesting with legacy languages, making their ability to understand legacy\nlanguages unknown and, hence, an area for empirical study. This paper examines\nthe application of LLMs in the modernization of legacy government code written\nin ALC and MUMPS, addressing the challenges of input limitations. We\ninvestigate various code-chunking methods to optimize the generation of summary\nmodule comments for legacy code files, evaluating the impact of code-chunking\nmethods on the quality of documentation produced by different LLMs, including\nGPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs\ncan select partition points closely aligned with human expert partitioning. We\nalso find that chunking approaches have significant impact on downstream tasks\nsuch as documentation generation. LLM-created partitions produce comments that\nare up to 20% more factual and up to 10% more useful than when humans create\npartitions. Therefore, we conclude that LLMs can be used as suitable\nreplacements for human partitioning of large codebases during LLM-aided\nmodernization.", "comment": null, "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE;cs.AI", "links": "http://arxiv.org/abs/2506.19897v1;http://arxiv.org/pdf/2506.19897v1", "pdf_url": "http://arxiv.org/pdf/2506.19897v1"}, {"title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning", "link": "https://arxiv.org/pdf/2506.20430", "details": "W Zhao, C Wu, Y Fan, X Zhang, P Qiu, Y Sun, X Zhou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 In our analysis of 240 cases, 88% demonstrated concordant assessments between physicians and the **LLM** **evaluation** system. In 10% of cases, physicians ranked the correct diagnosis higher (ie, assigned it a better position) than the LLM-based \u2026", "entry_id": "http://arxiv.org/abs/2506.20430v1", "updated": "2025-06-25 13:42:26", "published": "2025-06-25 13:42:26", "authors": "Weike Zhao;Chaoyi Wu;Yanjie Fan;Xiaoman Zhang;Pengcheng Qiu;Yuze Sun;Xiao Zhou;Yanfeng Wang;Ya Zhang;Yongguo Yu;Kun Sun;Weidi Xie", "summary": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.CV;cs.MA", "links": "http://arxiv.org/abs/2506.20430v1;http://arxiv.org/pdf/2506.20430v1", "pdf_url": "http://arxiv.org/pdf/2506.20430v1"}, {"title": "JsDeObsBench: Measuring and Benchmarking LLMs for JavaScript Deobfuscation", "link": "https://arxiv.org/pdf/2506.20170", "details": "G Chen, X Jin, Z Lin - arXiv preprint arXiv:2506.20170, 2025", "abstract": "\u2026 Note that the selection of coding-challenge solutions is a common practice in **LLM** **evaluation** benchmarks, such as HumanEval [24] and EvalPlus [5]. CodeNet contains 13.9 million code samples across 4,053 programming problems in 55 \u2026", "entry_id": "http://arxiv.org/abs/2506.20170v1", "updated": "2025-06-25 06:50:13", "published": "2025-06-25 06:50:13", "authors": "Guoqiang Chen;Xin Jin;Zhiqiang Lin", "summary": "Deobfuscating JavaScript (JS) code poses a significant challenge in web\nsecurity, particularly as obfuscation techniques are frequently used to conceal\nmalicious activities within scripts. While Large Language Models (LLMs) have\nrecently shown promise in automating the deobfuscation process, transforming\ndetection and mitigation strategies against these obfuscated threats, a\nsystematic benchmark to quantify their effectiveness and limitations has been\nnotably absent. To address this gap, we present JsDeObsBench, a dedicated\nbenchmark designed to rigorously evaluate the effectiveness of LLMs in the\ncontext of JS deobfuscation. We detail our benchmarking methodology, which\nincludes a wide range of obfuscation techniques ranging from basic variable\nrenaming to sophisticated structure transformations, providing a robust\nframework for assessing LLM performance in real-world scenarios. Our extensive\nexperimental analysis investigates the proficiency of cutting-edge LLMs, e.g.,\nGPT-4o, Mixtral, Llama, and DeepSeek-Coder, revealing superior performance in\ncode simplification despite challenges in maintaining syntax accuracy and\nexecution reliability compared to baseline methods. We further evaluate the\ndeobfuscation of JS malware to exhibit the potential of LLMs in security\nscenarios. The findings highlight the utility of LLMs in deobfuscation\napplications and pinpoint crucial areas for further improvement.", "comment": "Accepted by ACM CCS 2025", "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR", "links": "http://arxiv.org/abs/2506.20170v1;http://arxiv.org/pdf/2506.20170v1", "pdf_url": "http://arxiv.org/pdf/2506.20170v1"}, {"title": "When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs", "link": "https://arxiv.org/pdf/2506.20544", "details": "A Khairi, D D'souza, Y Shen, J Kreutzer, S Hooker - arXiv preprint arXiv:2506.20544, 2025", "abstract": "Recent advancements in large language models (LLMs) have shifted focus toward scaling inference-time compute, improving performance without retraining the model. A common approach is to sample multiple outputs in parallel, and select one of \u2026", "entry_id": "http://arxiv.org/abs/2506.20544v1", "updated": "2025-06-25 15:37:53", "published": "2025-06-25 15:37:53", "authors": "Ammar Khairi;Daniel D'souza;Ye Shen;Julia Kreutzer;Sara Hooker", "summary": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.20544v1;http://arxiv.org/pdf/2506.20544v1", "pdf_url": "http://arxiv.org/pdf/2506.20544v1"}]
