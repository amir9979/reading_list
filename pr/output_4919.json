[{"title": "Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models", "link": "https://arxiv.org/pdf/2408.01308", "details": "Y Zhang, D Li, M Okumura - arXiv preprint arXiv:2408.01308, 2024", "abstract": "Learning token embeddings based on token co-occurrence statistics has proven effective for both pre-training and fine-tuning in natural language processing. However, recent studies have pointed out the distribution of learned embeddings \u2026"}, {"title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment", "link": "https://arxiv.org/pdf/2407.10804", "details": "J Jiang, J Li, WX Zhao, Y Song, T Zhang, JR Wen - arXiv preprint arXiv:2407.10804, 2024", "abstract": "Adapting general large language models (LLMs) to specialized domains presents great challenges due to varied data distributions. This adaptation typically requires continual pre-training on massive domain-specific corpora to facilitate knowledge \u2026"}, {"title": "A Knowledge Graph Embedding Model for Answering Factoid Entity Questions", "link": "https://dl.acm.org/doi/pdf/10.1145/3678003", "details": "P Jafarzadeh, F Ensan, M Ali Akbar Alavi\u2026 - ACM Transactions on \u2026, 2024", "abstract": "Factoid entity questions (FEQ), which seek answers in the form of a single entity from knowledge sources such as DBpedia and Wikidata, constitute a substantial portion of user queries in search engines. This paper introduces the Knowledge Graph \u2026"}, {"title": "A Systematic Evaluation of GPT-4V's Multimodal Capability for Chest X-ray Image Analysis", "link": "https://www.sciencedirect.com/science/article/pii/S2950162824000535", "details": "Y Liu, Y Li, Z Wang, X Liang, L Liu, L Wang, L Cui, Z Tu\u2026 - Meta-Radiology, 2024", "abstract": "This work evaluates GPT-4V's multimodal capability for medical image analysis, focusing on three representative tasks radiology report generation, medical visual question answering, and medical visual grounding. For the evaluation, a set of \u2026"}, {"title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "link": "https://arxiv.org/pdf/2407.10817", "details": "T Vu, K Krishna, S Alzubi, C Tar, M Faruqui, YH Sung - arXiv preprint arXiv \u2026, 2024", "abstract": "As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their output due to the high costs of human evaluation. To make progress towards better LLM autoraters, we introduce FLAMe, a family of Foundational Large \u2026"}, {"title": "LLMBox: A Comprehensive Library for Large Language Models", "link": "https://arxiv.org/pdf/2407.05563", "details": "T Tang, Y Hu, B Li, W Luo, Z Qin, H Sun, J Wang, S Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To facilitate the research on large language models (LLMs), this paper presents a comprehensive and unified library, LLMBox, to ease the development, use, and evaluation of LLMs. This library is featured with three main merits:(1) a unified data \u2026"}, {"title": "Explainable Knowledge-Based Learning for Online Medical Question Answering", "link": "https://link.springer.com/chapter/10.1007/978-981-97-5489-2_26", "details": "M Cui, X Li, P Qin - \u2026 Conference on Knowledge Science, Engineering and \u2026, 2024", "abstract": "This study introduces an explainable AI framework for online medical Question Answering (QA) tasks, a growing need in Internet hospitals and digital healthcare services. In response to the increasing demand for automated yet accountable \u2026"}, {"title": "An Experimental Research of Text-to-SQL for Heterogeneous Data in Large Language Models", "link": "https://link.springer.com/chapter/10.1007/978-981-97-5663-6_32", "details": "W Yang, X Wang, B Chen, Y Liu, B Wang, H Wang\u2026 - International Conference on \u2026, 2024", "abstract": "The large language model (LLMs) technology has become a new paradigm for Text- to-SQL task. However, when faced with heterogeneous data, there is still a lack of comprehensive and effective solutions for Text-to-SQL task, especially in terms of \u2026"}, {"title": "On Speeding Up Language Model Evaluation", "link": "https://arxiv.org/pdf/2407.06172", "details": "JP Zhou, CK Belardi, R Wu, T Zhang, CP Gomes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) currently dominate the field of natural language processing (NLP), representing the state-of-the-art across a diverse array of tasks. Developing a model of this nature, from training to inference, requires making \u2026"}]
