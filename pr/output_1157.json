'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Simplifying Multimodality: Unimodal Approach to Multim'
[{"title": "TFUT: Task fusion upward transformer model for multi-task learning on dense prediction", "link": "https://www.sciencedirect.com/science/article/pii/S107731422400095X", "details": "Z Xin, S Sirejiding, Y Lu, Y Ding, C Wang, T Alsarhan\u2026 - Computer Vision and Image \u2026, 2024", "abstract": "Transformer-based advancements have shown great promise in solving multi-task learning on dense prediction tasks. Well-designed task interaction modules of these methods further improve the performances by effectively transferring contextual \u2026"}, {"title": "Conformal Prediction for Natural Language Processing: A Survey", "link": "https://arxiv.org/pdf/2405.01976", "details": "MM Campos, A Farinhas, C Zerva, MAT Figueiredo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid proliferation of large language models and natural language processing (NLP) applications creates a crucial need for uncertainty quantification to mitigate risks such as hallucinations and to enhance decision-making reliability in critical \u2026"}, {"title": "Text Quality-Based Pruning for Efficient Training of Language Models", "link": "https://arxiv.org/pdf/2405.01582", "details": "V Sharma, K Padthe, N Ardalani, K Tirumala, R Howes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent times training Language Models (LMs) have relied on computationally heavy training over massive datasets which makes this training process extremely laborious. In this paper we propose a novel method for numerically evaluating text \u2026"}]
