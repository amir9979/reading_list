[{"title": "Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2506.20168", "details": "Z He, C Zhang, Z Wu, Z Chen, Y Zhan, Y Li, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Recent advancements in multimodal **large** **language** **models** (MLLMs) have enhanced \u2026 , the first benchmark dedicated to **evaluating** OCR hallucination in degraded document understanding. \u2026 This setup allows for **evaluating** models\u2019 capacity \u2026", "entry_id": "http://arxiv.org/abs/2506.20168v1", "updated": "2025-06-25 06:44:07", "published": "2025-06-25 06:44:07", "authors": "Zhentao He;Can Zhang;Ziheng Wu;Zhenghao Chen;Yufei Zhan;Yifan Li;Zhao Zhang;Xian Wang;Minghui Qiu", "summary": "Recent advancements in multimodal large language models have enhanced\ndocument understanding by integrating textual and visual information. However,\nexisting models exhibit incompleteness within their paradigm in real-world\nscenarios, particularly under visual degradation. In such conditions, the\ncurrent response paradigm often fails to adequately perceive visual degradation\nand ambiguity, leading to overreliance on linguistic priors or misaligned\nvisual-textual reasoning. This difficulty in recognizing uncertainty frequently\nresults in the generation of hallucinatory content, especially when a precise\nanswer is not feasible. To better demonstrate and analyze this phenomenon and\nproblem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR\nhallucination in degraded document understanding. This dataset includes test\nsamples spanning identity cards and invoices, with simulated real-world\ndegradations for OCR reliability. This setup allows for evaluating models'\ncapacity, under degraded input, to distinguish reliable visual information and\nanswer accordingly, thereby highlighting the challenge of avoiding\nhallucination on uncertain data. To achieve vision-faithful reasoning and\nthereby avoid the aforementioned issues, we further introduce a GRPO-based\nframework featuring a novel reward mechanism. By incorporating a self-awareness\nof visual uncertainty and an analysis method that initiates refusal to answer\nto increase task difficulty within our supervised fine-tuning and reinforcement\nlearning framework, we successfully mitigated hallucinations in ambiguous\nregions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model\nachieves a 22\\% absolute improvement in hallucination-free accuracy over GPT-4o\non KIE-HVQA and there is no significant performance drop in standard tasks,\nhighlighting both effectiveness and robustness.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.20168v1;http://arxiv.org/pdf/2506.20168v1", "pdf_url": "http://arxiv.org/pdf/2506.20168v1"}, {"title": "Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning", "link": "https://arxiv.org/pdf/2506.20020", "details": "S Dash, A Reymond, ES Spiro, A Caliskan - arXiv preprint arXiv:2506.20020, 2025", "abstract": "\u2026 In addition to VDA, we also prompt the model to **evaluate** confidence in its assessment of the headline on a Likert scale of 1 to 6 (1 = \u201cnot \u2026 To **evaluate** evidence for motivated reasoning versus analytical reasoning explanations as \u2026", "entry_id": "http://arxiv.org/abs/2506.20020v1", "updated": "2025-06-24 21:35:17", "published": "2025-06-24 21:35:17", "authors": "Saloni Dash;Am\u00e9lie Reymond;Emma S. Spiro;Aylin Caliskan", "summary": "Reasoning in humans is prone to biases due to underlying motivations like\nidentity protection, that undermine rational decision-making and judgment. This\nmotivated reasoning at a collective level can be detrimental to society when\ndebating critical issues such as human-driven climate change or vaccine safety,\nand can further aggravate political polarization. Prior studies have reported\nthat large language models (LLMs) are also susceptible to human-like cognitive\nbiases, however, the extent to which LLMs selectively reason toward\nidentity-congruent conclusions remains largely unexplored. Here, we investigate\nwhether assigning 8 personas across 4 political and socio-demographic\nattributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and\nproprietary) across two reasoning tasks from human-subject studies -- veracity\ndiscernment of misinformation headlines and evaluation of numeric scientific\nevidence -- we find that persona-assigned LLMs have up to 9% reduced veracity\ndiscernment relative to models without personas. Political personas\nspecifically, are up to 90% more likely to correctly evaluate scientific\nevidence on gun control when the ground truth is congruent with their induced\npolitical identity. Prompt-based debiasing methods are largely ineffective at\nmitigating these effects. Taken together, our empirical findings are the first\nto suggest that persona-assigned LLMs exhibit human-like motivated reasoning\nthat is hard to mitigate through conventional debiasing prompts -- raising\nconcerns of exacerbating identity-congruent reasoning in both LLMs and humans.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL", "links": "http://arxiv.org/abs/2506.20020v1;http://arxiv.org/pdf/2506.20020v1", "pdf_url": "http://arxiv.org/pdf/2506.20020v1"}, {"title": "AutoElicit: Using **Large Language Models** for Expert Prior Elicitation in Predictive Modelling", "link": "https://openreview.net/pdf%3Fid%3DGekXB58ZS7", "details": "A Capstick, R Krishnan, P Barnaghi - Forty-second International Conference on \u2026, 2025", "abstract": "\u2026 We also **evaluate** AutoElicit on a private dataset not in the LLM\u2019s training, collected as part of our study on Dementia care. This contains 10 features of daily in-home activity and physiology data, and clinically validated labels of positive or negative \u2026"}, {"title": "QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges", "link": "https://arxiv.org/pdf/2506.20008", "details": "A Basit, M Shao, H Asif, N Innan, M Kashif, A Marchisio\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Additionally, we introduce a multi-agent **evaluation** pipeline that iteratively refines incorrect solutions, further enhancing execution success \u2026 a structured **evaluation** pipeline that systematically assesses model performance across different \u2026", "entry_id": "http://arxiv.org/abs/2506.20008v1", "updated": "2025-06-24 20:54:56", "published": "2025-06-24 20:54:56", "authors": "Abdul Basit;Minghao Shao;Haider Asif;Nouhaila Innan;Muhammad Kashif;Alberto Marchisio;Muhammad Shafique", "summary": "Recent advances in Large Language Models (LLMs) have demonstrated strong\npotential in code generation, yet their effectiveness in quantum computing\nremains underexplored. This paper benchmarks LLMs for PennyLane-based quantum\ncode generation using real-world challenges from the Quantum Hackathon (QHack).\nWe introduce QHackBench, a novel benchmark dataset derived from QHack\ncompetitions, and evaluate model performance under vanilla prompting and\nRetrieval-Augmented Generation (RAG). Our structured evaluation framework\nassesses functional correctness, syntactic validity, and execution success\nacross varying challenge difficulties. Results indicate that RAG-enhanced\nmodels, supplemented with an augmented PennyLane dataset, approximately\ngenerate similar results as the standard prompting, particularly in complex\nquantum algorithms. Additionally, we introduce a multi-agent evaluation\npipeline that iteratively refines incorrect solutions, further enhancing\nexecution success rates. To foster further research, we commit to publicly\nreleasing QHackBench, along with our evaluation framework and experimental\nresults, enabling continued advancements in AI-assisted quantum programming.", "comment": "8 pages, 6 figures, 3 tables, submitted to QAI 2025", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.PL;cs.SE;68T50, 81P68, 68T07, 68T20;I.2.7; I.2.2", "links": "http://arxiv.org/abs/2506.20008v1;http://arxiv.org/pdf/2506.20008v1", "pdf_url": "http://arxiv.org/pdf/2506.20008v1"}, {"title": "From Confucius to computational linguistics: quantifying cross-linguistic semantic similarity and semantic fidelity using **large language models**", "link": "https://academic.oup.com/dsh/advance-article/doi/10.1093/llc/fqaf052/8174070", "details": "L Yang, G Zhou, L Lin - Digital Scholarship in the Humanities, 2025", "abstract": "\u2026 , there is still no effective method to **evaluate** whether the translations are faithful to the origi\u2026 In particular, the burgeoning research on **large** **language** **models** marks an ongoing transfor\u2026 Therefore, employing **large** **language** **models** to \u2026"}, {"title": "Exploring the Capabilities of the Frontier Large Language Models for Nuclear Energy Research", "link": "https://arxiv.org/pdf/2506.19863", "details": "A Almeldein, M Alnaggar, R Archibald, T Beck\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 The AI for Nuclear Energy workshop at Oak Ridge National Laboratory **evaluated** the potential of **Large** **Language** **Models** (LLMs) to accelerate fusion and fission research. Fourteen interdisciplinary teams explored diverse nuclear science \u2026", "entry_id": "http://arxiv.org/abs/2506.19863v1", "updated": "2025-06-10 09:28:18", "published": "2025-06-10 09:28:18", "authors": "Ahmed Almeldein;Mohammed Alnaggar;Rick Archibald;Tom Beck;Arpan Biswas;Rike Bostelmann;Wes Brewer;Chris Bryan;Christopher Calle;Cihangir Celik;Rajni Chahal;Jong Youl Choi;Arindam Chowdhury;Mark Cianciosa;Franklin Curtis;Gregory Davidson;Sebastian De Pascuale;Lisa Fassino;Ana Gainaru;Yashika Ghai;Luke Gibson;Qian Gong;Christopher Greulich;Scott Greenwood;Cory Hauck;Ehab Hassan;Rinkle Juneja;Soyoung Kang;Scott Klasky;Atul Kumar;Vineet Kumar;Paul Laiu;Calvin Lear;Yan-Ru Lin;Jono McConnell;Furkan Oz;Anant Raj;Pradeep Ramuhalli;Marie Romedenne;Samantha Sabatino;Jos\u00e9 Salcedo-P\u00e9rez;Nathan D. See;Arpan Sircar;Punam Thankur;Tim Younkin;Xiao-Ying Yu;Prashant Jain;Tom Evans;Prasanna Balaprakash", "summary": "The AI for Nuclear Energy workshop at Oak Ridge National Laboratory evaluated\nthe potential of Large Language Models (LLMs) to accelerate fusion and fission\nresearch. Fourteen interdisciplinary teams explored diverse nuclear science\nchallenges using ChatGPT, Gemini, Claude, and other AI models over a single\nday. Applications ranged from developing foundation models for fusion reactor\ncontrol to automating Monte Carlo simulations, predicting material degradation,\nand designing experimental programs for advanced reactors. Teams employed\nstructured workflows combining prompt engineering, deep research capabilities,\nand iterative refinement to generate hypotheses, prototype code, and research\nstrategies. Key findings demonstrate that LLMs excel at early-stage\nexploration, literature synthesis, and workflow design, successfully\nidentifying research gaps and generating plausible experimental frameworks.\nHowever, significant limitations emerged, including difficulties with novel\nmaterials designs, advanced code generation for modeling and simulation, and\ndomain-specific details requiring expert validation. The successful outcomes\nresulted from expert-driven prompt engineering and treating AI as a\ncomplementary tool rather than a replacement for physics-based methods. The\nworkshop validated AI's potential to accelerate nuclear energy research through\nrapid iteration and cross-disciplinary synthesis while highlighting the need\nfor curated nuclear-specific datasets, workflow automation, and specialized\nmodel development. These results provide a roadmap for integrating AI tools\ninto nuclear science workflows, potentially reducing development cycles for\nsafer, more efficient nuclear energy systems while maintaining rigorous\nscientific standards.", "comment": null, "journal_ref": null, "primary_category": "physics.comp-ph", "categories": "physics.comp-ph;cs.AI", "links": "http://arxiv.org/abs/2506.19863v1;http://arxiv.org/pdf/2506.19863v1", "pdf_url": "http://arxiv.org/pdf/2506.19863v1"}, {"title": "Enhancing Large Language Models through Structured Reasoning", "link": "https://arxiv.org/pdf/2506.20241", "details": "Y Dong, H Fan - arXiv preprint arXiv:2506.20241, 2025", "abstract": "\u2026 In this subsection, we **evaluate** the effectiveness of our proposed methods by reporting Pass@1 accuracy (mean \u00b1 \u2026 **evaluation** protocols. For AIME24, AIME25 and AMC23, we perform **evaluations** in 10 seeds each, while MATH500, Minerva \u2026", "entry_id": "http://arxiv.org/abs/2506.20241v1", "updated": "2025-06-25 08:36:12", "published": "2025-06-25 08:36:12", "authors": "Yubo Dong;Hehe Fan", "summary": "Recent Large Language Models (LLMs) have significantly advanced natural\nlanguage processing and automated decision-making. However, these models still\nencounter difficulties when performing complex reasoning tasks involving\nlogical deduction and systematic planning, primarily due to their reliance on\nimplicit statistical relationships without structured knowledge\nrepresentation.Inspired by cognitive science and neurosymbolic AI, we introduce\na novel approach to enhance LLMs through explicit structured reasoning. First,\nwe convert unstructured data into structured formats by explicitly annotating\nreasoning steps. We then employ this structured dataset to train LLMs through\nSupervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning\ncapabilities of LLMs using Group Relative Policy Optimization (GRPO),\nincorporating two innovative algorithms--MAX-Flow and Longest Common\nSubsequence (LCS)--which notably improve reasoning effectiveness and reduce\ncomputational complexity. Experimental results from fine-tuning a\nDeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust\nperformance across various scenarios, and improved compatibility with\noptimization techniques, validating the efficacy of structured reasoning\nintegration in LLMs.", "comment": "Preprint. Under review", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.20241v1;http://arxiv.org/pdf/2506.20241v1", "pdf_url": "http://arxiv.org/pdf/2506.20241v1"}, {"title": "Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks", "link": "https://arxiv.org/pdf/2506.20009", "details": "K Vrettos, ME Klontzas - arXiv preprint arXiv:2506.20009, 2025", "abstract": "\u2026 A dataset of medical questions was used for the **evaluation**. \u2026 Jahangir, MB Riaz, MJ Saeed, and MA Sattar, \u201cIndustrial applications of **large** **language** **models** ,\u201d Sci Rep, \u2026 di Ruscio, \u201cPrompt engineering and its implications on the energy \u2026", "entry_id": "http://arxiv.org/abs/2506.20009v1", "updated": "2025-06-24 20:56:03", "published": "2025-06-24 20:56:03", "authors": "Konstantinos Vrettos;Michail E. Klontzas", "summary": "Background The increasing adoption of Artificial Intelligence (AI) in\nhealthcare has sparked growing concerns about its environmental and ethical\nimplications. Commercial Large Language Models (LLMs), such as ChatGPT and\nDeepSeek, require substantial resources, while the utilization of these systems\nfor medical purposes raises critical issues regarding patient privacy and\nsafety. Methods We developed a customizable Retrieval-Augmented Generation\n(RAG) framework for medical tasks, which monitors its energy usage and CO2\nemissions. This system was then used to create RAGs based on various\nopen-source LLMs. The tested models included both general purpose models like\nllama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs\nperformance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs\no4-mini model. A dataset of medical questions was used for the evaluation.\nResults Custom RAG models outperformed commercial models in accuracy and energy\nconsumption. The RAG model built on llama3.1:8B achieved the highest accuracy\n(58.5%) and was significantly better than other models, including o4-mini and\nDeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption\nand CO2 footprint among all models, with a Performance per kWh of 0.52 and a\ntotal CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x\ntimes more accuracy points per kWh and 172% less electricity usage while\nmaintaining higher accuracy. Conclusion Our study demonstrates that local LLMs\ncan be leveraged to develop RAGs that outperform commercial, online LLMs in\nmedical tasks, while having a smaller environmental impact. Our modular\nframework promotes sustainable AI development, reducing electricity usage and\naligning with the UNs Sustainable Development Goals.", "comment": "18 pages, 3 Figures", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL;I.2.7", "links": "http://arxiv.org/abs/2506.20009v1;http://arxiv.org/pdf/2506.20009v1", "pdf_url": "http://arxiv.org/pdf/2506.20009v1"}, {"title": "Stock Market Forecasting: From Traditional Predictive Models to **Large Language Models**", "link": "https://link.springer.com/article/10.1007/s10614-025-11024-w", "details": "M Darwish, EE Hassanien, AHB Eissa - Computational Economics, 2025", "abstract": "Stock market forecasting is a complex research problem due to the complexity of the factors influencing stock market trends. This survey provides a comprehensive overview of recent advancements in stock market forecasting, focusing on the impact \u2026"}]
