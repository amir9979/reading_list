'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Emergent Abilities in Reduced-Scale Generative Languag'
[{"title": "SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models on Natural Language Inference for Clinical Trials", "link": "https://arxiv.org/pdf/2404.03977", "details": "M Aguiar, P Zweigenbaum, N Naderi - arXiv preprint arXiv:2404.03977, 2024", "abstract": "This paper describes our submission to Task 2 of SemEval-2024: Safe Biomedical Natural Language Inference for Clinical Trials. The Multi-evidence Natural Language Inference for Clinical Trial Data (NLI4CT) consists of a Textual Entailment (TE) task \u2026"}, {"title": "Meta In-Context Learning Makes Large Language Models Better Zero and Few-Shot Relation Extractors", "link": "https://arxiv.org/pdf/2404.17807", "details": "G Li, P Wang, J Liu, Y Guo, K Ji, Z Shang, Z Xu - arXiv preprint arXiv:2404.17807, 2024", "abstract": "Relation extraction (RE) is an important task that aims to identify the relationships between entities in texts. While large language models (LLMs) have revealed remarkable in-context learning (ICL) capability for general zero and few-shot \u2026"}, {"title": "PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval", "link": "https://arxiv.org/pdf/2404.18424", "details": "S Zhuang, X Ma, B Koopman, J Lin, G Zuccon - arXiv preprint arXiv:2404.18424, 2024", "abstract": "The current use of large language models (LLMs) for zero-shot document ranking follows one of two ways: 1) prompt-based re-ranking methods, which require no further training but are feasible for only re-ranking a handful of candidate documents \u2026"}, {"title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers", "link": "https://arxiv.org/pdf/2404.18443", "details": "R Xu, W Shi, Y Yu, Y Zhuang, Y Zhu, MD Wang, JC Ho\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Developing effective biomedical retrieval models is important for excelling at knowledge-intensive biomedical tasks but still challenging due to the deficiency of sufficient publicly annotated biomedical data and computational resources. We \u2026"}, {"title": "Empowering Large Language Models for Textual Data Augmentation", "link": "https://arxiv.org/pdf/2404.17642", "details": "Y Li, K Ding, J Wang, K Lee - arXiv preprint arXiv:2404.17642, 2024", "abstract": "With the capabilities of understanding and executing natural language instructions, Large language models (LLMs) can potentially act as a powerful tool for textual data augmentation. However, the quality of augmented data depends heavily on the \u2026"}, {"title": "Benchmarking Benchmark Leakage in Large Language Models", "link": "https://arxiv.org/pdf/2404.18824", "details": "R Xu, Z Wang, RZ Fan, P Liu - arXiv preprint arXiv:2404.18824, 2024", "abstract": "Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary \u2026"}, {"title": "HFT: Half Fine-Tuning for Large Language Models", "link": "https://arxiv.org/pdf/2404.18466", "details": "T Hui, Z Zhang, S Wang, W Xu, Y Sun, H Wu - arXiv preprint arXiv:2404.18466, 2024", "abstract": "Large language models (LLMs) with one or more fine-tuning phases have become a necessary step to unlock various capabilities, enabling LLMs to follow natural language instructions or align with human preferences. However, it carries the risk of \u2026"}]
