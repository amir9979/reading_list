[{"title": "MegaCOIN: Enhancing Medium-Grained Color Perception for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.03927", "details": "MC Chiu, S Wen, PY Chen, X Ma - arXiv preprint arXiv:2412.03927, 2024", "abstract": "In vision-language models (VLMs), the ability to perceive and interpret color and physical environment is crucial for achieving contextually accurate understanding and interaction. However, despite advances in multimodal modeling, there remains a \u2026"}, {"title": "FoPru: Focal Pruning for Efficient Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.14164", "details": "L Jiang, W Huang, T Liu, Y Zeng, J Li, L Cheng, X Xu - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) represent a significant advancement toward achieving superior multimodal capabilities by enabling powerful Large Language Models (LLMs) to understand visual input. Typically, LVLMs utilize visual \u2026"}, {"title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step", "link": "https://arxiv.org/pdf/2411.10440%3F", "details": "G Xu, P Jin, L Hao, Y Song, L Sun, L Yuan - arXiv preprint arXiv:2411.10440, 2024", "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to \u2026"}, {"title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models", "link": "https://arxiv.org/pdf/2412.04467", "details": "S Yang, Y Chen, Z Tian, C Wang, J Li, B Yu, J Jia - arXiv preprint arXiv:2412.04467, 2024", "abstract": "Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens \u2026"}, {"title": "AdvDreamer Unveils: Are Vision-Language Models Truly Ready for Real-World 3D Variations?", "link": "https://arxiv.org/pdf/2412.03002", "details": "S Ruan, H Liu, Y Huang, X Wang, C Kang, H Su\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision Language Models (VLMs) have exhibited remarkable generalization capabilities, yet their robustness in dynamic real-world scenarios remains largely unexplored. To systematically evaluate VLMs' robustness to real-world 3D variations \u2026"}, {"title": "A cascaded retrieval-while-reasoning multi-document comprehension framework with incremental attention for medical question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0957417424025685", "details": "J Liu, J Ren, R Bai, Z Zhang, Z Lu - Expert Systems with Applications, 2024", "abstract": "Abstract Clinical Machine Reading Comprehension (MRC) is challenging due to the need for medical expertise and comprehensive reasoning chains for diagnosis. This paper introduces a novel cascade retrieval-while-reasoning framework for clinical \u2026"}, {"title": "PreAdapter: Pre-training Language Models on Knowledge Graphs", "link": "https://link.springer.com/chapter/10.1007/978-3-031-77850-6_12", "details": "J Omeliyanenko, A Hotho, D Schl\u00f6r - International Semantic Web Conference, 2024", "abstract": "Pre-trained language models have demonstrated state-of-the-art performance in various downstream tasks such as summarization, sentiment classification, and question answering. Leveraging vast amounts of textual data during training, these \u2026"}, {"title": "Task Arithmetic Through The Lens Of One-Shot Federated Learning", "link": "https://arxiv.org/pdf/2411.18607%3F", "details": "Z Tao, I Mason, S Kulkarni, X Boix - arXiv preprint arXiv:2411.18607, 2024", "abstract": "Task Arithmetic is a model merging technique that enables the combination of multiple models' capabilities into a single model through simple arithmetic in the weight space, without the need for additional fine-tuning or access to the original \u2026"}, {"title": "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.1052.pdf", "details": "W Zhai, A Zubiaga, B Liu, CJ Sun, Y Zhao - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "The fusion of language models (LMs) and knowledge graphs (KGs) is widely used in commonsense question answering, but generating faithful explanations remains challenging. Current methods often overlook path decoding faithfulness, leading to \u2026"}]
