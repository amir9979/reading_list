[{"title": "MFC-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2406.11288", "details": "S Wang, H Lin, Z Luo, Z Ye, G Chen, J Ma - arXiv preprint arXiv:2406.11288, 2024", "abstract": "Large vision-language models (LVLMs) have significantly improved multimodal reasoning tasks, such as visual question answering and image captioning. These models embed multimodal facts within their parameters, rather than relying on \u2026"}, {"title": "Refusal in Language Models Is Mediated by a Single Direction", "link": "https://arxiv.org/pdf/2406.11717", "details": "A Arditi, O Obeso, A Syed, D Paleka, N Rimsky\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying \u2026"}, {"title": "Advancing DNA Language Models through Motif-Oriented Pre-Training with MoDNA", "link": "https://www.mdpi.com/2673-7426/4/2/85", "details": "W An, Y Guo, Y Bian, H Ma, J Yang, C Li, J Huang - BioMedInformatics, 2024", "abstract": "Acquiring meaningful representations of gene expression is essential for the accurate prediction of downstream regulatory tasks, such as identifying promoters and transcription factor binding sites. However, the current dependency on \u2026"}, {"title": "What Matters in Learning Facts in Language Models? Multifaceted Knowledge Probing with Diverse Multi-Prompt Datasets", "link": "https://arxiv.org/pdf/2406.12277", "details": "X Zhao, N Yoshinaga, D Oba - arXiv preprint arXiv:2406.12277, 2024", "abstract": "Large language models (LLMs) face issues in handling factual knowledge, making it vital to evaluate their true ability to understand facts. In this study, we introduce knowledge probing frameworks, BELIEF (-ICL), to evaluate the knowledge \u2026"}, {"title": "Comparative Analysis of Open-Source Language Models in Summarizing Medical Text Data", "link": "https://arxiv.org/pdf/2405.16295", "details": "Y Chen, Z Wang, B Wen, F Zulkernine - arXiv preprint arXiv:2405.16295, 2024", "abstract": "Unstructured text in medical notes and dialogues contains rich information. Recent advancements in Large Language Models (LLMs) have demonstrated superior performance in question answering and summarization tasks on unstructured text \u2026"}, {"title": "Large Scale Transfer Learning for Tabular Data via Language Modeling", "link": "https://arxiv.org/pdf/2406.12031", "details": "J Gardner, JC Perdomo, L Schmidt - arXiv preprint arXiv:2406.12031, 2024", "abstract": "Tabular data--structured, heterogeneous, spreadsheet-style data with rows and columns--is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and \u2026"}]
