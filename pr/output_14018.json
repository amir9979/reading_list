[{"title": "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via Cross-Architecture Distillation", "link": "https://arxiv.org/pdf/2502.15016", "details": "J Ni, Z Liu, S Wang, M Jin, W Jin - arXiv preprint arXiv:2502.15016, 2025", "abstract": "Transformer-based and CNN-based methods demonstrate strong performance in long-term time series forecasting. However, their high computational and storage requirements can hinder large-scale deployment. To address this limitation, we \u2026"}, {"title": "Preserving Angles Improves Feature Distillation of Foundation Models", "link": "https://www.researchgate.net/profile/Evelyn-Mannix/publication/386112516_Preserving_Angles_Improves_Feature_Distillation_of_Foundation_Models/links/67d4da32be849d39d679003d/Preserving-Angles-Improves-Feature-Distillation-of-Foundation-Models.pdf", "details": "EJ Mannix, L Hodgkinson, H Bondell", "abstract": "Abstract Knowledge distillation approaches compress models by training a student network using the classification outputs of a high quality teacher model, but can fail to effectively transfer the properties of computer vision foundation models from the \u2026"}]
