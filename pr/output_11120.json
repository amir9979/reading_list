[{"title": "Boosting LLM via Learning from Data Iteratively and Selectively", "link": "https://arxiv.org/pdf/2412.17365%3F", "details": "Q Jia, S Ren, Z Qin, F Xue, J Ni, Y You - arXiv preprint arXiv:2412.17365, 2024", "abstract": "Datasets nowadays are generally constructed from multiple sources and using different synthetic techniques, making data de-noising and de-duplication crucial before being used for post-training. In this work, we propose to perform instruction \u2026"}, {"title": "BenCzechMark: A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism", "link": "https://arxiv.org/pdf/2412.17933", "details": "M Fajcik, M Docekal, J Dolezal, K Ondrej, K Bene\u0161\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present BenCzechMark (BCM), the first comprehensive Czech language benchmark designed for large language models, offering diverse tasks, multiple task formats, and multiple evaluation metrics. Its scoring system is grounded in statistical \u2026"}, {"title": "DeepSeek-V3 Technical Report", "link": "https://arxiv.org/pdf/2412.19437", "details": "A Liu, B Feng, B Xue, B Wang, B Wu, C Lu, C Zhao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent \u2026"}, {"title": "Self-guided Knowledgeable Network of Thoughts: Amplifying Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2412.16533", "details": "CC Chen, CY Yeh, HW Chen, DN Yang, MS Chen - arXiv preprint arXiv:2412.16533, 2024", "abstract": "We introduce Knowledgeable Network of Thoughts (kNoT): a prompt scheme that advances the capabilities of large language models (LLMs) beyond existing paradigms like Chain-of-Thought (CoT), Tree of Thoughts (ToT), and Graph of \u2026"}]
