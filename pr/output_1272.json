'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Comparing natural language processing representations'
[{"title": "Special Section on Efficiency in Neural Information Retrieval", "link": "https://dl.acm.org/doi/full/10.1145/3641203", "details": "S Bruch, C Lucchese, M Maistro, FM Nardini - ACM Transactions on Information \u2026, 2024", "abstract": "The rise of deep neural networks and self-supervised learning in recent years have brought about a paradigm shift in Information Retrieval. From retrieval to ranking, question answering to recommendation, search to conversational agents, models \u2026"}, {"title": "A Novel Phenotyping Pipeline to Improve Identification of Patients With Pulmonary Hypertension in Electronic Health Records", "link": "https://www.atsjournals.org/doi/pdf/10.1164/ajrccm-conference.2024.209.1_MeetingAbstracts.A7290", "details": "DM Vidmar, W Thompson, K Morland, G Lee, R Miotto\u2026 - D102. HOT TOPICS IN \u2026, 2024", "abstract": "RATIONALE: Accurately identifying patients with pulmonary hypertension (PH) through high-quality phenotyping is critical for defining clinical research cohorts and generating accurate labels for machine learning (ML) models. However, PH is often \u2026"}, {"title": "Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think", "link": "https://arxiv.org/pdf/2404.08382", "details": "X Wang, C Hu, B Ma, P R\u00f6ttger, B Plank - arXiv preprint arXiv:2404.08382, 2024", "abstract": "Multiple choice questions (MCQs) are commonly used to evaluate the capabilities of large language models (LLMs). One common way to evaluate the model response is to rank the candidate answers based on the log probability of the first token \u2026"}, {"title": "Quality of Answers of Generative Large Language Models Versus Peer Users for Interpreting Laboratory Test Results for Lay Patients: Evaluation Study", "link": "https://www.jmir.org/2024/1/e56655/", "details": "Z He, B Bhasuran, Q Jin, S Tian, K Hanna, C Shavor\u2026 - Journal of Medical Internet \u2026, 2024", "abstract": "Background Although patients have easy access to their electronic health records and laboratory test result data through patient portals, laboratory test results are often confusing and hard to understand. Many patients turn to web-based forums or \u2026"}, {"title": "Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language Selection and Cross-lingual Self-Distillation", "link": "https://arxiv.org/pdf/2404.08491", "details": "H Zhao, Z Cai, S Si, L Chen, Y He, K An, B Chang - arXiv preprint arXiv:2404.08491, 2024", "abstract": "Large-scale multilingual Pretrained Language Models (mPLMs) yield impressive performance on cross-language tasks, yet significant performance disparities exist across different languages within the same mPLM. Previous studies endeavored to \u2026"}, {"title": "Rethinking Software Engineering in the Foundation Model Era: From Task-Driven AI Copilots to Goal-Driven AI Pair Programmers", "link": "https://arxiv.org/pdf/2404.10225", "details": "AE Hassan, GA Oliva, D Lin, B Chen, Z Ming - arXiv preprint arXiv:2404.10225, 2024", "abstract": "The advent of Foundation Models (FMs) and AI-powered copilots has transformed the landscape of software development, offering unprecedented code completion capabilities and enhancing developer productivity. However, the current task-driven \u2026"}, {"title": "Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense", "link": "https://arxiv.org/pdf/2405.04655", "details": "S Shen, L Logeswaran, M Lee, H Lee, S Poria\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated substantial commonsense understanding through numerous benchmark evaluations. However, their understanding of cultural commonsense remains largely unexamined. In this paper \u2026"}, {"title": "Measuring Cross-lingual Transfer in Bytes", "link": "https://arxiv.org/pdf/2404.08191", "details": "LR de Souza, TS Almeida, R Lotufo, R Nogueira - arXiv preprint arXiv:2404.08191, 2024", "abstract": "Multilingual pretraining has been a successful solution to the challenges posed by the lack of resources for languages. These models can transfer knowledge to target languages with minimal or no examples. Recent research suggests that monolingual \u2026"}]
