[{"title": "Faithscore: Fine-grained evaluations of hallucinations in large vision-language models", "link": "https://aclanthology.org/2024.findings-emnlp.290.pdf", "details": "L Jing, R Li, Y Chen, X Du - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract We introduce FaithScore (Faithfulness to Atomic Image Facts Score), a reference-free and fine-grained evaluation metric that measures the faithfulness of the generated free-form answers from large vision-language models (LVLMs). The \u2026"}, {"title": "Unsupervised Foundation Model-Agnostic Slide-Level Representation Learning", "link": "https://arxiv.org/pdf/2411.13623", "details": "T Lenz, P Neidlinger, M Ligero, G W\u00f6lflein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Representation learning of pathology whole-slide images (WSIs) has primarily relied on weak supervision with Multiple Instance Learning (MIL). This approach leads to slide representations highly tailored to a specific clinical task. Self-supervised \u2026"}, {"title": "Label correlated contrastive learning for medical report generation", "link": "https://www.sciencedirect.com/science/article/pii/S0169260724004759", "details": "X Liu, J Xin, B Dai, Q Shen, Z Huang, Z Wang - Computer Methods and Programs in \u2026, 2024", "abstract": "Abstract Background and Objective: Automatic generation of medical reports reduces both the burden on radiologists and the possibility of errors due to the inexperience of radiologists. The model that utilizes attention mechanism and contrastive learning \u2026"}, {"title": "ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for Large Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3DPyTf2jj0SH", "details": "S Liu, K Ying, H Zhang, Y Yang, Y Lin, T Zhang, C Li\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Multi-turn visual conversation is an important ability of real-world AI assistants. However, the related evaluation benchmark is missed. This paper presents ConvBench, a multi-turn conversation benchmark with hierarchical capabilities \u2026"}, {"title": "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.1249.pdf", "details": "X Lin, M Li, R Zemel, H Ji, SF Chang - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "Recently, enabling pretrained language models (PLMs) to perform zero-shot crossmodal tasks such as video question answering has been extensively studied. A popular approach is to learn a projection network that projects visual features into the \u2026"}, {"title": "Classification Done Right for Vision-Language Pre-Training", "link": "https://arxiv.org/pdf/2411.03313%3F", "details": "H Zilong, Y Qinghao, K Bingyi, F Jiashi, F Haoqi - arXiv preprint arXiv:2411.03313, 2024", "abstract": "We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data. Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised \u2026"}, {"title": "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering", "link": "https://aclanthology.org/2024.emnlp-main.110.pdf", "details": "D Hao, Q Wang, L Guo, J Jiang, J Liu - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "While large pre-trained visual-language models have shown promising results on traditional visual question answering benchmarks, it is still challenging for them to answer complex VQA problems which requires diverse world knowledge. Motivated \u2026"}, {"title": "Empowering multi-step reasoning across languages via program-aided language models", "link": "https://aclanthology.org/2024.emnlp-main.678.pdf", "details": "L Ranaldi, G Pucci, B Haddow, A Birch - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "In-context learning methods are popular inference strategies where Large Language Models (LLMs) are elicited to solve a task using provided demonstrations without parameter updates. Among these approaches are the reasoning methods, best \u2026"}, {"title": "Decoding Report Generators: A Cyclic Vision-Language Adapter for Counterfactual Explanations", "link": "https://arxiv.org/pdf/2411.05261", "details": "Y Fang, Z Jin, S Guo, J Liu, Y Gao, J Ning, Z Yue, Z Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite significant advancements in report generation methods, a critical limitation remains: the lack of interpretability in the generated text. This paper introduces an innovative approach to enhance the explainability of text generated by report \u2026"}]
