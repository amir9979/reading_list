[{"title": "Exploring Self-Conditioning Co-Sample Strategy of Diffusion Models in Dermoscopic Images", "link": "https://ieeexplore.ieee.org/abstract/document/10856382/", "details": "H Guo, Y Jiang, Z Zhang, B Liu, Y Li, Y Liu, X Wang - IEEE Journal of Biomedical and \u2026, 2025", "abstract": "Generative dataset expansion methods can effectively alleviate the scarcity of data in dermoscopic image segmentation but commonly employ a two-stage synthesis strategy that contains additional learnable components and complex design, which \u2026"}, {"title": "Explainable Self-Supervised Dynamic Neuroimaging Using Time Reversal", "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11763917/", "details": "Z Iqbal, MM Rahman, U Mahmood, Q Zia, Z Fu\u2026 - Brain Sciences, 2025", "abstract": "Objective: Functional magnetic resonance imaging data pose significant challenges due to their inherently noisy and complex nature, making traditional statistical models less effective in capturing predictive features. While deep learning models offer \u2026"}, {"title": "Multi-Grained Contrastive Learning for Text-supervised Open-vocabulary Semantic Segmentation", "link": "https://dl.acm.org/doi/pdf/10.1145/3711868", "details": "Y Liu, P Ge, G Wang, Q Liu, D Huang - ACM Transactions on Multimedia Computing \u2026, 2025", "abstract": "Learning open-vocabulary semantic segmentation (OVSS) from text supervision has recently received increasing attention for its promising potential in real-world applications. However, only with image-level supervision, it struggles to achieve \u2026"}, {"title": "Double Visual Defense: Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness", "link": "https://arxiv.org/pdf/2501.09446", "details": "Z Wang, C Xie, B Bartoldson, B Kailkhura - arXiv preprint arXiv:2501.09446, 2025", "abstract": "This paper investigates the robustness of vision-language models against adversarial visual perturbations and introduces a novel``double visual defense\" to enhance this robustness. Unlike previous approaches that resort to lightweight \u2026"}, {"title": "multiHIVE: Hierarchical Multimodal Deep Generative Model for Single-cell Multiomics Integration", "link": "https://www.biorxiv.org/content/10.1101/2025.01.28.635222.full.pdf", "details": "A Nanduri, KP Musale, K Pandey, H Zafar - bioRxiv, 2025", "abstract": "Recently developed single-cell multiomics technologies are enhancing our understanding of cellular heterogeneity by providing multiple views of a biological system. CITE-seq (cellular indexing of transcriptomes and epitopes by sequencing) \u2026"}, {"title": "Towards Automated Self-Supervised Learning for Truly Unsupervised Graph Anomaly Detection", "link": "https://arxiv.org/pdf/2501.14694", "details": "Z Li, Y Wang, M van Leeuwen - arXiv preprint arXiv:2501.14694, 2025", "abstract": "Self-supervised learning (SSL) is an emerging paradigm that exploits supervisory signals generated from the data itself, and many recent studies have leveraged SSL to conduct graph anomaly detection. However, we empirically found that three \u2026"}, {"title": "Dual-Level Masked Semantic Inference for Semi-Supervised Semantic Segmentation", "link": "https://ieeexplore.ieee.org/abstract/document/10856422/", "details": "Q Ma, Z Zhang, P Qiao, Y Wang, R Ji, C Liu, J Chen - IEEE Transactions on \u2026, 2025", "abstract": "Semi-supervised semantic segmentation pursues a holistic pixel-wise understanding of unseen images with limited annotation. To this end, existing methods focus on regularizing per-pixel prediction consistency within unlabeled data, while rarely \u2026"}, {"title": "Context-aware prompt learning for test-time vision recognition with frozen vision-language model", "link": "https://www.sciencedirect.com/science/article/pii/S0031320325000196", "details": "J Yin, X Zhang, L Wu, X Wang - Pattern Recognition, 2025", "abstract": "Current pre-trained vision-language models, such as CLIP, have demonstrated remarkable zero-shot generalization capabilities across various downstream tasks. However, their performance significantly degrades when test inputs exhibit different \u2026"}, {"title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders", "link": "https://arxiv.org/pdf/2501.17148", "details": "Z Wu, A Arora, A Geiger, Z Wang, J Huang, D Jurafsky\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well \u2026"}]
