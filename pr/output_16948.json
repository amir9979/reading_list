[{"title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models", "link": "https://arxiv.org/pdf/2505.03075", "details": "Z Shi, L Yan, W Sun, Y Feng, P Ren, X Ma, S Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-augmented generation (RAG) integrates large language models (LLM s) with retrievers to access external knowledge, improving the factuality of LLM generation in knowledge-grounded tasks. To optimize the RAG performance, most \u2026", "entry_id": "http://arxiv.org/abs/2505.03075v1", "updated": "2025-05-05 23:54:53", "published": "2025-05-05 23:54:53", "authors": "Zhengliang Shi;Lingyong Yan;Weiwei Sun;Yue Feng;Pengjie Ren;Xinyu Ma;Shuaiqiang Wang;Dawei Yin;Maarten de Rijke;Zhaochun Ren", "summary": "Retrieval-augmented generation (RAG) integrates large language models ( LLM\ns) with retrievers to access external knowledge, improving the factuality of\nLLM generation in knowledge-grounded tasks. To optimize the RAG performance,\nmost previous work independently fine-tunes the retriever to adapt to frozen\nLLM s or trains the LLMs to use documents retrieved by off-the-shelf\nretrievers, lacking end-to-end training supervision. Recent work addresses this\nlimitation by jointly training these two components but relies on overly\nsimplifying assumptions of document independence, which has been criticized for\nbeing far from real-world scenarios. Thus, effectively optimizing the overall\nRAG performance remains a critical challenge.\n  We propose a direct retrieval-augmented optimization framework, named DRO,\nthat enables end-to-end training of two key components: (i) a generative\nknowledge selection model and (ii) an LLM generator. DRO alternates between two\nphases: (i) document permutation estimation and (ii) re-weighted maximization,\nprogressively improving RAG components through a variational approach. In the\nestimation step, we treat document permutation as a latent variable and\ndirectly estimate its distribution from the selection model by applying an\nimportance sampling strategy. In the maximization step, we calibrate the\noptimization expectation using importance weights and jointly train the\nselection model and LLM generator. Our theoretical analysis reveals that DRO is\nanalogous to policy-gradient methods in reinforcement learning. Extensive\nexperiments conducted on five datasets illustrate that DRO outperforms the best\nbaseline with 5%-15% improvements in EM and F1. We also provide in-depth\nexperiments to qualitatively analyze the stability, convergence, and variance\nof DRO.", "comment": null, "journal_ref": null, "primary_category": "cs.IR", "categories": "cs.IR", "links": "http://arxiv.org/abs/2505.03075v1;http://arxiv.org/pdf/2505.03075v1", "pdf_url": "http://arxiv.org/pdf/2505.03075v1"}, {"title": "Automated Capability Evaluation of Foundation Models", "link": "https://arxiv.org/pdf/2505.17228", "details": "A Afkanpour, O Dige, F Tavakoli - arXiv preprint arXiv:2505.17228, 2025", "abstract": "Current evaluation frameworks for foundation models rely heavily on fixed, manually curated benchmarks, limiting their ability to capture the full breadth of model capabilities. This paper introduces Active learning for Capability Evaluation (ACE), a \u2026", "entry_id": "http://arxiv.org/abs/2505.17228v1", "updated": "2025-05-22 19:09:57", "published": "2025-05-22 19:09:57", "authors": "Arash Afkanpour;Omkar Dige;Fatemeh Tavakoli", "summary": "Current evaluation frameworks for foundation models rely heavily on fixed,\nmanually curated benchmarks, limiting their ability to capture the full breadth\nof model capabilities. This paper introduces Active learning for Capability\nEvaluation (ACE), a novel framework for scalable, automated, and fine-grained\nevaluation of foundation models. ACE leverages the knowledge embedded in\npowerful language models to decompose a domain into semantically meaningful\ncapabilities and generate diverse evaluation tasks, significantly reducing\nhuman effort. To maximize coverage and efficiency, ACE models a subject model's\nperformance as a capability function over a latent semantic space and uses\nactive learning to prioritize the evaluation of the most informative\ncapabilities. This adaptive evaluation strategy enables cost-effective\ndiscovery of strengths, weaknesses, and failure modes that static benchmarks\nmay miss. Our results suggest that ACE provides a more complete and informative\npicture of model capabilities, which is essential for safe and well-informed\ndeployment of foundation models.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.17228v1;http://arxiv.org/pdf/2505.17228v1", "pdf_url": "http://arxiv.org/pdf/2505.17228v1"}, {"title": "Steering How Deep Neural Networks Generalize", "link": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-91.pdf", "details": "K Kang - 2025", "abstract": "In recent years, deep learning and Large Language Models (LLMs) have experienced an unprecedented rate of advancement. However, their transformative capabilities often outpace our understanding of how they work. In particular, while \u2026"}, {"title": "After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG", "link": "https://arxiv.org/pdf/2505.17118", "details": "X Dai, H Hu, Y Hua, J Li, Y Chen, R Jin, N Hu, G Qi - arXiv preprint arXiv:2505.17118, 2025", "abstract": "Retrieval-augmented generation (RAG) systems face critical challenges in balancing internal (parametric) and external (retrieved) knowledge, especially when these sources conflict or are unreliable. To analyze these scenarios comprehensively, we \u2026", "entry_id": "http://arxiv.org/abs/2505.17118v1", "updated": "2025-05-21 16:29:19", "published": "2025-05-21 16:29:19", "authors": "Xinbang Dai;Huikang Hu;Yuncheng Hua;Jiaqi Li;Yongrui Chen;Rihui Jin;Nan Hu;Guilin Qi", "summary": "Retrieval-augmented generation (RAG) systems face critical challenges in\nbalancing internal (parametric) and external (retrieved) knowledge, especially\nwhen these sources conflict or are unreliable. To analyze these scenarios\ncomprehensively, we construct the Trustworthiness Response Dataset (TRD) with\n36,266 questions spanning four RAG settings. We reveal that existing approaches\naddress isolated scenarios-prioritizing one knowledge source, naively merging\nboth, or refusing answers-but lack a unified framework to handle different\nreal-world conditions simultaneously. Therefore, we propose the BRIDGE\nframework, which dynamically determines a comprehensive response strategy of\nlarge language models (LLMs). BRIDGE leverages an adaptive weighting mechanism\nnamed soft bias to guide knowledge collection, followed by a Maximum Soft-bias\nDecision Tree to evaluate knowledge and select optimal response strategies\n(trust internal/external knowledge, or refuse). Experiments show BRIDGE\noutperforms baselines by 5-15% in accuracy while maintaining balanced\nperformance across all scenarios. Our work provides an effective solution for\nLLMs' trustworthy responses in real-world RAG applications.", "comment": "24 pages, 8 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;I.2.7", "links": "http://arxiv.org/abs/2505.17118v1;http://arxiv.org/pdf/2505.17118v1", "pdf_url": "http://arxiv.org/pdf/2505.17118v1"}, {"title": "Bidirectional Knowledge Distillation for Enhancing Sequential Recommendation with Large Language Models", "link": "https://arxiv.org/pdf/2505.18120", "details": "J Wu, J Liu, D Li, G Zhang, M Han, H Gu, P Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have demonstrated exceptional performance in understanding and generating semantic patterns, making them promising candidates for sequential recommendation tasks. However, when combined with conventional \u2026", "entry_id": "http://arxiv.org/abs/2505.18120v1", "updated": "2025-05-23 17:21:14", "published": "2025-05-23 17:21:14", "authors": "Jiongran Wu;Jiahao Liu;Dongsheng Li;Guangping Zhang;Mingzhe Han;Hansu Gu;Peng Zhang;Li Shang;Tun Lu;Ning Gu", "summary": "Large language models (LLMs) have demonstrated exceptional performance in\nunderstanding and generating semantic patterns, making them promising\ncandidates for sequential recommendation tasks. However, when combined with\nconventional recommendation models (CRMs), LLMs often face challenges related\nto high inference costs and static knowledge transfer methods. In this paper,\nwe propose a novel mutual distillation framework, LLMD4Rec, that fosters\ndynamic and bidirectional knowledge exchange between LLM-centric and CRM-based\nrecommendation systems. Unlike traditional unidirectional distillation methods,\nLLMD4Rec enables iterative optimization by alternately refining both models,\nenhancing the semantic understanding of CRMs and enriching LLMs with\ncollaborative signals from user-item interactions. By leveraging sample-wise\nadaptive weighting and aligning output distributions, our approach eliminates\nthe need for additional parameters while ensuring effective knowledge transfer.\nExtensive experiments on real-world datasets demonstrate that LLMD4Rec\nsignificantly improves recommendation accuracy across multiple benchmarks\nwithout increasing inference costs. This method provides a scalable and\nefficient solution for combining the strengths of both LLMs and CRMs in\nsequential recommendation systems.", "comment": "11 pages, under review", "journal_ref": null, "primary_category": "cs.IR", "categories": "cs.IR;cs.AI", "links": "http://arxiv.org/abs/2505.18120v1;http://arxiv.org/pdf/2505.18120v1", "pdf_url": "http://arxiv.org/pdf/2505.18120v1"}, {"title": "Rewriting Pre-Training Data Boosts LLM Performance in Math and Code", "link": "https://arxiv.org/pdf/2505.02881", "details": "K Fujii, Y Tajima, S Mizuki, H Shimada, T Shiotani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The performance of large language models (LLMs) in program synthesis and mathematical reasoning is fundamentally limited by the quality of their pre-training corpora. We introduce two openly licensed datasets, released under the Llama 3.3 \u2026", "entry_id": "http://arxiv.org/abs/2505.02881v2", "updated": "2025-05-10 14:45:30", "published": "2025-05-05 07:38:43", "authors": "Kazuki Fujii;Yukito Tajima;Sakae Mizuki;Hinari Shimada;Taihei Shiotani;Koshiro Saito;Masanari Ohi;Masaki Kawamura;Taishi Nakamura;Takumi Okamoto;Shigeki Ishida;Kakeru Hattori;Youmi Ma;Hiroya Takamura;Rio Yokota;Naoaki Okazaki", "summary": "The performance of large language models (LLMs) in program synthesis and\nmathematical reasoning is fundamentally limited by the quality of their\npre-training corpora. We introduce two openly licensed datasets, released under\nthe Llama 3.3 Community License, that significantly enhance LLM performance by\nsystematically rewriting public data. SwallowCode (approximately 16.1 billion\ntokens) refines Python snippets from The-Stack-v2 through a novel four-stage\npipeline: syntax validation, pylint-based style filtering, and a two-stage LLM\nrewriting process that enforces style conformity and transforms snippets into\nself-contained, algorithmically efficient examples. Unlike prior methods that\nrely on exclusionary filtering or limited transformations, our\ntransform-and-retain approach upgrades low-quality code, maximizing data\nutility. SwallowMath (approximately 2.3 billion tokens) enhances Finemath-4+ by\nremoving boilerplate, restoring context, and reformatting solutions into\nconcise, step-by-step explanations. Within a fixed 50 billion token training\nbudget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1\nby +17.0 on HumanEval and +17.7 on HumanEval+ compared to Stack-Edu, surpassing\nthe baseline model's code generation capabilities. Similarly, substituting\nSwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies\nconfirm that each pipeline stage contributes incrementally, with rewriting\ndelivering the largest gains. All datasets, prompts, and checkpoints are\npublicly available, enabling reproducible research and advancing LLM\npre-training for specialized domains.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.02881v2;http://arxiv.org/pdf/2505.02881v2", "pdf_url": "http://arxiv.org/pdf/2505.02881v2"}, {"title": "Exploring New Methods of Data Augmentation for Intent Classification Through Large Language Models", "link": "https://link.springer.com/chapter/10.1007/978-3-031-90341-0_2", "details": "N Madrue\u00f1o, A Fern\u00e1ndez-Isabel, RR Fern\u00e1ndez\u2026 - International Conference on \u2026, 2025", "abstract": "In intent classification tasks with constrained data, data augmentation can emerge as a powerful tool for enhancing Machine Learning (ML) model performance. In this context, existing augmentation methods using off-the-shelf general-purpose models \u2026"}, {"title": "PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval", "link": "https://arxiv.org/pdf/2505.17639", "details": "Z Pei, Y Zhang, HL Zhen, X Yu, W Liu, SJ Pan, M Yuan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment \u2026", "entry_id": "http://arxiv.org/abs/2505.17639v1", "updated": "2025-05-23 08:59:16", "published": "2025-05-23 08:59:16", "authors": "Zehua Pei;Ying Zhang;Hui-Ling Zhen;Xianzhi Yu;Wulong Liu;Sinno Jialin Pan;Mingxuan Yuan;Bei Yu", "summary": "Mixture-of-experts (MoE) architectures enable scaling large language models\n(LLMs) to vast parameter counts without a proportional rise in computational\ncosts. However, the significant memory demands of large MoE models hinder their\ndeployment across various computational environments, from cloud servers to\nconsumer devices. This study first demonstrates pronounced task-specific\nspecialization in expert activation patterns within MoE layers. Building on\nthis, we introduce PreMoe, a novel framework that enables efficient deployment\nof massive MoE models in memory-constrained environments. PreMoe features two\nmain components: probabilistic expert pruning (PEP) and task-adaptive expert\nretrieval (TAER). PEP employs a new metric, the task-conditioned expected\nselection score (TCESS), derived from router logits to quantify expert\nimportance for specific tasks, thereby identifying a minimal set of critical\nexperts. TAER leverages these task-specific expert importance profiles for\nefficient inference. It pre-computes and stores compact expert patterns for\ndiverse tasks. When a user query is received, TAER rapidly identifies the most\nrelevant stored task pattern and reconstructs the model by loading only the\nsmall subset of experts crucial for that task. This approach dramatically\nreduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B\nmaintains 97.2\\% accuracy on MATH500 when pruned to 8/128 configuration (50\\%\nexpert reduction), and still achieves 72.0\\% with aggressive 8/32 pruning\n(87.5\\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\\% on MATH500 and\n81.3\\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64\n(390GB memory) preserves 96.95\\% accuracy on MATH500. We make our code publicly\navailable at https://github.com/JarvisPei/PreMoe.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.17639v1;http://arxiv.org/pdf/2505.17639v1", "pdf_url": "http://arxiv.org/pdf/2505.17639v1"}, {"title": "Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data", "link": "https://arxiv.org/pdf/2505.17116", "details": "A Dhruv, Y Xie, J Branham, T Mallick - arXiv preprint arXiv:2505.17116, 2025", "abstract": "This paper presents a comparative study of large language models (LLMs) in interpreting grid-structured geospatial data. We evaluate the performance of a base model through structured prompting and contrast it with a fine-tuned variant trained \u2026", "entry_id": "http://arxiv.org/abs/2505.17116v1", "updated": "2025-05-21 16:27:51", "published": "2025-05-21 16:27:51", "authors": "Akash Dhruv;Yangxinyu Xie;Jordan Branham;Tanwi Mallick", "summary": "This paper presents a comparative study of large language models (LLMs) in\ninterpreting grid-structured geospatial data. We evaluate the performance of a\nbase model through structured prompting and contrast it with a fine-tuned\nvariant trained on a dataset of user-assistant interactions. Our results\nhighlight the strengths and limitations of zero-shot prompting and demonstrate\nthe benefits of fine-tuning for structured geospatial and temporal reasoning.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.ET", "links": "http://arxiv.org/abs/2505.17116v1;http://arxiv.org/pdf/2505.17116v1", "pdf_url": "http://arxiv.org/pdf/2505.17116v1"}]
