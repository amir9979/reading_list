[{"title": "ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models", "link": "https://arxiv.org/pdf/2406.13342", "details": "H Jo, H Lee, T Park - arXiv preprint arXiv:2406.13342, 2024", "abstract": "The recent advancements in large language models (LLMs) have brought significant progress in solving NLP tasks. Notably, in-context learning (ICL) is the key enabling mechanism for LLMs to understand specific tasks and grasping nuances. In this \u2026"}, {"title": "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference", "link": "https://arxiv.org/pdf/2406.17626", "details": "E Yu, J Li, M Liao, S Wang, Z Gao, F Mi, L Hong - arXiv preprint arXiv:2406.17626, 2024", "abstract": "As large language models (LLMs) constantly evolve, ensuring their safety remains a critical research problem. Previous red-teaming approaches for LLM safety have primarily focused on single prompt attacks or goal hijacking. To the best of our \u2026"}, {"title": "Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning", "link": "https://arxiv.org/pdf/2406.14022", "details": "X Wang, X Tang, WX Zhao, JR Wen - arXiv preprint arXiv:2406.14022, 2024", "abstract": "The emergence of in-context learning (ICL) is potentially attributed to two major abilities: task recognition (TR) for recognizing the task from demonstrations and utilizing pre-trained priors, and task learning (TL) for learning from demonstrations \u2026"}, {"title": "M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models", "link": "https://arxiv.org/pdf/2406.16783", "details": "R Maheshwary, V Yadav, H Nguyen, K Mahajan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Instruction finetuning (IFT) is critical for aligning Large Language Models (LLMs) to follow instructions. Numerous effective IFT datasets have been proposed in the recent past, but most focus on high resource languages such as English. In this work \u2026"}, {"title": "RadEx: A Framework for Structured Information Extraction from Radiology Reports based on Large Language Models", "link": "https://arxiv.org/pdf/2406.15465", "details": "D Reichenpfader, J Knupp, A Sander, K Denecke - arXiv preprint arXiv:2406.15465, 2024", "abstract": "Annually and globally, over three billion radiography examinations and computer tomography scans result in mostly unstructured radiology reports containing free text. Despite the potential benefits of structured reporting, its adoption is limited by factors \u2026"}, {"title": "Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models", "link": "https://arxiv.org/pdf/2406.09403", "details": "Y Hu, W Shi, X Fu, D Roth, M Ostendorf, L Zettlemoyer\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Humans draw to facilitate reasoning: we draw auxiliary lines when solving geometry problems; we mark and circle when reasoning on maps; we use sketches to amplify our ideas and relieve our limited-capacity working memory. However, such actions \u2026"}, {"title": "Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning", "link": "https://arxiv.org/pdf/2406.07543", "details": "C Yang, X Zhu, J Zhu, W Su, J Wang, X Dong, W Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, vision model pre-training has evolved from relying on manually annotated datasets to leveraging large-scale, web-crawled image-text data. Despite these advances, there is no pre-training method that effectively exploits the interleaved \u2026"}]
