[{"title": "LLM360 K2: Scaling Up 360-Open-Source Large Language Models", "link": "https://arxiv.org/pdf/2501.07124", "details": "Z Liu, B Tan, H Wang, W Neiswanger, T Tao, H Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree OPEN SOURCE approach to the largest and most powerful models under project LLM360. While open-source LLMs continue to advance, the answer to\" How are the \u2026"}, {"title": "LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2501.06986", "details": "MN Azadani, J Riddell, S Sedwards, K Czarnecki - arXiv preprint arXiv:2501.06986, 2025", "abstract": "Enhanced visual understanding serves as a cornerstone for multimodal large language models (MLLMs). Recent hybrid MLLMs incorporate a mixture of vision experts to address the limitations of using a single vision encoder and excessively \u2026"}, {"title": "Hierarchical Divide-and-Conquer for Fine-Grained Alignment in LLM-Based Medical Evaluation", "link": "https://arxiv.org/pdf/2501.06741", "details": "S Zheng, X Zhang, G de Melo, X Wang, L Wang - arXiv preprint arXiv:2501.06741, 2025", "abstract": "In the rapidly evolving landscape of large language models (LLMs) for medical applications, ensuring the reliability and accuracy of these models in clinical settings is paramount. Existing benchmarks often focus on fixed-format tasks like multiple \u2026"}]
