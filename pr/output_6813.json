[{"title": "Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination", "link": "https://arxiv.org/pdf/2409.12746", "details": "ES Salido, R Morante, J Gonzalo, G Marco\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this article we present UNED-ACCESS 2024, a bilingual dataset that consists of 1003 multiple-choice questions of university entrance level exams in Spanish and English. Questions are originally formulated in Spanish and translated manually into \u2026"}, {"title": "Understanding Defects in Generated Codes by Language Models", "link": "https://arxiv.org/pdf/2408.13372", "details": "AM Esfahani, N Kahani, SA Ajila - arXiv preprint arXiv:2408.13372, 2024", "abstract": "This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the \u2026"}, {"title": "Zero\u2010and few\u2010shot prompting of generative large language models provides weak assessment of risk of bias in clinical trials", "link": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1749", "details": "S \u0160uster, T Baldwin, K Verspoor - Research Synthesis Methods", "abstract": "Existing systems for automating the assessment of risk\u2010of\u2010bias (RoB) in medical studies are supervised approaches that require substantial training data to work well. However, recent revisions to RoB guidelines have resulted in a scarcity of available \u2026"}, {"title": "LLM with Relation Classifier for Document-Level Relation Extraction", "link": "https://arxiv.org/pdf/2408.13889", "details": "X Li, K Chen, Y Long, M Zhang - arXiv preprint arXiv:2408.13889, 2024", "abstract": "Large language models (LLMs) create a new paradigm for natural language processing. Despite their advancement, LLM-based methods still lag behind traditional approaches in document-level relation extraction (DocRE), a critical task \u2026"}, {"title": "A New Era in Computational Pathology: A Survey on Foundation and Vision-Language Models", "link": "https://arxiv.org/pdf/2408.14496", "details": "D Chanda, M Aryal, NY Soltani, M Ganji - arXiv preprint arXiv:2408.14496, 2024", "abstract": "Recent advances in deep learning have completely transformed the domain of computational pathology (CPath), which in turn altered the diagnostic workflow of pathologists by integrating foundation models (FMs) and vision-language models \u2026"}, {"title": "Small Language Models are Equation Reasoners", "link": "https://arxiv.org/pdf/2409.12393", "details": "B Kim, K Lee, J Kim, S Lee - arXiv preprint arXiv:2409.12393, 2024", "abstract": "Chain-of-Thought (CoT) reasoning has enabled Large Language Model (LLM) to achieve remarkable performance in various NLP tasks, including arithmetic problem- solving. However, this success does not generalize to small language model (sLM) \u2026"}, {"title": "Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization", "link": "https://direct.mit.edu/tacl/article/124459", "details": "G Chrysostomou, Z Zhao, M Williams, N Aletras - Transactions of the Association for \u2026, 2024", "abstract": "Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges: their considerable size and tendency to hallucinate. Hallucinations are concerning because they erode \u2026"}, {"title": "An open-source framework for end-to-end analysis of electronic health record data", "link": "https://www.nature.com/articles/s41591-024-03214-0", "details": "L Heumos, P Ehmele, T Treis, J Upmeier zu Belzen\u2026 - Nature Medicine, 2024", "abstract": "With progressive digitalization of healthcare systems worldwide, large-scale collection of electronic health records (EHRs) has become commonplace. However, an extensible framework for comprehensive exploratory analysis that accounts for \u2026"}, {"title": "Causal-Guided Active Learning for Debiasing Large Language Models", "link": "https://arxiv.org/pdf/2408.12942", "details": "Z Sun, L Du, X Ding, Y Ma, K Qiu, T Liu, B Qin - arXiv preprint arXiv:2408.12942, 2024", "abstract": "Although achieving promising performance, recent analyses show that current generative large language models (LLMs) may still capture dataset biases and utilize them for generation, leading to poor generalizability and harmfulness of LLMs \u2026"}]
