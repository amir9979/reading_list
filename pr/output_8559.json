[{"title": "Sleep apnea test prediction based on Electronic Health Records", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001552", "details": "LA Tahoun, AS Green, T Patalon, Y Dagan\u2026 - Journal of Biomedical \u2026, 2024", "abstract": "Abstract The identification of Obstructive Sleep Apnea (OSA) is done by a Polysomnography test which is often done in later ages. Being able to notify potential insured members at earlier ages is desirable. For that, we develop predictive models \u2026"}, {"title": "MEDS-Tab: Automated tabularization and baseline methods for MEDS datasets", "link": "https://arxiv.org/pdf/2411.00200", "details": "N Oufattole, T Bergamaschi, A Kolo, H Jeong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Effective, reliable, and scalable development of machine learning (ML) solutions for structured electronic health record (EHR) data requires the ability to reliably generate high-quality baseline models for diverse supervised learning tasks in an efficient and \u2026"}, {"title": "Tumor Location-weighted MRI-Report Contrastive Learning: A Framework for Improving the Explainability of Pediatric Brain Tumor Diagnosis", "link": "https://arxiv.org/pdf/2411.00609", "details": "S Ketabi, MW Wagner, C Hawkins, U Tabori\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the promising performance of convolutional neural networks (CNNs) in brain tumor diagnosis from magnetic resonance imaging (MRI), their integration into the clinical workflow has been limited. That is mainly due to the fact that the features \u2026"}, {"title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures", "link": "https://arxiv.org/pdf/2410.07698", "details": "Y Chen, Y Zhang, L Cao, K Yuan, Z Wen - arXiv preprint arXiv:2410.07698, 2024", "abstract": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs when adapting large language models (LLMs) for downstream applications. However, traditional first-order (FO) fine-tuning algorithms incur substantial memory overhead \u2026"}, {"title": "Prompt tuning discriminative language models for hierarchical text classification", "link": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/50E5499348A0E72F0C4F3AFC622133A7/S2977042424000517a.pdf/div-class-title-prompt-tuning-discriminative-language-models-for-hierarchical-text-classification-div.pdf", "details": "J du Toit, M Dunaiski - Natural Language Processing", "abstract": "Hierarchical text classification (HTC) is a natural language processing task which aims to categorise a text document into a set of classes from a hierarchical class structure. Recent approaches to solve HTC tasks focus on leveraging pre-trained \u2026"}, {"title": "ImageNet-RIB Benchmark: Large Pre-Training Datasets Don't Guarantee Robustness after Fine-Tuning", "link": "https://arxiv.org/pdf/2410.21582", "details": "J Hwang, B Cheung, ZW Hong, A Boopathy, P Agrawal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Highly performant large-scale pre-trained models promise to also provide a valuable foundation for learning specialized tasks, by fine-tuning the model to the desired task. By starting from a good general-purpose model, the goal is to achieve both \u2026"}, {"title": "Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs", "link": "https://arxiv.org/pdf/2410.10739", "details": "I Jindal, C Badrinath, P Bharti, L Vinay, SD Sharma - arXiv preprint arXiv:2410.10739, 2024", "abstract": "Large Language Models (LLMs) for public use require continuous pre-training to remain up-to-date with the latest data. The models also need to be fine-tuned with specific instructions to maintain their ability to follow instructions accurately. Typically \u2026"}, {"title": "A Deep Attention-Based Encoder for the Prediction of Type 2 Diabetes Longitudinal Outcomes from Routinely Collected Health Care Data", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/11/04/2024.11.02.24316561.full.pdf", "details": "E Manzini, B Vlacho, J Franch-Nadal, J Escudero\u2026 - medRxiv, 2024", "abstract": "Recent evidence indicates that Type 2 Diabetes Mellitus (T2DM) is a complex and highly heterogeneous disease involving various pathophysiological and genetic pathways, which presents clinicians with challenges in disease management. While \u2026"}, {"title": "Constraint Back-translation Improves Complex Instruction Following of Large Language Models", "link": "https://arxiv.org/pdf/2410.24175", "details": "Y Qi, H Peng, X Wang, B Xu, L Hou, J Li - arXiv preprint arXiv:2410.24175, 2024", "abstract": "Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs \u2026"}]
