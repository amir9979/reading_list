[{"title": "Super Tiny Language Models", "link": "https://arxiv.org/pdf/2405.14159", "details": "D Hillier, L Guertler, C Tan, P Agrawal, C Ruirui\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancement of large language models (LLMs) has led to significant improvements in natural language processing but also poses challenges due to their high computational and energy demands. This paper introduces a series of research \u2026"}, {"title": "Why are Visually-Grounded Language Models Bad at Image Classification?", "link": "https://arxiv.org/pdf/2405.18415", "details": "Y Zhang, A Unell, X Wang, D Ghosh, Y Su, L Schmidt\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Image classification is one of the most fundamental capabilities of machine vision intelligence. In this work, we revisit the image classification task using visually- grounded language models (VLMs) such as GPT-4V and LLaVA. We find that \u2026"}, {"title": "Comparative Analysis of Open-Source Language Models in Summarizing Medical Text Data", "link": "https://arxiv.org/pdf/2405.16295", "details": "Y Chen, Z Wang, B Wen, F Zulkernine - arXiv preprint arXiv:2405.16295, 2024", "abstract": "Unstructured text in medical notes and dialogues contains rich information. Recent advancements in Large Language Models (LLMs) have demonstrated superior performance in question answering and summarization tasks on unstructured text \u2026"}, {"title": "FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2405.18218", "details": "Y Zhang, Y Li, X Wang, Q Shen, B Plank, B Bischl\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Overparametrized transformer networks are the state-of-the-art architecture for Large Language Models (LLMs). However, such models contain billions of parameters making large compute a necessity, while raising environmental concerns. To \u2026"}, {"title": "Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models", "link": "https://arxiv.org/pdf/2405.17915", "details": "L Chen, Z Liu, W He, Y Li, R Luo, M Yang - arXiv preprint arXiv:2405.17915, 2024", "abstract": "Long-context modeling capabilities are important for large language models (LLMs) in various applications. However, directly training LLMs with long context windows is insufficient to enhance this capability since some training samples do not exhibit \u2026"}, {"title": "Peering into the Mind of Language Models: An Approach for Attribution in Contextual Question Answering", "link": "https://arxiv.org/pdf/2405.17980", "details": "A Phukan, S Somasundaram, A Saxena, K Goswami\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the enhancement in the field of generative artificial intelligence (AI), contextual question answering has become extremely relevant. Attributing model generations to the input source document is essential to ensure trustworthiness and reliability. We \u2026"}, {"title": "Reframing Spatial Reasoning Evaluation in Language Models: A Real-World Simulation Benchmark for Qualitative Reasoning", "link": "https://arxiv.org/pdf/2405.15064", "details": "F Li, DC Hogg, AG Cohn - arXiv preprint arXiv:2405.15064, 2024", "abstract": "Spatial reasoning plays a vital role in both human cognition and machine intelligence, prompting new research into language models'(LMs) capabilities in this regard. However, existing benchmarks reveal shortcomings in evaluating qualitative \u2026"}, {"title": "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning", "link": "https://arxiv.org/pdf/2405.13448", "details": "Y Yue, C Wang, J Huang, P Wang - arXiv preprint arXiv:2405.13448, 2024", "abstract": "The process of instruction tuning aligns pre-trained large language models (LLMs) with open-domain instructions and human-preferred responses. While several studies have explored autonomous approaches to distilling and annotating \u2026"}, {"title": "SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models", "link": "https://arxiv.org/pdf/2405.14917", "details": "W Huang, H Qin, Y Liu, Y Li, X Liu, L Benini, M Magno\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) achieve remarkable performance in natural language understanding but require substantial computation and memory resources. Post-training quantization (PTQ) is a powerful compression technique extensively \u2026"}]
