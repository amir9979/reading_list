[{"title": "Hierarchical Autoregressive Transformers: Combining Byte-and Word-Level Processing for Robust, Adaptable Language Models", "link": "https://arxiv.org/pdf/2501.10322", "details": "P Neitemeier, B Deiseroth, C Eichenberg, L Balles - arXiv preprint arXiv:2501.10322, 2025", "abstract": "Tokenization is a fundamental step in natural language processing, breaking text into units that computational models can process. While learned subword tokenizers have become the de-facto standard, they present challenges such as large \u2026"}, {"title": "Beyond the Singular: The Essential Role of Multiple Generations in Effective Benchmark Evaluation and Analysis", "link": "https://arxiv.org/pdf/2502.08943", "details": "W Zhang, H Cai, W Chen - arXiv preprint arXiv:2502.08943, 2025", "abstract": "Large language models (LLMs) have demonstrated significant utilities in real-world applications, exhibiting impressive capabilities in natural language processing and understanding. Benchmark evaluations are crucial for assessing the capabilities of \u2026"}, {"title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning", "link": "https://arxiv.org/pdf/2501.18858", "details": "H Zhong, Y Yin, S Zhang, X Xu, Y Liu, Y Zuo, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, yet generating reliable reasoning processes remains a significant challenge. We present a unified probabilistic framework that formalizes \u2026"}, {"title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models", "link": "https://arxiv.org/pdf/2502.07780", "details": "S Tang, O Sieberling, E Kurtic, Z Shen, D Alistarh - arXiv preprint arXiv:2502.07780, 2025", "abstract": "Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by \u2026"}, {"title": "JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation", "link": "https://arxiv.org/pdf/2502.07557", "details": "S Zhang, Y Zhai, K Guo, H Hu, S Guo, Z Fang, L Zhao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite the implementation of safety alignment strategies, large language models (LLMs) remain vulnerable to jailbreak attacks, which undermine these safety guardrails and pose significant security threats. Some defenses have been proposed \u2026"}, {"title": "RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation", "link": "https://arxiv.org/pdf/2502.09183", "details": "C Zhou, X Zhang, D Song, X Chen, W Gu, H Ma, Y Tian\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Code generation has attracted increasing attention with the rise of Large Language Models (LLMs). Many studies have developed powerful code LLMs by synthesizing code-related instruction data and applying supervised fine-tuning. However, these \u2026"}, {"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models", "link": "https://arxiv.org/pdf/2502.03199%3F", "details": "J Wu, Y Shen, S Liu, Y Tang, S Song, X Wang, L Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the \u2026"}, {"title": "Logical forms complement probability in understanding language model (and human) performance", "link": "https://arxiv.org/pdf/2502.09589", "details": "Y Wang, F Shi - arXiv preprint arXiv:2502.09589, 2025", "abstract": "With the increasing interest in using large language models (LLMs) for planning in natural language, understanding their behaviors becomes an important research question. This work conducts a systematic investigation of LLMs' ability to perform \u2026"}, {"title": "How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence", "link": "https://arxiv.org/pdf/2502.00678", "details": "HK Choi, M Khanov, H Wei, Y Li - arXiv preprint arXiv:2502.00678, 2025", "abstract": "Dataset contamination, where evaluation datasets overlap with pre-training corpora, inflates performance metrics and undermines the reliability of model evaluations. Quantifying dataset contamination thus becomes essential to ensure that \u2026"}]
