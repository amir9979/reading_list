[{"title": "Identifying and Mitigating Social Bias Knowledge in Language Models", "link": "https://aclanthology.org/2025.findings-naacl.39.pdf", "details": "R Chen, Y Li, J Yang, Y Feng, JT Zhou, J Wu, Z Liu - Findings of the Association for \u2026, 2025", "abstract": "Generating fair and accurate predictions plays a pivotal role in deploying pre-trained language models (PLMs) in the real world. However, existing debiasing methods may inevitably generate incorrect or nonsensical predictions as they are designed \u2026"}, {"title": "Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization", "link": "https://arxiv.org/pdf/2504.12083", "details": "P Sarkar, A Etemad - arXiv preprint arXiv:2504.12083, 2025", "abstract": "Despite recent advances in Large Video Language Models (LVLMs), they still struggle with fine-grained temporal understanding, hallucinate, and often make simple mistakes on even simple video question-answering tasks, all of which pose \u2026"}, {"title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models", "link": "https://arxiv.org/pdf/2504.12526", "details": "J Zhang, T Zhu, C Luo, A Anandkumar - arXiv preprint arXiv:2504.12526, 2025", "abstract": "Long-context language models exhibit impressive performance but remain challenging to deploy due to high GPU memory demands during inference. We propose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that \u2026"}, {"title": "Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models", "link": "https://arxiv.org/pdf/2504.12315", "details": "X Ji, J Wang, H Zhang, J Zhang, H Zhou, C Sun, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the development of Multimodal Large Language Models (MLLMs), numerous outstanding accomplishments have emerged within the open-source community. Due to the complexity of creating and training multimodal data pairs, it is still a \u2026"}, {"title": "ReXGradient-160K: A Large-Scale Publicly Available Dataset of Chest Radiographs with Free-text Reports", "link": "https://arxiv.org/pdf/2505.00228", "details": "X Zhang, JN Acosta, J Miller, O Huang, P Rajpurkar - arXiv preprint arXiv:2505.00228, 2025", "abstract": "We present ReXGradient-160K, representing the largest publicly available chest X- ray dataset to date in terms of the number of patients. This dataset contains 160,000 chest X-ray studies with paired radiological reports from 109,487 unique patients \u2026"}, {"title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation", "link": "https://arxiv.org/pdf/2504.06861%3F", "details": "D Jagpal, X Chen, VP Namboodiri - arXiv preprint arXiv:2504.06861, 2025", "abstract": "Zero-shot, training-free, image-based text-to-video generation is an emerging area that aims to generate videos using existing image-based diffusion models. Current methods in this space require specific architectural changes to image generation \u2026"}, {"title": "SRG-Net: A Self-supervised 3D Scene Representation Method via Graph Contrastive Learning for Novel View Synthesis", "link": "https://www.jstage.jst.go.jp/article/transfun/advpub/0/advpub_2024EAL2091/_pdf", "details": "Q Qi, Z Liu, Y Guo - IEICE Transactions on Fundamentals of Electronics \u2026, 2025", "abstract": "SUMMARYAccurate scene representation holds practical significance for autonomous driving and virtual reality. This letter proposes a network to optimize images encoding and features learning for better scene representations \u2026"}, {"title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer", "link": "https://arxiv.org/pdf/2504.10462", "details": "W Lei, J Wang, H Wang, X Li, JH Liew, J Feng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained \u2026"}, {"title": "SVLTA: Benchmarking Vision-Language Temporal Alignment via Synthetic Video Situation", "link": "https://arxiv.org/pdf/2504.05925", "details": "H Du, B Wu, Y Lu, Z Mao - arXiv preprint arXiv:2504.05925, 2025", "abstract": "Vision-language temporal alignment is a crucial capability for human dynamic recognition and cognition in real-world scenarios. While existing research focuses on capturing vision-language relevance, it faces limitations due to biased temporal \u2026"}]
