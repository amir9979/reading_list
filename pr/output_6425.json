[{"title": "GraphWiz: An Instruction-Following Language Model for Graph Computational Problems", "link": "https://dl.acm.org/doi/abs/10.1145/3637528.3672010", "details": "N Chen, Y Li, J Tang, J Li - Proceedings of the 30th ACM SIGKDD Conference on \u2026, 2024", "abstract": "Large language models (LLMs) have achieved impressive success across various domains, but their capability in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel \u2026"}, {"title": "Does Knowledge Localization Hold True? Surprising Differences Between Entity and Relation Perspectives in Language Models", "link": "https://arxiv.org/pdf/2409.00617", "details": "Y Wei, X Yu, Y Weng, H Ma, Y Zhang, J Zhao, K Liu - arXiv preprint arXiv:2409.00617, 2024", "abstract": "Large language models encapsulate knowledge and have demonstrated superior performance on various natural language processing tasks. Recent studies have localized this knowledge to specific model parameters, such as the MLP weights in \u2026"}, {"title": "Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models", "link": "https://aclanthology.org/2024.acl-long.815.pdf", "details": "T Madusanka, I Pratt-Hartmann, RT Batista-Navarro - \u2026 of the 62nd Annual Meeting of \u2026, 2024", "abstract": "Efforts to apply transformer-based language models (TLMs) to the problem of reasoning in natural language have enjoyed ever-increasing success in recent years. The most fundamental task in this area to which nearly all others can be \u2026"}, {"title": "TabSAL: Synthesizing tabular data with small agent assisted language models", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124010724", "details": "J Li, R Qian, Y Tan, Z Li, L Chen, S Liu, J Wu, H Chai - Knowledge-Based Systems, 2024", "abstract": "Tabular data are widely used in machine-learning tasks because of their prevalence in various fields; however, the potential risks of data breaches in tabular data and privacy protection regulations render such data almost unavailable. Tabular data \u2026"}, {"title": "CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks", "link": "https://arxiv.org/pdf/2409.03381", "details": "Y Deng, X Qiu, X Tan, C Qu, J Pan, Y Cheng, Y Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Cognitive psychology investigates perception, attention, memory, language, problem- solving, decision-making, and reasoning. Kahneman's dual-system theory elucidates the human decision-making process, distinguishing between the rapid, intuitive \u2026"}, {"title": "Selective Self-Rehearsal: A Fine-Tuning Approach to Improve Generalization in Large Language Models", "link": "https://arxiv.org/pdf/2409.04787", "details": "S Gupta, Y Nandwani, A Yehudai, M Mishra, G Pandey\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task \u2026"}, {"title": "Revolutionizing Database Q&A with Large Language Models: Comprehensive Benchmark and Evaluation", "link": "https://arxiv.org/pdf/2409.04475", "details": "Y Zheng, B Li, Z Lin, Y Luo, X Zhou, C Lin, J Su, G Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The development of Large Language Models (LLMs) has revolutionized Q&A across various industries, including the database domain. However, there is still a lack of a comprehensive benchmark to evaluate the capabilities of different LLMs and their \u2026"}, {"title": "Reasoning and Planning with Large Language Models in Code Development", "link": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671452", "details": "H Ding, Z Fan, I Guehring, G Gupta, W Ha, J Huan\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code \u2026"}, {"title": "Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback", "link": "https://arxiv.org/pdf/2409.00162", "details": "J Zhou, J Ji, J Dai, Y Yang - arXiv preprint arXiv:2409.00162, 2024", "abstract": "Aligning the behavior of Large language models (LLMs) with human intentions and values remains a critical challenge. Reinforcement learning from human feedback (RLHF) aligns LLMs by training a reward model (RM) on human preferences and fine \u2026"}]
