[{"title": "Symmetric Pruning of Large Language Models", "link": "https://arxiv.org/pdf/2501.18980", "details": "K Yi, P Richt\u00e1rik - arXiv preprint arXiv:2501.18980, 2025", "abstract": "Popular post-training pruning methods such as Wanda and RIA are known for their simple, yet effective, designs that have shown exceptional empirical performance. Wanda optimizes performance through calibrated activations during pruning, while \u2026"}, {"title": "The Relationship Between Reasoning and Performance in Large Language Models--o3 (mini) Thinks Harder, Not Longer", "link": "https://arxiv.org/pdf/2502.15631", "details": "M Ballon, A Algaba, V Ginis - arXiv preprint arXiv:2502.15631, 2025", "abstract": "Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token \u2026"}]
