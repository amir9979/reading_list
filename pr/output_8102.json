[{"title": "No Need to Talk: Asynchronous Mixture of Language Models", "link": "https://arxiv.org/pdf/2410.03529%3F", "details": "A Filippova, A Katharopoulos, D Grangier, R Collobert - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce SmallTalk LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each model of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth \u2026"}, {"title": "Scaling Parameter-Constrained Language Models with Quality Data", "link": "https://arxiv.org/pdf/2410.03083", "details": "E Chang, M Paltenghi, Y Li, PJ Lin, C Zhao, P Huber\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws in language modeling traditionally quantify training loss as a function of dataset size and model parameters, providing compute-optimal estimates but often neglecting the impact of data quality on model generalization. In this paper, we \u2026"}, {"title": "Ascle\u2014A Python Natural Language Processing Toolkit for Medical Text Generation: Development and Evaluation Study", "link": "https://www.jmir.org/2024/1/e60601/", "details": "R Yang, Q Zeng, K You, Y Qiao, L Huang, CC Hsieh\u2026 - Journal of Medical Internet \u2026, 2024", "abstract": "Background Medical texts present significant domain-specific challenges, and manually curating these texts is a time-consuming and labor-intensive process. To address this, natural language processing (NLP) algorithms have been developed to \u2026"}, {"title": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt Optimization for Text Generation", "link": "https://arxiv.org/pdf/2410.02748%3F", "details": "H He, Q Liu, L Xu, C Shivade, Y Zhang, S Srinivasan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) can generate fluent summaries across domains using prompting techniques, reducing the need to train models for summarization applications. However, crafting effective prompts that guide LLMs to generate \u2026"}, {"title": "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "link": "https://arxiv.org/pdf/2410.03122", "details": "Z Zhao, Y Yang, Y Li, Y Cao - arXiv preprint arXiv:2410.03122, 2024", "abstract": "The ripple effect poses a significant challenge in knowledge editing for large language models. Namely, when a single fact is edited, the model struggles to accurately update the related facts in a sequence, which is evaluated by multi-hop \u2026"}, {"title": "Prompt tuning discriminative language models for hierarchical text classification", "link": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/50E5499348A0E72F0C4F3AFC622133A7/S2977042424000517a.pdf/div-class-title-prompt-tuning-discriminative-language-models-for-hierarchical-text-classification-div.pdf", "details": "J du Toit, M Dunaiski - Natural Language Processing", "abstract": "Hierarchical text classification (HTC) is a natural language processing task which aims to categorise a text document into a set of classes from a hierarchical class structure. Recent approaches to solve HTC tasks focus on leveraging pre-trained \u2026"}, {"title": "POSIX: A Prompt Sensitivity Index For Large Language Models", "link": "https://arxiv.org/pdf/2410.02185", "details": "A Chatterjee, HK Renduchintala, S Bhatia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are found to be surprisingly sensitive to minor variations in prompts, often generating significantly divergent outputs in response to minor variations in the prompts, such as spelling \u2026"}, {"title": "Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers", "link": "https://arxiv.org/pdf/2410.02642", "details": "S Chen, BJ Guti\u00e9rrez, Y Su - arXiv preprint arXiv:2410.02642, 2024", "abstract": "Information retrieval (IR) systems have played a vital role in modern digital life and have cemented their continued usefulness in this new era of generative AI via retrieval-augmented generation. With strong language processing capabilities and \u2026"}, {"title": "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "link": "https://arxiv.org/pdf/2410.14897", "details": "M Roemmele, AS Gordon - arXiv preprint arXiv:2410.14897, 2024", "abstract": "LLMs can now perform a variety of complex writing tasks. They also excel in answering questions pertaining to natural language inference and commonsense reasoning. Composing these questions is itself a skilled writing task, so in this paper \u2026"}]
