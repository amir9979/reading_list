[{"title": "CAAT-EHR: Cross-Attentional Autoregressive Transformer for Multimodal Electronic Health Record Embeddings", "link": "https://arxiv.org/pdf/2501.18891", "details": "MA Olaimat, S Bozdag - arXiv preprint arXiv:2501.18891, 2025", "abstract": "Electronic health records (EHRs) provide a comprehensive source of longitudinal patient data, encompassing structured modalities such as laboratory results, imaging data, and vital signs, and unstructured clinical notes. These datasets, after necessary \u2026"}, {"title": "Self-supervised analogical learning using language models", "link": "https://arxiv.org/pdf/2502.00996", "details": "B Zhou, S Jain, Y Zhang, Q Ning, S Wang, Y Benajiba\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can \u2026"}, {"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}, {"title": "On the Robustness of Temporal Factual Knowledge in Language Models", "link": "https://arxiv.org/pdf/2502.01220", "details": "HA Khodja, F B\u00e9chet, Q Brabant, A Nasr, G Lecorv\u00e9 - arXiv preprint arXiv:2502.01220, 2025", "abstract": "This paper explores the temporal robustness of language models (LMs) in handling factual knowledge. While LMs can often complete simple factual statements, their ability to manage temporal facts (those valid only within specific timeframes) remains \u2026"}, {"title": "Weak Supervision Dynamic KL-Weighted Diffusion Models Guided by Large Language Models", "link": "https://arxiv.org/pdf/2502.00826", "details": "J Perry, F Sanders, C Scott - arXiv preprint arXiv:2502.00826, 2025", "abstract": "In this paper, we presents a novel method for improving text-to-image generation by combining Large Language Models (LLMs) with diffusion models, a hybrid approach aimed at achieving both higher quality and efficiency in image synthesis from text \u2026"}, {"title": "Generalization of Medical Large Language Models through Cross-Domain Weak Supervision", "link": "https://arxiv.org/pdf/2502.00832", "details": "R Long, E Gonzalez, H Fuller - arXiv preprint arXiv:2502.00832, 2025", "abstract": "The advancement of large language models (LLMs) has opened new frontiers in natural language processing, particularly in specialized domains like healthcare. In this paper, we propose the Incremental Curriculum-Based Fine-Tuning (ICFT) \u2026"}, {"title": "FIRE: Flexible Integration of Data Quality Ratings for Effective Pre-Training", "link": "https://arxiv.org/pdf/2502.00761", "details": "L Xu, X Zhang, F Duan, S Wang, J Wang, X Cai - arXiv preprint arXiv:2502.00761, 2025", "abstract": "Selecting high-quality data can significantly improve the pre-training efficiency of large language models (LLMs). Existing methods often rely on heuristic techniques and single quality signals, limiting their ability to comprehensively evaluate data \u2026"}, {"title": "Mordal: Automated Pretrained Model Selection for Vision Language Models", "link": "https://arxiv.org/pdf/2502.00241", "details": "S He, I Jang, M Chowdhury - arXiv preprint arXiv:2502.00241, 2025", "abstract": "Incorporating multiple modalities into large language models (LLMs) is a powerful way to enhance their understanding of non-textual data, enabling them to perform multimodal tasks. Vision language models (VLMs) form the fastest growing category \u2026"}, {"title": "MedFILIP: Medical Fine-Grained Language-Image Pre-Training", "link": "https://arxiv.org/pdf/2501.10775", "details": "X Liang, X Li, F Li, J Jiang, Q Dong, W Wang, K Wang\u2026 - IEEE Journal of Biomedical \u2026, 2025", "abstract": "Medical vision-language pretraining (VLP) that leverages naturally-paired medical image-report data is crucial for medical image analysis. However, existing methods struggle to accurately characterize associations between images and diseases \u2026"}]
