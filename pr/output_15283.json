[{"title": "Guiding Reasoning in Small Language Models with LLM Assistance", "link": "https://arxiv.org/pdf/2504.09923", "details": "Y Kim, E Yi, M Kim, SY Yun, T Kim - arXiv preprint arXiv:2504.09923, 2025", "abstract": "The limited reasoning capabilities of small language models (SLMs) cast doubt on their suitability for tasks demanding deep, multi-step logical deduction. This paper introduces a framework called Small Reasons, Large Hints (SMART), which \u2026"}, {"title": "SCARF: Single Cell ATAC-seq and RNA-seq Foundation model", "link": "https://www.biorxiv.org/content/biorxiv/early/2025/04/13/2025.04.07.647689.full.pdf", "details": "G Liu, Y Zhao, Y Zhao, T Wang, Q Cai, X Wang, Z Wen\u2026 - bioRxiv, 2025", "abstract": "Recent advances in single-cell multi-omics have provided unprecedented insights into gene regulation by jointly profiling transcriptomic (scRNA-seq) and chromatin accessibility (scATAC-seq) landscapes. However, the inherent heterogeneity and \u2026"}, {"title": "CL-CoTNav: Closed-Loop Hierarchical Chain-of-Thought for Zero-Shot Object-Goal Navigation with Vision-Language Models", "link": "https://arxiv.org/pdf/2504.09000", "details": "Y Cai, X He, M Wang, H Guo, WY Yau, C Lv - arXiv preprint arXiv:2504.09000, 2025", "abstract": "Visual Object Goal Navigation (ObjectNav) requires a robot to locate a target object in an unseen environment using egocentric observations. However, decision-making policies often struggle to transfer to unseen environments and novel target objects \u2026"}, {"title": "Exploration of Plan-Guided Summarization for Narrative Texts: the Case of Small Language Models", "link": "https://arxiv.org/pdf/2504.09071", "details": "M Grenander, S Varia, P Czarnowska, Y Vyas\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Plan-guided summarization attempts to reduce hallucinations in small language models (SLMs) by grounding generated summaries to the source text, typically by targeting fine-grained details such as dates or named entities. In this work, we \u2026"}, {"title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models", "link": "https://arxiv.org/pdf/2504.10415", "details": "P Shojaee, NH Nguyen, K Meidani, AB Farimani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their \u2026"}, {"title": "Learning interpretable representation for context-specific transcription regulatory networks using a foundation model", "link": "https://www.biorxiv.org/content/biorxiv/early/2025/04/06/2025.03.29.646077.full.pdf", "details": "Z Yu, D Yang, Q Chen, Y Zhang, Z Li, Y Wang, C Wang\u2026 - bioRxiv, 2025", "abstract": "Gene expression is shaped by transcription regulatory networks (TRNs), where transcription regulators interact within regulatory elements in a context-specific manner. Despite significant efforts, understanding the intricate interactions of \u2026"}, {"title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems", "link": "https://arxiv.org/pdf/2504.09037", "details": "Z Ke, F Jiao, Y Ming, XP Nguyen, A Xu, DX Long, M Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reasoning is a fundamental cognitive process that enables logical inference, problem-solving, and decision-making. With the rapid advancement of large language models (LLMs), reasoning has emerged as a key capability that \u2026"}, {"title": "How Robust Are Router-LLMs? Analysis of the Fragility of LLM Routing Capabilities", "link": "https://arxiv.org/pdf/2504.07113", "details": "AM Kassem, B Sch\u00f6lkopf, Z Jin - arXiv preprint arXiv:2504.07113, 2025", "abstract": "Large language model (LLM) routing has emerged as a crucial strategy for balancing computational costs with performance by dynamically assigning queries to the most appropriate model based on query complexity. Despite recent advances showing \u2026"}, {"title": "Probing the Symbolic Logical Reasoning Ability of Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3729238", "details": "J Ji, Z Li, S Xu, W Hua, J Tan, H Gong, Y Zhang - ACM Transactions on Intelligent Systems \u2026", "abstract": "Large Language Models (LLMs) have achieved significant successes in various research domains by learning the relationship between words. However, while these models are capable of making predictions and inferences based on the learned \u2026"}]
