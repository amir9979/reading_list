[{"title": "Report Cards: Qualitative Evaluation of Language Models Using Natural Language Summaries", "link": "https://arxiv.org/pdf/2409.00844", "details": "B Yang, F Cui, K Paster, J Ba, P Vaezipoor, S Pitis\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid development and dynamic nature of large language models (LLMs) make it difficult for conventional quantitative benchmarks to accurately assess their capabilities. We propose report cards, which are human-interpretable, natural \u2026"}, {"title": "Health professionals' perceptions of electronic health records system: a mixed method study in Ghana", "link": "https://link.springer.com/article/10.1186/s12911-024-02672-3", "details": "NK Mensah, G Adzakpah, J Kissi, K Abdulai\u2026 - BMC Medical Informatics \u2026, 2024", "abstract": "Abstract Background Electronic Health Record systems (EHRs) offer significant benefits and have transformed healthcare in developed countries. However, their implementation and adoption in low-and middle-income countries (LMICs) remains \u2026"}, {"title": "Language Models Benefit from Preparation with Elicited Knowledge", "link": "https://arxiv.org/pdf/2409.01345", "details": "J Yu, H An, LK Schubert - arXiv preprint arXiv:2409.01345, 2024", "abstract": "The zero-shot chain of thought (CoT) approach is often used in question answering (QA) by language models (LMs) for tasks that require multiple reasoning steps, typically enhanced by the prompt\" Let's think step by step.\" However, some QA tasks \u2026"}, {"title": "Focused Large Language Models are Stable Many-Shot Learners", "link": "https://arxiv.org/pdf/2408.13987", "details": "P Yuan, S Feng, Y Li, X Wang, Y Zhang, C Tan, B Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL \u2026"}, {"title": "Automated Mining of Structured Knowledge from Text in the Era of Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671469", "details": "Y Zhang, M Zhong, S Ouyang, Y Jiao, S Zhou, L Ding\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "Massive amount of unstructured text data are generated daily, ranging from news articles to scientific papers. How to mine structured knowledge from the text data remains a crucial research question. Recently, large language models (LLMs) have \u2026"}, {"title": "A Survey for Large Language Models in Biomedicine", "link": "https://arxiv.org/pdf/2409.00133", "details": "C Wang, M Li, J He, Z Wang, E Darzi, Z Chen, J Ye, T Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent breakthroughs in large language models (LLMs) offer unprecedented natural language understanding and generation capabilities. However, existing surveys on LLMs in biomedicine often focus on specific applications or model architectures \u2026"}]
