'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Predictive Modeling with Temporal Graphical Representa'
[{"title": "Filling the gaps: leveraging large language models for temporal harmonization of clinical text across multiple medical visits for clinical prediction", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/05/07/2024.05.06.24306959.full.pdf", "details": "I Choi, Q Long, E Getzen - medRxiv, 2024", "abstract": "Electronic health records offer great promise for early disease detection, treatment evaluation, information discovery, and other important facets of precision health. Clinical notes, in particular, may contain nuanced information about a patient's \u2026"}, {"title": "Masked Modeling Duo: Towards a Universal Audio Pre-Training Framework", "link": "https://ieeexplore.ieee.org/iel7/6570655/6633080/10502167.pdf", "details": "D Niizumi, D Takeuchi, Y Ohishi, N Harada, K Kashino - IEEE/ACM Transactions on \u2026, 2024", "abstract": "Self-supervised learning (SSL) using masked prediction has made great strides in general-purpose audio representation. This study proposes Masked Modeling Duo (M2D), an improved masked prediction SSL, which learns by predicting \u2026"}, {"title": "Evaluating Text Summaries Generated by Large Language Models Using OpenAI's GPT", "link": "https://arxiv.org/pdf/2405.04053", "details": "H Shakil, AM Mahi, P Nguyen, Z Ortiz, MT Mardini - arXiv preprint arXiv:2405.04053, 2024", "abstract": "This research examines the effectiveness of OpenAI's GPT models as independent evaluators of text summaries generated by six transformer-based models from Hugging Face: DistilBART, BERT, ProphetNet, T5, BART, and PEGASUS. We \u2026"}, {"title": "Topicwise Separable Sentence Retrieval for Medical Report Generation", "link": "https://arxiv.org/pdf/2405.04175", "details": "J Zhao, Y Zhou, Z Chen, H Fu, L Wan - arXiv preprint arXiv:2405.04175, 2024", "abstract": "Automated radiology reporting holds immense clinical potential in alleviating the burdensome workload of radiologists and mitigating diagnostic bias. Recently, retrieval-based report generation methods have garnered increasing attention due to \u2026"}, {"title": "CLE-SMOTE: Addressing Extreme Imbalanced Data Classification with Contrastive Learning-Enhanced SMOTE", "link": "https://openreview.net/pdf%3Fid%3De641mv2xsf", "details": "C Lee, F Nabulsi, M Xu, C Kan, A Kan, R Yun, B Jiang\u2026", "abstract": "Synthetic Minority Oversampling Technique (SMOTE) is a widely used oversampling method for addressing class imbalance by generating synthetic minority class examples. While effective, SMOTE occasionally introduces harmful examples into the \u2026"}, {"title": "CausalBench: A Comprehensive Benchmark for Causal Learning Capability of Large Language Models", "link": "https://arxiv.org/pdf/2404.06349", "details": "Y Zhou, X Wu, B Huang, J Wu, L Feng, KC Tan - arXiv preprint arXiv:2404.06349, 2024", "abstract": "Causality reveals fundamental principles behind data distributions in real-world scenarios, and the capability of large language models (LLMs) to understand causality directly impacts their efficacy across explaining outputs, adapting to new \u2026"}, {"title": "Understanding Inverse Scaling and Emergence in Multitask Representation Learning", "link": "https://proceedings.mlr.press/v238/e-ildiz24a/e-ildiz24a.pdf", "details": "ME Ildiz, Z Zhao, S Oymak - International Conference on Artificial Intelligence and \u2026, 2024", "abstract": "Large language models exhibit strong multitasking capabilities, however, their learning dynamics as a function of task characteristics, sample size, and model complexity remain mysterious. For instance, it is known that, as the model size grows \u2026"}]
