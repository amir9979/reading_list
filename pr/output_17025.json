[{"title": "How valuable are the **questions** and **answers** generated by **large language models** in oral and maxillofacial surgery?", "link": "https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0322529", "details": "K Kim, SB Mun, YJ Kim, BC Kim, KG Kim - PLoS One, 2025", "abstract": "\u2026 A flow diagram showing the process by which **large** **language** **models** generate and **answer** **questions** about oral and maxillofacial \u2026 **Medical** Licensing Examination (USMLE)? The implications of **large** **language** **models** for **medical** \u2026"}, {"title": "Evaluating the Performance of Seven **Large Language Models** \u200e\u200e(GPT4. 5, Gemini, Copilot, Claude,\u200e Perplexity, DeepSeek, and Manus)\u200e in **Answering Healthcare** Quality \u2026", "link": "https://www.paradigmpress.org/rae/article/view/1642", "details": "M Sallam, J Snygg, A Hamdan, D Allam, R Kassem\u2026 - Research and Advances in \u2026, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) are increasingly utilized across education, **healthcare** , and decision support due to their advanced text \u2026 in **answering** multiple-choice **questions** related to **healthcare** quality management. The assessment included 20 \u2026"}, {"title": "Benchmarking clinical reasoning and accuracy of **large language models** on breast oncology multiple-choice **questions**.", "link": "https://ascopubs.org/doi/abs/10.1200/JCO.2025.43.16_suppl.e13637", "details": "R Odabashian, AS Basta, R Sidgal, A Chao, T Lin\u2026 - 2025", "abstract": "e13637 Background: **Large** **language** **models** (LLMs) like GPT-4 (OpenAI) and Claude Opus (Anthropic) showed high accuracy in **medical** multiple-choice exams, but data on their oncology-specific clinical reasoning and performance is limited \u2026"}, {"title": "An Exhaustive Analysis Of **Large Language Models**", "link": "https://www.researchgate.net/profile/Tejesvi-Prasad-2/publication/392131562_Nanotechnology_Perceptions_ISSN_1660-6795_www/links/68366237df0e3f544f5b1a92/Nanotechnology-Perceptions-ISSN-1660-6795-www.pdf", "details": "TA Prasad", "abstract": "\u2026 in translation, summarization, and **question** - **answering**. Transformer models are used in neural \u2026 In **question** **answering** , LLMs break down context to extract accurate **answers** , with 92% \u2026 **Large** **language** **models** in **medical** ethics: Useful but \u2026"}, {"title": "A Comprehensive Overview and Analysis of **Large Language Models** : Trends and Challenges", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/11015742.pdf", "details": "A Mohammed, R Kora - IEEE Access, 2025", "abstract": "\u2026 , balances performance and computational efficiency for tasks such as **question** **answering** and text summarization. ModernBERT-Large [\u2026 as visual **question** **answer** ing and content creation. Domain-specific GPT models [175], such as \u2026"}, {"title": "The accuracy and efficiency of **large language models** for chart review in cancer genetics.", "link": "https://ascopubs.org/doi/abs/10.1200/JCO.2025.43.16_suppl.e22603", "details": "J Dickerson, M Shaw, M Satoyoshi, S Rios-Ventura\u2026 - 2025", "abstract": "\u2026 Background: Constructing databases is crucial for **answering** clinical **questions** but is time-consuming and error-prone. Our institution has \u2026 We made two API calls per note using Stanford **Healthcare** Secure GPT with OpenAI\u2019s gpt-4o model. The \u2026"}, {"title": "Analysis of a large language model-based system versus manual review in clinical data abstraction and deduction from real-world **medical** records of patients with \u2026", "link": "https://ascopubs.org/doi/abs/10.1200/JCO.2025.43.16_suppl.1571", "details": "C Vecchio, S Braley, LB Kennedy, J Isaacs, TG Truong\u2026 - 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs), have shown promise in natural language understanding, and automating chart review and abstraction \u2026 each, and Cohort (B) consisting of 25 different EMRs, posed 22 eligibility **questions** each. In total, there \u2026"}, {"title": "BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain", "link": "https://arxiv.org/pdf/2505.22240", "details": "Y Kim, Y Abdulle, H Wu - arXiv preprint arXiv:2505.22240, 2025", "abstract": "\u2026 Recent advances in **large** **language** **models** (LLMs) and **Question** **Answering** (QA) systems have shifted the focus from simple factoid \u2026 Medmcqa: A large-scale multi-subject multi-choice dataset for **medical** domain **question** **answering**. In Conference on \u2026", "entry_id": "http://arxiv.org/abs/2505.22240v1", "updated": "2025-05-28 11:19:01", "published": "2025-05-28 11:19:01", "authors": "Yunsoo Kim;Yusuf Abdulle;Honghan Wu", "summary": "Biomedical reasoning often requires traversing interconnected relationships\nacross entities such as drugs, diseases, and proteins. Despite the increasing\nprominence of large language models (LLMs), existing benchmarks lack the\nability to evaluate multi-hop reasoning in the biomedical domain, particularly\nfor queries involving one-to-many and many-to-many relationships. This gap\nleaves the critical challenges of biomedical multi-hop reasoning underexplored.\nTo address this, we introduce BioHopR, a novel benchmark designed to evaluate\nmulti-hop, multi-answer reasoning in structured biomedical knowledge graphs.\nBuilt from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop\nreasoning tasks that reflect real-world biomedical complexities.\n  Evaluations of state-of-the-art models reveal that O3-mini, a proprietary\nreasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on\n2-hop tasks, outperforming proprietary models such as GPT4O and open-source\nbiomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all\nmodels exhibit significant declines in multi-hop performance, underscoring the\nchallenges of resolving implicit reasoning steps in the biomedical domain. By\naddressing the lack of benchmarks for multi-hop reasoning in biomedical domain,\nBioHopR sets a new standard for evaluating reasoning capabilities and\nhighlights critical gaps between proprietary and open-source models while\npaving the way for future advancements in biomedical LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.22240v1;http://arxiv.org/pdf/2505.22240v1", "pdf_url": "http://arxiv.org/pdf/2505.22240v1"}, {"title": "Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning", "link": "https://arxiv.org/pdf/2505.21958", "details": "Q Zhong, L Ding, F Liao, J Liu, B Du, D Tao - arXiv preprint arXiv:2505.21958, 2025", "abstract": "\u2026 Abstract\u2014Domain-specific instruction-tuning has become the defacto standard for improving the performance of **large** **language** **models** (LLMs) in specialized applications, eg, **medical** **question** **answering**. Since the instruction-tuning dataset \u2026", "entry_id": "http://arxiv.org/abs/2505.21958v1", "updated": "2025-05-28 04:18:24", "published": "2025-05-28 04:18:24", "authors": "Qihuang Zhong;Liang Ding;Fei Liao;Juhua Liu;Bo Du;Dacheng Tao", "summary": "Domain-specific instruction-tuning has become the defacto standard for\nimproving the performance of large language models (LLMs) in specialized\napplications, e.g., medical question answering. Since the instruction-tuning\ndataset might contain redundant or low-quality data, data selection (DS) is\nusually required to maximize the data efficiency. Despite the successes in the\ngeneral domain, current DS methods often struggle to select the desired data\nfor domain-specific instruction-tuning. One of the main reasons is that they\nneglect the impact of knowledge conflicts, i.e., the discrepancy between LLMs'\npretrained knowledge and context knowledge of instruction data, which could\ndamage LLMs' prior abilities and lead to hallucination. To this end, we propose\na simple-yet-effective Knowledge-aware Data Selection (namely KDS) framework to\nselect the domain-specific instruction-tuning data that meets LLMs' actual\nneeds. The core of KDS is to leverage two knowledge-aware metrics for\nquantitatively measuring knowledge conflicts from two aspects: context-memory\nknowledge alignment and intra-memory knowledge consistency. By filtering the\ndata with large knowledge conflicts and sampling the high-quality and diverse\ndata, KDS can effectively stimulate the LLMs' abilities and achieve better\ndomain-specific performance. Taking the medical domain as the testbed, we\nconduct extensive experiments and empirically prove that KDS surpasses the\nother baselines and brings significant and consistent performance gains among\nall LLMs. More encouragingly, KDS effectively improves the model generalization\nand alleviates the hallucination problem.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.21958v1;http://arxiv.org/pdf/2505.21958v1", "pdf_url": "http://arxiv.org/pdf/2505.21958v1"}]
