[{"title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records", "link": "https://arxiv.org/pdf/2505.16941", "details": "C Pang, V Jeanselme, YS Choi, X Jiang, Z Jing\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Foundation models hold significant promise in healthcare, given their capacity to extract meaningful representations independent of downstream tasks. This property has enabled state-of-the-art performance across several clinical applications trained \u2026", "entry_id": "http://arxiv.org/abs/2505.16941v2", "updated": "2025-05-23 02:06:25", "published": "2025-05-22 17:29:52", "authors": "Chao Pang;Vincent Jeanselme;Young Sang Choi;Xinzhuo Jiang;Zilin Jing;Aparajita Kashyap;Yuta Kobayashi;Yanwei Li;Florent Pollet;Karthik Natarajan;Shalmali Joshi", "summary": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.16941v2;http://arxiv.org/pdf/2505.16941v2", "pdf_url": "http://arxiv.org/pdf/2505.16941v2"}, {"title": "Multimodal Representation Learning Based on Personalized Graph-Based Fusion for Mortality Prediction Using Electronic Medical Records", "link": "https://ieeexplore.ieee.org/iel8/8254253/11002434/11002447.pdf", "details": "A Al-Dailami, H Kuang, J Wang - Big Data Mining and Analytics, 2025", "abstract": "Predicting mortality risk in the Intensive Care Unit (ICU) using Electronic Medical Records (EMR) is crucial for identifying patients in need of immediate attention. However, the incompleteness and the variability of EMR features for each patient \u2026"}, {"title": "Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability", "link": "https://arxiv.org/pdf/2505.16789", "details": "PS Pandey, S Simko, K Pelrine, Z Jin - arXiv preprint arXiv:2505.16789, 2025", "abstract": "As large language models gain popularity, their vulnerability to adversarial attacks remains a primary concern. While fine-tuning models on domain-specific datasets is often employed to improve model performance, it can introduce vulnerabilities within \u2026", "entry_id": "http://arxiv.org/abs/2505.16789v1", "updated": "2025-05-22 15:30:00", "published": "2025-05-22 15:30:00", "authors": "Punya Syon Pandey;Samuel Simko;Kellin Pelrine;Zhijing Jin", "summary": "As large language models gain popularity, their vulnerability to adversarial\nattacks remains a primary concern. While fine-tuning models on domain-specific\ndatasets is often employed to improve model performance, it can introduce\nvulnerabilities within the underlying model. In this work, we investigate\nAccidental Misalignment, unexpected vulnerabilities arising from\ncharacteristics of fine-tuning data. We begin by identifying potential\ncorrelation factors such as linguistic features, semantic similarity, and\ntoxicity within our experimental datasets. We then evaluate the adversarial\nperformance of these fine-tuned models and assess how dataset factors correlate\nwith attack success rates. Lastly, we explore potential causal links, offering\nnew insights into adversarial defense strategies and highlighting the crucial\nrole of dataset design in preserving model alignment. Our code is available at\nhttps://github.com/psyonp/accidental_misalignment.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.16789v1;http://arxiv.org/pdf/2505.16789v1", "pdf_url": "http://arxiv.org/pdf/2505.16789v1"}, {"title": "Continually Self-Improving Language Models for Bariatric Surgery Question--Answering", "link": "https://arxiv.org/pdf/2505.16102", "details": "YK Atri, TH Shin, T Hartvigsen - arXiv preprint arXiv:2505.16102, 2025", "abstract": "While bariatric and metabolic surgery (MBS) is considered the gold standard treatment for severe and morbid obesity, its therapeutic efficacy hinges upon active and longitudinal engagement with multidisciplinary providers, including surgeons \u2026", "entry_id": "http://arxiv.org/abs/2505.16102v1", "updated": "2025-05-22 01:02:51", "published": "2025-05-22 01:02:51", "authors": "Yash Kumar Atri;Thomas H Shin;Thomas Hartvigsen", "summary": "While bariatric and metabolic surgery (MBS) is considered the gold standard\ntreatment for severe and morbid obesity, its therapeutic efficacy hinges upon\nactive and longitudinal engagement with multidisciplinary providers, including\nsurgeons, dietitians/nutritionists, psychologists, and endocrinologists. This\nengagement spans the entire patient journey, from preoperative preparation to\nlong-term postoperative management. However, this process is often hindered by\nnumerous healthcare disparities, such as logistical and access barriers, which\nimpair easy patient access to timely, evidence-based, clinician-endorsed\ninformation. To address these gaps, we introduce bRAGgen, a novel adaptive\nretrieval-augmented generation (RAG)-based model that autonomously integrates\nreal-time medical evidence when response confidence dips below dynamic\nthresholds. This self-updating architecture ensures that responses remain\ncurrent and accurate, reducing the risk of misinformation. Additionally, we\npresent bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,\nvalidated by an expert bariatric surgeon. bRAGq constitutes the first\nlarge-scale, domain-specific benchmark for comprehensive MBS care. In a\ntwo-phase evaluation, bRAGgen is benchmarked against state-of-the-art models\nusing both large language model (LLM)--based metrics and expert surgeon review.\nAcross all evaluation dimensions, bRAGgen demonstrates substantially superior\nperformance in generating clinically accurate and relevant responses.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.16102v1;http://arxiv.org/pdf/2505.16102v1", "pdf_url": "http://arxiv.org/pdf/2505.16102v1"}, {"title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios", "link": "https://arxiv.org/pdf/2505.16944", "details": "Y Qi, H Peng, X Wang, A Xin, Y Liu, B Xu, L Hou, J Li - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have demonstrated advanced capabilities in real- world agentic applications. Growing research efforts aim to develop LLM-based agents to address practical demands, introducing a new challenge: agentic \u2026", "entry_id": "http://arxiv.org/abs/2505.16944v1", "updated": "2025-05-22 17:31:10", "published": "2025-05-22 17:31:10", "authors": "Yunjia Qi;Hao Peng;Xiaozhi Wang;Amy Xin;Youfeng Liu;Bin Xu;Lei Hou;Juanzi Li", "summary": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL", "links": "http://arxiv.org/abs/2505.16944v1;http://arxiv.org/pdf/2505.16944v1", "pdf_url": "http://arxiv.org/pdf/2505.16944v1"}, {"title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO", "link": "https://arxiv.org/pdf/2505.16673", "details": "H Yao, Q Yin, J Zhang, M Yang, Y Wang, W Wu, F Su\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during \u2026", "entry_id": "http://arxiv.org/abs/2505.16673v1", "updated": "2025-05-22 13:39:32", "published": "2025-05-22 13:39:32", "authors": "Huanjin Yao;Qixiang Yin;Jingyi Zhang;Min Yang;Yibo Wang;Wenhao Wu;Fei Su;Li Shen;Minghui Qiu;Dacheng Tao;Jiaxing Huang", "summary": "In this work, we aim to incentivize the reasoning ability of Multimodal Large\nLanguage Models (MLLMs) via reinforcement learning (RL) and develop an\neffective approach that mitigates the sparse reward and advantage vanishing\nissues during RL. To this end, we propose Share-GRPO, a novel RL approach that\ntackle these issues by exploring and sharing diverse reasoning trajectories\nover expanded question space. Specifically, Share-GRPO first expands the\nquestion space for a given question via data transformation techniques, and\nthen encourages MLLM to effectively explore diverse reasoning trajectories over\nthe expanded question space and shares the discovered reasoning trajectories\nacross the expanded questions during RL. In addition, Share-GRPO also shares\nreward information during advantage computation, which estimates solution\nadvantages hierarchically across and within question variants, allowing more\naccurate estimation of relative advantages and improving the stability of\npolicy training. Extensive evaluations over six widely-used reasoning\nbenchmarks showcase the superior performance of our method. Code will be\navailable at https://github.com/HJYao00/R1-ShareVL.", "comment": "Technical report", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI;cs.CL", "links": "http://arxiv.org/abs/2505.16673v1;http://arxiv.org/pdf/2505.16673v1", "pdf_url": "http://arxiv.org/pdf/2505.16673v1"}, {"title": "LIFEBench: Evaluating Length Instruction Following in Large Language Models", "link": "https://arxiv.org/pdf/2505.16234", "details": "W Zhang, Z Zhou, J Fang, R Xu, K Wang, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While large language models (LLMs) can solve PhD-level reasoning problems over long context inputs, they still struggle with a seemingly simpler task: following explicit length instructions-eg, write a 10,000-word novel. Additionally, models often \u2026", "entry_id": "http://arxiv.org/abs/2505.16234v1", "updated": "2025-05-22 05:08:27", "published": "2025-05-22 05:08:27", "authors": "Wei Zhang;Zhenhong Zhou;Junfeng Fang;Rongwu Xu;Kun Wang;Yuanhe Zhang;Rui Wang;Ge Zhang;Xinfeng Li;Li Sun;Lingjuan Lyu;Yang Liu;Sen Su", "summary": "While large language models (LLMs) can solve PhD-level reasoning problems\nover long context inputs, they still struggle with a seemingly simpler task:\nfollowing explicit length instructions-e.g., write a 10,000-word novel.\nAdditionally, models often generate far too short outputs, terminate\nprematurely, or even refuse the request. Existing benchmarks focus primarily on\nevaluating generations quality, but often overlook whether the generations meet\nlength constraints. To this end, we introduce Length Instruction Following\nEvaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to\nfollow length instructions across diverse tasks and a wide range of specified\nlengths. LIFEBench consists of 10,800 instances across 4 task categories in\nboth English and Chinese, covering length constraints ranging from 16 to 8192\nwords. We evaluate 26 widely-used LLMs and find that most models reasonably\nfollow short-length instructions but deteriorate sharply beyond a certain\nthreshold. Surprisingly, almost all models fail to reach the vendor-claimed\nmaximum output lengths in practice, as further confirmed by our evaluations\nextending up to 32K words. Even long-context LLMs, despite their extended\ninput-output windows, counterintuitively fail to improve length-instructions\nfollowing. Notably, Reasoning LLMs outperform even specialized long-text\ngeneration models, achieving state-of-the-art length following. Overall,\nLIFEBench uncovers fundamental limitations in current LLMs' length instructions\nfollowing ability, offering critical insights for future progress.", "comment": "81 pages, 22 tables, 32 figures. Homepage:\n  https://ydyjya.github.io/LIFEBench/", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.16234v1;http://arxiv.org/pdf/2505.16234v1", "pdf_url": "http://arxiv.org/pdf/2505.16234v1"}, {"title": "IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models", "link": "https://arxiv.org/pdf/2505.16774", "details": "Y Gao, B Wang, C Wei, S Sun, AT Aw - arXiv preprint arXiv:2505.16774, 2025", "abstract": "Large language models (LLMs) have demonstrated strong instruction-following capabilities in text-based tasks. However, this ability often deteriorates in multimodal models after alignment with non-text modalities such as images or audio. While \u2026", "entry_id": "http://arxiv.org/abs/2505.16774v1", "updated": "2025-05-22 15:15:29", "published": "2025-05-22 15:15:29", "authors": "Yiming Gao;Bin Wang;Chengwei Wei;Shuo Sun;AiTi Aw", "summary": "Large language models (LLMs) have demonstrated strong instruction-following\ncapabilities in text-based tasks. However, this ability often deteriorates in\nmultimodal models after alignment with non-text modalities such as images or\naudio. While several recent efforts have investigated instruction-following\nperformance in text and vision-language models, instruction-following in\naudio-based large language models remains largely unexplored. To bridge this\ngap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess\nthe ability to follow instructions in an audio LLM. IFEval-Audio contains 280\naudio-instruction-answer triples across six diverse dimensions: Content,\nCapitalization, Symbol, List Structure, Length, and Format. Each example pairs\nan audio input with a text instruction, requiring the model to generate an\noutput that follows a specified structure. We benchmark state-of-the-art audio\nLLMs on their ability to follow audio-involved instructions. The dataset is\nreleased publicly to support future research in this emerging area.", "comment": "Link: https://github.com/AudioLLMs/AudioBench/tree/main/IFEval-Audio", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.16774v1;http://arxiv.org/pdf/2505.16774v1", "pdf_url": "http://arxiv.org/pdf/2505.16774v1"}]
