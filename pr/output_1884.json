[{"title": "Elements of World Knowledge (EWOK): A cognition-inspired framework for evaluating basic world knowledge in language models", "link": "https://arxiv.org/pdf/2405.09605", "details": "AA Ivanova, A Sathe, B Lipkin, U Kumar, S Radkani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The ability to build and leverage world models is essential for a general-purpose AI agent. Testing such capabilities is hard, in part because the building blocks of world models are ill-defined. We present Elements of World Knowledge (EWOK), a \u2026"}, {"title": "GRAMMAR: Grounded and Modular Evaluation of Domain-Specific Retrieval-Augmented Language Models", "link": "https://arxiv.org/pdf/2404.19232", "details": "X Li, M Liu, S Gao - arXiv preprint arXiv:2404.19232, 2024", "abstract": "Retrieval-augmented Generation (RAG) systems have been actively studied and deployed across various industries to query on domain-specific knowledge base. However, evaluating these systems presents unique challenges due to the scarcity of \u2026"}, {"title": "An in-depth evaluation of federated learning on biomedical natural language processing for information extraction", "link": "https://www.nature.com/articles/s41746-024-01126-4", "details": "L Peng, G Luo, S Zhou, J Chen, Z Xu, J Sun, R Zhang - NPJ Digital Medicine, 2024", "abstract": "Abstract Language models (LMs) such as BERT and GPT have revolutionized natural language processing (NLP). However, the medical field faces challenges in training LMs due to limited data access and privacy constraints imposed by \u2026"}, {"title": "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning", "link": "https://arxiv.org/pdf/2405.13448", "details": "Y Yue, C Wang, J Huang, P Wang - arXiv preprint arXiv:2405.13448, 2024", "abstract": "The process of instruction tuning aligns pre-trained large language models (LLMs) with open-domain instructions and human-preferred responses. While several studies have explored autonomous approaches to distilling and annotating \u2026"}, {"title": "An Improved Machine Learning Model for Pulmonary Embolism Detection and Segmentation", "link": "https://www.preprints.org/manuscript/202404.1810/download/final_file", "details": "K Do\u011fan, T SEL\u00c7UK, A ALKAN - 2024", "abstract": "Pulmonary Embolism (PE) is the obstruction of blood arteries in the lungs by a blood clot. The mortality risk for PE is approximately 30%. Detecting pulmonary embolism in the segmental arteries of the lung is more challenging than in the main arteries \u2026"}, {"title": "NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition", "link": "https://arxiv.org/pdf/2405.07609", "details": "E Merdjanovska, A Aynetdinov, A Akbik - arXiv preprint arXiv:2405.07609, 2024", "abstract": "Available training data for named entity recognition (NER) often contains a significant percentage of incorrect labels for entity types and entity boundaries. Such label noise poses challenges for supervised learning and may significantly deteriorate model \u2026"}, {"title": "Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "link": "https://arxiv.org/pdf/2405.10431", "details": "S Furniturewala, S Jandial, A Java, P Banerjee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing debiasing techniques are typically training-based or require access to the model's internals and output distributions, so they are inaccessible to end-users looking to adapt LLM outputs for their particular needs. In this study, we examine \u2026"}, {"title": "Characterizing the Accuracy-Efficiency Trade-off of Low-rank Decomposition in Language Models", "link": "https://arxiv.org/pdf/2405.06626", "details": "C Moar, M Pellauer, H Kwon - arXiv preprint arXiv:2405.06626, 2024", "abstract": "Large language models (LLMs) have emerged and presented their general problem- solving capabilities with one model. However, the model size has increased dramatically with billions of parameters to enable such broad problem-solving \u2026"}, {"title": "ERAGent: Enhancing Retrieval-Augmented Language Models with Improved Accuracy, Efficiency, and Personalization", "link": "https://arxiv.org/pdf/2405.06683", "details": "Y Shi, X Zi, Z Shi, H Zhang, Q Wu, M Xu - arXiv preprint arXiv:2405.06683, 2024", "abstract": "Retrieval-augmented generation (RAG) for language models significantly improves language understanding systems. The basic retrieval-then-read pipeline of response generation has evolved into a more extended process due to the integration of \u2026"}]
