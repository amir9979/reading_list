[{"title": "BiasICL: In-Context Learning and Demographic Biases of Vision Language Models", "link": "https://arxiv.org/pdf/2503.02334", "details": "S Xu, J Janizek, Y Jiang, R Daneshjou - arXiv preprint arXiv:2503.02334, 2025", "abstract": "Vision language models (VLMs) show promise in medical diagnosis, but their performance across demographic subgroups when using in-context learning (ICL) remains poorly understood. We examine how the demographic composition of \u2026"}, {"title": "Graph out-of-distribution generalization through contrastive learning paradigm", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125003636", "details": "H Du, X Li, M Shao - Knowledge-Based Systems, 2025", "abstract": "The problem we want to address is graph generalization in the out-of-distribution (OOD) scenario. Mainstream approaches to OOD generalization tasks specific to graph data primarily emphasize domain adaptation and invariant learning and do not \u2026"}, {"title": "Research on medical publication recommendation based on cross-modal semantic alignment with contrastive learning", "link": "https://www.sciencedirect.com/science/article/pii/S0306457325000858", "details": "H Ding, Z Xia, W Zhu - Information Processing & Management, 2025", "abstract": "Since the problem of the semantic divide between different modalities plagues medical publication recommendation, a cross-modal contrastive learning semantic alignment-based medical publication recommendation model is proposed to improve \u2026"}, {"title": "Continual Multimodal Contrastive Learning", "link": "https://arxiv.org/pdf/2503.14963", "details": "X Liu, X Xia, SK Ng, TS Chua - arXiv preprint arXiv:2503.14963, 2025", "abstract": "Multimodal contrastive learning (MCL) advances in aligning different modalities and generating multimodal representations in a joint space. By leveraging contrastive learning across diverse modalities, large-scale multimodal data enhances \u2026"}, {"title": "X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography from Chest Radiography via Tri-Modal Contrastive Learning", "link": "https://arxiv.org/pdf/2503.02162", "details": "J You, Y Gao, S Kim, C Mcintosh - arXiv preprint arXiv:2503.02162, 2025", "abstract": "Computed tomography (CT) is a key imaging modality for diagnosis, yet its clinical utility is marred by high radiation exposure and long turnaround times, restricting its use for larger-scale screening. Although chest radiography (CXR) is more accessible \u2026"}, {"title": "Impact of Noisy Supervision in Foundation Model Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10934976/", "details": "H Chen, Z Wang, R Tao, H Wei, X Xie, M Sugiyama\u2026 - IEEE Transactions on \u2026, 2025", "abstract": "Foundation models are usually pre-trained on large-scale datasets and then adapted to different downstream tasks through tuning. This pre-training and then fine-tuning paradigm has become a standard practice in deep learning. However, the large \u2026"}, {"title": "DynamicVis: An Efficient and General Visual Foundation Model for Remote Sensing Image Understanding", "link": "https://arxiv.org/pdf/2503.16426", "details": "K Chen, C Liu, B Chen, W Li, Z Zou, Z Shi - arXiv preprint arXiv:2503.16426, 2025", "abstract": "The advancement of remote sensing technology has improved the spatial resolution of satellite imagery, facilitating more detailed visual representations for diverse interpretations. However, existing methods exhibit limited generalization capabilities \u2026"}, {"title": "Impact of Glyph Information on Latent Space Diffusion Models for Accurate Handwritten Text Generation", "link": "https://ieeexplore.ieee.org/abstract/document/10890644/", "details": "YL Lin, HC Cheng, CI Huang, CY Wang, JC Wang - ICASSP 2025-2025 IEEE \u2026, 2025", "abstract": "The generation of high-quality stylized handwritten text images is a challenging task in computer vision and artificial intelligence. While advanced approaches using Latent Diffusion Models (LDMs) for generating stylized handwritten text have shown \u2026"}, {"title": "UMIT: Unifying Medical Imaging Tasks via Vision-Language Models", "link": "https://arxiv.org/pdf/2503.15892", "details": "H Yu, S Yi, K Niu, M Zhuo, B Li - arXiv preprint arXiv:2503.15892, 2025", "abstract": "With the rapid advancement of deep learning, particularly in the field of medical image analysis, an increasing number of Vision-Language Models (VLMs) are being widely applied to solve complex health and biomedical challenges. However \u2026"}]
