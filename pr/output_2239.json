[{"title": "Comparative Analysis of Open-Source Language Models in Summarizing Medical Text Data", "link": "https://arxiv.org/pdf/2405.16295", "details": "Y Chen, Z Wang, B Wen, F Zulkernine - arXiv preprint arXiv:2405.16295, 2024", "abstract": "Unstructured text in medical notes and dialogues contains rich information. Recent advancements in Large Language Models (LLMs) have demonstrated superior performance in question answering and summarization tasks on unstructured text \u2026"}, {"title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions", "link": "https://arxiv.org/pdf/2405.19744", "details": "C Li, W Yang, J Zhang, J Lu, S Wang, C Zong - arXiv preprint arXiv:2405.19744, 2024", "abstract": "Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples \u2026"}, {"title": "A fine-grained self-adapting prompt learning approach for few-shot learning with pre-trained language models", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124006026", "details": "X Chen, T Liu, P Fournier-Viger, B Zhang, G Long\u2026 - Knowledge-Based Systems, 2024", "abstract": "Pre-trained language models have demonstrated remarkable performance in few- shot learning through the emergence of \u201cprompt-based learning\u201d methods, where the performance of these tasks highly rely on the quality of prompts. Existing prompt \u2026"}, {"title": "SLM as Guardian: Pioneering AI Safety with Small Language Models", "link": "https://arxiv.org/pdf/2405.19795", "details": "O Kwon, D Jeon, N Choi, GH Cho, C Kim, H Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Most prior safety research of large language models (LLMs) has focused on enhancing the alignment of LLMs to better suit the safety requirements of humans. However, internalizing such safeguard features into larger models brought \u2026"}, {"title": "TAIA: Large Language Models are Out-of-Distribution Data Learners", "link": "https://arxiv.org/pdf/2405.20192", "details": "S Jiang, Y Liao, Y Zhang, Y Wang, Y Wang - arXiv preprint arXiv:2405.20192, 2024", "abstract": "Fine-tuning on task-specific question-answer pairs is a predominant method for enhancing the performance of instruction-tuned large language models (LLMs) on downstream tasks. However, in certain specialized domains, such as healthcare or \u2026"}, {"title": "I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models", "link": "https://arxiv.org/pdf/2405.17849", "details": "X Hu, Y Chen, D Yang, S Zhou, Z Yuan, J Yu, C Xu - arXiv preprint arXiv:2405.17849, 2024", "abstract": "Post-training quantization (PTQ) serves as a potent technique to accelerate the inference of large language models (LLMs). Nonetheless, existing works still necessitate a considerable number of floating-point (FP) operations during inference \u2026"}, {"title": "Adaptive In-conversation Team Building for Language Model Agents", "link": "https://arxiv.org/pdf/2405.19425", "details": "L Song, J Liu, J Zhang, S Zhang, A Luo, S Wang, Q Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Leveraging multiple large language model (LLM) agents has shown to be a promising approach for tackling complex tasks, while the effective design of multiple agents for a particular application remains an art. It is thus intriguing to answer a \u2026"}, {"title": "MathChat: Benchmarking Mathematical Reasoning and Instruction Following in Multi-Turn Interactions", "link": "https://arxiv.org/pdf/2405.19444", "details": "Z Liang, D Yu, W Yu, W Yao, Z Zhang, X Zhang, D Yu - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in mathematical problem solving, particularly in single turn question answering formats. However, real world scenarios often involve mathematical question answering that \u2026"}, {"title": "Context Injection Attacks on Large Language Models", "link": "https://arxiv.org/pdf/2405.20234", "details": "C Wei, K Chen, Y Zhao, Y Gong, L Xiang, S Zhu - arXiv preprint arXiv:2405.20234, 2024", "abstract": "Large Language Models (LLMs) such as ChatGPT and Llama-2 have become prevalent in real-world applications, exhibiting impressive text generation performance. LLMs are fundamentally developed from a scenario where the input \u2026"}]
