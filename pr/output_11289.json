[{"title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules", "link": "https://arxiv.org/pdf/2412.13536", "details": "K Chen, L Wang, Q Zhang, R Xu - arXiv preprint arXiv:2412.13536, 2024", "abstract": "Recent studies have highlighted the limitations of large language models in mathematical reasoning, particularly their inability to capture the underlying logic. Inspired by meta-learning, we propose that models should acquire not only task \u2026"}, {"title": "Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces", "link": "https://arxiv.org/pdf/2412.14171%3F", "details": "J Yang, S Yang, AW Gupta, R Han, L Fei-Fei, S Xie - arXiv preprint arXiv:2412.14171, 2024", "abstract": "Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also``think in space''from videos? We present \u2026"}, {"title": "Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models", "link": "https://arxiv.org/pdf/2412.15287", "details": "Y Chow, G Tennenholtz, I Gur, V Zhuang, B Dai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent studies have indicated that effectively utilizing inference-time compute is crucial for attaining better performance from large language models (LLMs). In this work, we propose a novel inference-aware fine-tuning paradigm, in which the model \u2026"}, {"title": "Dynamic Skill Adaptation for Large Language Models", "link": "https://arxiv.org/pdf/2412.19361%3F", "details": "J Chen, D Yang - arXiv preprint arXiv:2412.19361, 2024", "abstract": "We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework to adapt novel and complex skills to Large Language Models (LLMs). Compared with previous work which learns from human-curated and static data in random orders \u2026"}, {"title": "How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models", "link": "https://arxiv.org/pdf/2501.01741", "details": "S Corbo, L Bancale, V De Gennaro, L Lestingi, V Scotti\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Language is a deep-rooted means of perpetration of stereotypes and discrimination. Large Language Models (LLMs), now a pervasive technology in our everyday lives, can cause extensive harm when prone to generating toxic responses. The standard \u2026"}, {"title": "LLM+ AL: Bridging Large Language Models and Action Languages for Complex Reasoning about Actions", "link": "https://arxiv.org/pdf/2501.00830", "details": "A Ishay, J Lee - arXiv preprint arXiv:2501.00830, 2025", "abstract": "Large Language Models (LLMs) have made significant strides in various intelligent tasks but still struggle with complex action reasoning tasks that require systematic search. To address this limitation, we propose a method that bridges the natural \u2026"}, {"title": "Adaptive Pruning for Large Language Models with Structural Importance Awareness", "link": "https://arxiv.org/pdf/2412.15127", "details": "H Zheng, J Ren, Y Sun, R Zhang, W Zhang, Z Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The recent advancements in large language models (LLMs) have significantly improved language understanding and generation capabilities. However, it is difficult to deploy LLMs on resource-constrained edge devices due to their high \u2026"}, {"title": "InfAlign: Inference-aware language model alignment", "link": "https://arxiv.org/pdf/2412.19792%3F", "details": "A Balashankar, Z Sun, J Berant, J Eisenstein, M Collins\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language model alignment has become a critical step in training modern generative language models. The goal of alignment is to finetune a reference model such that the win rate of a sample from the aligned model over a sample from the reference \u2026"}, {"title": "Extracting Interpretable Task-Specific Circuits from Large Language Models for Faster Inference", "link": "https://arxiv.org/pdf/2412.15750", "details": "J Garc\u00eda-Carrasco, A Mat\u00e9, J Trujillo - arXiv preprint arXiv:2412.15750, 2024", "abstract": "Large Language Models (LLMs) have shown impressive performance across a wide range of tasks. However, the size of LLMs is steadily increasing, hindering their application on computationally constrained environments. On the other hand, despite \u2026"}]
