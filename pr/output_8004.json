[{"title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE", "link": "https://arxiv.org/pdf/2409.17508", "details": "X Zhu, Y Hu, F Mo, M Li, J Wu - arXiv preprint arXiv:2409.17508, 2024", "abstract": "Multi-modal large language models (MLLMs) have shown impressive capabilities as a general-purpose interface for various visual and linguistic tasks. However, building a unified MLLM for multi-task learning in the medical field remains a thorny \u2026"}, {"title": "E3D-GPT: Enhanced 3D Visual Foundation for Medical Vision-Language Model", "link": "https://arxiv.org/pdf/2410.14200", "details": "H Lai, Z Jiang, Q Yao, R Wang, Z He, X Tao, W Wei\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The development of 3D medical vision-language models holds significant potential for disease diagnosis and patient treatment. However, compared to 2D medical images, 3D medical images, such as CT scans, face challenges related to limited \u2026"}, {"title": "NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples", "link": "https://arxiv.org/pdf/2410.14669", "details": "B Li, Z Lin, W Peng, JD Nyandwi, D Jiang, Z Ma\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language models (VLMs) have made significant progress in recent visual- question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that \u2026"}, {"title": "Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models", "link": "https://arxiv.org/pdf/2410.09047%3F", "details": "Q Liu, C Shang, L Liu, N Pappas, J Ma, NA John\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be degraded by the integration of the vision module compared to its LLM backbone. We investigate this phenomenon, dubbed as''safety alignment degradation''in this paper \u2026"}, {"title": "Expert-level vision-language foundation model for real-world radiology and comprehensive evaluation", "link": "https://arxiv.org/pdf/2409.16183", "details": "X Liu, G Yang, Y Luo, J Mao, X Zhang, M Gao, S Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Radiology is a vital and complex component of modern clinical workflow and covers many tasks. Recently, vision-language (VL) foundation models in medicine have shown potential in processing multimodal information, offering a unified solution for \u2026"}, {"title": "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness", "link": "https://arxiv.org/pdf/2409.17791", "details": "J Li, H Huang, Y Zhang, P Xu, X Chen, R Song, L Shi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, there has been significant interest in replacing the reward model in Reinforcement Learning with Human Feedback (RLHF) methods for Large Language Models (LLMs), such as Direct Preference Optimization (DPO) and its \u2026"}, {"title": "Unsupervised SapBERT-based bi-encoders for medical concept annotation of clinical narratives with SNOMED CT", "link": "https://journals.sagepub.com/doi/pdf/10.1177/20552076241288681", "details": "A Abdulnazar, R Roller, S Schulz, M Kreuzthaler - DIGITAL HEALTH, 2024", "abstract": "Objective Clinical narratives provide comprehensive patient information. Achieving interoperability involves mapping relevant details to standardized medical vocabularies. Typically, natural language processing divides this task into named \u2026"}, {"title": "Improve Vision Language Model Chain-of-thought Reasoning", "link": "https://arxiv.org/pdf/2410.16198", "details": "R Zhang, B Zhang, Y Li, H Zhang, Z Sun, Z Gan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with \u2026"}, {"title": "SciDFM: A Large Language Model with Mixture-of-Experts for Science", "link": "https://arxiv.org/pdf/2409.18412", "details": "L Sun, D Luo, D Ma, Z Zhao, B Chen, Z Shen, S Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, there has been a significant upsurge of interest in leveraging large language models (LLMs) to assist scientific discovery. However, most LLMs only focus on general science, while they lack domain-specific knowledge, such as \u2026"}]
