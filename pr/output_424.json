'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Small Language Models Learn Enhanced Reasoning Skills '
[{"title": "Anatomical Structure-Guided Medical Vision-Language Pre-training", "link": "https://arxiv.org/html/2403.09294v1", "details": "Q Li, X Yan, J Xu, R Yuan, Y Zhang, R Feng, Q Shen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Learning medical visual representations through vision-language pre-training has reached remarkable progress. Despite the promising performance, it still faces challenges, ie, local alignment lacks interpretability and clinical relevance, and the \u2026"}, {"title": "Adaptive Prompt Routing for Arbitrary Text Style Transfer with Pre-trained Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29832/31446", "details": "Q Liu, J Qin, W Ye, H Mou, Y He, K Wang - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Recently, arbitrary text style transfer (TST) has made significant progress with the paradigm of prompt learning. In this paradigm, researchers often design or search for a fixed prompt for any input. However, existing evidence shows that large language \u2026"}, {"title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models", "link": "https://arxiv.org/pdf/2403.08281", "details": "N Ding, Y Chen, G Cui, X Lv, R Xie, B Zhou, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three \u2026"}, {"title": "Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs", "link": "https://arxiv.org/pdf/2404.01430", "details": "Z Zhang, F Yang, Z Jiang, Z Chen, Z Zhao, C Ma\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in large language models (LLMs) have enhanced their ability to process long input contexts. This development is particularly crucial for tasks that involve retrieving knowledge from an external datastore, which can result in long \u2026"}, {"title": "LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation", "link": "https://arxiv.org/html/2403.12019v1", "details": "Y Lan, F Hong, S Yang, S Zhou, X Meng, B Dai, X Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The field of neural rendering has witnessed significant progress with advancements in generative models and differentiable rendering techniques. Though 2D diffusion has achieved success, a unified 3D diffusion pipeline remains unsettled. This paper \u2026"}, {"title": "A Diffusion Model with State Estimation for Degradation-Blind Inverse Imaging", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/28023/28060", "details": "L Ji, Z Rao, SJ Pan, C Lei, Q Chen - Proceedings of the AAAI Conference on Artificial \u2026, 2024", "abstract": "Solving the task of inverse imaging problems can restore unknown clean images from input measurements that have incomplete information. Utilizing powerful generative models, such as denoising diffusion models, could better tackle the ill \u2026"}, {"title": "Pushing the limits of zero-shot self-supervised super-resolution of anisotropic MR images", "link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12926/1292606/Pushing-the-limits-of-zero-shot-self-supervised-super-resolution/10.1117/12.3007304.short", "details": "SW Remedios, S Wei, BE Dewey, A Carass, DL Pham\u2026 - Medical Imaging 2024 \u2026, 2024", "abstract": "Magnetic resonance images are often acquired as several 2D slices and stacked into a 3D volume, yielding a lower through-plane resolution than in-plane resolution. Many super-resolution (SR) methods have been proposed to address this, including \u2026"}, {"title": "Diffusion Models are Geometry Critics: Single Image 3D Editing Using Pre-Trained Diffusion Priors", "link": "https://arxiv.org/pdf/2403.11503", "details": "R Wang, J Xiang, J Yang, X Tong - arXiv preprint arXiv:2403.11503, 2024", "abstract": "We propose a novel image editing technique that enables 3D manipulations on single images, such as object rotation and translation. Existing 3D-aware image editing approaches typically rely on synthetic multi-view datasets for training \u2026"}, {"title": "From Pixels to Graphs: Open-Vocabulary Scene Graph Generation with Vision-Language Models", "link": "https://arxiv.org/html/2404.00906v1", "details": "R Li, S Zhang, D Lin, K Chen, X He - arXiv preprint arXiv:2404.00906, 2024", "abstract": "Scene graph generation (SGG) aims to parse a visual scene into an intermediate graph representation for downstream reasoning tasks. Despite recent advancements, existing methods struggle to generate scene graphs with novel visual \u2026"}]
