[{"title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models", "link": "https://arxiv.org/pdf/2407.11484", "details": "N Chen, Y Wang, Y Deng, J Li - arXiv preprint arXiv:2407.11484, 2024", "abstract": "This survey explores the burgeoning field of role-playing with language models, focusing on their development from early persona-based models to advanced character-driven simulations facilitated by Large Language Models (LLMs). Initially \u2026"}, {"title": "Improving Context-Aware Preference Modeling for Language Models", "link": "https://arxiv.org/pdf/2407.14916", "details": "S Pitis, Z Xiao, NL Roux, A Sordoni - arXiv preprint arXiv:2407.14916, 2024", "abstract": "While finetuning language models from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where \u2026"}, {"title": "ScholarChemQA: Unveiling the Power of Language Models in Chemical Research Question Answering", "link": "https://arxiv.org/pdf/2407.16931", "details": "X Chen, T Wang, T Guo, K Guo, J Zhou, H Li, M Zhuge\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Question Answering (QA) effectively evaluates language models' reasoning and knowledge depth. While QA datasets are plentiful in areas like general domain and biomedicine, academic chemistry is less explored. Chemical QA plays a crucial role \u2026"}, {"title": "Perceptions of Linguistic Uncertainty by Language Models and Humans", "link": "https://arxiv.org/pdf/2407.15814", "details": "CG Belem, M Kelly, M Steyvers, S Singh, P Smyth - arXiv preprint arXiv:2407.15814, 2024", "abstract": "Uncertainty expressions such as``probably''or``highly unlikely''are pervasive in human language. While prior work has established that there is population-level agreement in terms of how humans interpret these expressions, there has been little \u2026"}, {"title": "Language models are robotic planners: reframing plans as goal refinement graphs", "link": "https://arxiv.org/pdf/2407.15677", "details": "A Sharfuddin, T Breaux - arXiv preprint arXiv:2407.15677, 2024", "abstract": "Successful application of large language models (LLMs) to robotic planning and execution may pave the way to automate numerous real-world tasks. Promising recent research has been conducted showing that the knowledge contained in LLMs \u2026"}, {"title": "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models", "link": "https://arxiv.org/pdf/2407.17467", "details": "J Gu, Z Yang, C Ding, R Zhao, F Tan - arXiv preprint arXiv:2407.17467, 2024", "abstract": "Large Language Models (LLMs) excel in diverse tasks but often underperform in specialized fields due to limited domain-specific or proprietary corpus. Continual pre- training (CPT) enhances LLM capabilities by imbuing new domain-specific or \u2026"}, {"title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2407.11681", "details": "H Cheng, M Zhang, JQ Shi - arXiv preprint arXiv:2407.11681, 2024", "abstract": "As Large Language Models (LLMs) grow dramatically in size, there is an increasing trend in compressing and speeding up these models. Previous studies have highlighted the usefulness of gradients for importance scoring in neural network \u2026"}, {"title": "Imposter. AI: Adversarial Attacks with Hidden Intentions towards Aligned Large Language Models", "link": "https://arxiv.org/pdf/2407.15399", "details": "X Liu, L Li, T Xiang, F Ye, L Wei, W Li, N Garcia - arXiv preprint arXiv:2407.15399, 2024", "abstract": "With the development of large language models (LLMs) like ChatGPT, both their vast applications and potential vulnerabilities have come to the forefront. While developers have integrated multiple safety mechanisms to mitigate their misuse, a \u2026"}, {"title": "Cross-Lingual Multi-Hop Knowledge Editing--Benchmarks, Analysis and a Simple Contrastive Learning based Approach", "link": "https://arxiv.org/pdf/2407.10275", "details": "A Khandelwal, H Singh, H Gu, T Chen, K Zhou - arXiv preprint arXiv:2407.10275, 2024", "abstract": "Large language models are often expected to constantly adapt to new sources of knowledge and knowledge editing techniques aim to efficiently patch the outdated model knowledge, with minimal modification. Most prior works focus on monolingual \u2026"}]
