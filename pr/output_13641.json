[{"title": "Chest X-ray Foundation Model with Global and Local Representations Integration", "link": "https://arxiv.org/pdf/2502.05142", "details": "Z Yang, X Xu, J Zhang, G Wang, MK Kalra, P Yan - arXiv preprint arXiv:2502.05142, 2025", "abstract": "Chest X-ray (CXR) is the most frequently ordered imaging test, supporting diverse clinical tasks from thoracic disease detection to postoperative monitoring. However, task-specific classification models are limited in scope, require costly labeled data \u2026"}, {"title": "Is Self-Supervised Pre-training on Satellite Imagery Better than ImageNet? A Systematic Study with Sentinel-2", "link": "https://arxiv.org/pdf/2502.10669", "details": "S Lahrichi, Z Sheng, S Xia, K Bradbury, J Malof - arXiv preprint arXiv:2502.10669, 2025", "abstract": "Self-supervised learning (SSL) has demonstrated significant potential in pre-training robust models with limited labeled data, making it particularly valuable for remote sensing (RS) tasks. A common assumption is that pre-training on domain-aligned \u2026"}, {"title": "MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data with Vision Generation Task using Discrete Visual Representations", "link": "https://arxiv.org/pdf/2503.01019", "details": "Z Zhang, Y Yu, Y Chen, X Yang, SY Yeo - arXiv preprint arXiv:2503.01019, 2025", "abstract": "Despite significant progress in Vision-Language Pre-training (VLP), current approaches predominantly emphasize feature extraction and cross-modal comprehension, with limited attention to generating or transforming visual content \u2026"}, {"title": "Cross-Modal Alignment Regularization: Enhancing Language Models with Vision Model Representations", "link": "https://openreview.net/pdf%3Fid%3D4Yag8mHVtc", "details": "Y Gan, KI Zhao, P Isola - Second Workshop on Representational Alignment at \u2026", "abstract": "Cross-modal distillation has emerged as a critical technique for leveraging the complementary strengths of different modalities. However, existing work has not enabled direct benefits between models trained on data from different modalities. In \u2026"}, {"title": "Utilizing GPT-4 to interpret oral mucosal disease photographs for structured report generation", "link": "https://www.nature.com/articles/s41598-025-89328-y", "details": "ZZ Zhan, YT Xiong, CY Wang, BT Zhang, WJ Lian\u2026 - Scientific Reports, 2025", "abstract": "The aim of this study is to evaluate GPT-4's reasoning ability to interpret oral mucosal disease photos and generate structured reports from free-text inputs, while exploring the role of prompt engineering in enhancing its performance. Prompt received by \u2026"}, {"title": "Retrieval-augmented generation improves precision and trust of a GPT-4 model for emergency radiology diagnosis and classification: a proof-of-concept study", "link": "https://link.springer.com/article/10.1007/s00330-025-11445-z", "details": "A Fink, J Nattenm\u00fcller, S Rau, A Rau, H Tran\u2026 - European Radiology, 2025", "abstract": "Objectives This study evaluated the effect of enhancing a GPT-4 model with retrieval- augmented generation on its ability to diagnose and classify traumatic injuries based on radiology reports. Materials and methods In this prospective proof-of-concept \u2026"}, {"title": "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions", "link": "https://arxiv.org/pdf/2503.03278", "details": "J Li, C Liu, W Bai, R Arcucci, CI Bercea, JA Schnabel - arXiv preprint arXiv \u2026, 2025", "abstract": "Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains \u2026"}, {"title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models", "link": "https://arxiv.org/pdf/2502.10250%3F", "details": "GK Kumar, I Chaabane, K Wu - arXiv preprint arXiv:2502.10250, 2025", "abstract": "Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved \u2026"}, {"title": "Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding", "link": "https://arxiv.org/pdf/2502.09906", "details": "TD Truong, HQ Nguyen, XB Nguyen, A Dowling, X Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual \u2026"}]
