[{"title": "AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs", "link": "https://arxiv.org/pdf/2505.21389", "details": "X Ding, C Pan, Z Li, J Zhang, S Wang, Z Wei - arXiv preprint arXiv:2505.21389, 2025", "abstract": "Evaluating multimodal large language models (MLLMs) is increasingly expensive, as the growing size and cross-modality complexity of benchmarks demand significant scoring efforts. To tackle with this difficulty, we introduce AutoJudger, an agent-driven \u2026", "entry_id": "http://arxiv.org/abs/2505.21389v1", "updated": "2025-05-27 16:17:15", "published": "2025-05-27 16:17:15", "authors": "Xuanwen Ding;Chengjun Pan;Zejun Li;Jiwen Zhang;Siyuan Wang;Zhongyu Wei", "summary": "Evaluating multimodal large language models (MLLMs) is increasingly\nexpensive, as the growing size and cross-modality complexity of benchmarks\ndemand significant scoring efforts. To tackle with this difficulty, we\nintroduce AutoJudger, an agent-driven framework for efficient and adaptive\nbenchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the\nItem Response Theory (IRT) to estimate the question difficulty and an\nautonomous evaluation agent to dynamically select the most informative test\nquestions based on the model's real-time performance. Specifically, AutoJudger\nincorporates two pivotal components: a semantic-aware retrieval mechanism to\nensure that selected questions cover diverse and challenging scenarios across\nboth vision and language modalities, and a dynamic memory that maintains\ncontextual statistics of previously evaluated questions to guide coherent and\nglobally informed question selection throughout the evaluation process.\nExtensive experiments on four representative multimodal benchmarks demonstrate\nthat our adaptive framework dramatically reduces evaluation expenses, i.e.\nAutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with\nthe full benchmark evaluation on MMT-Bench.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.21389v1;http://arxiv.org/pdf/2505.21389v1", "pdf_url": "http://arxiv.org/pdf/2505.21389v1"}, {"title": "Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation", "link": "https://arxiv.org/pdf/2505.22222", "details": "Y Kim, J Wu, SH Kim, P Vasudev, J Shen, H Wu - arXiv preprint arXiv:2505.22222, 2025", "abstract": "Recent advancements in multimodal Large Language Models (LLMs) have significantly enhanced the automation of medical image analysis, particularly in generating radiology reports from chest X-rays (CXR). However, these models still \u2026", "entry_id": "http://arxiv.org/abs/2505.22222v1", "updated": "2025-05-28 10:54:40", "published": "2025-05-28 10:54:40", "authors": "Yunsoo Kim;Jinge Wu;Su-Hwan Kim;Pardeep Vasudev;Jiashu Shen;Honghan Wu", "summary": "Recent advancements in multimodal Large Language Models (LLMs) have\nsignificantly enhanced the automation of medical image analysis, particularly\nin generating radiology reports from chest X-rays (CXR). However, these models\nstill suffer from hallucinations and clinically significant errors, limiting\ntheir reliability in real-world applications. In this study, we propose Look &\nMark (L&M), a novel grounding fixation strategy that integrates radiologist eye\nfixations (Look) and bounding box annotations (Mark) into the LLM prompting\nframework. Unlike conventional fine-tuning, L&M leverages in-context learning\nto achieve substantial performance gains without retraining. When evaluated\nacross multiple domain-specific and general-purpose models, L&M demonstrates\nsignificant gains, including a 1.2% improvement in overall metrics (A.AVG) for\nCXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for\nLLaVA-Med. General-purpose models also benefit from L&M combined with\nin-context learning, with LLaVA-OV achieving an 87.3% clinical average\nperformance (C.AVG)-the highest among all models, even surpassing those\nexplicitly trained for CXR report generation. Expert evaluations further\nconfirm that L&M reduces clinically significant errors (by 0.43 average errors\nper report), such as false predictions and omissions, enhancing both accuracy\nand reliability. These findings highlight L&M's potential as a scalable and\nefficient solution for AI-assisted radiology, paving the way for improved\ndiagnostic workflows in low-resource clinical settings.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.CL", "links": "http://arxiv.org/abs/2505.22222v1;http://arxiv.org/pdf/2505.22222v1", "pdf_url": "http://arxiv.org/pdf/2505.22222v1"}, {"title": "Zero-Shot Vision Encoder Grafting via LLM Surrogates", "link": "https://arxiv.org/pdf/2505.22664", "details": "K Yue, V Singla, M Jia, J Kirchenbauer, R Qadri, Z Cai\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision language models (VLMs) typically pair a modestly sized vision encoder with a large language model (LLM), eg, Llama-70B, making the decoder the primary computational burden during training. To reduce costs, a potential promising strategy \u2026", "entry_id": "http://arxiv.org/abs/2505.22664v1", "updated": "2025-05-28 17:59:59", "published": "2025-05-28 17:59:59", "authors": "Kaiyu Yue;Vasu Singla;Menglin Jia;John Kirchenbauer;Rifaa Qadri;Zikui Cai;Abhinav Bhatele;Furong Huang;Tom Goldstein", "summary": "Vision language models (VLMs) typically pair a modestly sized vision encoder\nwith a large language model (LLM), e.g., Llama-70B, making the decoder the\nprimary computational burden during training. To reduce costs, a potential\npromising strategy is to first train the vision encoder using a small language\nmodel before transferring it to the large one. We construct small \"surrogate\nmodels\" that share the same embedding space and representation language as the\nlarge target LLM by directly inheriting its shallow layers. Vision encoders\ntrained on the surrogate can then be directly transferred to the larger model,\na process we call zero-shot grafting -- when plugged directly into the\nfull-size target LLM, the grafted pair surpasses the encoder-surrogate pair\nand, on some benchmarks, even performs on par with full decoder training with\nthe target LLM. Furthermore, our surrogate training approach reduces overall\nVLM training costs by ~45% when using Llama-70B as the decoder.", "comment": "15 pages", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.22664v1;http://arxiv.org/pdf/2505.22664v1", "pdf_url": "http://arxiv.org/pdf/2505.22664v1"}, {"title": "Towards Scalable Language-Image Pre-training for 3D Medical Imaging", "link": "https://arxiv.org/pdf/2505.21862", "details": "C Zhao, Y Lyu, A Chowdury, E Harake, A Kondepudi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Language-image pre-training has demonstrated strong performance in 2D medical imaging, but its success in 3D modalities such as CT and MRI remains limited due to the high computational demands of volumetric data, which pose a significant barrier \u2026", "entry_id": "http://arxiv.org/abs/2505.21862v1", "updated": "2025-05-28 01:16:34", "published": "2025-05-28 01:16:34", "authors": "Chenhui Zhao;Yiwei Lyu;Asadur Chowdury;Edward Harake;Akhil Kondepudi;Akshay Rao;Xinhai Hou;Honglak Lee;Todd Hollon", "summary": "Language-image pre-training has demonstrated strong performance in 2D medical\nimaging, but its success in 3D modalities such as CT and MRI remains limited\ndue to the high computational demands of volumetric data, which pose a\nsignificant barrier to training on large-scale, uncurated clinical studies. In\nthis study, we introduce Hierarchical attention for Language-Image Pre-training\n(HLIP), a scalable pre-training framework for 3D medical imaging. HLIP adopts a\nlightweight hierarchical attention mechanism inspired by the natural hierarchy\nof radiology data: slice, scan, and study. This mechanism exhibits strong\ngeneralizability, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when\npre-trained on CT-RATE. Moreover, the computational efficiency of HLIP enables\ndirect training on uncurated datasets. Trained on 220K patients with 3.13\nmillion scans for brain MRI and 240K patients with 1.44 million scans for head\nCT, HLIP achieves state-of-the-art performance, e.g., +32.4% balanced ACC on\nthe proposed publicly available brain MRI benchmark Pub-Brain-5; +1.4% and\n+6.9% macro AUC on head CT benchmarks RSNA and CQ500, respectively. These\nresults demonstrate that, with HLIP, directly pre-training on uncurated\nclinical datasets is a scalable and effective direction for language-image\npre-training in 3D medical imaging. The code is available at\nhttps://github.com/Zch0414/hlip", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.21862v1;http://arxiv.org/pdf/2505.21862v1", "pdf_url": "http://arxiv.org/pdf/2505.21862v1"}, {"title": "Pre-training on high-resolution X-ray images: an experimental study", "link": "https://link.springer.com/article/10.1007/s44267-025-00080-3", "details": "X Wang, Y Li, W Wu, J Jin, Y Rong, B Jiang, C Li\u2026 - Visual Intelligence, 2025", "abstract": "Existing X-ray image based pre-trained vision models are typically trained on a relatively small-scale dataset (less than 500,000 samples) with limited resolution (eg,\\\\(224\\times 224\\\\)). However, the key to the success of self-supervised pre \u2026"}, {"title": "Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations", "link": "https://arxiv.org/pdf/2505.21657", "details": "Z Dehghani, K Aslansefat, A Khan, MN Akram - arXiv preprint arXiv:2505.21657, 2025", "abstract": "Large language models like GPT, LLAMA, and Claude have become incredibly powerful at generating text, but they are still black boxes, so it is hard to understand how they decide what to say. That lack of transparency can be problematic \u2026", "entry_id": "http://arxiv.org/abs/2505.21657v1", "updated": "2025-05-27 18:32:38", "published": "2025-05-27 18:32:38", "authors": "Zeinab Dehghani;Koorosh Aslansefat;Adil Khan;Mohammed Naveed Akram", "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy.", "comment": "arXiv admin note: text overlap with arXiv:2412.16277", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG", "links": "http://arxiv.org/abs/2505.21657v1;http://arxiv.org/pdf/2505.21657v1", "pdf_url": "http://arxiv.org/pdf/2505.21657v1"}, {"title": "Parameter-efficient fine-tuning in large language models: a survey of methodologies", "link": "https://link.springer.com/article/10.1007/s10462-025-11236-4", "details": "L Wang, S Chen, L Jiang, S Pan, R Cai, S Yang\u2026 - Artificial Intelligence Review, 2025", "abstract": "The large language models, as predicted by scaling law forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the \u2026"}, {"title": "Enhancing discriminative ability in multimodal LLMs: A contrastive learning approach for CT report generation", "link": "https://www.sciencedirect.com/science/article/pii/S1566253525003136", "details": "Q Su, C Feng, G Shi, B Wang, Y Zhuang - Information Fusion, 2025", "abstract": "Automated CT report generation (CTRG) systems hold significant promise for enhancing clinical workflows. However, current approaches, including those leveraging advanced multimodal large language models (MLLMs), continue to face \u2026"}, {"title": "Systematic identification of rare disease patients in electronic health records enables evaluation of clinical outcomes", "link": "https://www.medrxiv.org/content/medrxiv/early/2025/05/06/2025.05.02.25325348.full.pdf", "details": "AS Yadaw, E Sid, H Sidky, C Zeng, Q Zhu, EA Mathe - medRxiv, 2025", "abstract": "Background Identifying rare disease (RD) patients in electronic health records (EHR) is challenging, as more than 10,000 rare diseases are not typically captured by clinical coding systems. This limits the assessment of clinical outcomes for RD \u2026"}]
