[{"title": "Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision", "link": "https://arxiv.org/pdf/2501.07886", "details": "Y Ye, C Laidlaw, J Steinhardt - arXiv preprint arXiv:2501.07886, 2025", "abstract": "Language model (LM) post-training relies on two stages of human supervision: task demonstrations for supervised finetuning (SFT), followed by preference comparisons for reinforcement learning from human feedback (RLHF). As LMs become more \u2026"}, {"title": "Evidence-based multimodal fusion on structured EHRs and free-text notes for ICU outcome prediction", "link": "https://arxiv.org/pdf/2501.04389", "details": "Y Ruan, DJ Tan, SK Ng, L Huang, M Feng - arXiv preprint arXiv:2501.04389, 2025", "abstract": "Objective: Accurate Intensive Care Unit (ICU) outcome prediction is critical for improving patient treatment quality and ICU resource allocation. Existing research mainly focuses on structured data and lacks effective frameworks to integrate clinical \u2026"}, {"title": "TOWARDS ROBUST TAI LANGUAGE ASPECT-BASED SENTIMENT ANALYSIS: A WEAK-SUPERVISION APPROACH", "link": "http://www.jatit.org/volumes/Vol103No1/17Vol103No1.pdf", "details": "K DUTTA, R REHMAN - Journal of Theoretical and Applied Information \u2026, 2025", "abstract": "Speech signals carry extensive information about the speaker, including various non- linguistic elements such as sentiment, emotion, and intent. Analyzing these aspects has garnered significant attention due to its wideranging applications in fields such \u2026"}, {"title": "Language Models as Continuous Self-Evolving Data Engineers", "link": "https://arxiv.org/pdf/2412.15151%3F", "details": "P Wang, M Wang, Z Ma, X Yang, S Feng, D Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert \u2026"}, {"title": "Enhancing radiology report generation through pre-trained language models", "link": "https://link.springer.com/article/10.1007/s13748-024-00358-5", "details": "G Leonardi, L Portinale, A Santomauro - Progress in Artificial Intelligence, 2024", "abstract": "In the healthcare field, the ability to integrate and process data from various modalities, such as medical images, clinical notes, and patient records, plays a central role in enabling Artificial Intelligence models to provide more informed \u2026"}, {"title": "Read Like a Radiologist: Efficient Vision-Language Model for 3D Medical Imaging Interpretation", "link": "https://arxiv.org/pdf/2412.13558", "details": "C Lee, S Park, CI Shin, WH Choi, HJ Park, JE Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent medical vision-language models (VLMs) have shown promise in 2D medical image interpretation. However extending them to 3D medical imaging has been challenging due to computational complexities and data scarcity. Although a few \u2026"}, {"title": "Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models", "link": "https://arxiv.org/pdf/2501.08248", "details": "Y Qiu, V Embar, Y Zhang, N Jaitly, SB Cohen, B Han - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in long-context language models (LCLMs) promise to transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With their expanded context windows, LCLMs can process entire knowledge bases and \u2026"}, {"title": "Fine-tuning Large Language Models for Improving Factuality in Legal Question Answering", "link": "https://arxiv.org/pdf/2501.06521", "details": "Y Hu, L Gan, W Xiao, K Kuang, F Wu - arXiv preprint arXiv:2501.06521, 2025", "abstract": "Hallucination, or the generation of incorrect or fabricated information, remains a critical challenge in large language models (LLMs), particularly in high-stake domains such as legal question answering (QA). In order to mitigate the hallucination \u2026"}, {"title": "Cascaded Self-Evaluation Augmented Training for Efficient Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2501.05662", "details": "Z Lv, W Wang, J Wang, S Zhang, F Wu - arXiv preprint arXiv:2501.05662, 2025", "abstract": "Efficient Multimodal Large Language Models (EMLLMs) have rapidly advanced recently. Incorporating Chain-of-Thought (CoT) reasoning and step-by-step self- evaluation has improved their performance. However, limited parameters often \u2026"}]
