[{"title": "A Philosophical Introduction to Language Models-Part II: The Way Forward", "link": "https://arxiv.org/pdf/2405.03207", "details": "R Milli\u00e8re, C Buckner - arXiv preprint arXiv:2405.03207, 2024", "abstract": "In this paper, the second of two companion pieces, we explore novel philosophical questions raised by recent progress in large language models (LLMs) that go beyond the classical debates covered in the first part. We focus particularly on issues \u2026"}, {"title": "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning", "link": "https://arxiv.org/pdf/2405.13448", "details": "Y Yue, C Wang, J Huang, P Wang - arXiv preprint arXiv:2405.13448, 2024", "abstract": "The process of instruction tuning aligns pre-trained large language models (LLMs) with open-domain instructions and human-preferred responses. While several studies have explored autonomous approaches to distilling and annotating \u2026"}, {"title": "Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization", "link": "https://arxiv.org/pdf/2405.10616", "details": "Y Ji, Y Xiang, J Li, W Chen, Z Liu, K Chen, M Zhang - arXiv preprint arXiv:2405.10616, 2024", "abstract": "In recent years, large language models (LLMs) have driven advances in natural language processing. Still, their growing scale has increased the computational burden, necessitating a balance between efficiency and performance. Low-rank \u2026"}, {"title": "MBIAS: Mitigating Bias in Large Language Models While Retaining Context", "link": "https://arxiv.org/pdf/2405.11290", "details": "S Raza, A Raval, V Chatrath - arXiv preprint arXiv:2405.11290, 2024", "abstract": "In addressing the critical need for safety in Large Language Models (LLMs), it is crucial to ensure that the outputs are not only safe but also retain their contextual accuracy. Many existing LLMs are safe fine-tuned either with safety demonstrations \u2026"}, {"title": "Lonas: Elastic low-rank adapters for efficient large language models", "link": "https://aclanthology.org/2024.lrec-main.940.pdf", "details": "JP Munoz, J Yuan, Y Zheng, N Jain - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) continue to grow, reaching hundreds of billions of parameters and making it challenging for Deep Learning practitioners with resource-constrained systems to use them, eg, fine-tuning these models for a \u2026"}, {"title": "C3L: Content Correlated Vision-Language Instruction Tuning Data Generation via Contrastive Learning", "link": "https://arxiv.org/pdf/2405.12752", "details": "J Ma, W Suo, P Wang, Y Zhang - arXiv preprint arXiv:2405.12752, 2024", "abstract": "Vision-Language Instruction Tuning (VLIT) is a critical training phase for Large Vision- Language Models (LVLMs). With the improving capabilities of open-source LVLMs, researchers have increasingly turned to generate VLIT data by using open-source \u2026"}, {"title": "DocReLM: Mastering Document Retrieval with Language Model", "link": "https://arxiv.org/pdf/2405.11461", "details": "G Wei, X Pang, T Zhang, Y Sun, X Qian, C Lin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With over 200 million published academic documents and millions of new documents being written each year, academic researchers face the challenge of searching for information within this vast corpus. However, existing retrieval systems \u2026"}, {"title": "Correcting Language Model Bias for Text Classification in True Zero-Shot Learning", "link": "https://aclanthology.org/2024.lrec-main.359.pdf", "details": "F Zhao, W Xianlin, C Yan, CK Loo - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Combining pre-trained language models (PLMs) and manual templates is a common practice for text classification in zero-shot scenarios. However, the effect of this approach is highly volatile, ranging from random guesses to near state-of-the-art \u2026"}, {"title": "ECLIPSE: Semantic Entropy-LCS for Cross-Lingual Industrial Log Parsing", "link": "https://arxiv.org/pdf/2405.13548", "details": "W Zhang, X Cheng, Y Zhang, J Yang, H Guo, Z Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Log parsing, a vital task for interpreting the vast and complex data produced within software architectures faces significant challenges in the transition from academic benchmarks to the industrial domain. Existing log parsers, while highly effective on \u2026"}]
