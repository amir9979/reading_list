[{"title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules", "link": "https://arxiv.org/pdf/2412.13536", "details": "K Chen, L Wang, Q Zhang, R Xu - arXiv preprint arXiv:2412.13536, 2024", "abstract": "Recent studies have highlighted the limitations of large language models in mathematical reasoning, particularly their inability to capture the underlying logic. Inspired by meta-learning, we propose that models should acquire not only task \u2026"}, {"title": "Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2412.05934", "details": "M Teng, J Xiaojun, D Ranjie, L Xinfeng, H Yihao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the rapid advancement of multimodal large language models (MLLMs), concerns regarding their security have increasingly captured the attention of both academia and industry. Although MLLMs are vulnerable to jailbreak attacks \u2026"}, {"title": "Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces", "link": "https://arxiv.org/pdf/2412.14171%3F", "details": "J Yang, S Yang, AW Gupta, R Han, L Fei-Fei, S Xie - arXiv preprint arXiv:2412.14171, 2024", "abstract": "Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also``think in space''from videos? We present \u2026"}, {"title": "Smoothed Embeddings for Robust Language Models", "link": "https://www.merl.com/publications/docs/TR2024-170.pdf", "details": "H Ryo, MRU Rashid, A Lewis, J Liu, T Koike-Akino\u2026", "abstract": "Improving the safety and reliability of large language models (LLMs) is a crucial aspect of realizing trustworthy AI systems. Although alignment methods aim to suppress harmful content generation, LLMs are often still vulnerable to jail-breaking \u2026"}, {"title": "Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models", "link": "https://arxiv.org/pdf/2412.15287", "details": "Y Chow, G Tennenholtz, I Gur, V Zhuang, B Dai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent studies have indicated that effectively utilizing inference-time compute is crucial for attaining better performance from large language models (LLMs). In this work, we propose a novel inference-aware fine-tuning paradigm, in which the model \u2026"}, {"title": "OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large Language Models", "link": "https://arxiv.org/pdf/2412.15235", "details": "K Sharma, P Kumar, Y Li - arXiv preprint arXiv:2412.15235, 2024", "abstract": "This paper presents OG-RAG, an Ontology-Grounded Retrieval Augmented Generation method designed to enhance LLM-generated responses by anchoring retrieval processes in domain-specific ontologies. While LLMs are widely used for \u2026"}, {"title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models", "link": "https://arxiv.org/pdf/2411.19477", "details": "Y Chen, X Pan, Y Li, B Ding, J Zhou - arXiv preprint arXiv:2411.19477, 2024", "abstract": "We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates $ N $ candidate solutions, and then chooses the \u2026"}, {"title": "Curriculum Learning for Cross-Lingual Data-to-Text Generation With Noisy Data", "link": "https://arxiv.org/pdf/2412.13484", "details": "KA Hari, M Gupta, V Varma - arXiv preprint arXiv:2412.13484, 2024", "abstract": "Curriculum learning has been used to improve the quality of text generation systems by ordering the training samples according to a particular schedule in various tasks. In the context of data-to-text generation (DTG), previous studies used various \u2026"}, {"title": "CareBot: A Pioneering Full-Process Open-Source Medical Language Model", "link": "https://arxiv.org/pdf/2412.15236", "details": "L Zhao, W Zeng, X Shi, H Zhou - arXiv preprint arXiv:2412.15236, 2024", "abstract": "Recently, both closed-source LLMs and open-source communities have made significant strides, outperforming humans in various general domains. However, their performance in specific professional domains such as medicine, especially within the \u2026"}]
