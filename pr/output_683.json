'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [A Quantitative Bias Analysis Approach to Informative Presenc'
[{"title": "Why do small language models underperform? Studying Language Model Saturation via the Softmax Bottleneck", "link": "https://arxiv.org/pdf/2404.07647", "details": "N Godey, \u00c9 de la Clergerie, B Sagot - arXiv preprint arXiv:2404.07647, 2024", "abstract": "Recent advances in language modeling consist in pretraining highly parameterized neural networks on extremely large web-mined text corpora. Training and inference with such models can be costly in practice, which incentivizes the use of smaller \u2026"}, {"title": "SAGED: Few-Shot Meta Learning for Tabular Data Error Detection", "link": "https://openproceedings.org/2024/conf/edbt/paper-95.pdf", "details": "M Abdelaal, T Ktitarev, D St\u00e4dtler, H Sch\u00f6ning - 27th International Conference on \u2026, 2024", "abstract": "High data quality is paramount for the success of machine learning (ML) applications, as it broadly impacts model performance and decision outcomes. In domains like medical diagnosis and financial systems, inaccuracies or data \u2026"}, {"title": "Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge", "link": "https://arxiv.org/pdf/2404.05880", "details": "W Lu, Z Zeng, J Wang, Z Lu, Z Chen, H Zhuang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Jailbreaking attacks can enable Large Language Models (LLMs) to bypass the safeguard and generate harmful content. Existing jailbreaking defense methods have failed to address the fundamental issue that harmful knowledge resides within \u2026"}]
