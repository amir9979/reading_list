[{"title": "Liquid: Language Models are Scalable Multi-modal Generators", "link": "https://arxiv.org/pdf/2412.04332", "details": "J Wu, Y Jiang, C Ma, Y Liu, H Zhao, Z Yuan, S Bai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Liquid, an auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared \u2026"}, {"title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "link": "https://arxiv.org/pdf/2411.08733%3F", "details": "S Singla, Z Wang, T Liu, A Ashfaq, Z Hu, EP Xing - arXiv preprint arXiv:2411.08733, 2024", "abstract": "Aligning Large Language Models (LLMs) traditionally relies on costly training and human preference annotations. Self-alignment seeks to reduce these expenses by enabling models to align themselves. To further lower costs and achieve alignment \u2026"}, {"title": "Automatic dataset shift identification to support root cause analysis of AI performance drift", "link": "https://arxiv.org/pdf/2411.07940%3F", "details": "M Roschewitz, R Mehta, C Jones, B Glocker - arXiv preprint arXiv:2411.07940, 2024", "abstract": "Shifts in data distribution can substantially harm the performance of clinical AI models. Hence, various methods have been developed to detect the presence of such shifts at deployment time. However, root causes of dataset shifts are varied, and \u2026"}, {"title": "Examining Whether Patient Portal and Video Visit Use Differs by Race and Ethnicity Among Older Adults in a US Integrated Health Care Delivery System: Cross \u2026", "link": "https://aging.jmir.org/2024/1/e63814", "details": "NP Gordon, C Yin, JC Lo - JMIR aging, 2024", "abstract": "Background Health care systems are increasingly encouraging patients to use patient portals and participate in video visits. However, there is limited information about how portal use differs among older adults. Objective This study aimed to \u2026"}, {"title": "Is Large-Scale Pretraining the Secret to Good Domain Generalization?", "link": "https://arxiv.org/pdf/2412.02856", "details": "P Teterwak, K Saito, T Tsiligkaridis, BA Plummer\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multi-Source Domain Generalization (DG) is the task of training on multiple source domains and achieving high classification performance on unseen target domains. Recent methods combine robust features from web-scale pretrained backbones with \u2026"}, {"title": "Mixed Distillation Helps Smaller Language Models Reason Better", "link": "https://aclanthology.org/2024.findings-emnlp.91.pdf", "details": "L Chenglin, Q Chen, L Li, C Wang, F Tao, Y Li, Z Chen\u2026 - Findings of the Association \u2026, 2024", "abstract": "As large language models (LLMs) have demonstrated impressive multiple step-by- step reasoning capabilities in recent natural language processing (NLP) reasoning tasks, many studies are interested in distilling reasoning abilities into smaller \u2026"}, {"title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models", "link": "https://openreview.net/pdf%3Fid%3DLC1QAqhePv", "details": "D Zhang, Z Hu, S Zhoubian, Z Du, K Yang, Z Wang\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving \u2026"}, {"title": "SETLEXSEM CHALLENGE: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models", "link": "https://openreview.net/pdf%3Fid%3DMd1mEoPEaQ", "details": "NA Dronen, B Akhbari, M Gawali - The Thirty-eight Conference on Neural Information \u2026", "abstract": "Set theory is foundational to mathematics and, when sets are finite, to reasoning about the world. An intelligent system should perform set operations consistently, regardless of superficial variations in the operands. Initially designed for semantically \u2026"}, {"title": "BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating Machine Learning Tasks", "link": "https://arxiv.org/pdf/2411.07464", "details": "S Gandhi, M Patwardhan, L Vig, G Shroff - arXiv preprint arXiv:2411.07464, 2024", "abstract": "Large Language Models (LLMs) excel in diverse applications including generation of code snippets, but often struggle with generating code for complex Machine Learning (ML) tasks. Although existing LLM single-agent based systems give varying \u2026"}]
