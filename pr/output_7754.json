[{"title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.10139", "details": "P Xia, S Han, S Qiu, Y Zhou, Z Wang, W Zheng, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this \u2026"}, {"title": "A foundation model for generalizable disease diagnosis in chest X-ray images", "link": "https://arxiv.org/pdf/2410.08861", "details": "L Xu, Z Ni, H Sun, H Li, S Zhang - arXiv preprint arXiv:2410.08861, 2024", "abstract": "Medical artificial intelligence (AI) is revolutionizing the interpretation of chest X-ray (CXR) images by providing robust tools for disease diagnosis. However, the effectiveness of these AI models is often limited by their reliance on large amounts of \u2026"}, {"title": "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment", "link": "https://arxiv.org/pdf/2410.09421", "details": "L Li, Z Xie, M Li, S Chen, P Wang, L Chen, Y Yang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As large vision-language models (LVLMs) evolve rapidly, the demand for high- quality and diverse data to align these models becomes increasingly crucial. However, the creation of such data with human supervision proves costly and time \u2026"}, {"title": "On Unsupervised Prompt Learning for Classification with Black-box Language Models", "link": "https://arxiv.org/pdf/2410.03124", "details": "ZY Zhang, J Zhang, H Yao, G Niu, M Sugiyama - arXiv preprint arXiv:2410.03124, 2024", "abstract": "Large language models (LLMs) have achieved impressive success in text-formatted learning problems, and most popular LLMs have been deployed in a black-box fashion. Meanwhile, fine-tuning is usually necessary for a specific downstream task \u2026"}, {"title": "Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models", "link": "https://arxiv.org/pdf/2409.18943", "details": "J Li, L Zhang, Y Li, Z Liu, R Luo, L Chen, M Yang - arXiv preprint arXiv:2409.18943, 2024", "abstract": "The instruction-following ability of large language models enables humans to interact with AI agents in a natural way. However, when required to generate responses of a specific length, large language models often struggle to meet users' needs due to \u2026"}, {"title": "AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning", "link": "https://arxiv.org/pdf/2410.08405", "details": "M Awais, AHSA Alharthi, A Kumar, H Cholakkal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Significant progress has been made in advancing large multimodal conversational models (LMMs), capitalizing on vast repositories of image-text data available online. Despite this progress, these models often encounter substantial domain gaps \u2026"}, {"title": "EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation", "link": "https://arxiv.org/pdf/2410.09704", "details": "M Vukadinovic, X Tang, N Yuan, P Cheng, D Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Echocardiography is the most widely used cardiac imaging modality, capturing ultrasound video data to assess cardiac structure and function. Artificial intelligence (AI) in echocardiography has the potential to streamline manual tasks and improve \u2026"}, {"title": "Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning", "link": "https://arxiv.org/pdf/2409.13641", "details": "DR Cambrin, G Gallipoli, I Benedetto, L Cagliero\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across various tasks. However, current training approaches combine standard cross-entropy loss with extensive data, human feedback, or ad hoc methods to enhance \u2026"}, {"title": "Large Language Model Evaluation via Matrix Nuclear-Norm", "link": "https://arxiv.org/pdf/2410.10672", "details": "Y Li, T Xia, Y Chang, Y Wu - arXiv preprint arXiv:2410.10672, 2024", "abstract": "As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are \u2026"}]
