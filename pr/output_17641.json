[{"title": "Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time Series Forecasting Model", "link": "https://arxiv.org/pdf/2506.11029", "details": "X Wang, T Zhou, J Gao, B Ding, J Zhou - arXiv preprint arXiv:2506.11029, 2025", "abstract": "We present a joint forecasting framework for time series prediction that contrasts with traditional direct or recursive methods. This framework achieves state-of-the-art performance for our designed foundation model, YingLong, and reveals a novel \u2026", "entry_id": "http://arxiv.org/abs/2506.11029v1", "updated": "2025-05-20 14:31:06", "published": "2025-05-20 14:31:06", "authors": "Xue Wang;Tian Zhou;Jinyang Gao;Bolin Ding;Jingren Zhou", "summary": "We present a joint forecasting framework for time series prediction that\ncontrasts with traditional direct or recursive methods. This framework achieves\nstate-of-the-art performance for our designed foundation model, YingLong, and\nreveals a novel scaling effect: longer outputs significantly enhance model\naccuracy due to delayed chain-of-thought reasoning in our non-causal approach.\nYingLong is a non-causal, bidirectional attention encoder-only transformer\ntrained through masked token recovery, aligning more effectively with language\nunderstanding tasks than with generation tasks. Additionally, we boost\nperformance by tackling output variance with a multi-input ensemble. We release\nfour foundation models ranging from 6M to 300M parameters, demonstrating\nsuperior results in zero-shot tasks on the ETT and Weather datasets. YingLong\nachieves more than 60% best performance. To ensure generalizability, we\nassessed the models using the GIFT-Eval benchmark, which comprises 23 time\nseries datasets across 7 domains. Yinglong significantly outperformed the best\ntime-series foundation models, end-to-end trained models by 14% and 44% in rank\nrespectively.The pretrained 300M model is available at\nhttps://huggingface.co/qcw1314/YingLong_300m", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2506.11029v1;http://arxiv.org/pdf/2506.11029v1", "pdf_url": "http://arxiv.org/pdf/2506.11029v1"}, {"title": "HEIST: A Graph Foundation Model for Spatial Transcriptomics and Proteomics Data", "link": "https://arxiv.org/pdf/2506.11152", "details": "H Madhu, JF Rocha, T Huang, S Viswanath\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Single-cell transcriptomics has become a great source for data-driven insights into biology, enabling the use of advanced deep learning methods to understand cellular heterogeneity and transcriptional regulation at the single-cell level. With the advent \u2026", "entry_id": "http://arxiv.org/abs/2506.11152v1", "updated": "2025-06-11 12:29:01", "published": "2025-06-11 12:29:01", "authors": "Hiren Madhu;Jo\u00e3o Felipe Rocha;Tinglin Huang;Siddharth Viswanath;Smita Krishnaswamy;Rex Ying", "summary": "Single-cell transcriptomics has become a great source for data-driven\ninsights into biology, enabling the use of advanced deep learning methods to\nunderstand cellular heterogeneity and transcriptional regulation at the\nsingle-cell level. With the advent of spatial transcriptomics data we have the\npromise of learning about cells within a tissue context as it provides both\nspatial coordinates and transcriptomic readouts. However, existing models\neither ignore spatial resolution or the gene regulatory information. Gene\nregulation in cells can change depending on microenvironmental cues from\nneighboring cells, but existing models neglect gene regulatory patterns with\nhierarchical dependencies across levels of abstraction. In order to create\ncontextualized representations of cells and genes from spatial transcriptomics\ndata, we introduce HEIST, a hierarchical graph transformer-based foundation\nmodel for spatial transcriptomics and proteomics data. HEIST models tissue as\nspatial cellular neighborhood graphs, and each cell is, in turn, modeled as a\ngene regulatory network graph. The framework includes a hierarchical graph\ntransformer that performs cross-level message passing and message passing\nwithin levels. HEIST is pre-trained on 22.3M cells from 124 tissues across 15\norgans using spatially-aware contrastive learning and masked auto-encoding\nobjectives. Unsupervised analysis of HEIST representations of cells, shows that\nit effectively encodes the microenvironmental influences in cell embeddings,\nenabling the discovery of spatially-informed subpopulations that prior models\nfail to differentiate. Further, HEIST achieves state-of-the-art results on four\ndownstream task such as clinical outcome prediction, cell type annotation, gene\nimputation, and spatially-informed cell clustering across multiple\ntechnologies, highlighting the importance of hierarchical modeling and\nGRN-based representations.", "comment": null, "journal_ref": null, "primary_category": "q-bio.GN", "categories": "q-bio.GN;cs.LG;q-bio.CB", "links": "http://arxiv.org/abs/2506.11152v1;http://arxiv.org/pdf/2506.11152v1", "pdf_url": "http://arxiv.org/pdf/2506.11152v1"}, {"title": "Foundation Models for Clinical Records at Health System Scale", "link": "https://openreview.net/pdf%3Fid%3DbnX1KRerWl", "details": "HR Rajamohan, X Gao, W Zhu, SL Huang, L Chen\u2026 - 1st ICML Workshop on Foundation \u2026", "abstract": "Large-scale pretraining has transformed modeling of language and other data types, but its potential remains underexplored in healthcare with structured electronic health records (EHRs). We present a novel generative pretraining strategy for \u2026"}, {"title": "ConTraPh: Contrastive Learning for Parallelization and Performance Optimization", "link": "https://hpcrl.github.io/ICS2025-webpage/program/Proceedings_ICS25/ics25-19.pdf", "details": "QI Mahmud, A TehraniJamsaz, NK Ahmed, TL Willke\u2026 - 2025", "abstract": "With the advancement of HPC platforms, the demand for high-performing applications continues to grow. One effective way to enhance program performance is through parallelization. However, fully leveraging the powerful hardware of HPC \u2026"}]
