'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [A Comparison of Parameter-Efficient ASR Domain Adaptation Me'
[{"title": "Grounding and Enhancing Grid-based Models for Neural Fields", "link": "https://arxiv.org/pdf/2403.20002", "details": "Z Zhao, F Fan, W Liao, J Yan - arXiv preprint arXiv:2403.20002, 2024", "abstract": "Many contemporary studies utilize grid-based models for neural field representation, but a systematic analysis of grid-based models is still missing, hindering the improvement of those models. Therefore, this paper introduces a theoretical \u2026"}, {"title": "Generative Language Models for Personalized Information Understanding", "link": "https://scholarworks.umass.edu/cgi/viewcontent.cgi%3Farticle%3D4123%26context%3Ddissertations_2", "details": "P Cai - 2024", "abstract": "A major challenge in information understanding stems from the diverse nature of the audience, where individuals possess varying preferences, experiences, educational and cultural backgrounds. Consequently, adopting a one-size-fits-all approach to \u2026"}, {"title": "ATG: Benchmarking Automated Theorem Generation for Generative Language Models", "link": "https://eleanor-h.github.io/publication/confnaacl-2024-atg/confnaacl-2024-atg.pdf", "details": "X Lin, Q Cao, Y Huang, Z Yang, Z Liu, Z Li, X Liang15", "abstract": "Humans can develop new theorems to explore broader and more complex mathematical results. While current generative language models (LMs) have achieved significant improvement in automatically proving theorems, their ability to \u2026"}, {"title": "A Diffusion Model with State Estimation for Degradation-Blind Inverse Imaging", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/28023/28060", "details": "L Ji, Z Rao, SJ Pan, C Lei, Q Chen - Proceedings of the AAAI Conference on Artificial \u2026, 2024", "abstract": "Solving the task of inverse imaging problems can restore unknown clean images from input measurements that have incomplete information. Utilizing powerful generative models, such as denoising diffusion models, could better tackle the ill \u2026"}, {"title": "Invertible Diffusion Models for Compressed Sensing", "link": "https://arxiv.org/pdf/2403.17006", "details": "B Chen, Z Zhang, W Li, C Zhao, J Yu, S Zhao, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although \u2026"}, {"title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies", "link": "https://arxiv.org/pdf/2404.06395", "details": "S Hu, Y Tu, X Han, C He, G Cui, X Long, Z Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This \u2026"}, {"title": "Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data", "link": "https://arxiv.org/pdf/2404.03862", "details": "J Zhang, M Marone, T Li, B Van Durme, D Khashabi - arXiv preprint arXiv:2404.03862, 2024", "abstract": "For humans to trust the fluent generations of large language models (LLMs), they must be able to verify their correctness against trusted, external sources. Recent efforts aim to increase verifiability through citations of retrieved documents or post \u2026"}, {"title": "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "link": "https://arxiv.org/pdf/2403.16442", "details": "R Esfandiarpoor, C Menghini, SH Bach - arXiv preprint arXiv:2403.16442, 2024", "abstract": "Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore \u2026"}, {"title": "FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations", "link": "https://arxiv.org/pdf/2404.06619", "details": "J Dwivedi-Yu, R Dwivedi, T Schick - arXiv preprint arXiv:2404.06619, 2024", "abstract": "The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes \u2026"}]
