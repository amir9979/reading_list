[{"title": "Single-cell multiomics data integration and generation with scPairing", "link": "https://www.biorxiv.org/content/10.1101/2025.01.04.631299.full.pdf", "details": "J Niu, J Ding - bioRxiv, 2025", "abstract": "Single-cell multiomics technologies generate paired or multiple measurements of different cellular properties (modalities), such as gene expression and chromatin accessibility. However, multiomics technologies are more expensive than their \u2026"}, {"title": "B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions", "link": "https://ieeexplore.ieee.org/abstract/document/10816024/", "details": "H Zhang, W Shao, H Liu, Y Ma, P Luo, Y Qiao, N Zheng\u2026 - IEEE Transactions on \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) have shown significant progress in responding well to visual-instructions from users. However, these instructions, encompassing images and text, are susceptible to both intentional and inadvertent \u2026"}, {"title": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over Aligned Large Language Models", "link": "https://arxiv.org/pdf/2412.12865", "details": "Y Fan, Y Hong, Q Wang, J Bao, H Jiang, Y Song - arXiv preprint arXiv:2412.12865, 2024", "abstract": "Alignment, endowing a pre-trained Large language model (LLM) with the ability to follow instructions, is crucial for its real-world applications. Conventional supervised fine-tuning (SFT) methods formalize it as causal language modeling typically with a \u2026"}, {"title": "FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering", "link": "https://arxiv.org/pdf/2412.07030", "details": "A Abaskohi, S Gella, G Carenini, IH Laradji - arXiv preprint arXiv:2412.07030, 2024", "abstract": "Multimodal multihop question answering is a complex task that requires reasoning over multiple sources of information, such as images and text, to answer questions. While there has been significant progress in visual question answering, the multihop \u2026"}, {"title": "AutoReason: Automatic Few-Shot Reasoning Decomposition", "link": "https://arxiv.org/pdf/2412.06975", "details": "A Sevinc, A Gumus - arXiv preprint arXiv:2412.06975, 2024", "abstract": "Chain of Thought (CoT) was introduced in recent research as a method for improving step-by-step reasoning in Large Language Models. However, CoT has limited applications such as its need for hand-crafted few-shot exemplar prompts and no \u2026"}, {"title": "Dynamic Ensemble Reasoning for LLM Experts", "link": "https://arxiv.org/pdf/2412.07448", "details": "J Hu, Y Wang, S Zhang, K Zhou, G Chen, Y Hu, B Xiao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Ensemble reasoning for the strengths of different LLM experts is critical to achieving consistent and satisfactory performance on diverse inputs across a wide range of tasks. However, existing LLM ensemble methods are either computationally \u2026"}, {"title": "CareBot: A Pioneering Full-Process Open-Source Medical Language Model", "link": "https://arxiv.org/pdf/2412.15236", "details": "L Zhao, W Zeng, X Shi, H Zhou - arXiv preprint arXiv:2412.15236, 2024", "abstract": "Recently, both closed-source LLMs and open-source communities have made significant strides, outperforming humans in various general domains. However, their performance in specific professional domains such as medicine, especially within the \u2026"}, {"title": "Language hooks: a modular framework for augmenting LLM reasoning that decouples tool usage from the model and its prompt", "link": "https://arxiv.org/pdf/2412.05967", "details": "D de Mijolla, W Yang, P Duckett, C Frye, M Worrall - arXiv preprint arXiv:2412.05967, 2024", "abstract": "Prompting and fine-tuning have emerged as two competing paradigms for augmenting language models with new capabilities, such as the use of tools. Prompting approaches are quick to set up but rely on providing explicit \u2026"}, {"title": "Hint Marginalization for Improved Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2412.13292", "details": "S Pal, D Ch\u00e9telat, Y Zhang, M Coates - arXiv preprint arXiv:2412.13292, 2024", "abstract": "Large Language Models (LLMs) have exhibited an impressive capability to perform reasoning tasks, especially if they are encouraged to generate a sequence of intermediate steps. Reasoning performance can be improved by suitably combining \u2026"}]
