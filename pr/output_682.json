'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Consistency and Uncertainty: Identifying Unreliable Re'
[{"title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models", "link": "https://arxiv.org/pdf/2403.18814", "details": "Y Li, Y Zhang, C Wang, Z Zhong, Y Chen, R Chu, S Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared \u2026"}, {"title": "Mechanisms of non-factual hallucinations in language models", "link": "https://arxiv.org/pdf/2403.18167", "details": "L Yu, M Cao, JCK Cheung, Y Dong - arXiv preprint arXiv:2403.18167, 2024", "abstract": "State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains \u2026"}, {"title": "VLRM: Vision-Language Models act as Reward Models for Image Captioning", "link": "https://arxiv.org/pdf/2404.01911", "details": "M Dzabraev, A Kunitsyn, A Ivaniuta - arXiv preprint arXiv:2404.01911, 2024", "abstract": "In this work, we present an unsupervised method for enhancing an image captioning model (in our case, BLIP2) using reinforcement learning and vision-language models like CLIP and BLIP2-ITM as reward models. The RL-tuned model is able to \u2026"}, {"title": "Conceptual and Unbiased Reasoning in Language Models", "link": "https://arxiv.org/pdf/2404.00205", "details": "B Zhou, H Zhang, S Chen, D Yu, H Wang, B Peng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Conceptual reasoning, the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on large language models' capability to perform conceptual reasoning. In this work, we \u2026"}, {"title": "Self-playing Adversarial Language Game Enhances LLM Reasoning", "link": "https://arxiv.org/pdf/2404.10642", "details": "P Cheng, T Hu, H Xu, Z Zhang, Y Dai, L Han, N Du - arXiv preprint arXiv:2404.10642, 2024", "abstract": "We explore the self-play training procedure of large language models (LLMs) in a two-player adversarial language game called Adversarial Taboo. In this game, an attacker and a defender communicate with respect to a target word only visible to the \u2026"}, {"title": "Adaptive Prompt Routing for Arbitrary Text Style Transfer with Pre-trained Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29832/31446", "details": "Q Liu, J Qin, W Ye, H Mou, Y He, K Wang - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Recently, arbitrary text style transfer (TST) has made significant progress with the paradigm of prompt learning. In this paradigm, researchers often design or search for a fixed prompt for any input. However, existing evidence shows that large language \u2026"}, {"title": "Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards", "link": "https://arxiv.org/pdf/2404.10346", "details": "H Hwang, D Kim, S Kim, S Ye, M Seo - arXiv preprint arXiv:2404.10346, 2024", "abstract": "Training on large amounts of rationales (ie, CoT Fine-tuning) is effective at improving the reasoning capabilities of large language models (LLMs). However, acquiring human-authored rationales or augmenting rationales from proprietary models is \u2026"}, {"title": "ASMR: Aggregated Semantic Matching Retrieval Unleashing Commonsense Ability of LLM through Open-Ended Question Answering", "link": "https://proceedings.aaai-make.info/AAAI-MAKE-PREPRINTS-2024/02547-LinP.pdf", "details": "PY Lin, E Chandra, JY Hsu - 2024", "abstract": "Commonsense reasoning refers to the ability to make inferences, draw conclusions, and understand the world based on general knowledge and commonsense. Whether Large Language Models (LLMs) have commonsense reasoning ability remains a \u2026"}, {"title": "Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models", "link": "https://arxiv.org/pdf/2403.13590", "details": "A Liusie, Y Fathullah, MJF Gales - arXiv preprint arXiv:2403.13590, 2024", "abstract": "Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities and versatility in NLP tasks, however they sometimes fail to maintain crucial invariances for specific tasks. One example is permutation sensitivity, where \u2026"}]
