[{"title": "Infrared Image Generation Based on Visual State Space and Contrastive Learning", "link": "https://www.mdpi.com/2072-4292/16/20/3817", "details": "B Li, D Ma, F He, Z Zhang, D Zhang, S Li - Remote Sensing, 2024", "abstract": "The preparation of infrared reference images is of great significance for improving the accuracy and precision of infrared imaging guidance. However, collecting infrared data on-site is difficult and time-consuming. Fortunately, the infrared images \u2026"}, {"title": "Adaptively Hierarchical Quantization Variational Autoencoder Based on Feature Decoupling and Semantic Consistency for Image Generation", "link": "https://ieeexplore.ieee.org/abstract/document/10647765/", "details": "Y Zhang, H Park, H Jia, F Wang, J Zhang, X Kong - 2024 IEEE International \u2026, 2024", "abstract": "The Vector Quantized Variational AutoEncoder (VQ-VAE) has shown great potential in image generation, especially the methods with hierarchical features. However, the lack of decoupling of structural information between hierarchical features leads to \u2026"}, {"title": "Design of a differentiable L-1 norm for pattern recognition and machine learning", "link": "https://www.sciencedirect.com/science/article/pii/S0167865524002812", "details": "M Zhang, Y Wang, H Chen, T Li, S Liu, X Gu, X Xu - Pattern Recognition Letters, 2024", "abstract": "In various applications of pattern recognition, feature selection, and machine learning, L-1 norm is used as either an objective function or a regularizer. Mathematically, L-1 norm has unique characteristics that make it attractive in \u2026"}, {"title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation", "link": "https://arxiv.org/abs/2410.01912", "details": "L Chen, S Tan, Z Cai, W Xie, H Zhao, Y Zhang, J Lin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer \u2026"}, {"title": "Mining Your Own Secrets: Diffusion Classifier Scores for Continual Personalization of Text-to-Image Diffusion Models", "link": "https://arxiv.org/pdf/2410.00700", "details": "S Jha, S Yang, M Ishii, M Zhao, C Simon, MJ Mirza\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Personalized text-to-image diffusion models have grown popular for their ability to efficiently acquire a new concept from user-defined text descriptions and a few images. However, in the real world, a user may wish to personalize a model on \u2026"}, {"title": "ANT: Adaptive Noise Schedule for Time Series Diffusion Models", "link": "https://arxiv.org/pdf/2410.14488", "details": "S Lee, K Lee, T Park - arXiv preprint arXiv:2410.14488, 2024", "abstract": "Advances in diffusion models for generative artificial intelligence have recently propagated to the time series (TS) domain, demonstrating state-of-the-art performance on various tasks. However, prior works on TS diffusion models often \u2026"}, {"title": "Simple and Fast Distillation of Diffusion Models", "link": "https://arxiv.org/pdf/2409.19681", "details": "Z Zhou, D Chen, C Wang, C Chen, S Lyu - arXiv preprint arXiv:2409.19681, 2024", "abstract": "Diffusion-based generative models have demonstrated their powerful performance across various tasks, but this comes at a cost of the slow sampling speed. To achieve both efficient and high-quality synthesis, various distillation-based accelerated \u2026"}]
