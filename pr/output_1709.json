'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [DaVinci at SemEval-2024 Task 9: Few-shot prompting GPT'
[{"title": "Elements of World Knowledge (EWOK): A cognition-inspired framework for evaluating basic world knowledge in language models", "link": "https://arxiv.org/pdf/2405.09605", "details": "AA Ivanova, A Sathe, B Lipkin, U Kumar, S Radkani\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The ability to build and leverage world models is essential for a general-purpose AI agent. Testing such capabilities is hard, in part because the building blocks of world models are ill-defined. We present Elements of World Knowledge (EWOK), a \u2026"}, {"title": "Continuous Predictive Modeling of Clinical Notes and ICD Codes in Patient Health Records", "link": "https://arxiv.org/pdf/2405.11622", "details": "MH Caralt, CBL Ng, M Rei - arXiv preprint arXiv:2405.11622, 2024", "abstract": "Electronic Health Records (EHR) serve as a valuable source of patient information, offering insights into medical histories, treatments, and outcomes. Previous research has developed systems for detecting applicable ICD codes that should be assigned \u2026"}, {"title": "Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "link": "https://arxiv.org/pdf/2405.10431", "details": "S Furniturewala, S Jandial, A Java, P Banerjee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing debiasing techniques are typically training-based or require access to the model's internals and output distributions, so they are inaccessible to end-users looking to adapt LLM outputs for their particular needs. In this study, we examine \u2026"}, {"title": "ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios", "link": "https://arxiv.org/pdf/2405.10808", "details": "M Bayer, C Reuter - arXiv preprint arXiv:2405.10808, 2024", "abstract": "Active learning is designed to minimize annotation efforts by prioritizing instances that most enhance learning. However, many active learning strategies struggle with a'cold start'problem, needing substantial initial data to be effective. This limitation \u2026"}, {"title": "Human-Centered Evaluation and Auditing of Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3613905.3636302", "details": "Z Xiao, WH Deng, MS Lam, M Eslami, J Kim, M Lee\u2026 - Extended Abstracts of the \u2026, 2024", "abstract": "The recent advancements in Large Language Models (LLMs) have significantly impacted numerous, and will impact more, real-world applications. However, these models also pose significant risks to individuals and society. To mitigate these issues \u2026"}, {"title": "LG AI Research & KAIST at EHRSQL 2024: Self-Training Large Language Models with Pseudo-Labeled Unanswerable Questions for a Reliable Text-to-SQL System \u2026", "link": "https://arxiv.org/pdf/2405.11162", "details": "Y Jo, S Lee, M Seo, SJ Hwang, M Lee - arXiv preprint arXiv:2405.11162, 2024", "abstract": "Text-to-SQL models are pivotal for making Electronic Health Records (EHRs) accessible to healthcare professionals without SQL knowledge. With the advancements in large language models, these systems have become more adept at \u2026"}, {"title": "SpaceByte: Towards Deleting Tokenization from Large Language Modeling", "link": "https://arxiv.org/pdf/2404.14408", "details": "K Slagle - arXiv preprint arXiv:2404.14408, 2024", "abstract": "Tokenization is widely used in large language models because it significantly improves performance. However, tokenization imposes several disadvantages, such as performance biases, increased adversarial vulnerability, decreased character \u2026"}, {"title": "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "link": "https://arxiv.org/pdf/2405.07551", "details": "S Yin, W You, Z Ji, G Zhong, J Bai - arXiv preprint arXiv:2405.07551, 2024", "abstract": "The tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs, while tool-free methods chose another track: augmenting math \u2026"}]
