[{"title": "Multi-head attention debiasing and contrastive learning for mitigating Dataset Artifacts in Natural Language Inference", "link": "https://arxiv.org/pdf/2412.16194", "details": "K Sivakoti - arXiv preprint arXiv:2412.16194, 2024", "abstract": "While Natural Language Inference (NLI) models have achieved high performances on benchmark datasets, there are still concerns whether they truly capture the intended task, or largely exploit dataset artifacts. Through detailed analysis of the \u2026"}, {"title": "An analytic theory of creativity in convolutional diffusion models", "link": "https://arxiv.org/pdf/2412.20292", "details": "M Kamb, S Ganguli - arXiv preprint arXiv:2412.20292, 2024", "abstract": "We obtain the first analytic, interpretable and predictive theory of creativity in convolutional diffusion models. Indeed, score-based diffusion models can generate highly creative images that lie far from their training data. But optimal score-matching \u2026"}, {"title": "Improving Generative Pre-Training: An In-depth Study of Masked Image Modeling and Denoising Models", "link": "https://arxiv.org/pdf/2412.19104", "details": "H Choi, D Kim, S Cha, KM Yi, D Min - arXiv preprint arXiv:2412.19104, 2024", "abstract": "In this work, we dive deep into the impact of additive noise in pre-training deep networks. While various methods have attempted to use additive noise inspired by the success of latent denoising diffusion models, when used in combination with \u2026"}, {"title": "Layer-and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers", "link": "https://arxiv.org/pdf/2412.16822", "details": "H You, C Barnes, Y Zhou, Y Kang, Z Du, W Zhou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) image generation quality but suffer from high latency and memory inefficiency, making them difficult to deploy on resource-constrained devices. One key efficiency bottleneck is \u2026"}, {"title": "Multimodal Variational Autoencoder: a Barycentric View", "link": "https://arxiv.org/pdf/2412.20487", "details": "P Qiu, W Zhu, S Kumar, X Chen, X Sun, J Yang, A Razi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multiple signal modalities, such as vision and sounds, are naturally present in real- world phenomena. Recently, there has been growing interest in learning generative models, in particular variational autoencoder (VAE), to for multimodal representation \u2026"}, {"title": "How Vision-Language Tasks Benefit from Large Pre-trained Models: A Survey", "link": "https://arxiv.org/pdf/2412.08158", "details": "Y Qi, H Li, Y Song, X Wu, J Luo - arXiv preprint arXiv:2412.08158, 2024", "abstract": "The exploration of various vision-language tasks, such as visual captioning, visual question answering, and visual commonsense reasoning, is an important area in artificial intelligence and continuously attracts the research community's attention \u2026"}, {"title": "DreamOmni: Unified Image Generation and Editing", "link": "https://arxiv.org/pdf/2412.17098", "details": "B Xia, Y Zhang, J Li, C Wang, Y Wang, X Wu, B Yu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Currently, the success of large language models (LLMs) illustrates that a unified multitasking approach can significantly enhance model usability, streamline deployment, and foster synergistic benefits across different tasks. However, in \u2026"}]
