[{"title": "Developing safe and responsible large language model: can we balance bias reduction and language understanding?", "link": "https://link.springer.com/article/10.1007/s10994-025-06767-4", "details": "S Raza, O Bamgbose, S Ghuge, F Tavakoli, DJ Reji\u2026 - Machine Learning, 2025", "abstract": "Abstract Large Language Models (LLMs) have advanced various Natural Language Processing (NLP) tasks, such as text generation and translation, among others. However, these models often generate texts that can perpetuate biases. Existing \u2026"}, {"title": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models", "link": "https://arxiv.org/pdf/2504.18041", "details": "B An, S Zhang, M Dredze - arXiv preprint arXiv:2504.18041, 2025", "abstract": "Efforts to ensure the safety of large language models (LLMs) include safety fine- tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on \u2026"}, {"title": "Establishing Reliability Metrics for Reward Models in Large Language Models", "link": "https://arxiv.org/pdf/2504.14838", "details": "Y Chen, Y Liu, X Wang, Q Yu, G Huzhang, A Zeng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The reward model (RM) that represents human preferences plays a crucial role in optimizing the outputs of large language models (LLMs), eg, through reinforcement learning from human feedback (RLHF) or rejection sampling. However, a long \u2026"}, {"title": "HyperFlow: Gradient-Free Emulation of Few-Shot Fine-Tuning", "link": "https://arxiv.org/pdf/2504.15323", "details": "D Kim, C Kim, S Hong - arXiv preprint arXiv:2504.15323, 2025", "abstract": "While test-time fine-tuning is beneficial in few-shot learning, the need for multiple backpropagation steps can be prohibitively expensive in real-time or low-resource scenarios. To address this limitation, we propose an approach that emulates gradient \u2026"}, {"title": "A Note on Statistically Accurate Tabular Data Generation Using Large Language Models", "link": "https://arxiv.org/pdf/2505.02659", "details": "A Sidorenko - arXiv preprint arXiv:2505.02659, 2025", "abstract": "Large language models (LLMs) have shown promise in synthetic tabular data generation, yet existing methods struggle to preserve complex feature dependencies, particularly among categorical variables. This work introduces a \u2026"}, {"title": "Pointwise Mutual Information as a Performance Gauge for Retrieval-Augmented Generation", "link": "https://aclanthology.org/2025.naacl-long.78.pdf", "details": "T Liu, J Qi, P He, A Bisazza, M Sachan, R Cotterell - \u2026 of the 2025 Conference of the \u2026, 2025", "abstract": "Recent work suggests that large language models enhanced with retrieval- augmented generation are easily influenced by the order in which the retrieved documents are presented to the model when solving tasks such as question \u2026"}]
