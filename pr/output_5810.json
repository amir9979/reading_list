[{"title": "Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models", "link": "https://arxiv.org/pdf/2408.01308", "details": "Y Zhang, D Li, M Okumura - arXiv preprint arXiv:2408.01308, 2024", "abstract": "Learning token embeddings based on token co-occurrence statistics has proven effective for both pre-training and fine-tuning in natural language processing. However, recent studies have pointed out the distribution of learned embeddings \u2026"}, {"title": "MedSyn: LLM-based Synthetic Medical Text Generation Framework", "link": "https://arxiv.org/pdf/2408.02056", "details": "G Kumichev, P Blinov, Y Kuzkina, V Goncharov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating synthetic text addresses the challenge of data availability in privacy- sensitive domains such as healthcare. This study explores the applicability of synthetic data in real-world medical settings. We introduce MedSyn, a novel medical \u2026"}, {"title": "Making Long-Context Language Models Better Multi-Hop Reasoners", "link": "https://arxiv.org/pdf/2408.03246", "details": "Y Li, S Liang, MR Lyu, L Wang - arXiv preprint arXiv:2408.03246, 2024", "abstract": "Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased \u2026"}, {"title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "link": "https://arxiv.org/pdf/2408.00724", "details": "Y Wu, Z Sun, S Li, S Welleck, Y Yang - arXiv preprint arXiv:2408.00724, 2024", "abstract": "The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth \u2026"}, {"title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models", "link": "https://arxiv.org/pdf/2408.03907", "details": "SH Kumar, S Sahay, S Mazumder, E Okur\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have excelled at language understanding and generating human-level text. However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users \u2026"}, {"title": "Fine-tuning Language Models for Joint Rewriting and Completion of Code with Potential Bugs", "link": "https://aclanthology.org/2024.findings-acl.938.pdf", "details": "D Wang, J Zhao, H Pei, S Tan, S Zha - Findings of the Association for Computational \u2026, 2024", "abstract": "Handling drafty partial code remains a notable challenge in real-time code suggestion applications. Previous work has demonstrated shortcomings of large language models of code (CodeLLMs) in completing partial code with potential bugs \u2026"}, {"title": "KoCommonGEN v2: A Benchmark for Navigating Korean Commonsense Reasoning Challenges in Large Language Models", "link": "https://aclanthology.org/2024.findings-acl.141.pdf", "details": "J Seo, J Lee, C Park, ST Hong, S Lee, HS Lim - Findings of the Association for \u2026, 2024", "abstract": "The evolution of large language models (LLMs) has culminated in a multitask model paradigm where prompts drive the generation of user-specific outputs. However, this advancement has revealed a critical challenge: LLMs frequently produce outputs \u2026"}, {"title": "Prompting Encoder Models for Zero-Shot Classification: A Cross-Domain Study in Italian", "link": "https://arxiv.org/pdf/2407.20654", "details": "S Auriemma, M Miliani, M Madeddu, A Bondielli\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Addressing the challenge of limited annotated data in specialized fields and low- resource languages is crucial for the effective use of Language Models (LMs). While most Large Language Models (LLMs) are trained on general-purpose English \u2026"}, {"title": "Unmasking Large Language Models By Means of OpenAI GPT-4 and Google AI: A Deep Instruction-Based Analysis", "link": "https://www.sciencedirect.com/science/article/pii/S2667305324001054", "details": "IA Zahid, SS Joudar, AS Albahri, OS Albahri\u2026 - Intelligent Systems with \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have become a hot topic in AI due to their ability to mimic human conversation. This study compares the open artificial intelligence generative pretrained transformer-4 (GPT-4) model, based on the (GPT) \u2026"}]
