[{"title": "Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge", "link": "https://arxiv.org/pdf/2503.04036", "details": "X Cui, JTZ Wei, S Swayamdipta, R Jia - arXiv preprint arXiv:2503.04036, 2025", "abstract": "Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques \u2026"}, {"title": "Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment", "link": "https://arxiv.org/pdf/2503.04647", "details": "W Yang, J Wu, C Wang, C Zong, J Zhang - arXiv preprint arXiv:2503.04647, 2025", "abstract": "Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences. While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is \u2026"}, {"title": "Assessing Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks", "link": "https://openreview.net/pdf%3Fid%3D3YyyiyV4B6", "details": "F Lin, S Mao, E La Malfa, V Hofmann, A de Wynter\u2026 - Workshop on Reasoning and \u2026", "abstract": "Language is not monolithic. While benchmarks, including those designed for multiple languages, are often used as proxies to evaluate the performance of Large Language Models (LLMs), they tend to overlook the nuances of within-language \u2026"}, {"title": "Language Model Personalization via Reward Factorization", "link": "https://arxiv.org/pdf/2503.06358", "details": "I Shenfeld, F Faltings, P Agrawal, A Pacchiano - arXiv preprint arXiv:2503.06358, 2025", "abstract": "Modern large language models (LLMs) are optimized for human-aligned responses using Reinforcement Learning from Human Feedback (RLHF). However, existing RLHF approaches assume a universal preference model and fail to account for \u2026"}, {"title": "Mitigating Social Bias in Large Language Models by Self-Correction", "link": "https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/Q2-22.pdf", "details": "PAM Kaneko, N Okazaki", "abstract": "Abstract Self-Correction enables Large Language Models (LLMs) to refine their responses during inference based on feedback. While prior research mainly examines the impact of Self-Correction on reasoning tasks such as arithmetic \u2026"}]
