[{"title": "Language Models as Continuous Self-Evolving Data Engineers", "link": "https://arxiv.org/pdf/2412.15151", "details": "P Wang, M Wang, Z Ma, X Yang, S Feng, D Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert \u2026"}, {"title": "Adaptive Pruning for Large Language Models with Structural Importance Awareness", "link": "https://arxiv.org/pdf/2412.15127", "details": "H Zheng, J Ren, Y Sun, R Zhang, W Zhang, Z Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The recent advancements in large language models (LLMs) have significantly improved language understanding and generation capabilities. However, it is difficult to deploy LLMs on resource-constrained edge devices due to their high \u2026"}]
