[{"title": "Discrete JEPA: Learning Discrete Token Representations without Reconstruction", "link": "https://arxiv.org/pdf/2506.14373", "details": "J Baek, H Lee, C Hoang, M Ren, S Ahn - arXiv preprint arXiv:2506.14373, 2025", "abstract": "The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant \u2026", "entry_id": "http://arxiv.org/abs/2506.14373v1", "updated": "2025-06-17 10:15:17", "published": "2025-06-17 10:15:17", "authors": "Junyeob Baek;Hosung Lee;Christopher Hoang;Mengye Ren;Sungjin Ahn", "summary": "The cornerstone of cognitive intelligence lies in extracting hidden patterns\nfrom observations and leveraging these principles to systematically predict\nfuture outcomes. However, current image tokenization methods demonstrate\nsignificant limitations in tasks requiring symbolic abstraction and logical\nreasoning capabilities essential for systematic inference. To address this\nchallenge, we propose Discrete-JEPA, extending the latent predictive coding\nframework with semantic tokenization and novel complementary objectives to\ncreate robust tokenization for symbolic reasoning tasks. Discrete-JEPA\ndramatically outperforms baselines on visual symbolic prediction tasks, while\nstriking visual evidence reveals the spontaneous emergence of deliberate\nsystematic patterns within the learned semantic token space. Though an initial\nmodel, our approach promises a significant impact for advancing Symbolic world\nmodeling and planning capabilities in artificial intelligence systems.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.14373v1;http://arxiv.org/pdf/2506.14373v1", "pdf_url": "http://arxiv.org/pdf/2506.14373v1"}, {"title": "Exploring Non-contrastive Self-supervised Representation Learning for Image-based Profiling", "link": "https://arxiv.org/pdf/2506.14265", "details": "S Dai, Q Xu, P Wen, Y Liu, Q Huang - arXiv preprint arXiv:2506.14265, 2025", "abstract": "Image-based cell profiling aims to create informative representations of cell images. This technique is critical in drug discovery and has greatly advanced with recent improvements in computer vision. Inspired by recent developments in non \u2026", "entry_id": "http://arxiv.org/abs/2506.14265v1", "updated": "2025-06-17 07:25:57", "published": "2025-06-17 07:25:57", "authors": "Siran Dai;Qianqian Xu;Peisong Wen;Yang Liu;Qingming Huang", "summary": "Image-based cell profiling aims to create informative representations of cell\nimages. This technique is critical in drug discovery and has greatly advanced\nwith recent improvements in computer vision. Inspired by recent developments in\nnon-contrastive Self-Supervised Learning (SSL), this paper provides an initial\nexploration into training a generalizable feature extractor for cell images\nusing such methods. However, there are two major challenges: 1) There is a\nlarge difference between the distributions of cell images and natural images,\ncausing the view-generation process in existing SSL methods to fail; and 2)\nUnlike typical scenarios where each representation is based on a single image,\ncell profiling often involves multiple input images, making it difficult to\neffectively combine all available information. To overcome these challenges, we\npropose SSLProfiler, a non-contrastive SSL framework specifically designed for\ncell profiling. We introduce specialized data augmentation and representation\npost-processing methods tailored to cell images, which effectively address the\nissues mentioned above and result in a robust feature extractor. With these\nimprovements, SSLProfiler won the Cell Line Transferability challenge at CVPR\n2025.", "comment": "CVPR 2025 Computer Vision for Drug Discovery", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.14265v1;http://arxiv.org/pdf/2506.14265v1", "pdf_url": "http://arxiv.org/pdf/2506.14265v1"}, {"title": "Utilization of deep learning and machine learning models to approach high glucose and low glucose prediction with type 1 diabetes mellitus in adult patients", "link": "https://www.taylorfrancis.com/chapters/edit/10.1201/9781003658221-23/utilization-deep-learning-machine-learning-models-approach-high-glucose-low-glucose-prediction-type-1-diabetes-mellitus-adult-patients-aditya-prasad-pattnayak-soumya-ranjan-mishra-sachikanta-dash-aparna-baboo", "details": "AP Pattnayak, SR Mishra, S Dash, A Baboo - Intelligent Computing Techniques and \u2026, 2025", "abstract": "For patients suffering from type 1 diabetes, particularly those who are managed with MDI therapy, controlling blood sugars within normal ranges overnight remains one of the major problems. Researchers in this study used ML and DL models to forecast \u2026"}, {"title": "HE-GAD: a behavior-enhanced contrastive learning framework for graph anomaly detection", "link": "https://link.springer.com/article/10.1007/s10994-025-06809-x", "details": "L Zheng, Q Song, Y Wang, Z Wang, X Li - Machine Learning, 2025", "abstract": "The effective detection of anomalies in graph data is crucial for various applications. Although existing contrastive learning methods have made some progress, they still struggle to handle diverse anomaly types concealed in the complex graph structures \u2026"}, {"title": "Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models", "link": "https://arxiv.org/pdf/2506.14399", "details": "T Xia, FDS Ribeiro, RR Rasal, A Kori, R Mehta\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier \u2026", "entry_id": "http://arxiv.org/abs/2506.14399v1", "updated": "2025-06-17 10:56:09", "published": "2025-06-17 10:56:09", "authors": "Tian Xia;Fabio De Sousa Ribeiro;Rajat R Rasal;Avinash Kori;Raghav Mehta;Ben Glocker", "summary": "Counterfactual image generation aims to simulate realistic visual outcomes\nunder specific causal interventions. Diffusion models have recently emerged as\na powerful tool for this task, combining DDIM inversion with conditional\ngeneration via classifier-free guidance (CFG). However, standard CFG applies a\nsingle global weight across all conditioning variables, which can lead to poor\nidentity preservation and spurious attribute changes - a phenomenon known as\nattribute amplification. To address this, we propose Decoupled Classifier-Free\nGuidance (DCFG), a flexible and model-agnostic framework that introduces\ngroup-wise conditioning control. DCFG builds on an attribute-split embedding\nstrategy that disentangles semantic inputs, enabling selective guidance on\nuser-defined attribute groups. For counterfactual generation, we partition\nattributes into intervened and invariant sets based on a causal graph and apply\ndistinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show\nthat DCFG improves intervention fidelity, mitigates unintended changes, and\nenhances reversibility, enabling more faithful and interpretable counterfactual\nimage generation.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2506.14399v1;http://arxiv.org/pdf/2506.14399v1", "pdf_url": "http://arxiv.org/pdf/2506.14399v1"}, {"title": "Deep learning driven interpretable and informed decision making model for brain tumour prediction using explainable AI", "link": "https://www.nature.com/articles/s41598-025-03358-0", "details": "KM Adnan, TM Ghazal, M Saleem, MS Farooq\u2026 - Scientific Reports, 2025", "abstract": "Brain Tumours are highly complex, particularly when it comes to their initial and accurate diagnosis, as this determines patient prognosis. Conventional methods rely on MRI and CT scans and employ generic machine learning techniques, which are \u2026"}, {"title": "Accurate and Scalable Stochastic Gaussian Process Regression via Learnable Coreset-based Variational Inference", "link": "https://openreview.net/pdf%3Fid%3DMCfTk4K1Ig", "details": "M Ketenci, AJ Perotte, N Elhadad, I Urteaga - The 41st Conference on Uncertainty in Artificial \u2026", "abstract": "We introduce a novel stochastic variational inference method for Gaussian process ($\\mathcal {GP} $) regression, by deriving a posterior over a learnable set of coresets: ie, over pseudo-input/output, weighted pairs. Unlike former free-form \u2026"}, {"title": "AMPLIFY: Actionless Motion Priors for Robot Learning from Videos", "link": "https://arxiv.org/pdf/2506.14198", "details": "JA Collins, L Cheng, K Aneja, A Wilcox, B Joffe, A Garg - arXiv preprint arXiv \u2026, 2025", "abstract": "Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a \u2026", "entry_id": "http://arxiv.org/abs/2506.14198v1", "updated": "2025-06-17 05:31:42", "published": "2025-06-17 05:31:42", "authors": "Jeremy A. Collins;Lor\u00e1nd Cheng;Kunal Aneja;Albert Wilcox;Benjamin Joffe;Animesh Garg", "summary": "Action-labeled data for robotics is scarce and expensive, limiting the\ngeneralization of learned policies. In contrast, vast amounts of action-free\nvideo data are readily available, but translating these observations into\neffective policies remains a challenge. We introduce AMPLIFY, a novel framework\nthat leverages large-scale video data by encoding visual dynamics into compact,\ndiscrete motion tokens derived from keypoint trajectories. Our modular approach\nseparates visual motion prediction from action inference, decoupling the\nchallenges of learning what motion defines a task from how robots can perform\nit. We train a forward dynamics model on abundant action-free videos and an\ninverse dynamics model on a limited set of action-labeled examples, allowing\nfor independent scaling. Extensive evaluations demonstrate that the learned\ndynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x\nbetter pixel prediction accuracy compared to prior approaches, and broadly\nuseful. In downstream policy learning, our dynamics predictions enable a\n1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by\nlearning from action-free human videos, and the first generalization to LIBERO\ntasks from zero in-distribution action data. Beyond robotic control, we find\nthe dynamics learned by AMPLIFY to be a versatile latent world model, enhancing\nvideo prediction quality. Our results present a novel paradigm leveraging\nheterogeneous data sources to build efficient, generalizable world models. More\ninformation can be found at https://amplify-robotics.github.io/.", "comment": null, "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.CV;cs.LG", "links": "http://arxiv.org/abs/2506.14198v1;http://arxiv.org/pdf/2506.14198v1", "pdf_url": "http://arxiv.org/pdf/2506.14198v1"}, {"title": "A Pretraining and Fine-Tuning Framework for 3D Fault Detection Based on Global-Local Feature Learning", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425022602", "details": "S Chu, K Li, G Zhou - Expert Systems with Applications, 2025", "abstract": "Abstract 3D fault detection is a core task in structural modeling and reservoir characterization, yet obtaining high-quality annotated 3D seismic fault data is costly, and networks trained on synthetic data often face limited generalization capabilities \u2026"}]
