[{"title": "Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models", "link": "https://arxiv.org/pdf/2412.11041", "details": "D Wu, X Lu, Y Zhao, B Qin - arXiv preprint arXiv:2412.11041, 2024", "abstract": "Although large language models (LLMs) achieve effective safety alignment at the time of release, they still face various safety challenges. A key issue is that fine- tuning often compromises the safety alignment of LLMs. To address this issue, we \u2026"}, {"title": "HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.08378", "details": "S Zhu, W Dong, J Song, Y Guo, B Zheng - arXiv preprint arXiv:2412.08378, 2024", "abstract": "Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into \u2026"}, {"title": "Enhancing the Reasoning Capabilities of Small Language Models via Solution Guidance Fine-Tuning", "link": "https://arxiv.org/pdf/2412.09906", "details": "J Bi, Y Wu, W Xing, Z Wei - arXiv preprint arXiv:2412.09906, 2024", "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks. Advances in prompt engineering and fine-tuning techniques have further enhanced their ability to address complex reasoning challenges \u2026"}, {"title": "Copyright-Protected Language Generation via Adaptive Model Fusion", "link": "https://arxiv.org/pdf/2412.06619%3F", "details": "J Abad, K Donhauser, F Pinto, F Yang - arXiv preprint arXiv:2412.06619, 2024", "abstract": "The risk of language models reproducing copyrighted material from their training data has led to the development of various protective measures. Among these, inference-time strategies that impose constraints via post-processing have shown \u2026"}, {"title": "Training large language models to reason in a continuous latent space", "link": "https://arxiv.org/pdf/2412.06769%3F", "details": "S Hao, S Sukhbaatar, DJ Su, X Li, Z Hu, J Weston\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) are restricted to reason in the\" language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may \u2026"}, {"title": "Interpretable LLM-based Table Question Answering", "link": "https://arxiv.org/pdf/2412.12386", "details": "I Brugere, S Sharma, S Kariyappa, AT Nguyen, F Lecue - arXiv preprint arXiv \u2026, 2024", "abstract": "Interpretability for Table Question Answering (Table QA) is critical, particularly in high- stakes industries like finance or healthcare. Although recent approaches using Large Language Models (LLMs) have significantly improved Table QA performance, their \u2026"}, {"title": "Exploring Multi-Grained Concept Annotations for Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2412.05939", "details": "X Xu, T Niu, Y Xie, L Qin, W Che, MY Kan - arXiv preprint arXiv:2412.05939, 2024", "abstract": "Multimodal Large Language Models (MLLMs) excel in vision--language tasks by pre- training solely on coarse-grained concept annotations (eg, image captions). We hypothesize that integrating fine-grained concept annotations (eg, object labels and \u2026"}, {"title": "The Superalignment of Superhuman Intelligence with Large Language Models", "link": "https://arxiv.org/pdf/2412.11145", "details": "M Huang, Y Wang, S Cui, P Ke, J Tang - arXiv preprint arXiv:2412.11145, 2024", "abstract": "We have witnessed superhuman intelligence thanks to the fast development of large language models and multimodal language models. As the application of such superhuman models becomes more and more common, a critical question rises \u2026"}, {"title": "Structured Extraction of Real World Medical Knowledge using LLMs for Summarization and Search", "link": "https://arxiv.org/pdf/2412.15256", "details": "E Kim, M Shrestha, R Foty, T DeLay, V Seyfert-Margolis - arXiv preprint arXiv \u2026, 2024", "abstract": "Creation and curation of knowledge graphs can accelerate disease discovery and analysis in real-world data. While disease ontologies aid in biological data annotation, codified categories (SNOMED-CT, ICD10, CPT) may not capture patient \u2026"}]
