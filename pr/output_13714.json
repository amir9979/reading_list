[{"title": "Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning", "link": "https://arxiv.org/pdf/2502.11751", "details": "Y Pang, B Yang, H Tu, Y Cao, Z Zhang - arXiv preprint arXiv:2502.11751, 2025", "abstract": "Although Large Language Models (LLMs) excel in reasoning and generation for language tasks, they are not specifically designed for multimodal challenges. Training Multimodal Large Language Models (MLLMs), however, is resource \u2026"}, {"title": "Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration", "link": "https://openreview.net/pdf%3Fid%3DPp90xRxITT", "details": "Q Zhu, R Zhao, H Yan, Y He, Y Chen, L Gui - Workshop on Reasoning and Planning for \u2026", "abstract": "Large Language Models (LLMs) struggle with reasoning due to limited diversity and inefficient search. We propose an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) \u2026"}]
