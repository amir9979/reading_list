[{"title": "Harnessing the Intrinsic Knowledge of Pretrained Language Models for Challenging Text Classification Settings", "link": "https://arxiv.org/pdf/2408.15650", "details": "L Gao - arXiv preprint arXiv:2408.15650, 2024", "abstract": "Text classification is crucial for applications such as sentiment analysis and toxic text filtering, but it still faces challenges due to the complexity and ambiguity of natural language. Recent advancements in deep learning, particularly transformer \u2026"}, {"title": "Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific Expert Pruning", "link": "https://arxiv.org/pdf/2409.01483", "details": "S Sarkar, L Lausen, V Cevher, S Zha, T Brox, G Karypis - arXiv preprint arXiv \u2026, 2024", "abstract": "Sparse Mixture of Expert (SMoE) models have emerged as a scalable alternative to dense models in language modeling. These models use conditionally activated feedforward subnetworks in transformer blocks, allowing for a separation between \u2026"}, {"title": "Improving Extraction of Clinical Event Contextual Properties from Electronic Health Records: A Comparative Study", "link": "https://arxiv.org/pdf/2408.17181", "details": "S Agarwal, T Searle, M Ratas, A Shek, J Teo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Electronic Health Records are large repositories of valuable clinical data, with a significant portion stored in unstructured text format. This textual data includes clinical events (eg, disorders, symptoms, findings, medications and procedures) in \u2026"}, {"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "link": "https://arxiv.org/pdf/2408.12337", "details": "KS Phogat, SA Puranam, S Dasaratha, C Harsha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain \u2026"}, {"title": "Evaluation of pretrained language models on music understanding", "link": "https://arxiv.org/pdf/2409.11449", "details": "Y Vasilakis, R Bittner, J Pauwels - arXiv preprint arXiv:2409.11449, 2024", "abstract": "Music-text multimodal systems have enabled new approaches to Music Information Research (MIR) applications such as audio-to-text and text-to-audio retrieval, text- based song generation, and music captioning. Despite the reported success, little \u2026"}, {"title": "Language Models Benefit from Preparation with Elicited Knowledge", "link": "https://arxiv.org/pdf/2409.01345", "details": "J Yu, H An, LK Schubert - arXiv preprint arXiv:2409.01345, 2024", "abstract": "The zero-shot chain of thought (CoT) approach is often used in question answering (QA) by language models (LMs) for tasks that require multiple reasoning steps, typically enhanced by the prompt\" Let's think step by step.\" However, some QA tasks \u2026"}, {"title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer", "link": "https://arxiv.org/pdf/2408.16978", "details": "J Yao, SA Jacobs, M Tanaka, O Ruwase, A Shafi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) with long context capabilities are integral to complex tasks in natural language processing and computational biology, such as text generation and protein sequence analysis. However, training LLMs directly on \u2026"}]
