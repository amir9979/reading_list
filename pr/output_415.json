'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [MicroHD: An Accuracy-Driven Optimization of Hyperdimen'
[{"title": "Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks", "link": "https://arxiv.org/pdf/2404.00376", "details": "H Kim, H Hwang, J Lee, S Park, D Kim, T Lee, C Yoon\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While recent advancements in commercial large language models (LM) have shown promising results in medical tasks, their closed-source nature poses significant privacy and security concerns, hindering their widespread use in the medical field \u2026"}, {"title": "Source-Aware Training Enables Knowledge Attribution in Language Models", "link": "https://arxiv.org/pdf/2404.01019", "details": "M Khalifa, D Wadden, E Strubell, H Lee, L Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source (s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining \u2026"}, {"title": "Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models", "link": "https://arxiv.org/html/2403.19521v1", "details": "A Lv, K Zhang, Y Chen, Y Wang, L Liu, JR Wen, J Xie\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like\" The capital of France is,\" task-specific attention heads extract the topic entity, such \u2026"}, {"title": "TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale", "link": "https://arxiv.org/pdf/2403.10351", "details": "P Jiang, C Xiao, Z Wang, P Bhatia, J Sun, J Han - arXiv preprint arXiv:2403.10351, 2024", "abstract": "The advent of large language models (LLMs) has significantly advanced natural language processing tasks like text summarization. However, their large size and computational demands, coupled with privacy concerns in data transmission, limit \u2026"}]
