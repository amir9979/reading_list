[{"title": "Assessing and alleviating state anxiety in large language models", "link": "https://www.nature.com/articles/s41746-025-01512-6", "details": "Z Ben-Zion, K Witte, AK Jagadish, O Duek\u2026 - npj Digital Medicine, 2025", "abstract": "Abstract The use of Large Language Models (LLMs) in mental health highlights the need to understand their responses to emotional content. Previous research shows that emotion-inducing prompts can elevate \u201canxiety\u201d in LLMs, affecting behavior and \u2026"}, {"title": "Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective", "link": "https://arxiv.org/pdf/2502.01524%3F", "details": "X Ma, H Xie, SJ Qin - arXiv preprint arXiv:2502.01524, 2025", "abstract": "The integration of vision-language modalities has been a significant focus in multimodal learning, traditionally relying on Vision-Language Pretrained Models. However, with the advent of Large Language Models (LLMs), there has been a \u2026"}, {"title": "LATTE-CXR: Locally Aligned TexT and imagE, Explainable dataset for Chest X-Rays", "link": "https://physionet.org/content/latte-cxr/", "details": "E Ghelichkhan, T Tasdizen", "abstract": "Local annotation of medical data is both expensive and time-consuming due to the high cost of expert annotators, the precision required for accurate annotation, and the inherent challenges of medical diagnosis. To address these problems, we developed \u2026"}, {"title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features", "link": "https://arxiv.org/pdf/2502.14786", "details": "M Tschannen, A Gritsenko, X Wang, MF Naeem\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed \u2026"}, {"title": "Position: Editing Large Language Models Poses Serious Safety Risks", "link": "https://arxiv.org/pdf/2502.02958", "details": "P Youssef, Z Zhao, D Braun, J Schl\u00f6tterer, C Seifert - arXiv preprint arXiv:2502.02958, 2025", "abstract": "Large Language Models (LLMs) contain large amounts of facts about the world. These facts can become outdated over time, which has led to the development of knowledge editing methods (KEs) that can change specific facts in LLMs with limited \u2026"}, {"title": "OPTISHEAR: Towards Efficient and Adaptive Pruning of Large Language Models via Evolutionary Optimization", "link": "https://arxiv.org/pdf/2502.10735", "details": "S Liu, B He, H Wu, L Song - arXiv preprint arXiv:2502.10735, 2025", "abstract": "Post-training pruning has emerged as a crucial optimization technique as large language models (LLMs) continue to grow rapidly. However, the significant variations in weight distributions across different LLMs make fixed pruning strategies \u2026"}, {"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models", "link": "https://arxiv.org/pdf/2502.03199%3F", "details": "J Wu, Y Shen, S Liu, Y Tang, S Song, X Wang, L Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the \u2026"}, {"title": "Knowing When to Stop: Dynamic Context Cutoff for Large Language Models", "link": "https://arxiv.org/pdf/2502.01025", "details": "R Xie, J Wang, P Rosu, C Deng, B Sun, Z Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) process entire input contexts indiscriminately, which is inefficient in cases where the information required to answer a query is localized within the context. We present dynamic context cutoff, a human-inspired method \u2026"}, {"title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models", "link": "https://arxiv.org/pdf/2502.10743", "details": "S Liu, H Wu, B He, Z Liu, X Han, M Yuan, L Song - arXiv preprint arXiv:2502.10743, 2025", "abstract": "Recent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they \u2026"}]
