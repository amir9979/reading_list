[{"title": "Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of Large Language Models", "link": "https://arxiv.org/pdf/2408.04556", "details": "Y Chang, Y Chang, Y Wu - arXiv preprint arXiv:2408.04556, 2024", "abstract": "Large language models (LLMs) have exhibited remarkable proficiency across a diverse array of natural language processing (NLP) tasks. However, adapting LLMs to downstream applications typically necessitates computationally intensive and \u2026"}, {"title": "MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector", "link": "https://arxiv.org/pdf/2408.08661", "details": "W Fu, H Wang, C Gao, G Liu, Y Li, T Jiang - arXiv preprint arXiv:2408.08661, 2024", "abstract": "The increasing parameters and expansive dataset of large language models (LLMs) highlight the urgent demand for a technical solution to audit the underlying privacy risks and copyright issues associated with LLMs. Existing studies have partially \u2026"}, {"title": "Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs", "link": "https://arxiv.org/pdf/2408.14397", "details": "X Zhang, JN Acosta, HY Zhou, P Rajpurkar - arXiv preprint arXiv:2408.14397, 2024", "abstract": "Recent advancements in artificial intelligence have significantly improved the automatic generation of radiology reports. However, existing evaluation methods fail to reveal the models' understanding of radiological images and their capacity to \u2026"}, {"title": "SeA: Semantic Adversarial Augmentation for Last Layer Features from Unsupervised Representation Learning", "link": "https://arxiv.org/pdf/2408.13351", "details": "Q Qian, Y Xu, J Hu - arXiv preprint arXiv:2408.13351, 2024", "abstract": "Deep features extracted from certain layers of a pre-trained deep model show superior performance over the conventional hand-crafted features. Compared with fine-tuning or linear probing that can explore diverse augmentations,\\eg, random \u2026"}, {"title": "DPDLLM: A Black-box Framework for Detecting Pre-training Data from Large Language Models", "link": "https://aclanthology.org/2024.findings-acl.35.pdf", "details": "B Zhou, Z Wang, L Wang, H Wang, Y Zhang, K Song\u2026 - Findings of the Association \u2026, 2024", "abstract": "The success of large language models (LLM) benefits from large-scale model parameters and large amounts of pre-training data. However, the textual data for training LLM can not be confirmed to be legal because they are crawled from \u2026"}, {"title": "A Practitioner's Guide to Continual Multimodal Pretraining", "link": "https://arxiv.org/pdf/2408.14471", "details": "K Roth, V Udandarao, S Dziadzio, A Prabhu, M Cherti\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multimodal foundation models serve numerous applications at the intersection of vision and language. Still, despite being pretrained on extensive data, they become outdated over time. To keep models updated, research into continual pretraining \u2026"}, {"title": "Effective prompt extraction from language models", "link": "https://openreview.net/pdf%3Fid%3D0o95CVdNuz", "details": "Y Zhang, N Carlini, D Ippolito - First Conference on Language Modeling, 2024", "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden \u2026"}, {"title": "Large Language Models Can Learn Representation in Natural Language", "link": "https://aclanthology.org/2024.findings-acl.542.pdf", "details": "Y Guo, Y Liang, D Zhao, N Duan - Findings of the Association for Computational \u2026, 2024", "abstract": "One major challenge for Large Language Models (LLMs) is completing complex tasks involving multiple entities, such as tool APIs. To tackle this, one approach is to retrieve relevant entities to enhance LLMs in task completion. A crucial issue here is \u2026"}, {"title": "Foundation Model for Biomedical Graphs: Integrating Knowledge Graphs and Protein Structures to Large Language Models", "link": "https://aclanthology.org/2024.acl-srw.30.pdf", "details": "Y Kim - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Transformer model has been a de-facto standard in natural language processing. Its adaptations in other fields such as computer vision showed promising results that this architecture is a powerful neural network in representation learning regardless of \u2026"}]
