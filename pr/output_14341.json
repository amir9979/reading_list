[{"title": "RESPONSE: Benchmarking the Ability of Language Models to Undertake Commonsense Reasoning in Crisis Situation", "link": "https://arxiv.org/pdf/2503.11348", "details": "A Diallo, A Bikakis, L Dickens, A Hunter, R Miller - arXiv preprint arXiv:2503.11348, 2025", "abstract": "An interesting class of commonsense reasoning problems arises when people are faced with natural disasters. To investigate this topic, we present\\textsf {RESPONSE}, a human-curated dataset containing 1789 annotated instances featuring 6037 sets of \u2026"}, {"title": "Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA", "link": "https://arxiv.org/pdf/2503.17933", "details": "J Ou, T Huang, Y Zhao, Z Yu, P Lu, R Ying - arXiv preprint arXiv:2503.17933, 2025", "abstract": "To improve the reliability of Large Language Models (LLMs) in clinical applications, retrieval-augmented generation (RAG) is extensively applied to provide factual medical knowledge. However, beyond general medical knowledge from open-ended \u2026"}, {"title": "Language Models May Verbatim Complete TextThey Were Not Explicitly Trained On", "link": "https://arxiv.org/pdf/2503.17514", "details": "KZ Liu, CA Choquette-Choo, M Jagielski, P Kairouz\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "An important question today is whether a given text was used to train a large language model (LLM). A\\emph {completion} test is often employed: check if the LLM completes a sufficiently complex text. This, however, requires a ground-truth \u2026"}, {"title": "Unsupervised Topic Models are Data Mixers for Pre-training Language Models", "link": "https://arxiv.org/pdf/2502.16802", "details": "J Peng, X Zhuang, Q Jiantao, R Ma, J Yu, T Bai, C He - arXiv preprint arXiv \u2026, 2025", "abstract": "The performance of large language models (LLMs) is significantly affected by the quality and composition of their pre-training data, which is inherently diverse, spanning various domains, sources, and topics. Effectively integrating these \u2026"}, {"title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing", "link": "https://arxiv.org/pdf/2503.08588", "details": "X Xu, W Xu, N Zhang, J McAuley - arXiv preprint arXiv:2503.08588, 2025", "abstract": "Previous studies have established that language models manifest stereotyped biases. Existing debiasing strategies, such as retraining a model with counterfactual data, representation projection, and prompting often fail to efficiently eliminate bias or \u2026"}, {"title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs", "link": "https://arxiv.org/pdf/2503.01743%3F", "details": "A Abouelenin, A Ashfaq, A Atkinson, H Awadalla\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent \u2026"}, {"title": "Process-based self-rewarding language models", "link": "https://arxiv.org/pdf/2503.03746", "details": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' \u2026"}, {"title": "Language Models Predict Empathy Gaps Between Social In-groups and Out-groups", "link": "https://arxiv.org/pdf/2503.01030", "details": "Y Hou, H Daum\u00e9 III, R Rudinger - arXiv preprint arXiv:2503.01030, 2025", "abstract": "Studies of human psychology have demonstrated that people are more motivated to extend empathy to in-group members than out-group members (Cikara et al., 2011). In this study, we investigate how this aspect of intergroup relations in humans is \u2026"}, {"title": "Chain-of-Thought Matters: Improving Long-Context Language Models with Reasoning Path Supervision", "link": "https://arxiv.org/pdf/2502.20790", "details": "D Zhu, X Wei, G Zhao, W Wu, H Zou, J Ran, X Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in Large Language Models (LLMs) have highlighted the challenge of handling long-context tasks, where models need to reason over extensive input contexts to aggregate target information. While Chain-of-Thought (CoT) prompting \u2026"}]
