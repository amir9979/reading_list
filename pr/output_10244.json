[{"title": "The Radiance of Neural Fields: Democratizing Photorealistic and Dynamic Robotic Simulation", "link": "https://arxiv.org/pdf/2411.16940", "details": "G Nuthall, R Bowden, O Mendez - arXiv preprint arXiv:2411.16940, 2024", "abstract": "As robots increasingly coexist with humans, they must navigate complex, dynamic environments rich in visual information and implicit social dynamics, like when to yield or move through crowds. Addressing these challenges requires significant \u2026"}, {"title": "Synthetic Vision: Training Vision-Language Models to Understand Physics", "link": "https://arxiv.org/pdf/2412.08619", "details": "V Balazadeh, M Ataei, H Cheong, AH Khasahmadi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Physical reasoning, which involves the interpretation, understanding, and prediction of object behavior in dynamic environments, remains a significant challenge for current Vision-Language Models (VLMs). In this work, we propose two methods to \u2026"}, {"title": "Enhancing Instruction-Following Capability of Visual-Language Models by Reducing Image Redundancy", "link": "https://arxiv.org/pdf/2411.15453", "details": "T Yang, J Jia, X Zhu, W Zhao, B Wang, Y Cheng, Y Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have strong instruction-following capability to interpret and execute tasks as directed by human commands. Multimodal Large Language Models (MLLMs) have inferior instruction-following ability compared to \u2026"}, {"title": "Chain of Thought Prompting in Vision-Language Model for Vision Reasoning Tasks", "link": "https://link.springer.com/chapter/10.1007/978-981-96-0351-0_22", "details": "J Ou, J Zhou, Y Dong, F Chen - Australasian Joint Conference on Artificial Intelligence, 2024", "abstract": "The large language model has demonstrated its ability to reason and interpret in text- to-text applications. Current Chain of Thought (CoT) research focuses on either explaining reasoning steps or improving prediction results. This paper proposes a \u2026"}, {"title": "ACE: Action Concept Enhancement of Video-Language Models in Procedural Videos", "link": "https://arxiv.org/pdf/2411.15628", "details": "R Ghoddoosian, N Agarwal, I Dwivedi, B Darisuh - arXiv preprint arXiv:2411.15628, 2024", "abstract": "Vision-language models (VLMs) are capable of recognizing unseen actions. However, existing VLMs lack intrinsic understanding of procedural action concepts. Hence, they overfit to fixed labels and are not invariant to unseen action synonyms \u2026"}, {"title": "Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality", "link": "https://arxiv.org/pdf/2411.11531", "details": "V Chekalina, A Razzigaev, E Goncharova, A Kuznetsov - arXiv preprint arXiv \u2026, 2024", "abstract": "In this paper we present an approach to reduce hallucinations in Large Language Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional modality. Our method involves transforming input text into a set of KG embeddings and using \u2026"}, {"title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large Language Model", "link": "https://arxiv.org/pdf/2411.10803", "details": "T Liu, L Shi, R Hong, Y Hu, Q Yin, L Zhang - arXiv preprint arXiv:2411.10803, 2024", "abstract": "The vision tokens in multimodal large language models usually exhibit significant spatial and temporal redundancy and take up most of the input tokens, which harms their inference efficiency. To solve this problem, some recent works were introduced \u2026"}, {"title": "Large Language Model with Region-guided Referring and Grounding for CT Report Generation", "link": "https://arxiv.org/pdf/2411.15539", "details": "Z Chen, Y Bie, H Jin, H Chen - arXiv preprint arXiv:2411.15539, 2024", "abstract": "Computed tomography (CT) report generation is crucial to assist radiologists in interpreting CT volumes, which can be time-consuming and labor-intensive. Existing methods primarily only consider the global features of the entire volume, making it \u2026"}, {"title": "Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents", "link": "https://arxiv.org/pdf/2411.16740", "details": "J Chen, D Xu, J Fei, CM Feng, M Elhoseiny - arXiv preprint arXiv:2411.16740, 2024", "abstract": "Large multimodal models (LMMs) have achieved impressive progress in vision- language understanding, yet they face limitations in real-world applications requiring complex reasoning over a large number of images. Existing benchmarks for multi \u2026"}]
