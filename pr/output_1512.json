'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Comparing generative and retrieval-based chatbots in answeri'
[{"title": "Empowering PET Imaging Reporting with Retrieval-Augmented Large Language Models and Reading Reports Database: A Pilot Single Center Study", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/05/14/2024.05.13.24307312.full.pdf", "details": "H Choi, D Lee, Y Kang - medRxiv, 2024", "abstract": "Introduction: The potential of Large Language Models (LLMs) in enhancing a variety of natural language tasks in clinical fields includes medical imaging reporting. This pilot study examines the efficacy of a retrieval-augmented LLM system considering \u2026"}, {"title": "On the Importance of Expert Knowledge to Improve Foundation Models for Retinal Fundus Images", "link": "https://openreview.net/pdf%3Fid%3DRnaTcnSI8n", "details": "J Silva-Rodr\u00edguez, H Chakor, J Dolz, IB Ayed - Medical Imaging with Deep Learning, 2024", "abstract": "Foundation models are currently revolutionizing the medical image analysis community. Pre-trained on large data sources, such networks provide efficient transferability to downstream tasks. In this context, a myriad of foundation models \u2026"}, {"title": "On the Reliability of Large Language Models to Misinformed and Demographically-Informed Prompts", "link": "https://www.researchgate.net/profile/Toluwani-Aremu/publication/377742905_On_the_Reliability_of_Large_Language_Models_to_Misinformed_and_Demographically-Informed_Prompts/links/6637d1ff7091b94e93f401f8/On-the-Reliability-of-Large-Language-Models-to-Misinformed-and-Demographically-Informed-Prompts.pdf", "details": "T Aremu, O Akinwehinmi, C Nwagu, SI Ahmed, R Orji\u2026 - 2024", "abstract": "We investigate and observe the behaviour and performance of Large Language Model (LLM)-backed chatbots in addressing misinformed prompts and questions with demographic information within the domains of Climate Change and Mental \u2026"}, {"title": "Addax: Memory-Efficient Fine-Tuning of Language Models with a Combination of Forward-Backward and Forward-Only Passes", "link": "https://openreview.net/pdf%3Fid%3DYtZv36CY5p", "details": "Z Li, X Zhang, M Razaviyayn - 5th Workshop on practical ML for limited/low resource \u2026", "abstract": "Fine-tuning language models (LMs) with first-order optimizers often demands excessive memory, limiting accessibility, while zeroth-order optimizers use less memory, but suffer from slow convergence depending on model size. We introduce a \u2026"}]
