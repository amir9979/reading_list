[{"title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models", "link": "https://arxiv.org/pdf/2506.07468", "details": "M Liu, L Jiang, Y Liang, SS Du, Y Choi, T Althoff\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch--attackers \u2026", "entry_id": "http://arxiv.org/abs/2506.07468v1", "updated": "2025-06-09 06:35:12", "published": "2025-06-09 06:35:12", "authors": "Mickel Liu;Liwei Jiang;Yancheng Liang;Simon Shaolei Du;Yejin Choi;Tim Althoff;Natasha Jaques", "summary": "Conventional language model (LM) safety alignment relies on a reactive,\ndisjoint procedure: attackers exploit a static model, followed by defensive\nfine-tuning to patch exposed vulnerabilities. This sequential approach creates\na mismatch -- attackers overfit to obsolete defenses, while defenders\nperpetually lag behind emerging threats. To address this, we propose\nSelf-RedTeam, an online self-play reinforcement learning algorithm where an\nattacker and defender agent co-evolve through continuous interaction. We cast\nsafety alignment as a two-player zero-sum game, where a single model alternates\nbetween attacker and defender roles -- generating adversarial prompts and\nsafeguarding against them -- while a reward LM adjudicates outcomes. This\nenables dynamic co-adaptation. Grounded in the game-theoretic framework of\nzero-sum games, we establish a theoretical safety guarantee which motivates the\ndesign of our method: if self-play converges to a Nash Equilibrium, the\ndefender will reliably produce safe responses to any adversarial input.\nEmpirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared\nto attackers trained against static defenders and achieves higher robustness on\nsafety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained\nagainst static attackers. We further propose hidden Chain-of-Thought, allowing\nagents to plan privately, which boosts adversarial diversity and reduces\nover-refusals. Our results motivate a shift from reactive patching to proactive\nco-evolution in LM safety training, enabling scalable, autonomous, and robust\nself-improvement of LMs via multi-agent reinforcement learning (MARL).", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CL;cs.MA", "links": "http://arxiv.org/abs/2506.07468v1;http://arxiv.org/pdf/2506.07468v1", "pdf_url": "http://arxiv.org/pdf/2506.07468v1"}, {"title": "Revisiting Backdoor Attacks against Large Vision-Language Models from Domain Shift", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liang_Revisiting_Backdoor_Attacks_against_Large_Vision-Language_Models_from_Domain_Shift_CVPR_2025_paper.pdf", "details": "S Liang, J Liang, T Pang, C Du, A Liu, M Zhu, X Cao\u2026 - Proceedings of the \u2026, 2025", "abstract": "Instruction tuning enhances large vision-language models (LVLMs) but increases their vulnerability to backdoor attacks due to their open design. Unlike prior studies in static settings, this paper explores backdoor attacks in LVLM instruction tuning \u2026"}, {"title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "link": "https://arxiv.org/pdf/2506.09965", "details": "J Wu, J Guan, K Feng, Q Liu, S Wu, L Wang, W Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As textual reasoning with large language models (LLMs) has advanced significantly, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs). However, existing methods primarily \u2026", "entry_id": "http://arxiv.org/abs/2506.09965v1", "updated": "2025-06-11 17:41:50", "published": "2025-06-11 17:41:50", "authors": "Junfei Wu;Jian Guan;Kaituo Feng;Qiang Liu;Shu Wu;Liang Wang;Wei Wu;Tieniu Tan", "summary": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI;I.2", "links": "http://arxiv.org/abs/2506.09965v1;http://arxiv.org/pdf/2506.09965v1", "pdf_url": "http://arxiv.org/pdf/2506.09965v1"}, {"title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models", "link": "https://arxiv.org/pdf/2506.06395", "details": "P Li, M Skripkin, A Zubrey, A Kuznetsov, I Oseledets - arXiv preprint arXiv:2506.06395, 2025", "abstract": "Large language models (LLMs) excel at reasoning, yet post-training remains critical for aligning their behavior with task goals. Existing reinforcement learning (RL) methods often depend on costly human annotations or external reward models. We \u2026", "entry_id": "http://arxiv.org/abs/2506.06395v3", "updated": "2025-06-11 06:21:59", "published": "2025-06-05 19:55:15", "authors": "Pengyi Li;Matvey Skripkin;Alexander Zubrey;Andrey Kuznetsov;Ivan Oseledets", "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,\nRLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on\nMinerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a\nsimple, scalable post-training method for inference models, requiring only a\nsmall number of samples and unlabelled supervision.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.LG", "links": "http://arxiv.org/abs/2506.06395v3;http://arxiv.org/pdf/2506.06395v3", "pdf_url": "http://arxiv.org/pdf/2506.06395v3"}, {"title": "Flexible Realignment of Language Models", "link": "https://arxiv.org/pdf/2506.12704", "details": "W Zhu, R Xie, W Zhang, R Wang - arXiv preprint arXiv:2506.12704, 2025", "abstract": "Realignment becomes necessary when a language model (LM) fails to meet expected performance. We propose a flexible realignment framework that supports quantitative control of alignment degree during training and inference. This \u2026", "entry_id": "http://arxiv.org/abs/2506.12704v1", "updated": "2025-06-15 03:26:59", "published": "2025-06-15 03:26:59", "authors": "Wenhong Zhu;Ruobing Xie;Weinan Zhang;Rui Wang", "summary": "Realignment becomes necessary when a language model (LM) fails to meet\nexpected performance. We propose a flexible realignment framework that supports\nquantitative control of alignment degree during training and inference. This\nframework incorporates Training-time Realignment (TrRa), which efficiently\nrealigns the reference model by leveraging the controllable fusion of logits\nfrom both the reference and already aligned models. For example, TrRa reduces\ntoken usage by 54.63% on DeepSeek-R1-Distill-Qwen-1.5B without any performance\ndegradation, outperforming DeepScaleR-1.5B's 33.86%. To complement TrRa during\ninference, we introduce a layer adapter that enables smooth Inference-time\nRealignment (InRa). This adapter is initialized to perform an identity\ntransformation at the bottom layer and is inserted preceding the original\nlayers. During inference, input embeddings are simultaneously processed by the\nadapter and the original layer, followed by the remaining layers, and then\ncontrollably interpolated at the logit level. We upgraded\nDeepSeek-R1-Distill-Qwen-7B from a slow-thinking model to one that supports\nboth fast and slow thinking, allowing flexible alignment control even during\ninference. By encouraging deeper reasoning, it even surpassed its original\nperformance.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.12704v1;http://arxiv.org/pdf/2506.12704v1", "pdf_url": "http://arxiv.org/pdf/2506.12704v1"}, {"title": "RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics", "link": "https://arxiv.org/pdf/2506.04308", "details": "E Zhou, J An, C Chi, Y Han, S Rong, C Zhang, P Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the \u2026", "entry_id": "http://arxiv.org/abs/2506.04308v1", "updated": "2025-06-04 17:59:27", "published": "2025-06-04 17:59:27", "authors": "Enshen Zhou;Jingkun An;Cheng Chi;Yi Han;Shanyu Rong;Chi Zhang;Pengwei Wang;Zhongyuan Wang;Tiejun Huang;Lu Sheng;Shanghang Zhang", "summary": "Spatial referring is a fundamental capability of embodied robots to interact\nwith the 3D physical world. However, even with the powerful pretrained vision\nlanguage models (VLMs), recent approaches are still not qualified to accurately\nunderstand the complex 3D scenes and dynamically reason about the\ninstruction-indicated locations for interaction. To this end, we propose\nRoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding\nby integrating a disentangled but dedicated depth encoder via supervised\nfine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial\nreasoning via reinforcement fine-tuning (RFT), with metric-sensitive process\nreward functions tailored for spatial referring tasks. To support SFT and RFT\ntraining, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x\nprior), covering 31 spatial relations (vs. 15 prior) and supporting complex\nreasoning processes (up to 5 steps). In addition, we introduce\nRefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial\nreferring with multi-step reasoning. Experiments show that SFT-trained\nRoboRefer achieves state-of-the-art spatial understanding, with an average\nsuccess rate of 89.6%. RFT-trained RoboRefer further outperforms all other\nbaselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average\naccuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various\ncontrol policies to execute long-horizon, dynamic tasks across diverse robots\n(e,g., UR5, G1 humanoid) in cluttered real-world scenes.", "comment": "Project page: https://zhoues.github.io/RoboRefer/", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2506.04308v1;http://arxiv.org/pdf/2506.04308v1", "pdf_url": "http://arxiv.org/pdf/2506.04308v1"}, {"title": "SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models", "link": "https://arxiv.org/pdf/2506.12935", "details": "X Diao, C Zhang, K Kong, W Wu, C Ma, Z Ouyang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While large language models have shown reasoning capabilities, their application to the audio modality, particularly in large audio-language models (ALMs), remains significantly underdeveloped. Addressing this gap requires a systematic approach \u2026", "entry_id": "http://arxiv.org/abs/2506.12935v1", "updated": "2025-06-15 18:26:08", "published": "2025-06-15 18:26:08", "authors": "Xingjian Diao;Chunhui Zhang;Keyi Kong;Weiyi Wu;Chiyu Ma;Zhongyu Ouyang;Peijun Qing;Soroush Vosoughi;Jiang Gui", "summary": "While large language models have shown reasoning capabilities, their\napplication to the audio modality, particularly in large audio-language models\n(ALMs), remains significantly underdeveloped. Addressing this gap requires a\nsystematic approach, involving a capable base model, high-quality\nreasoning-oriented audio data, and effective training algorithms. In this\nstudy, we present a comprehensive solution: we introduce the Audio Logical\nReasoning (ALR) dataset, consisting of 6,446 text-audio annotated samples\nspecifically designed for complex reasoning tasks. Building on this resource,\nwe propose SoundMind, a rule-based reinforcement learning (RL) algorithm\ntailored to endow ALMs with deep bimodal reasoning abilities. By training\nQwen2.5-Omni-7B on the ALR dataset using SoundMind, our approach achieves\nstate-of-the-art performance in audio logical reasoning. This work highlights\nthe impact of combining high-quality, reasoning-focused datasets with\nspecialized RL techniques, advancing the frontier of auditory intelligence in\nlanguage models. Our code and the proposed dataset are available at\nhttps://github.com/xid32/SoundMind.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.MM;cs.SD;eess.AS", "links": "http://arxiv.org/abs/2506.12935v1;http://arxiv.org/pdf/2506.12935v1", "pdf_url": "http://arxiv.org/pdf/2506.12935v1"}, {"title": "ICT-QA: Question Answering over Multi-modal Contexts including Image, Chart, and Text Modalities", "link": "https://openaccess.thecvf.com/content/CVPR2025W/MULA2025/papers/Jang_ICT-QA_Question_Answering_over_Multi-modal_Contexts_including_Image_Chart_and_CVPRW_2025_paper.pdf", "details": "Y Jang, H Kong, G Kim, Y Lee, J Choi, K Bae - \u2026 of the Computer Vision and Pattern \u2026, 2025", "abstract": "For question answering in multi-modal contexts that include image, chart, and text modalities, a model must be proficient in understanding each individual modality. Furthermore, the model must be able to find the necessary evidence from multiple \u2026"}, {"title": "Large Language Models Enhanced by Plug and Play Syntactic Knowledge for Aspect-based Sentiment Analysis", "link": "https://arxiv.org/pdf/2506.12991", "details": "Y Tian, X Li, W Wang, G Jin, P Cheng, Y Song - arXiv preprint arXiv:2506.12991, 2025", "abstract": "Aspect-based sentiment analysis (ABSA) generally requires a deep understanding of the contextual information, including the words associated with the aspect terms and their syntactic dependencies. Most existing studies employ advanced encoders (eg \u2026", "entry_id": "http://arxiv.org/abs/2506.12991v1", "updated": "2025-06-15 23:16:12", "published": "2025-06-15 23:16:12", "authors": "Yuanhe Tian;Xu Li;Wei Wang;Guoqing Jin;Pengsen Cheng;Yan Song", "summary": "Aspect-based sentiment analysis (ABSA) generally requires a deep\nunderstanding of the contextual information, including the words associated\nwith the aspect terms and their syntactic dependencies. Most existing studies\nemploy advanced encoders (e.g., pre-trained models) to capture such context,\nespecially large language models (LLMs). However, training these encoders is\nresource-intensive, and in many cases, the available data is insufficient for\nnecessary fine-tuning. Therefore it is challenging for learning LLMs within\nsuch restricted environments and computation efficiency requirement. As a\nresult, it motivates the exploration of plug-and-play methods that adapt LLMs\nto ABSA with minimal effort. In this paper, we propose an approach that\nintegrates extendable components capable of incorporating various types of\nsyntactic knowledge, such as constituent syntax, word dependencies, and\ncombinatory categorial grammar (CCG). Specifically, we propose a memory module\nthat records syntactic information and is incorporated into LLMs to instruct\nthe prediction of sentiment polarities. Importantly, this encoder acts as a\nversatile, detachable plugin that is trained independently of the LLM. We\nconduct experiments on benchmark datasets, which show that our approach\noutperforms strong baselines and previous approaches, thus demonstrates its\neffectiveness.", "comment": "12 pages, 4 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.12991v1;http://arxiv.org/pdf/2506.12991v1", "pdf_url": "http://arxiv.org/pdf/2506.12991v1"}]
