[{"title": "Gradient-Guided Conditional Diffusion Models for Private Image Reconstruction: Analyzing Adversarial Impacts of Differential Privacy and Denoising", "link": "https://arxiv.org/pdf/2411.03053%3F", "details": "T Huang, J Meng, H Chen, G Zheng, X Yang, X Yi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We investigate the construction of gradient-guided conditional diffusion models for reconstructing private images, focusing on the adversarial interplay between differential privacy noise and the denoising capabilities of diffusion models. While \u2026"}, {"title": "Reward Fine-Tuning Two-Step Diffusion Models via Learning Differentiable Latent-Space Surrogate Reward", "link": "https://arxiv.org/pdf/2411.15247", "details": "Z Jia, Y Nan, H Zhao, G Liu - arXiv preprint arXiv:2411.15247, 2024", "abstract": "Recent research has shown that fine-tuning diffusion models (DMs) with arbitrary rewards, including non-differentiable ones, is feasible with reinforcement learning (RL) techniques, enabling flexible model alignment. However, applying existing RL \u2026"}, {"title": "GIFT: A Framework for Global Interpretable Faithful Textual Explanations of Vision Classifiers", "link": "https://arxiv.org/pdf/2411.15605", "details": "\u00c9 Zablocki, V Gerard, A Cardiel, E Gaussier, M Cord\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Understanding deep models is crucial for deploying them in safety-critical applications. We introduce GIFT, a framework for deriving post-hoc, global, interpretable, and faithful textual explanations for vision classifiers. GIFT starts from \u2026"}, {"title": "Improving Pre-Trained Self-Supervised Embeddings Through Effective Entropy Maximization", "link": "https://arxiv.org/pdf/2411.15931", "details": "D Chakraborty, Y LeCun, TGJ Rudner\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "A number of different architectures and loss functions have been applied to the problem of self-supervised learning (SSL), with the goal of developing embeddings that provide the best possible pre-training for as-yet-unknown, lightly supervised \u2026"}, {"title": "Steering Language Model Refusal with Sparse Autoencoders", "link": "https://arxiv.org/pdf/2411.11296", "details": "K O'Brien, D Majercak, X Fernandes, R Edgar, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Responsible practices for deploying language models include guiding models to recognize and refuse answering prompts that are considered unsafe, while complying with safe prompts. Achieving such behavior typically requires updating \u2026"}, {"title": "Interpretability as Approximation: Understanding Black-Box Models by Decision Boundary", "link": "https://www.mdpi.com/2079-9292/13/22/4339", "details": "H Dong, B Liu, D Ye, G Liu - Electronics, 2024", "abstract": "Currently, interpretability methods focus more on less objective human- understandable semantics. To objectify and standardize interpretability research, in this study, we provide notions of interpretability based on approximation theory. We \u2026"}, {"title": "ICEv2: Interpretability, Comprehensiveness, and Explainability in Vision Transformer", "link": "https://link.springer.com/article/10.1007/s11263-024-02290-6", "details": "H Choi, S Jin, K Han - International Journal of Computer Vision, 2024", "abstract": "Vision transformers use [CLS] token to predict image classes. Their explainability visualization has been studied using relevant information from the [CLS] token or focusing on attention scores during self-attention. However, such visualization is \u2026"}, {"title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time", "link": "https://openreview.net/pdf%3Fid%3DKkYZmepjHn", "details": "Z Chen, H Yuan, Y Li, Y Kou, J Zhang, Q Gu - The Thirty-eighth Annual Conference \u2026, 2024", "abstract": "Discrete diffusion models have emerged as powerful tools for high-quality data generation. Despite their success in discrete spaces, such as text generation tasks, the acceleration of discrete diffusion models remains under-explored. In this paper \u2026"}, {"title": "Efficient Vision-Language pre-training via domain-specific learning for human activities", "link": "https://aclanthology.org/2024.emnlp-main.454.pdf", "details": "A Bulat, Y Ouali, R Guerrero, B Martinez\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract Current Vision-Language (VL) models owe their success to large-scale pre- training on web-collected data, which in turn requires high-capacity architectures and large compute resources for training. We posit that when the downstream tasks are \u2026"}]
