[{"title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression", "link": "https://arxiv.org/pdf/2505.19433", "details": "P Dong, Z Tang, X Liu, L Li, X Chu, B Li - arXiv preprint arXiv:2505.19433, 2025", "abstract": "Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (eg, perplexity) and \u2026", "entry_id": "http://arxiv.org/abs/2505.19433v1", "updated": "2025-05-26 02:49:07", "published": "2025-05-26 02:49:07", "authors": "Peijie Dong;Zhenheng Tang;Xiang Liu;Lujun Li;Xiaowen Chu;Bo Li", "summary": "Post-training compression reduces the computational and memory costs of large\nlanguage models (LLMs), enabling resource-efficient deployment. However,\nexisting compression benchmarks only focus on language modeling (e.g.,\nperplexity) and natural language understanding tasks (e.g., GLUE accuracy),\nignoring the agentic capabilities - workflow, tool use/function call,\nlong-context understanding and real-world application. We introduce the Agent\nCompression Benchmark (ACBench), the first comprehensive benchmark for\nevaluating how compression impacts LLMs' agentic abilities. ACBench spans (1)\n12 tasks across 4 capabilities (e.g., WorfBench for workflow generation,\nNeedle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ)\nand pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B),\nstandard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill).\nOur experiments reveal compression tradeoffs: 4-bit quantization preserves\nworkflow generation and tool use (1%-3% drop) but degrades real-world\napplication accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation\nand Energy to systematize analysis. ACBench provides actionable insights for\noptimizing LLM compression in agentic scenarios. The code can be found in\nhttps://github.com/pprp/ACBench.", "comment": "Accepted by ICML2025 as Poster", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.19433v1;http://arxiv.org/pdf/2505.19433v1", "pdf_url": "http://arxiv.org/pdf/2505.19433v1"}, {"title": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge", "link": "https://arxiv.org/pdf/2505.19176", "details": "Z Liu, M Li, X Deng, Q Wang, F Feng - arXiv preprint arXiv:2505.19176, 2025", "abstract": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to evaluate the quality of LLM-generated responses, gaining popularity for its cost-effectiveness and strong alignment with human evaluations. However, training proxy judge \u2026", "entry_id": "http://arxiv.org/abs/2505.19176v1", "updated": "2025-05-25 14:48:49", "published": "2025-05-25 14:48:49", "authors": "Zhuo Liu;Moxin Li;Xun Deng;Qifan Wang;Fuli Feng", "summary": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge.", "comment": "Under review", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.19176v1;http://arxiv.org/pdf/2505.19176v1", "pdf_url": "http://arxiv.org/pdf/2505.19176v1"}, {"title": "FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information", "link": "https://arxiv.org/pdf/2505.20650", "details": "Y Wang, Y Ren, L Qian, X Peng, K Wang, Y Han\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Table 8 shows that all LLMs fail completely with extreme classification, yielding zero precision, recall, and F1, confirming that a single-step choice among thousands of flat labels is not a valid **LLM** **evaluation** protocol. By decoupling extraction from \u2026", "entry_id": "http://arxiv.org/abs/2505.20650v1", "updated": "2025-05-27 02:55:53", "published": "2025-05-27 02:55:53", "authors": "Yan Wang;Yang Ren;Lingfei Qian;Xueqing Peng;Keyi Wang;Yi Han;Dongji Feng;Xiao-Yang Liu;Jimin Huang;Qianqian Xie", "summary": "We introduce FinTagging, the first full-scope, table-aware XBRL benchmark\ndesigned to evaluate the structured information extraction and semantic\nalignment capabilities of large language models (LLMs) in the context of\nXBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL\ntagging as flat multi-class classification and focus solely on narrative text,\nFinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for\nfinancial entity extraction and FinCL for taxonomy-driven concept alignment. It\nrequires models to jointly extract facts and align them with the full 10k+\nUS-GAAP taxonomy across both unstructured text and structured tables, enabling\nrealistic, fine-grained evaluation. We assess a diverse set of LLMs under\nzero-shot settings, systematically analyzing their performance on both subtasks\nand overall tagging accuracy. Our results reveal that, while LLMs demonstrate\nstrong generalization in information extraction, they struggle with\nfine-grained concept alignment, particularly in disambiguating closely related\ntaxonomy entries. These findings highlight the limitations of existing LLMs in\nfully automating XBRL tagging and underscore the need for improved semantic\nreasoning and schema-aware modeling to meet the demands of accurate financial\ndisclosure. Code is available at our GitHub repository and data is at our\nHugging Face repository.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.CE", "links": "http://arxiv.org/abs/2505.20650v1;http://arxiv.org/pdf/2505.20650v1", "pdf_url": "http://arxiv.org/pdf/2505.20650v1"}, {"title": "LLM Performance in Low-Resource Languages: Selecting an Optimal Model for Migrant Integration Support in Greek", "link": "https://www.mdpi.com/1999-5903/17/6/235", "details": "A Tassios, S Tegos, C Bouas, K Manousaridis\u2026 - Future Internet, 2025", "abstract": "The integration of Large Language Models (LLMs) in chatbot applications gains momentum. However, to successfully deploy such systems, the underlying capabilities of LLMs must be carefully considered, especially when dealing with low-resource \u2026"}, {"title": "ALLSTaR: Automated LLM-Driven Scheduler Generation and Testing for Intent-Based RAN", "link": "https://arxiv.org/pdf/2505.18389", "details": "M Elkael, M Polese, R Prasad, S Maxenti, T Melodia - arXiv preprint arXiv:2505.18389, 2025", "abstract": "The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates unprecedented opportunities for Intent-Based Networking (IBN) to dynamically optimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...] \u2026", "entry_id": "http://arxiv.org/abs/2505.18389v2", "updated": "2025-05-27 14:13:53", "published": "2025-05-23 21:33:16", "authors": "Maxime Elkael;Michele Polese;Reshma Prasad;Stefano Maxenti;Tommaso Melodia", "summary": "The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates\nunprecedented opportunities for Intent-Based Networking (IBN) to dynamically\noptimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...]\nremains a significant challenge. Current approaches predominantly rely on\ncoarse-grained network slicing, lacking the granularity for dynamic adaptation\nto individual user conditions and traffic patterns. Despite the existence of a\nvast body of scheduling algorithms [...], their practical utilization is\nhindered by implementation heterogeneity, insufficient systematic evaluation in\nproduction environments, and the complexity of developing high-performance\nscheduler implementations.[...] To address these limitations, we propose\nALLSTaR (Automated LLm-driven Scheduler generation and Testing for intent-based\nRAN), a novel framework leveraging LLMs for automated, intent-driven scheduler\ndesign, implementation, and evaluation. ALLSTaR interprets NL intents,\nautomatically generates functional scheduler code from the research literature\nusing OCR and LLMs, and intelligently matches operator intents to the most\nsuitable scheduler(s). Our implementation deploys these schedulers as O-RAN\ndApps, enabling on-the-fly deployment and testing on a production-grade,\n5G-compliant testbed. This approach has enabled the largest-scale OTA\nexperimental comparison of 18 scheduling algorithms automatically synthesized\nfrom the academic literature. The resulting performance profiles serve as the\ninput for our Intent-Based Scheduling (IBS) framework, which dynamically\nselects and deploys appropriate schedulers that optimally satisfy operator\nintents. We validate our approach through multiple use cases unattainable with\ncurrent slicing-based optimization techniques, demonstrating fine-grained\ncontrol based on buffer status, physical layer conditions, and heterogeneous\ntraffic types", "comment": "Under submission to an IEEE journal, copyright may change without\n  notice", "journal_ref": null, "primary_category": "cs.NI", "categories": "cs.NI", "links": "http://arxiv.org/abs/2505.18389v2;http://arxiv.org/pdf/2505.18389v2", "pdf_url": "http://arxiv.org/pdf/2505.18389v2"}, {"title": "It's Not Just Labeling -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features", "link": "https://arxiv.org/pdf/2505.19419", "details": "B Li, L Powell, T Hammond - arXiv preprint arXiv:2505.19419, 2025", "abstract": "\u2026 To answer the first research question, we first calculated the distance correlation between SR features and each **LLM** - **evaluation** metric \u2026 At first, for each prompting strategy, we examined the distribution and normality of each **LLM** **evaluation** metric \u2026", "entry_id": "http://arxiv.org/abs/2505.19419v2", "updated": "2025-05-27 02:53:28", "published": "2025-05-26 02:13:52", "authors": "Baichuan Li;Larry Powell;Tracy Hammond", "summary": "The quality of training data is critical to the performance of machine\nlearning applications in domains like transportation, healthcare, and robotics.\nAccurate image labeling, however, often relies on time-consuming, expert-driven\nmethods with limited feedback. This research introduces a sketch-based\nannotation approach supported by large language models (LLMs) to reduce\ntechnical barriers and enhance accessibility. Using a synthetic dataset, we\nexamine how sketch recognition features relate to LLM feedback metrics, aiming\nto improve the reliability and interpretability of LLM-assisted labeling. We\nalso explore how prompting strategies and sketch variations influence feedback\nquality. Our main contribution is a sketch-based virtual assistant that\nsimplifies annotation for non-experts and advances LLM-driven labeling tools in\nterms of scalability, accessibility, and explainability.", "comment": null, "journal_ref": null, "primary_category": "cs.HC", "categories": "cs.HC;cs.AI", "links": "http://arxiv.org/abs/2505.19419v2;http://arxiv.org/pdf/2505.19419v2", "pdf_url": "http://arxiv.org/pdf/2505.19419v2"}, {"title": "Safety of LLM-based AI chatbots for young consumers in purchase decisions", "link": "https://www.emerald.com/insight/content/doi/10.1108/yc-10-2024-2277/full/html", "details": "AHT Tan, KC Leo - Young Consumers, 2025", "abstract": "\u2026 By mirroring its guidelines on prompt acquisition, refinement and multi- **LLM** **evaluation** , we provide a comprehensive view of the vulnerabilities that Generation Z consumers may encounter when relying on LLM-based AI chatbots for their \u2026"}, {"title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History", "link": "https://arxiv.org/pdf/2505.21362", "details": "Q Zhong, Z Li, S Fan, A Sun - arXiv preprint arXiv:2505.21362, 2025", "abstract": "Effective engagement by large language models (LLMs) requires adapting responses to users' sociodemographic characteristics, such as age, occupation, and education level. While many real-world applications leverage dialogue history for \u2026", "entry_id": "http://arxiv.org/abs/2505.21362v1", "updated": "2025-05-27 15:52:39", "published": "2025-05-27 15:52:39", "authors": "Qishuai Zhong;Zongmin Li;Siqi Fan;Aixin Sun", "summary": "Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.HC", "links": "http://arxiv.org/abs/2505.21362v1;http://arxiv.org/pdf/2505.21362v1", "pdf_url": "http://arxiv.org/pdf/2505.21362v1"}, {"title": "LLM-QFL: Distilling Large Language Model for Quantum Federated Learning", "link": "https://arxiv.org/pdf/2505.18656", "details": "D Gurung, SR Pokhrel - arXiv preprint arXiv:2505.18656, 2025", "abstract": "\u2026 With an incremental approach, we compute the ratio between the current performance of the device and the loss value of the **LLM** **evaluation** , calculate the increment to align performance, and increment maxiter gradually. In dynamic \u2026", "entry_id": "http://arxiv.org/abs/2505.18656v1", "updated": "2025-05-24 11:49:21", "published": "2025-05-24 11:49:21", "authors": "Dev Gurung;Shiva Raj Pokhrel", "summary": "Inspired by the power of large language models (LLMs), our research adapts\nthem to quantum federated learning (QFL) to boost efficiency and performance.\nWe propose a federated fine-tuning method that distills an LLM within QFL,\nallowing each client to locally adapt the model to its own data while\npreserving privacy and reducing unnecessary global updates. The fine-tuned LLM\nalso acts as a reinforcement agent, optimizing QFL by adjusting optimizer\nsteps, cutting down communication rounds, and intelligently selecting clients.\nExperiments show significant efficiency gains. We pioneer a synergy between LLM\nand QFL, offering: i) practical efficiency: Reduced communication costs and\nfaster convergence. ii) theoretical rigor: Provable guarantees for adaptive\nfederated optimization. iii) scalability: PEFT methods (LoRA, QLoRA) enable\ndeployment on resource-constrained quantum devices. Code implementation is\navailable here 1.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.18656v1;http://arxiv.org/pdf/2505.18656v1", "pdf_url": "http://arxiv.org/pdf/2505.18656v1"}]
