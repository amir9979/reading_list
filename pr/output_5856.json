[{"title": "Prompt Learning with Extended Kalman Filter for Pre-trained Language Models", "link": "https://www.ijcai.org/proceedings/2024/0492.pdf", "details": "Q Li, X Xie, C Wang, SK Zhou", "abstract": "Prompt learning has gained popularity as a means to leverage the knowledge embedded in pre-trained language models (PLMs) for NLP tasks while using a limited number of trainable parameters. While it has shown promise in tasks like \u2026"}, {"title": "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "link": "https://arxiv.org/pdf/2407.21417", "details": "Z Wu, Y Zhang, P Qi, Y Xu, R Han, Y Zhang, J Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern language models (LMs) need to follow human instructions while being faithful; yet, they often fail to achieve both. Here, we provide concrete evidence of a trade-off between instruction following (ie, follow open-ended instructions) and \u2026"}, {"title": "Understanding Defects in Generated Codes by Language Models", "link": "https://arxiv.org/pdf/2408.13372", "details": "AM Esfahani, N Kahani, SA Ajila - arXiv preprint arXiv:2408.13372, 2024", "abstract": "This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the \u2026"}, {"title": "Symbolic Working Memory Enhances Language Models for Complex Rule Application", "link": "https://arxiv.org/pdf/2408.13654", "details": "S Wang, Z Wei, Y Choi, X Ren - arXiv preprint arXiv:2408.13654, 2024", "abstract": "Large Language Models (LLMs) have shown remarkable reasoning performance but struggle with multi-step deductive reasoning involving a series of rule application steps, especially when rules are presented non-sequentially. Our preliminary \u2026"}, {"title": "MobileQuant: Mobile-friendly Quantization for On-device Language Models", "link": "https://arxiv.org/pdf/2408.13933", "details": "F Tan, R Lee, \u0141 Dudziak, SX Hu, S Bhattacharya\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have revolutionized language processing, delivering outstanding results across multiple applications. However, deploying LLMs on edge devices poses several challenges with respect to memory, energy, and compute \u2026"}, {"title": "LLMEmbed: Rethinking Lightweight LLM's Genuine Function in Text Classification", "link": "https://aclanthology.org/2024.acl-long.433.pdf", "details": "CL ChunLiu, H Zhang, K Zhao, X Ju, L Yang - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "With the booming of Large Language Models (LLMs), prompt-learning has become a promising method mainly researched in various research areas. Recently, many attempts based on prompt-learning have been made to improve the performance of \u2026"}, {"title": "Tad: A plug-and-play task-aware decoding method to better adapt llms on downstream tasks", "link": "https://www.ijcai.org/proceedings/2024/0728.pdf", "details": "X Xu, H Chen, Z Lin, J Han, L Gong, G Wang, Y Bao\u2026 - Proceedings of the Thirty \u2026, 2024", "abstract": "Fine-tuning pre-trained models on downstream tasks is a common practice in leveraging large language models (LLMs) today. A critical issue is how to adapt pre- trained models to downstream tasks better, thereby enhancing their performance \u2026"}, {"title": "DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Financial Documents", "link": "https://aclanthology.org/2024.acl-long.852.pdf", "details": "Y Zhao, Y Long, H Liu, R Kamoi, L Nan, L Chen, Y Liu\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely \u2026"}, {"title": "Monotonic Representation of Numeric Attributes in Language Models", "link": "https://aclanthology.org/2024.acl-short.18.pdf", "details": "B Heinzerling, K Inui - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Abstract Language models (LMs) can express factual knowledge involving numeric properties such as Karl Popper was born in 1902. However, how this information is encoded in the model's internal representations is not understood well. Here, we \u2026"}]
