[{"title": "Tool Learning in the Wild: Empowering Language Models as Automatic Tool Agents", "link": "https://openreview.net/pdf%3Fid%3DT4wMdeFEjX", "details": "Z Shi, S Gao, L Yan, Y Feng, X Chen, Z Chen, D Yin\u2026 - THE WEB CONFERENCE 2025", "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a promising approach to extend their utility, enabling them to solve practical tasks. Previous methods manually parse tool documentation and create in-context \u2026"}, {"title": "Topology-of-Question-Decomposition: Enhancing Large Language Models with Information Retrieval for Knowledge-Intensive Tasks", "link": "https://aclanthology.org/2025.coling-main.191.pdf", "details": "W Li, J Wang, LC Yu, X Zhang - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Large language models (LLMs) are increasingly deployed for general problem- solving across various domains yet remain constrained to chaining immediate reasoning steps and depending solely on parametric knowledge. Integrating an \u2026"}, {"title": "FinMoE: A MoE-based Large Chinese Financial Language Model", "link": "https://aclanthology.org/2025.finnlp-1.4.pdf", "details": "X Zhang, Q Yang - Proceedings of the Joint Workshop of the 9th Financial \u2026, 2025", "abstract": "Large-scale language models have demonstrated remarkable success, achieving strong performance across a variety of general tasks. However, when applied to domain-specific fields, such as finance, these models face challenges due to the \u2026"}, {"title": "Assessing and Post-Processing Black Box Large Language Models for Knowledge Editing", "link": "https://openreview.net/pdf%3Fid%3DaGhk1VNcRJ", "details": "X Song, Z Wang, K He, G Dong, Y Mou, J Zhao, W Xu - THE WEB CONFERENCE 2025", "abstract": "The rapid evolution of the Web as a key platform for information dissemination has led to the growing integration of large language models (LLMs) in Web-based applications. However, the swift changes in web content present challenges in \u2026"}, {"title": "Context Is King: Large Language Models' Interpretability in Divergent Knowledge Scenarios", "link": "https://www.mdpi.com/2076-3417/15/3/1192", "details": "A Pi\u00f1eiro-Mart\u00edn, FJ Santos-Criado, C Garc\u00eda-Mateo\u2026 - Applied Sciences, 2025", "abstract": "Large language models (LLMs) have revolutionized the field of artificial intelligence in both academia and industry, transforming how we communicate, search for information, and create content. However, these models face knowledge cutoffs and \u2026"}, {"title": "Improving Contextual Faithfulness of Large Language Models via Retrieval Heads-Induced Optimization", "link": "https://arxiv.org/pdf/2501.13573", "details": "L Huang, X Feng, W Ma, Y Fan, X Feng, Y Ye, W Zhong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Ensuring contextual faithfulness in retrieval-augmented large language models (LLMs) is crucial for building trustworthy information-seeking systems, particularly in long-form question-answering (LFQA) scenarios. In this work, we identify a salient \u2026"}, {"title": "Dual Intention Escape: Jailbreak Attack against Large Language Models", "link": "https://openreview.net/pdf%3Fid%3DV8lBZMzahD", "details": "Y Xue, J Wang, Z Yin, Y Ma, H Qin, R Tao, X Liu - THE WEB CONFERENCE 2025", "abstract": "Recently, the jailbreak attack, which generates adversarial prompts to bypass safety measures and mislead large language models (LLMs) to output harmful answers, has attracted extensive interest due to its potential to reveal the vulnerabilities of \u2026"}, {"title": "Astral: Automated safety testing of large language models", "link": "https://arxiv.org/pdf/2501.17132", "details": "M Ugarte, P Valle, JA Parejo, S Segura, A Arrieta - arXiv preprint arXiv:2501.17132, 2025", "abstract": "Large Language Models (LLMs) have recently gained attention due to their ability to understand and generate sophisticated human-like content. However, ensuring their safety is paramount as they might provide harmful and unsafe responses. Existing \u2026"}, {"title": "CASE-Bench: Context-Aware Safety Evaluation Benchmark for Large Language Models", "link": "https://arxiv.org/pdf/2501.14940", "details": "G Sun, X Zhan, S Feng, PC Woodland, J Such - arXiv preprint arXiv:2501.14940, 2025", "abstract": "Aligning large language models (LLMs) with human values is essential for their safe deployment and widespread adoption. Current LLM safety benchmarks often focus solely on the refusal of individual problematic queries, which overlooks the \u2026"}]
