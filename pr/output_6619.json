[{"title": "BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems", "link": "https://arxiv.org/pdf/2408.15971", "details": "W Wang, D Zhang, T Feng, B Wang, J Tang - arXiv preprint arXiv:2408.15971, 2024", "abstract": "Large Language Models (LLMs) are becoming increasingly powerful and capable of handling complex tasks, eg, building single agents and multi-agent systems. Compared to single agents, multi-agent systems have higher requirements for the \u2026"}, {"title": "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models", "link": "https://arxiv.org/pdf/2408.14866", "details": "H Liu, Y Xie, Y Wang, M Shieh - arXiv preprint arXiv:2408.14866, 2024", "abstract": "Language Language Models (LLMs) face safety concerns due to potential misuse by malicious users. Recent red-teaming efforts have identified adversarial suffixes capable of jailbreaking LLMs using the gradient-based search algorithm Greedy \u2026"}, {"title": "Safety Layers of Aligned Large Language Models: The Key to LLM Security", "link": "https://arxiv.org/pdf/2408.17003", "details": "S Li, L Yao, L Zhang, Y Li - arXiv preprint arXiv:2408.17003, 2024", "abstract": "Aligned LLMs are highly secure, capable of recognizing and refusing to answer malicious questions. However, the role of internal parameters in maintaining this security is not well understood, further these models are vulnerable to security \u2026"}, {"title": "Focused Large Language Models are Stable Many-Shot Learners", "link": "https://arxiv.org/pdf/2408.13987", "details": "P Yuan, S Feng, Y Li, X Wang, Y Zhang, C Tan, B Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL \u2026"}, {"title": "Towards a Unified View of Preference Learning for Large Language Models: A Survey", "link": "https://arxiv.org/pdf/2409.02795", "details": "B Gao, F Song, Y Miao, Z Cai, Z Yang, L Chen, H Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of the crucial factors to achieve success is aligning the LLM's output with human preferences. This alignment process often requires only a small amount of data to \u2026"}, {"title": "AutoTQA: Towards Autonomous Tabular Question Answering through Multi-Agent Large Language Models", "link": "https://www.vldb.org/pvldb/vol17/p3920-zhu.pdf", "details": "JP Zhu, P Cai, K Xu, L Li, Y Sun, S Zhou, H Su, L Tang\u2026", "abstract": "With the growing significance of data analysis, several studies aim to provide precise answers to users' natural language questions from tables, a task referred to as tabular question answering (TQA). The state-of-the-art TQA approaches are limited to \u2026"}, {"title": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.14470", "details": "A Agarwal, SK Ramesh, A Sengupta, T Chakraborty - arXiv preprint arXiv:2408.14470, 2024", "abstract": "Fine-tuning large language models (LLMs) on downstream tasks requires substantial computational resources. A class of parameter-efficient fine-tuning (PEFT) aims to mitigate these computational challenges by selectively fine-tuning only a small \u2026"}, {"title": "An Evaluation Framework for Attributed Information Retrieval using Large Language Models", "link": "https://arxiv.org/pdf/2409.08014", "details": "H Djeddal, P Erbacher, R Toukal, L Soulier\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the growing success of Large Language models (LLMs) in information-seeking scenarios, search engines are now adopting generative approaches to provide answers along with in-line citations as attribution. While existing work focuses mainly \u2026"}, {"title": "Pairwise Proximal Policy Optimization: Language Model Alignment with Comparative RL", "link": "https://openreview.net/pdf%3Fid%3D7iaAlIlV2H", "details": "T Wu, B Zhu, R Zhang, Z Wen, K Ramchandran, J Jiao - First Conference on Language \u2026", "abstract": "LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior is Reinforcement Learning with Human Feedback (RLHF). This involves training a reward model with \u2026"}]
