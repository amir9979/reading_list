[{"title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?", "link": "https://arxiv.org/pdf/2410.23856", "details": "Z Zhou, R Tao, J Zhu, Y Luo, Z Wang, B Han - arXiv preprint arXiv:2410.23856, 2024", "abstract": "This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We \u2026"}, {"title": "Accelerating Blockwise Parallel Language Models with Draft Refinement", "link": "https://openreview.net/pdf%3Fid%3DKT6F5Sw0eg", "details": "T Kim, AT Suresh, KA Papineni, M Riley, S Kumar\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Autoregressive language models have achieved remarkable advancements, yet their potential is often limited by the slow inference speeds associated with sequential token generation. Blockwise parallel decoding (BPD) was proposed by Stern et \u2026"}, {"title": "Mitigating reasoning hallucination through Multi-agent Collaborative Filtering", "link": "https://www.sciencedirect.com/science/article/pii/S0957417424025909", "details": "J Shi, J Zhao, X Wu, R Xu, YH Jiang, L He - Expert Systems with Applications, 2024", "abstract": "Large language models (LLMs) have demonstrated excellent performance in various natural language tasks. However, in practical applications, LLMs frequently exhibit hallucinations, generating content that deviates from instructions or facts, especially \u2026"}, {"title": "Language-Emphasized Cross-Lingual In-Context Learning for Multilingual LLM", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9437-9_26", "details": "J Li, X Wei, X Wang, N Zhuang, L Wang, J Dang - CCF International Conference on \u2026, 2024", "abstract": "With the recent rise of large language models (LLMs), in-context learning (ICL) has shown remarkable performance, eliminating the need for fine-tuning parameters and reducing the reliance on extensive labeled data. However, the intricacies of cross \u2026"}, {"title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models", "link": "https://arxiv.org/pdf/2411.09595", "details": "Z Wang, J Lorraine, Y Wang, H Su, J Zhu, S Fidler\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This work explores expanding the capabilities of large language models (LLMs) pretrained on text to generate 3D meshes within a unified model. This offers key advantages of (1) leveraging spatial knowledge already embedded in LLMs, derived \u2026"}, {"title": "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "link": "https://arxiv.org/pdf/2410.20008", "details": "Z Zhao, Y Ziser, SB Cohen - arXiv preprint arXiv:2410.20008, 2024", "abstract": "Fine-tuning pre-trained large language models (LLMs) on a diverse array of tasks has become a common approach for building models that can solve various natural language processing (NLP) tasks. However, where and to what extent these models \u2026"}, {"title": "Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2410.21728", "details": "K Luo, Z Ding, Z Weng, L Qiao, M Zhao, X Li, D Yin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Chain of Thought (CoT) prompting approaches have significantly consolidated the reasoning capabilities of large language models (LLMs), they still face limitations that require extensive human effort or have performance needs to be improved \u2026"}, {"title": "CLR-Bench: Evaluating Large Language Models in College-level Reasoning", "link": "https://arxiv.org/pdf/2410.17558", "details": "J Dong, Z Hong, Y Bei, F Huang, X Wang, X Huang - arXiv preprint arXiv:2410.17558, 2024", "abstract": "Large language models (LLMs) have demonstrated their remarkable performance across various language understanding tasks. While emerging benchmarks have been proposed to evaluate LLMs in various domains such as mathematics and \u2026"}, {"title": "Pandora's Box: Towards Building Universal Attackers against Real-World Large Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3DgDpWYpocE1", "details": "D Liu, M Yang, X Qu, P Zhou, X Fang, K Tang, Y Wan\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across a wide range of multimodal understanding tasks. Nevertheless, these models are susceptible to adversarial examples. In real-world applications, existing LVLM \u2026"}]
