[{"title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models", "link": "https://arxiv.org/pdf/2504.14395", "details": "B Jalaian, ND Bastian - arXiv preprint arXiv:2504.14395, 2025", "abstract": "To develop trustworthy Vision-Language Models (VLMs), it is essential to address adversarial robustness and hallucination mitigation, both of which impact factual accuracy in high-stakes applications such as defense and healthcare. Existing \u2026"}, {"title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "link": "https://arxiv.org/pdf/2504.14194", "details": "X Zhuang, J Peng, R Ma, Y Wang, T Bai, X Wei, J Qiu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural \u2026"}, {"title": "Exploring Multimodal Language Models for Sustainability Disclosure Extraction: A Comparative Study", "link": "https://aclanthology.org/2025.insights-1.13.pdf", "details": "T Gupta, T Goel, I Verma - The Sixth Workshop on Insights from Negative Results \u2026, 2025", "abstract": "Sustainability metrics have increasingly become a crucial non-financial criterion in investment decision-making. Organizations worldwide are recognizing the importance of sustainability and are proactively highlighting their efforts through \u2026"}, {"title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "link": "https://arxiv.org/pdf/2504.14366", "details": "P Haller, J Golde, A Akbik - arXiv preprint arXiv:2504.14366, 2025", "abstract": "Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures \u2026"}, {"title": "FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models", "link": "https://arxiv.org/pdf/2505.00725", "details": "B Yuan - arXiv preprint arXiv:2505.00725, 2025", "abstract": "Motivated by the emerging demand in the financial industry for the automatic analysis of unstructured and structured data at scale, Question Answering (QA) systems can provide lucrative and competitive advantages to companies by \u2026"}, {"title": "Knowledge-enhanced Parameter-efficient Transfer Learning with METER for medical vision-language tasks", "link": "https://www.sciencedirect.com/science/article/pii/S1532046425000693", "details": "X Liang, J Xie, J Wei, M Zhang, H Zhang - Journal of Biomedical Informatics, 2025", "abstract": "Objective: The full fine-tuning paradigm becomes impractical when applying pre- trained models to downstream tasks due to significant computational and storage costs. Parameter-efficient fine-tuning (PEFT) methods can alleviate the issue \u2026"}, {"title": "Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training", "link": "https://arxiv.org/pdf/2504.13123%3F", "details": "X Zhang, Y Zeng, X Huang, H Hu, R Xie, H Hu, Z Kang - arXiv preprint arXiv \u2026, 2025", "abstract": "In recent years, the field of vision-language model pre-training has experienced rapid advancements, driven primarily by the continuous enhancement of textual capabilities in large language models. However, existing training paradigms for \u2026"}, {"title": "DEANE: Context-Aware Dual-Craft Graph Contrastive Learning for Enhanced Extractive Question Answering", "link": "https://link.springer.com/article/10.1007/s44196-025-00801-y", "details": "D Ye, J Zhou, G Huang - International Journal of Computational Intelligence \u2026, 2025", "abstract": "Abstract Extractive Question Answering (EQA) involves extracting accurate answer spans from a background passage in response to a given question. In recent years, there has been significant interest in leveraging Pre-trained Language Models \u2026"}, {"title": "Can dependency parses facilitate generalization in language models? A case study of cross-lingual relation extraction", "link": "https://aclanthology.org/2025.knowledgenlp-1.28.pdf", "details": "R Dutt, S Sural, C Rose - Proceedings of the 4th International Workshop on \u2026, 2025", "abstract": "In this work, we propose DEPGEN, a framework for evaluating the generalization capabilities of language models on the task of relation extraction, with dependency parses as scaffolds. We use a GNN-based framework that takes dependency parses \u2026"}]
