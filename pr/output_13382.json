[{"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}, {"title": "Self-supervised analogical learning using language models", "link": "https://arxiv.org/pdf/2502.00996", "details": "B Zhou, S Jain, Y Zhang, Q Ning, S Wang, Y Benajiba\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can \u2026"}, {"title": "Dynamic link prediction: Using language models and graph structures for temporal knowledge graph completion with emerging entities and relations", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425002702", "details": "R Ong, J Sun, YK Guo, O Serban - Expert Systems with Applications, 2025", "abstract": "Abstract Knowledge graphs (KGs) represent real-world facts through entities and relations. However, static KGs fail to capture continuously emerging entities and relations over time. Temporal knowledge graphs address this by incorporating time \u2026"}, {"title": "Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More", "link": "https://arxiv.org/pdf/2502.11494", "details": "Z Wen, Y Gao, S Wang, J Zhang, Q Zhang, W Li, C He\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision tokens in multimodal large language models often dominate huge computational overhead due to their excessive length compared to linguistic modality. Abundant recent methods aim to solve this problem with token pruning \u2026"}, {"title": "Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding", "link": "https://arxiv.org/pdf/2502.09906", "details": "TD Truong, HQ Nguyen, XB Nguyen, A Dowling, X Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual \u2026"}, {"title": "Multilingual Language Model Pretraining using Machine-translated Data", "link": "https://arxiv.org/pdf/2502.13252", "details": "J Wang, Y Lu, M Weber, M Ryabinin, D Adelani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-resource languages such as English, enables the pretraining of high-quality large language models (LLMs). The same can not be said for most other languages as LLMs still underperform for non-English languages, likely due to a gap in the \u2026"}, {"title": "AIDE: Agentically Improve Visual Language Model with Domain Experts", "link": "https://arxiv.org/pdf/2502.09051", "details": "MC Chiu, F Liu, K Sapra, A Tao, Y Jacoob, X Ma, Z Yu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The enhancement of Visual Language Models (VLMs) has traditionally relied on knowledge distillation from larger, more capable models. This dependence creates a fundamental bottleneck for improving state-of-the-art systems, particularly when no \u2026"}, {"title": "RetriEVAL: Evaluating Text Generation with Contextualized Lexical Match", "link": "https://dl.acm.org/doi/abs/10.1145/3701551.3703581", "details": "Z Li, X Li, C Tao, J Feng, T Shen, C Xu, H Wang\u2026 - Proceedings of the \u2026, 2025", "abstract": "Pre-trained language models have made significant advancements in text generation tasks. Nevertheless, evaluating the generated text with automatic metrics is still challenging. Compared with supervised metrics, unsupervised metrics which \u2026"}, {"title": "ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language Interactions for Computer-Aided Diagnosis", "link": "https://arxiv.org/pdf/2502.10620", "details": "X Li, X Hou, Z Huang, Y Gan - arXiv preprint arXiv:2502.10620, 2025", "abstract": "Recent advancements in large language models (LLMs) have demonstrated extraordinary comprehension capabilities with remarkable breakthroughs on various vision-language tasks. However, the application of LLMs in generating reliable \u2026"}]
