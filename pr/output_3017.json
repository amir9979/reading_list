[{"title": "Bootstrapping Language Models with DPO Implicit Rewards", "link": "https://arxiv.org/pdf/2406.09760", "details": "C Chen, Z Liu, C Du, T Pang, Q Liu, A Sinha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Human alignment in large language models (LLMs) is an active area of research. A recent groundbreaking work, direct preference optimization (DPO), has greatly simplified the process from past work in reinforcement learning from human feedback \u2026"}, {"title": "From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline", "link": "https://arxiv.org/pdf/2406.11939", "details": "T Li, WL Chiang, E Frick, L Dunlap, T Wu, B Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid evolution of language models has necessitated the development of more challenging benchmarks. Current static benchmarks often struggle to consistently distinguish between the capabilities of different models and fail to align with real \u2026"}, {"title": "Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning", "link": "https://arxiv.org/pdf/2406.12742", "details": "B Zhao, Y Zong, L Zhang, T Hospedales - arXiv preprint arXiv:2406.12742, 2024", "abstract": "The advancement of large language models (LLMs) has significantly broadened the scope of applications in natural language processing, with multi-modal LLMs extending these capabilities to integrate and interpret visual data. However, existing \u2026"}, {"title": "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence", "link": "https://arxiv.org/pdf/2406.10957", "details": "J Lu, J Li, S An, M Zhao, Y He, D Yin, X Sun - arXiv preprint arXiv:2406.10957, 2024", "abstract": "Direct Preference Optimization (DPO) has emerged as a prominent algorithm for the direct and robust alignment of Large Language Models (LLMs) with human preferences, offering a more straightforward alternative to the complex \u2026"}, {"title": "Achieving Sparse Activation in Small Language Models", "link": "https://arxiv.org/pdf/2406.06562", "details": "J Song, K Huang, X Yin, B Yang, W Gao - arXiv e-prints, 2024", "abstract": "Sparse activation, which selectively activates only an input-dependent set of neurons in inference, is a useful technique to reduce the computing cost of Large Language Models (LLMs) without retraining or adaptation efforts. However, whether it can be \u2026"}, {"title": "CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models", "link": "https://arxiv.org/pdf/2406.12257", "details": "Y Li, Z Xu, F Jiang, L Niu, D Sahabandu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The remarkable performance of large language models (LLMs) in generation tasks has enabled practitioners to leverage publicly available models to power custom applications, such as chatbots and virtual assistants. However, the data used to train \u2026"}, {"title": "Detecting hallucinations in large language models using semantic entropy", "link": "https://www.nature.com/articles/s41586-024-07421-0", "details": "S Farquhar, J Kossen, L Kuhn, Y Gal - Nature, 2024", "abstract": "Large language model (LLM) systems, such as ChatGPT or Gemini, can show impressive reasoning and question-answering capabilities but often 'hallucinate'false outputs and unsubstantiated answers,. Answering unreliably or without the \u2026"}, {"title": "RUPBench: Benchmarking Reasoning Under Perturbations for Robustness Evaluation in Large Language Models", "link": "https://arxiv.org/pdf/2406.11020", "details": "Y Wang, Y Zhao - arXiv preprint arXiv:2406.11020, 2024", "abstract": "With the increasing use of large language models (LLMs), ensuring reliable performance in diverse, real-world environments is essential. Despite their remarkable achievements, LLMs often struggle with adversarial inputs, significantly \u2026"}, {"title": "Black-Box Detection of Language Model Watermarks", "link": "https://files.sri.inf.ethz.ch/website/papers/gloaguen2024detectingwatermarks.pdf", "details": "T Gloaguen, N Jovanovic, R Staab, M Vechev", "abstract": "Watermarking has emerged as a promising way to detect LLM-generated text. To apply a watermark an LLM provider, given a secret key, augments generations with a signal that is later detectable by any party with the same key. Recent work has \u2026"}]
