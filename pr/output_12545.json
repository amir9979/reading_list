[{"title": "Self-supervised analogical learning using language models", "link": "https://arxiv.org/pdf/2502.00996", "details": "B Zhou, S Jain, Y Zhang, Q Ning, S Wang, Y Benajiba\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can \u2026"}, {"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}, {"title": "Mordal: Automated Pretrained Model Selection for Vision Language Models", "link": "https://arxiv.org/pdf/2502.00241", "details": "S He, I Jang, M Chowdhury - arXiv preprint arXiv:2502.00241, 2025", "abstract": "Incorporating multiple modalities into large language models (LLMs) is a powerful way to enhance their understanding of non-textual data, enabling them to perform multimodal tasks. Vision language models (VLMs) form the fastest growing category \u2026"}, {"title": "Evaluating Small Language Models for News Summarization: Implications and Factors Influencing Performance", "link": "https://arxiv.org/pdf/2502.00641", "details": "B Xu, Y Chen, Z Wen, W Liu, B He - arXiv preprint arXiv:2502.00641, 2025", "abstract": "The increasing demand for efficient summarization tools in resource-constrained environments highlights the need for effective solutions. While large language models (LLMs) deliver superior summarization quality, their high computational \u2026"}, {"title": "Transfer Learning of Tabular Data by Finetuning Large Language Models", "link": "https://arxiv.org/pdf/2501.06863", "details": "SB Rabbani, I Kowsar, MD Samad - arXiv preprint arXiv:2501.06863, 2025", "abstract": "Despite the artificial intelligence (AI) revolution, deep learning has yet to achieve much success with tabular data due to heterogeneous feature space and limited sample sizes without viable transfer learning. The new era of generative AI, powered \u2026"}, {"title": "A generalist medical language model for disease diagnosis assistance", "link": "https://www.nature.com/articles/s41591-024-03416-6", "details": "X Liu, H Liu, G Yang, Z Jiang, S Cui, Z Zhang, H Wang\u2026 - Nature Medicine, 2025", "abstract": "The delivery of accurate diagnoses is crucial in healthcare and represents the gateway to appropriate and timely treatment. Although recent large language models (LLMs) have demonstrated impressive capabilities in few-shot or zero-shot learning \u2026"}, {"title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2501.14851", "details": "MK Chen, X Zhang, D Tao - arXiv preprint arXiv:2501.14851, 2025", "abstract": "Logical reasoning is a critical component of Large Language Models (LLMs), and substantial research efforts in recent years have aimed to enhance their deductive reasoning capabilities. However, existing deductive reasoning benchmarks, which \u2026"}, {"title": "Calling a Spade a Heart: Gaslighting Multimodal Large Language Models via Negation", "link": "https://arxiv.org/pdf/2501.19017", "details": "B Zhu, Y Gui, J Chen, CW Ngo, EP Lim - arXiv preprint arXiv:2501.19017, 2025", "abstract": "Multimodal Large Language Models (MLLMs) have exhibited remarkable advancements in integrating different modalities, excelling in complex understanding and generation tasks. Despite their success, MLLMs remain vulnerable to \u2026"}]
