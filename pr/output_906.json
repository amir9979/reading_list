'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Adapting transformer-based language models for heart '
[{"title": "Emergent Abilities in Reduced-Scale Generative Language Models", "link": "https://arxiv.org/pdf/2404.02204", "details": "S Muckatira, V Deshpande, V Lialin, A Rumshisky - arXiv preprint arXiv:2404.02204, 2024", "abstract": "Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study \u2026"}, {"title": "Mining Clinical Notes for Physical Rehabilitation Exercise Information: Natural Language Processing Algorithm Development and Validation Study", "link": "https://medinform.jmir.org/2024/1/e52289/", "details": "S Sivarajkumar, F Gao, P Denny, B Aldhahwani\u2026 - JMIR Medical Informatics, 2024", "abstract": "Background: The rehabilitation of a patient who had a stroke requires precise, personalized treatment plans. Natural language processing (NLP) offers the potential to extract valuable exercise information from clinical notes, aiding in the development \u2026"}, {"title": "Question-answering system extracts information on injection drug use from clinical notes", "link": "https://www.nature.com/articles/s43856-024-00470-6", "details": "M Mahbub, I Goethert, I Danciu, K Knight, S Srinivasan\u2026 - Communications Medicine, 2024", "abstract": "Background Injection drug use (IDU) can increase mortality and morbidity. Therefore, identifying IDU early and initiating harm reduction interventions can benefit individuals at risk. However, extracting IDU behaviors from patients' electronic health \u2026"}, {"title": "Large Scale Self-Supervised Pretraining for Active Speaker Detection", "link": "https://ieeexplore.ieee.org/abstract/document/10447899/", "details": "O Braga, W Xia, K Johnson, A Chuang, Y Ye, O Siohan\u2026 - ICASSP 2024-2024 IEEE \u2026, 2024", "abstract": "In this work we investigate the impact of a large-scale self-supervised pretraining strategy for active speaker detection (ASD) on an unlabeled dataset consisting of over 125k hours of YouTube videos. When compared to a baseline trained from \u2026"}, {"title": "Language model-based labeling of German thoracic radiology reports", "link": "https://www.thieme-connect.com/products/ejournals/html/10.1055/a-2287-5054", "details": "A Wollek, P Haitzer, T Sedlmeyr, S Hyska, J Rueckel\u2026 - R\u00f6Fo-Fortschritte auf dem \u2026, 2024", "abstract": "The aim of this study was to explore the potential of weak supervision in a deep learning-based label prediction model. The goal was to use this model to extract labels from German free-text thoracic radiology reports on chest X-ray images and for \u2026"}, {"title": "Context versus Prior Knowledge in Language Models", "link": "https://arxiv.org/pdf/2404.04633", "details": "K Du, V Sn\u00e6bjarnarson, N Stoehr, JC White, A Schein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To answer a question, language models often need to integrate prior knowledge learned during pretraining and new information presented in context. We hypothesize that models perform this integration in a predictable way across different \u2026"}, {"title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI", "link": "https://arxiv.org/pdf/2404.16006", "details": "K Ying, F Meng, J Wang, Z Li, H Lin, Y Yang, H Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal \u2026"}, {"title": "Investigating Regularization of Self-Play Language Models", "link": "https://arxiv.org/pdf/2404.04291", "details": "R Alami, A Abubaker, M Achab, MEA Seddik, S Lahlou - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper explores the effects of various forms of regularization in the context of language model alignment via self-play. While both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) require to collect \u2026"}, {"title": "Scaling Properties of Speech Language Models", "link": "https://arxiv.org/pdf/2404.00685", "details": "S Cuervo, R Marxer - arXiv preprint arXiv:2404.00685, 2024", "abstract": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language \u2026"}]
