[{"title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models", "link": "https://arxiv.org/pdf/2504.12526", "details": "J Zhang, T Zhu, C Luo, A Anandkumar - arXiv preprint arXiv:2504.12526, 2025", "abstract": "Long-context language models exhibit impressive performance but remain challenging to deploy due to high GPU memory demands during inference. We propose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that \u2026"}, {"title": "DICE: A Framework for Dimensional and Contextual Evaluation of Language Models", "link": "https://arxiv.org/pdf/2504.10359", "details": "A Shrivastava, PA Aoyagui - arXiv preprint arXiv:2504.10359, 2025", "abstract": "Language models (LMs) are increasingly being integrated into a wide range of applications, yet the modern evaluation paradigm does not sufficiently reflect how they are actually being used. Current evaluations rely on benchmarks that often lack \u2026"}, {"title": "Beyond Standard MoE: Mixture of Latent Experts for Resource-Efficient Language Models", "link": "https://arxiv.org/pdf/2503.23100", "details": "Z Liu, H Wu, R She, X Fu, X Han, T Zhong, M Yuan - arXiv preprint arXiv:2503.23100, 2025", "abstract": "Mixture of Experts (MoE) has emerged as a pivotal architectural paradigm for efficient scaling of Large Language Models (LLMs), operating through selective activation of parameter subsets for each input token. Nevertheless, conventional MoE \u2026"}, {"title": "A Scalable Framework for Evaluating Health Language Models", "link": "https://arxiv.org/pdf/2503.23339", "details": "N Mallinar, AA Heydari, X Liu, AZ Faranesh, B Winslow\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have emerged as powerful tools for analyzing complex datasets. Recent studies demonstrate their potential to generate useful, personalized responses when provided with patient-specific health information that \u2026"}, {"title": "Code Generation with Small Language Models: A Deep Evaluation on Codeforces", "link": "https://arxiv.org/pdf/2504.07343", "details": "D Souza, R Gheyi, L Albuquerque, G Soares, M Ribeiro - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have demonstrated capabilities in code generation, potentially boosting developer productivity. However, their widespread adoption remains limited by high computational costs, significant energy demands, and \u2026"}, {"title": "Understanding contraceptive switching rationales from real world clinical notes using large language models", "link": "https://www.nature.com/articles/s41746-025-01615-0", "details": "BY Miao, CYK Williams, E Chinedu-Eneh, T Zack\u2026 - npj Digital Medicine, 2025", "abstract": "Understanding reasons for treatment switching is of significant medical interest, but these factors are often only found in unstructured clinical notes and can be difficult to extract. We evaluated the zero-shot abilities of GPT-4 and eight other open-source \u2026"}, {"title": "How Large Language Models Are Changing MOOC Essay Answers: A Comparison of Pre-and Post-LLM Responses", "link": "https://arxiv.org/pdf/2504.13038", "details": "L Lepp\u00e4nen, L Aunimo, A Hellas, JK Nurminen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The release of ChatGPT in late 2022 caused a flurry of activity and concern in the academic and educational communities. Some see the tool's ability to generate human-like text that passes at least cursory inspections for factual accuracy``often \u2026"}, {"title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models", "link": "https://arxiv.org/pdf/2504.10415", "details": "P Shojaee, NH Nguyen, K Meidani, AB Farimani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their \u2026"}, {"title": "Estimating depression severity in narrative clinical notes using large language models", "link": "https://www.sciencedirect.com/science/article/pii/S016503272500566X", "details": "TH McCoy, VM Castro, RH Perlis - Journal of Affective Disorders, 2025", "abstract": "Background Depression treatment guidelines emphasize measurement-based care using patient-reported outcome measures, yet their impact on narrative documentation quality remains underexplored. Methods We sampled 18,000 \u2026"}]
