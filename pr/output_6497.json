[{"title": "Does Knowledge Localization Hold True? Surprising Differences Between Entity and Relation Perspectives in Language Models", "link": "https://arxiv.org/pdf/2409.00617", "details": "Y Wei, X Yu, Y Weng, H Ma, Y Zhang, J Zhao, K Liu - arXiv preprint arXiv:2409.00617, 2024", "abstract": "Large language models encapsulate knowledge and have demonstrated superior performance on various natural language processing tasks. Recent studies have localized this knowledge to specific model parameters, such as the MLP weights in \u2026"}, {"title": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories", "link": "https://arxiv.org/pdf/2409.07440", "details": "B Bogin, K Yang, S Gupta, K Richardson, E Bransom\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping \u2026"}, {"title": "CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks", "link": "https://arxiv.org/pdf/2409.03381", "details": "Y Deng, X Qiu, X Tan, C Qu, J Pan, Y Cheng, Y Xu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Cognitive psychology investigates perception, attention, memory, language, problem- solving, decision-making, and reasoning. Kahneman's dual-system theory elucidates the human decision-making process, distinguishing between the rapid, intuitive \u2026"}, {"title": "Selective Self-Rehearsal: A Fine-Tuning Approach to Improve Generalization in Large Language Models", "link": "https://arxiv.org/pdf/2409.04787", "details": "S Gupta, Y Nandwani, A Yehudai, M Mishra, G Pandey\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task \u2026"}, {"title": "Revolutionizing Database Q&A with Large Language Models: Comprehensive Benchmark and Evaluation", "link": "https://arxiv.org/pdf/2409.04475", "details": "Y Zheng, B Li, Z Lin, Y Luo, X Zhou, C Lin, J Su, G Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The development of Large Language Models (LLMs) has revolutionized Q&A across various industries, including the database domain. However, there is still a lack of a comprehensive benchmark to evaluate the capabilities of different LLMs and their \u2026"}, {"title": "Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback", "link": "https://arxiv.org/pdf/2409.00162", "details": "J Zhou, J Ji, J Dai, Y Yang - arXiv preprint arXiv:2409.00162, 2024", "abstract": "Aligning the behavior of Large language models (LLMs) with human intentions and values remains a critical challenge. Reinforcement learning from human feedback (RLHF) aligns LLMs by training a reward model (RM) on human preferences and fine \u2026"}, {"title": "SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large Language Models", "link": "https://arxiv.org/pdf/2408.15565", "details": "D Yu, B Peng, Y Tian, L Song, H Mi, D Yu - arXiv preprint arXiv:2408.15565, 2024", "abstract": "There is a growing trend of teaching large language models (LLMs) to solve mathematical problems through coding. Existing studies primarily focus on prompting powerful, closed-source models to generate seed training data followed by in \u2026"}, {"title": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models", "link": "https://arxiv.org/pdf/2408.15915", "details": "Y Yang, Y Qin, T Wu, Z Xu, G Li, P Guo, H Shao, Y Shi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The cultivation of expertise for large language models (LLMs) to solve tasks of specific areas often requires special-purpose tuning with calibrated behaviors on the expected stable outputs. To avoid huge cost brought by manual preparation of \u2026"}, {"title": "Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications", "link": "https://arxiv.org/pdf/2409.05314", "details": "A Maatouk, KC Ampudia, R Ying, L Tassiulas - arXiv preprint arXiv:2409.05314, 2024", "abstract": "The emergence of large language models (LLMs) has significantly impacted various fields, from natural language processing to sectors like medicine and finance. However, despite their rapid proliferation, the applications of LLMs in \u2026"}]
