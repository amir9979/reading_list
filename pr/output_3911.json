[{"title": "Comprehensive insights into a decade-long journey: The evolution, impact, and human factors of an asynchronous telemedicine program for diabetic retinopathy \u2026", "link": "https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0305586", "details": "FJ Bonilla-Escobar, AI Ghobrial, DS Gallagher, A Eller\u2026 - PLOS ONE, 2024", "abstract": "Diabetic Retinopathy stands as a leading cause of irreversible blindness, necessitating frequent examinations, especially in the early stages where effective treatments are available. However, current examination rates vary widely, ranging \u2026"}, {"title": "MFC-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models", "link": "https://arxiv.org/pdf/2406.11288", "details": "S Wang, H Lin, Z Luo, Z Ye, G Chen, J Ma - arXiv preprint arXiv:2406.11288, 2024", "abstract": "Large vision-language models (LVLMs) have significantly improved multimodal reasoning tasks, such as visual question answering and image captioning. These models embed multimodal facts within their parameters, rather than relying on \u2026"}, {"title": "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "link": "https://arxiv.org/pdf/2406.10777", "details": "H Wang, T Liu, T Zhao, J Gao - arXiv preprint arXiv:2406.10777, 2024", "abstract": "Pre-trained language models, trained on large-scale corpora, demonstrate strong generalizability across various NLP tasks. Fine-tuning these models for specific tasks typically involves updating all parameters, which is resource-intensive. Parameter \u2026"}, {"title": "Abstraction-of-Thought Makes Language Models Better Reasoners", "link": "https://arxiv.org/pdf/2406.12442", "details": "R Hong, H Zhang, X Pan, D Yu, C Zhang - arXiv preprint arXiv:2406.12442, 2024", "abstract": "Abstract reasoning, the ability to reason from the abstract essence of a problem, serves as a key to generalization in human reasoning. However, eliciting language models to perform reasoning with abstraction remains unexplored. This paper seeks \u2026"}, {"title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models", "link": "https://arxiv.org/pdf/2407.03181", "details": "H Puerto, T Chubakov, X Zhu, HT Madabushi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, it has been found that instruction tuning on these intermediary reasoning steps improves model \u2026"}, {"title": "A Refer-and-Ground Multimodal Large Language Model for Biomedicine", "link": "https://arxiv.org/pdf/2406.18146", "details": "X Huang, H Huang, L Shen, Y Yang, F Shang, J Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the rapid development of multimodal large language models (MLLMs), especially their capabilities in visual chat through refer and ground functionalities, their significance is increasingly recognized. However, the biomedical field currently \u2026"}]
