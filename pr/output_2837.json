[{"title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective", "link": "https://arxiv.org/pdf/2405.16747", "details": "A Tomihari, I Sato - arXiv preprint arXiv:2405.16747, 2024", "abstract": "The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data. This success is largely \u2026"}, {"title": "Exploring Activation Patterns of Parameters in Language Models", "link": "https://arxiv.org/pdf/2405.17799", "details": "Y Wang, D Dai, Z Sui - arXiv preprint arXiv:2405.17799, 2024", "abstract": "Most work treats large language models as black boxes without in-depth understanding of their internal working mechanism. In order to explain the internal representations of LLMs, we propose a gradient-based metric to assess the \u2026"}, {"title": "Demonstration Augmentation for Zero-shot In-context Learning", "link": "https://arxiv.org/pdf/2406.01224", "details": "Y Su, Y Tai, Y Ji, J Li, B Yan, M Zhang - arXiv preprint arXiv:2406.01224, 2024", "abstract": "Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates. However, many studies \u2026"}, {"title": "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark", "link": "https://arxiv.org/pdf/2406.01574", "details": "Y Wang, X Ma, G Zhang, Y Ni, A Chandra, S Guo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse \u2026"}, {"title": "Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction", "link": "https://arxiv.org/pdf/2406.00755", "details": "X Li, W Wang, M Li, J Guo, Y Zhang, F Feng - arXiv preprint arXiv:2406.00755, 2024", "abstract": "The rapid advancement of Large Language Models (LLMs) in the realm of mathematical reasoning necessitates comprehensive evaluations to gauge progress and inspire future directions. Existing assessments predominantly focus on problem \u2026"}, {"title": "Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models", "link": "https://arxiv.org/pdf/2406.02061", "details": "M Nezhurina, L Cipolina-Kun, M Cherti, J Jitsev - arXiv preprint arXiv:2406.02061, 2024", "abstract": "Large Language Models (LLMs) are often described as being instances of foundation models-that is, models that transfer strongly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that \u2026"}, {"title": "Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration", "link": "https://openreview.net/pdf%3Fid%3DDLTjFFiuUJ", "details": "Z Yu, Z Wang, Y Fu, H Shi, K Shaikh, YC Lin - Forty-first International Conference on Machine \u2026", "abstract": "Attention is a fundamental component behind the remarkable achievements of large language models (LLMs). However, our current understanding of the attention mechanism, especially regarding how attention distributions are established \u2026"}, {"title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation", "link": "https://arxiv.org/pdf/2405.13769", "details": "C Chhun, FM Suchanek, C Clavel - arXiv preprint arXiv:2405.13769, 2024", "abstract": "Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high \u2026"}, {"title": "Large Language Models Can Self-Correct with Minimal Effort", "link": "https://arxiv.org/pdf/2405.14092", "details": "Z Wu, Q Zeng, Z Zhang, Z Tan, C Shen, M Jiang - arXiv preprint arXiv:2405.14092, 2024", "abstract": "Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple \u2026"}]
