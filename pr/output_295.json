'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Simple linear attention language models balance the r'
[{"title": "Grounding Language Models for Visual Entity Recognition", "link": "https://arxiv.org/pdf/2402.18695", "details": "Z Xiao, M Gong, P Cascante-Bonilla, X Zhang, J Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of \u2026"}, {"title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.07860", "details": "S Zhao, S Hao, B Zi, H Xu, KYK Wong - arXiv preprint arXiv:2403.07860, 2024", "abstract": "Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding \u2026"}, {"title": "Synth $^ 2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings", "link": "https://arxiv.org/pdf/2403.07750", "details": "S Sharifzadeh, C Kaplanis, S Pathak, D Kumaran, A Ilic\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models \u2026"}, {"title": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models", "link": "https://arxiv.org/html/2402.19427v1", "details": "S De, SL Smith, A Fernando, A Botev\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated \u2026"}, {"title": "BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English", "link": "https://arxiv.org/pdf/2403.10900", "details": "S Shafayat, HM Hasan, MRC Mahim, RA Putri\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this study, we introduce BEnQA, a dataset comprising parallel Bengali and English exam questions for middle and high school levels in Bangladesh. Our dataset consists of approximately 5K questions covering several subjects in science with \u2026"}, {"title": "$\\mathbf {(N, K)} $-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model", "link": "https://arxiv.org/html/2403.07191v1", "details": "Y Zhang, L Chen, B Liu, Y Yang, Q Cui, Y Tao, H Yang - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these \u2026"}, {"title": "Few-Shot Adversarial Prompt Learning on Vision-Language Models", "link": "https://arxiv.org/html/2403.14774v1", "details": "Y Zhou, X Xia, Z Lin, B Han, T Liu - arXiv preprint arXiv:2403.14774, 2024", "abstract": "The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by \u2026"}, {"title": "Multi-modal Attribute Prompting for Vision-Language Models", "link": "https://arxiv.org/pdf/2403.00219", "details": "X Liu, J Wu, T Zhang - arXiv preprint arXiv:2403.00219, 2024", "abstract": "Large pre-trained Vision-Language Models (VLMs), like CLIP, exhibit strong generalization ability to downstream tasks but struggle in few-shot scenarios. Existing prompting techniques primarily focus on global text and image \u2026"}, {"title": "ZVQAF: Zero-shot visual question answering with feedback from large language models", "link": "https://www.sciencedirect.com/science/article/pii/S0925231224002765", "details": "C Liu, C Wang, Y Peng, Z Li - Neurocomputing, 2024", "abstract": "Due to the prominent zero-shot generalization in new language tasks shown by large language models (LLMs), applying LLMs for zero-shot visual question answering (VQA) has been a new trend. However, most prior approaches directly use off-the \u2026"}]
