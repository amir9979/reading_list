[{"title": "Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models", "link": "https://arxiv.org/pdf/2504.12315", "details": "X Ji, J Wang, H Zhang, J Zhang, H Zhou, C Sun, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the development of Multimodal Large Language Models (MLLMs), numerous outstanding accomplishments have emerged within the open-source community. Due to the complexity of creating and training multimodal data pairs, it is still a \u2026"}, {"title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability", "link": "https://arxiv.org/pdf/2504.07416%3F", "details": "J Park, S Kim, B Yoon, K Choi - arXiv preprint arXiv:2504.07416, 2025", "abstract": "Recent advancements in multi-modal models have significantly improved vision- language alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning, rely on low-resolution \u2026"}, {"title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation", "link": "https://arxiv.org/pdf/2504.06861%3F", "details": "D Jagpal, X Chen, VP Namboodiri - arXiv preprint arXiv:2504.06861, 2025", "abstract": "Zero-shot, training-free, image-based text-to-video generation is an emerging area that aims to generate videos using existing image-based diffusion models. Current methods in this space require specific architectural changes to image generation \u2026"}, {"title": "SRG-Net: A Self-supervised 3D Scene Representation Method via Graph Contrastive Learning for Novel View Synthesis", "link": "https://www.jstage.jst.go.jp/article/transfun/advpub/0/advpub_2024EAL2091/_pdf", "details": "Q Qi, Z Liu, Y Guo - IEICE Transactions on Fundamentals of Electronics \u2026, 2025", "abstract": "SUMMARYAccurate scene representation holds practical significance for autonomous driving and virtual reality. This letter proposes a network to optimize images encoding and features learning for better scene representations \u2026"}, {"title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer", "link": "https://arxiv.org/pdf/2504.10462", "details": "W Lei, J Wang, H Wang, X Li, JH Liew, J Feng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained \u2026"}, {"title": "SVLTA: Benchmarking Vision-Language Temporal Alignment via Synthetic Video Situation", "link": "https://arxiv.org/pdf/2504.05925", "details": "H Du, B Wu, Y Lu, Z Mao - arXiv preprint arXiv:2504.05925, 2025", "abstract": "Vision-language temporal alignment is a crucial capability for human dynamic recognition and cognition in real-world scenarios. While existing research focuses on capturing vision-language relevance, it faces limitations due to biased temporal \u2026"}, {"title": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting", "link": "https://arxiv.org/pdf/2504.10486", "details": "Z Jiang, S Wang, S Tang - arXiv preprint arXiv:2504.10486, 2025", "abstract": "Creating relightable and animatable human avatars from monocular videos is a rising research topic with a range of applications, eg virtual reality, sports, and video games. Previous works utilize neural fields together with physically based rendering \u2026"}, {"title": "Probing the Symbolic Logical Reasoning Ability of Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3729238", "details": "J Ji, Z Li, S Xu, W Hua, J Tan, H Gong, Y Zhang - ACM Transactions on Intelligent Systems \u2026", "abstract": "Large Language Models (LLMs) have achieved significant successes in various research domains by learning the relationship between words. However, while these models are capable of making predictions and inferences based on the learned \u2026"}, {"title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs", "link": "https://arxiv.org/pdf/2504.07866%3F", "details": "Y Yin, W Huang, K Song, Y Tang, X Wu, W Guo, P Guo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing \u2026"}]
