'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Prometheus 2: An Open Source Language Model Specialize'
[{"title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "link": "https://arxiv.org/pdf/2405.00402", "details": "L Ranaldi, A Freitas - arXiv preprint arXiv:2405.00402, 2024", "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches \u2026"}, {"title": "DynaMo: Accelerating Language Model Inference with Dynamic Multi-Token Sampling", "link": "https://arxiv.org/pdf/2405.00888", "details": "S Tuli, CH Lin, YC Hsu, NK Jha, Y Shen, H Jin - arXiv preprint arXiv:2405.00888, 2024", "abstract": "Traditional language models operate autoregressively, ie, they predict one token at a time. Rapid explosion in model sizes has resulted in high inference times. In this work, we propose DynaMo, a suite of multi-token prediction language models that \u2026"}]
