[{"title": "Deep Learning Based Segmentation of Blood Vessels from H&E Stained Oesophageal Adenocarcinoma Whole-Slide Images", "link": "https://arxiv.org/pdf/2501.12323", "details": "J Lv, SS Antonowicz, SEA Raza - arXiv preprint arXiv:2501.12323, 2025", "abstract": "Blood vessels (BVs) play a critical role in the Tumor Micro-Environment (TME), potentially influencing cancer progression and treatment response. However, manually quantifying BVs in Hematoxylin and Eosin (H&E) stained images is \u2026"}, {"title": "Nonmydriatic Diabetic Retinopathy Screening in the Federally Qualified Health Center Primary Care Setting: Comparison of Active Outreach Versus Point-of-Care \u2026", "link": "https://muse.jhu.edu/pub/1/article/951586/summary", "details": "AG Hidad, PJ Bryar - Journal of Health Care for the Poor and Underserved, 2025", "abstract": "Purpose. This study evaluates the efficacy of a diabetic eye care coordinator (DECC) conducting non-mydriatic diabetic retinopathy (DR) screening in federally qualified health centers (FQHCs) compared with primary care team screenings at the point-of \u2026"}, {"title": "Pathophysiology and classification of diabetic retinopathy\u2013risk factors for proliferative diabetic retinopathy progression", "link": "https://onlinelibrary.wiley.com/doi/abs/10.1111/aos.16880", "details": "K Pappelis - Acta Ophthalmologica, 2025", "abstract": "Diabetic retinopathy (DR) is the most frequent complication of diabetes mellitus (DM), occurring in at least 1 out of 3 diabetic patients and causing significant visual impairment in approximately 1 out of 100. DR is an important matter of public health \u2026"}, {"title": "Hierarchical Autoregressive Transformers: Combining Byte-and Word-Level Processing for Robust, Adaptable Language Models", "link": "https://arxiv.org/pdf/2501.10322", "details": "P Neitemeier, B Deiseroth, C Eichenberg, L Balles - arXiv preprint arXiv:2501.10322, 2025", "abstract": "Tokenization is a fundamental step in natural language processing, breaking text into units that computational models can process. While learned subword tokenizers have become the de-facto standard, they present challenges such as large \u2026"}, {"title": "CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models", "link": "https://arxiv.org/pdf/2502.01378", "details": "G Chen, Y He, Y Hu, K Yuan, B Yuan - arXiv preprint arXiv:2502.01378, 2025", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory \u2026"}, {"title": "Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding", "link": "https://arxiv.org/pdf/2501.14548%3F", "details": "Z Shui, J Zhang, W Cao, S Wang, R Guo, L Lu, L Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Artificial intelligence (AI) shows great potential in assisting radiologists to improve the efficiency and accuracy of medical image interpretation and diagnosis. However, a versatile AI model requires large-scale data and comprehensive annotations, which \u2026"}, {"title": "Intelligent Legal Assistant: An Interactive Clarification System for Legal Question Answering", "link": "https://arxiv.org/pdf/2502.07904", "details": "R Yao, Y Wu, T Zhang, X Zhang, Y Huang, Y Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The rise of large language models has opened new avenues for users seeking legal advice. However, users often lack professional legal knowledge, which can lead to questions that omit critical information. This deficiency makes it challenging for \u2026"}, {"title": "Efficient Vocabulary Reduction for Small Language Models", "link": "https://aclanthology.org/2025.coling-industry.64.pdf", "details": "Y Nozaki, D Nakashima, R Sato, N Asaba - \u2026 of the 31st International Conference on \u2026, 2025", "abstract": "The increasing size of large language models (LLMs) poses significant challenges due to their high computational costs and energy consumption, making their deployment in industrial settings difficult. Small language models (SLMs) have been \u2026"}, {"title": "EHealth: A Chinese Biomedical Language Model Built via Multi-Level Text Discrimination", "link": "https://ieeexplore.ieee.org/abstract/document/10857372/", "details": "Q Wang, S Dai, B Xu, Y Lyu, H Wu, H Wang - IEEE Transactions on Audio, Speech \u2026, 2025", "abstract": "Pre-trained language models (PLMs) have recently revolutionized the field of natural language processing, impacting not only the general domain but also the biomedical domain. Most previous studies on constructing biomedical PLMs relied simply on \u2026"}]
