[{"title": "GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models", "link": "https://aclanthology.org/anthology-files/pdf/naacl/2025.naacl-long.420.pdf", "details": "H Kohli, S Kumar, H Sun", "abstract": "The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their reasoning to \u2026"}, {"title": "DSQG-Syn: Synthesizing High-quality Data for Text-to-SQL Parsing by Domain Specific Question Generation", "link": "https://aclanthology.org/anthology-files/pdf/findings/2025.findings-naacl.162.pdf", "details": "S Duan, Y Wu, C Liu, Y Zhang, Z Wang, P Han, S Yu\u2026", "abstract": "Synthetic data has recently proven effective in enhancing the accuracy of Text-to- SQL parsers. However, existing methods generate SQL queries first by randomly sampling tables and columns based on probability and then synthesize natural \u2026"}, {"title": "DICE: A Framework for Dimensional and Contextual Evaluation of Language Models", "link": "https://arxiv.org/pdf/2504.10359", "details": "A Shrivastava, PA Aoyagui - arXiv preprint arXiv:2504.10359, 2025", "abstract": "Language models (LMs) are increasingly being integrated into a wide range of applications, yet the modern evaluation paradigm does not sufficiently reflect how they are actually being used. Current evaluations rely on benchmarks that often lack \u2026"}, {"title": "Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2504.13945", "details": "Z Wu, T Song, N Xie, W Zhang, M Zhu, S Wu, S Sun\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The rapid advancement of large vision-language models (LVLMs) has significantly propelled applications in document understanding, particularly in optical character recognition (OCR) and multilingual translation. However, current evaluations of \u2026"}, {"title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models", "link": "https://arxiv.org/pdf/2504.14395", "details": "B Jalaian, ND Bastian - arXiv preprint arXiv:2504.14395, 2025", "abstract": "To develop trustworthy Vision-Language Models (VLMs), it is essential to address adversarial robustness and hallucination mitigation, both of which impact factual accuracy in high-stakes applications such as defense and healthcare. Existing \u2026"}, {"title": "Enhancing Text-to-SQL with Question Classification and Multi-Agent Collaboration", "link": "https://aclanthology.org/anthology-files/pdf/findings/2025.findings-naacl.245.pdf", "details": "Z Shao, S Cai, R Lin, Z Ming", "abstract": "Abstract Large Language Models (LLMs) have recently demonstrated remarkable performance in Textto-SQL tasks. However, existing research primarily focuses on the optimization of prompts and improvements in workflow, with few studies delving \u2026"}, {"title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "link": "https://arxiv.org/pdf/2504.14194", "details": "X Zhuang, J Peng, R Ma, Y Wang, T Bai, X Wei, J Qiu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural \u2026"}, {"title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "link": "https://arxiv.org/pdf/2504.14366", "details": "P Haller, J Golde, A Akbik - arXiv preprint arXiv:2504.14366, 2025", "abstract": "Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures \u2026"}, {"title": "Exploring Multimodal Language Models for Sustainability Disclosure Extraction: A Comparative Study", "link": "https://aclanthology.org/anthology-files/anthology-files/pdf/insights/2025.insights-1.pdf%23page%3D146", "details": "TK Gupta, T Goel, I Verma - The 5th Workshop on Insights from Negative Results in \u2026, 2025", "abstract": "Sustainability metrics have increasingly become a crucial non-financial criterion in investment decision-making. Organizations worldwide are recognizing the importance of sustainability and are proactively highlighting their efforts through \u2026"}]
