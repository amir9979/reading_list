[{"title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners", "link": "https://arxiv.org/pdf/2406.14491", "details": "D Cheng, Y Gu, S Huang, J Bi, M Huang, F Wei - arXiv preprint arXiv:2406.14491, 2024", "abstract": "Unsupervised multitask pre-training has been the critical method behind the recent success of language models (LMs). However, supervised multitask learning still holds significant promise, as scaling it in the post-training stage trends towards better \u2026"}, {"title": "Cross-modal Contrastive Learning with Spatio-temporal Context for Correlation-aware Multi-scale Remote Sensing Image Retrieval", "link": "https://ieeexplore.ieee.org/abstract/document/10568105/", "details": "L Zhu, Y Wang, Y Hu, X Su, K Fu - IEEE Transactions on Geoscience and Remote \u2026, 2024", "abstract": "Optical satellites are the most popular observation platforms for humans viewing Earth. Driven by rapidly developing multi-source optical remote sensing technology, content-based remote sensing image retrieval (CBRSIR), which aims to retrieve \u2026"}, {"title": "Improving Expert Radiology Report Summarization by Prompting Large Language Models with a Layperson Summary", "link": "https://arxiv.org/pdf/2406.14500", "details": "X Zhao, T Wang, A Rios - arXiv preprint arXiv:2406.14500, 2024", "abstract": "Radiology report summarization (RRS) is crucial for patient care, requiring concise\" Impressions\" from detailed\" Findings.\" This paper introduces a novel prompting strategy to enhance RRS by first generating a layperson summary. This approach \u2026"}, {"title": "Protecting Privacy Through Approximating Optimal Parameters for Sequence Unlearning in Language Models", "link": "https://arxiv.org/pdf/2406.14091", "details": "D Lee, D Rim, M Choi, J Choo - arXiv preprint arXiv:2406.14091, 2024", "abstract": "Although language models (LMs) demonstrate exceptional capabilities on various tasks, they are potentially vulnerable to extraction attacks, which represent a significant privacy risk. To mitigate the privacy concerns of LMs, machine unlearning \u2026"}, {"title": "HoTPP Benchmark: Are We Good at the Long Horizon Events Forecasting?", "link": "https://arxiv.org/pdf/2406.14341", "details": "I Karpukhin, F Shipilov, A Savchenko - arXiv preprint arXiv:2406.14341, 2024", "abstract": "In sequential event prediction, which finds applications in finance, retail, social networks, and healthcare, a crucial task is forecasting multiple future events within a specified time horizon. Traditionally, this has been addressed through \u2026"}, {"title": "Evaluating accuracy and fairness of clinical decision support algorithms when health care resources are limited", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424000820", "details": "EL Meerwijk, DC Mcelfresh, S Martins, SR Tamang - Journal of Biomedical Informatics, 2024", "abstract": "Objective Guidance on how to evaluate accuracy and algorithmic fairness across subgroups is missing for clinical models that flag patients for an intervention but when health care resources to administer that intervention are limited. We aimed to \u2026"}, {"title": "Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated Meta-Learning: Convergence-Generalization Trade-Offs", "link": "https://arxiv.org/pdf/2406.11569", "details": "H Wen, H Xing, O Simeone - arXiv preprint arXiv:2406.11569, 2024", "abstract": "For modern artificial intelligence (AI) applications such as large language models (LLMs), the training paradigm has recently shifted to pre-training followed by fine- tuning. Furthermore, owing to dwindling open repositories of data and thanks to \u2026"}, {"title": "Cross-Table Pretraining towards a Universal Function Space for Heterogeneous Tabular Data", "link": "https://arxiv.org/pdf/2406.00281", "details": "J Chen, Z Lin, Q Chen, J Sun - arXiv preprint arXiv:2406.00281, 2024", "abstract": "Tabular data from different tables exhibit significant diversity due to varied definitions and types of features, as well as complex inter-feature and feature-target relationships. Cross-dataset pretraining, which learns reusable patterns from \u2026"}, {"title": "Leveraging Large Language Models for Knowledge-free Weak Supervision in Clinical Natural Language Processing", "link": "https://arxiv.org/pdf/2406.06723", "details": "E Hsu, K Roberts - arXiv preprint arXiv:2406.06723, 2024", "abstract": "The performance of deep learning-based natural language processing systems is based on large amounts of labeled training data which, in the clinical domain, are not easily available or affordable. Weak supervision and in-context learning offer partial \u2026"}]
