[{"title": "ModalTune: Fine-Tuning Slide-Level Foundation Models with Multi-Modal Information for Multi-task Learning in Digital Pathology", "link": "https://arxiv.org/pdf/2503.17564", "details": "V Ramanathan, T Xu, P Pati, F Ahmed, M Goubran\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Prediction tasks in digital pathology are challenging due to the massive size of whole- slide images (WSIs) and the weak nature of training signals. Advances in computing, data availability, and self-supervised learning (SSL) have paved the way for slide \u2026"}, {"title": "Best Vitelliform Macular Dystrophy Natural History Study Report 2: Fundus Autofluorescence and Optical Coherence Tomography.", "link": "https://www.sciencedirect.com/science/article/pii/S2468653025001034", "details": "Y Laich, M Georgiou, K Fujinami, MD Varela\u2026 - Ophthalmology Retina, 2025", "abstract": "Purpose To analyze the retinal imaging findings and natural history of Best vitelliform macular dystrophy (BVMD). Design Single-center retrospective, consecutive, observational study. Participants Patients with a clinical diagnosis of BVMD, from \u2026"}, {"title": "FundusGAN: A Hierarchical Feature-Aware Generative Framework for High-Fidelity Fundus Image Generation", "link": "https://arxiv.org/pdf/2503.17831", "details": "Q Hou, M Wang, P Cao, Z Ke, X Liu, H Fu, OR Zaiane - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in ophthalmology foundation models such as RetFound have demonstrated remarkable diagnostic capabilities but require massive datasets for effective pre-training, creating significant barriers for development and deployment \u2026"}, {"title": "HistoMSC: Density and topology analysis for AI-based visual annotation of histopathology whole slide images", "link": "https://www.sciencedirect.com/science/article/pii/S0010482525003427", "details": "Z Ahmad, K Al-Thelaya, M Alzubaidi, F Joad, NU Gilal\u2026 - Computers in Biology and \u2026, 2025", "abstract": "We introduce an end-to-end framework for the automated visual annotation of histopathology whole slide images. Our method integrates deep learning models to achieve precise localization and classification of cell nuclei with spatial data \u2026"}, {"title": "MedHEval: Benchmarking Hallucinations and Mitigation Strategies in Medical Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.02157", "details": "A Chang, L Huang, P Bhatia, T Kass-Hout, F Ma\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Vision Language Models (LVLMs) are becoming increasingly important in the medical domain, yet Medical LVLMs (Med-LVLMs) frequently generate hallucinations due to limited expertise and the complexity of medical applications \u2026"}, {"title": "CoCa-CXR: Contrastive Captioners Learn Strong Temporal Structures for Chest X-Ray Vision-Language Understanding", "link": "https://arxiv.org/pdf/2502.20509", "details": "Y Chen, S Xu, A Sellergren, Y Matias, A Hassidim\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models have proven to be of great benefit for medical image analysis since they learn rich semantics from both images and reports. Prior efforts have focused on better alignment of image and text representations to enhance \u2026"}, {"title": "Revisiting Automatic Data Curation for Vision Foundation Models in Digital Pathology", "link": "https://arxiv.org/pdf/2503.18709", "details": "B Chen, C Vincent-Cuaz, LA Schoenpflug, M Madeira\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision foundation models (FMs) are accelerating the development of digital pathology algorithms and transforming biomedical research. These models learn, in a self-supervised manner, to represent histological features in highly heterogeneous \u2026"}, {"title": "Open-source framework for detecting bias and overfitting for large pathology images", "link": "https://arxiv.org/pdf/2503.01827%3F", "details": "A Sildnes, N Shvetsov, M Tafavvoghi, VNN Tran\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Even foundational models that are trained on datasets with billions of data samples may develop shortcuts that lead to overfitting and bias. Shortcuts are non-relevant patterns in data, such as the background color or color intensity. So, to ensure the \u2026"}, {"title": "Toward a Large Language Model-Driven Medical Knowledge Retrieval and QA System: Framework Design and Evaluation", "link": "https://www.sciencedirect.com/science/article/pii/S2095809925001080", "details": "Y Liu, X Li, Y Luo, J Du, Y Zhang, T Lv, H Yin, X Tang\u2026 - Engineering, 2025", "abstract": "Recent advancements in large language models (LLMs) have driven remarkable progress in text processing, opening new avenues for medical knowledge discovery. In this study, we present ERQA, a mEdical (E) knowledge Retrieval (R) and Question \u2026"}]
