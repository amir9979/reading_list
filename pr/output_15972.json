[{"title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models", "link": "https://arxiv.org/pdf/2505.00551", "details": "C Zhang, Y Deng, X Lin, B Wang, D Ng, H Ye, X Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The recent development of reasoning language models (RLMs) represents a novel evolution in large language models. In particular, the recent release of DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in the research \u2026"}, {"title": "Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning", "link": "https://arxiv.org/pdf/2505.00024", "details": "S Zhang, Y Dong, J Zhang, J Kautz, B Catanzaro\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Enabling large language models with external tools has become a pivotal strategy for extending their functionality beyond text generation tasks. Prior work typically enhances tool-use abilities by either applying supervised fine-tuning (SFT) to enforce \u2026"}, {"title": "On the generalization of language models from in-context learning and finetuning: a controlled study", "link": "https://arxiv.org/pdf/2505.00661", "details": "AK Lampinen, A Chaudhry, SCY Chan, C Wild, D Wan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning--from failing to generalize to simple reversals of relations they are trained on, to missing logical deductions that can be made from \u2026"}, {"title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation", "link": "https://arxiv.org/pdf/2504.20859", "details": "G Hadad, H Roitman, Y Eshel, B Shapira, L Rokach - arXiv preprint arXiv:2504.20859, 2025", "abstract": "As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents``X-Cross''--a novel cross-domain sequential-recommendation model \u2026"}, {"title": "Consistency in Language Models: Current Landscape, Challenges, and Future Directions", "link": "https://arxiv.org/pdf/2505.00268", "details": "J Novikova, C Anderson, B Blili-Hamelin, S Majumdar - arXiv preprint arXiv \u2026, 2025", "abstract": "The hallmark of effective language use lies in consistency--expressing similar meanings in similar contexts and avoiding contradictions. While human communication naturally demonstrates this principle, state-of-the-art language \u2026"}, {"title": "Foundation Models in Robotics", "link": "https://link.springer.com/chapter/10.1007/979-8-8688-0989-7_4", "details": "A Imran, K Gopalakrishnan - AI for Robotics: Toward Embodied and General \u2026, 2025", "abstract": "This chapter explores how large language models (LLMs) enable robotic planning, control, and mapping through techniques like supervised fine-tuning (SFT) and direct preference optimization (DPO). It covers transformer-based models such as SayCan \u2026"}, {"title": "DeepCritic: Deliberate Critique with Large Language Models", "link": "https://arxiv.org/pdf/2505.00662", "details": "W Yang, J Chen, Y Lin, JR Wen - arXiv preprint arXiv:2505.00662, 2025", "abstract": "As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a \u2026"}, {"title": "Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study", "link": "https://arxiv.org/pdf/2504.16414", "details": "M Khodadad, AS Kasmaee, M Astaraki, N Sherck\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a \u2026"}, {"title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language Models", "link": "https://arxiv.org/pdf/2504.14245", "details": "Y Ji, Y Hong, J Zhan, H Chen, H Zhu, W Wang, L Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Progress in image generation raises significant public security concerns. We argue that fake image detection should not operate as a\" black box\". Instead, an ideal approach must ensure both strong generalization and transparency. Recent \u2026"}]
