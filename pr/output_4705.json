[{"title": "LIONs: An Empirically Optimized Approach to Align Language Models", "link": "https://arxiv.org/pdf/2407.06542", "details": "X Yu, Q Wu, Y Li, Z Yu - arXiv preprint arXiv:2407.06542, 2024", "abstract": "Alignment is a crucial step to enhance the instruction-following and conversational abilities of language models. Despite many recent work proposing new algorithms, datasets, and training pipelines, there is a lack of comprehensive studies measuring \u2026"}, {"title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT", "link": "https://arxiv.org/pdf/2407.21058", "details": "M Ali, S Panda, Q Shen, M Wick, A Kobren - arXiv preprint arXiv:2407.21058, 2024", "abstract": "In the current landscape of language model research, larger models, larger datasets and more compute seems to be the only way to advance towards intelligence. While there have been extensive studies of scaling laws and models' scaling behaviors, the \u2026"}, {"title": "OQA: A question-answering dataset on orthodontic literature", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/07/07/2024.07.05.24309412.full.pdf", "details": "M Rousseau, A Zouaq, N Huynh - medRxiv, 2024", "abstract": "Background The near-exponential increase in the number of publications in orthodontics poses a challenge for efficient literature appraisal and evidence-based practice. Language models (LM) have the potential, through their question \u2026"}, {"title": "Modular Quantitative Temporal Transformer for Biobank-Scale Unified Representations", "link": "https://link.springer.com/chapter/10.1007/978-3-031-66535-6_24", "details": "M Antal, M Marosi, T Nagy, A Millinghoffer, A G\u00e9zsi\u2026 - International Conference on \u2026, 2024", "abstract": "Transformers provide a novel approach for unifying large-scale biobank data spread across different modalities and omic domains. We introduce Modular Quantitative Temporal Transformer (MQTT), a modular architecture for multimodal data that offers \u2026"}, {"title": "WeakAL: Combining Active Learning and Weak Supervision", "link": "https://www.academia.edu/download/79241724/paper.pdf", "details": "W Lehner", "abstract": "Supervised Learning requires a huge amount of labeled data, making efficient labeling one of the most critical components for the success of Machine Learning (ML). One well-known method to gain labeled data efficiently is Active Learning (AL) \u2026"}, {"title": "Zero Shot Health Trajectory Prediction Using Transformer", "link": "https://arxiv.org/pdf/2407.21124", "details": "P Renc, Y Jia, AE Samir, J Was, Q Li, DW Bates\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Integrating modern machine learning and clinical decision-making has great promise for mitigating healthcare's increasing cost and complexity. We introduce the Enhanced Transformer for Health Outcome Simulation (ETHOS), a novel application \u2026"}, {"title": "SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training", "link": "https://arxiv.org/pdf/2407.06654", "details": "N He, W Xiong, H Liu, Y Liao, L Ding, K Zhang, G Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The effectiveness of large language models (LLMs) is often hindered by duplicated data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, which risks the loss of valuable information and \u2026"}, {"title": "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models", "link": "https://arxiv.org/pdf/2407.16470", "details": "K Benkirane, L Gongas, S Pelles, N Fuchs, J Darmon\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting \u2026"}, {"title": "Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning", "link": "https://arxiv.org/pdf/2407.06112", "details": "Y Zhang, S Mao, W Wu, Y Xia, T Ge, M Lan, F Wei - arXiv preprint arXiv:2407.06112, 2024", "abstract": "This paper introduces BI-Directional DEliberation Reasoning (BIDDER), a novel reasoning approach to enhance the decision rationality of language models. Traditional reasoning methods typically rely on historical information and employ uni \u2026"}]
