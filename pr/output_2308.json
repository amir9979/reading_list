[{"title": "CSAMDT: Conditional Self Attention Memory-Driven Transformers for Radiology Report Generation from Chest X-Ray", "link": "https://link.springer.com/article/10.1007/s10278-024-01126-6", "details": "I Shahzadi, TM Madni, UI Janjua, G Batool, B Naz\u2026 - Journal of Imaging \u2026, 2024", "abstract": "A radiology report plays a crucial role in guiding patient treatment, but writing these reports is a time-consuming task that demands a radiologist's expertise. In response to this challenge, researchers in the subfields of artificial intelligence for healthcare \u2026"}, {"title": "Contrastive Learning Via Equivariant Representation", "link": "https://arxiv.org/pdf/2406.00262", "details": "S Song, J Wang, Q Zhao, X Li, D Wu, A Stefanidis, J Su\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Invariant-based Contrastive Learning (ICL) methods have achieved impressive performance across various domains. However, the absence of latent space representation for distortion (augmentation)-related information in the latent space \u2026"}, {"title": "Cross-Dimensional Medical Self-Supervised Representation Learning Based on a Pseudo-3D Transformation", "link": "https://arxiv.org/pdf/2406.00947", "details": "F Gao, S Wang, C Wang, F Zhang, HY Zhou, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical image analysis suffers from a shortage of data, whether annotated or not. This becomes even more pronounced when it comes to 3D medical images. Self- Supervised Learning (SSL) can partially ease this situation by using unlabeled data \u2026"}, {"title": "Complex Style Image Transformations for Domain Generalization in Medical Images", "link": "https://arxiv.org/pdf/2406.00298", "details": "N Spanos, A Arsenos, PA Theofilou, P Tzouveli\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The absence of well-structured large datasets in medical computer vision results in decreased performance of automated systems and, especially, of deep learning models. Domain generalization techniques aim to approach unknown domains from \u2026"}, {"title": "Compute-Efficient Medical Image Classification with Softmax-Free Transformers and Sequence Normalization", "link": "https://arxiv.org/pdf/2406.01314", "details": "F Khader, OSM El Nahhas, T Han, G M\u00fcller-Franzes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The Transformer model has been pivotal in advancing fields such as natural language processing, speech recognition, and computer vision. However, a critical limitation of this model is its quadratic computational and memory complexity relative \u2026"}]
