[{"title": "Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models via Watermarking", "link": "https://arxiv.org/pdf/2503.04636", "details": "Y Xu, A Liu, X Hu, L Wen, H Xiong - arXiv preprint arXiv:2503.04636, 2025", "abstract": "As open-source large language models (LLMs) like Llama3 become more capable, it is crucial to develop watermarking techniques to detect their potential misuse. Existing watermarking methods either add watermarks during LLM inference, which \u2026"}, {"title": "Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation", "link": "https://arxiv.org/pdf/2502.11306", "details": "H Nguyen, Z He, SA Gandre, U Pasupulety\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) often suffer from hallucination, generating factually incorrect or ungrounded content, which limits their reliability in high-stakes applications. A key factor contributing to hallucination is the use of hard labels during \u2026"}, {"title": "HuDEx: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM responses", "link": "https://arxiv.org/pdf/2502.08109", "details": "S Lee, H Lee, S Heo, W Choi - arXiv preprint arXiv:2502.08109, 2025", "abstract": "Recent advances in large language models (LLMs) have shown promising improvements, often surpassing existing methods across a wide range of downstream tasks in natural language processing. However, these models still face \u2026"}, {"title": "ARIES: Stimulating Self-Refinement of Large Language Models by Iterative Preference Optimization", "link": "https://arxiv.org/pdf/2502.05605", "details": "Y Zeng, X Cui, X Jin, G Liu, Z Sun, Q He, D Li, N Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "A truly intelligent Large Language Model (LLM) should be capable of correcting errors in its responses through external interactions. However, even the most advanced models often face challenges in improving their outputs. In this paper, we \u2026"}, {"title": "A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks", "link": "https://arxiv.org/pdf/2502.06470", "details": "HM Nguyen - arXiv preprint arXiv:2502.06470, 2025", "abstract": "Theory of Mind (ToM), the ability to attribute mental states to others and predict their behaviour, is fundamental to social intelligence. In this paper, we survey studies evaluating behavioural and representational ToM in Large Language Models \u2026"}, {"title": "Multi-Agent Verification: Scaling Test-Time Compute with Goal Verifiers", "link": "https://openreview.net/pdf%3Fid%3DH22e93wnMe", "details": "S Lifshitz, SA McIlraith, Y Du - Workshop on Reasoning and Planning for Large \u2026", "abstract": "Scaling test-time computation has recently emerged as a promising direction for improving large language model (LLM) performance. A common approach relies on external verifiers---models or programs that assess solution quality---to select \u2026"}, {"title": "Towards Principled Training and Serving of Large Language Models", "link": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-6.pdf", "details": "B Zhu - 2025", "abstract": "Large Language Models (LLMs) have emerged as a transformative technology in artificial intelligence (AI), demonstrating unprecedented capabilities in tasks ranging from translation and summarization to code generation, complex reasoning, and \u2026"}, {"title": "Optimizing Test-Time Compute via Meta Reinforcement Finetuning", "link": "https://openreview.net/forum%3Fid%3DWGz4ytjo1h", "details": "Y Qu, MYR Yang, A Setlur, L Tunstall, EE Beeching\u2026 - Workshop on Reasoning and \u2026", "abstract": "Training models to efficiently use test-time compute is crucial for improving the reasoning performance of LLMs. While current methods mostly do so via fine-tuning on search traces or running RL against the 0/1 outcome reward, do these \u2026"}]
