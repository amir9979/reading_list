[{"title": "Show, Don't Tell: Aligning Language Models with Demonstrated Feedback", "link": "https://arxiv.org/pdf/2406.00888", "details": "O Shaikh, M Lam, J Hejna, Y Shao, M Bernstein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large \u2026"}, {"title": "Probing Language Models for Pre-training Data Detection", "link": "https://arxiv.org/pdf/2406.01333", "details": "Z Liu, T Zhu, C Tan, H Lu, B Liu, W Chen - arXiv preprint arXiv:2406.01333, 2024", "abstract": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to \u2026"}, {"title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions", "link": "https://arxiv.org/pdf/2405.19744", "details": "C Li, W Yang, J Zhang, J Lu, S Wang, C Zong - arXiv preprint arXiv:2405.19744, 2024", "abstract": "Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples \u2026"}, {"title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning", "link": "https://arxiv.org/pdf/2406.00645", "details": "Y Fu, H Zhang, D Wu, W Xu, B Boulet - arXiv preprint arXiv:2406.00645, 2024", "abstract": "In this work, we investigate how to leverage pre-trained visual-language models (VLM) for online Reinforcement Learning (RL). In particular, we focus on sparse reward tasks with pre-defined textual task descriptions. We first identify the problem \u2026"}, {"title": "Code Pretraining Improves Entity Tracking Abilities of Language Models", "link": "https://arxiv.org/pdf/2405.21068", "details": "N Kim, S Schuster, S Toshniwal - arXiv preprint arXiv:2405.21068, 2024", "abstract": "Recent work has provided indirect evidence that pretraining language models on code improves the ability of models to track state changes of discourse entities expressed in natural language. In this work, we systematically test this claim by \u2026"}, {"title": "Automatic Instruction Evolving for Large Language Models", "link": "https://arxiv.org/pdf/2406.00770", "details": "W Zeng, C Xu, Y Zhao, JG Lou, W Chen - arXiv preprint arXiv:2406.00770, 2024", "abstract": "Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This \u2026"}, {"title": "Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark", "link": "https://arxiv.org/pdf/2405.20574", "details": "C Park, H Kim, D Kim, S Cho, S Kim, S Lee, Y Kim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a \u2026"}, {"title": "Unraveling and Mitigating Retriever Inconsistencies in Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2405.20680", "details": "M Li, X Li, Y Chen, W Xuan, W Zhang - arXiv preprint arXiv:2405.20680, 2024", "abstract": "Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their superiority in terms of factuality, they do not consistently outperform the original retrieval-free Language Models (LMs). Our experiments reveal that this example \u2026"}, {"title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models", "link": "https://arxiv.org/pdf/2406.00548", "details": "T Liu, H Wang, S Wang, Y Cheng, J Gao - arXiv preprint arXiv:2406.00548, 2024", "abstract": "Large language models (LLMs) have achieved impressive performance on various natural language generation tasks. Nonetheless, they suffer from generating negative and harmful contents that are biased against certain demographic groups \u2026"}]
