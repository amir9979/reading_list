[{"title": "Composable Interventions for Language Models", "link": "https://arxiv.org/pdf/2407.06483", "details": "A Kolbeinsson, K O'Brien, T Huang, S Gao, S Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Test-time interventions for language models can enhance factual accuracy, mitigate harmful outputs, and improve model efficiency without costly retraining. But despite a flood of new methods, different types of interventions are largely developing \u2026"}, {"title": "System-1. x: Learning to Balance Fast and Slow Planning with Language Models", "link": "https://arxiv.org/pdf/2407.14414", "details": "S Saha, A Prasad, JCY Chen, P Hase, E Stengel-Eskin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language models can be used to solve long-horizon planning problems in two distinct modes: a fast'System-1'mode, directly generating plans without any explicit search or backtracking, and a slow'System-2'mode, planning step-by-step by \u2026"}, {"title": "Steering Language Models with Game-Theoretic Solvers", "link": "https://openreview.net/pdf%3Fid%3D5QLtIodDmu", "details": "I Gemp, R Patel, Y Bachrach, M Lanctot, V Dasagi\u2026 - Agentic Markets Workshop at ICML \u2026", "abstract": "Mathematical models of strategic interactions among rational agents have long been studied in game theory. However the interactions studied are often over a small set of discrete actions which is very different from how humans communicate in natural \u2026"}, {"title": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation", "link": "https://arxiv.org/pdf/2407.08441", "details": "R Cantini, G Cosenza, A Orsino, D Talia - arXiv preprint arXiv:2407.08441, 2024", "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence, demonstrating remarkable computational power and linguistic capabilities. However, these models are inherently prone to various biases stemming from their training \u2026"}, {"title": "Tuning Language Models with Spatial Logic for Complex Reasoning", "link": "https://openreview.net/pdf%3Fid%3DcDm7zGenhZ", "details": "T Premsri, P Kordjamshidi - The 4th International Combined Workshop on Spatial \u2026", "abstract": "Recent research shows that more data and larger models can provide more accurate solutions to natural language problems that require reasoning. However, models can easily fail to provide solutions in unobserved levels of compositional complexity \u2026"}, {"title": "Patch-Level Training for Large Language Models", "link": "https://arxiv.org/pdf/2407.12665", "details": "C Shao, F Meng, J Zhou - arXiv preprint arXiv:2407.12665, 2024", "abstract": "As Large Language Models (LLMs) achieve remarkable progress in language understanding and generation, their training efficiency has become a critical concern. Traditionally, LLMs are trained to predict the next token in a sequence \u2026"}, {"title": "On the attribution of confidence to large language models", "link": "https://arxiv.org/pdf/2407.08388", "details": "G Keeling, W Street - arXiv preprint arXiv:2407.08388, 2024", "abstract": "Credences are mental states corresponding to degrees of confidence in propositions. Attribution of credences to Large Language Models (LLMs) is commonplace in the empirical literature on LLM evaluation. Yet the theoretical basis \u2026"}, {"title": "ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts", "link": "https://arxiv.org/pdf/2407.09447", "details": "AF Hardy, H Liu, B Lange, MJ Kochenderfer - arXiv preprint arXiv:2407.09447, 2024", "abstract": "Typical schemes for automated red-teaming large language models (LLMs) focus on discovering prompts that trigger a frozen language model (the defender) to generate toxic text. This often results in the prompting model (the adversary) producing text \u2026"}, {"title": "Beyond Instruction Following: Evaluating Rule Following of Large Language Models", "link": "https://arxiv.org/pdf/2407.08440", "details": "W Sun, C Zhang, X Zhang, Z Huang, H Xu, P Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Although Large Language Models (LLMs) have demonstrated strong instruction- following ability to be helpful, they are further supposed to be controlled and guided by rules in real-world scenarios to be safe, and accurate in responses. This demands \u2026"}]
