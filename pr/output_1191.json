'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Mirrored X-Net: Joint Classification and Contrastive Learnin'
[{"title": "Leveraging transformers and large language models with antimicrobial prescribing data to predict sources of infection for electronic health record studies", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/04/19/2024.04.17.24305966.full.pdf", "details": "K Yuan, CH Yoon, Q Gu, H Munby, AS Walker, T Zhu\u2026 - medRxiv, 2024", "abstract": "** Background** Electronic health records frequently contain extensive unstructured free-text data, but extracting information accurately from these data and at scale is challenging. Using free-text from antibiotic prescribing data as an example, we \u2026"}, {"title": "Large Language Models are Inconsistent and Biased Evaluators", "link": "https://arxiv.org/pdf/2405.01724", "details": "R Stureborg, D Alikaniotis, Y Suhara - arXiv preprint arXiv:2405.01724, 2024", "abstract": "The zero-shot capability of Large Language Models (LLMs) has enabled highly flexible, reference-free metrics for various tasks, making LLM evaluators common tools in NLP. However, the robustness of these LLM evaluators remains relatively \u2026"}, {"title": "MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning", "link": "https://arxiv.org/pdf/2405.03000", "details": "W Shi, R Xu, Y Zhuang, Y Yu, H Wu, C Yang, MD Wang - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their improved capabilities in generation and reasoning, adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and corporate privacy. In this work, we propose MedAdapter, a unified \u2026"}, {"title": "Exploring Frequencies via Feature Mixing and Meta-Learning for Improving Adversarial Transferability", "link": "https://arxiv.org/pdf/2405.03193", "details": "J Weng, Z Luo, S Li - arXiv preprint arXiv:2405.03193, 2024", "abstract": "Recent studies have shown that Deep Neural Networks (DNNs) are susceptible to adversarial attacks, with frequency-domain analysis underscoring the significance of high-frequency components in influencing model predictions. Conversely, targeting \u2026"}, {"title": "Enabling Patient-side Disease Prediction via the Integration of Patient Narratives", "link": "https://arxiv.org/pdf/2405.02935", "details": "Z Su, Y Zhang, J Jing, J Xiao, Z Shen - arXiv preprint arXiv:2405.02935, 2024", "abstract": "Disease prediction holds considerable significance in modern healthcare, because of its crucial role in facilitating early intervention and implementing effective prevention measures. However, most recent disease prediction approaches heavily \u2026"}, {"title": "Astro-NER--Astronomy Named Entity Recognition: Is GPT a Good Domain Expert Annotator?", "link": "https://arxiv.org/pdf/2405.02602", "details": "J Evans, S Sadruddin, J D'Souza - arXiv preprint arXiv:2405.02602, 2024", "abstract": "In this study, we address one of the challenges of developing NER models for scholarly domains, namely the scarcity of suitable labeled data. We experiment with an approach using predictions from a fine-tuned LLM model to aid non-domain \u2026"}, {"title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training", "link": "https://arxiv.org/pdf/2405.03133", "details": "Z Zhong, M Xia, D Chen, M Lewis - arXiv preprint arXiv:2405.03133, 2024", "abstract": "Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture, SMEAR, was proposed \u2026"}, {"title": "Check for updates Knowledge Graph Embeddings for Multi-lingual Structured Representations of Radiology Reports", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DUeoEEQAAQBAJ%26oi%3Dfnd%26pg%3DPA84%26ots%3Dit50N3Y-lG%26sig%3DVslrsw2O3aAcfz3p4gDhJnPE7Ds", "details": "T van Sonsbeek, X Zhen, M Worring - \u2026 Labelling, and Imperfections: Third MICCAI Workshop \u2026", "abstract": "The way we analyse clinical texts has undergone major changes over the last years. The introduction of language models such as BERT led to adaptations for the (bio) medical domain like PubMedBERT and ClinicalBERT. These models rely on large \u2026"}]
