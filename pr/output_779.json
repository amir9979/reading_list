'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Emergent Abilities in Reduced-Scale Generative Languag'
[{"title": "Fine-Tuning Language Models with Reward Learning on Policy", "link": "https://arxiv.org/pdf/2403.19279", "details": "H Lang, F Huang, Y Li - arXiv preprint arXiv:2403.19279, 2024", "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences. RLHF contains three steps, ie, human preference collecting, reward learning, and policy \u2026"}, {"title": "Harnessing the Power of Large Vision Language Models for Synthetic Image Detection", "link": "https://arxiv.org/pdf/2404.02726", "details": "M Keita, W Hamidouche, H Bougueffa, A Hadid\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, the emergence of models capable of generating images from text has attracted considerable interest, offering the possibility of creating realistic images from text descriptions. Yet these advances have also raised concerns about the \u2026"}, {"title": "NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation", "link": "https://arxiv.org/pdf/2404.02185", "details": "S Li, H Li, Y Liao, L Yu - arXiv preprint arXiv:2404.02185, 2024", "abstract": "The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene modeling and novel-view synthesis. As a kind of visual media for 3D scene representation, compression with high rate-distortion performance is an eternal \u2026"}, {"title": "Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context", "link": "https://arxiv.org/pdf/2404.02000", "details": "A Caubri\u00e8re, E Gauthier - arXiv preprint arXiv:2404.02000, 2024", "abstract": "We present the first self-supervised multilingual speech model trained exclusively on African speech. The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa. On the SSA \u2026"}, {"title": "NeRF-MAE: Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields", "link": "https://arxiv.org/pdf/2404.01300", "details": "MZ Irshad, S Zakahrov, V Guizilini, A Gaidon, Z Kira\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we \u2026"}, {"title": "Source-Aware Training Enables Knowledge Attribution in Language Models", "link": "https://arxiv.org/pdf/2404.01019", "details": "M Khalifa, D Wadden, E Strubell, H Lee, L Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source (s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining \u2026"}, {"title": "Jamba: A Hybrid Transformer-Mamba Language Model", "link": "https://arxiv.org/pdf/2403.19887.pdf%3Ftrk%3Dpublic_post_comment-text", "details": "O Lieber, B Lenz, H Bata, G Cohen, J Osin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both \u2026"}, {"title": "Scaling Properties of Speech Language Models", "link": "https://arxiv.org/pdf/2404.00685", "details": "S Cuervo, R Marxer - arXiv preprint arXiv:2404.00685, 2024", "abstract": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language \u2026"}, {"title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback", "link": "https://arxiv.org/pdf/2404.00934", "details": "Z Hou, Y Niu, Z Du, X Zhang, X Liu, A Zeng, Q Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline--a reinforcement learning from human feedback (RLHF) system--designed to enhance \u2026"}]
