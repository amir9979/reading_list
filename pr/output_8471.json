[{"title": "Belief in the Machine: Investigating Epistemological Blind Spots of Language Models", "link": "https://arxiv.org/pdf/2410.21195", "details": "M Suzgun, T Gur, F Bianchi, DE Ho, T Icard, D Jurafsky\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As language models (LMs) become integral to fields like healthcare, law, and journalism, their ability to differentiate between fact, belief, and knowledge is essential for reliable decision-making. Failure to grasp these distinctions can lead to \u2026"}, {"title": "MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.17637", "details": "Z Liu, Y Zang, X Dong, P Zhang, Y Cao, H Duan, C He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs. This is typically achieved by using labeled datasets of chosen/rejected pairs and employing \u2026"}, {"title": "ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains", "link": "https://arxiv.org/pdf/2410.09870", "details": "Y Park, C Yoon, J Park, D Lee, M Jeong, J Kang - arXiv preprint arXiv:2410.09870, 2024", "abstract": "Large language models (LLMs) have significantly impacted many aspects of our lives. However, assessing and ensuring their chronological knowledge remains challenging. Existing approaches fall short in addressing the accumulative nature of \u2026"}, {"title": "Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language Models", "link": "https://arxiv.org/pdf/2410.20745%3F", "details": "Y Jin, Z Li, C Zhang, T Cao, Y Gao, P Jayarao, M Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Online shopping is a complex multi-task, few-shot learning problem with a wide and evolving range of entities, relations, and tasks. However, existing models and benchmarks are commonly tailored to specific tasks, falling short of capturing the full \u2026"}, {"title": "CLR-Bench: Evaluating Large Language Models in College-level Reasoning", "link": "https://arxiv.org/pdf/2410.17558", "details": "J Dong, Z Hong, Y Bei, F Huang, X Wang, X Huang - arXiv preprint arXiv:2410.17558, 2024", "abstract": "Large language models (LLMs) have demonstrated their remarkable performance across various language understanding tasks. While emerging benchmarks have been proposed to evaluate LLMs in various domains such as mathematics and \u2026"}, {"title": "SWITCH: Studying with Teacher for Knowledge Distillation of Large Language Models", "link": "https://arxiv.org/pdf/2410.19503", "details": "J Koo, Y Hwang, Y Kim, T Kang, H Bae, K Jung - arXiv preprint arXiv:2410.19503, 2024", "abstract": "Despite the success of Large Language Models (LLMs), they still face challenges related to high inference costs and memory requirements. To address these issues, Knowledge Distillation (KD) has emerged as a popular method for model \u2026"}, {"title": "Flaming-hot Initiation with Regular Execution Sampling for Large Language Models", "link": "https://arxiv.org/pdf/2410.21236", "details": "W Chen, Z Zhang, G Liu, R Zheng, W Shi, C Dun, Z Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Since the release of ChatGPT, large language models (LLMs) have demonstrated remarkable capabilities across various domains. A key challenge in developing these general capabilities is efficiently sourcing diverse, high-quality data. This \u2026"}, {"title": "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "link": "https://arxiv.org/pdf/2410.20008", "details": "Z Zhao, Y Ziser, SB Cohen - arXiv preprint arXiv:2410.20008, 2024", "abstract": "Fine-tuning pre-trained large language models (LLMs) on a diverse array of tasks has become a common approach for building models that can solve various natural language processing (NLP) tasks. However, where and to what extent these models \u2026"}, {"title": "LongReward: Improving Long-context Large Language Models with AI Feedback", "link": "https://arxiv.org/pdf/2410.21252", "details": "J Zhang, Z Hou, X Lv, S Cao, Z Hou, Y Niu, L Hou\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT \u2026"}]
