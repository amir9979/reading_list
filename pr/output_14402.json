[{"title": "Optimizing generative AI by backpropagating language model feedback", "link": "https://www.nature.com/articles/s41586-025-08661-4", "details": "M Yuksekgonul, F Bianchi, J Boen, S Liu, P Lu\u2026 - Nature, 2025", "abstract": "Recent breakthroughs in artificial intelligence (AI) are increasingly driven by systems orchestrating multiple large language models (LLMs) and other specialized tools, such as search engines and simulators. So far, these systems are primarily \u2026"}, {"title": "Optimizing Language Models for Inference Time Objectives using Reinforcement Learning", "link": "https://arxiv.org/pdf/2503.19595", "details": "Y Tang, K Zheng, G Synnaeve, R Munos - arXiv preprint arXiv:2503.19595, 2025", "abstract": "In this work, we investigate the merits of explicitly optimizing for inference time algorithmic performance during model training. We show how optimizing for inference time performance can improve overall model efficacy. We consider generic \u2026"}, {"title": "Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy", "link": "https://arxiv.org/pdf/2503.19757", "details": "Z Hou, T Zhang, Y Xiong, H Duan, H Pu, R Tong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains \u2026"}, {"title": "ASIDE: Architectural Separation of Instructions and Data in Language Models", "link": "https://arxiv.org/pdf/2503.10566", "details": "E Zverev, E Kortukov, A Panfilov, S Tabesh, A Volkova\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their remarkable performance, large language models lack elementary safety features, and this makes them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation \u2026"}, {"title": "Improving Multimodal Large Language Models through Combining Resampler and MLP Projections", "link": "https://ieeexplore.ieee.org/abstract/document/10888340/", "details": "Z Bai, Y Bai - ICASSP 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "Current multimodal large language models (LLMs) achieve impressive performance through connecting visual encoders and LLMs by resampler or MLP projections. Although the MLP projections are effective and widely used in recent works when \u2026"}, {"title": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots", "link": "https://arxiv.org/pdf/2503.14734", "details": "J Bjorck, F Casta\u00f1eda, N Cherniadev, X Da, R Ding\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "General-purpose robots need a versatile body and an intelligent mind. Recent advancements in humanoid robots have shown great promise as a hardware platform for building generalist autonomy in the human world. A robot foundation \u2026"}, {"title": "MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation", "link": "https://arxiv.org/pdf/2503.13446", "details": "Z Wu, Y Zhou, X Xu, Z Wang, H Yan - arXiv preprint arXiv:2503.13446, 2025", "abstract": "Mobile manipulation is the fundamental challenge for robotics to assist humans with diverse tasks and environments in everyday life. However, conventional mobile manipulation approaches often struggle to generalize across different tasks and \u2026"}, {"title": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs", "link": "https://arxiv.org/pdf/2503.05139", "details": "L Team, B Zeng, C Huang, C Zhang, C Tian, C Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this technical report, we tackle the challenges of training large-scale Mixture of Experts (MoE) models, focusing on overcoming cost inefficiency and resource limitations prevalent in such systems. To address these issues, we present two \u2026"}, {"title": "RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete", "link": "https://arxiv.org/pdf/2502.21257", "details": "Y Ji, H Tan, J Shi, X Hao, Y Zhang, H Zhang, P Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have shown remarkable capabilities across various multimodal contexts. However, their application in robotic scenarios, particularly for long-horizon manipulation tasks \u2026"}]
