[{"title": "Effective prompt extraction from language models", "link": "https://openreview.net/pdf%3Fid%3D0o95CVdNuz", "details": "Y Zhang, N Carlini, D Ippolito - First Conference on Language Modeling, 2024", "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden \u2026"}, {"title": "Understanding Defects in Generated Codes by Language Models", "link": "https://arxiv.org/pdf/2408.13372", "details": "AM Esfahani, N Kahani, SA Ajila - arXiv preprint arXiv:2408.13372, 2024", "abstract": "This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the \u2026"}, {"title": "VTPL: Visual and Text Prompt Learning for visual-language models", "link": "https://www.sciencedirect.com/science/article/pii/S1047320324002360", "details": "B Sun, Z Wu, H Zhang, J He - Journal of Visual Communication and Image \u2026, 2024", "abstract": "Visual-language (VL) models have achieved remarkable success in learning combined visual\u2013textual representations from large web datasets. Prompt learning, as a solution for downstream tasks, can address the forgetting of knowledge \u2026"}, {"title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios", "link": "https://aclanthology.org/2024.findings-acl.230.pdf", "details": "L Lin, J Fu, P Liu, Q Li, Y Gong, J Wan, F Zhang\u2026 - Findings of the Association \u2026, 2024", "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local \u2026"}, {"title": "Fast Randomized Low-Rank Adaptation of Pre-trained Language Models with PAC Regularization", "link": "https://aclanthology.org/2024.findings-acl.310.pdf", "details": "Z Lei, D Qian, W Cheung - Findings of the Association for Computational \u2026, 2024", "abstract": "Low-rank adaptation (LoRA) achieves parameter efficient fine-tuning for large language models (LLMs) by decomposing the model weight update into a pair of low- rank projection matrices. Yet, the memory overhead restricts it to scale up when the \u2026"}, {"title": "Teaching Small Language Models to Reason for Knowledge-Intensive Multi-Hop Question Answering", "link": "https://aclanthology.org/2024.findings-acl.464.pdf", "details": "X Li, S He, F Lei, JY JunYang, T Su, K Liu, J Zhao - Findings of the Association for \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) can teach small language models (SLMs) to solve complex reasoning tasks (eg, mathematical question answering) by Chain-of- thought Distillation (CoTD). Specifically, CoTD fine-tunes SLMs by utilizing rationales \u2026"}, {"title": "Synthesis estimators for transportability with positivity violations by a continuous covariate", "link": "https://academic.oup.com/jrsssa/advance-article/doi/10.1093/jrsssa/qnae084/7747432", "details": "PN Zivich, JK Edwards, BE Shook-Sa, ET Lofgren\u2026 - Journal of the Royal \u2026, 2024", "abstract": "Studies intended to estimate the effect of a treatment, like randomized trials, may not be sampled from the desired target population. To correct for this discrepancy, estimates can be transported to the target population. Methods for transporting \u2026"}, {"title": "Improving Extraction of Clinical Event Contextual Properties from Electronic Health Records: A Comparative Study", "link": "https://arxiv.org/pdf/2408.17181", "details": "S Agarwal, T Searle, M Ratas, A Shek, J Teo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Electronic Health Records are large repositories of valuable clinical data, with a significant portion stored in unstructured text format. This textual data includes clinical events (eg, disorders, symptoms, findings, medications and procedures) in \u2026"}, {"title": "Rescue: Ranking llm responses with partial ordering to improve response generation", "link": "https://aclanthology.org/2024.acl-srw.32.pdf", "details": "Y Wang, R Zheng, H Li, Q Zhang, T Gui, F Liu - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Customizing LLMs for a specific task involves separating high-quality responses from lower-quality ones. This skill can be developed using supervised fine-tuning with extensive human preference data. However, obtaining a large volume of expert \u2026"}]
