[{"title": "Alphaedit: Null-space constrained knowledge editing for language models", "link": "https://arxiv.org/pdf/2410.02355%3F", "details": "J Fang, H Jiang, K Wang, Y Ma, X Wang, X He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) often exhibit hallucinations due to incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating \u2026"}, {"title": "Unraveling cross-modality knowledge conflict in large vision-language models", "link": "https://arxiv.org/pdf/2410.03659", "details": "T Zhu, Q Liu, F Wang, Z Tu, M Chen - arXiv preprint arXiv:2410.03659, 2024", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities for capturing and reasoning over multimodal inputs. However, these models are prone to parametric knowledge conflicts, which arise from inconsistencies of \u2026"}, {"title": "Self-Comparison for Dataset-Level Membership Inference in Large (Vision-) Language Models", "link": "https://arxiv.org/pdf/2410.13088", "details": "J Ren, K Chen, C Chen, V Sehwag, Y Xing, J Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have made significant advancements in a wide range of natural language processing and vision- language tasks. Access to large web-scale datasets has been a key factor in their \u2026"}, {"title": "Probing-RAG: Self-Probing to Guide Language Models in Selective Document Retrieval", "link": "https://arxiv.org/pdf/2410.13339", "details": "I Baek, H Chang, B Kim, J Lee, H Lee - arXiv preprint arXiv:2410.13339, 2024", "abstract": "Retrieval-Augmented Generation (RAG) enhances language models by retrieving and incorporating relevant external knowledge. However, traditional retrieve-and- generate processes may not be optimized for real-world scenarios, where queries \u2026"}, {"title": "Improving Instruction-Following in Language Models through Activation Steering", "link": "https://arxiv.org/pdf/2410.12877", "details": "A Stolfo, V Balachandran, S Yousefi, E Horvitz, B Nushi - arXiv preprint arXiv \u2026, 2024", "abstract": "The ability to follow instructions is crucial for numerous real-world applications of language models. In pursuit of deeper insights and more powerful capabilities, we derive instruction-specific vector representations from language models and use \u2026"}, {"title": "Tuning Language Models by Mixture-of-Depths Ensemble", "link": "https://arxiv.org/pdf/2410.13077", "details": "H Luo, L Specia - arXiv preprint arXiv:2410.13077, 2024", "abstract": "Transformer-based Large Language Models (LLMs) traditionally rely on final-layer loss for training and final-layer representations for predictions, potentially overlooking the predictive power embedded in intermediate layers. Surprisingly, we \u2026"}, {"title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition", "link": "https://arxiv.org/pdf/2410.13259", "details": "Q Yang, P Wang, LD Plonsky, FL Oswald, H Chen - arXiv preprint arXiv:2410.13259, 2024", "abstract": "We examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of \u2026"}, {"title": "Mapping Bias in Vision Language Models: Signposts, Pitfalls, and the Road Ahead", "link": "https://arxiv.org/pdf/2410.13146", "details": "K Sasse, S Chen, J Pond, D Bitterman, J Osborne - arXiv preprint arXiv:2410.13146, 2024", "abstract": "As Vision Language Models (VLMs) gain widespread use, their fairness remains under-explored. In this paper, we analyze demographic biases across five models and six datasets. We find that portrait datasets like UTKFace and CelebA are the best \u2026"}, {"title": "PLRTE: Progressive learning for biomedical relation triplet extraction using large language models", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001564", "details": "YK Zheng, B Zeng, YC Feng, L Zhou, YX Li - Journal of Biomedical Informatics, 2024", "abstract": "Document-level relation triplet extraction is crucial in biomedical text mining, aiding in drug discovery and the construction of biomedical knowledge graphs. Current language models face challenges in generalizing to unseen datasets and relation \u2026"}]
