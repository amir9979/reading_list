[{"title": "Differentiable Prompt Learning for Vision Language Models", "link": "https://arxiv.org/pdf/2501.00457", "details": "Z Huang, T Pedapati, PY Chen, J Gao - arXiv preprint arXiv:2501.00457, 2024", "abstract": "Prompt learning is an effective way to exploit the potential of large-scale pre-trained foundational models. Continuous prompts parameterize context tokens in prompts by turning them into differentiable vectors. Deep continuous prompts insert prompts not \u2026"}, {"title": "CARE-SD: classifier-based analysis for recognizing provider stigmatizing and doubt marker labels in electronic health records: model development and validation", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae310/7933303", "details": "A Walker, A Thorne, S Das, J Love, HLF Cooper\u2026 - Journal of the American \u2026, 2024", "abstract": "Objective To detect and classify features of stigmatizing and biased language in intensive care electronic health records (EHRs) using natural language processing techniques. Materials and Methods We first created a lexicon and regular expression \u2026"}, {"title": "Comparing Few-Shot Prompting of GPT-4 LLMs with BERT Classifiers for Open-Response Assessment in Tutor Equity Training", "link": "https://arxiv.org/pdf/2501.06658", "details": "S Kakarla, C Borchers, D Thomas, S Bhushan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Assessing learners in ill-defined domains, such as scenario-based human tutoring training, is an area of limited research. Equity training requires a nuanced understanding of context, but do contemporary large language models (LLMs) have \u2026"}, {"title": "Automated Identification of Breast Cancer Relapse in Computed Tomography Reports Using Natural Language Processing", "link": "https://ascopubs.org/doi/pdfdirect/10.1200/CCI.24.00107", "details": "JJ Lee, A Zepeda, G Arbour, KV Isaac, RT Ng\u2026 - JCO Clinical Cancer \u2026, 2024", "abstract": "PURPOSE Breast cancer relapses are rarely collected by cancer registries because of logistical and financial constraints. Hence, we investigated natural language processing (NLP), enhanced with state-of-the-art deep learning transformer tools and \u2026"}, {"title": "Enhancing few-shot KB-VQA with panoramic image captions guided by Large Language Models", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225000451", "details": "P Qiang, H Tan, X Li, D Wang, R Li, X Sun, H Zhang\u2026 - Neurocomputing, 2025", "abstract": "Current state-of-the-art (SOTA) KB-VQA techniques involve transforming images into image captions as prompts to harness the potent reasoning capabilities of large language models (LLMs) for generating answers. However, generic image captions \u2026"}, {"title": "A Pilot Report on Extracting Symptom Onset Date and Time from Clinical Notes in Patients Presenting with Chest Pain", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/12/31/2024.12.26.24319658.full.pdf", "details": "A George, A Maisa, C Dreisbach, S Suba - medRxiv, 2024", "abstract": "Acute coronary syndrome (ACS) is an acute heart disease that often evolves rapidly. In ACS patients presenting with no-ST-segment elevation (NSTE-ACS), the timing of symptom onset pre-hospital may inform the disease stage and prognosis. We pilot \u2026"}, {"title": "Knowledge Enhanced Language Model for Biomedical Natural Language Processing: Introducing a New Language Model for BioNLP", "link": "https://ieeexplore.ieee.org/abstract/document/10836827/", "details": "U Naseem, Q Zhang, L Hu, S Hussain, S Wang - IEEE Systems, Man, and \u2026, 2025", "abstract": "Following the success of pre-trained language models (PLMs), the biomedical research community has presented various domain-specific PLMs trained on a large biomedical and clinical corpus for biomedical natural language processing (BioNLP) \u2026"}, {"title": "Transfer Learning of Tabular Data by Finetuning Large Language Models", "link": "https://arxiv.org/pdf/2501.06863", "details": "SB Rabbani, I Kowsar, MD Samad - arXiv preprint arXiv:2501.06863, 2025", "abstract": "Despite the artificial intelligence (AI) revolution, deep learning has yet to achieve much success with tabular data due to heterogeneous feature space and limited sample sizes without viable transfer learning. The new era of generative AI, powered \u2026"}, {"title": "Understanding Before Reasoning: Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting", "link": "https://arxiv.org/pdf/2501.04341%3F", "details": "DH Zhu, YJ Xiong, JC Zhang, XJ Xie, CM Xia - arXiv preprint arXiv:2501.04341, 2025", "abstract": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language Models (LLMs) to enhance complex reasoning. It guides LLMs to present multi-step reasoning, rather than generating the final answer directly. However, CoT \u2026"}]
