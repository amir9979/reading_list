[{"title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis", "link": "https://arxiv.org/pdf/2506.02987", "details": "R Armitage - arXiv preprint arXiv:2506.02987, 2025", "abstract": "\u2026 **large** **language** **models** (LLMs) to assist and improve the delivery of clinical **medicine** has been widely discussed. For example, LLMs are able to improve **healthcare** \u2026 and up-skill **healthcare** workforces has been recognised in supporting \u2026", "entry_id": "http://arxiv.org/abs/2506.02987v1", "updated": "2025-06-03 15:25:38", "published": "2025-06-03 15:25:38", "authors": "Richard Armitage", "summary": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata.", "comment": "12 pages, 1 Table", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.HC", "links": "http://arxiv.org/abs/2506.02987v1;http://arxiv.org/pdf/2506.02987v1", "pdf_url": "http://arxiv.org/pdf/2506.02987v1"}, {"title": "Performance evaluation of **large language models** in pediatric nephrology clinical decision support: a comprehensive assessment", "link": "https://link.springer.com/article/10.1007/s00467-025-06819-w", "details": "O Niel, D Dookhun, A Caliment - Pediatric Nephrology, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) have emerged as potential tools in **health** **care** following advancements in artificial intelligence. Despite promising applications across multiple **medical** specialties, limited research exists regarding LLM \u2026"}, {"title": "RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models", "link": "https://arxiv.org/pdf/2506.02726", "details": "Q Yan, X Zhang, L Guo, Q Zhang, F Liu - arXiv preprint arXiv:2506.02726, 2025", "abstract": "\u2026 Current applications and challenges in **large** **language** **models** for patient care: a systematic review. Communications **Medicine** , 5(1):26, \u2026 Rgr-kbqa: Generating **question** **answering** logical forms using knowledge graph enhanced **large** **language** \u2026", "entry_id": "http://arxiv.org/abs/2506.02726v1", "updated": "2025-06-03 10:36:38", "published": "2025-06-03 10:36:38", "authors": "Qihang Yan;Xinyu Zhang;Luming Guo;Qi Zhang;Feifan Liu", "summary": "Large Language Models (LLMs) struggle with accuracy, domain-specific\nreasoning, and interpretability in vertical domains. Traditional preference\nalignment methods like Reinforcement Learning from Human Feedback (RLHF) and\nDirect Preference Optimization (DPO) often overlook the underlying knowledge\nsources and reasoning logic. This paper introduces RACE-Align\n(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel\nframework designed to address these limitations. RACE-Align systematically\nconstructs a binary preference dataset incorporating external knowledge support\nand explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO\nalgorithm. The core innovation lies in its preference data construction\nstrategy: it integrates AI-driven retrieval for factual grounding, enhancing\nknowledgeability and accuracy, and emphasizes the optimization of\ndomain-specific CoT, treating the reasoning process itself as a key preference\ndimension. A multi-stage, AI-driven refinement pipeline cost-effectively\ngenerates these preference pairs. Experimental validation in Traditional\nChinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that\nRACE-Align significantly outperforms the original base model and a model\nfine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed\nacross multiple dimensions, including answer accuracy, information richness,\napplication of TCM thinking patterns, logicality and depth of reasoning, and\ninterpretability. These findings suggest RACE-Align offers an effective pathway\nto enhance LLMs' knowledge application, reasoning reliability, and process\ntransparency in complex vertical domains.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.LG;I.2.7; I.2.6; H.3.3", "links": "http://arxiv.org/abs/2506.02726v1;http://arxiv.org/pdf/2506.02726v1", "pdf_url": "http://arxiv.org/pdf/2506.02726v1"}, {"title": "MIMO: A **Medical** Vision Language Model with Visual Referring Multimodal Input and Pixel Grounding Multimodal Output", "link": "https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_MIMO_A_Medical_Vision_Language_Model_with_Visual_Referring_Multimodal_CVPR_2025_paper.pdf", "details": "Y Chen, D Xu, Y Huang, S Zhan, H Wang, D Chen\u2026 - Proceedings of the \u2026, 2025", "abstract": "\u2026 Currently, **medical** vision language models are widely used in **medical** vision **question** **answering** tasks. However, existing models are \u2026 Currently, **medical** vision-language models (MVLMs) equip **large** **language** **models** (LLMs) with visual perception \u2026"}, {"title": "PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain", "link": "https://arxiv.org/pdf/2506.00250", "details": "MJR Kalahroodi, A Sheikholselami, S Karimi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) have achieved remarkable performance on a wide range of NLP benchmarks, often surpassing human-\u2026 (on the **answered** subset) and coverage (the fraction of **questions** **answered** ). Aggregating across all \u2026", "entry_id": "http://arxiv.org/abs/2506.00250v2", "updated": "2025-06-03 00:22:37", "published": "2025-05-30 21:34:30", "authors": "Mohammad Javad Ranjbar Kalahroodi;Amirhossein Sheikholselami;Sepehr Karimi;Sepideh Ranjbar Kalahroodi;Heshaam Faili;Azadeh Shakery", "summary": "Large Language Models (LLMs) have achieved remarkable performance on a wide\nrange of NLP benchmarks, often surpassing human-level accuracy. However, their\nreliability in high-stakes domains such as medicine, particularly in\nlow-resource languages, remains underexplored. In this work, we introduce\nPersianMedQA, a large-scale, expert-validated dataset of multiple-choice\nPersian medical questions, designed to evaluate LLMs across both Persian and\nEnglish. We benchmark over 40 state-of-the-art models, including\ngeneral-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and\nchain-of-thought (CoT) settings. Our results show that closed-source general\nmodels (e.g., GPT-4.1) consistently outperform all other categories, achieving\n83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models\nsuch as Dorna underperform significantly (e.g., 35.9% in Persian), often\nstruggling with both instruction-following and domain reasoning. We also\nanalyze the impact of translation, showing that while English performance is\ngenerally higher, Persian responses are sometimes more accurate due to cultural\nand clinical contextual cues. Finally, we demonstrate that model size alone is\ninsufficient for robust performance without strong domain or language\nadaptation. PersianMedQA provides a foundation for evaluating multilingual and\nculturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be\naccessed at: https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.IT;math.IT", "links": "http://arxiv.org/abs/2506.00250v2;http://arxiv.org/pdf/2506.00250v2", "pdf_url": "http://arxiv.org/pdf/2506.00250v2"}, {"title": "VM14K: First Vietnamese Medical Benchmark", "link": "https://arxiv.org/pdf/2506.01305", "details": "T Nguyen, D Nguyen, M Dang, T Dao, L Nguyen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 The rise of **Large** **Language** **Models** (LLMs) in **healthcare** has become more prominent than ever, projecting tremendous impacts across \u2026 Medmcqa: A large-scale multi-subject multi-choice dataset for **medical** domain **question** **answering**. In \u2026", "entry_id": "http://arxiv.org/abs/2506.01305v1", "updated": "2025-06-02 04:32:15", "published": "2025-06-02 04:32:15", "authors": "Thong Nguyen;Duc Nguyen;Minh Dang;Thai Dao;Long Nguyen;Quan H. Nguyen;Dat Nguyen;Kien Tran;Minh Tran", "summary": "Medical benchmarks are indispensable for evaluating the capabilities of\nlanguage models in healthcare for non-English-speaking communities,therefore\nhelp ensuring the quality of real-life applications. However, not every\ncommunity has sufficient resources and standardized methods to effectively\nbuild and design such benchmark, and available non-English medical data is\nnormally fragmented and difficult to verify. We developed an approach to tackle\nthis problem and applied it to create the first Vietnamese medical question\nbenchmark, featuring 14,000 multiple-choice questions across 34 medical\nspecialties. Our benchmark was constructed using various verifiable sources,\nincluding carefully curated medical exams and clinical records, and eventually\nannotated by medical experts. The benchmark includes four difficulty levels,\nranging from foundational biological knowledge commonly found in textbooks to\ntypical clinical case studies that require advanced reasoning. This design\nenables assessment of both the breadth and depth of language models' medical\nunderstanding in the target language thanks to its extensive coverage and\nin-depth subject-specific expertise. We release the benchmark in three parts: a\nsample public set (4k questions), a full public set (10k questions), and a\nprivate set (2k questions) used for leaderboard evaluation. Each set contains\nall medical subfields and difficulty levels. Our approach is scalable to other\nlanguages, and we open-source our data construction pipeline to support the\ndevelopment of future multilingual benchmarks in the medical domain.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.01305v1;http://arxiv.org/pdf/2506.01305v1", "pdf_url": "http://arxiv.org/pdf/2506.01305v1"}, {"title": "MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine", "link": "https://arxiv.org/pdf/2506.01252", "details": "S Kong, X Yang, Y Wei, Z Wang, H Tang, J Qin, S Lan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **medicine**. Yet, their systematic evaluation in the TCM domain remains underdeveloped. Existing benchmarks either focus narrowly on factual **question** **answering** or \u2026 To comprehensively evaluate the performance of **large** **language** \u2026", "entry_id": "http://arxiv.org/abs/2506.01252v1", "updated": "2025-06-02 02:01:40", "published": "2025-06-02 02:01:40", "authors": "Shufeng Kong;Xingru Yang;Yuanyuan Wei;Zijie Wang;Hao Tang;Jiuqi Qin;Shuting Lan;Yingheng Wang;Junwen Bai;Zhuangbin Chen;Zibin Zheng;Caihua Liu;Hao Liang", "summary": "Traditional Chinese Medicine (TCM) is a holistic medical system with\nmillennia of accumulated clinical experience, playing a vital role in global\nhealthcare-particularly across East Asia. However, the implicit reasoning,\ndiverse textual forms, and lack of standardization in TCM pose major challenges\nfor computational modeling and evaluation. Large Language Models (LLMs) have\ndemonstrated remarkable potential in processing natural language across diverse\ndomains, including general medicine. Yet, their systematic evaluation in the\nTCM domain remains underdeveloped. Existing benchmarks either focus narrowly on\nfactual question answering or lack domain-specific tasks and clinical realism.\nTo fill this gap, we introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs\non TCM Knowledge, Reasoning, and Safety. Developed in collaboration with\ncertified TCM experts, MTCMB comprises 12 sub-datasets spanning five major\ncategories: knowledge QA, language understanding, diagnostic reasoning,\nprescription generation, and safety evaluation. The benchmark integrates\nreal-world case records, national licensing exams, and classical texts,\nproviding an authentic and comprehensive testbed for TCM-capable models.\nPreliminary results indicate that current LLMs perform well on foundational\nknowledge but fall short in clinical reasoning, prescription planning, and\nsafety compliance. These findings highlight the urgent need for domain-aligned\nbenchmarks like MTCMB to guide the development of more competent and\ntrustworthy medical AI systems. All datasets, code, and evaluation tools are\npublicly available at: https://github.com/Wayyuanyuan/MTCMB.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.01252v1;http://arxiv.org/pdf/2506.01252v1", "pdf_url": "http://arxiv.org/pdf/2506.01252v1"}, {"title": "ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists", "link": "https://arxiv.org/pdf/2506.01241", "details": "J Ruan, I Nair, S Cao, A Liu, S Munir\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Beyond **question** **answering** , the application-driven tasks in EXPERTLONGBENCH demand \u2026 We benchmark 11 **large** **language** **models** (LLMs) and analyze components in CLEAR, \u2026 [1], legal case summarization [2], and \u2026", "entry_id": "http://arxiv.org/abs/2506.01241v1", "updated": "2025-06-02 01:39:02", "published": "2025-06-02 01:39:02", "authors": "Jie Ruan;Inderjeet Nair;Shuyang Cao;Amy Liu;Sheza Munir;Micah Pollens-Dempsey;Tiffany Chiang;Lucy Kates;Nicholas David;Sihan Chen;Ruxin Yang;Yuqian Yang;Jasmine Gump;Tessa Bialek;Vivek Sankaran;Margo Schlanger;Lu Wang", "summary": "This paper introduces ExpertLongBench, an expert-level benchmark containing\n11 tasks from 9 domains that reflect realistic expert workflows and\napplications. Beyond question answering, the application-driven tasks in\nExpertLongBench demand long-form outputs that can exceed 5,000 tokens and\nstrict adherence to domain-specific requirements. Notably, each task in\nExpertLongBench includes a rubric, designed or validated by domain experts, to\nspecify task requirements and guide output evaluation. Furthermore, we propose\nCLEAR, an evaluation framework that supports accurate evaluation of long-form\nmodel outputs in our benchmark. To achieve fine-grained, expert-aligned\nevaluation, CLEAR derives checklists from both model outputs and references by\nextracting information corresponding to items in the task-specific rubric.\nChecklist items for model outputs are then compared with corresponding items\nfor reference outputs to assess their correctness, enabling grounded\nevaluation. We benchmark 11 large language models (LLMs) and analyze components\nin CLEAR, showing that (1) existing LLMs, with the top performer achieving only\na 26.8% F1 score, require significant improvement for expert-level tasks; (2)\nmodels can generate content corresponding to the required aspects, though often\nnot accurately; and (3) accurate checklist extraction and comparison in CLEAR\ncan be achieved by open-weight models for more scalable and low-cost usage.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.01241v1;http://arxiv.org/pdf/2506.01241v1", "pdf_url": "http://arxiv.org/pdf/2506.01241v1"}, {"title": "Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs", "link": "https://arxiv.org/pdf/2506.00072", "details": "N Naderi, Z Atf, PR Lewis, SAA Safavi-Naini, A Soroush - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 This paper investigates the efficacy of prompt engineering techniques in enhancing both the accuracy and confidence elicitation of **Large** **Language** **Models** (LLMs) when applied to high-stakes **medical** contexts. A stratified dataset of Persian board \u2026", "entry_id": "http://arxiv.org/abs/2506.00072v1", "updated": "2025-05-29 17:13:26", "published": "2025-05-29 17:13:26", "authors": "Nariman Naderi;Zahra Atf;Peter R Lewis;Aref Mahjoub far;Seyed Amir Ahmad Safavi-Naini;Ali Soroush", "summary": "This paper investigates how prompt engineering techniques impact both\naccuracy and confidence elicitation in Large Language Models (LLMs) applied to\nmedical contexts. Using a stratified dataset of Persian board exam questions\nacross multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini,\nLlama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These\nconfigurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles\n(Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales\n(1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error\n(ECE) to evaluate alignment between confidence and actual performance.\nChain-of-Thought prompts improved accuracy but also led to overconfidence,\nhighlighting the need for calibration. Emotional prompting further inflated\nconfidence, risking poor decisions. Smaller models like Llama-3.1-8b\nunderperformed across all metrics, while proprietary models showed higher\naccuracy but still lacked calibrated confidence. These results suggest prompt\nengineering must address both accuracy and uncertainty to be effective in\nhigh-stakes medical tasks.", "comment": "This paper was accepted for presentation at the 7th International\n  Workshop on EXplainable, Trustworthy, and Responsible AI and Multi-Agent\n  Systems (EXTRAAMAS 2025). Workshop website:\n  https://extraamas.ehealth.hevs.ch/index.html", "journal_ref": null, "primary_category": "cs.CY", "categories": "cs.CY;cs.AI;cs.CL;cs.LG", "links": "http://arxiv.org/abs/2506.00072v1;http://arxiv.org/pdf/2506.00072v1", "pdf_url": "http://arxiv.org/pdf/2506.00072v1"}]
