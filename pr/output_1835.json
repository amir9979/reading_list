[{"title": "EHRMamba: Towards Generalizable and Scalable Foundation Models for Electronic Health Records", "link": "https://arxiv.org/pdf/2405.14567", "details": "A Fallahpour, M Alinoori, A Afkanpour, A Krishnan - arXiv preprint arXiv:2405.14567, 2024", "abstract": "Transformers have significantly advanced the modeling of Electronic Health Records (EHR), yet their deployment in real-world healthcare is limited by several key challenges. Firstly, the quadratic computational cost and insufficient context length of \u2026"}, {"title": "Multi-domain Knowledge Graph Collaborative Pre-training and Prompt Tuning for Diverse Downstream Tasks", "link": "https://arxiv.org/pdf/2405.13085", "details": "Y Zhang, B Hu, Z Chen, L Guo, Z Liu, Z Zhang, L Liang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Knowledge graphs (KGs) provide reliable external knowledge for a wide variety of AI tasks in the form of structured triples. Knowledge graph pre-training (KGP) aims to pre-train neural networks on large-scale KGs and provide unified interfaces to \u2026"}, {"title": "The Impact of Common Variations in Sequential Organ Failure Assessment Score Calculation on Sepsis Measurement Using Sepsis-3 Criteria: A Retrospective \u2026", "link": "https://journals.lww.com/ccmjournal/fulltext/9900/the_impact_of_common_variations_in_sequential.337.aspx", "details": "M Alrawashdeh, M Klompas, C Rhee - Critical Care Medicine, 2024", "abstract": "Objectives: To assess the impact of different methods of calculating Sequential Organ Failure Assessment (SOFA) scores using electronic health record data on the incidence, outcomes, agreement, and predictive validity of Sepsis-3 criteria. Design \u2026"}, {"title": "LFDe: A Lighter, Faster and More Data-Efficient Pre-training Framework for Event Extraction", "link": "https://dl.acm.org/doi/abs/10.1145/3589334.3645318", "details": "Z Kan, L Peng, Y Gao, N Liu, L Qiao, D Li - Proceedings of the ACM on Web \u2026, 2024", "abstract": "Pre-training Event Extraction (EE) models on unlabeled data is an effective strategy that frees researchers from costly and labor-intensive data annotation. However, existing pre-training methods necessitate substantial computational resources \u2026"}, {"title": "Text-Free Multi-domain Graph Pre-training: Toward Graph Foundation Models", "link": "https://arxiv.org/pdf/2405.13934", "details": "X Yu, C Zhou, Y Fang, X Zhang - arXiv preprint arXiv:2405.13934, 2024", "abstract": "Given the ubiquity of graph data, it is intriguing to ask: Is it possible to train a graph foundation model on a broad range of graph data across diverse domains? A major hurdle toward this goal lies in the fact that graphs from different domains often exhibit \u2026"}, {"title": "Towards Cross-modal Backward-compatible Representation Learning for Vision-Language Models", "link": "https://arxiv.org/pdf/2405.14715", "details": "YK Jang, S Lim - arXiv preprint arXiv:2405.14715, 2024", "abstract": "Modern retrieval systems often struggle with upgrading to new and more powerful models due to the incompatibility of embeddings between the old and new models. This necessitates a costly process known as backfilling, which involves re-computing \u2026"}, {"title": "LiBERTa: Advancing Ukrainian Language Modeling through Pre-training from Scratch", "link": "https://aclanthology.org/2024.unlp-1.14.pdf", "details": "M Haltiuk, A Smywi\u0144ski-Pohl - Proceedings of the Third Ukrainian Natural Language \u2026, 2024", "abstract": "Abstract Recent advancements in Natural Language Processing (NLP) have spurred remarkable progress in language modeling, predominantly benefiting English. While Ukrainian NLP has long grappled with significant challenges due to limited data and \u2026"}, {"title": "Benchmarking Benchmark Leakage in Large Language Models", "link": "https://arxiv.org/pdf/2404.18824%3Ftrk%3Dpublic_post_comment-text", "details": "R Xu, Z Wang, RZ Fan, P Liu - arXiv preprint arXiv:2404.18824, 2024", "abstract": "Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary \u2026"}, {"title": "HFT: Half Fine-Tuning for Large Language Models", "link": "https://arxiv.org/pdf/2404.18466", "details": "T Hui, Z Zhang, S Wang, W Xu, Y Sun, H Wu - arXiv preprint arXiv:2404.18466, 2024", "abstract": "Large language models (LLMs) with one or more fine-tuning phases have become a necessary step to unlock various capabilities, enabling LLMs to follow natural language instructions or align with human preferences. However, it carries the risk of \u2026"}]
