[{"title": "Inter-structure and intra-semantics graph contrastive learning for disease prediction", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124006932", "details": "Y Kang, J Zheng, M Yang, N An - Knowledge-Based Systems, 2024", "abstract": "Ever-evolving healthcare applications have witnessed a surge in the utilization of electronic health records (EHR) for predicting future patient diagnoses. While Graph Neural Networks have demonstrated that promise in modeling disease-patient \u2026"}, {"title": "Timo: Towards Better Temporal Reasoning for Language Models", "link": "https://arxiv.org/pdf/2406.14192", "details": "Z Su, J Zhang, T Zhu, X Qu, J Li, M Zhang, Y Cheng - arXiv preprint arXiv:2406.14192, 2024", "abstract": "Reasoning about time is essential for Large Language Models (LLMs) to understand the world. Previous works focus on solving specific tasks, primarily on time-sensitive question answering. While these methods have proven effective, they cannot \u2026"}, {"title": "Fast and Slow Generating: An Empirical Study on Large and Small Language Models Collaborative Decoding", "link": "https://arxiv.org/pdf/2406.12295", "details": "K Zhang, J Wang, N Ding, B Qi, E Hua, X Lv, B Zhou - arXiv preprint arXiv:2406.12295, 2024", "abstract": "Large Language Models (LLMs) demonstrate impressive performance in diverse applications, yet they face significant drawbacks, including high inference latency, expensive training cost, and generation of hallucination. Collaborative decoding \u2026"}, {"title": "Graph Reasoning Enhanced Language Models for Text-to-SQL", "link": "https://dl.acm.org/doi/abs/10.1145/3626772.3657961", "details": "Z Gong, Y Sun - Proceedings of the 47th International ACM SIGIR \u2026, 2024", "abstract": "Text-to-SQL parsing has attracted substantial attention recently due to its potential to remove barriers for non-expert end users interacting with databases. A key challenge in Text-to-SQL parsing is developing effective encoding mechanisms to capture the \u2026"}, {"title": "Abstraction-of-Thought Makes Language Models Better Reasoners", "link": "https://arxiv.org/pdf/2406.12442", "details": "R Hong, H Zhang, X Pan, D Yu, C Zhang - arXiv preprint arXiv:2406.12442, 2024", "abstract": "Abstract reasoning, the ability to reason from the abstract essence of a problem, serves as a key to generalization in human reasoning. However, eliciting language models to perform reasoning with abstraction remains unexplored. This paper seeks \u2026"}, {"title": "On the Robustness of Language Models for Tabular Question Answering", "link": "https://arxiv.org/pdf/2406.12719", "details": "KR Bhandari, S Xing, S Dan, J Gao - arXiv preprint arXiv:2406.12719, 2024", "abstract": "Large Language Models (LLMs), originally shown to ace various text comprehension tasks have also remarkably been shown to tackle table comprehension tasks without specific training. While previous research has explored LLM capabilities with tabular \u2026"}, {"title": "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation", "link": "https://arxiv.org/pdf/2407.02056", "details": "X Wang, Y Li, S Feng, P Yuan, B Pan, H Wang, Y Hu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-consistency (SC), leveraging multiple samples from LLMs, shows significant gains on various reasoning tasks but struggles with free-form generation due to the difficulty of aggregating answers. Its variants, UCS and USC, rely on sample \u2026"}, {"title": "Semantic Graph Consistency: Going Beyond Patches for Regularizing Self-Supervised Vision Transformers", "link": "https://arxiv.org/pdf/2406.12944", "details": "C Devaguptapu, S Aithal, S Ramasubramanian\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-supervised learning (SSL) with vision transformers (ViTs) has proven effective for representation learning as demonstrated by the impressive performance on various downstream tasks. Despite these successes, existing ViT-based SSL \u2026"}, {"title": "Fine-tuning Diffusion Models for Enhancing Face Quality in Text-to-image Generation", "link": "https://arxiv.org/pdf/2406.17100", "details": "Z Liao, Q Xie, C Chen, H Lu, Z Deng - arXiv preprint arXiv:2406.17100, 2024", "abstract": "Diffusion models (DMs) have achieved significant success in generating imaginative images given textual descriptions. However, they are likely to fall short when it comes to real-life scenarios with intricate details. The low-quality, unrealistic human faces in \u2026"}]
