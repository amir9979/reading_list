[{"title": "Removal of Hallucination on Hallucination: Debate-Augmented RAG", "link": "https://arxiv.org/pdf/2505.18581", "details": "W Hu, W Zhang, Y Jiang, CJ Zhang, X Wei, Q Li - arXiv preprint arXiv:2505.18581, 2025", "abstract": "Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating external knowledge, yet it introduces a critical issue: erroneous or biased retrieval can mislead generation, compounding hallucinations, a phenomenon we term \u2026", "entry_id": "http://arxiv.org/abs/2505.18581v1", "updated": "2025-05-24 08:15:22", "published": "2025-05-24 08:15:22", "authors": "Wentao Hu;Wengyu Zhang;Yiyang Jiang;Chen Jason Zhang;Xiaoyong Wei;Qing Li", "summary": "Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating\nexternal knowledge, yet it introduces a critical issue: erroneous or biased\nretrieval can mislead generation, compounding hallucinations, a phenomenon we\nterm Hallucination on Hallucination. To address this, we propose\nDebate-Augmented RAG (DRAG), a training-free framework that integrates\nMulti-Agent Debate (MAD) mechanisms into both retrieval and generation stages.\nIn retrieval, DRAG employs structured debates among proponents, opponents, and\njudges to refine retrieval quality and ensure factual reliability. In\ngeneration, DRAG introduces asymmetric information roles and adversarial\ndebates, enhancing reasoning robustness and mitigating factual inconsistencies.\nEvaluations across multiple tasks demonstrate that DRAG improves retrieval\nreliability, reduces RAG-induced hallucinations, and significantly enhances\noverall factual accuracy. Our code is available at\nhttps://github.com/Huenao/Debate-Augmented-RAG.", "comment": "Accepted by ACL 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.18581v1;http://arxiv.org/pdf/2505.18581v1", "pdf_url": "http://arxiv.org/pdf/2505.18581v1"}, {"title": "DeepResearchGym: A Free, Transparent, and Reproducible Evaluation Sandbox for Deep Research", "link": "https://arxiv.org/pdf/2505.19253", "details": "J Coelho, J Ning, J He, K Mao, A Paladugu, P Setlur\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Deep research systems represent an emerging class of agentic information retrieval methods that generate comprehensive and well-supported reports to complex queries. However, most existing frameworks rely on dynamic commercial search \u2026", "entry_id": "http://arxiv.org/abs/2505.19253v1", "updated": "2025-05-25 18:16:13", "published": "2025-05-25 18:16:13", "authors": "Jo\u00e3o Coelho;Jingjie Ning;Jingyuan He;Kangrui Mao;Abhijay Paladugu;Pranav Setlur;Jiahe Jin;Jamie Callan;Jo\u00e3o Magalh\u00e3es;Bruno Martins;Chenyan Xiong", "summary": "Deep research systems represent an emerging class of agentic information\nretrieval methods that generate comprehensive and well-supported reports to\ncomplex queries. However, most existing frameworks rely on dynamic commercial\nsearch APIs, which pose reproducibility and transparency challenges in addition\nto their cost. To address these limitations, we introduce DeepResearchGym, an\nopen-source sandbox that combines a reproducible search API with a rigorous\nevaluation protocol for benchmarking deep research systems. The API indexes\nlarge-scale public web corpora, namely ClueWeb22 and FineWeb, using a\nstate-of-the-art dense retriever and approximate nearest neighbor search via\nDiskANN. It achieves lower latency than popular commercial APIs while ensuring\nstable document rankings across runs, and is freely available for research use.\nTo evaluate deep research systems' outputs, we extend the Researchy Questions\nbenchmark with automatic metrics through LLM-as-a-judge assessments to measure\nalignment with users' information needs, retrieval faithfulness, and report\nquality. Experimental results show that systems integrated with DeepResearchGym\nachieve performance comparable to those using commercial APIs, with performance\nrankings remaining consistent across evaluation metrics. A human evaluation\nstudy further confirms that our automatic protocol aligns with human\npreferences, validating the framework's ability to help support controlled\nassessment of deep research systems. Our code and API documentation are\navailable at https://www.deepresearchgym.ai.", "comment": null, "journal_ref": null, "primary_category": "cs.IR", "categories": "cs.IR", "links": "http://arxiv.org/abs/2505.19253v1;http://arxiv.org/pdf/2505.19253v1", "pdf_url": "http://arxiv.org/pdf/2505.19253v1"}, {"title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models", "link": "https://arxiv.org/pdf/2505.07558", "details": "R Higuchi, T Suzuki - arXiv preprint arXiv:2505.07558, 2025", "abstract": "Aligning large language models (LLMs) with human preferences is crucial for safe deployment, yet existing methods assume specific preference models like Bradley- Terry model. This assumption leads to statistical inconsistency, where more data \u2026", "entry_id": "http://arxiv.org/abs/2505.07558v2", "updated": "2025-05-19 20:28:26", "published": "2025-05-12 13:36:25", "authors": "Rei Higuchi;Taiji Suzuki", "summary": "Aligning large language models (LLMs) with human preferences is crucial for\nsafe deployment, yet existing methods assume specific preference models like\nBradley-Terry model. This assumption leads to statistical inconsistency, where\nmore data doesn't guarantee convergence to true human preferences. To address\nthis critical gap, we introduce a novel alignment method Direct Density Ratio\nOptimization (DDRO). DDRO directly estimates the density ratio between\npreferred and unpreferred output distributions, circumventing the need for\nexplicit human preference modeling. We theoretically prove that DDRO is\nstatistically consistent, ensuring convergence to the true preferred\ndistribution as the data size grows, regardless of the underlying preference\nstructure. Experiments demonstrate that DDRO achieves superior performance\ncompared to existing methods on many major benchmarks. DDRO unlocks the\npotential for truly data-driven alignment, paving the way for more reliable and\nhuman-aligned LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CL;stat.ML", "links": "http://arxiv.org/abs/2505.07558v2;http://arxiv.org/pdf/2505.07558v2", "pdf_url": "http://arxiv.org/pdf/2505.07558v2"}, {"title": "Genome-Bench: A Scientific Reasoning Benchmark from Real-World Expert Discussions", "link": "https://arxiv.org/pdf/2505.19501", "details": "M Yin, Y Qu, D Liu, L Yang, L Cong, M Wang - arXiv preprint arXiv:2505.19501, 2025", "abstract": "In this short report, we present an automated pipeline tailored for the genomics domain and introduce\\textit {Genome-Bench}, a new benchmark constructed from over a decade of scientific forum discussions on genome engineering. Our pipeline \u2026", "entry_id": "http://arxiv.org/abs/2505.19501v1", "updated": "2025-05-26 04:28:46", "published": "2025-05-26 04:28:46", "authors": "Ming Yin;Yuanhao Qu;Dyllan Liu;Ling Yang;Le Cong;Mengdi Wang", "summary": "In this short report, we present an automated pipeline tailored for the\ngenomics domain and introduce \\textit{Genome-Bench}, a new benchmark\nconstructed from over a decade of scientific forum discussions on genome\nengineering. Our pipeline transforms raw interactions into a reinforcement\nlearning friendly multiple-choice questions format, supported by 3000+ high\nquality question answer pairs spanning foundational biology, experimental\ntroubleshooting, tool usage, and beyond. To our knowledge, this is the first\nend-to-end pipeline for teaching LLMs to reason from scientific discussions,\nwith promising potential for generalization across scientific domains beyond\nbiology.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.19501v1;http://arxiv.org/pdf/2505.19501v1", "pdf_url": "http://arxiv.org/pdf/2505.19501v1"}, {"title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning", "link": "https://arxiv.org/pdf/2505.20096", "details": "T Nguyen, P Chin, YW Tai - arXiv preprint arXiv:2505.20096, 2025", "abstract": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation (RAG) that addresses the inherent ambiguities and reasoning challenges in complex information-seeking tasks. Unlike conventional RAG methods that rely on either end \u2026", "entry_id": "http://arxiv.org/abs/2505.20096v1", "updated": "2025-05-26 15:05:18", "published": "2025-05-26 15:05:18", "authors": "Thang Nguyen;Peter Chin;Yu-Wing Tai", "summary": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\neither end-to-end fine-tuning or isolated component enhancements, MA-RAG\norchestrates a collaborative set of specialized AI agents: Planner, Step\nDefiner, Extractor, and QA Agents, to tackle each stage of the RAG pipeline\nwith task-aware reasoning. Ambiguities may arise from underspecified queries,\nsparse or indirect evidence in retrieved documents, or the need to integrate\ninformation scattered across multiple sources. MA-RAG mitigates these\nchallenges by decomposing the problem into subtasks, such as query\ndisambiguation, evidence extraction, and answer synthesis, and dispatching them\nto dedicated agents equipped with chain-of-thought prompting. These agents\ncommunicate intermediate reasoning and progressively refine the retrieval and\nsynthesis process. Our design allows fine-grained control over information flow\nwithout any model fine-tuning. Crucially, agents are invoked on demand,\nenabling a dynamic and efficient workflow that avoids unnecessary computation.\nThis modular and reasoning-driven architecture enables MA-RAG to deliver\nrobust, interpretable results. Experiments on multi-hop and ambiguous QA\nbenchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free\nbaselines and rivals fine-tuned systems, validating the effectiveness of\ncollaborative agent-based reasoning in RAG.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.20096v1;http://arxiv.org/pdf/2505.20096v1", "pdf_url": "http://arxiv.org/pdf/2505.20096v1"}, {"title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "link": "https://arxiv.org/pdf/2505.20246", "details": "J Qiu, F Xiao, Y Wang, Y Mao, Y Chen, X Juan, S Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in large language models (LLMs) have led to remarkable progress across domains, yet their capabilities in the humanities, particularly history, remain underexplored. Historical reasoning poses unique challenges for AI, involving \u2026", "entry_id": "http://arxiv.org/abs/2505.20246v1", "updated": "2025-05-26 17:22:20", "published": "2025-05-26 17:22:20", "authors": "Jiahao Qiu;Fulian Xiao;Yimin Wang;Yuchen Mao;Yijia Chen;Xinzhe Juan;Siran Wang;Xuan Qi;Tongcheng Zhang;Zixin Yao;Jiacheng Guo;Yifu Lu;Charles Argon;Jundi Cui;Daixin Chen;Junran Zhou;Shuyao Zhou;Zhanpeng Zhou;Ling Yang;Shilong Liu;Hongru Wang;Kaixuan Huang;Xun Jiang;Yuming Cao;Yue Chen;Yunfei Chen;Zhengyi Chen;Ruowei Dai;Mengqiu Deng;Jiye Fu;Yunting Gu;Zijie Guan;Zirui Huang;Xiaoyan Ji;Yumeng Jiang;Delong Kong;Haolong Li;Jiaqi Li;Ruipeng Li;Tianze Li;Zhuoran Li;Haixia Lian;Mengyue Lin;Xudong Liu;Jiayi Lu;Jinghan Lu;Wanyu Luo;Ziyue Luo;Zihao Pu;Zhi Qiao;Ruihuan Ren;Liang Wan;Ruixiang Wang;Tianhui Wang;Yang Wang;Zeyu Wang;Zihua Wang;Yujia Wu;Zhaoyi Wu;Hao Xin;Weiao Xing;Ruojun Xiong;Weijie Xu;Yao Shu;Xiao Yao;Xiaorui Yang;Yuchen Yang;Nan Yi;Jiadong Yu;Yangyuxuan Yu;Huiting Zeng;Danni Zhang;Yunjie Zhang;Zhaoyu Zhang;Zhiheng Zhang;Xiaofeng Zheng;Peirong Zhou;Linyan Zhong;Xiaoyin Zong;Ying Zhao;Zhenxin Chen;Lin Ding;Xiaoyu Gao;Bingbing Gong;Yichao Li;Yang Liao;Guang Ma;Tianyuan Ma;Xinrui Sun;Tianyi Wang;Han Xia;Ruobing Xian;Gen Ye;Tengfei Yu;Wentao Zhang;Yuxi Wang;Xi Gao;Mengdi Wang", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.", "comment": "17 pages, 7 figures", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL", "links": "http://arxiv.org/abs/2505.20246v1;http://arxiv.org/pdf/2505.20246v1", "pdf_url": "http://arxiv.org/pdf/2505.20246v1"}, {"title": "Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models", "link": "https://arxiv.org/pdf/2505.18638", "details": "MT Hosain, RD Gupta, MK Morol - arXiv preprint arXiv:2505.18638, 2025", "abstract": "In this work, we provide DZEN, a dataset of parallel Dzongkha and English test questions for Bhutanese middle and high school students. The over 5K questions in our collection span a variety of scientific topics and include factual, application, and \u2026", "entry_id": "http://arxiv.org/abs/2505.18638v2", "updated": "2025-05-29 17:11:54", "published": "2025-05-24 11:01:05", "authors": "Md. Tanzib Hosain;Rajan Das Gupta;Md. Kishor Morol", "summary": "In this work, we provide DZEN, a dataset of parallel Dzongkha and English\ntest questions for Bhutanese middle and high school students. The over 5K\nquestions in our collection span a variety of scientific topics and include\nfactual, application, and reasoning-based questions. We use our parallel\ndataset to test a number of Large Language Models (LLMs) and find a significant\nperformance difference between the models in English and Dzongkha. We also look\nat different prompting strategies and discover that Chain-of-Thought (CoT)\nprompting works well for reasoning questions but less well for factual ones. We\nalso find that adding English translations enhances the precision of Dzongkha\nquestion responses. Our results point to exciting avenues for further study to\nimprove LLM performance in Dzongkha and, more generally, in low-resource\nlanguages. We release the dataset at:\nhttps://github.com/kraritt/llm_dzongkha_evaluation.", "comment": "24 pages, 20 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.18638v2;http://arxiv.org/pdf/2505.18638v2", "pdf_url": "http://arxiv.org/pdf/2505.18638v2"}, {"title": "Behavior Injection: Preparing Language Models for Reinforcement Learning", "link": "https://arxiv.org/pdf/2505.18917", "details": "Z Cen, Y Yao, W Han, Z Liu, D Zhao - arXiv preprint arXiv:2505.18917, 2025", "abstract": "Reinforcement fine-tuning (RFT) has emerged as a powerful post-training technique to incentivize the reasoning ability of large language models (LLMs). However, LLMs can respond very inconsistently to RFT: some show substantial performance gains \u2026", "entry_id": "http://arxiv.org/abs/2505.18917v1", "updated": "2025-05-25 00:54:50", "published": "2025-05-25 00:54:50", "authors": "Zhepeng Cen;Yihang Yao;William Han;Zuxin Liu;Ding Zhao", "summary": "Reinforcement fine-tuning (RFT) has emerged as a powerful post-training\ntechnique to incentivize the reasoning ability of large language models (LLMs).\nHowever, LLMs can respond very inconsistently to RFT: some show substantial\nperformance gains, while others plateau or even degrade. To understand this\ndivergence, we analyze the per-step influence of the RL objective and identify\ntwo key conditions for effective post-training: (1) RL-informative rollout\naccuracy, and (2) strong data co-influence, which quantifies how much the\ntraining data affects performance on other samples. Guided by these insights,\nwe propose behavior injection, a task-agnostic data-augmentation scheme applied\nprior to RL. Behavior injection enriches the supervised finetuning (SFT) data\nby seeding exploratory and exploitative behaviors, effectively making the model\nmore RL-ready. We evaluate our method across two reasoning benchmarks with\nmultiple base models. The results demonstrate that our theoretically motivated\naugmentation can significantly increases the performance gain from RFT over the\npre-RL model.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.18917v1;http://arxiv.org/pdf/2505.18917v1", "pdf_url": "http://arxiv.org/pdf/2505.18917v1"}, {"title": "RRO: LLM Agent Optimization Through Rising Reward Trajectories", "link": "https://arxiv.org/pdf/2505.20737", "details": "Z Wang, J Yang, S Nag, S Varshney, X Tang, H Jiang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have exhibited extraordinary performance in a variety of tasks while it remains challenging for them to solve complex multi-step tasks as agents. In practice, agents sensitive to the outcome of certain key steps \u2026", "entry_id": "http://arxiv.org/abs/2505.20737v1", "updated": "2025-05-27 05:27:54", "published": "2025-05-27 05:27:54", "authors": "Zilong Wang;Jingfeng Yang;Sreyashi Nag;Samarth Varshney;Xianfeng Tang;Haoming Jiang;Jingbo Shang;Sheikh Muhammad Sarwar", "summary": "Large language models (LLMs) have exhibited extraordinary performance in a\nvariety of tasks while it remains challenging for them to solve complex\nmulti-step tasks as agents. In practice, agents sensitive to the outcome of\ncertain key steps which makes them likely to fail the task because of a subtle\nmistake in the planning trajectory. Recent approaches resort to calibrating the\nreasoning process through reinforcement learning. They reward or penalize every\nreasoning step with process supervision, as known as Process Reward Models\n(PRMs). However, PRMs are difficult and costly to scale up with a large number\nof next action candidates since they require extensive computations to acquire\nthe training data through the per-step trajectory exploration. To mitigate this\nissue, we focus on the relative reward trend across successive reasoning steps\nand propose maintaining an increasing reward in the collected trajectories for\nprocess supervision, which we term Reward Rising Optimization (RRO).\nSpecifically, we incrementally augment the process supervision until\nidentifying a step exhibiting positive reward differentials, i.e. rising\nrewards, relative to its preceding iteration. This method dynamically expands\nthe search space for the next action candidates, efficiently capturing\nhigh-quality data. We provide mathematical groundings and empirical results on\nthe WebShop and InterCode-SQL benchmarks, showing that our proposed RRO\nachieves superior performance while requiring much less exploration cost.", "comment": "preprint", "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.20737v1;http://arxiv.org/pdf/2505.20737v1", "pdf_url": "http://arxiv.org/pdf/2505.20737v1"}]
