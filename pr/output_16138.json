[{"title": "CSPLADE: Learned Sparse Retrieval with Causal Language Models", "link": "https://arxiv.org/pdf/2504.10816", "details": "Z Xu, A Feng, Y Tian, H Ding, LL Cheong - arXiv preprint arXiv:2504.10816, 2025", "abstract": "In recent years, dense retrieval has been the focus of information retrieval (IR) research. While effective, dense retrieval produces uninterpretable dense vectors, and suffers from the drawback of large index size. Learned sparse retrieval (LSR) \u2026"}, {"title": "DICE: A Framework for Dimensional and Contextual Evaluation of Language Models", "link": "https://arxiv.org/pdf/2504.10359%3F", "details": "A Shrivastava, PA Aoyagui - arXiv preprint arXiv:2504.10359, 2025", "abstract": "Language models (LMs) are increasingly being integrated into a wide range of applications, yet the modern evaluation paradigm does not sufficiently reflect how they are actually being used. Current evaluations rely on benchmarks that often lack \u2026"}, {"title": "Summarization of Multimodal Presentations with Vision-Language Models: Study of the Effect of Modalities and Structure", "link": "https://arxiv.org/pdf/2504.10049%3F", "details": "T Gigant, C Guinaudeau, F Dufaux - arXiv preprint arXiv:2504.10049, 2025", "abstract": "Vision-Language Models (VLMs) can process visual and textual information in multiple formats: texts, images, interleaved texts and images, or even hour-long videos. In this work, we conduct fine-grained quantitative and qualitative analyses of \u2026"}, {"title": "NorEval: A Norwegian Language Understanding and Generation Evaluation Benchmark", "link": "https://arxiv.org/pdf/2504.07749%3F", "details": "V Mikhailov, T Enstad, D Samuel, HC Farseth\u00e5s\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper introduces NorEval, a new and comprehensive evaluation suite for large- scale standardized benchmarking of Norwegian generative language models (LMs). NorEval consists of 24 high-quality human-created datasets--of which five are \u2026"}, {"title": "Learning to Erase Private Knowledge from Multi-Documents for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2504.09910", "details": "Y Wang, H Zhang, L Pang, Y Tong, B Guo, H Zheng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-Augmented Generation (RAG) is a promising technique for applying LLMs to proprietary domains. However, retrieved documents may contain sensitive knowledge, posing risks of privacy leakage in generative results. Thus, effectively \u2026"}, {"title": "Rewriting Pre-Training Data Boosts LLM Performance in Math and Code", "link": "https://arxiv.org/pdf/2505.02881", "details": "K Fujii, Y Tajima, S Mizuki, H Shimada, T Shiotani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The performance of large language models (LLMs) in program synthesis and mathematical reasoning is fundamentally limited by the quality of their pre-training corpora. We introduce two openly licensed datasets, released under the Llama 3.3 \u2026"}, {"title": "Function Calling in Large Language Models: Industrial Practices, Challenges, and Future Directions", "link": "https://openreview.net/pdf%3Fid%3DLNxVGPedFW", "details": "M WANG, Y ZHANG, C PENG, Y CHEN, WEI ZHOU\u2026 - 2025", "abstract": "1 INTRODUCTION Recent advancements in artificial intelligence have ushered in a transformative era with the development of large language models (LLMs) such as GPT series, LLama [164], ChatGLM [33, 212] and Qwen [10]. These models \u2026"}, {"title": "Large language models could be rote learners", "link": "https://arxiv.org/pdf/2504.08300", "details": "Y Xu, R Hu, H Ying, J Wu, X Shi, W Lin - arXiv preprint arXiv:2504.08300, 2025", "abstract": "Multiple-choice question (MCQ) benchmarks are widely used for evaluating Large Language Models (LLMs), yet their reliability is undermined by benchmark contamination. In this study, we reframe contamination as an inherent aspect of \u2026"}, {"title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge", "link": "https://arxiv.org/pdf/2504.07887", "details": "R Cantini, A Orsino, M Ruggiero, D Talia - arXiv preprint arXiv:2504.07887, 2025", "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised \u2026"}]
