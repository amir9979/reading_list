[{"title": "No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language models", "link": "https://arxiv.org/pdf/2503.11985", "details": "CV Kumar, A Urlana, G Kanumolu, BM Garlapati\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Advancements in Large Language Models (LLMs) have increased the performance of different natural language understanding as well as generation tasks. Although LLMs have breached the state-of-the-art performance in various tasks, they often \u2026"}, {"title": "KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on Textualized Knowledge Graphs", "link": "https://arxiv.org/pdf/2504.07087", "details": "E Markowitz, K Galiya, GV Steeg, A Galstyan - arXiv preprint arXiv:2504.07087, 2025", "abstract": "Knowledge graphs have emerged as a popular method for injecting up-to-date, factual knowledge into large language models (LLMs). This is typically achieved by converting the knowledge graph into text that the LLM can process in context. While \u2026"}, {"title": "How do Copilot Suggestions Impact Developers' Frustration and Productivity?", "link": "https://arxiv.org/pdf/2504.06808", "details": "E Guglielmi, V Arnoudova, G Bavota, R Oliveto\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Context. AI-based development tools, such as GitHub Copilot, are transforming the software development process by offering real-time code suggestions. These tools promise to improve the productivity by reducing cognitive load and speeding up task \u2026"}, {"title": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks", "link": "https://arxiv.org/pdf/2503.15478%3F", "details": "Y Zhou, S Jiang, Y Tian, J Weston, S Levine\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language model (LLM) agents need to perform multi-turn interactions in real- world tasks. However, existing multi-turn RL algorithms for optimizing LLM agents fail to perform effective credit assignment over multiple turns while leveraging the \u2026"}, {"title": "S'MoRE: Structural Mixture of Residual Experts for LLM Fine-tuning", "link": "https://arxiv.org/pdf/2504.06426", "details": "H Zeng, Y Xia, Z Zhao, G Jiang, Q Zhang, J Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Fine-tuning pre-trained large language models (LLMs) presents a dual challenge of balancing parameter efficiency and model capacity. Existing methods like low-rank adaptations (LoRA) are efficient but lack flexibility, while Mixture-of-Experts (MoE) \u2026"}]
