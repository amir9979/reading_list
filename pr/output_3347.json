[{"title": "PathAlign: A vision-language model for whole slide images in histopathology", "link": "https://arxiv.org/pdf/2406.19578", "details": "F Ahmed, A Sellergren, L Yang, S Xu, B Babenko\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Microscopic interpretation of histopathology images underlies many important diagnostic and treatment decisions. While advances in vision-language modeling raise new opportunities for analysis of such images, the gigapixel-scale size of whole \u2026"}, {"title": "Visual-Text Cross Alignment: Refining the Similarity Score in Vision-Language Models", "link": "https://arxiv.org/pdf/2406.02915", "details": "J Li, H Li, S Erfani, L Feng, J Bailey, F Liu - arXiv preprint arXiv:2406.02915, 2024", "abstract": "It has recently been discovered that using a pre-trained vision-language model (VLM), eg, CLIP, to align a whole query image with several finer text descriptions generated by a large language model can significantly enhance zero-shot \u2026"}, {"title": "See It from My Perspective: Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding", "link": "https://arxiv.org/pdf/2406.11665", "details": "A Ananthram, E Stengel-Eskin, C Vondrick, M Bansal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language models (VLMs) can respond to queries about images in many languages. However, beyond language, culture affects how we see things. For example, individuals from Western cultures focus more on the central figure in an \u2026"}, {"title": "CPLIP: Zero-Shot Learning for Histopathology with Comprehensive Vision-Language Alignment", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Javed_CPLIP_Zero-Shot_Learning_for_Histopathology_with_Comprehensive_Vision-Language_Alignment_CVPR_2024_paper.pdf", "details": "S Javed, A Mahmood, II Ganapathi, FA Dharejo\u2026 - Proceedings of the IEEE \u2026, 2024", "abstract": "Abstract This paper proposes Comprehensive Pathology Language Image Pre- training (CPLIP) a new unsupervised technique designed to enhance the alignment of images and text in histopathology for tasks such as classification and \u2026"}, {"title": "Large Language Model Uncertainty Measurement and Calibration for Medical Diagnosis and Treatment", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/06/07/2024.06.06.24308399.full.pdf", "details": "T Savage, J Wang, R Gallo, A Boukil, V Patel\u2026 - medRxiv, 2024", "abstract": "Introduction The inability for Large Language Models (LLMs) to communicate uncertainty is a significant barrier to their use in medicine. Before LLMs can be integrated into patient care, the field must assess methods to measure uncertainty in \u2026"}, {"title": "Benchmarking Vision-Language Contrastive Methods for Medical Representation Learning", "link": "https://arxiv.org/pdf/2406.07450", "details": "S Roy, Y Parhizkar, F Ogidi, VR Khazaie, M Colacci\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We perform a comprehensive benchmarking of contrastive frameworks for learning multimodal representations in the medical domain. Through this study, we aim to answer the following research questions:(i) How transferable are general-domain \u2026"}, {"title": "ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for Whole Slide Image Classification", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_ViLa-MIL_Dual-scale_Vision-Language_Multiple_Instance_Learning_for_Whole_Slide_Image_CVPR_2024_paper.pdf", "details": "J Shi, C Li, T Gong, Y Zheng, H Fu - Proceedings of the IEEE/CVF Conference on \u2026, 2024", "abstract": "Multiple instance learning (MIL)-based framework has become the mainstream for processing the whole slide image (WSI) with giga-pixel size and hierarchical image context in digital pathology. However these methods heavily depend on a substantial \u2026"}, {"title": "Situational Awareness Matters in 3D Vision Language Reasoning", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Man_Situational_Awareness_Matters_in_3D_Vision_Language_Reasoning_CVPR_2024_paper.pdf", "details": "Y Man, LY Gui, YX Wang - Proceedings of the IEEE/CVF Conference on Computer \u2026, 2024", "abstract": "Being able to carry out complicated vision language reasoning tasks in 3D space represents a significant milestone in developing household robots and human- centered embodied AI. In this work we demonstrate that a critical and distinct \u2026"}, {"title": "VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models", "link": "https://arxiv.org/pdf/2406.10228", "details": "C Zhou, M Zhang, P Chen, C Fu, Y Shen, X Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The swift progress of Multi-modal Large Models (MLLMs) has showcased their impressive ability to tackle tasks blending vision and language. Yet, most current models and benchmarks cater to scenarios with a narrow scope of visual and textual \u2026"}]
