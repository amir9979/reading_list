[{"title": "Comparing the accuracy of **large language models** and prompt engineering in diagnosing realworld cases", "link": "https://www.sciencedirect.com/science/article/pii/S1386505625002436", "details": "G Yao, WJ Zhang, Y Zhu, U Wong, Y Zhang, C Yang\u2026 - International Journal of \u2026, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) hold potential in clinical decision-making, especially for complex and rare disease diagnoses. However, real-world applications require further **evaluation** \u2026 Four LLMs were **evaluated** using two \u2026"}, {"title": "MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages", "link": "https://arxiv.org/pdf/2506.19468", "details": "W Han, Y Zhang, Z Chen, B Liu, H Lin, B Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 We present MUBENCH, a comprehensive multilingual benchmark for **evaluating** **large** **language** **models** (LLMs) across 61 languages. Through rigorous translation quality control and cross-lingual consistency **evaluation** , MUBENCH provides \u2026", "entry_id": "http://arxiv.org/abs/2506.19468v1", "updated": "2025-06-24 09:53:00", "published": "2025-06-24 09:53:00", "authors": "Wenhan Han;Yifan Zhang;Zhixun Chen;Binbin Liu;Haobin Lin;Bingni Zhang;Taifeng Wang;Mykola Pechenizkiy;Meng Fang;Yin Zheng", "summary": "Multilingual large language models (LLMs) are advancing rapidly, with new\nmodels frequently claiming support for an increasing number of languages.\nHowever, existing evaluation datasets are limited and lack cross-lingual\nalignment, leaving assessments of multilingual capabilities fragmented in both\nlanguage and skill coverage. To address this, we introduce MuBench, a benchmark\ncovering 61 languages and evaluating a broad range of capabilities. We evaluate\nseveral state-of-the-art multilingual LLMs and find notable gaps between\nclaimed and actual language coverage, particularly a persistent performance\ndisparity between English and low-resource languages. Leveraging MuBench's\nalignment, we propose Multilingual Consistency (MLC) as a complementary metric\nto accuracy for analyzing performance bottlenecks and guiding model\nimprovement. Finally, we pretrain a suite of 1.2B-parameter models on English\nand Chinese with 500B tokens, varying language ratios and parallel data\nproportions to investigate cross-lingual transfer dynamics.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.19468v1;http://arxiv.org/pdf/2506.19468v1", "pdf_url": "http://arxiv.org/pdf/2506.19468v1"}, {"title": "Multimodal large language models and physics visual tasks: comparative analysis of performance and costs", "link": "https://arxiv.org/pdf/2506.19662", "details": "G Polverini, B Gregorcic - arXiv preprint arXiv:2506.19662, 2025", "abstract": "\u2026 Multimodal **large** **language** **models** (MLLMs) capable of processing both text and visual inputs \u2026 Since late 2023, LLMs have also been upgraded to multimodal **large** **language** **models** (\u2026 This study offers an example of an **evaluation** that can help \u2026", "entry_id": "http://arxiv.org/abs/2506.19662v1", "updated": "2025-06-24 14:25:04", "published": "2025-06-24 14:25:04", "authors": "Giulia Polverini;Bor Gregorcic", "summary": "Multimodal large language models (MLLMs) capable of processing both text and\nvisual inputs are increasingly being explored for uses in physics education,\nsuch as tutoring, formative assessment, and grading. This study evaluates a\nrange of publicly available MLLMs on a set of standardized, image-based physics\nresearch-based conceptual assessments (concept inventories). We benchmark 15\nmodels from three major providers (Anthropic, Google, and OpenAI) across 102\nphysics items, focusing on two main questions: (1) How well do these models\nperform on conceptual physics tasks involving visual representations? and (2)\nWhat are the financial costs associated with their use? The results show high\nvariability in both performance and cost. The performance of the tested models\nranges from around 76\\% to as low as 21\\%. We also found that expensive models\ndo not always outperform cheaper ones and that, depending on the demands of the\ncontext, cheaper models may be sufficiently capable for some tasks. This is\nespecially relevant in contexts where financial resources are limited or for\nlarge-scale educational implementation of MLLMs. By providing these analyses,\nour aim is to inform teachers, institutions, and other educational stakeholders\nso that they can make evidence-based decisions about the selection of models\nfor use in AI-supported physics education.", "comment": null, "journal_ref": null, "primary_category": "physics.ed-ph", "categories": "physics.ed-ph", "links": "http://arxiv.org/abs/2506.19662v1;http://arxiv.org/pdf/2506.19662v1", "pdf_url": "http://arxiv.org/pdf/2506.19662v1"}, {"title": " **Large Language Models** for Logical Fallacy Detection", "link": "https://link.springer.com/chapter/10.1007/978-981-96-8197-6_29", "details": "N Teo, D Huang, E Cambria, Z Wang - \u2026 Asia Conference on Knowledge Discovery and \u2026, 2025", "abstract": "\u2026 We present an extensive study on the use of **large** **language** **models** (LLMs) for logical fallacy detection and provide a comparative \u2026 In this study, we **evaluate** the logical fallacy detection capabilities of different state-of-the-art **large** **language** \u2026"}, {"title": "Can Large Language Models Capture Human Annotator Disagreements?", "link": "https://arxiv.org/pdf/2506.19467", "details": "J Ni, Y Fan, V Zouhar, D Rooein, A Hoyle, M Sachan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 While **Large** **Language** **Models** (LLMs) are increasingly used for automatic annotation to reduce human effort, their **evaluation** often focuses \u2026 Hence, we extend the **evaluation** of distribution prediction to disagreement in NLP annotation \u2026", "entry_id": "http://arxiv.org/abs/2506.19467v1", "updated": "2025-06-24 09:49:26", "published": "2025-06-24 09:49:26", "authors": "Jingwei Ni;Yu Fan;Vil\u00e9m Zouhar;Donya Rooein;Alexander Hoyle;Mrinmaya Sachan;Markus Leippold;Dirk Hovy;Elliott Ash", "summary": "Human annotation variation (i.e., annotation disagreements) is common in NLP\nand often reflects important information such as task subjectivity and sample\nambiguity. While Large Language Models (LLMs) are increasingly used for\nautomatic annotation to reduce human effort, their evaluation often focuses on\npredicting the majority-voted \"ground truth\" labels. It is still unclear,\nhowever, whether these models also capture informative human annotation\nvariation. Our work addresses this gap by extensively evaluating LLMs' ability\nto predict annotation disagreements without access to repeated human labels.\nOur results show that LLMs struggle with modeling disagreements, which can be\noverlooked by majority label-based evaluations. Notably, while RLVR-style\n(Reinforcement learning with verifiable rewards) reasoning generally boosts LLM\nperformance, it degrades performance in disagreement prediction. Our findings\nhighlight the critical need for evaluating and improving LLM annotators in\ndisagreement modeling. Code and data at\nhttps://github.com/EdisonNi-hku/Disagreement_Prediction.", "comment": "Preprint Under Review", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.19467v1;http://arxiv.org/pdf/2506.19467v1", "pdf_url": "http://arxiv.org/pdf/2506.19467v1"}, {"title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models", "link": "https://arxiv.org/pdf/2506.19505", "details": "Z Li, C Xiao, Y Wang, X Liu, Z Tang, B Lu, M Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 In this experiment, we **evaluate** the efficiency of our AnTKV implementation against the FlashAttention-accelerated FP16 baseline on LLaMA-3\u2026 To **evaluate** decoding efficiency, we **evaluate** the throughput of AnTKV under a fixed context \u2026", "entry_id": "http://arxiv.org/abs/2506.19505v1", "updated": "2025-06-24 10:45:48", "published": "2025-06-24 10:45:48", "authors": "Zeyu Li;Chuanfu Xiao;Yang Wang;Xiang Liu;Zhenheng Tang;Baotong Lu;Mao Yang;Xinyu Chen;Xiaowen Chu", "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.19505v1;http://arxiv.org/pdf/2506.19505v1", "pdf_url": "http://arxiv.org/pdf/2506.19505v1"}, {"title": "Black-Box Test Code Fault Localization Driven by Large Language Models and Execution Estimation", "link": "https://arxiv.org/pdf/2506.19045", "details": "AS Yaraghi, G Gharachorlu, S Fatima, LC Briand\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 We further propose a masking strategy inspired by the token masking approach used in LLMs [23] to statically **evaluate** the accuracy of our \u2026 3) We **evaluate** our TCFL technique at different levels of test code granularity, ie, function, block, and \u2026", "entry_id": "http://arxiv.org/abs/2506.19045v1", "updated": "2025-06-23 19:04:51", "published": "2025-06-23 19:04:51", "authors": "Ahmadreza Saboor Yaraghi;Golnaz Gharachorlu;Sakina Fatima;Lionel C. Briand;Ruiyuan Wan;Ruifeng Gao", "summary": "Fault localization (FL) is a critical step in debugging which typically\nrelies on repeated executions to pinpoint faulty code regions. However,\nrepeated executions can be impractical in the presence of non-deterministic\nfailures or high execution costs. While recent efforts have leveraged Large\nLanguage Models (LLMs) to aid execution-free FL, these have primarily focused\non identifying faults in the system under test (SUT) rather than in the often\ncomplex system test code. However, the latter is also important as, in\npractice, many failures are triggered by faulty test code. To overcome these\nchallenges, we introduce a fully static, LLM-driven approach for system test\ncode fault localization (TCFL) that does not require executing the test case.\nOur method uses a single failure execution log to estimate the test's execution\ntrace through three novel algorithms that identify only code statements likely\ninvolved in the failure. This pruned trace, combined with the error message, is\nused to prompt the LLM to rank potential faulty locations. Our black-box,\nsystem-level approach requires no access to the SUT source code and is\napplicable to large test scripts that assess full system behavior. We evaluate\nour technique at function, block, and line levels using an industrial dataset\nof faulty test cases not previously used in pre-training LLMs. Results show\nthat our best estimated trace closely match actual traces, with an F1 score of\naround 90%. Additionally, pruning the complex system test code reduces the\nLLM's inference time by up to 34% without any loss in FL performance. Our\nresults further suggest that block-level TCFL offers a practical balance,\nnarrowing the search space while preserving useful context, achieving an 81%\nhit rate at top-3 (Hit@3).", "comment": null, "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE", "links": "http://arxiv.org/abs/2506.19045v1;http://arxiv.org/pdf/2506.19045v1", "pdf_url": "http://arxiv.org/pdf/2506.19045v1"}, {"title": "Enhancing Biosecurity in Tamper-Resistant Large Language Models With Quantum Gradient Descent", "link": "https://arxiv.org/pdf/2506.19086", "details": "F Hai, S Nirzhor, R Khan, D Roosan - arXiv preprint arXiv:2506.19086, 2025", "abstract": "\u2026 An extensive subset of the MIMIC database served as the foundational corpus for training and **evaluations** in this project. This database portion included both structured patient data and unstructured clinical notes, with the intent of covering a \u2026", "entry_id": "http://arxiv.org/abs/2506.19086v1", "updated": "2025-06-23 20:03:35", "published": "2025-06-23 20:03:35", "authors": "Fahmida Hai;Saif Nirzhor;Rubayat Khan;Don Roosan", "summary": "This paper introduces a tamper-resistant framework for large language models\n(LLMs) in medical applications, utilizing quantum gradient descent (QGD) to\ndetect malicious parameter modifications in real time. Integrated into a\nLLaMA-based model, QGD monitors weight amplitude distributions, identifying\nadversarial fine-tuning anomalies. Tests on the MIMIC and eICU datasets show\nminimal performance impact (accuracy: 89.1 to 88.3 on MIMIC) while robustly\ndetecting tampering. PubMedQA evaluations confirm preserved biomedical\nquestion-answering capabilities. Compared to baselines like selective\nunlearning and cryptographic fingerprinting, QGD offers superior sensitivity to\nsubtle weight changes. This quantum-inspired approach ensures secure, reliable\nmedical AI, extensible to other high-stakes domains.", "comment": "The conference schedule and details can be found here:\n  https://www.insticc.org/node/technicalprogram/DATA/2025", "journal_ref": null, "primary_category": "q-bio.MN", "categories": "q-bio.MN", "links": "http://arxiv.org/abs/2506.19086v1;http://arxiv.org/pdf/2506.19086v1", "pdf_url": "http://arxiv.org/pdf/2506.19086v1"}, {"title": "Total Recall? **Evaluating** the Macroeconomic Knowledge of **Large Language Models**", "link": "https://www.federalreserve.gov/econres/feds/total-recall-evaluating-the-macroeconomic-knowledge-of-large-language-models.htm", "details": "LD Crane, A Karra, PE Soto - 2025", "abstract": "\u2026 We **evaluate** the ability of **large** **language** **models** (LLMs) to estimate historical macroeconomic variables and data release dates. We find that LLMs have precise knowledge of some recent statistics, but performance degrades as we go farther \u2026"}]
