[{"title": "On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?", "link": "https://arxiv.org/pdf/2405.02266", "details": "M Zanella, IB Ayed - arXiv preprint arXiv:2405.02266, 2024", "abstract": "The development of large vision-language models, notably CLIP, has catalyzed research into effective adaptation techniques, with a particular focus on soft prompt tuning. Conjointly, test-time augmentation, which utilizes multiple augmented views \u2026"}, {"title": "Benchmarking Benchmark Leakage in Large Language Models", "link": "https://arxiv.org/pdf/2404.18824%3Ftrk%3Dpublic_post_comment-text", "details": "R Xu, Z Wang, RZ Fan, P Liu - arXiv preprint arXiv:2404.18824, 2024", "abstract": "Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary \u2026"}]
