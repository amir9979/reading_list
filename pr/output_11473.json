[{"title": "UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for Diverse Medical Imaging Modalities", "link": "https://arxiv.org/pdf/2412.10372", "details": "MU Khattak, S Kunhimon, M Naseer, S Khan, FS Khan - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-Language Models (VLMs) trained via contrastive learning have achieved notable success in natural image tasks. However, their application in the medical domain remains limited due to the scarcity of openly accessible, large-scale medical \u2026"}, {"title": "Do language models understand time?", "link": "https://arxiv.org/pdf/2412.13845", "details": "X Ding, L Wang - arXiv preprint arXiv:2412.13845, 2024", "abstract": "Large language models (LLMs) have revolutionized video-based computer vision applications, including action recognition, anomaly detection, and video summarization. Videos inherently pose unique challenges, combining spatial \u2026"}, {"title": "LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation", "link": "https://arxiv.org/pdf/2412.15188", "details": "W Shi, X Han, C Zhou, W Liang, XV Lin, L Zettlemoyer\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present LlamaFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LlamaFusion \u2026"}, {"title": "Self-critical strategy adjustment based artificial intelligence method in generating diagnostic reports of respiratory diseases", "link": "https://iopscience.iop.org/article/10.1088/1361-6579/ada869/meta", "details": "B Chen, G Liu, Q Zhang - Physiological Measurement, 2025", "abstract": "Objective. Humanity faces many health challenges, among which respiratory diseases are one of the leading causes of human death. Existing AI-driven pre- diagnosis approaches can enhance the efficiency of diagnosis but still face \u2026"}, {"title": "GIRAFFE: Design Choices for Extending the Context Length of Visual Language Models", "link": "https://arxiv.org/pdf/2412.12735", "details": "M Li, L Li, S Gong, Q Liu - arXiv preprint arXiv:2412.12735, 2024", "abstract": "Visual Language Models (VLMs) demonstrate impressive capabilities in processing multimodal inputs, yet applications such as visual agents, which require handling multiple images and high-resolution videos, demand enhanced long-range \u2026"}, {"title": "CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology", "link": "https://arxiv.org/pdf/2412.12077", "details": "Y Sun, Y Si, C Zhu, X Gong, K Zhang, P Chen, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence of large multimodal models (LMMs) has brought significant advancements to pathology. Previous research has primarily focused on separately training patch-level and whole-slide image (WSI)-level models, limiting the \u2026"}, {"title": "Token Preference Optimization with Self-Calibrated Visual-Anchored Rewards for Hallucination Mitigation", "link": "https://arxiv.org/pdf/2412.14487", "details": "J Gu, Y Wang, M Cao, P Bu, J Song, Y He, S Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Direct Preference Optimization (DPO) has been demonstrated to be highly effective in mitigating hallucinations in Large Vision Language Models (LVLMs) by aligning their outputs more closely with human preferences. Despite the recent progress \u2026"}, {"title": "Activating Distributed Visual Region within LLMs for Efficient and Effective Vision-Language Training and Inference", "link": "https://arxiv.org/pdf/2412.12785", "details": "S Wang, D Wang, C Zhou, Z Li, Z Fan, X Huang, Z Wei - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) typically learn visual capacity through visual instruction tuning, involving updates to both a projector and their LLM backbones. Drawing inspiration from the concept of visual region in the human brain \u2026"}, {"title": "A Novel Pathology Foundation Model by Mayo Clinic, Charit\\'e, and Aignostics", "link": "https://arxiv.org/pdf/2501.05409", "details": "M Alber, S Tietz, J Dippel, T Milbich, T Lesort\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in digital pathology have demonstrated the effectiveness of foundation models across diverse applications. In this report, we present a novel vision foundation model based on the RudolfV approach. Our model was trained on \u2026"}]
