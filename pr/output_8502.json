[{"title": "PLDR-LLM: Large Language Model from Power Law Decoder Representations", "link": "https://arxiv.org/pdf/2410.16703", "details": "B Gokden - arXiv preprint arXiv:2410.16703, 2024", "abstract": "We present the Large Language Model from Power Law Decoder Representations (PLDR-LLM), a language model that leverages non-linear and linear transformations through Power Law Graph Attention mechanism to generate well-defined deductive \u2026"}, {"title": "Fast Training of Sinusoidal Neural Fields via Scaling Initialization", "link": "https://arxiv.org/pdf/2410.04779", "details": "T Yeom, S Lee, J Lee - arXiv preprint arXiv:2410.04779, 2024", "abstract": "Neural fields are an emerging paradigm that represent data as continuous functions parameterized by neural networks. Despite many advantages, neural fields often have a high training cost, which prevents a broader adoption. In this paper, we focus \u2026"}, {"title": "H2OVL-Mississippi Vision Language Models Technical Report", "link": "https://arxiv.org/pdf/2410.13611", "details": "S Galib, S Wang, G Xu, P Pfeiffer, R Chesler, M Landry\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Smaller vision-language models (VLMs) are becoming increasingly important for privacy-focused, on-device applications due to their ability to run efficiently on consumer hardware for processing enterprise commercial documents and images \u2026"}, {"title": "BUMBLE: Unifying Reasoning and Acting with Vision-Language Models for Building-wide Mobile Manipulation", "link": "https://arxiv.org/pdf/2410.06237", "details": "R Shah, A Yu, Y Zhu, Y Zhu, R Mart\u00edn-Mart\u00edn - arXiv preprint arXiv:2410.06237, 2024", "abstract": "To operate at a building scale, service robots must perform very long-horizon mobile manipulation tasks by navigating to different rooms, accessing different floors, and interacting with a wide and unseen range of everyday objects. We refer to these \u2026"}, {"title": "Health Care Language Models and Their Fine-Tuning for Information Extraction: Scoping Review", "link": "https://medinform.jmir.org/2024/1/e60164/", "details": "M Nunes, J Bone, JC Ferreira, LB Elvas - JMIR Medical Informatics, 2024", "abstract": "Background: In response to the intricate language, specialized terminology outside everyday life, and the frequent presence of abbreviations and acronyms inherent in health care text data, domain adaptation techniques have emerged as crucial to \u2026"}, {"title": "LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies", "link": "https://arxiv.org/pdf/2410.04749", "details": "A Hamza, YH Ahn, S Lee, ST Kim - arXiv preprint arXiv:2410.04749, 2024", "abstract": "Generating Natural Language Explanations (NLEs) for model predictions on medical images, particularly those depicting thoracic pathologies, remains a critical and challenging task. Existing methodologies often struggle due to general models' \u2026"}, {"title": "ReTok: Replacing Tokenizer to Enhance Representation Efficiency in Large Language Model", "link": "https://arxiv.org/pdf/2410.04335", "details": "S Gu, M Zhao, B Zhang, L Wang, J Li, G Liu - arXiv preprint arXiv:2410.04335, 2024", "abstract": "Tokenizer is an essential component for large language models (LLMs), and a tokenizer with a high compression rate can improve the model's representation and processing efficiency. However, the tokenizer cannot ensure high compression rate \u2026"}, {"title": "MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router", "link": "https://arxiv.org/pdf/2410.12013", "details": "Y Xie, Z Zhang, D Zhou, C Xie, Z Song, X Liu, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mixture-of-Experts (MoE) architectures face challenges such as high memory consumption and redundancy in experts. Pruning MoE can reduce network weights while maintaining model performance. Motivated by the recent observation of \u2026"}, {"title": "SPRIG: Improving Large Language Model Performance by System Prompt Optimization", "link": "https://arxiv.org/pdf/2410.14826", "details": "L Zhang, T Ergen, L Logeswaran, M Lee, D Jurgens - arXiv preprint arXiv:2410.14826, 2024", "abstract": "Large Language Models (LLMs) have shown impressive capabilities in many scenarios, but their performance depends, in part, on the choice of prompt. Past research has focused on optimizing prompts specific to a task. However, much less \u2026"}]
