[{"title": "Designing Retrieval-Augmented Language Models for Clinical Decision Support", "link": "https://link.springer.com/chapter/10.1007/978-3-031-63592-2_13", "details": "K Quigley, T Koker, J Taylor, V Mancuso, L Brattain - AI for Health Equity and Fairness \u2026, 2024", "abstract": "Ever-increasing demands for physician expertise drive the need for trustworthy point- of-care tools that can help aid decision-making in all clinical settings. Retrieval- augmented language models carry potential to relieve the information burden on \u2026"}, {"title": "Decoding the NCCN Guidelines With AI: A Comparative Evaluation of ChatGPT-4.0 and Llama 2 in the Management of Thyroid Carcinoma", "link": "https://journals.sagepub.com/doi/abs/10.1177/00031348241269430", "details": "S Pandya, TE Bresler, T Wilson, Z Htway, M Fujita - The American Surgeon\u2122, 2024", "abstract": "Introduction Artificial Intelligence (AI) has emerged as a promising tool in the delivery of health care. ChatGPT-4.0 (OpenAI, San Francisco, California) and Llama 2 (Meta, Menlo Park, CA) have each gained attention for their use in various medical \u2026"}, {"title": "Multi-modal Concept Alignment Pre-training for Generative Medical Visual Question Answering", "link": "https://aclanthology.org/2024.findings-acl.319.pdf", "details": "Q Yan, J Duan, J Wang - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Medical Visual Question Answering (Med-VQA) seeks to accurately respond to queries regarding medical images, a task particularly challenging for open-ended questions. This study unveils the Multi-modal Concept Alignment Pre-training \u2026"}, {"title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism", "link": "https://arxiv.org/pdf/2408.10473", "details": "G Li, X Zhao, L Liu, Z Li, D Li, L Tian, J He, A Sirasao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational \u2026"}, {"title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios", "link": "https://aclanthology.org/2024.findings-acl.230.pdf", "details": "L Lin, J Fu, P Liu, Q Li, Y Gong, J Wan, F Zhang\u2026 - Findings of the Association \u2026, 2024", "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local \u2026"}, {"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "link": "https://arxiv.org/pdf/2408.12337", "details": "KS Phogat, SA Puranam, S Dasaratha, C Harsha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain \u2026"}, {"title": "Unraveling the Inner Workings of Massive Language Models: Architecture, Training, and Linguistic Capacities", "link": "https://www.igi-global.com/chapter/unraveling-the-inner-workings-of-massive-language-models/354398", "details": "CVS Babu, CSA Anniyappa - Challenges in Large Language Model Development \u2026, 2024", "abstract": "This study explores the evolution of language models, emphasizing the shift from traditional statistical methods to advanced neural networks, particularly the transformer architecture. It aims to understand the impact of these advancements on \u2026"}, {"title": "Self-Supervised Position Debiasing for Large Language Models", "link": "https://aclanthology.org/2024.findings-acl.170.pdf", "details": "Z Liu, Z Chen, M Zhang, Z Ren, P Ren, Z Chen - Findings of the Association for \u2026, 2024", "abstract": "Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Previous \u2026"}, {"title": "NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks", "link": "https://arxiv.org/pdf/2408.13106", "details": "H Huang, T Park, K Dhawan, I Medennikov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-supervised learning has been proved to benefit a wide range of speech processing tasks, such as speech recognition/translation, speaker verification and diarization, etc. However, most of these approaches are computationally intensive \u2026"}]
