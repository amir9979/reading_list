[{"title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models", "link": "https://arxiv.org/pdf/2504.12526", "details": "J Zhang, T Zhu, C Luo, A Anandkumar - arXiv preprint arXiv:2504.12526, 2025", "abstract": "Long-context language models exhibit impressive performance but remain challenging to deploy due to high GPU memory demands during inference. We propose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that \u2026"}, {"title": "GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models", "link": "https://aclanthology.org/2025.naacl-long.420.pdf", "details": "H Kohli, S Kumar, H Sun - Proceedings of the 2025 Conference of the Nations of \u2026, 2025", "abstract": "The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their reasoning to \u2026"}, {"title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math", "link": "https://arxiv.org/pdf/2504.21233", "details": "H Xu, B Peng, H Awadalla, D Chen, YC Chen, M Gao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving \u2026"}, {"title": "Exploring Multimodal Language Models for Sustainability Disclosure Extraction: A Comparative Study", "link": "https://aclanthology.org/2025.insights-1.13.pdf", "details": "T Gupta, T Goel, I Verma - The Sixth Workshop on Insights from Negative Results \u2026, 2025", "abstract": "Sustainability metrics have increasingly become a crucial non-financial criterion in investment decision-making. Organizations worldwide are recognizing the importance of sustainability and are proactively highlighting their efforts through \u2026"}, {"title": "Scaling Large Language Models for Next-Generation Single-Cell Analysis", "link": "https://www.biorxiv.org/content/10.1101/2025.04.14.648850.full.pdf", "details": "SA Rizvi, D Levine, A Patel, S Zhang, E Wang, S He\u2026 - bioRxiv, 2025", "abstract": "Single-cell RNA sequencing has transformed our understanding of cellular diversity, yet current single-cell foundation models (scFMs) remain limited in their scalability, flexibility across diverse tasks, and ability to natively integrate textual information. In \u2026"}, {"title": "A Natural Language Processing-Based Approach for Early Detection of Heart Failure Onset using Electronic Health Records", "link": "https://www.medrxiv.org/content/medrxiv/early/2025/04/06/2025.04.04.25325211.full.pdf", "details": "Y Liu, Z Tan, Z Zhang, S Wang, J Guo, H Liu, T Chen\u2026 - medRxiv, 2025", "abstract": "Objectives. This study set out to develop and validate a risk prediction tool for the early detection of heart failure (HF) onset using real-world electronic health records (EHRs). Background. While existing HF risk assessment models have shown \u2026"}, {"title": "Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks", "link": "https://arxiv.org/pdf/2504.18838", "details": "Y Cao, S Hong, X Li, J Ying, Y Ma, H Liang, Y Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) are advancing at an amazing speed and have become indispensable across academia, industry, and daily applications. To keep pace with the status quo, this survey probes the core challenges that the rise of LLMs \u2026"}, {"title": "Code Copycat Conundrum: Demystifying Repetition in LLM-based Code Generation", "link": "https://arxiv.org/pdf/2504.12608", "details": "M Liu, J Li, Y Wang, X Du, Z Ou, Q Chen, B An, Z Wei\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite recent advances in Large Language Models (LLMs) for code generation, the quality of LLM-generated code still faces significant challenges. One significant issue is code repetition, which refers to the model's tendency to generate structurally \u2026"}, {"title": "Adapting LLM Agents with Universal Communication Feedback", "link": "https://aclanthology.org/2025.findings-naacl.339.pdf", "details": "K Wang, Y Lu, M Santacroce, Y Gong, C Zhang\u2026 - Findings of the Association \u2026, 2025", "abstract": "Recent advances in large language models (LLMs) have demonstrated potential for LLM agents. To facilitate the training for these agents with both linguistic feedback and non-linguistic reward signals, we introduce Learning through Communication \u2026"}]
