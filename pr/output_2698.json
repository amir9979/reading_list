[{"title": "Mastering Transformers: The Journey from BERT to Large Language Models and Stable Diffusion", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DM_wJEQAAQBAJ%26oi%3Dfnd%26pg%3DPP1%26ots%3DavP0-Yfg2z%26sig%3DR-lqDQYxb4UAe0Ia8_B1KsGFtwc", "details": "S Y\u0131ld\u0131r\u0131m, M Asgari-Chenaghlu - 2024", "abstract": "Explore transformer-based language models from BERT to GPT, delving into NLP and computer vision tasks, while tackling challenges effectively Key Features Understand the complexity of deep learning architecture and transformers \u2026"}, {"title": "TAeKD: Teacher Assistant Enhanced Knowledge Distillation for Closed-Source Multilingual Neural Machine Translation", "link": "https://aclanthology.org/2024.lrec-main.1350.pdf", "details": "B Lv, X Liu, K Wei, P Luo, Y Yu - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Knowledge Distillation (KD) serves as an efficient method for transferring language knowledge from open-source large language models (LLMs) to more computationally efficient models. However, challenges arise when attempting to \u2026"}]
