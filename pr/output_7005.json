[{"title": "Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific Expert Pruning", "link": "https://arxiv.org/pdf/2409.01483", "details": "S Sarkar, L Lausen, V Cevher, S Zha, T Brox, G Karypis - arXiv preprint arXiv \u2026, 2024", "abstract": "Sparse Mixture of Expert (SMoE) models have emerged as a scalable alternative to dense models in language modeling. These models use conditionally activated feedforward subnetworks in transformer blocks, allowing for a separation between \u2026"}, {"title": "Language-model-based patient embedding using electronic health records facilitates phenotyping, disease forecasting, and progression analysis", "link": "https://www.researchsquare.com/article/rs-4708839/latest.pdf", "details": "S Xian, M Grabowska, I Kullo, Y Luo, J Smoller\u2026 - 2024", "abstract": "Current studies regarding the secondary use of electronic health records (EHR) predominantly rely on domain expertise and existing medical knowledge. Though significant efforts have been devoted to investigating the application of machine \u2026"}, {"title": "Studying Veteran food insecurity longitudinally using electronic health record data and natural language processing", "link": "https://www.medrxiv.org/content/10.1101/2024.08.30.24312861.full.pdf", "details": "AB Chapman, T Panadero, R Dalrymple, A Cohen\u2026 - medRxiv, 2024", "abstract": "Food insecurity is an important social risk factor that is directly linked to patient health and well-being. The Department of Veterans Affairs (VA) aims to identify and resolve food insecurity through social and clinical interventions. However, evaluating the \u2026"}, {"title": "CALM: Context Augmentation with Large Language Model for Named Entity Recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-72437-4_16", "details": "T Luiggi, T Herserant, T Tran, L Soulier, V Guigue - \u2026 on Theory and Practice of Digital \u2026, 2024", "abstract": "In prior research on Named Entity Recognition (NER), the focus has been on addressing challenges arising from data scarcity and overfitting, particularly in the context of increasingly complex transformer-based architectures. A framework based \u2026"}, {"title": "Language Models Benefit from Preparation with Elicited Knowledge", "link": "https://arxiv.org/pdf/2409.01345", "details": "J Yu, H An, LK Schubert - arXiv preprint arXiv:2409.01345, 2024", "abstract": "The zero-shot chain of thought (CoT) approach is often used in question answering (QA) by language models (LMs) for tasks that require multiple reasoning steps, typically enhanced by the prompt\" Let's think step by step.\" However, some QA tasks \u2026"}, {"title": "On-device Collaborative Language Modeling via a Mixture of Generalists and Specialists", "link": "https://arxiv.org/pdf/2409.13931", "details": "D Fan, B Messmer, M Jaggi - arXiv preprint arXiv:2409.13931, 2024", "abstract": "We target on-device collaborative fine-tuning of Large Language Models (LLMs) by adapting a Mixture of Experts (MoE) architecture, where experts are Low-Rank Adaptation (LoRA) modules. In conventional MoE approaches, experts develop into \u2026"}, {"title": "Investigating Layer Importance in Large Language Models", "link": "https://arxiv.org/pdf/2409.14381", "details": "Y Zhang, Y Dong, K Kawaguchi - arXiv preprint arXiv:2409.14381, 2024", "abstract": "Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in \u2026"}]
