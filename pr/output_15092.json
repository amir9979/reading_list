[{"title": "Text to image generation with bidirectional multiway transformers", "link": "https://ieeexplore.ieee.org/iel8/10750449/10901938/10960474.pdf", "details": "H Bao, L Dong, S Piao, F Wei - Computational Visual Media, 2025", "abstract": "In this study, we explore the potential of Multiway Transformers for text-to-image generation to achieve performance improvements through a concise and efficient decoupled model design and the inference efficiency provided by bidirectional \u2026"}, {"title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models", "link": "https://arxiv.org/pdf/2504.02821", "details": "M Pach, S Karthik, Q Bouniot, S Belongie, Z Akata - arXiv preprint arXiv:2504.02821, 2025", "abstract": "Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce \u2026"}, {"title": "SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders", "link": "https://arxiv.org/pdf/2503.14530", "details": "Q Li, J Geng, D Zhu, F Cai, C Lyu, F Karray - arXiv preprint arXiv:2503.14530, 2025", "abstract": "Unlearning methods for vision-language models (VLMs) have primarily adapted techniques from large language models (LLMs), relying on weight updates that demand extensive annotated forget sets. Moreover, these methods perform \u2026"}, {"title": "Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression", "link": "https://arxiv.org/pdf/2504.07389", "details": "H Xiao, YL Sung, E Stengel-Eskin, M Bansal - arXiv preprint arXiv:2504.07389, 2025", "abstract": "Post-training quantization (PTQ) reduces a model's memory footprint by mapping full precision weights into low bit weights without costly retraining, but can degrade its downstream performance especially in low 2-to 3-bit settings. We develop a new \u2026"}]
