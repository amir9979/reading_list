[{"title": "Towards Artwork Explanation in Large-scale Vision Language Models", "link": "https://aclanthology.org/2024.acl-short.65.pdf", "details": "K Hayashi, Y Sakai, H Kamigaito, K Hayashi\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Abstract Large-scale Vision-Language Models (LVLMs) output text from images and instructions, demonstrating advanced capabilities in text generation and comprehension. However, it has not been clarified to what extent LVLMs understand \u2026"}, {"title": "Prompt Learning with Extended Kalman Filter for Pre-trained Language Models", "link": "https://www.ijcai.org/proceedings/2024/0492.pdf", "details": "Q Li, X Xie, C Wang, SK Zhou", "abstract": "Prompt learning has gained popularity as a means to leverage the knowledge embedded in pre-trained language models (PLMs) for NLP tasks while using a limited number of trainable parameters. While it has shown promise in tasks like \u2026"}, {"title": "End-to-End Clustering Enhanced Contrastive Learning for Radiology Reports Generation", "link": "https://ieeexplore.ieee.org/abstract/document/10663478/", "details": "X Liu, J Xin, Q Shen, C Li, Z Huang, Z Wang - IEEE Transactions on Emerging Topics \u2026, 2024", "abstract": "With the rapid growth of medical imaging data, radiologists must dedicate a significant amount of time to report writing. Automated generation of radiology reports not only alleviates the heavy workload of physicians but, more importantly, can \u2026"}, {"title": "Few-shot Relation Extraction through Prompt with Relation Information and Multi-level Contrastive Learning", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10662978.pdf", "details": "Y Dong, R Yang, J Liu, X Qin - IEEE Access, 2024", "abstract": "Few-shot relation extraction uses only limited labeled data to predict relations between entities. Recently, several studies have introduced prompts to better guide models in understanding relations between entities. Although effective, these \u2026"}, {"title": "Language Models Pre-training", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-65647-7_2.pdf", "details": "U Kamath, K Keenan, G Somers, S Sorenson - Large Language Models: A Deep Dive \u2026, 2024", "abstract": "Pre-training forms the foundation for LLMs' capabilities. LLMs gain vital language comprehension and generative language skills by using large-scale datasets. The size and quality of these datasets are essential for maximizing LLMs' potential. It is \u2026"}, {"title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.06663", "details": "K Sun, M Dredze - arXiv preprint arXiv:2408.06663, 2024", "abstract": "The development of large language models leads to the formation of a pre-train-then- align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream \u2026"}]
