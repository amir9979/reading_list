'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [How does Architecture Influence the Base Capabilities '
[{"title": "Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering", "link": "https://arxiv.org/html/2403.04890v1", "details": "O Gramopadhye, SS Nachane, P Chanda\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language models (LLMs) have demonstrated significant potential in transforming healthcare by automating tasks such as clinical documentation, information retrieval, and decision support. In this aspect, carefully engineered \u2026"}, {"title": "Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models", "link": "https://arxiv.org/pdf/2403.12966", "details": "Z Liu, Y Dong, Y Rao, J Zhou, J Lu - arXiv preprint arXiv:2403.12966, 2024", "abstract": "In the realm of vision-language understanding, the proficiency of models in interpreting and reasoning over visual content has become a cornerstone for numerous applications. However, it is challenging for the visual encoder in Large \u2026"}, {"title": "Language models scale reliably with over-training and on downstream tasks", "link": "https://arxiv.org/pdf/2403.08540", "details": "SY Gadre, G Smyrnis, V Shankar, S Gururangan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute \u2026"}, {"title": "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models", "link": "https://arxiv.org/html/2403.02178v1", "details": "C Chen, X Wang, TE Lin, A Lv, Y Wu, X Gao, JR Wen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains. Earlier fine- tuning approaches sought to mitigate this by leveraging more precise supervisory \u2026"}, {"title": "Debiasing Large Visual Language Models", "link": "https://arxiv.org/pdf/2403.05262", "details": "YF Zhang, W Yu, Q Wen, X Wang, Z Zhang, L Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the realms of computer vision and natural language processing, Large Vision- Language Models (LVLMs) have become indispensable tools, proficient in generating textual descriptions based on visual inputs. Despite their advancements \u2026"}, {"title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models", "link": "https://arxiv.org/pdf/2403.08281", "details": "N Ding, Y Chen, G Cui, X Lv, R Xie, B Zhou, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three \u2026"}, {"title": "Enhancing Information Maximization with Distance-Aware Contrastive Learning for Source-Free Cross-Domain Few-Shot Learning", "link": "https://arxiv.org/html/2403.01966v1", "details": "H Xu, L Liu, S Zhi, S Fu, Z Su, MM Cheng, Y Liu - IEEE Transactions on Image \u2026, 2024", "abstract": "Existing Cross-Domain Few-Shot Learning (CDFSL) methods require access to source domain data to train a model in the pre-training phase. However, due to increasing concerns about data privacy and the desire to reduce data transmission \u2026"}, {"title": "Monotonic Paraphrasing Improves Generalization of Language Model Prompting", "link": "https://arxiv.org/html/2403.16038v1", "details": "Q Liu, F Wang, N Xu, T Yan, T Meng, M Chen - arXiv preprint arXiv:2403.16038, 2024", "abstract": "Performance of large language models (LLMs) may vary with different prompts or instructions of even the same task. One commonly recognized factor for this phenomenon is the model's familiarity with the given prompt or instruction, which is \u2026"}, {"title": "DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.08857", "details": "M Huang, Y Long, X Deng, R Chu, J Xiong, X Liang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Text-to-image (T2I) generation models have significantly advanced in recent years. However, effective interaction with these models is challenging for average users due to the need for specialized prompt engineering knowledge and the inability to \u2026"}]
