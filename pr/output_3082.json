[{"title": "Zero-Shot Out-of-Distribution Detection with Outlier Label Exposure", "link": "https://arxiv.org/pdf/2406.01170", "details": "C Ding, G Pang - arXiv preprint arXiv:2406.01170, 2024", "abstract": "As vision-language models like CLIP are widely applied to zero-shot tasks and gain remarkable performance on in-distribution (ID) data, detecting and rejecting out-of- distribution (OOD) inputs in the zero-shot setting have become crucial for ensuring \u2026"}, {"title": "UP2ME: Univariate Pre-training to Multivariate Fine-tuning as a General-purpose Framework for Multivariate Time Series Analysis", "link": "https://openreview.net/pdf%3Fid%3DaR3uxWlZhX", "details": "Y Zhang, M Liu, S Zhou, J Yan - Forty-first International Conference on Machine \u2026", "abstract": "Despite the success of self-supervised pre-training in texts and images, applying it to multivariate time series (MTS) falls behind tailored methods for tasks like forecasting, imputation and anomaly detection. We propose a general-purpose framework \u2026"}, {"title": "Generative Pre-Trained Diffusion Paradigm for Zero-Shot Time Series Forecasting", "link": "https://arxiv.org/pdf/2406.02212", "details": "J Yang, T Dai, N Li, J Wu, P Liu, J Li, J Bao, H Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, generative pre-trained paradigms such as Large Language Models (LLMs) and Large Vision Models (LVMs) have achieved revolutionary advancements and widespread real-world applications. Particularly, the emergence of pre-trained \u2026"}, {"title": "Demonstration Augmentation for Zero-shot In-context Learning", "link": "https://arxiv.org/pdf/2406.01224", "details": "Y Su, Y Tai, Y Ji, J Li, B Yan, M Zhang - arXiv preprint arXiv:2406.01224, 2024", "abstract": "Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates. However, many studies \u2026"}, {"title": "Generative Data Augmentation with Liveness Information Preserving for Face Anti-Spoofing", "link": "https://dl.acm.org/doi/abs/10.1145/3652583.3658078", "details": "C Chen, Y Li, J Zhang, J Liu, C Wang - \u2026 of the 2024 International Conference on \u2026, 2024", "abstract": "Face anti-spoofing is a critical aspect of ensuring security in the context of human- robot interaction and collaboration. Recently, disentangled-based data augmentation methods have achieved great success in face anti-spoofing tasks. The underlying \u2026"}, {"title": "Multivariate Time Series Modeling and Forecasting with Parallelized Convolution and Decomposed Sparse-Transformer", "link": "https://ieeexplore.ieee.org/abstract/document/10552140/", "details": "S Ma, YB Zhao, Y Kang, P Bai - IEEE Transactions on Artificial Intelligence, 2024", "abstract": "Many real-world scenarios require accurate predictions of time series, especially in the case of long sequence time-series forecasting (LSTF), such as predicting traffic flow and electricity consumption. However, existing time series prediction models \u2026"}, {"title": "A Multi-scale Parallel Unsupervised Model for Multivariate Time Series Anomaly Detection", "link": "https://link.springer.com/chapter/10.1007/978-3-031-63223-5_18", "details": "J Bao, H Gao, C Zhang, W Jia, J Gao, T Yang - IFIP International Conference on \u2026, 2024", "abstract": "Anomaly detection for multivariate time series data is of great significance for practical applications. The existing anomaly detection methods mainly adopt a fixed length sliding window to extract data features and perform deep learning training \u2026"}, {"title": "LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices", "link": "https://arxiv.org/pdf/2406.08765", "details": "R Jin, Q Xu, M Wu, Y Xu, D Li, X Li, Z Chen - arXiv preprint arXiv:2406.08765, 2024", "abstract": "Limited by the scale and diversity of time series data, the neural networks trained on time series data often overfit and show unsatisfacotry performances. In comparison, large language models (LLMs) recently exhibit impressive generalization in diverse \u2026"}, {"title": "Flat Posterior Does Matter For Bayesian Transfer Learning", "link": "https://arxiv.org/pdf/2406.15664", "details": "S Lim, J Yeom, S Kim, H Byun, J Kang, Y Jung, J Jung\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The large-scale pre-trained neural network has achieved notable success in enhancing performance for downstream tasks. Another promising approach for generalization is Bayesian Neural Network (BNN), which integrates Bayesian \u2026"}]
