[{"title": "Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation", "link": "https://arxiv.org/pdf/2504.02438", "details": "C Cheng, J Guan, W Wu, R Yan - arXiv preprint arXiv:2504.02438, 2025", "abstract": "Long-form video processing fundamentally challenges vision-language models (VLMs) due to the high computational costs of handling extended temporal sequences. Existing token pruning and feature merging methods often sacrifice \u2026"}, {"title": "Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning", "link": "https://arxiv.org/pdf/2504.05632", "details": "S Kabra, A Jha, C Reddy - arXiv preprint arXiv:2504.05632, 2025", "abstract": "Recent advances in large-scale generative language models have shown that reasoning capabilities can significantly improve model performance across a variety of tasks. However, the impact of reasoning on a model's ability to mitigate \u2026"}, {"title": "NNTile: a machine learning framework capable of training extremely large GPT language models on a single node", "link": "https://arxiv.org/pdf/2504.13236", "details": "A Mikhalev, A Katrutsa, K Sozykin, I Oseledets - arXiv preprint arXiv:2504.13236, 2025", "abstract": "This study presents an NNTile framework for training large deep neural networks in heterogeneous clusters. The NNTile is based on a StarPU library, which implements task-based parallelism and schedules all provided tasks onto all available \u2026"}, {"title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models", "link": "https://arxiv.org/pdf/2503.18931", "details": "Y Chen, L Meng, W Peng, Z Wu, YG Jiang - arXiv preprint arXiv:2503.18931, 2025", "abstract": "Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of \u2026"}, {"title": "Breaking the Encoder Barrier for Seamless Video-Language Understanding", "link": "https://arxiv.org/pdf/2503.18422", "details": "H Li, Y Zhang, L Guo, X Yue, J Liu - arXiv preprint arXiv:2503.18422, 2025", "abstract": "Most Video-Large Language Models (Video-LLMs) adopt an encoder-decoder framework, where a vision encoder extracts frame-wise features for processing by a language model. However, this approach incurs high computational costs \u2026"}, {"title": "Knowledge-Instruct: Effective Continual Pre-training from Limited Data using Instructions", "link": "https://arxiv.org/pdf/2504.05571", "details": "O Ovadia, M Brief, R Lemberg, E Sheetrit - arXiv preprint arXiv:2504.05571, 2025", "abstract": "While Large Language Models (LLMs) acquire vast knowledge during pre-training, they often lack domain-specific, new, or niche information. Continual pre-training (CPT) attempts to address this gap but suffers from catastrophic forgetting and \u2026"}, {"title": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection", "link": "https://arxiv.org/pdf/2504.00695", "details": "X Zhu, Z Gu, S Zheng, T Wang, T Li, H Feng, Y Xiao - arXiv preprint arXiv:2504.00695, 2025", "abstract": "Pre-training large language models (LLMs) necessitates enormous diverse textual corpora, making effective data selection a key challenge for balancing computational resources and model performance. Current methodologies primarily emphasize data \u2026"}, {"title": "Slow-fast architecture for video multi-modal large language models", "link": "https://arxiv.org/pdf/2504.01328%3F", "details": "M Shi, S Wang, CY Chen, J Jain, K Wang, J Xiong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using \u2026"}, {"title": "LLaVAction: evaluating and training multi-modal large language models for action recognition", "link": "https://arxiv.org/pdf/2503.18712", "details": "S Ye, H Qi, A Mathis, MW Mathis - arXiv preprint arXiv:2503.18712, 2025", "abstract": "Understanding human behavior requires measuring behavioral actions. Due to its complexity, behavior is best mapped onto a rich, semantic structure such as language. The recent development of multi-modal large language models (MLLMs) \u2026"}]
