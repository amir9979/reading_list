[{"title": "Exploring Coding Spot: Understanding Parametric Contributions to LLM Coding Performance", "link": "https://arxiv.org/pdf/2412.07113", "details": "D Kim, M Kim, YC Chun, C Park, H Lim - arXiv preprint arXiv:2412.07113, 2024", "abstract": "Large Language Models (LLMs) have demonstrated notable proficiency in both code generation and comprehension across multiple programming languages. However, the mechanisms underlying this proficiency remain underexplored, particularly with \u2026"}, {"title": "Political-LLM: Large Language Models in Political Science", "link": "https://arxiv.org/pdf/2412.06864", "details": "L Li, J Li, C Chen, F Gui, H Yang, C Yu, Z Wang, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent years, large language models (LLMs) have been widely adopted in political science tasks such as election prediction, sentiment analysis, policy impact assessment, and misinformation detection. Meanwhile, the need to systematically \u2026"}, {"title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases", "link": "https://arxiv.org/pdf/2412.04862", "details": "LG Research, S An, K Bae, E Choi, K Choi, SJ Choi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8 B, and 2.4 B. These models feature several \u2026"}, {"title": "AutoReason: Automatic Few-Shot Reasoning Decomposition", "link": "https://arxiv.org/pdf/2412.06975", "details": "A Sevinc, A Gumus - arXiv preprint arXiv:2412.06975, 2024", "abstract": "Chain of Thought (CoT) was introduced in recent research as a method for improving step-by-step reasoning in Large Language Models. However, CoT has limited applications such as its need for hand-crafted few-shot exemplar prompts and no \u2026"}, {"title": "MAPLE: A Framework for Active Preference Learning Guided by Large Language Models", "link": "https://arxiv.org/pdf/2412.07207", "details": "S Mahmud, M Nakamura, S Zilberstein - arXiv preprint arXiv:2412.07207, 2024", "abstract": "The advent of large language models (LLMs) has sparked significant interest in using natural language for preference learning. However, existing methods often suffer from high computational burdens, taxing human supervision, and lack of \u2026"}, {"title": "Dynamic Ensemble Reasoning for LLM Experts", "link": "https://arxiv.org/pdf/2412.07448", "details": "J Hu, Y Wang, S Zhang, K Zhou, G Chen, Y Hu, B Xiao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Ensemble reasoning for the strengths of different LLM experts is critical to achieving consistent and satisfactory performance on diverse inputs across a wide range of tasks. However, existing LLM ensemble methods are either computationally \u2026"}, {"title": "SpecFuse: Ensembling Large Language Models via Next-Segment Prediction", "link": "https://arxiv.org/pdf/2412.07380", "details": "B Lv, C Tang, Y Zhang, X Liu, Y Yu, P Luo - arXiv preprint arXiv:2412.07380, 2024", "abstract": "Ensembles of generative large language models (LLMs) can integrate the strengths of different LLMs to compensate for the limitations of individual models. However, recent work has focused on training an additional fusion model to combine complete \u2026"}]
