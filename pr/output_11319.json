[{"title": "Representation Learning of Lab Values via Masked AutoEncoder", "link": "https://arxiv.org/pdf/2501.02648", "details": "D Restrepo, C Wu, Y Jia, JK Sun, J Gallifant, CG Bielick\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Accurate imputation of missing laboratory values in electronic health records (EHRs) is critical to enable robust clinical predictions and reduce biases in AI systems in healthcare. Existing methods, such as variational autoencoders (VAEs) and decision \u2026"}, {"title": "Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation", "link": "https://arxiv.org/pdf/2501.03225", "details": "Y Zhang, Y Su, Y Liu, X Wang, J Burgess, E Sui\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to \u2026"}, {"title": "Self-Supervised Learning for Detecting AI-Generated Faces as Anomalies", "link": "https://arxiv.org/pdf/2501.02207", "details": "M Zou, B Yu, Y Zhan, K Ma - arXiv preprint arXiv:2501.02207, 2025", "abstract": "The detection of AI-generated faces is commonly approached as a binary classification task. Nevertheless, the resulting detectors frequently struggle to adapt to novel AI face generators, which evolve rapidly. In this paper, we describe an \u2026"}, {"title": "3VL: Using Trees to Improve Vision-Language Models' Interpretability", "link": "https://ieeexplore.ieee.org/abstract/document/10829542/", "details": "N Yellinek, L Karlinsky, R Giryes - IEEE Transactions on Image Processing, 2025", "abstract": "Vision-Language models (VLMs) have proven to be effective at aligning image and text representations, producing superior zero-shot results when transferred to many downstream tasks. However, these representations suffer from some key \u2026"}, {"title": "ICONS: Influence Consensus for Vision-Language Data Selection", "link": "https://arxiv.org/pdf/2501.00654", "details": "X Wu, M Xia, R Shao, Z Deng, PW Koh, O Russakovsky - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual Instruction Tuning typically requires a large amount of vision-language training data. This data often containing redundant information that increases computational costs without proportional performance gains. In this work, we \u2026"}]
