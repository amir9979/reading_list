[{"title": "Robustness-Privacy Trade-Off In Bayesian Neural Networks", "link": "https://www.imperial.ac.uk/media/imperial-college/faculty-of-engineering/computing/public/2324-ug-projects/Ghitu-Mihnea---Final-Project-Report.pdf", "details": "M Ghitu, M Wicker, W Knottenbelt - 2024", "abstract": "Substantial developments have recently been made to devise provable methods that ensure the trustworthiness of deep neural networks. Most of these pieces of work study properties that constitute the trustworthy aspect individually, often isolating \u2026"}, {"title": "Federated Distillation for Medical Image Classification: Towards Trustworthy Computer-Aided Diagnosis", "link": "https://arxiv.org/pdf/2407.02261", "details": "S Ren, Y Hu, S Chen, G Wang - arXiv preprint arXiv:2407.02261, 2024", "abstract": "Medical image classification plays a crucial role in computer-aided clinical diagnosis. While deep learning techniques have significantly enhanced efficiency and reduced costs, the privacy-sensitive nature of medical imaging data complicates centralized \u2026"}, {"title": "FedPA: Generator-Based Heterogeneous Federated Prototype Adversarial Learning", "link": "https://ieeexplore.ieee.org/abstract/document/10579863/", "details": "L Jiang, X Wang, X Yang, J Shu, H Lin, X Yi - IEEE Transactions on Dependable and \u2026, 2024", "abstract": "Federated Learning is an emerging distributed algorithm that is designed to collaboratively train the global model without accessing clients' private data. However, heterogeneity of data among clients leads to significant degradation in \u2026"}, {"title": "Revisiting Attention for Multivariate Time Series Forecasting", "link": "https://arxiv.org/pdf/2407.13806", "details": "H Wu - arXiv preprint arXiv:2407.13806, 2024", "abstract": "Current Transformer methods for Multivariate Time-Series Forecasting (MTSF) are all based on the conventional attention mechanism. They involve sequence embedding and performing a linear projection of Q, K, and V, and then computing attention within \u2026"}, {"title": "One Process Spatiotemporal Learning of Transformers via Vcls Token for Multivariate Time Series Forecasting", "link": "https://www.researchgate.net/profile/Jingzehua-Xu/publication/381739943_One_Process_Spatiotemporal_Learning_of_Transformers_via_Vcls_Token_for_Multivariate_Time_Series_Forecasting/links/667d11d5f3b61c4e2c8ebd08/One-Process-Spatiotemporal-Learning-of-Transformers-via-Vcls-Token-for-Multivariate-Time-Series-Forecasting.pdf", "details": "T Cai, H Wu, D Niu, X Xia, J Jiang, J Xu", "abstract": "Previous Transformer-based models for multivariate time series forecasting mainly focus on temporal dependence learning and neglect the association between variables. The recent method of adding Attention on spatial (variate) tokens before or \u2026"}]
