[{"title": "Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation", "link": "https://arxiv.org/pdf/2407.03056", "details": "M Mistretta, A Baldrati, M Bertini, AD Bagdanov - arXiv preprint arXiv:2407.03056, 2024", "abstract": "Vision-Language Models (VLMs) demonstrate remarkable zero-shot generalization to unseen tasks, but fall short of the performance of supervised methods in generalizing to downstream tasks with limited data. Prompt learning is emerging as a \u2026"}, {"title": "FlowCon: Out-of-Distribution Detection using Flow-Based Contrastive Learning", "link": "https://arxiv.org/pdf/2407.03489", "details": "S Aathreya, S Canavan - arXiv preprint arXiv:2407.03489, 2024", "abstract": "Identifying Out-of-distribution (OOD) data is becoming increasingly critical as the real- world applications of deep learning methods expand. Post-hoc methods modify softmax scores fine-tuned on outlier data or leverage intermediate feature layers to \u2026"}, {"title": "FedGK: Communication-Efficient Federated Learning through Group-Guided Knowledge Distillation", "link": "https://dl.acm.org/doi/pdf/10.1145/3674973", "details": "W Zhang, XL Liu, S Tarkoma - ACM Transactions on Internet Technology, 2024", "abstract": "Federated learning (FL) empowers a cohort of participating devices to contribute collaboratively to a global neural network model, ensuring that their training data remains private and stored locally. Despite its advantages in computational efficiency \u2026"}]
