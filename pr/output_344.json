'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [MFORT-QA: Multi-hop Few-shot Open Rich Table Question '
[{"title": "Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation", "link": "https://arxiv.org/pdf/2403.07860", "details": "S Zhao, S Hao, B Zi, H Xu, KYK Wong - arXiv preprint arXiv:2403.07860, 2024", "abstract": "Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding \u2026"}, {"title": "Synth $^ 2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings", "link": "https://arxiv.org/pdf/2403.07750", "details": "S Sharifzadeh, C Kaplanis, S Pathak, D Kumaran, A Ilic\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models \u2026"}, {"title": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models", "link": "https://arxiv.org/html/2402.19427v1", "details": "S De, SL Smith, A Fernando, A Botev\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated \u2026"}, {"title": "Aligning Large and Small Language Models via Chain-of-Thought Reasoning", "link": "https://aclanthology.org/2024.eacl-long.109.pdf", "details": "L Ranaldi, A Freitas - Proceedings of the 18th Conference of the European \u2026, 2024", "abstract": "Abstract Chain-of-Thought (CoT) prompting empowersthe reasoning abilities of Large Language Models (LLMs), eliciting them to solve complexreasoning tasks in a step-wise manner. However, these capabilities appear only in models with billions of \u2026"}, {"title": "Multivariate Time Series Forecasting with Causal-Temporal Attention Network", "link": "https://ieeexplore.ieee.org/abstract/document/10448031/", "details": "W Liu, Y He, J Guan, S Zhou - ICASSP 2024-2024 IEEE International Conference on \u2026, 2024", "abstract": "The task of multivariate time series (MTS) forecasting has attracted much attention in recent years. However, most existing methods overlook the causal relationship among different variables, which may lead to inaccurate forecasting results. In this \u2026"}, {"title": "BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English", "link": "https://arxiv.org/pdf/2403.10900", "details": "S Shafayat, HM Hasan, MRC Mahim, RA Putri\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this study, we introduce BEnQA, a dataset comprising parallel Bengali and English exam questions for middle and high school levels in Bangladesh. Our dataset consists of approximately 5K questions covering several subjects in science with \u2026"}, {"title": "$\\mathbf {(N, K)} $-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model", "link": "https://arxiv.org/html/2403.07191v1", "details": "Y Zhang, L Chen, B Liu, Y Yang, Q Cui, Y Tao, H Yang - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these \u2026"}, {"title": "Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models", "link": "https://arxiv.org/html/2403.19631v1", "details": "Y Shi, Q Tan, X Wu, S Zhong, K Zhou, N Liu - arXiv preprint arXiv:2403.19631, 2024", "abstract": "Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more \u2026"}, {"title": "ZVQAF: Zero-shot visual question answering with feedback from large language models", "link": "https://www.sciencedirect.com/science/article/pii/S0925231224002765", "details": "C Liu, C Wang, Y Peng, Z Li - Neurocomputing, 2024", "abstract": "Due to the prominent zero-shot generalization in new language tasks shown by large language models (LLMs), applying LLMs for zero-shot visual question answering (VQA) has been a new trend. However, most prior approaches directly use off-the \u2026"}]
