[{"title": "Noise is an Efficient Learner for Zero-Shot Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06019", "details": "R Imam, A Hanif, J Zhang, KW Dawoud\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, test-time adaptation has garnered attention as a method for tuning models without labeled data. The conventional modus operandi for adapting pre-trained vision-language models (VLMs) during test-time primarily focuses on tuning \u2026"}, {"title": "Learning Conformal Abstention Policies for Adaptive Risk Management in Large Language and Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06884", "details": "S Tayebati, D Kumar, N Darabi, D Jayasuriya\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language and Vision-Language Models (LLMs/VLMs) are increasingly used in safety-critical applications, yet their opaque decision-making complicates risk assessment and reliability. Uncertainty quantification (UQ) helps assess prediction \u2026"}, {"title": "SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities", "link": "https://arxiv.org/pdf/2502.12025%3F", "details": "F Jiang, Z Xu, Y Li, L Niu, Z Xiang, B Li, BY Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently \u2026"}, {"title": "Modular Prompt Learning Improves Vision-Language Models", "link": "https://arxiv.org/pdf/2502.14125", "details": "Z Huang, T Pedapati, PY Chen, J Gao - arXiv preprint arXiv:2502.14125, 2025", "abstract": "Pre-trained vision-language models are able to interpret visual concepts and language semantics. Prompt learning, a method of constructing prompts for text encoders or image encoders, elicits the potentials of pre-trained models and readily \u2026"}, {"title": "CSP-DCPE: Category-Specific Prompt with Deep Contextual Prompt Enhancement for Vision\u2013Language Models", "link": "https://www.mdpi.com/2079-9292/14/4/673", "details": "C Wu, Y Wu, Q Xu, X Zi - Electronics, 2025", "abstract": "Recently, prompt learning has emerged as a viable technique for fine-tuning pre- trained vision\u2013language models (VLMs). The use of prompts allows pre-trained VLMs to be quickly adapted to specific downstream tasks, bypassing the necessity to \u2026"}, {"title": "Adversary-Aware DPO: Enhancing Safety Alignment in Vision Language Models via Adversarial Training", "link": "https://arxiv.org/pdf/2502.11455", "details": "F Weng, J Lou, J Feng, M Huang, W Wang - arXiv preprint arXiv:2502.11455, 2025", "abstract": "Safety alignment is critical in pre-training large language models (LLMs) to generate responses aligned with human values and refuse harmful queries. Unlike LLM, the current safety alignment of VLMs is often achieved with post-hoc safety fine-tuning \u2026"}, {"title": "Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning", "link": "https://arxiv.org/pdf/2502.11751", "details": "Y Pang, B Yang, H Tu, Y Cao, Z Zhang - arXiv preprint arXiv:2502.11751, 2025", "abstract": "Although Large Language Models (LLMs) excel in reasoning and generation for language tasks, they are not specifically designed for multimodal challenges. Training Multimodal Large Language Models (MLLMs), however, is resource \u2026"}, {"title": "Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Ownership Verification with Reasoning", "link": "https://arxiv.org/pdf/2502.10440", "details": "J Guo, Y Li, R Chen, Y Wu, C Liu, Y Chen, H Huang - arXiv preprint arXiv:2502.10440, 2025", "abstract": "Large language models (LLMs) are increasingly integrated into real-world applications through retrieval-augmented generation (RAG) mechanisms to supplement their responses with up-to-date and domain-specific knowledge \u2026"}, {"title": "Evaluating the Meta-and Object-Level Reasoning of Large Language Models for Question Answering", "link": "https://arxiv.org/pdf/2502.10338%3F", "details": "N Ferguson, L Guillou, A Bundy, K Nuamah - arXiv preprint arXiv:2502.10338, 2025", "abstract": "Large Language Models (LLMs) excel in natural language tasks but still face challenges in Question Answering (QA) tasks requiring complex, multi-step reasoning. We outline the types of reasoning required in some of these tasks, and \u2026"}]
