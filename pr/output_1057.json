'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Refining Pre-trained Language Models for Domain Adapta'
[{"title": "Automated Evaluation of Large Vision-Language Models on Self-driving Corner Cases", "link": "https://arxiv.org/pdf/2404.10595", "details": "Y Li, W Zhang, K Chen, Y Liu, P Li, R Gao, L Hong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs), due to the remarkable visual reasoning ability to understand images and videos, have received widespread attention in the autonomous driving domain, which significantly advances the development of \u2026"}, {"title": "CVTN: Cross Variable and Temporal Integration for Time Series Forecasting", "link": "https://arxiv.org/pdf/2404.18730", "details": "H Zhou, Y Chen - arXiv preprint arXiv:2404.18730, 2024", "abstract": "In multivariate time series forecasting, the Transformer architecture encounters two significant challenges: effectively mining features from historical sequences and avoiding overfitting during the learning of temporal dependencies. To tackle these \u2026"}, {"title": "A Primer on the Inner Workings of Transformer-based Language Models", "link": "https://arxiv.org/pdf/2405.00208", "details": "J Ferrando, G Sarti, A Bisazza, MR Costa-juss\u00e0 - arXiv preprint arXiv:2405.00208, 2024", "abstract": "The rapid progress of research aimed at interpreting the inner workings of advanced language models has highlighted a need for contextualizing the insights gained from years of work in this area. This primer provides a concise technical introduction to the \u2026"}, {"title": "ATG: Benchmarking Automated Theorem Generation for Generative Language Models", "link": "https://eleanor-h.github.io/publication/confnaacl-2024-atg/confnaacl-2024-atg.pdf", "details": "X Lin, Q Cao, Y Huang, Z Yang, Z Liu, Z Li, X Liang15", "abstract": "Humans can develop new theorems to explore broader and more complex mathematical results. While current generative language models (LMs) have achieved significant improvement in automatically proving theorems, their ability to \u2026"}, {"title": "Can Language Models Solve Olympiad Programming?", "link": "https://arxiv.org/pdf/2404.10952", "details": "Q Shi, M Tang, K Narasimhan, S Yao - arXiv preprint arXiv:2404.10952, 2024", "abstract": "Computing olympiads contain some of the most challenging problems for humans, requiring complex algorithmic reasoning, puzzle solving, in addition to generating efficient code. However, it has been understudied as a domain to evaluate language \u2026"}, {"title": "Causal Evaluation of Language Models", "link": "https://arxiv.org/pdf/2405.00622", "details": "S Chen, B Peng, M Chen, R Wang, M Xu, X Zeng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for \u2026"}, {"title": "HLAT: High-quality Large Language Model Pre-trained on AWS Trainium", "link": "https://arxiv.org/pdf/2404.10630", "details": "H Fan, H Zhou, G Huang, P Raman, X Fu, G Gupta\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Getting large language models (LLMs) to perform well on the downstream tasks requires pre-training over trillions of tokens. This typically demands a large number of powerful computational devices in addition to a stable distributed training \u2026"}, {"title": "Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models", "link": "https://arxiv.org/pdf/2404.02657", "details": "T Wu, C Tao, J Wang, Z Zhao, N Wong - arXiv preprint arXiv:2404.02657, 2024", "abstract": "Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the \u2026"}, {"title": "Self-supervised Pre-training of Text Recognizers", "link": "https://arxiv.org/pdf/2405.00420", "details": "M Ki\u0161\u0161, M Hradi\u0161 - arXiv preprint arXiv:2405.00420, 2024", "abstract": "In this paper, we investigate self-supervised pre-training methods for document text recognition. Nowadays, large unlabeled datasets can be collected for many research tasks, including text recognition, but it is costly to annotate them. Therefore, methods \u2026"}]
