[{"title": "How do language models learn facts? Dynamics, curricula and hallucinations", "link": "https://arxiv.org/pdf/2503.21676", "details": "N Zucchet, J Bornschein, S Chan, A Lampinen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall \u2026"}, {"title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language Models", "link": "https://arxiv.org/pdf/2503.18923", "details": "M Cao, P Hu, Y Wang, J Gu, H Tang, H Zhao, J Dong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in Large Video Language Models (LVLMs) have highlighted their potential for multi-modal understanding, yet evaluating their factual grounding in video contexts remains a critical unsolved challenge. To address this gap, we \u2026"}, {"title": "QA-Calibration of language model confidence scores", "link": "https://www.amazon.science/publications/qa-calibration-of-language-model-confidence-scores", "details": "A Mastakouri, E Kirschbaum, S Kasiviswanathan\u2026 - 2025", "abstract": "To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim \u2026"}, {"title": "Towards reasoning era: A survey of long chain-of-thought for reasoning large language models", "link": "https://arxiv.org/pdf/2503.09567%3F", "details": "Q Chen, L Qin, J Liu, D Peng, J Guan, P Wang, M Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains like mathematics and coding. A central factor in their success lies \u2026"}, {"title": "DAST: Difficulty-Aware Self-Training on Large Language Models", "link": "https://arxiv.org/pdf/2503.09029", "details": "B Xue, Q Zhu, H Wang, R Wang, S Wang, H Xu, F Mi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Present Large Language Models (LLM) self-training methods always under-sample on challenging queries, leading to inadequate learning on difficult problems which limits LLMs' ability. Therefore, this work proposes a difficulty-aware self-training \u2026"}, {"title": "GRU: Mitigating the Trade-off between Unlearning and Retention for Large Language Models", "link": "https://arxiv.org/pdf/2503.09117", "details": "Y Wang, Q Wang, F Liu, W Huang, Y Du, X Du, B Han - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language model (LLM) unlearning has demonstrated its essential role in removing privacy and copyright-related responses, crucial for their legal and safe applications. However, the pursuit of complete unlearning often comes with \u2026"}, {"title": "Efficient and explainable sequential recommendation with language model", "link": "https://drive.google.com/file/d/11rlrXoCrYwH9mIJCDfw2gHwD8PCImjoK/view", "details": "Z Li, L Zou, C Ma, C Li - Information Processing & Management, 2025", "abstract": "Motivated by the outstanding success of large language models (LLMs) in a broad spectrum of NLP tasks, applying them for explainable recommendation become a cutting-edge recently. However, due to the inherent inconsistency in the information \u2026"}, {"title": "CEFW: A Comprehensive Evaluation Framework for Watermark in Large Language Models", "link": "https://arxiv.org/pdf/2503.20802", "details": "S Zhang, B Cheng, J Han, Y Chen, Z Wu, C Li, P Gu - arXiv preprint arXiv:2503.20802, 2025", "abstract": "Text watermarking provides an effective solution for identifying synthetic text generated by large language models. However, existing techniques often focus on satisfying specific criteria while ignoring other key aspects, lacking a unified \u2026"}, {"title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models", "link": "https://arxiv.org/pdf/2503.19540", "details": "D Jung, S Lee, H Moon, C Park, H Lim - arXiv preprint arXiv:2503.19540, 2025", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced interactions between users and models. These advancements concurrently underscore the need for rigorous safety evaluations due to the \u2026"}]
