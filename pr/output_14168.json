[{"title": "DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models", "link": "https://arxiv.org/pdf/2503.13443", "details": "H Li, L Wang, C Wang, J Jiang, Y Peng, G Long - arXiv preprint arXiv:2503.13443, 2025", "abstract": "The Base-New Trade-off (BNT) problem universally exists during the optimization of CLIP-based prompt tuning, where continuous fine-tuning on base (target) classes leads to a simultaneous decrease of generalization ability on new (unseen) classes \u2026"}, {"title": "Improving Dietary Supplement Information Retrieval: Development of a Retrieval-Augmented Generation System With Large Language Models", "link": "https://www.jmir.org/2025/1/e67677/", "details": "Y Hou, JR Bishop, H Liu, R Zhang - Journal of Medical Internet Research, 2025", "abstract": "Background Dietary supplements (DSs) are widely used to improve health and nutrition, but challenges related to misinformation, safety, and efficacy persist due to less stringent regulations compared with pharmaceuticals. Accurate and reliable DS \u2026"}, {"title": "Zero-shot Medical Event Prediction Using a Generative Pre-trained Transformer on Electronic Health Records", "link": "https://arxiv.org/pdf/2503.05893", "details": "E Redekop, Z Wang, R Kulkarni, M Pleasure, A Chin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Longitudinal data in electronic health records (EHRs) represent an individuals clinical history through a sequence of codified concepts, including diagnoses, procedures, medications, and laboratory tests. Foundational models, such as \u2026"}, {"title": "Navigating Rifts in Human-LLM Grounding: Study and Benchmark", "link": "https://arxiv.org/pdf/2503.13975", "details": "O Shaikh, H Mozannar, G Bansal, A Fourney, E Horvitz - arXiv preprint arXiv \u2026, 2025", "abstract": "Language models excel at following instructions but often struggle with the collaborative aspects of conversation that humans naturally employ. This limitation in grounding--the process by which conversation participants establish mutual \u2026"}, {"title": "Mmrag: Multi-mode retrieval-augmented generation with large language models for biomedical in-context learning", "link": "https://arxiv.org/pdf/2502.15954", "details": "Z Zhan, J Wang, S Zhou, J Deng, R Zhang - arXiv preprint arXiv:2502.15954, 2025", "abstract": "Objective: To optimize in-context learning in biomedical natural language processing by improving example selection. Methods: We introduce a novel multi-mode retrieval- augmented generation (MMRAG) framework, which integrates four retrieval \u2026"}, {"title": "MedTransTab: Advancing Medical Cross-Table Tabular Data Generation", "link": "https://dl.acm.org/doi/abs/10.1145/3701551.3703501", "details": "Y Chen, Q Guo, S You, Z Li - Proceedings of the Eighteenth ACM International \u2026, 2025", "abstract": "In medical research, clinical trials are pivotal. While prospective clinical research provides a systematic approach to collecting patient data, it grapples with challenges like long durations, increased costs, and most crucially, data scarcity. To address \u2026"}, {"title": "PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation", "link": "https://arxiv.org/pdf/2502.15857", "details": "T Fan, G Ma, Y Song, L Fan, K Chen, Q Yang - arXiv preprint arXiv:2502.15857, 2025", "abstract": "Compressing Large Language Models (LLMs) into task-specific Small Language Models (SLMs) encounters two significant challenges: safeguarding domain-specific knowledge privacy and managing limited resources. To tackle these challenges, we \u2026"}]
