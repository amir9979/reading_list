[{"title": "Understanding Refusal in Language Models with Sparse Autoencoders", "link": "https://arxiv.org/pdf/2505.23556", "details": "WJ Yeo, N Prakash, C Neo, RKW Lee, E Cambria\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Refusal is a key safety behavior in aligned language models, yet the internal mechanisms driving refusals remain opaque. In this work, we conduct a mechanistic study of refusal in instruction-tuned LLMs using sparse autoencoders to identify \u2026", "entry_id": "http://arxiv.org/abs/2505.23556v1", "updated": "2025-05-29 15:33:39", "published": "2025-05-29 15:33:39", "authors": "Wei Jie Yeo;Nirmalendu Prakash;Clement Neo;Roy Ka-Wei Lee;Erik Cambria;Ranjan Satapathy", "summary": "Refusal is a key safety behavior in aligned language models, yet the internal\nmechanisms driving refusals remain opaque. In this work, we conduct a\nmechanistic study of refusal in instruction-tuned LLMs using sparse\nautoencoders to identify latent features that causally mediate refusal\nbehaviors. We apply our method to two open-source chat models and intervene on\nrefusal-related features to assess their influence on generation, validating\ntheir behavioral impact across multiple harmful datasets. This enables a\nfine-grained inspection of how refusal manifests at the activation level and\naddresses key research questions such as investigating upstream-downstream\nlatent relationship and understanding the mechanisms of adversarial\njailbreaking techniques. We also establish the usefulness of refusal features\nin enhancing generalization for linear probes to out-of-distribution\nadversarial samples in classification tasks. We open source our code in\nhttps://github.com/wj210/refusal_sae.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.23556v1;http://arxiv.org/pdf/2505.23556v1", "pdf_url": "http://arxiv.org/pdf/2505.23556v1"}, {"title": "AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer", "link": "https://arxiv.org/pdf/2505.16463", "details": "J Shan, J Wang, L Zhao, L Cai, H Zhang, I Liritzis - arXiv preprint arXiv:2505.16463, 2025", "abstract": "Recently, vision transformers (ViTs) have achieved excellent performance on vision tasks by measuring the global self-attention among the image patches. Given $ n $ patches, they will have quadratic complexity such as $\\mathcal {O}(n^ 2) $ and the \u2026", "entry_id": "http://arxiv.org/abs/2505.16463v2", "updated": "2025-05-25 08:10:35", "published": "2025-05-22 09:44:44", "authors": "Jiquan Shan;Junxiao Wang;Lifeng Zhao;Liang Cai;Hongyuan Zhang;Ioannis Liritzis", "summary": "Recently, vision transformers (ViTs) have achieved excellent performance on\nvision tasks by measuring the global self-attention among the image patches.\nGiven $n$ patches, they will have quadratic complexity such as\n$\\mathcal{O}(n^2)$ and the time cost is high when splitting the input image\nwith a small granularity. Meanwhile, the pivotal information is often randomly\ngathered in a few regions of an input image, some tokens may not be helpful for\nthe downstream tasks. To handle this problem, we introduce an anchor-based\nefficient vision transformer (AnchorFormer), which employs the anchor tokens to\nlearn the pivotal information and accelerate the inference. Firstly, by\nestimating the bipartite attention between the anchors and tokens, the\ncomplexity will be reduced from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(mn)$, where\n$m$ is an anchor number and $m < n$. Notably, by representing the anchors with\nthe neurons in a neural layer, we can differentiable learn these distributions\nand approximate global self-attention through the Markov process. Moreover, we\nextend the proposed model to three downstream tasks including classification,\ndetection, and segmentation. Extensive experiments show the effectiveness of\nour AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs\nreduction on ImageNet classification, 81.3% higher mAP on COCO detection under\ncomparable FLOPs, as compared to the current baselines.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.LG", "links": "http://arxiv.org/abs/2505.16463v2;http://arxiv.org/pdf/2505.16463v2", "pdf_url": "http://arxiv.org/pdf/2505.16463v2"}]
