[{"title": "HE-GAD: a behavior-enhanced contrastive learning framework for graph anomaly detection", "link": "https://link.springer.com/article/10.1007/s10994-025-06809-x", "details": "L Zheng, Q Song, Y Wang, Z Wang, X Li - Machine Learning, 2025", "abstract": "The effective detection of anomalies in graph data is crucial for various applications. Although existing contrastive learning methods have made some progress, they still struggle to handle diverse anomaly types concealed in the complex graph structures \u2026"}, {"title": "SDMG: Smoothing Your Diffusion Models for Powerful Graph Representation Learning", "link": "https://openreview.net/pdf%3Fid%3DlNyaQIJ5Z7", "details": "J Zhu, L He, C Gao, D Hou, Z Su, PS Yu, J Kurths\u2026 - Forty-second International \u2026", "abstract": "Diffusion probabilistic models (DPMs) have recently demonstrated impressive generative capabilities. There is emerging evidence that their sample reconstruction ability can yield meaningful representations for recognition tasks. In this paper, we \u2026"}, {"title": "Automated Essential Concept Discovery for Few-Shot Out-of-Distribution Detection", "link": "https://openaccess.thecvf.com/content/CVPR2025W/VAND/papers/Chen_Automated_Essential_Concept_Discovery_for_Few-Shot_Out-of-Distribution_Detection_CVPRW_2025_paper.pdf", "details": "G Chen, K Horstmann, Z Wang, F You - Proceedings of the Computer Vision and \u2026, 2025", "abstract": "Abstract Vision-Language Models (VLMs), trained on extensive internet-scale datasets, excel at identifying diverse objects and entities. Building on this foundation, several prompt learning methods have demonstrated effectiveness in Few-shot Out \u2026"}, {"title": "MP-Nav: Enhancing Data Poisoning Attacks against Multimodal Learning", "link": "https://openreview.net/pdf%3Fid%3Dzy7VeNtSLM", "details": "J Zhang, P Krishnamurthy, N Patel, A Tzes, F Khorrami - Forty-second International \u2026", "abstract": "Despite the success of current multimodal learning at scale, its susceptibility to data poisoning attacks poses security concerns in critical applications. Attacker can manipulate model behavior by injecting maliciously crafted yet minute instances into \u2026"}, {"title": "Unified Vision-Language-Action Model", "link": "https://arxiv.org/pdf/2506.19850", "details": "Y Wang, X Li, W Wang, J Zhang, Y Li, Y Chen, X Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language-action models (VLAs) have garnered significant attention for their potential in advancing robotic manipulation. However, previous approaches predominantly rely on the general comprehension capabilities of vision-language \u2026", "entry_id": "http://arxiv.org/abs/2506.19850v1", "updated": "2025-06-24 17:59:57", "published": "2025-06-24 17:59:57", "authors": "Yuqi Wang;Xinghang Li;Wenxuan Wang;Junbo Zhang;Yingyan Li;Yuntao Chen;Xinlong Wang;Zhaoxiang Zhang", "summary": "Vision-language-action models (VLAs) have garnered significant attention for\ntheir potential in advancing robotic manipulation. However, previous approaches\npredominantly rely on the general comprehension capabilities of vision-language\nmodels (VLMs) to generate action signals, often overlooking the rich temporal\nand causal structure embedded in visual observations. In this paper, we present\nUniVLA, a unified and native multimodal VLA model that autoregressively models\nvision, language, and action signals as discrete token sequences. This\nformulation enables flexible multimodal tasks learning, particularly from\nlarge-scale video data. By incorporating world modeling during post-training,\nUniVLA captures causal dynamics from videos, facilitating effective transfer to\ndownstream policy learning--especially for long-horizon tasks. Our approach\nsets new state-of-the-art results across several widely used simulation\nbenchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly\nsurpassing previous methods. For example, UniVLA achieves 95.5% average success\nrate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate\nits broad applicability on real-world ALOHA manipulation and autonomous\ndriving.", "comment": "technical report", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.RO", "links": "http://arxiv.org/abs/2506.19850v1;http://arxiv.org/pdf/2506.19850v1", "pdf_url": "http://arxiv.org/pdf/2506.19850v1"}]
