[{"title": "Mechanistic Behavior Editing of Language Models", "link": "https://arxiv.org/pdf/2410.04277", "details": "J Singh, S Dutta, T Chakraborty - arXiv preprint arXiv:2410.04277, 2024", "abstract": "Large Language Models trained on web-scale text acquire language generation abilities that can solve a wide range of tasks, particularly when task knowledge is refined into the generative prior using in-context examples. However, spurious \u2026"}, {"title": "Are Expert-Level Language Models Expert-Level Annotators?", "link": "https://arxiv.org/pdf/2410.03254", "details": "YM Tseng, WL Chen, CC Chen, HH Chen - arXiv preprint arXiv:2410.03254, 2024", "abstract": "Data annotation refers to the labeling or tagging of textual data with relevant information. A large body of works have reported positive results on leveraging LLMs as an alternative to human annotators. However, existing studies focus on classic \u2026"}, {"title": "Can Transformers Learn $ n $-gram Language Models?", "link": "https://arxiv.org/pdf/2410.03001", "details": "A Svete, N Borenstein, M Zhou, I Augenstein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Much theoretical work has described the ability of transformers to represent formal languages. However, linking theoretical results to empirical performance is not straightforward due to the complex interplay between the architecture, the learning \u2026"}, {"title": "Activation Scaling for Steering and Interpreting Language Models", "link": "https://arxiv.org/pdf/2410.04962", "details": "N Stoehr, K Du, V Sn\u00e6bjarnarson, R West, R Cotterell\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Given the prompt\" Rome is in\", can we steer a language model to flip its prediction of an incorrect token\" France\" to a correct token\" Italy\" by only multiplying a few relevant activation vectors with scalars? We argue that successfully intervening on a model is \u2026"}, {"title": "How Language Models Prioritize Contextual Grammatical Cues?", "link": "https://arxiv.org/pdf/2410.03447", "details": "H Amirzadeh, A Alishahi, H Mohebbi - arXiv preprint arXiv:2410.03447, 2024", "abstract": "Transformer-based language models have shown an excellent ability to effectively capture and utilize contextual information. Although various analysis techniques have been used to quantify and trace the contribution of single contextual cues to a \u2026"}, {"title": "SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?", "link": "https://arxiv.org/pdf/2410.03859", "details": "J Yang, CE Jimenez, AL Zhang, K Lieret, J Yang, X Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Autonomous systems for software engineering are now capable of fixing bugs and developing features. These systems are commonly evaluated on SWE-bench (Jimenez et al., 2024a), which assesses their ability to solve software issues from \u2026"}, {"title": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe", "link": "https://arxiv.org/pdf/2410.05248", "details": "Y Xiao, S Zhang, W Zhou, M Ghassemi, S Zhao - arXiv preprint arXiv:2410.05248, 2024", "abstract": "To induce desired behaviors in large language models (LLMs) for interaction-driven tasks, the instruction-tuning stage typically trains LLMs on instruction-response pairs using the next-token prediction (NTP) loss. Previous work aiming to improve \u2026"}, {"title": "ProcBench: Benchmark for Multi-Step Reasoning and Following Procedure", "link": "https://arxiv.org/pdf/2410.03117", "details": "I Fujisawa, S Nobe, H Seto, R Onda, Y Uchida, H Ikoma\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Reasoning is central to a wide range of intellectual activities, and while the capabilities of large language models (LLMs) continue to advance, their performance in reasoning tasks remains limited. The processes and mechanisms underlying \u2026"}, {"title": "The Role of Deductive and Inductive Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2410.02892", "details": "C Cai, X Zhao, H Liu, Z Jiang, T Zhang, Z Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have achieved substantial progress in artificial intelligence, particularly in reasoning tasks. However, their reliance on static prompt structures, coupled with limited dynamic reasoning capabilities, often constrains their \u2026"}]
