[{"title": "Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding", "link": "https://arxiv.org/pdf/2502.09906", "details": "TD Truong, HQ Nguyen, XB Nguyen, A Dowling, X Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual \u2026"}, {"title": "EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models", "link": "https://arxiv.org/pdf/2502.06663", "details": "X Xing, Z Liu, S Xiao, B Gao, Y Liang, W Zhang, H Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Modern large language models (LLMs) driven by scaling laws, achieve intelligence emergency in large model sizes. Recently, the increasing concerns about cloud costs, latency, and privacy make it an urgent requirement to develop compact edge \u2026"}, {"title": "Do we really have to filter out random noise in pre-training data for language models?", "link": "https://arxiv.org/pdf/2502.06604", "details": "J Ru, Y Xie, X Zhuang, Y Yin, Y Zou - arXiv preprint arXiv:2502.06604, 2025", "abstract": "Web-scale pre-training datasets are the cornerstone of LLMs' success. However, text data curated from the internet inevitably contains random noise caused by decoding errors or unregulated web content. In contrast to previous works that focus on low \u2026"}, {"title": "Improving Foundation Model for Endoscopy Video Analysis via Representation Learning on Long Sequences", "link": "https://ieeexplore.ieee.org/abstract/document/10885043/", "details": "Z Wang, C Liu, L Zhu, T Wang, S Zhang, Q Dou - IEEE Journal of Biomedical and \u2026, 2025", "abstract": "Recent advancements in endoscopy video analysis have relied on the utilization of relatively short video clips extracted from longer videos or millions of individual frames. However, these approaches tend to neglect the domain-specific \u2026"}, {"title": "Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More", "link": "https://arxiv.org/pdf/2502.11494", "details": "Z Wen, Y Gao, S Wang, J Zhang, Q Zhang, W Li, C He\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision tokens in multimodal large language models often dominate huge computational overhead due to their excessive length compared to linguistic modality. Abundant recent methods aim to solve this problem with token pruning \u2026"}, {"title": "ACF-R+: An asymmetry-sensitive method for image-text retrieval enhanced by cross-modal fusion and re-ranking based on contrastive learning", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225003145", "details": "Z Gong, Y Huang, C Yu, P Dai, X Ge, Y Shen, Y Liu - Neurocomputing, 2025", "abstract": "The task of multi-modal retrieval between the image and text modality is to find pertinent information from a designated image or textual corpus. The principal challenge lies in the integration of multi-modal representations and the discernment \u2026"}, {"title": "SKI Models: Skeleton Induced Vision-Language Embeddings for Understanding Activities of Daily Living", "link": "https://arxiv.org/pdf/2502.03459", "details": "A Sinha, D Reilly, F Bremond, P Wang, S Das - arXiv preprint arXiv:2502.03459, 2025", "abstract": "The introduction of vision-language models like CLIP has enabled the development of foundational video models capable of generalizing to unseen videos and human actions. However, these models are typically trained on web videos, which often fail \u2026"}, {"title": "A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models", "link": "https://arxiv.org/pdf/2502.13942", "details": "H Huang, S Yuan, Y Hao, C Wen, Y Fang - arXiv preprint arXiv:2502.13942, 2025", "abstract": "A large-scale vision and language model that has been pretrained on massive data encodes visual and linguistic prior, which makes it easier to generate images and language that are more natural and realistic. Despite this, there is still a significant \u2026"}, {"title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features", "link": "https://arxiv.org/pdf/2502.14786", "details": "M Tschannen, A Gritsenko, X Wang, MF Naeem\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed \u2026"}]
