[{"title": "MAGNET: Improving the Multilingual Fairness of Language Models with Adaptive Gradient-Based Tokenization", "link": "https://arxiv.org/pdf/2407.08818", "details": "O Ahia, S Kumar, H Gonen, V Hoffman, T Limisiewicz\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In multilingual settings, non-Latin scripts and low-resource languages are usually disadvantaged in terms of language models' utility, efficiency, and cost. Specifically, previous studies have reported multiple modeling biases that the current tokenization \u2026"}, {"title": "Evaluating $ n $-Gram Novelty of Language Models Using Rusty-DAWG", "link": "https://arxiv.org/pdf/2406.13069", "details": "W Merrill, NA Smith, Y Elazar - arXiv preprint arXiv:2406.13069, 2024", "abstract": "How novel are texts generated by language models (LMs) relative to their training corpora? In this work, we investigate the extent to which modern LMs generate $ n $- grams from their training data, evaluating both (i) the probability LMs assign to \u2026"}, {"title": "Abstraction-of-Thought Makes Language Models Better Reasoners", "link": "https://arxiv.org/pdf/2406.12442", "details": "R Hong, H Zhang, X Pan, D Yu, C Zhang - arXiv preprint arXiv:2406.12442, 2024", "abstract": "Abstract reasoning, the ability to reason from the abstract essence of a problem, serves as a key to generalization in human reasoning. However, eliciting language models to perform reasoning with abstraction remains unexplored. This paper seeks \u2026"}, {"title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models", "link": "https://arxiv.org/pdf/2406.19146", "details": "T Porian, M Wortsman, J Jitsev, L Schmidt, Y Carmon - arXiv preprint arXiv \u2026, 2024", "abstract": "Kaplan et al. and Hoffmann et al. developed influential scaling laws for the optimal model size as a function of the compute budget, but these laws yield substantially different predictions. We explain the discrepancy by reproducing the Kaplan scaling \u2026"}, {"title": "Open (Clinical) LLMs are Sensitive to Instruction Phrasings", "link": "https://arxiv.org/pdf/2407.09429", "details": "AMC Arroyo, M Munnangi, J Sun, KYC Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Instruction-tuned Large Language Models (LLMs) can perform a wide range of tasks given natural language instructions to do so, but they are sensitive to how such instructions are phrased. This issue is especially concerning in healthcare, as \u2026"}, {"title": "Semi-supervised Double Deep Learning Temporal Risk Prediction (SeDDLeR) with Electronic Health Records", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001035", "details": "IE Nogues, J Wen, Y Zhao, CL Bonzel, VM Castro\u2026 - Journal of Biomedical \u2026, 2024", "abstract": "Background: Risk prediction plays a crucial role in planning for prevention, monitoring, and treatment. Electronic Health Records (EHRs) offer an expansive repository of temporal medical data encompassing both risk factors and outcome \u2026"}, {"title": "Confidence Regulation Neurons in Language Models", "link": "https://arxiv.org/pdf/2406.16254", "details": "A Stolfo, B Wu, W Gurnee, Y Belinkov, X Song\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite their widespread use, the mechanisms by which large language models (LLMs) represent and regulate uncertainty in next-token predictions remain largely unexplored. This study investigates two critical components believed to influence this \u2026"}, {"title": "Scaling Laws for Linear Complexity Language Models", "link": "https://arxiv.org/pdf/2406.16690", "details": "X Shen, D Li, R Leng, Z Qin, W Sun, Y Zhong - arXiv preprint arXiv:2406.16690, 2024", "abstract": "The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their \u2026"}, {"title": "Generating Profiles of News Commentators with Language Models", "link": "https://link.springer.com/chapter/10.1007/978-3-031-63215-0_4", "details": "W Power, Z Obradovic - IFIP International Conference on Artificial Intelligence \u2026, 2024", "abstract": "Understanding the ebb and flow of online conversation has become a core task in a variety of domains. Public policy, public relations, marketing, and a host of other fields concern themselves with extracting, predicting, and reacting to, changes in the \u2026"}]
