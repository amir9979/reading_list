[{"title": "Enhancing the Reasoning Capabilities of Small Language Models via Solution Guidance Fine-Tuning", "link": "https://arxiv.org/pdf/2412.09906", "details": "J Bi, Y Wu, W Xing, Z Wei - arXiv preprint arXiv:2412.09906, 2024", "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks. Advances in prompt engineering and fine-tuning techniques have further enhanced their ability to address complex reasoning challenges \u2026"}, {"title": "Knowledge-Guided Biomarker Identification for Label-Free Single-Cell RNA-Seq Data: A Reinforcement Learning Perspective", "link": "https://arxiv.org/pdf/2501.04718", "details": "M Xiao, W Zhang, X Huang, H Zhu, M Wu, X Li, Y Zhou - arXiv preprint arXiv \u2026, 2025", "abstract": "Gene panel selection aims to identify the most informative genomic biomarkers in label-free genomic datasets. Traditional approaches, which rely on domain expertise, embedded machine learning models, or heuristic-based iterative \u2026"}, {"title": "Interpretable LLM-based Table Question Answering", "link": "https://arxiv.org/pdf/2412.12386", "details": "I Brugere, S Sharma, S Kariyappa, AT Nguyen, F Lecue - arXiv preprint arXiv \u2026, 2024", "abstract": "Interpretability for Table Question Answering (Table QA) is critical, particularly in high- stakes industries like finance or healthcare. Although recent approaches using Large Language Models (LLMs) have significantly improved Table QA performance, their \u2026"}, {"title": "The Superalignment of Superhuman Intelligence with Large Language Models", "link": "https://arxiv.org/pdf/2412.11145", "details": "M Huang, Y Wang, S Cui, P Ke, J Tang - arXiv preprint arXiv:2412.11145, 2024", "abstract": "We have witnessed superhuman intelligence thanks to the fast development of large language models and multimodal language models. As the application of such superhuman models becomes more and more common, a critical question rises \u2026"}, {"title": "Reasoning-Enhanced Self-Training for Long-Form Personalized Text Generation", "link": "https://arxiv.org/pdf/2501.04167", "details": "A Salemi, C Li, M Zhang, Q Mei, W Kong, T Chen, Z Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Personalized text generation requires a unique ability of large language models (LLMs) to learn from context that they often do not encounter during their standard training. One way to encourage LLMs to better use personalized context for \u2026"}, {"title": "Analyzing Memorization in Large Language Models through the Lens of Model Attribution", "link": "https://arxiv.org/pdf/2501.05078", "details": "TR Menta, S Agrawal, C Agarwal - arXiv preprint arXiv:2501.05078, 2025", "abstract": "Large Language Models (LLMs) are prevalent in modern applications but often memorize training data, leading to privacy breaches and copyright issues. Existing research has mainly focused on posthoc analyses, such as extracting memorized \u2026"}]
