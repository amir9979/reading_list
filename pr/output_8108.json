[{"title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE", "link": "https://arxiv.org/pdf/2409.17508", "details": "X Zhu, Y Hu, F Mo, M Li, J Wu - arXiv preprint arXiv:2409.17508, 2024", "abstract": "Multi-modal large language models (MLLMs) have shown impressive capabilities as a general-purpose interface for various visual and linguistic tasks. However, building a unified MLLM for multi-task learning in the medical field remains a thorny \u2026"}, {"title": "MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.17637", "details": "Z Liu, Y Zang, X Dong, P Zhang, Y Cao, H Duan, C He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs. This is typically achieved by using labeled datasets of chosen/rejected pairs and employing \u2026"}, {"title": "Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities", "link": "https://arxiv.org/pdf/2410.17385", "details": "Z Zhang, F Hu, J Lee, F Shi, P Kordjamshidi, J Chai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Spatial expressions in situated communication can be ambiguous, as their meanings vary depending on the frames of reference (FoR) adopted by speakers and listeners. While spatial language understanding and reasoning by vision-language models \u2026"}, {"title": "Ascle\u2014A Python Natural Language Processing Toolkit for Medical Text Generation: Development and Evaluation Study", "link": "https://www.jmir.org/2024/1/e60601/", "details": "R Yang, Q Zeng, K You, Y Qiao, L Huang, CC Hsieh\u2026 - Journal of Medical Internet \u2026, 2024", "abstract": "Background Medical texts present significant domain-specific challenges, and manually curating these texts is a time-consuming and labor-intensive process. To address this, natural language processing (NLP) algorithms have been developed to \u2026"}, {"title": "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness", "link": "https://arxiv.org/pdf/2409.17791", "details": "J Li, H Huang, Y Zhang, P Xu, X Chen, R Song, L Shi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, there has been significant interest in replacing the reward model in Reinforcement Learning with Human Feedback (RLHF) methods for Large Language Models (LLMs), such as Direct Preference Optimization (DPO) and its \u2026"}, {"title": "Fast Training of Sinusoidal Neural Fields via Scaling Initialization", "link": "https://arxiv.org/pdf/2410.04779", "details": "T Yeom, S Lee, J Lee - arXiv preprint arXiv:2410.04779, 2024", "abstract": "Neural fields are an emerging paradigm that represent data as continuous functions parameterized by neural networks. Despite many advantages, neural fields often have a high training cost, which prevents a broader adoption. In this paper, we focus \u2026"}, {"title": "Unsupervised SapBERT-based bi-encoders for medical concept annotation of clinical narratives with SNOMED CT", "link": "https://journals.sagepub.com/doi/pdf/10.1177/20552076241288681", "details": "A Abdulnazar, R Roller, S Schulz, M Kreuzthaler - DIGITAL HEALTH, 2024", "abstract": "Objective Clinical narratives provide comprehensive patient information. Achieving interoperability involves mapping relevant details to standardized medical vocabularies. Typically, natural language processing divides this task into named \u2026"}, {"title": "A foundation model for generalizable disease diagnosis in chest X-ray images", "link": "https://arxiv.org/pdf/2410.08861", "details": "L Xu, Z Ni, H Sun, H Li, S Zhang - arXiv preprint arXiv:2410.08861, 2024", "abstract": "Medical artificial intelligence (AI) is revolutionizing the interpretation of chest X-ray (CXR) images by providing robust tools for disease diagnosis. However, the effectiveness of these AI models is often limited by their reliance on large amounts of \u2026"}, {"title": "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "link": "https://arxiv.org/pdf/2410.05639", "details": "R Zhao, ZL Thai, Y Zhang, S Hu, Y Ba, J Zhou, J Cai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The performance of Large Language Models (LLMs) is substantially influenced by the pretraining corpus, which consists of vast quantities of unsupervised data processed by the models. Despite its critical role in model performance, ensuring the \u2026"}]
