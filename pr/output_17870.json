[{"title": "Assessing the System-Instruction Vulnerabilities of **Large Language Models** to Malicious Conversion Into Health Disinformation Chatbots", "link": "https://www.acpjournals.org/doi/abs/10.7326/ANNALS-24-03933", "details": "ND Modi, BD Menz, AA Awaty, CA Alex, JM Logan\u2026 - Annals of Internal Medicine, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) offer substantial promise for improving health care; however, some risks warrant **evaluation** and \u2026 \u2019s Llama 3.2-90B Vision, and xAI\u2019s Grok Beta\u2014were **evaluated** via their application programming interfaces (APIs) \u2026"}, {"title": "Overview of deep learning and **large language models** in machine translation: a special perspective on the Arabic language", "link": "https://link.springer.com/article/10.1186/s43067-025-00211-2", "details": "SA Elhamayed, M Nour - Journal of Electrical Systems and Information \u2026, 2025", "abstract": "\u2026 The work in [41] presented an **evaluation** of **large** **language** **models** on discourse modeling. The authors mentioned that the machine translation tasks presented better performance than before by adopting the pre-trained models such as BERT \u2026"}, {"title": "Rethinking chemical research in the age of **large language models**", "link": "https://www.nature.com/articles/s43588-025-00811-y", "details": "R MacKnight, DA Boiko, JE Regio, LC Gallegos\u2026 - Nature Computational \u2026, 2025", "abstract": "\u2026 The development of **large** **language** **models** (LLMs) has expanded opportunities for data-driven chemical research. LLMs, which typically \u2026 We first discuss criteria for **evaluating** the utility and performance of LLMs in chemical research, illustrating \u2026"}, {"title": "OJBench: A Competition Level Code Benchmark For Large Language Models", "link": "https://arxiv.org/pdf/2506.16395", "details": "Z Wang, Y Liu, Y Wang, W He, B Gao, M Diao, Y Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 for advanced Reasoning **Large** **Language** **Models**. Additionally, there are benchmarks specifically designed to **evaluate** models\u2019 \u2026 ICPC, aiming to truly explore the limits of current **large** **language** **models** \u2019 code reasoning capabilities. We \u2026", "entry_id": "http://arxiv.org/abs/2506.16395v1", "updated": "2025-06-19 15:27:02", "published": "2025-06-19 15:27:02", "authors": "Zhexu Wang;Yiping Liu;Yejie Wang;Wenyang He;Bofei Gao;Muxi Diao;Yanxu Chen;Kelin Fu;Flood Sung;Zhilin Yang;Tianyu Liu;Weiran Xu", "summary": "Recent advancements in large language models (LLMs) have demonstrated\nsignificant progress in math and code reasoning capabilities. However, existing\ncode benchmark are limited in their ability to evaluate the full spectrum of\nthese capabilities, particularly at the competitive level. To bridge this gap,\nwe introduce OJBench, a novel and challenging benchmark designed to assess the\ncompetitive-level code reasoning abilities of LLMs. OJBench comprises 232\nprogramming competition problems from NOI and ICPC, providing a more rigorous\ntest of models' reasoning skills. We conducted a comprehensive evaluation using\nOJBench on 37 models, including both closed-source and open-source models,\nreasoning-oriented and non-reasoning-oriented models. Our results indicate that\neven state-of-the-art reasoning-oriented models, such as o4-mini and\nGemini-2.5-pro-exp, struggle with highly challenging competition-level\nproblems. This highlights the significant challenges that models face in\ncompetitive-level code reasoning.", "comment": "9 pages, 5 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.16395v1;http://arxiv.org/pdf/2506.16395v1", "pdf_url": "http://arxiv.org/pdf/2506.16395v1"}, {"title": "TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models", "link": "https://arxiv.org/pdf/2506.18421", "details": "C Li, X Liu, Z Song, C Chi, C Zhao, J Yang, Z Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 for **large** **language** **models** (LLMs) due to its hidden semantics, inherent complexity, and structured nature. One of these challenges is lacking an effective **evaluation** \u2026 Robust **evaluation** methods: To ensure reliable **evaluation** , we design \u2026", "entry_id": "http://arxiv.org/abs/2506.18421v1", "updated": "2025-06-23 09:02:04", "published": "2025-06-23 09:02:04", "authors": "Ce Li;Xiaofan Liu;Zhiyan Song;Ce Chi;Chen Zhao;Jingjing Yang;Zhendong Wang;Kexin Yang;Boshen Shi;Xing Wang;Chao Deng;Junlan Feng", "summary": "The majority of data in businesses and industries is stored in tables,\ndatabases, and data warehouses. Reasoning with table-structured data poses\nsignificant challenges for large language models (LLMs) due to its hidden\nsemantics, inherent complexity, and structured nature. One of these challenges\nis lacking an effective evaluation benchmark fairly reflecting the performances\nof LLMs on broad table reasoning abilities. In this paper, we fill in this gap,\npresenting a comprehensive table reasoning evolution benchmark, TReB, which\nmeasures both shallow table understanding abilities and deep table reasoning\nabilities, a total of 26 sub-tasks. We construct a high quality dataset through\nan iterative data processing procedure. We create an evaluation framework to\nrobustly measure table reasoning capabilities with three distinct inference\nmodes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs\nusing this frame work and prove its effectiveness. Experimental results reveal\nthat existing LLMs still have significant room for improvement in addressing\nthe complex and real world Table related tasks. Both the dataset and evaluation\nframework are publicly available, with the dataset hosted on [HuggingFace] and\nthe framework on [GitHub].", "comment": "Benmark report v1.0", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2506.18421v1;http://arxiv.org/pdf/2506.18421v1", "pdf_url": "http://arxiv.org/pdf/2506.18421v1"}, {"title": "Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation", "link": "https://arxiv.org/pdf/2506.17088", "details": "J Cheng, T Su, J Yuan, G He, J Liu, X Tao, J Xie, H Li - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) often exhibit hallucinations, generating factually incorrect or \u2026 **evaluation**. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLM\u2019s internal states and token probability \u2026", "entry_id": "http://arxiv.org/abs/2506.17088v1", "updated": "2025-06-20 15:49:37", "published": "2025-06-20 15:49:37", "authors": "Jiahao Cheng;Tiancheng Su;Jia Yuan;Guoxiu He;Jiawei Liu;Xinqi Tao;Jingwen Xie;Huaxia Li", "summary": "Large Language Models (LLMs) often exhibit \\textit{hallucinations},\ngenerating factually incorrect or semantically irrelevant content in response\nto prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by\nencouraging step-by-step reasoning, but its impact on hallucination detection\nremains underexplored. To bridge this gap, we conduct a systematic empirical\nevaluation. We begin with a pilot experiment, revealing that CoT reasoning\nsignificantly affects the LLM's internal states and token probability\ndistributions. Building on this, we evaluate the impact of various CoT\nprompting methods on mainstream hallucination detection methods across both\ninstruction-tuned and reasoning-oriented LLMs. Specifically, we examine three\nkey dimensions: changes in hallucination score distributions, variations in\ndetection accuracy, and shifts in detection confidence. Our findings show that\nwhile CoT prompting helps reduce hallucination frequency, it also tends to\nobscure critical signals used for detection, impairing the effectiveness of\nvarious detection methods. Our study highlights an overlooked trade-off in the\nuse of reasoning. Code is publicly available at:\nhttps://anonymous.4open.science/r/cot-hallu-detect.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.17088v1;http://arxiv.org/pdf/2506.17088v1", "pdf_url": "http://arxiv.org/pdf/2506.17088v1"}, {"title": "Uncovering the Linguistic Roots of Bias: Insights and Mitigation in **Large Language Models**", "link": "https://dl.acm.org/doi/pdf/10.1145/3715275.3732127", "details": "L Benson, A Okutan, R Vasan - Proceedings of the 2025 ACM Conference on \u2026, 2025", "abstract": "\u2026 In this paper, we propose a novel methodology for bias mitigation in **large** **language** **models** , leveraging correlations between linguistic feature **evaluations** and bias benchmarks. We first employed a continuous learning framework in order \u2026"}, {"title": "Probing the Robustness of Large Language Models Safety to Latent Perturbations", "link": "https://arxiv.org/pdf/2506.16078", "details": "T Gu, K Huang, Z Wang, Y Wang, J Li, Y Yao, Y Yao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Despite significant progress in safety alignment, existing work shows that current **large** **language** **models** remain vulnerable to various \u2026 In our work, we aim to **evaluate** structural robustness by probing deeper internal model representations \u2026", "entry_id": "http://arxiv.org/abs/2506.16078v1", "updated": "2025-06-19 07:03:05", "published": "2025-06-19 07:03:05", "authors": "Tianle Gu;Kexin Huang;Zongqi Wang;Yixu Wang;Jie Li;Yuanqi Yao;Yang Yao;Yujiu Yang;Yan Teng;Yingchun Wang", "summary": "Safety alignment is a key requirement for building reliable Artificial\nGeneral Intelligence. Despite significant advances in safety alignment, we\nobserve that minor latent shifts can still trigger unsafe responses in aligned\nmodels. We argue that this stems from the shallow nature of existing alignment\nmethods, which focus on surface-level refusal behaviors without sufficiently\naltering internal representations. Consequently, small shifts in hidden\nactivations can re-trigger harmful behaviors embedded in the latent space. To\nexplore the robustness of safety alignment to latent perturbations, we\nintroduce a probing method that measures the Negative Log-Likelihood of the\noriginal response generated by the model. This probe quantifies local\nsensitivity in the latent space, serving as a diagnostic tool for identifying\nvulnerable directions. Based on this signal, we construct effective jailbreak\ntrajectories, giving rise to the Activation Steering Attack (ASA). More\nimportantly, these insights offer a principled foundation for improving\nalignment robustness. To this end, we introduce Layer-wise Adversarial Patch\nTraining~(LAPT), a fine-tuning strategy that inject controlled perturbations\ninto hidden representations during training. Experimental results highlight\nthat LAPT strengthen alignment robustness without compromising general\ncapabilities. Our findings reveal fundamental flaws in current alignment\nparadigms and call for representation-level training strategies that move\nbeyond surface-level behavior supervision. Codes and results are available at\nhttps://github.com/Carol-gutianle/LatentSafety.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CL;cs.CR", "links": "http://arxiv.org/abs/2506.16078v1;http://arxiv.org/pdf/2506.16078v1", "pdf_url": "http://arxiv.org/pdf/2506.16078v1"}, {"title": "Selective fine-tuning for **large language models** via matrix nuclear norm", "link": "https://www.sciencedirect.com/science/article/pii/S0306457325002006", "details": "T Xia, Y Li, Y Wu, Y Chang - Information Processing & Management, 2025", "abstract": "In recent years, the primary focus of supervised fine-tuning (SFT) for **large** **language** **models** (LLMs) has concentrated on the utilization of a small set of high-quality data to fine-tune models. To address the challenge of selecting the most suitable data \u2026"}]
