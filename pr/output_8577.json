[{"title": "Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization", "link": "https://arxiv.org/pdf/2410.09302", "details": "G Liu, K Ji, R Zheng, Z Wu, C Dun, Q Gu, L Yan - arXiv preprint arXiv:2410.09302, 2024", "abstract": "Reinforcement Learning (RL) plays a crucial role in aligning large language models (LLMs) with human preferences and improving their ability to perform complex tasks. However, current approaches either require significant computational resources due \u2026"}, {"title": "MSc-SQL: Multi-Sample Critiquing Small Language Models For Text-To-SQL Translation", "link": "https://arxiv.org/pdf/2410.12916", "details": "SK Gorti, I Gofman, Z Liu, J Wu, N Vouitsis, G Yu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Text-to-SQL generation enables non-experts to interact with databases via natural language. Recent advances rely on large closed-source models like GPT-4 that present challenges in accessibility, privacy, and latency. To address these issues, we \u2026"}, {"title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ", "link": "https://arxiv.org/pdf/2410.09613", "details": "A Poulis, E Tsalapati, M Koubarakis - arXiv preprint arXiv:2410.09613, 2024", "abstract": "Recent advancements in transformer-based language models have sparked research into their logical reasoning capabilities. Most of the benchmarks used to evaluate these models are simple: generated from short (fragments of) first-order \u2026"}, {"title": "Understanding Graphical Perception in Data Visualization through Zero-shot Prompting of Vision-Language Models", "link": "https://arxiv.org/pdf/2411.00257", "details": "G Guo, JJ Kang, RS Shah, H Pfister, S Varma - arXiv preprint arXiv:2411.00257, 2024", "abstract": "Vision Language Models (VLMs) have been successful at many chart comprehension tasks that require attending to both the images of charts and their accompanying textual descriptions. However, it is not well established how VLM \u2026"}, {"title": "Zero-shot extraction of seizure outcomes from clinical notes using generative pretrained transformers", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/11/04/2024.11.01.24316573.full.pdf", "details": "WKS Ojemann, K Xie, K Liu, E Chang, D Roth, B Litt\u2026 - medRxiv, 2024", "abstract": "Purpose Pre\u2013trained encoder transformer models have extracted information from unstructured clinic note text but require manual annotation for supervised fine\u2013 tuning. Large, Generative Pre\u2013trained Transformers (GPTs) may streamline this \u2026"}, {"title": "Multi-Accurate CATE is Robust to Unknown Covariate Shifts", "link": "https://www.researchgate.net/profile/Christoph-Kern-3/publication/385491907_Multi-Accurate_CATE_is_Robust_to_Unknown_Covariate_Shifts/links/67262645db208342dee6642a/Multi-Accurate-CATE-is-Robust-to-Unknown-Covariate-Shifts.pdf", "details": "C Kern, M Kim, A Zhou", "abstract": "Estimating heterogeneous treatment effects is important to tailor treatments to those individuals who would most likely benefit. However, conditional average treatment effect predictors may often be trained on one population but possibly deployed on \u2026"}, {"title": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning", "link": "https://arxiv.org/pdf/2410.19290", "details": "Y Liu, S Chang, T Jaakkola, Y Zhang - arXiv preprint arXiv:2410.19290, 2024", "abstract": "Recent studies have identified one aggravating factor of LLM hallucinations as the knowledge inconsistency between pre-training and fine-tuning, where unfamiliar fine- tuning data mislead the LLM to fabricate plausible but wrong outputs. In this paper \u2026"}, {"title": "Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models", "link": "https://arxiv.org/pdf/2410.12011", "details": "K Tatariya, V Araujo, T Bauwens, M de Lhoneux - arXiv preprint arXiv:2410.12011, 2024", "abstract": "Pixel-based language models have emerged as a compelling alternative to subword- based language modelling, particularly because they can represent virtually any script. PIXEL, a canonical example of such a model, is a vision transformer that has \u2026"}, {"title": "TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles", "link": "https://arxiv.org/pdf/2410.05262", "details": "Q Yu, S Song, K Fang, Y Shi, Z Zheng, H Wang, S Niu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As the application of Large Language Models (LLMs) expands, the demand for reliable evaluations increases. Existing LLM evaluation benchmarks primarily rely on static datasets, making it challenging to assess model performance in dynamic \u2026"}]
