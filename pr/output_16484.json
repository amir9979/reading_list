[{"title": "MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment", "link": "https://arxiv.org/pdf/2505.09372", "details": "S Yan, X Li, M Hu, Y Jiang, Z Yu, Z Ge - arXiv preprint arXiv:2505.09372, 2025", "abstract": "Dermatological diagnosis represents a complex multimodal challenge that requires integrating visual features with specialized clinical knowledge. While vision- language pretraining (VLP) has advanced medical AI, its effectiveness in \u2026"}, {"title": "Enhancing Semi-Supervised Brain Tumor Segmentation via Dual-Uncertainty Guided 2.5 D CNN-Transformer and Voxel-Wise Contrastive Learning", "link": "https://ieeexplore.ieee.org/abstract/document/11004024/", "details": "Y Zhou, J Bao, Z Xing, D Cao, C Cai, Z Chen, S Cai - IEEE Transactions on \u2026, 2025", "abstract": "Data annotation in medical image segmentation is time-consuming and expensive. Semi-supervised learning presents a viable solution. However, unlike organ segmentation, current semi-supervised methods often produce highly uncertain and \u2026"}, {"title": "Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation", "link": "https://arxiv.org/pdf/2505.05472", "details": "C Liao, L Liu, X Wang, Z Luo, X Zhang, W Zhao, J Wu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent progress in unified models for image understanding and generation has been impressive, yet most approaches remain limited to single-modal generation conditioned on multiple modalities. In this paper, we present Mogao, a unified \u2026"}, {"title": "PonderV2: Improved 3D Representation with A Universal Pre-training Paradigm", "link": "https://ieeexplore.ieee.org/abstract/document/10969802/", "details": "H Zhu, H Yang, X Wu, D Huang, S Zhang, X He\u2026 - IEEE Transactions on \u2026, 2025", "abstract": "In contrast to numerous NLP and 2D vision foundational models, training a 3D foundational model poses considerably greater challenges. This is primarily due to the inherent data variability and diversity of downstream tasks. In this paper, we \u2026"}, {"title": "Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training", "link": "https://arxiv.org/pdf/2504.13123%3F", "details": "X Zhang, Y Zeng, X Huang, H Hu, R Xie, H Hu, Z Kang - arXiv preprint arXiv \u2026, 2025", "abstract": "In recent years, the field of vision-language model pre-training has experienced rapid advancements, driven primarily by the continuous enhancement of textual capabilities in large language models. However, existing training paradigms for \u2026"}, {"title": "Comparative performance of large language models in structuring head CT radiology reports: multi-institutional validation study in Japan", "link": "https://link.springer.com/article/10.1007/s11604-025-01799-1", "details": "H Takita, SL Walston, Y Mitsuyama, K Watanabe\u2026 - Japanese Journal of \u2026, 2025", "abstract": "Purpose To compare the diagnostic performance of three proprietary large language models (LLMs)\u2014Claude, GPT, and Gemini\u2014in structuring free-text Japanese radiology reports for intracranial hemorrhage and skull fractures, and to assess the \u2026"}, {"title": "Scaling LLaNA: Advancing NeRF-Language Understanding Through Large-Scale Training", "link": "https://arxiv.org/pdf/2504.13995", "details": "A Amaduzzi, PZ Ramirez, G Lisanti, S Salti\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have shown remarkable capabilities in understanding both images and 3D data, yet these modalities face inherent limitations in comprehensively representing object geometry \u2026"}, {"title": "Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High-and Low-Resource Languages", "link": "https://arxiv.org/pdf/2504.18560", "details": "A Buscemi, C Lothritz, S Morales, M Gomez-Vazquez\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have exhibited impressive natural language processing capabilities but often perpetuate social biases inherent in their training data. To address this, we introduce MultiLingual Augmented Bias Testing (MLA \u2026"}, {"title": "A Guide To Effectively Leveraging LLMs for Low-Resource Text Summarization: Data Augmentation and Semi-supervised Approaches", "link": "https://aclanthology.org/2025.findings-naacl.86.pdf", "details": "G Sahu, O Vechtomova, IH Laradji - Findings of the Association for Computational \u2026, 2025", "abstract": "Existing approaches for low-resource text summarization primarily employ large language models (LLMs) like GPT-3 or GPT-4 at inference time to generate summaries directly; however, such approaches often suffer from inconsistent LLM \u2026"}]
