[{"title": "Small Language Models: Survey, Measurements, and Insights", "link": "https://arxiv.org/pdf/2409.15790%3F", "details": "Z Lu, X Li, D Cai, R Yi, F Liu, X Zhang, ND Lane, M Xu - arXiv preprint arXiv \u2026, 2024", "abstract": "Small language models (SLMs), despite their widespread adoption in modern smart devices, have received significantly less academic attention compared to their large language model (LLM) counterparts, which are predominantly deployed in data \u2026"}, {"title": "Efficient Fine-Tuning for Low-Resource Tibetan Pre-trained Language Models", "link": "https://link.springer.com/chapter/10.1007/978-3-031-72350-6_28", "details": "M Zhou, Z Daiqing, N Qun, T Nyima - International Conference on Artificial Neural \u2026, 2024", "abstract": "For low-resource languages like Tibetan, the availability of pre-trained language models (PLMs) is severely limited both in quantity and performance. Therefore, it is crucial to explore the optimization of these limited PLMs. In this paper, leveraging the \u2026"}, {"title": "Characterization and Racial Stratification of Social Determinants of Health for Individuals with Type 2 Diabetes as Recorded in Electronic Health Records: Implications \u2026", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/10/08/2024.10.07.24315048.full.pdf", "details": "PV Kukhareva, MJ O'Brien, DC Malone, K Kawamoto\u2026 - medRxiv, 2024", "abstract": "Background: Accurate documentation of social determinants of health (SDoH) in electronic health records (EHRs) is critical for developing equitable AI models for diabetes management. This study investigates SDoH data in a cross-institutional \u2026"}, {"title": "Empirical Study of Mutual Reinforcement Effect and Application in Few-shot Text Classification Tasks via Prompt", "link": "https://arxiv.org/pdf/2410.09745", "details": "C Gan, T Mori - arXiv preprint arXiv:2410.09745, 2024", "abstract": "The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced \u2026"}, {"title": "Retrieval-Pretrained Transformer: Long-range Language Modeling with Self-retrieval", "link": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00693/124629", "details": "O Rubin, J Berant - Transactions of the Association for Computational \u2026, 2024", "abstract": "Retrieval-augmented language models (LMs) have received much attention recently. However, typically the retriever is not trained jointly as a native component of the LM, but added post-hoc to an already-pretrained LM, which limits the ability of the LM and \u2026"}, {"title": "Evaluating Expert-Layperson Agreement in Identifying Jargon Terms in Electronic Health Record Notes: Observational Study", "link": "https://www.jmir.org/2024/1/e49704/", "details": "JP Lalor, DA Levy, HS Jordan, W Hu, JK Smirnova\u2026 - Journal of Medical Internet \u2026, 2024", "abstract": "Background Studies have shown that patients have difficulty understanding medical jargon in electronic health record (EHR) notes, particularly patients with low health literacy. In creating the NoteAid dictionary of medical jargon for patients, a panel of \u2026"}, {"title": "Comparative Analysis of Large Language Models in Chinese Medical Named Entity Recognition", "link": "https://www.mdpi.com/2306-5354/11/10/982", "details": "Z Zhu, Q Zhao, J Li, Y Ge, X Ding, T Gu, J Zou, S Lv\u2026 - Bioengineering, 2024", "abstract": "The emergence of large language models (LLMs) has provided robust support for application tasks across various domains, such as name entity recognition (NER) in the general domain. However, due to the particularity of the medical domain, the \u2026"}, {"title": "Testing and Evaluation of Health Care Applications of Large Language Models: A Systematic Review", "link": "https://jamanetwork.com/journals/jama/fullarticle/2825147", "details": "S Bedi, Y Liu, L Orr-Ewing, D Dash, S Koyejo\u2026 - JAMA", "abstract": "Importance Large language models (LLMs) can assist in various health care activities, but current evaluation approaches may not adequately identify the most useful application areas. Objective To summarize existing evaluations of LLMs in \u2026"}, {"title": "Enhanced Prompt Learning for Few-shot Text Classification Method", "link": "http://crestapress.org/index.php/sidr/article/view/78", "details": "E Zio, M Rossi, E Garcia, YE Shuqin, Z Guangwei - Scientific Insights and Discoveries \u2026, 2024", "abstract": "An enhanced prompt learning method (EPL4FTC) for few-shot text classification task is proposed. This algorithm first converts the text classification task into the form of prompt learning based on natural language inference. Thus, the implicit data \u2026"}]
