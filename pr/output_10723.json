[{"title": "PathOmCLIP: Connecting tumor histology with spatial gene expression via locally enhanced contrastive learning of Pathology and Single-cell foundation model", "link": "https://www.biorxiv.org/content/biorxiv/early/2024/12/11/2024.12.10.627865.full.pdf", "details": "Y Lee, X Liu, M Hao, T Liu, A Regev - bioRxiv, 2024", "abstract": "Tumor morphological features from histology images are a cornerstone of clinical pathology, diagnostic biomarkers, and basic cancer biology research. Spatial transcriptomics, which provides spatially resolved gene expression profiles overlaid \u2026"}, {"title": "Self-supervised graph contrastive learning with diffusion augmentation for functional MRI analysis and brain disorder detection", "link": "https://www.sciencedirect.com/science/article/pii/S1361841524003281", "details": "X Wang, Y Fang, Q Wang, PT Yap, H Zhu, M Liu - Medical Image Analysis, 2024", "abstract": "Resting-state functional magnetic resonance imaging (rs-fMRI) provides a non- invasive imaging technique to study patterns of brain activity, and is increasingly used to facilitate automated brain disorder analysis. Existing fMRI-based learning \u2026"}, {"title": "Margin-aware optimized contrastive learning for enhanced self-supervised histopathological image classification", "link": "https://link.springer.com/article/10.1007/s13755-024-00316-4", "details": "E Gupta, V Gupta - Health Information Science and Systems, 2025", "abstract": "Histopathological images, characterized by their high resolution and intricate cellular structures, present unique challenges for automated analysis. Traditional supervised learning-based methods often rely on extensive labeled datasets, which are labour \u2026"}, {"title": "Label-template based Few-Shot Text Classification with Contrastive Learning", "link": "https://arxiv.org/pdf/2412.10110%3F", "details": "G Hou, S Cao, D Ouyang, N Wang - arXiv preprint arXiv:2412.10110, 2024", "abstract": "As an algorithmic framework for learning to learn, meta-learning provides a promising solution for few-shot text classification. However, most existing research fail to give enough attention to class labels. Traditional basic framework building \u2026"}, {"title": "Multi-Scale Contrastive Learning for Video Temporal Grounding", "link": "https://arxiv.org/pdf/2412.07157", "details": "TT Nguyen, Y Bin, X Wu, Z Hu, CDT Nguyen, SK Ng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Temporal grounding, which localizes video moments related to a natural language query, is a core problem of vision-language learning and video understanding. To encode video moments of varying lengths, recent methods employ a multi-level \u2026"}, {"title": "A Knowledge-enhanced Pathology Vision-language Foundation Model for Cancer Diagnosis", "link": "https://arxiv.org/pdf/2412.13126", "details": "X Zhou, L Sun, D He, W Guan, R Wang, L Wang, X Sun\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Deep learning has enabled the development of highly robust foundation models for various pathological tasks across diverse diseases and patient cohorts. Among these models, vision-language pre-training, which leverages large-scale paired data to \u2026"}, {"title": "Motion-aware Contrastive Learning for Temporal Panoptic Scene Graph Generation", "link": "https://arxiv.org/pdf/2412.07160", "details": "TT Nguyen, X Wu, Y Bin, CDT Nguyen, SK Ng, AT Luu - arXiv preprint arXiv \u2026, 2024", "abstract": "To equip artificial intelligence with a comprehensive understanding towards a temporal world, video and 4D panoptic scene graph generation abstracts visual data into nodes to represent entities and edges to capture temporal relations. Existing \u2026"}, {"title": "Single-View Graph Contrastive Learning with Soft Neighborhood Awareness", "link": "https://arxiv.org/pdf/2412.09261", "details": "Q Sun, C Chen, Z Qiao, X Zheng, K Wang - arXiv preprint arXiv:2412.09261, 2024", "abstract": "Most graph contrastive learning (GCL) methods heavily rely on cross-view contrast, thus facing several concomitant challenges, such as the complexity of designing effective augmentations, the potential for information loss between views, and \u2026"}, {"title": "RingMoGPT: A Unified Remote Sensing Foundation Model for Vision, Language, and grounded tasks", "link": "https://ieeexplore.ieee.org/abstract/document/10777289/", "details": "P Wang, H Hu, B Tong, Z Zhang, F Yao, Y Feng, Z Zhu\u2026 - IEEE Transactions on \u2026, 2024", "abstract": "Recently, multi-modal large language models (MLLMs) have shown excellent reasoning capabilities in various fields. Most of the existing remote sensing MLLMs solve image-level text generation problems (eg, image captioning), but ignore the \u2026"}]
