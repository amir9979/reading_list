[{"title": "Evaluating Vision-Language Models as Evaluators in Path Planning", "link": "https://arxiv.org/pdf/2411.18711", "details": "M Aghzal, X Yue, E Plaku, Z Yao - arXiv preprint arXiv:2411.18711, 2024", "abstract": "Despite their promise to perform complex reasoning, large language models (LLMs) have been shown to have limited effectiveness in end-to-end planning. This has inspired an intriguing question: if these models cannot plan well, can they still \u2026"}, {"title": "MegaCOIN: Enhancing Medium-Grained Color Perception for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.03927", "details": "MC Chiu, S Wen, PY Chen, X Ma - arXiv preprint arXiv:2412.03927, 2024", "abstract": "In vision-language models (VLMs), the ability to perceive and interpret color and physical environment is crucial for achieving contextually accurate understanding and interaction. However, despite advances in multimodal modeling, there remains a \u2026"}, {"title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models", "link": "https://arxiv.org/pdf/2412.04467", "details": "S Yang, Y Chen, Z Tian, C Wang, J Li, B Yu, J Jia - arXiv preprint arXiv:2412.04467, 2024", "abstract": "Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens \u2026"}, {"title": "DHCP: Detecting Hallucinations by Cross-modal Attention Pattern in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.18659", "details": "Y Zhang, R Xie, J Chen, X Sun, Y Wang - arXiv preprint arXiv:2411.18659, 2024", "abstract": "Large vision-language models (LVLMs) have demonstrated exceptional performance on complex multimodal tasks. However, they continue to suffer from significant hallucination issues, including object, attribute, and relational \u2026"}, {"title": "Self-Generated Critiques Boost Reward Modeling for Language Models", "link": "https://arxiv.org/pdf/2411.16646%3F", "details": "Y Yu, Z Chen, A Zhang, L Tan, C Zhu, RY Pang, Y Qian\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to \u2026"}, {"title": "NVILA: Efficient Frontier Visual Language Models", "link": "https://arxiv.org/pdf/2412.04468", "details": "Z Liu, L Zhu, B Shi, Z Zhang, Y Lou, S Yang, H Xi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and \u2026"}, {"title": "KoSEL: Knowledge subgraph enhanced large language model for medical question answering", "link": "https://www.sciencedirect.com/science/article/pii/S0950705124014710", "details": "Z Zeng, Q Cheng, X Hu, Y Zhuang, X Liu, K He, Z Liu - Knowledge-Based Systems, 2024", "abstract": "The integration of medical knowledge graphs (KGs) and large language models (LLMs) for medical question answering (Q&A) has attracted considerable interest in recent studies. However, current approaches that combine KGs and LLMs tend to \u2026"}, {"title": "ECG-LM: Understanding Electrocardiogram with Large Language Model", "link": "https://spj.science.org/doi/pdf/10.34133/hds.0221", "details": "K Yang, M Hong, J Zhang, Y Luo, Y Su, O Zhang, X Yu\u2026 - Health Data Science", "abstract": "The electrocardiogram (ECG) is a widely used, non-invasive medical tool essential for detecting 34 potential health risks and supporting home health monitoring. Medical professionals frequently rely 35 on ECGs, combined with patient-related \u2026"}, {"title": "\\textsc {Neon}: News Entity-Interaction Extraction for Enhanced Question Answering", "link": "https://arxiv.org/pdf/2411.12449", "details": "S Singhania, S Cucerzan, A Herring, SK Jauhar - arXiv preprint arXiv:2411.12449, 2024", "abstract": "Capturing fresh information in near real-time and using it to augment existing large language models (LLMs) is essential to generate up-to-date, grounded, and reliable output. This problem becomes particularly challenging when LLMs are used for \u2026"}]
