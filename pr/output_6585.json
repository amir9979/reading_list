[{"title": "Goldfish: Monolingual Language Models for 350 Languages", "link": "https://arxiv.org/pdf/2408.10441", "details": "TA Chang, C Arnett, Z Tu, BK Bergen - arXiv preprint arXiv:2408.10441, 2024", "abstract": "For many low-resource languages, the only available language models are large multilingual models trained on many languages simultaneously. However, using FLORES perplexity as a metric, we find that these models perform worse than \u2026"}, {"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "link": "https://arxiv.org/pdf/2408.12337", "details": "KS Phogat, SA Puranam, S Dasaratha, C Harsha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain \u2026"}, {"title": "Beyond Relevant Documents: A Knowledge-Intensive Approach for Query-Focused Summarization using Large Language Models", "link": "https://arxiv.org/pdf/2408.10357", "details": "W Zhang, JH Huang, S Vakulenko, Y Xu, T Rajapakse\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Query-focused summarization (QFS) is a fundamental task in natural language processing with broad applications, including search engines and report generation. However, traditional approaches assume the availability of relevant documents \u2026"}, {"title": "GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization", "link": "https://arxiv.org/pdf/2408.10115", "details": "R Liu, M Liu, M Yu, J Jiang, G Li, D Zhang, J Li, X Meng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models are increasingly being used in multi-document summarization tasks. However, these models need large-scale corpora for pre- training and are domain-dependent. Other non-neural unsupervised summarization \u2026"}, {"title": "Large Language Model Sentinel: LLM Agent for Adversarial Purification", "link": "https://www.researchgate.net/profile/Guang-Lin-6/publication/383762518_Large_Language_Model_Sentinel_LLM_Agent_for_Adversarial_Purification/links/66d96075b1606e24c2e1be77/Large-Language-Model-Sentinel-LLM-Agent-for-Adversarial-Purification.pdf", "details": "G Lin, Q Zhao", "abstract": "Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some welldesigned \u2026"}, {"title": "Benchmarking Large Language Models for Math Reasoning Tasks", "link": "https://arxiv.org/pdf/2408.10839", "details": "K Se\u00dfler, Y Rong, E G\u00f6zl\u00fckl\u00fc, E Kasneci - arXiv preprint arXiv:2408.10839, 2024", "abstract": "The use of Large Language Models (LLMs) in mathematical reasoning has become a cornerstone of related research, demonstrating the intelligence of these models and enabling potential practical applications through their advanced performance \u2026"}, {"title": "Low-Hanging Fruit: Knowledge Distillation from Noisy Teachers for Open Domain Spoken Language Understanding", "link": "https://link.springer.com/chapter/10.1007/978-3-031-70359-1_7", "details": "C Chen, B Xing, IW Tsang - Joint European Conference on Machine Learning and \u2026, 2024", "abstract": "Abstract Spoken Language Understanding (SLU) plays an integral role in dialogue systems. However, conventional SLU relies heavily on manually annotated datasets, which are impractical for open-domain SLU, given the wide variety of topics that must \u2026"}]
