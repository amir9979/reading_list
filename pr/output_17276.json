[{"title": "Clinical Management of Wasp Stings Using **Large Language Models** : Cross-Sectional **Evaluation** Study", "link": "https://www.jmir.org/2025/1/e67489/", "details": "W Pan, S Zhang, Y Wang, Z Quan, Y Zhu, Z Fang\u2026 - Journal of Medical Internet \u2026, 2025", "abstract": "\u2026 intelligence (AI) technologies, **large** **language** **models** (LLMs) are increasingly being used in \u2026 Objective: The objective of this research was to systematically **evaluate** and compare the \u2026 Responses from the 4 LLMs were independently \u2026"}, {"title": "Empirical Evaluation of Generalizable Automated Program Repair with Large Language Models", "link": "https://arxiv.org/pdf/2506.03283", "details": "V Campos, R Shariffdeen, A Ulges, Y Noller - arXiv preprint arXiv:2506.03283, 2025", "abstract": "\u2026 In this work, we present an intensive empirical **evaluation** of LLMs\u2019 capabilities in generating patches. We **evaluate** a diverse set of 13 \u2026 Our **evaluation** represents a snapshot of the current repair capabilities of the latest LLMs. Key results include that \u2026", "entry_id": "http://arxiv.org/abs/2506.03283v1", "updated": "2025-06-03 18:15:14", "published": "2025-06-03 18:15:14", "authors": "Viola Campos;Ridwan Shariffdeen;Adrian Ulges;Yannic Noller", "summary": "Automated Program Repair (APR) proposes bug fixes to aid developers in\nmaintaining software. The state of the art in this domain focuses on using\nLLMs, leveraging their strong capabilities to comprehend specifications in\nnatural language and to generate program code. Recent works have shown that\nLLMs can be used to generate repairs. However, despite the APR community's\nresearch achievements and several industry deployments in the last decade, APR\nstill lacks the capabilities to generalize broadly. In this work, we present an\nintensive empirical evaluation of LLMs for generating patches. We evaluate a\ndiverse set of 13 recent models, including open ones (e.g., Llama 3.3, Qwen 2.5\nCoder, and DeepSeek R1 (dist.)) and closed ones (e.g., o3-mini, GPT-4o, Claude\n3.7 Sonnet, Gemini 2.0 Flash). In particular, we explore language-agnostic\nrepairs by utilizing benchmarks for Java (e.g., Defects4J), JavaScript (e.g.,\nBugsJS), Python (e.g., BugsInPy), and PHP (e.g., BugsPHP). Besides the\ngeneralization between different languages and levels of patch complexity, we\nalso investigate the effects of fault localization (FL) as a preprocessing step\nand compare the progress for open vs closed models. Our evaluation represents a\nsnapshot of the current repair capabilities of the latest LLMs. Key results\ninclude: (1) Different LLMs tend to perform best for different languages, which\nmakes it hard to develop cross-platform repair techniques with single LLMs. (2)\nThe combinations of models add value with respect to uniquely fixed bugs, so a\ncommittee of expert models should be considered. (3) Under realistic\nassumptions of imperfect FL, we observe significant drops in accuracy from the\nusual practice of using perfect FL. Our findings and insights will help both\nresearchers and practitioners develop reliable and generalizable APR techniques\nand evaluate them in realistic and fair environments.", "comment": null, "journal_ref": null, "primary_category": "cs.SE", "categories": "cs.SE", "links": "http://arxiv.org/abs/2506.03283v1;http://arxiv.org/pdf/2506.03283v1", "pdf_url": "http://arxiv.org/pdf/2506.03283v1"}, {"title": "Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales", "link": "https://arxiv.org/pdf/2506.03619", "details": "A Tsutsumi, Y Jinnai - arXiv preprint arXiv:2506.03619, 2025", "abstract": "\u2026 Although **Large** **Language** **Models** (LLMs) have demonstrated strong language understanding and generation abilities across various lan\u2026 In this study, we focus on **evaluating** knowledge of folktales, a key medium for conveying and circulating \u2026", "entry_id": "http://arxiv.org/abs/2506.03619v1", "updated": "2025-06-04 06:58:19", "published": "2025-06-04 06:58:19", "authors": "Ayuto Tsutsumi;Yuu Jinnai", "summary": "Although Large Language Models (LLMs) have demonstrated strong language\nunderstanding and generation abilities across various languages, their cultural\nknowledge is often limited to English-speaking communities, which can\nmarginalize the cultures of non-English communities. To address the problem,\nevaluation of the cultural awareness of the LLMs and the methods to develop\nculturally aware LLMs have been investigated. In this study, we focus on\nevaluating knowledge of folktales, a key medium for conveying and circulating\nculture. In particular, we focus on Japanese folktales, specifically on\nknowledge of Yokai. Yokai are supernatural creatures originating from Japanese\nfolktales that continue to be popular motifs in art and entertainment today.\nYokai have long served as a medium for cultural expression, making them an\nideal subject for assessing the cultural awareness of LLMs. We introduce\nYokaiEval, a benchmark dataset consisting of 809 multiple-choice questions\n(each with four options) designed to probe knowledge about yokai. We evaluate\nthe performance of 31 Japanese and multilingual LLMs on this dataset. The\nresults show that models trained with Japanese language resources achieve\nhigher accuracy than English-centric models, with those that underwent\ncontinued pretraining in Japanese, particularly those based on Llama-3,\nperforming especially well. The code and dataset are available at\nhttps://github.com/CyberAgentA ILab/YokaiEval.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.03619v1;http://arxiv.org/pdf/2506.03619v1", "pdf_url": "http://arxiv.org/pdf/2506.03619v1"}, {"title": "Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons", "link": "https://arxiv.org/pdf/2506.03785", "details": "IB Sandan, TA Dinh, J Niehues - arXiv preprint arXiv:2506.03785, 2025", "abstract": "\u2026 , **Large** **Language** **Models** (LLMs) take the role of the expert to **evaluate** complex tasks. Using LLMs as evaluators allows us to mimic the abilities of human experts, making **evaluations** \u2026 **evaluation** prompt consists of only the question and the \u2026", "entry_id": "http://arxiv.org/abs/2506.03785v2", "updated": "2025-06-05 13:01:33", "published": "2025-06-04 09:46:43", "authors": "Isik Baran Sandan;Tu Anh Dinh;Jan Niehues", "summary": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring.", "comment": "Accepted to GEM @ ACL 2025", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;I.2.7", "links": "http://arxiv.org/abs/2506.03785v2;http://arxiv.org/pdf/2506.03785v2", "pdf_url": "http://arxiv.org/pdf/2506.03785v2"}, {"title": "ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch", "link": "https://arxiv.org/pdf/2506.03558", "details": "J Chen, X Guan, Q Yuan, G Mo, W Zhou, Y Lu, H Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Performance in Refinement task Building on these insights, we **evaluate** recent open-source models, including Qwen-2.5-Instruct and LLaMA-3.1-Instruct (ranging from 7B to 72B parameters), using the Refinement task from the MT-Eval benchmark \u2026", "entry_id": "http://arxiv.org/abs/2506.03558v1", "updated": "2025-06-04 04:21:48", "published": "2025-06-04 04:21:48", "authors": "Jiawei Chen;Xinyan Guan;Qianhao Yuan;Guozhao Mo;Weixiang Zhou;Yaojie Lu;Hongyu Lin;Ben He;Le Sun;Xianpei Han", "summary": "Current instruction data synthesis methods primarily focus on single-turn\ninstructions and often neglect cross-turn coherence, resulting in context drift\nand reduced task completion rates in extended conversations. To address this\nlimitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a\nframework that constrains multi-turn instruction synthesis by explicitly\nmodeling human conversational intent. It operates in two stages: (1) Intent\nModeling, which captures the global structure of human dialogues by assigning\neach conversation to one of nine well-defined intent trajectories, ensuring a\ncoherent and goal-oriented information flow; and (2) Skeleton Generation, which\nconstructs a structurally grounded sequence of user queries aligned with the\nmodeled intent, thereby serving as a scaffold that constrains and guides the\ndownstream instruction synthesis process. Based on this process, we construct\nConsistentChat, a multi-turn instruction dataset with approximately 15,000\nmulti-turn conversations and 224,392 utterances. Experiments on the Light,\nTopdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat\nachieve a 20-30% improvement in chat consistency and up to a 15% increase in\ntask success rate, significantly outperforming models trained on existing\nsingle-turn and multi-turn instruction datasets.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.03558v1;http://arxiv.org/pdf/2506.03558v1", "pdf_url": "http://arxiv.org/pdf/2506.03558v1"}, {"title": "Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems", "link": "https://arxiv.org/pdf/2506.03259", "details": "ME Garcia-Alcoser, M GhojoghNejad, FI Tushar, D Kim\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Purpose: This study aims to **evaluate** the effectiveness of **large** **language** **models** (LLMs) in automating disease annotation of CT radiology reports. We compare a rule-based algorithm (RBA), RadBERT, and three lightweight open-weight LLMs for multi-disease \u2026", "entry_id": "http://arxiv.org/abs/2506.03259v1", "updated": "2025-06-03 18:00:08", "published": "2025-06-03 18:00:08", "authors": "Michael E. Garcia-Alcoser;Mobina GhojoghNejad;Fakrul Islam Tushar;David Kim;Kyle J. Lafata;Geoffrey D. Rubin;Joseph Y. Lo", "summary": "Purpose: This study aims to evaluate the effectiveness of large language\nmodels (LLMs) in automating disease annotation of CT radiology reports. We\ncompare a rule-based algorithm (RBA), RadBERT, and three lightweight\nopen-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP)\nCT reports.\n  Materials and Methods: This retrospective study analyzed 40,833 CT reports\nfrom 29,540 patients, with 1,789 CAP reports manually annotated across three\norgan systems. External validation was conducted using the CT-RATE dataset.\nThree open-weight LLMs were tested with zero-shot prompting. Performance was\nevaluated using Cohen's Kappa and micro/macro-averaged F1 scores.\n  Results: In 12,197 Duke CAP reports from 8,854 patients, Llama-3.1 8B and\nGemma-3 27B showed the highest agreement ($\\kappa$ median: 0.87). On the\nmanually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed\nby Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT-RATE\ndataset (lungs/pleura only), Llama-3.1 8B performed best (0.91), with Gemma-3\n27B close behind (0.89). Performance differences were mainly due to differing\nlabeling practices, especially for lung atelectasis.\n  Conclusion: Lightweight LLMs outperform rule-based methods for CT report\nannotation and generalize across organ systems with zero-shot prompting.\nHowever, binary labels alone cannot capture the full nuance of report language.\nLLMs can provide a flexible, efficient solution aligned with clinical judgment\nand user needs.", "comment": "23 pages, 10 figures, to be submitted in Radiology: Artificial\n  Intelligence", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;I.2.7", "links": "http://arxiv.org/abs/2506.03259v1;http://arxiv.org/pdf/2506.03259v1", "pdf_url": "http://arxiv.org/pdf/2506.03259v1"}, {"title": " **Evaluation** of Enhanced Programming Error Messages Generated by **Large Language Models** from Instructors' Point of View", "link": "https://ieeexplore.ieee.org/abstract/document/11016595/", "details": "LGJ Araujo, RA Bittencourt, C von Flach - 2025 IEEE Global Engineering Education \u2026, 2025", "abstract": "\u2026 We investigate how **Large** **Language** **Models** (LLM) may be used to provide additional feedback beyond simple compiler or interpreter \u2026 In an experiment, we asked instructors to **evaluate** the prompt text generated by an LLM from a variety of \u2026"}, {"title": "Can **Large Language Models** Understand Argument Schemes?", "link": "https://kclpure.kcl.ac.uk/portal/files/337205978/Can_Large_Language_Models_Understand_Argument_Schemes.pdf", "details": "EB Vrakatseli, O Cocarascu, S Modgil - The 63rd Annual Meeting of the Association \u2026, 2025", "abstract": "\u2026 In this paper, we **evaluated** the ability of **large** **language** **models** to identify argument schemes. Our comprehensive **evaluation** covered zero-shot, few-shot, and chain-of-thought prompting methods across seven open-source and proprietary \u2026"}, {"title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models", "link": "https://arxiv.org/pdf/2506.04180", "details": "Y Wu, Y Bai, Z Hu, J Li, RKW Lee - arXiv preprint arXiv:2506.04180, 2025", "abstract": "\u2026 The first is WritingBench [60], a comprehensive benchmark designed to **evaluate** **large** **language** **models** across six major writing domains and 100 sub-domains, encompassing creative, persuasive, informational, and technical tasks. The \u2026", "entry_id": "http://arxiv.org/abs/2506.04180v1", "updated": "2025-06-04 17:27:42", "published": "2025-06-04 17:27:42", "authors": "Yuhao Wu;Yushi Bai;Zhiqiang Hu;Juanzi Li;Roy Ka-Wei Lee", "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.04180v1;http://arxiv.org/pdf/2506.04180v1", "pdf_url": "http://arxiv.org/pdf/2506.04180v1"}]
