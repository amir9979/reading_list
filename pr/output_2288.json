[{"title": "EFTNAS: Searching for Efficient Language Models in First-Order Weight-Reordered Super-Networks", "link": "https://aclanthology.org/2024.lrec-main.497.pdf", "details": "JP Munoz, Y Zheng, N Jain - Proceedings of the 2024 Joint International Conference \u2026, 2024", "abstract": "Transformer-based models have demonstrated outstanding performance in natural language processing (NLP) tasks and many other domains, eg, computer vision. Depending on the size of these models, which have grown exponentially in the past \u2026"}, {"title": "Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data", "link": "https://arxiv.org/pdf/2405.14212", "details": "H Li, X Zhao, D Guo, H Gu, Z Zeng, Y Han, Y Song\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As large language models (LLMs) demonstrate unparalleled performance and generalization ability, LLMs are widely used and integrated into various applications. When it comes to sensitive domains, as commonly described in federated learning \u2026"}, {"title": "Large Language Multimodal Models for New-Onset Type 2 Diabetes Prediction using Five-Year Cohort Electronic Health Records", "link": "https://www.researchsquare.com/article/rs-4414387/latest.pdf", "details": "JE Ding, NMT Phan, WC Peng, JZ Wang, CC Chug\u2026 - 2024", "abstract": "Type 2 diabetes mellitus (T2DM) is a prevalent health challenge faced by countries worldwide. In this study, we propose a novel large language multimodal models (LLMMs) framework incorporating multimodal data from clinical notes and laboratory \u2026"}, {"title": "EVA-X: A Foundation Model for General Chest X-ray Analysis with Self-supervised Learning", "link": "https://arxiv.org/pdf/2405.05237", "details": "J Yao, X Wang, Y Song, H Zhao, J Ma, Y Chen, W Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The diagnosis and treatment of chest diseases play a crucial role in maintaining human health. X-ray examination has become the most common clinical examination means due to its efficiency and cost-effectiveness. Artificial intelligence analysis \u2026"}, {"title": "Exploring and Mitigating Shortcut Learning for Generative Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.602.pdf", "details": "Z Sun, Y Xiao, J Li, Y Ji, W Chen, M Zhang - Proceedings of the 2024 Joint \u2026, 2024", "abstract": "Recent generative large language models (LLMs) have exhibited incredible instruction-following capabilities while keeping strong task completion ability, even without task-specific fine-tuning. Some works attribute this to the bonus of the new \u2026"}, {"title": "A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks", "link": "https://arxiv.org/pdf/2405.10251", "details": "X Ni, P Li - arXiv preprint arXiv:2405.10251, 2024", "abstract": "Recent efforts have evaluated large language models (LLMs) in areas such as commonsense reasoning, mathematical reasoning, and code generation. However, to the best of our knowledge, no work has specifically investigated the performance \u2026"}, {"title": "Large Language Models Synergize with Automated Machine Learning", "link": "https://arxiv.org/pdf/2405.03727", "details": "J Xu, Z Liu, NAV Suryanarayanan, H Iba - arXiv preprint arXiv:2405.03727, 2024", "abstract": "Recently, code generation driven by large language models (LLMs) has become increasingly popular. However, automatically generating code for machine learning (ML) tasks still poses significant challenges. This paper explores the limits of \u2026"}, {"title": "Correcting Language Model Bias for Text Classification in True Zero-Shot Learning", "link": "https://aclanthology.org/2024.lrec-main.359.pdf", "details": "F Zhao, W Xianlin, C Yan, CK Loo - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Combining pre-trained language models (PLMs) and manual templates is a common practice for text classification in zero-shot scenarios. However, the effect of this approach is highly volatile, ranging from random guesses to near state-of-the-art \u2026"}, {"title": "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks", "link": "https://dl.acm.org/doi/pdf/10.1145/3589334.3645363", "details": "S Xu, L Pang, H Shen, X Cheng, TS Chua - Proceedings of the ACM on Web \u2026, 2024", "abstract": "Making the contents generated by Large Language Model (LLM), accurate, credible and traceable is crucial, especially in complex knowledge-intensive tasks that require multi-step reasoning and each step needs knowledge to solve. Retrieval \u2026"}]
