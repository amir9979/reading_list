[{"title": "Do Clinicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation", "link": "https://aclanthology.org/2024.bionlp-1.15.pdf", "details": "Z Yao, A Jaafar, B Wang, Z Yang, H Yu - Proceedings of the 23rd Workshop on \u2026, 2024", "abstract": "This study examines the effect of prompt engineering on the performance of Large Language Models (LLMs) in clinical note generation. We introduce an Automatic Prompt Optimization (APO) framework to refine initial prompts and compare the \u2026"}, {"title": "ULLME: A Unified Framework for Large Language Model Embeddings with Generation-Augmented Learning", "link": "https://arxiv.org/pdf/2408.03402", "details": "H Man, NT Ngo, F Dernoncourt, TH Nguyen - arXiv preprint arXiv:2408.03402, 2024", "abstract": "Large Language Models (LLMs) excel in various natural language processing tasks, but leveraging them for dense passage embedding remains challenging. This is due to their causal attention mechanism and the misalignment between their pre-training \u2026"}, {"title": "Open-domain Implicit Format Control for Large Language Model Generation", "link": "https://arxiv.org/pdf/2408.04392", "details": "Y Yao, W Ma, X Fang, X Jiang, X Li, X Meng, P Han\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Controlling the format of outputs generated by large language models (LLMs) is a critical functionality in various applications. Current methods typically employ constrained decoding with rule-based automata or fine-tuning with manually crafted \u2026"}]
