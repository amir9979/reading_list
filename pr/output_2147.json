[{"title": "Unlocking the Power of Spatial and Temporal Information in Medical Multimodal Pre-training", "link": "https://arxiv.org/pdf/2405.19654", "details": "J Yang, B Su, WX Zhao, JR Wen - arXiv preprint arXiv:2405.19654, 2024", "abstract": "Medical vision-language pre-training methods mainly leverage the correspondence between paired medical images and radiological reports. Although multi-view spatial images and temporal sequences of image-report pairs are available in off-the-shelf \u2026"}, {"title": "CheXpert Plus: Hundreds of Thousands of Aligned Radiology Texts, Images and Patients", "link": "https://arxiv.org/pdf/2405.19538", "details": "P Chambon, JB Delbrouck, T Sounack, SC Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Since the release of the original CheXpert paper five years ago, CheXpert has become one of the most widely used and cited clinical AI datasets. The emergence of vision language models has sparked an increase in demands for sharing reports \u2026"}, {"title": "Knowledge-grounded Adaptation Strategy for Vision-language Models: Building Unique Case-set for Screening Mammograms for Residents Training", "link": "https://arxiv.org/pdf/2405.19675", "details": "AU Khan, J Garrett, T Bradshaw, L Salkowski, JJ Jeong\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "A visual-language model (VLM) pre-trained on natural images and text pairs poses a significant barrier when applied to medical contexts due to domain shift. Yet, adapting or fine-tuning these VLMs for medical use presents considerable hurdles \u2026"}, {"title": "Layer-Condensed KV Cache for Efficient Inference of Large Language Models", "link": "https://arxiv.org/pdf/2405.10637", "details": "H Wu, K Tu - arXiv preprint arXiv:2405.10637, 2024", "abstract": "Huge memory consumption has been a major bottleneck for deploying high- throughput large language models in real-world applications. In addition to the large number of parameters, the key-value (KV) cache for the attention mechanism in the \u2026"}, {"title": "Mammo-CLIP: A Vision Language Foundation Model to Enhance Data Efficiency and Robustness in Mammography", "link": "https://arxiv.org/pdf/2405.12255", "details": "S Ghosh, CB Poynton, S Visweswaran\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The lack of large and diverse training data on Computer-Aided Diagnosis (CAD) in breast cancer detection has been one of the concerns that impedes the adoption of the system. Recently, pre-training with large-scale image text datasets via Vision \u2026"}, {"title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions", "link": "https://arxiv.org/pdf/2405.19744", "details": "C Li, W Yang, J Zhang, J Lu, S Wang, C Zong - arXiv preprint arXiv:2405.19744, 2024", "abstract": "Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples \u2026"}, {"title": "Beyond Imitation: Learning Key Reasoning Steps from Dual Chain-of-Thoughts in Reasoning Distillation", "link": "https://arxiv.org/pdf/2405.19737", "details": "C Dai, K Li, W Zhou, S Hu - arXiv preprint arXiv:2405.19737, 2024", "abstract": "As Large Language Models (LLMs) scale up and gain powerful Chain-of-Thoughts (CoTs) reasoning abilities, practical resource constraints drive efforts to distill these capabilities into more compact Smaller Language Models (SLMs). We find that CoTs \u2026"}]
