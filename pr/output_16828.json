[{"title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis", "link": "https://arxiv.org/pdf/2505.16834", "details": "S Sun, H Song, Y Wang, R Ren, J Jiang, J Zhang, F Bai\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-augmented generation (RAG) systems have advanced large language models (LLMs) in complex deep search scenarios requiring multi-step reasoning and iterative information retrieval. However, existing approaches face critical limitations \u2026", "entry_id": "http://arxiv.org/abs/2505.16834v1", "updated": "2025-05-22 16:05:02", "published": "2025-05-22 16:05:02", "authors": "Shuang Sun;Huatong Song;Yuhao Wang;Ruiyang Ren;Jinhao Jiang;Junjie Zhang;Fei Bai;Jia Deng;Wayne Xin Zhao;Zheng Liu;Lei Fang;Zhongyuan Wang;Ji-Rong Wen", "summary": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.IR", "links": "http://arxiv.org/abs/2505.16834v1;http://arxiv.org/pdf/2505.16834v1", "pdf_url": "http://arxiv.org/pdf/2505.16834v1"}, {"title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems", "link": "https://arxiv.org/pdf/2505.16988", "details": "R Ye, K Huang, Q Wu, Y Cai, T Jin, X Pang, X Liu, J Su\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "LLM-based multi-agent systems (MAS) have demonstrated significant potential in enhancing single LLMs to address complex and diverse tasks in practical applications. Despite considerable advancements, the field lacks a unified codebase \u2026", "entry_id": "http://arxiv.org/abs/2505.16988v1", "updated": "2025-05-22 17:54:38", "published": "2025-05-22 17:54:38", "authors": "Rui Ye;Keduan Huang;Qimin Wu;Yuzhu Cai;Tian Jin;Xianghe Pang;Xiangrui Liu;Jiaqi Su;Chen Qian;Bohan Tang;Kaiqu Liang;Jiaao Chen;Yue Hu;Zhenfei Yin;Rongye Shi;Bo An;Yang Gao;Wenjun Wu;Lei Bai;Siheng Chen", "summary": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community.", "comment": "18 pages, 11 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI;cs.MA", "links": "http://arxiv.org/abs/2505.16988v1;http://arxiv.org/pdf/2505.16988v1", "pdf_url": "http://arxiv.org/pdf/2505.16988v1"}, {"title": "MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models", "link": "https://arxiv.org/pdf/2505.16700", "details": "X Gao, S Xie, J Zhai, S Ma, C Shen - arXiv preprint arXiv:2505.16700, 2025", "abstract": "As Large Language Models (LLMs) evolve from passive text generators to active reasoning agents capable of tool interaction, the Model Context Protocol (MCP) has emerged as a standardized framework for dynamic tool discovery and orchestration \u2026", "entry_id": "http://arxiv.org/abs/2505.16700v1", "updated": "2025-05-22 14:02:37", "published": "2025-05-22 14:02:37", "authors": "Xuanqi Gao;Siyi Xie;Juan Zhai;Shqing Ma;Chao Shen", "summary": "As Large Language Models (LLMs) evolve from passive text generators to active\nreasoning agents capable of tool interaction, the Model Context Protocol (MCP)\nhas emerged as a standardized framework for dynamic tool discovery and\norchestration. Despite widespread industry adoption, existing evaluation\nmethodologies fail to adequately assess tool utilization capabilities within\nthis new paradigm. This paper introduces MCP-RADAR, the first comprehensive\nbenchmark specifically designed to evaluate LLM performance in the MCP\nframework through a novel five-dimensional approach measuring: answer accuracy,\ntool selection efficiency, computational resource efficiency, parameter\nconstruction accuracy, and execution speed. Unlike conventional benchmarks that\nrely on subjective human evaluations or binary success metrics, MCP-RADAR\nemploys objective, quantifiable measurements across multiple task domains\nincluding software engineering, mathematical reasoning, and general\nproblem-solving. Our evaluations of leading commercial and open-source LLMs\nreveal distinctive capability profiles with significant trade-offs between\naccuracy, efficiency, and speed, challenging traditional single-metric\nperformance rankings. Besides, we provide valuable guidance for developers to\noptimize their tools for maximum model compatibility and effectiveness. While\nfocused on MCP due to its standardized approach, our methodology remains\napplicable across all LLM agent tool integration frameworks, providing valuable\ninsights for both LLM developers and tool creators to optimize the entire\nLLM-tool interaction ecosystem. The implementation, configurations, and\ndatasets used in our evaluation are publicly available at\nhttps://anonymous.4open.science/r/MCPRadar-B143.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI", "links": "http://arxiv.org/abs/2505.16700v1;http://arxiv.org/pdf/2505.16700v1", "pdf_url": "http://arxiv.org/pdf/2505.16700v1"}, {"title": "Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning", "link": "https://arxiv.org/pdf/2505.16483", "details": "S Si, H Zhao, C Gao, Y Bai, Z Wang, B Gao, K Luo, W Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Teaching large language models (LLMs) to be faithful in the provided context is crucial for building reliable information-seeking systems. Therefore, we propose a systematic framework, CANOE, to improve the faithfulness of LLMs in both short-form \u2026", "entry_id": "http://arxiv.org/abs/2505.16483v1", "updated": "2025-05-22 10:10:07", "published": "2025-05-22 10:10:07", "authors": "Shuzheng Si;Haozhe Zhao;Cheng Gao;Yuzhuo Bai;Zhitong Wang;Bofei Gao;Kangyang Luo;Wenhao Li;Yufei Huang;Gang Chen;Fanchao Qi;Minjia Zhang;Baobao Chang;Maosong Sun", "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.16483v1;http://arxiv.org/pdf/2505.16483v1", "pdf_url": "http://arxiv.org/pdf/2505.16483v1"}, {"title": "Where is the answer? An empirical study of positional bias for parametric knowledge extraction in language model", "link": "https://aclanthology.org/2025.naacl-long.58.pdf", "details": "K Saito, CY Lee, K Sohn, Y Ushiku - Proceedings of the 2025 Conference of the \u2026, 2025", "abstract": "Abstract Language model (LM) stores diverse factual knowledge in their parameters, which is learned during self-supervised training on unlabeled documents and is made extractable by instruction-tuning. For knowledge-intensive tasks, it is essential \u2026"}, {"title": "Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models", "link": "https://arxiv.org/pdf/2505.16385", "details": "K He, T Zhou, Y Chen, D Qiu, S Liu, K Liu, J Zhao - arXiv preprint arXiv:2505.16385, 2025", "abstract": "Large language models (LLMs) demonstrate remarkable ability in cross-lingual tasks. Understanding how LLMs acquire this ability is crucial for their interpretability. To quantify the cross-lingual ability of LLMs accurately, we propose a Word-Level \u2026", "entry_id": "http://arxiv.org/abs/2505.16385v1", "updated": "2025-05-22 08:37:04", "published": "2025-05-22 08:37:04", "authors": "Kaiyu He;Tong Zhou;Yubo Chen;Delai Qiu;Shengping Liu;Kang Liu;Jun Zhao", "summary": "Large language models (LLMs) demonstrate remarkable ability in cross-lingual\ntasks. Understanding how LLMs acquire this ability is crucial for their\ninterpretability. To quantify the cross-lingual ability of LLMs accurately, we\npropose a Word-Level Cross-Lingual Translation Task. To find how LLMs learn\ncross-lingual ability, we trace the outputs of LLMs' intermediate layers in the\nword translation task. We identify and distinguish two distinct behaviors in\nthe forward pass of LLMs: co-occurrence behavior and semantic pivot behavior.\nWe attribute LLMs' two distinct behaviors to the co-occurrence frequency of\nwords and find the semantic pivot from the pre-training dataset. Finally, to\napply our findings to improve the cross-lingual ability of LLMs, we reconstruct\na semantic pivot-aware pre-training dataset using documents with a high\nproportion of semantic pivots. Our experiments validate the effectiveness of\nour approach in enhancing cross-lingual ability. Our research contributes\ninsights into the interpretability of LLMs and offers a method for improving\nLLMs' cross-lingual ability.", "comment": "14 pages, 10 figures", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.16385v1;http://arxiv.org/pdf/2505.16385v1", "pdf_url": "http://arxiv.org/pdf/2505.16385v1"}, {"title": "LIFEBench: Evaluating Length Instruction Following in Large Language Models", "link": "https://arxiv.org/pdf/2505.16234", "details": "W Zhang, Z Zhou, J Fang, R Xu, K Wang, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While large language models (LLMs) can solve PhD-level reasoning problems over long context inputs, they still struggle with a seemingly simpler task: following explicit length instructions-eg, write a 10,000-word novel. Additionally, models often \u2026", "entry_id": "http://arxiv.org/abs/2505.16234v1", "updated": "2025-05-22 05:08:27", "published": "2025-05-22 05:08:27", "authors": "Wei Zhang;Zhenhong Zhou;Junfeng Fang;Rongwu Xu;Kun Wang;Yuanhe Zhang;Rui Wang;Ge Zhang;Xinfeng Li;Li Sun;Lingjuan Lyu;Yang Liu;Sen Su", "summary": "While large language models (LLMs) can solve PhD-level reasoning problems\nover long context inputs, they still struggle with a seemingly simpler task:\nfollowing explicit length instructions-e.g., write a 10,000-word novel.\nAdditionally, models often generate far too short outputs, terminate\nprematurely, or even refuse the request. Existing benchmarks focus primarily on\nevaluating generations quality, but often overlook whether the generations meet\nlength constraints. To this end, we introduce Length Instruction Following\nEvaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to\nfollow length instructions across diverse tasks and a wide range of specified\nlengths. LIFEBench consists of 10,800 instances across 4 task categories in\nboth English and Chinese, covering length constraints ranging from 16 to 8192\nwords. We evaluate 26 widely-used LLMs and find that most models reasonably\nfollow short-length instructions but deteriorate sharply beyond a certain\nthreshold. Surprisingly, almost all models fail to reach the vendor-claimed\nmaximum output lengths in practice, as further confirmed by our evaluations\nextending up to 32K words. Even long-context LLMs, despite their extended\ninput-output windows, counterintuitively fail to improve length-instructions\nfollowing. Notably, Reasoning LLMs outperform even specialized long-text\ngeneration models, achieving state-of-the-art length following. Overall,\nLIFEBench uncovers fundamental limitations in current LLMs' length instructions\nfollowing ability, offering critical insights for future progress.", "comment": "81 pages, 22 tables, 32 figures. Homepage:\n  https://ydyjya.github.io/LIFEBench/", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.16234v1;http://arxiv.org/pdf/2505.16234v1", "pdf_url": "http://arxiv.org/pdf/2505.16234v1"}, {"title": "Large Language Models in Crisis Informatics for Zero and Few-Shot Classification", "link": "https://dl.acm.org/doi/pdf/10.1145/3736160", "details": "C S\u00e1nchez, A Abeliuk, B Poblete - ACM Transactions on the Web, 2025", "abstract": "This article presents an exploration of the use of pre-trained Large Language Models (LLMs) for crisis classification to address labeled data dependency issues. We present a methodology that enhances open LLMs through fine-tuning, creating zero \u2026"}, {"title": "Function Calling in Large Language Models: Industrial Practices, Challenges, and Future Directions", "link": "https://openreview.net/pdf%3Fid%3DLNxVGPedFW", "details": "M WANG, Y ZHANG, C PENG, Y CHEN, WEI ZHOU\u2026 - 2025", "abstract": "1 INTRODUCTION Recent advancements in artificial intelligence have ushered in a transformative era with the development of large language models (LLMs) such as GPT series, LLama [164], ChatGLM [33, 212] and Qwen [10]. These models \u2026"}]
