[{"title": "Scalable Language Models with Posterior Inference of Latent Thought Vectors", "link": "https://arxiv.org/pdf/2502.01567%3F", "details": "D Kong, M Zhao, D Xu, B Pang, S Wang, E Honig, Z Si\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We propose a novel family of language models, Latent-Thought Language Models (LTMs), which incorporate explicit latent thought vectors that follow an explicit prior model in latent space. These latent thought vectors guide the autoregressive \u2026"}, {"title": "When Evolution Strategy Meets Language Models Tuning", "link": "https://aclanthology.org/2025.coling-main.357.pdf", "details": "B Huang, Y Jiang, M Chen, Y Wang, H Chen, W Wang - Proceedings of the 31st \u2026, 2025", "abstract": "Supervised Fine-tuning has been pivotal in training autoregressive language models, yet it introduces exposure bias. To mitigate this, Post Fine-tuning, including on-policy and off-policy methods, has emerged as a solution to enhance models \u2026"}, {"title": "Evaluating Generalization Capability of Language Models across Abductive, Deductive and Inductive Logical Reasoning", "link": "https://aclanthology.org/2025.coling-main.330.pdf", "details": "Y Sheng, W Wen, L Li, D Zeng - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Transformer-based language models (LMs) have demonstrated remarkable performance on many natural language tasks, yet to what extent LMs possess the capability of generalizing to unseen logical rules remains not explored sufficiently. In \u2026"}, {"title": "Clinical Decision Support using Pseudo-notes from multiple streams of EHR Data", "link": "https://openreview.net/pdf%3Fid%3DFG45RbP9Ka", "details": "SA Lee, S Jain, A Chen, K Ono, A Biswas, A Rudas\u2026", "abstract": "Electronic health records (EHR) contain data from disparate sources, spanning various biological and temporal scales. In this work, we introduce the Multiple Embedding Model for EHR (MEME), a deep learning framework for clinical decision \u2026"}, {"title": "META-LORA: Memory-Efficient Sample Reweighting for Fine-Tuning Large Language Models", "link": "https://aclanthology.org/2025.coling-main.568.pdf", "details": "W Li, L Zou, M Tang, Q Yu, W Li, C Li - \u2026 of the 31st International Conference on \u2026, 2025", "abstract": "Supervised fine-tuning (SFT) is widely adopted for tailoring large language models (LLMs) to specific downstream tasks. However, the substantial computational demands of LLMs hinder iterative exploration of fine-tuning datasets and accurate \u2026"}, {"title": "Embedding-Driven Diversity Sampling to Improve Few-Shot Synthetic Data Generation", "link": "https://arxiv.org/pdf/2501.11199", "details": "I Lopez, FN Haredasht, K Caoili, JH Chen, A Chaudhari - arXiv preprint arXiv \u2026, 2025", "abstract": "Accurate classification of clinical text often requires fine-tuning pre-trained language models, a process that is costly and time-consuming due to the need for high-quality data and expert annotators. Synthetic data generation offers an alternative, though \u2026"}, {"title": "Rethinking-based Code Summarization with Chain of Comments", "link": "https://aclanthology.org/2025.coling-main.204.pdf", "details": "L Cao, H He, H Huang, J Wang, Y Cai - \u2026 of the 31st International Conference on \u2026, 2025", "abstract": "Automatic code summarization aims to generate concise natural language descriptions (summary) for source code, which can free software developers from the heavy burden of manual commenting and software maintenance. Existing methods \u2026"}, {"title": "An Empirical Study on Challenging Math Problem Solving with LLM-based Conversational Agents", "link": "https://etda.libraries.psu.edu/files/final_submissions/31835", "details": "Y Wu - 2025", "abstract": "The application of Large Language Models (LLMs) in solving mathematical problems expressed in natural language is a promising area of research, especially given their potential to serve as foundational models across various domains. This paper \u2026"}]
