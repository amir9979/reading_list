[{"title": "Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3637528.3671690", "details": "D Zhu, Z Li, M Zhang, J Yuan, J Liu, K Kuang, C Wu - Proceedings of the 30th ACM \u2026, 2024", "abstract": "Large-scale vision-language (VL) models have demonstrated remarkable generalization capabilities for downstream tasks through prompt tuning. However, the mechanisms behind the learned text representations are unknown, limiting \u2026"}, {"title": "LM-PUB-QUIZ: A Comprehensive Framework for Zero-Shot Evaluation of Relational Knowledge in Language Models", "link": "https://arxiv.org/pdf/2408.15729", "details": "M Ploner, J Wiland, S Pohl, A Akbik - arXiv preprint arXiv:2408.15729, 2024", "abstract": "Knowledge probing evaluates the extent to which a language model (LM) has acquired relational knowledge during its pre-training phase. It provides a cost- effective means of comparing LMs of different sizes and training setups and is useful \u2026"}, {"title": "Reasoning and Planning with Large Language Models in Code Development", "link": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671452", "details": "H Ding, Z Fan, I Guehring, G Gupta, W Ha, J Huan\u2026 - Proceedings of the 30th \u2026, 2024", "abstract": "Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code \u2026"}, {"title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models", "link": "https://arxiv.org/pdf/2408.12798", "details": "Y Li, H Huang, Y Zhao, X Ma, J Sun - arXiv preprint arXiv:2408.12798, 2024", "abstract": "Generative Large Language Models (LLMs) have made significant strides across various tasks, but they remain vulnerable to backdoor attacks, where specific triggers in the prompt cause the LLM to generate adversary-desired responses. While most \u2026"}, {"title": "Modeling perceived information needs in human-AI teams: improving AI teammate utility and driving team cognition", "link": "https://www.tandfonline.com/doi/abs/10.1080/0144929X.2024.2396476", "details": "BG Schelble, C Flathmann, JP Macdonald\u2026 - Behaviour & Information \u2026, 2024", "abstract": "As AI technologies advance, teams are beginning to see AI transition from a tool to a full-fledged teammate. Introducing an AI teammate brings several challenges, ranging from how human teammates perceive their new AI teammates from an \u2026"}, {"title": "Pairwise Proximal Policy Optimization: Language Model Alignment with Comparative RL", "link": "https://openreview.net/pdf%3Fid%3D7iaAlIlV2H", "details": "T Wu, B Zhu, R Zhang, Z Wen, K Ramchandran, J Jiao - First Conference on Language \u2026", "abstract": "LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior is Reinforcement Learning with Human Feedback (RLHF). This involves training a reward model with \u2026"}, {"title": "Path-Consistency: Prefix Enhancement for Efficient Inference in LLM", "link": "https://arxiv.org/pdf/2409.01281", "details": "J Zhu, Y Shen, J Zhao, A Zou - arXiv preprint arXiv:2409.01281, 2024", "abstract": "To enhance the reasoning capabilities of large language models (LLMs), self- consistency has gained significant popularity by combining multiple sampling with majority voting. However, the state-of-the-art self-consistency approaches consume \u2026"}, {"title": "Towards Harnessing Large Language Models as Autonomous Agents for Semantic Triple Extraction from Unstructured Text", "link": "https://ceur-ws.org/Vol-3747/text2kg_paper1.pdf", "details": "A Ananya, S Tiwari, N Mihindukulasooriya, T Soru\u2026 - 2024", "abstract": "Abstract The use of Large Language Models as autonomous agents interacting with tools has shown to improve the performance of several tasks from code generation to API calling and sequencing. This paper proposes a framework for using Large \u2026"}, {"title": "Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators", "link": "https://arxiv.org/pdf/2408.12325", "details": "D Yang, D Xiao, J Wei, M Li, Z Chen, K Li, L Zhang - arXiv preprint arXiv:2408.12325, 2024", "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone to generate responses that contradict verifiable facts, ie, unfaithful hallucination content. Existing efforts generally focus on optimizing model parameters or editing semantic \u2026"}]
