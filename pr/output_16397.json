[{"title": "A self-conformation-aware pre-training framework for molecular property prediction with substructure interpretability", "link": "https://www.nature.com/articles/s41467-025-59634-0", "details": "J Qiao, J Jin, D Wang, S Teng, J Zhang, X Yang, Y Liu\u2026 - Nature Communications, 2025", "abstract": "The major challenges in drug development stem from frequent structure-activity cliffs and unknown drug properties, which are expensive and time-consuming to estimate, contributing to a high rate of failures and substantial unavoidable costs in the clinical \u2026"}, {"title": "MiMo: Unlocking the Reasoning Potential of Language Model--From Pretraining to Posttraining", "link": "https://arxiv.org/pdf/2505.07608", "details": "B Xia, B Shen, D Zhu, D Zhang, G Wang, H Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing \u2026"}, {"title": "Diffusion-driven SpatioTemporal Graph KANsformer for Medical Examination Recommendation", "link": "https://arxiv.org/pdf/2505.07431", "details": "J Li, Y Zhou, Z Zhao, Q Huang, J Qi, X He, H Chu, F Li - arXiv preprint arXiv \u2026, 2025", "abstract": "Recommendation systems in AI-based medical diagnostics and treatment constitute a critical component of AI in healthcare. Although some studies have explored this area and made notable progress, healthcare recommendation systems remain in \u2026"}, {"title": "Learning Dynamics in Continual Pre-Training for Large Language Models", "link": "https://arxiv.org/pdf/2505.07796", "details": "X Wang, H Tissue, L Wang, L Li, DD Zeng - arXiv preprint arXiv:2505.07796, 2025", "abstract": "Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We \u2026"}, {"title": "A Benchmark for Multi-Task Evaluation of Pretrained Models in Medical Report Generation", "link": "https://www.bio-conferences.org/articles/bioconf/pdf/2025/25/bioconf_icbb2025_03010.pdf", "details": "R Lin, C Li, R Wang - BIO Web of Conferences, 2025", "abstract": "MRG for medical images has become increasingly important due to the growing workload of radiologists in hospitals. However, current studies in the MRG field predominantly focus on specific modal-ities or training foundation models with a \u2026"}, {"title": "MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks", "link": "https://arxiv.org/pdf/2505.06152", "details": "W Zeng, Y Sun, C Ma, W Tan, B Yan - arXiv preprint arXiv:2505.06152, 2025", "abstract": "Medical vision-language models (VLMs) have shown promise as clinical assistants across various medical fields. However, specialized dermatology VLM capable of delivering professional and detailed diagnostic analysis remains underdeveloped \u2026"}]
