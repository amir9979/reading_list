[{"title": "Comparative performance of large language models in structuring head CT radiology reports: multi-institutional validation study in Japan", "link": "https://link.springer.com/article/10.1007/s11604-025-01799-1", "details": "H Takita, SL Walston, Y Mitsuyama, K Watanabe\u2026 - Japanese Journal of \u2026, 2025", "abstract": "Purpose To compare the diagnostic performance of three proprietary large language models (LLMs)\u2014Claude, GPT, and Gemini\u2014in structuring free-text Japanese radiology reports for intracranial hemorrhage and skull fractures, and to assess the \u2026"}, {"title": "Hierarchical Document Refinement for Long-context Retrieval-augmented Generation", "link": "https://arxiv.org/pdf/2505.10413", "details": "J Jin, X Li, G Dong, Y Zhang, Y Zhu, Y Wu, Z Li, Q Ye\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Real-world RAG applications often encounter long-context input scenarios, where redundant information and noise results in higher inference costs and reduced performance. To address these challenges, we propose LongRefiner, an efficient \u2026", "entry_id": "http://arxiv.org/abs/2505.10413v1", "updated": "2025-05-15 15:34:15", "published": "2025-05-15 15:34:15", "authors": "Jiajie Jin;Xiaoxi Li;Guanting Dong;Yuyao Zhang;Yutao Zhu;Yongkang Wu;Zhonghua Li;Qi Ye;Zhicheng Dou", "summary": "Real-world RAG applications often encounter long-context input scenarios,\nwhere redundant information and noise results in higher inference costs and\nreduced performance. To address these challenges, we propose LongRefiner, an\nefficient plug-and-play refiner that leverages the inherent structural\ncharacteristics of long documents. LongRefiner employs dual-level query\nanalysis, hierarchical document structuring, and adaptive refinement through\nmulti-task learning on a single foundation model. Experiments on seven QA\ndatasets demonstrate that LongRefiner achieves competitive performance in\nvarious scenarios while using 10x fewer computational costs and latency\ncompared to the best baseline. Further analysis validates that LongRefiner is\nscalable, efficient, and effective, providing practical insights for real-world\nlong-text RAG applications. Our code is available at\nhttps://github.com/ignorejjj/LongRefiner.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.10413v1;http://arxiv.org/pdf/2505.10413v1", "pdf_url": "http://arxiv.org/pdf/2505.10413v1"}, {"title": "Evaluating Logical Reasoning Ability of Large Language Models", "link": "https://www.preprints.org/frontend/manuscript/9e037bed340c892b7e0f2084f1f17252/download_pub", "details": "E Chan - 2025", "abstract": "Large language models (LLMs) such as ChatGPT and DeepSeek have recently made significant progress in natural language processing, demonstrating reasoning ability close to human intelligence. This has sparked considerable research interest \u2026"}]
