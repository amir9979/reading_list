[{"title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style", "link": "https://arxiv.org/pdf/2410.16184%3F", "details": "Y Liu, Z Yao, R Min, Y Cao, L Hou, J Li - arXiv preprint arXiv:2410.16184, 2024", "abstract": "Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward \u2026"}, {"title": "Mixed Distillation Helps Smaller Language Models Reason Better", "link": "https://aclanthology.org/2024.findings-emnlp.91.pdf", "details": "L Chenglin, Q Chen, L Li, C Wang, F Tao, Y Li, Z Chen\u2026 - Findings of the Association \u2026, 2024", "abstract": "As large language models (LLMs) have demonstrated impressive multiple step-by- step reasoning capabilities in recent natural language processing (NLP) reasoning tasks, many studies are interested in distilling reasoning abilities into smaller \u2026"}, {"title": "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "link": "https://aclanthology.org/2024.findings-emnlp.61.pdf", "details": "X Wei, H Chen, H Yu, H Fei, Q Liu - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have achieved notable success in commonsense reasoning tasks, benefiting from their extensive world knowledge acquired through extensive pretraining. While approaches like Chain-of-Thought \u2026"}, {"title": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning", "link": "https://arxiv.org/pdf/2411.05193", "details": "J Hong, A Dragan, S Levine - arXiv preprint arXiv:2411.05193, 2024", "abstract": "Value-based reinforcement learning (RL) can in principle learn effective policies for a wide range of multi-turn problems, from games to dialogue to robotic control, including via offline RL from static previously collected datasets. However, despite \u2026"}, {"title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?", "link": "https://arxiv.org/pdf/2410.23856", "details": "Z Zhou, R Tao, J Zhu, Y Luo, Z Wang, B Han - arXiv preprint arXiv:2410.23856, 2024", "abstract": "This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We \u2026"}, {"title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "link": "https://aclanthology.org/2024.emnlp-main.1220.pdf", "details": "S Singla, Z Wang, T Liu, A Ashfaq, Z Hu, E Xing - \u2026 of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Abstract Aligning Large Language Models (LLMs) traditionally relies on complex and costly training processes like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). To address the challenge of achieving \u2026"}, {"title": "Metaaligner: Towards generalizable multi-objective alignment of language models", "link": "https://openreview.net/pdf%3Fid%3DdIVb5C0QFf", "details": "K Yang, Z Liu, Q Xie, J Huang, T Zhang, S Ananiadou - The Thirty-eighth Annual \u2026, 2024", "abstract": "Recent advancements in large language models (LLMs) focus on aligning to heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are dependent on the policy model \u2026"}, {"title": "Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting", "link": "https://aclanthology.org/2024.findings-emnlp.400.pdf", "details": "M Naguib, X Tannier, A Neveol - Findings of the Association for Computational \u2026, 2024", "abstract": "Large language models (LLMs) have become the preferred solution for many natural language processing tasks. In low-resource environments such as specialized domains, their few-shot capabilities are expected to deliver high performance \u2026"}, {"title": "Utilizing Human Behavior Modeling to Manipulate Explanations in AI-Assisted Decision Making: The Good, the Bad, and the Scary", "link": "https://openreview.net/pdf%3Fid%3D7XkwzaPMvX", "details": "Z Li, M Yin - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Recent advances in AI models have increased the integration of AI-based decision aids into the human decision making process. To fully unlock the potential of AI- assisted decision making, researchers have computationally modeled how humans \u2026"}]
