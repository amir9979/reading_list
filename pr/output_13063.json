[{"title": "The Target Trial Framework for Causal Inference From Observational Data: Why and When Is It Helpful?", "link": "https://www.acpjournals.org/doi/abs/10.7326/ANNALS-24-01871", "details": "MA Hern\u00e1n, IJ Dahabreh, BA Dickerman, SA Swanson - Annals of Internal Medicine, 2025", "abstract": "When randomized trials are not available to answer a causal question about the comparative effectiveness or safety of interventions, causal inferences are drawn using observational data. A helpful 2-step framework for causal inference from \u2026"}, {"title": "Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning", "link": "https://arxiv.org/pdf/2502.11751", "details": "Y Pang, B Yang, H Tu, Y Cao, Z Zhang - arXiv preprint arXiv:2502.11751, 2025", "abstract": "Although Large Language Models (LLMs) excel in reasoning and generation for language tasks, they are not specifically designed for multimodal challenges. Training Multimodal Large Language Models (MLLMs), however, is resource \u2026"}, {"title": "Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2502.06130", "details": "C Zhang, Z Wan, Z Kan, MQ Ma, S Stepputtis\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "While recent Large Vision-Language Models (LVLMs) have shown remarkable performance in multi-modal tasks, they are prone to generating hallucinatory text responses that do not align with the given visual input, which restricts their practical \u2026"}, {"title": "KGGen: Extracting Knowledge Graphs from Plain Text with Language Models", "link": "https://arxiv.org/pdf/2502.09956", "details": "B Mo, K Yu, J Kazdan, P Mpala, L Yu, C Cundy\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent interest in building foundation models for KGs has highlighted a fundamental challenge: knowledge-graph data is relatively scarce. The best-known KGs are primarily human-labeled, created by pattern-matching, or extracted using early NLP \u2026"}, {"title": "K-Bloom: unleashing the power of pre-trained language models in extracting knowledge graph with predefined relations", "link": "https://link.springer.com/article/10.1007/s10115-025-02345-1", "details": "T Vo, ST Luu, LM Nguyen - Knowledge and Information Systems, 2025", "abstract": "Pre-trained language models have become popular in natural language processing tasks, but their inner workings and knowledge acquisition processes remain unclear. To address this issue, we introduce K-Bloom\u2014a refined search-and-score \u2026"}, {"title": "Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation", "link": "https://proceedings.neurips.cc/paper_files/paper/2024/file/e142fd2b70f10db2543c64bca1417de8-Paper-Conference.pdf", "details": "W JIAWEI, R Jiang, C Yang, Z Wu, R Shibasaki\u2026 - Advances in Neural \u2026, 2025", "abstract": "This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively \u2026"}, {"title": "LLM Cyber Evaluations Don't Capture Real-World Risk", "link": "https://arxiv.org/pdf/2502.00072", "details": "K Luko\u0161i\u016bt\u0117, A Swanda - arXiv preprint arXiv:2502.00072, 2025", "abstract": "Large language models (LLMs) are demonstrating increasing prowess in cybersecurity applications, creating creating inherent risks alongside their potential for strengthening defenses. In this position paper, we argue that current efforts to \u2026"}, {"title": "Weak Supervision Performance Evaluation via Partial Identification", "link": "https://proceedings.neurips.cc/paper_files/paper/2024/file/f4c6bec746b0aeca8c2cd15096f1ad1f-Paper-Conference.pdf", "details": "F Maia Polo, S Maity, M Yurochkin, M Banerjee, Y Sun - Advances in Neural \u2026, 2025", "abstract": "Abstract Programmatic Weak Supervision (PWS) enables supervised model training without direct access to ground truth labels, utilizing weak labels from heuristics, crowdsourcing, or pre-trained models. However, the absence of ground truth \u2026"}, {"title": "MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for Few-Shot WSI Classification", "link": "https://arxiv.org/pdf/2502.07409", "details": "AT Nguyen, DMH Nguyen, NT Diep, TQ Nguyen, N Ho\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Whole slide pathology image classification presents challenges due to gigapixel image sizes and limited annotation labels, hindering model generalization. This paper introduces a prompt learning method to adapt large vision-language models \u2026"}]
