[{"title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs", "link": "https://arxiv.org/pdf/2503.01743%3F", "details": "A Abouelenin, A Ashfaq, A Atkinson, H Awadalla\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent \u2026"}, {"title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?", "link": "https://arxiv.org/pdf/2503.02199", "details": "A Deng, T Cao, Z Chen, B Hooi - arXiv preprint arXiv:2503.02199, 2025", "abstract": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual \u2026"}, {"title": "DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models", "link": "https://arxiv.org/pdf/2503.13443", "details": "H Li, L Wang, C Wang, J Jiang, Y Peng, G Long - arXiv preprint arXiv:2503.13443, 2025", "abstract": "The Base-New Trade-off (BNT) problem universally exists during the optimization of CLIP-based prompt tuning, where continuous fine-tuning on base (target) classes leads to a simultaneous decrease of generalization ability on new (unseen) classes \u2026"}, {"title": "Slide-Level Prompt Learning with Vision Language Models for Few-Shot Multiple Instance Learning in Histopathology", "link": "https://arxiv.org/pdf/2503.17238", "details": "D Tomar, G Vray, D Mahapatra, S Roy, JP Thiran\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this paper, we address the challenge of few-shot classification in histopathology whole slide images (WSIs) by utilizing foundational vision-language models (VLMs) and slide-level prompt learning. Given the gigapixel scale of WSIs, conventional \u2026"}, {"title": "Joint Self-Supervised Video Alignment and Action Segmentation", "link": "https://arxiv.org/pdf/2503.16832", "details": "AS Ali, SA Mahmood, M Saeed, A Konin, MZ Zia\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce a novel approach for simultaneous self-supervised video alignment and action segmentation based on a unified optimal transport framework. In particular, we first tackle self-supervised video alignment by developing a fused \u2026"}, {"title": "Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA", "link": "https://arxiv.org/pdf/2503.17933", "details": "J Ou, T Huang, Y Zhao, Z Yu, P Lu, R Ying - arXiv preprint arXiv:2503.17933, 2025", "abstract": "To improve the reliability of Large Language Models (LLMs) in clinical applications, retrieval-augmented generation (RAG) is extensively applied to provide factual medical knowledge. However, beyond general medical knowledge from open-ended \u2026"}, {"title": "LLM-Match: An Open-Sourced Patient Matching Model Based on Large Language Models and Retrieval-Augmented Generation", "link": "https://arxiv.org/pdf/2503.13281", "details": "X Li, S Chowdhury, CI Wi, M Vassilaki, K Liu, TT Sio\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Patient matching is the process of linking patients to appropriate clinical trials by accurately identifying and matching their medical records with trial eligibility criteria. We propose LLM-Match, a novel framework for patient matching leveraging fine \u2026"}, {"title": "GPBench: A Comprehensive and Fine-Grained Benchmark for Evaluating Large Language Models as General Practitioners", "link": "https://arxiv.org/pdf/2503.17599", "details": "Z Li, Y Yang, J Lang, W Jiang, Y Zhao, S Li, D Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "General practitioners (GPs) serve as the cornerstone of primary healthcare systems by providing continuous and comprehensive medical services. However, due to community-oriented nature of their practice, uneven training and resource gaps, the \u2026"}, {"title": "TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster", "link": "https://arxiv.org/pdf/2503.07649", "details": "K Ning, Z Pan, Y Liu, Y Jiang, JY Zhang, K Rasul\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, Large Language Models (LLMs) and Foundation Models (FMs) have become prevalent for time series forecasting tasks. However, fine-tuning large language models (LLMs) for forecasting enables the adaptation to specific domains \u2026"}]
