'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Prometheus 2: An Open Source Language Model Specialize'
[{"title": "Few Shot Class Incremental Learning using Vision-Language models", "link": "https://arxiv.org/pdf/2405.01040", "details": "A Kumar, C Bharti, S Dutta, S Karanam, B Banerjee - arXiv preprint arXiv:2405.01040, 2024", "abstract": "Recent advancements in deep learning have demonstrated remarkable performance comparable to human capabilities across various supervised computer vision tasks. However, the prevalent assumption of having an extensive pool of training data \u2026"}, {"title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "link": "https://arxiv.org/pdf/2405.00402", "details": "L Ranaldi, A Freitas - arXiv preprint arXiv:2405.00402, 2024", "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches \u2026"}, {"title": "AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts", "link": "https://arxiv.org/pdf/2405.00361", "details": "Z Liu, J Luo - arXiv preprint arXiv:2405.00361, 2024", "abstract": "We introduce AdaMoLE, a novel method for fine-tuning large language models (LLMs) through an Adaptive Mixture of Low-Rank Adaptation (LoRA) Experts. Moving beyond conventional methods that employ a static top-k strategy for activating \u2026"}, {"title": "Leveraging Large Language Models for Multimodal Search", "link": "https://arxiv.org/pdf/2404.15790", "details": "O Barbany, M Huang, X Zhu, A Dhua - arXiv preprint arXiv:2404.15790, 2024", "abstract": "Multimodal search has become increasingly important in providing users with a natural and effective way to ex-press their search intentions. Images offer fine- grained details of the desired products, while text allows for easily incorporating \u2026"}, {"title": "Measuring Cross-lingual Transfer in Bytes", "link": "https://arxiv.org/pdf/2404.08191", "details": "LR de Souza, TS Almeida, R Lotufo, R Nogueira - arXiv preprint arXiv:2404.08191, 2024", "abstract": "Multilingual pretraining has been a successful solution to the challenges posed by the lack of resources for languages. These models can transfer knowledge to target languages with minimal or no examples. Recent research suggests that monolingual \u2026"}, {"title": "Temporal Scaling Law for Large Language Models", "link": "https://arxiv.org/pdf/2404.17785", "details": "Y Xiong, X Chen, X Ye, H Chen, Z Lin, H Lian, J Niu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, Large Language Models (LLMs) are widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed as Scaling Laws, have discovered \u2026"}]
