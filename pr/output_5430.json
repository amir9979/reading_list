[{"title": "Zero-Shot Embeddings Inform Learning and Forgetting with Vision-Language Encoders", "link": "https://arxiv.org/pdf/2407.15731", "details": "L Niss, K Vogt-Lowell, T Tsiligkaridis - arXiv preprint arXiv:2407.15731, 2024", "abstract": "Despite the proliferation of large vision-language foundation models, estimation of the learning and forgetting outcomes following fine-tuning of these models remains largely unexplored. Inspired by work highlighting the significance of the modality gap \u2026"}, {"title": "DDK: Distilling Domain Knowledge for Efficient Large Language Models", "link": "https://arxiv.org/pdf/2407.16154", "details": "J Liu, C Zhang, J Guo, Y Zhang, H Que, K Deng, Z Bai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the advanced intelligence abilities of large language models (LLMs) in various applications, they still face significant computational and storage demands. Knowledge Distillation (KD) has emerged as an effective strategy to improve the \u2026"}, {"title": "A federated large language model for long-term time series forecasting", "link": "https://arxiv.org/pdf/2407.20503", "details": "R Abdel-Sater, AB Hamza - arXiv preprint arXiv:2407.20503, 2024", "abstract": "Long-term time series forecasting in centralized environments poses unique challenges regarding data privacy, communication overhead, and scalability. To address these challenges, we propose FedTime, a federated large language model \u2026"}, {"title": "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "link": "https://arxiv.org/pdf/2408.02103", "details": "P Wang, X Wang, C Lou, S Mao, P Xie, Y Jiang - arXiv preprint arXiv:2408.02103, 2024", "abstract": "In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language \u2026"}, {"title": "Boosting Large Language Models with Socratic Method for Conversational Mathematics Teaching", "link": "https://arxiv.org/pdf/2407.17349", "details": "Y Ding, H Hu, J Zhou, Q Chen, B Jiang, L He - arXiv preprint arXiv:2407.17349, 2024", "abstract": "With the introduction of large language models (LLMs), automatic math reasoning has seen tremendous success. However, current methods primarily focus on providing solutions or using techniques like Chain-of-Thought to enhance problem \u2026"}, {"title": "Improving Multilingual Neural Machine Translation by Utilizing Semantic and Linguistic Features", "link": "https://arxiv.org/pdf/2408.01394", "details": "M Bu, S Gu, Y Feng - arXiv preprint arXiv:2408.01394, 2024", "abstract": "The many-to-many multilingual neural machine translation can be regarded as the process of integrating semantic features from the source sentences and linguistic features from the target sentences. To enhance zero-shot translation, models need to \u2026"}, {"title": "Coalitions of Large Language Models Increase the Robustness of AI Agents", "link": "https://arxiv.org/pdf/2408.01380", "details": "P Mangal, C Mak, T Kanakis, T Donovan, D Braines\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence of Large Language Models (LLMs) have fundamentally altered the way we interact with digital systems and have led to the pursuit of LLM powered AI agents to assist in daily workflows. LLMs, whilst powerful and capable of \u2026"}, {"title": "Subjectivity Detection in English News using Large Language Models", "link": "https://aclanthology.org/2024.wassa-1.17.pdf", "details": "M Shokri, V Sharma, E Filatova, S Jain, S Levitan - Proceedings of the 14th Workshop \u2026, 2024", "abstract": "Trust in media has reached a historical low as consumers increasingly doubt the credibility of the news they encounter. This growing skepticism is exacerbated by the prevalence of opinion-driven articles, which can influence readers' beliefs to align \u2026"}, {"title": "RetinaQA: A Robust Knowledge Base Question Answering Model for both Answerable and Unanswerable Questions", "link": "https://aclanthology.org/2024.acl-long.359.pdf", "details": "P Faldu, I Bhattacharya - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "An essential requirement for a real-world Knowledge Base Question Answering (KBQA) system is the ability to detect the answerability of questions when generating logical forms. However, state-of-the-art KBQA models assume all questions to be \u2026"}]
