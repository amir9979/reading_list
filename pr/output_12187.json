[{"title": "LLM360 K2: Scaling Up 360-Open-Source Large Language Models", "link": "https://arxiv.org/pdf/2501.07124", "details": "Z Liu, B Tan, H Wang, W Neiswanger, T Tao, H Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree OPEN SOURCE approach to the largest and most powerful models under project LLM360. While open-source LLMs continue to advance, the answer to\" How are the \u2026"}, {"title": "Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful Behaviors with Proximity Constraints", "link": "https://arxiv.org/pdf/2501.08246", "details": "J N\u00f6ther, A Singla, G Radanovi\u0107 - arXiv preprint arXiv:2501.08246, 2025", "abstract": "Recent work has proposed automated red-teaming methods for testing the vulnerabilities of a given target large language model (LLM). These methods use red- teaming LLMs to uncover inputs that induce harmful behavior in a target LLM. In this \u2026"}, {"title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models", "link": "https://arxiv.org/pdf/2501.04945", "details": "Q Ren, J Zeng, Q He, J Liang, Y Xiao, W Zhou, Z Sun\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "It is crucial for large language models (LLMs) to follow instructions that involve multiple constraints. However, soft constraints are semantically related and difficult to verify through automated methods. These constraints remain a significant challenge \u2026"}, {"title": "Disentangling Exploration of Large Language Models by Optimal Exploitation", "link": "https://arxiv.org/pdf/2501.08925", "details": "T Grams, P Betz, C Bartelt - arXiv preprint arXiv:2501.08925, 2025", "abstract": "Exploration is a crucial skill for self-improvement and open-ended problem-solving. However, it remains uncertain whether large language models can effectively explore the state-space. Existing evaluations predominantly focus on the trade-off \u2026"}, {"title": "Analyzing Memorization in Large Language Models through the Lens of Model Attribution", "link": "https://arxiv.org/pdf/2501.05078", "details": "TR Menta, S Agrawal, C Agarwal - arXiv preprint arXiv:2501.05078, 2025", "abstract": "Large Language Models (LLMs) are prevalent in modern applications but often memorize training data, leading to privacy breaches and copyright issues. Existing research has mainly focused on posthoc analyses, such as extracting memorized \u2026"}, {"title": "Fine-tuning Large Language Models for Improving Factuality in Legal Question Answering", "link": "https://arxiv.org/pdf/2501.06521", "details": "Y Hu, L Gan, W Xiao, K Kuang, F Wu - arXiv preprint arXiv:2501.06521, 2025", "abstract": "Hallucination, or the generation of incorrect or fabricated information, remains a critical challenge in large language models (LLMs), particularly in high-stake domains such as legal question answering (QA). In order to mitigate the hallucination \u2026"}, {"title": "Incorporating Molecular Knowledge in Large Language Models via Multimodal Modeling", "link": "https://ieeexplore.ieee.org/abstract/document/10838383/", "details": "Z Yang, K Lv, J Shu, Z Li, P Xiao - IEEE Transactions on Computational Social \u2026, 2025", "abstract": "In recent years, large language models (LLMs) represented by GPT-4 have achieved tremendous success in natural language-centered tasks. Nevertheless, LLMs face inherent challenges in tasks involving both natural language and molecular \u2026"}, {"title": "Benchmarking Large Language Models via Random Variables", "link": "https://arxiv.org/pdf/2501.11790", "details": "Z Hong, H Wu, S Dong, J Dong, Y Xiao, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "With the continuous advancement of large language models (LLMs) in mathematical reasoning, evaluating their performance in this domain has become a prominent research focus. Recent studies have raised concerns about the reliability of current \u2026"}, {"title": "CallNavi: A Study and Challenge on Function Calling Routing and Invocation in Large Language Models", "link": "https://arxiv.org/pdf/2501.05255", "details": "Y Song, C Lothritz, X Tang, S Ezzini, J Klein\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Interacting with a software system via a chatbot can be challenging, especially when the chatbot needs to generate API calls, in the right order and with the right parameters, to communicate with the system. API calling in chatbot systems poses \u2026"}]
