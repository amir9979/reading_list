[{"title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning", "link": "https://arxiv.org/pdf/2503.15621", "details": "F Cocchi, N Moratelli, D Caffagni, S Sarto, L Baraldi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted the critical roles of both the visual backbone and the underlying language model. While prior work has primarily focused on scaling these components to billions of \u2026"}, {"title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2503.06749", "details": "W Huang, B Jia, Z Zhai, S Cao, Z Ye, F Zhao, Y Hu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning \u2026"}, {"title": "Alignment for Efficient Tool Calling of Large Language Models", "link": "https://arxiv.org/pdf/2503.06708", "details": "H Xu, Z Wang, Z Zhu, L Pan, X Chen, L Chen, K Yu - arXiv preprint arXiv:2503.06708, 2025", "abstract": "Recent advancements in tool learning have enabled large language models (LLMs) to integrate external tools, enhancing their task performance by expanding their knowledge boundaries. However, relying on tools often introduces tradeoffs between \u2026"}, {"title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for Multilingual Medical Question Answering", "link": "https://arxiv.org/pdf/2503.16131", "details": "F Li, Y Chen, H Liu, R Yang, H Yuan, Y Jiang, T Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have shown remarkable progress in medical question answering (QA), yet their effectiveness remains predominantly limited to English due to imbalanced multilingual training data and scarce medical resources \u2026"}, {"title": "InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2503.06692", "details": "Y Yan, Y Shen, Y Liu, J Jiang, M Zhang, J Shao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Advanced reasoning in large language models has achieved remarkable performance on challenging tasks, but the prevailing long-context reasoning paradigm faces critical limitations: quadratic computational scaling with sequence \u2026"}, {"title": "Enhancing Large Language Models on Domain-specific Tasks: A Novel Training Strategy via Domain Adaptation and Preference Alignment", "link": "https://ieeexplore.ieee.org/abstract/document/10890050/", "details": "J Deng, Z Zhang, JK Cheng, J Ma - \u2026 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "In handling complex, domain-specific tasks, particularly in the context of state-owned assets and enterprises (SOAEs), general LLMs suffer from the knowledge gap due to insufficient exposure to domain-specific corpora, and the value disagreement, as \u2026"}, {"title": "LLM-IE: a python package for biomedical generative information extraction with large language models", "link": "https://academic.oup.com/jamiaopen/article/8/2/ooaf012/8071856", "details": "E Hsu, K Roberts - JAMIA open, 2025", "abstract": "Objectives Despite the recent adoption of large language models (LLMs) for biomedical information extraction (IE), challenges in prompt engineering and algorithms persist, with no dedicated software available. To address this, we \u2026"}, {"title": "GRU: Mitigating the Trade-off between Unlearning and Retention for Large Language Models", "link": "https://arxiv.org/pdf/2503.09117", "details": "Y Wang, Q Wang, F Liu, W Huang, Y Du, X Du, B Han - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language model (LLM) unlearning has demonstrated its essential role in removing privacy and copyright-related responses, crucial for their legal and safe applications. However, the pursuit of complete unlearning often comes with \u2026"}, {"title": "Harnessing Multiple Large Language Models: A Survey on LLM Ensemble", "link": "https://arxiv.org/pdf/2502.18036", "details": "Z Chen, J Li, P Chen, Z Li, K Sun, Y Luo, Q Mao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "LLM Ensemble--which involves the comprehensive use of multiple large language models (LLMs), each aimed at handling user queries during downstream inference, to benefit from their individual strengths--has gained substantial attention recently \u2026"}]
