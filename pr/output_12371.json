[{"title": "Prompting robotic modalities (PRM): A structured architecture for centralizing language models in complex systems", "link": "https://www.sciencedirect.com/science/article/pii/S0167739X25000184", "details": "B Benjdira, A Koubaa, AM Ali - Future Generation Computer Systems, 2025", "abstract": "Despite significant advancements in robotics and AI, existing systems often struggle to integrate diverse modalities (eg, image, sound, actuator data) into a unified framework, resulting in fragmented architectures that limit adaptability, scalability \u2026"}, {"title": "WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in Post-Training", "link": "https://arxiv.org/pdf/2501.18511", "details": "B Feuer, C Hegde - arXiv preprint arXiv:2501.18511, 2025", "abstract": "Language model (LLM) post-training, from DPO to distillation, can refine behaviors and unlock new skills, but the open science supporting these post-training techniques is still in its infancy. One limiting factor has been the difficulty of \u2026"}, {"title": "CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language Model Question Answering", "link": "https://arxiv.org/pdf/2501.18457", "details": "Y Wang, Z Fan, Q Wang, M Fung, H Ji - arXiv preprint arXiv:2501.18457, 2025", "abstract": "Large Language Models (LLMs) are pretrained on extensive multilingual corpora to acquire both language-specific cultural knowledge and general knowledge. Ideally, while LLMs should provide consistent responses to culture-independent questions \u2026"}, {"title": "Foundations of Large Language Models", "link": "http://readwise-assets.s3.amazonaws.com/media/wisereads/articles/foundations-of-large-language-/2501.09223v1.pdf", "details": "T Xiao, J Zhu - arXiv preprint arXiv:2501.09223, 2025", "abstract": "Large language models originated from natural language processing, but they have undoubtedly become one of the most revolutionary technological advancements in the field of artificial intelligence in recent years. An important insight brought by large \u2026"}, {"title": "FinMoE: A MoE-based Large Chinese Financial Language Model", "link": "https://aclanthology.org/2025.finnlp-1.4.pdf", "details": "X Zhang, Q Yang - Proceedings of the Joint Workshop of the 9th Financial \u2026, 2025", "abstract": "Large-scale language models have demonstrated remarkable success, achieving strong performance across a variety of general tasks. However, when applied to domain-specific fields, such as finance, these models face challenges due to the \u2026"}, {"title": "Disentangling Exploration of Large Language Models by Optimal Exploitation", "link": "https://arxiv.org/pdf/2501.08925", "details": "T Grams, P Betz, C Bartelt - arXiv preprint arXiv:2501.08925, 2025", "abstract": "Exploration is a crucial skill for self-improvement and open-ended problem-solving. However, it remains uncertain whether large language models can effectively explore the state-space. Existing evaluations predominantly focus on the trade-off \u2026"}, {"title": "LLM-BS: Enhancing Large Language Models for Recommendation through Exogenous Behavior-Semantics Integration", "link": "https://openreview.net/pdf%3Fid%3Drm07DoACiF", "details": "M Hong, Y Xia, Z Wang, J Zhu, Y Wang, S Cai, X Yang\u2026 - THE WEB CONFERENCE 2025", "abstract": "Large language models (LLMs) are increasingly leveraged as foundational backbones in the development of advanced recommender systems, offering enhanced capabilities through their extensive knowledge and reasoning. Existing \u2026"}, {"title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models", "link": "https://arxiv.org/pdf/2501.17905", "details": "M Feng, J Wu, S Zhang, P Shao, R Jin, Z Wen, J Tao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have achieved significant progress across various domains, but their increasing scale results in high computational and memory costs. Recent studies have revealed that LLMs exhibit sparsity, providing the potential to \u2026"}, {"title": "Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models", "link": "https://arxiv.org/pdf/2501.18119", "details": "Q Lin, T Zhao, K He, Z Peng, F Xu, L Huang, J Ma\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To \u2026"}]
