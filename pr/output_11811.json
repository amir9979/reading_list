[{"title": "Foundations of Large Language Models", "link": "https://arxiv.org/pdf/2501.09223", "details": "T Xiao, J Zhu - arXiv preprint arXiv:2501.09223, 2025", "abstract": "This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting- edge technologies. The book is structured into four main chapters, each exploring a \u2026"}, {"title": "Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful Behaviors with Proximity Constraints", "link": "https://arxiv.org/pdf/2501.08246", "details": "J N\u00f6ther, A Singla, G Radanovi\u0107 - arXiv preprint arXiv:2501.08246, 2025", "abstract": "Recent work has proposed automated red-teaming methods for testing the vulnerabilities of a given target large language model (LLM). These methods use red- teaming LLMs to uncover inputs that induce harmful behavior in a target LLM. In this \u2026"}, {"title": "On Linear Representations and Pretraining Data Frequency in Language Models", "link": "https://openreview.net/pdf%3Fid%3D90tmmXyaaV", "details": "J Merullo, NA Smith, S Wiegreffe, Y Elazar", "abstract": "Pretraining data has a direct impact on the behaviors and quality of language models (LMs), but we only understand the most basic principles of this relationship. While most work focuses on pretraining data and downstream task behavior, we look at the \u2026"}, {"title": "Aligning Crowd-Sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10818581/", "details": "MF Wong, CW Tan - IEEE Transactions on Big Data, 2024", "abstract": "This paper studies how AI-assisted programming and large language models (LLM) improve software developers' ability via AI tools (LLM agents) like Github Copilot and Amazon CodeWhisperer, while integrating human feedback to enhance \u2026"}, {"title": "Cascaded Self-Evaluation Augmented Training for Efficient Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2501.05662", "details": "Z Lv, W Wang, J Wang, S Zhang, F Wu - arXiv preprint arXiv:2501.05662, 2025", "abstract": "Efficient Multimodal Large Language Models (EMLLMs) have rapidly advanced recently. Incorporating Chain-of-Thought (CoT) reasoning and step-by-step self- evaluation has improved their performance. However, limited parameters often \u2026"}, {"title": "ArithmeticGPT: empowering small-size large language models with advanced arithmetic skills", "link": "https://link.springer.com/article/10.1007/s10994-024-06681-1", "details": "Z Liu, Y Zheng, Z Yin, J Chen, T Liu, M Tian, W Luo - Machine Learning, 2025", "abstract": "Large language models (LLMs) have shown remarkable capabilities in understanding and generating language across a wide range of domains. However, their performance in advanced arithmetic calculation remains a significant challenge \u2026"}, {"title": "A quantitative analysis of knowledge-learning preferences in large language models in molecular science", "link": "https://www.nature.com/articles/s42256-024-00977-6", "details": "P Liu, J Tao, Z Ren - Nature Machine Intelligence, 2025", "abstract": "Deep learning has significantly advanced molecular modelling and design, enabling an efficient understanding and discovery of novel molecules. In particular, large language models introduce a fresh research paradigm to tackle scientific problems \u2026"}, {"title": "Aligning Large Language Models for Faithful Integrity Against Opposing Argument", "link": "https://arxiv.org/pdf/2501.01336", "details": "Y Zhao, Y Deng, SK Ng, TS Chua - arXiv preprint arXiv:2501.01336, 2025", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks. However, they can be easily misled by unfaithful arguments during conversations, even when their original statements are correct. To \u2026"}, {"title": "Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model", "link": "https://arxiv.org/pdf/2501.02790", "details": "Y Yin, S Yang, Y Xie, Z Yang, Y Sun, H Awadalla\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reinforcement learning from human feedback (RLHF) has been widely adopted to align language models (LMs) with human preference. Prior RLHF works typically take a bandit formulation, which, though intuitive, ignores the sequential nature of LM \u2026"}]
