[{"title": "Harnessing the Intrinsic Knowledge of Pretrained Language Models for Challenging Text Classification Settings", "link": "https://arxiv.org/pdf/2408.15650", "details": "L Gao - arXiv preprint arXiv:2408.15650, 2024", "abstract": "Text classification is crucial for applications such as sentiment analysis and toxic text filtering, but it still faces challenges due to the complexity and ambiguity of natural language. Recent advancements in deep learning, particularly transformer \u2026"}, {"title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism", "link": "https://arxiv.org/pdf/2408.10473", "details": "G Li, X Zhao, L Liu, Z Li, D Li, L Tian, J He, A Sirasao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational \u2026"}, {"title": "Untie the Knots: An Efficient Data Augmentation Strategy for Long-Context Pre-Training in Language Models", "link": "https://arxiv.org/pdf/2409.04774", "details": "J Tian, D Zheng, Y Cheng, R Wang, C Zhang, D Zhang - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLM) have prioritized expanding the context window from which models can incorporate more information. However, training models to handle long contexts presents significant challenges. These include the scarcity of high \u2026"}, {"title": "Improving Extraction of Clinical Event Contextual Properties from Electronic Health Records: A Comparative Study", "link": "https://arxiv.org/pdf/2408.17181", "details": "S Agarwal, T Searle, M Ratas, A Shek, J Teo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Electronic Health Records are large repositories of valuable clinical data, with a significant portion stored in unstructured text format. This textual data includes clinical events (eg, disorders, symptoms, findings, medications and procedures) in \u2026"}, {"title": "Optimizing Large Language Models for Discharge Prediction: Best Practices in Leveraging Electronic Health Record Audit Logs", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/09/13/2024.09.12.24313594.full.pdf", "details": "X Zhang, C Yan, Y Yang, Z Li, Y Feng, BA Malin\u2026 - medRxiv, 2024", "abstract": "Electronic Health Record (EHR) audit log data are increasingly utilized for clinical tasks, from workflow modeling to predictive analyses of discharge events, adverse kidney outcomes, and hospital readmissions. These data encapsulate user-EHR \u2026"}, {"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "link": "https://arxiv.org/pdf/2408.12337", "details": "KS Phogat, SA Puranam, S Dasaratha, C Harsha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain \u2026"}, {"title": "DetoxBench: Benchmarking Large Language Models for Multitask Fraud & Abuse Detection", "link": "https://arxiv.org/pdf/2409.06072", "details": "J Chakraborty, W Xia, A Majumder, D Ma, W Chaabene\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks. However, their practical application in high-stake domains, such as fraud and abuse detection, remains an area that requires further \u2026"}, {"title": "Large language models (LLMs): survey, technical frameworks, and future challenges", "link": "https://link.springer.com/article/10.1007/s10462-024-10888-y", "details": "P Kumar - Artificial Intelligence Review, 2024", "abstract": "Artificial intelligence (AI) has significantly impacted various fields. Large language models (LLMs) like GPT-4, BARD, PaLM, Megatron-Turing NLG, Jurassic-1 Jumbo etc., have contributed to our understanding and application of AI in these domains \u2026"}, {"title": "Enhanced Prompt Learning for Few-shot Text Classification Method", "link": "https://search.proquest.com/openview/58909c856764791ee70d5ec9bee01321/1%3Fpq-origsite%3Dgscholar%26cbl%3D2048897", "details": "L Ruifan, W Zhiyu, F Yuantao, Y Shuqin, Z Guangwei - Beijing Da Xue Xue Bao, 2024", "abstract": "An enhanced prompt learning method (EPL4FTC) for few-shot text classification task is proposed. This algorithm first converts the text classification task into the form of prompt learning based on natural language inference. Thus, the implicit data \u2026"}]
