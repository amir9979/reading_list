[{"title": "Selective Prefix Tuning for Pre-trained Language Models", "link": "https://aclanthology.org/2024.findings-acl.164.pdf", "details": "H Zhang, Z Li, P Wang, H Zhao - Findings of the Association for Computational \u2026, 2024", "abstract": "The prevalent approach for optimizing pre-trained language models in downstream tasks is fine-tuning. However, it is both time-consuming and memory-inefficient. In response, a more efficient method called Prefix Tuning, which insert learnable \u2026"}, {"title": "XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models", "link": "https://aclanthology.org/2024.bionlp-1.35.pdf", "details": "OC Thawakar, AM Shaker, SS Mullappilly, H Cholakkal\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "The latest breakthroughs in large language models (LLMs) and vision-language models (VLMs) have showcased promising capabilities toward performing a wide range of tasks. Such models are typically trained on massive datasets comprising \u2026"}, {"title": "Editable Fairness: Fine-Grained Bias Mitigation in Language Models", "link": "https://arxiv.org/pdf/2408.11843", "details": "R Chen, Y Li, J Yang, JT Zhou, Z Liu - arXiv preprint arXiv:2408.11843, 2024", "abstract": "Generating fair and accurate predictions plays a pivotal role in deploying large language models (LLMs) in the real world. However, existing debiasing methods inevitably generate unfair or incorrect predictions as they are designed and \u2026"}, {"title": "Teaching Small Language Models to Reason for Knowledge-Intensive Multi-Hop Question Answering", "link": "https://aclanthology.org/2024.findings-acl.464.pdf", "details": "X Li, S He, F Lei, JY JunYang, T Su, K Liu, J Zhao - Findings of the Association for \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) can teach small language models (SLMs) to solve complex reasoning tasks (eg, mathematical question answering) by Chain-of- thought Distillation (CoTD). Specifically, CoTD fine-tunes SLMs by utilizing rationales \u2026"}, {"title": "A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models", "link": "https://aclanthology.org/2024.gebnlp-1.1.pdf", "details": "Y Wang, V Demberg - Proceedings of the 5th Workshop on Gender Bias in \u2026, 2024", "abstract": "Pre-trained language models have shown impressive abilities of understanding and generating natural languages. However, they typically inherit undesired human-like bias and stereotypes from training data, which raises concerns about putting these \u2026"}, {"title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.06663", "details": "K Sun, M Dredze - arXiv preprint arXiv:2408.06663, 2024", "abstract": "The development of large language models leads to the formation of a pre-train-then- align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream \u2026"}, {"title": "EPFL-MAKE at \u201cDischarge Me!\u201d: An LLM System for Automatically Generating Discharge Summaries of Clinical Electronic Health Record", "link": "https://aclanthology.org/2024.bionlp-1.61.pdf", "details": "H Wu, P Boulenger, A Faure, B C\u00e9spedes, F Boukil\u2026 - Proceedings of the 23rd \u2026, 2024", "abstract": "This paper presents our contribution to the Streamlining Discharge Documentation shared task organized as part of the ACL'24 workshop. We propose MEDISCHARGE (Meditron-7B Based Medical Summary Generation System for Discharge Me), an \u2026"}, {"title": "Improving Sentence Embeddings with Automatic Generation of Training Data Using Few-shot Examples", "link": "https://aclanthology.org/2024.acl-srw.43.pdf", "details": "S Sato, H Tsukagoshi, R Sasano, K Takeda - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best \u2026"}, {"title": "Does the structure of textual content have an impact on language models for automatic summarization?", "link": "https://aclanthology.org/2024.acl-srw.25.pdf", "details": "E Sauvage, S Campano, L Ouali, C Grouin - Proceedings of the 62nd Annual Meeting \u2026, 2024", "abstract": "The processing of long sequences with models remains a subject in its own right, including automatic summary, despite recent improvements. In this work, we present experiments on the automatic summarization of scientific articles using BART \u2026"}]
