'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Overview of the EHRSQL 2024 Shared Task on Reliable Te'
[{"title": "A taxonomy for advancing systematic error analysis in multi-site electronic health record-based clinical concept extraction", "link": "https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocae101/7671274", "details": "S Fu, L Wang, H He, A Wen, N Zong, A Kumari, F Liu\u2026 - Journal of the American \u2026, 2024", "abstract": "Background Error analysis plays a crucial role in clinical concept extraction, a fundamental subtask within clinical natural language processing (NLP). The process typically involves a manual review of error types, such as contextual and linguistic \u2026"}, {"title": "Learning to Rewrite Prompts for Personalized Text Generation", "link": "https://dl.acm.org/doi/abs/10.1145/3589334.3645408", "details": "C Li, M Zhang, Q Mei, W Kong, M Bendersky - Proceedings of the ACM on Web \u2026, 2024", "abstract": "Facilitated by large language models (LLMs), personalized text generation has become a rapidly growing research direction. Most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the \u2026"}, {"title": "Phi-3 technical report: A highly capable language model locally on your phone", "link": "https://arxiv.org/pdf/2404.14219%3Ftrk%3Dpublic_post_comment-text", "details": "M Abdin, SA Jacobs, AA Awan, J Aneja, A Awadallah\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT \u2026"}, {"title": "MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems", "link": "https://arxiv.org/pdf/2404.09486", "details": "K Li, Y Tian, Q Hu, Z Luo, J Ma - arXiv preprint arXiv:2404.09486, 2024", "abstract": "Programming often involves converting detailed and complex specifications into code, a process during which developers typically utilize visual aids to more effectively convey concepts. While recent developments in Large Multimodal Models \u2026"}, {"title": "Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots", "link": "https://arxiv.org/pdf/2405.07990", "details": "C Wu, Y Ge, Q Guo, J Wang, Z Liang, Z Lu, Y Shan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has attracted significant attention due to their superior performance in visual contexts. However, their capabilities in turning visual figure to executable code, have not been \u2026"}, {"title": "SigBart: Enhanced Pre-training via Salient Content Representation Learning for Social Media Summarization", "link": "https://dl.acm.org/doi/abs/10.1145/3589335.3652505", "details": "S Sotudeh, N Goharian - Companion Proceedings of the ACM on Web \u2026, 2024", "abstract": "Our approach to automatically summarizing online mental health posts could help counselors by reducing their reading time, enabling quicker and more effective support for individuals seeking mental health assistance. Neural text summarization \u2026"}]
