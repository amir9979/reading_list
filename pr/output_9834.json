[{"title": "Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image Classification Using Large Language Models", "link": "https://link.springer.com/chapter/10.1007/978-3-031-78495-8_10", "details": "A Scius-Bertrand, M Jungo, L V\u00f6gtlin, JM Spat\u2026 - International Conference on \u2026, 2025", "abstract": "Classifying scanned documents is a challenging problem that involves image, layout, and text analysis for document understanding. Nevertheless, for certain benchmark datasets, notably RVL-CDIP, the state of the art is closing in to near-perfect \u2026"}, {"title": "Factuality of Large Language Models: A Survey", "link": "https://aclanthology.org/2024.emnlp-main.1088.pdf", "details": "Y Wang, M Wang, MA Manzoor, F Liu, G Georgiev\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a \u2026"}, {"title": "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "link": "https://aclanthology.org/2024.findings-emnlp.693.pdf", "details": "M Li, W Wang, F Feng, F Zhu, Q Wang, TS Chua - Findings of the Association for \u2026, 2024", "abstract": "Abstract Self-detection for Large Language Models (LLMs) seeks to evaluate the trustworthiness of the LLM's output by leveraging its own capabilities, thereby alleviating the issue of output hallucination. However, existing self-detection \u2026"}, {"title": "Distract Large Language Models for Automatic Jailbreak Attack", "link": "https://aclanthology.org/2024.emnlp-main.908.pdf", "details": "Z Xiao, Y Yang, G Chen, Y Chen - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Extensive efforts have been made before the public release of Large language models (LLMs) to align their behaviors with human values. However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as \u2026"}, {"title": "Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models", "link": "https://aclanthology.org/2024.emnlp-main.699.pdf", "details": "M Lan, P Torr, F Barez - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits \u2026"}, {"title": "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.313.pdf", "details": "M Xu, Y Li, K Sun, T Qian - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Large language models (LLMs) have shown excellent capability for solving reasoning problems. Existing approaches do not differentiate the question difficulty when designing prompting methods for them. Clearly, a simple method cannot elicit \u2026"}, {"title": "DecoPrompt: Decoding Prompts Reduces Hallucinations when Large Language Models Meet False Premises", "link": "https://arxiv.org/pdf/2411.07457", "details": "N Xu, X Ma - arXiv preprint arXiv:2411.07457, 2024", "abstract": "While large language models (LLMs) have demonstrated increasing power, they have also called upon studies on their hallucinated outputs that deviate from factually correct statements. In this paper, we focus on one important scenario of false \u2026"}]
