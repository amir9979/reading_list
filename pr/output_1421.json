'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Overview of the EHRSQL 2024 Shared Task on Reliable Te'
[{"title": "On the Generalizability of Machine Learning Classification Algorithms and Their Application to the Framingham Heart Study", "link": "https://www.mdpi.com/2078-2489/15/5/252/pdf", "details": "N Kahouadji - Information, 2024", "abstract": "The use of machine learning algorithms in healthcare can amplify social injustices and health inequities. While the exacerbation of biases can occur and be compounded during problem selection, data collection, and outcome definition, this \u2026"}, {"title": "Multi-Task Deep Neural Networks for Irregularly Sampled Multivariate Clinical Time Series", "link": "https://www.researchgate.net/profile/Yuxi-Liu-28/publication/380431922_Multi-Task_Deep_Neural_Networks_for_Irregularly_Sampled_Multivariate_Clinical_Time_Series/links/663c60e906ea3d0b74433150/Multi-Task-Deep-Neural-Networks-for-Irregularly-Sampled-Multivariate-Clinical-Time-Series.pdf", "details": "Y Liu, Z Zhang, S Qin, J Bian", "abstract": "Multivariate clinical time series data, such as those contained in Electronic Health Records (EHR), often exhibit high levels of irregularity, notably, many missing values and varying time intervals. Existing methods usually construct deep neural network \u2026"}, {"title": "Large Language Models Are Poor Medical Coders\u2014Benchmarking of Medical Code Querying", "link": "https://ai.nejm.org/doi/pdf/10.1056/AIdbp2300040%3Fdownload%3Dtrue", "details": "A Soroush, BS Glicksberg, E Zimlichman, Y Barash\u2026 - NEJM AI, 2024", "abstract": "Abstract Background Large language models (LLMs) have attracted significant interest for automated clinical coding. However, early data show that LLMs are highly error-prone when mapping medical codes. We sought to quantify and benchmark \u2026"}, {"title": "MEDVOC: Vocabulary Adaptation for Fine-tuning Pre-trained Language Models on Medical Text Summarization", "link": "https://arxiv.org/pdf/2405.04163", "details": "G Balde, S Roy, M Mondal, N Ganguly - arXiv preprint arXiv:2405.04163, 2024", "abstract": "This work presents a dynamic vocabulary adaptation strategy, MEDVOC, for fine- tuning pre-trained language models (PLMs) like BertSumAbs, BART, and PEGASUS for improved medical text summarization. In contrast to existing domain adaptation \u2026"}, {"title": "Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games", "link": "https://openreview.net/pdf%3Fid%3Ddhy1NBeusb", "details": "Y Zhang, J Lu, N Jaitly - ICLR 2024 Workshop: How Far Are We From AGI", "abstract": "Large language models (LLMs) are effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of \u2026"}]
