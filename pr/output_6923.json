[{"title": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.14470", "details": "A Agarwal, SK Ramesh, A Sengupta, T Chakraborty - arXiv preprint arXiv:2408.14470, 2024", "abstract": "Fine-tuning large language models (LLMs) on downstream tasks requires substantial computational resources. A class of parameter-efficient fine-tuning (PEFT) aims to mitigate these computational challenges by selectively fine-tuning only a small \u2026"}, {"title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles", "link": "https://arxiv.org/pdf/2409.10502", "details": "K Shah, N Dikkala, X Wang, R Panigrahy - arXiv preprint arXiv:2409.10502, 2024", "abstract": "Causal language modeling using the Transformer architecture has yielded remarkable capabilities in Large Language Models (LLMs) over the last few years. However, the extent to which fundamental search and reasoning capabilities \u2026"}, {"title": "A Comprehensive Analysis of Memorization in Large Language Models", "link": "https://aclanthology.org/2024.inlg-main.45.pdf", "details": "H Kiyomaru, I Sugiura, D Kawahara, S Kurohashi - Proceedings of the 17th \u2026, 2024", "abstract": "This paper presents a comprehensive study that investigates memorization in large language models (LLMs) from multiple perspectives. Experiments are conducted with the Pythia and LLM-jp model suites, both of which offer LLMs with over 10B \u2026"}]
