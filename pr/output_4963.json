[{"title": "SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers", "link": "https://arxiv.org/pdf/2407.09413", "details": "S Pramanick, R Chellappa, S Venugopalan - arXiv preprint arXiv:2407.09413, 2024", "abstract": "Seeking answers to questions within long scientific research articles is a crucial area of study that aids readers in quickly addressing their inquiries. However, existing question-answering (QA) datasets based on scientific papers are limited in scale and \u2026"}, {"title": "Reporting and Analysing the Environmental Impact of Language Models on the Example of Commonsense Question Answering with External Knowledge", "link": "https://arxiv.org/pdf/2408.01453", "details": "A Usmanova, J Huang, D Banerjee, R Usbeck - arXiv preprint arXiv:2408.01453, 2024", "abstract": "Human-produced emissions are growing at an alarming rate, causing already observable changes in the climate and environment in general. Each year global carbon dioxide emissions hit a new record, and it is reported that 0.5% of total US \u2026"}, {"title": "A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models", "link": "https://arxiv.org/pdf/2408.02377", "details": "V Zavarella, JC Gamero-Salinas, S Consoli - arXiv preprint arXiv:2408.02377, 2024", "abstract": "Knowledge graphs (KGs) have been successfully applied to the analysis of complex scientific and technological domains, with automatic KG generation methods typically building upon relation extraction models capturing fine-grained relations between \u2026"}, {"title": "A Closer Look at Benchmarking Self-Supervised Pre-training with Image Classification", "link": "https://arxiv.org/pdf/2407.12210", "details": "M Marks, M Knott, N Kondapaneni, E Cole, T Defraeye\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-supervised learning (SSL) is a machine learning approach where the data itself provides supervision, eliminating the need for external labels. The model is forced to learn about the data structure or context by solving a pretext task. With SSL, models \u2026"}, {"title": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models", "link": "https://arxiv.org/pdf/2408.01933", "details": "B Wang, J Chang, Y Qian, G Chen, J Chen, Z Jiang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have recently showcased remarkable capabilities, spanning a wide range of tasks and applications, including those in the medical domain. Models like GPT-4 excel in medical question answering but may face \u2026"}, {"title": "MedSyn: LLM-based Synthetic Medical Text Generation Framework", "link": "https://arxiv.org/pdf/2408.02056", "details": "G Kumichev, P Blinov, Y Kuzkina, V Goncharov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Generating synthetic text addresses the challenge of data availability in privacy- sensitive domains such as healthcare. This study explores the applicability of synthetic data in real-world medical settings. We introduce MedSyn, a novel medical \u2026"}, {"title": "Large Language Models for Tabular Data: Progresses and Future Directions", "link": "https://dl.acm.org/doi/pdf/10.1145/3626772.3661384", "details": "H Dong, Z Wang - Proceedings of the 47th International ACM SIGIR \u2026, 2024", "abstract": "Tables contain a significant portion of the world's structured information. The ability to efficiently and accurately understand, process, reason about, analyze, and generate tabular data is critical for achieving Artificial General Intelligence (AGI) systems \u2026"}, {"title": "SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training", "link": "https://arxiv.org/pdf/2407.06654", "details": "N He, W Xiong, H Liu, Y Liao, L Ding, K Zhang, G Tang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The effectiveness of large language models (LLMs) is often hindered by duplicated data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, which risks the loss of valuable information and \u2026"}, {"title": "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "link": "https://arxiv.org/pdf/2408.02103", "details": "P Wang, X Wang, C Lou, S Mao, P Xie, Y Jiang - arXiv preprint arXiv:2408.02103, 2024", "abstract": "In-context learning (ICL) is a few-shot learning paradigm that involves learning mappings through input-output pairs and appropriately applying them to new instances. Despite the remarkable ICL capabilities demonstrated by Large Language \u2026"}]
