[{"title": "Few-shot Adaptation of Medical Vision-Language Models", "link": "https://arxiv.org/pdf/2409.03868", "details": "F Shakeri, Y Huang, J Silva-Rodr\u00edguez, H Bahig\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Integrating image and text data through multi-modal learning has emerged as a new approach in medical imaging research, following its successful deployment in computer vision. While considerable efforts have been dedicated to establishing \u2026"}, {"title": "Selective Prefix Tuning for Pre-trained Language Models", "link": "https://aclanthology.org/2024.findings-acl.164.pdf", "details": "H Zhang, Z Li, P Wang, H Zhao - Findings of the Association for Computational \u2026, 2024", "abstract": "The prevalent approach for optimizing pre-trained language models in downstream tasks is fine-tuning. However, it is both time-consuming and memory-inefficient. In response, a more efficient method called Prefix Tuning, which insert learnable \u2026"}, {"title": "Exploiting Pre-trained Language Models for Black-box Attack against Knowledge Graph Embeddings", "link": "https://dl.acm.org/doi/pdf/10.1145/3688850", "details": "G Yang, L Zhang, Y Liu, H Xie, Z Mao - ACM Transactions on Knowledge Discovery \u2026, 2024", "abstract": "Despite the emerging research on adversarial attacks against Knowledge Graph Embedding (KGE) models, most of them focus on white-box attack settings. However, white-box attacks are difficult to apply in practice compared to black-box attacks \u2026"}, {"title": "Teaching Small Language Models to Reason for Knowledge-Intensive Multi-Hop Question Answering", "link": "https://aclanthology.org/2024.findings-acl.464.pdf", "details": "X Li, S He, F Lei, JY JunYang, T Su, K Liu, J Zhao - Findings of the Association for \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) can teach small language models (SLMs) to solve complex reasoning tasks (eg, mathematical question answering) by Chain-of- thought Distillation (CoTD). Specifically, CoTD fine-tunes SLMs by utilizing rationales \u2026"}, {"title": "Unraveling the Inner Workings of Massive Language Models: Architecture, Training, and Linguistic Capacities", "link": "https://www.igi-global.com/chapter/unraveling-the-inner-workings-of-massive-language-models/354398", "details": "CVS Babu, CSA Anniyappa - Challenges in Large Language Model Development \u2026, 2024", "abstract": "This study explores the evolution of language models, emphasizing the shift from traditional statistical methods to advanced neural networks, particularly the transformer architecture. It aims to understand the impact of these advancements on \u2026"}, {"title": "Language Models Pre-training", "link": "https://link.springer.com/content/pdf/10.1007/978-3-031-65647-7_2.pdf", "details": "U Kamath, K Keenan, G Somers, S Sorenson - Large Language Models: A Deep Dive \u2026, 2024", "abstract": "Pre-training forms the foundation for LLMs' capabilities. LLMs gain vital language comprehension and generative language skills by using large-scale datasets. The size and quality of these datasets are essential for maximizing LLMs' potential. It is \u2026"}, {"title": "Cross-Domain Abbreviation Disambiguation on Vietnamese Clinical Texts in Online Processing", "link": "https://link.springer.com/chapter/10.1007/978-3-031-70259-4_10", "details": "C Vo, HP Nguyen - International Conference on Computational Collective \u2026, 2024", "abstract": "Readability of clinical texts in electronic medical records (EMRs) is more significant when EMRs are shared and required to be understood by both human users and computer programs. This feature is seldom reached successfully in the real world \u2026"}, {"title": "Improving Sentence Embeddings with Automatic Generation of Training Data Using Few-shot Examples", "link": "https://aclanthology.org/2024.acl-srw.43.pdf", "details": "S Sato, H Tsukagoshi, R Sasano, K Takeda - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best \u2026"}, {"title": "Analysis of Plan-based Retrieval for Grounded Text Generation", "link": "https://arxiv.org/pdf/2408.10490", "details": "A Godbole, N Monath, S Kim, AS Rawat, A McCallum\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In text generation, hallucinations refer to the generation of seemingly coherent text that contradicts established knowledge. One compelling hypothesis is that hallucinations occur when a language model is given a generation task outside its \u2026"}]
