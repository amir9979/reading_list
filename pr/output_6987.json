[{"title": "Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts", "link": "https://arxiv.org/abs/2409.13728", "details": "A M\u00e9sz\u00e1ros, S Ujv\u00e1ry, W Brendel, P Reizinger\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "LLMs show remarkable emergent abilities, such as inferring concepts from presumably out-of-distribution prompts, known as in-context learning. Though this success is often attributed to the Transformer architecture, our systematic \u2026"}, {"title": "Forecasting Causal Effects of Future Interventions: Confounding and Transportability Issues", "link": "https://arxiv.org/pdf/2409.13060", "details": "L Forastiere, F Li, M Baccini - arXiv preprint arXiv:2409.13060, 2024", "abstract": "Recent developments in causal inference allow us to transport a causal effect of a time-fixed treatment from a randomized trial to a target population across space but within the same time frame. In contrast to transportability across space, transporting \u2026"}, {"title": "Leveraging Large Language Models for Building Interpretable Rule-Based Data-to-Text Systems", "link": "https://aclanthology.org/2024.inlg-main.48.pdf", "details": "J Warczy\u0144ski, M Lango, O Du\u0161ek - Proceedings of the 17th International Natural \u2026, 2024", "abstract": "We introduce a simple approach that uses a large language model (LLM) to automatically implement a fully interpretable rule-based data-to-text system in pure Python. Experimental evaluation on the WebNLG dataset showed that such a \u2026"}, {"title": "Modeling perceived information needs in human-AI teams: improving AI teammate utility and driving team cognition", "link": "https://www.tandfonline.com/doi/abs/10.1080/0144929X.2024.2396476", "details": "BG Schelble, C Flathmann, JP Macdonald\u2026 - Behaviour & Information \u2026, 2024", "abstract": "As AI technologies advance, teams are beginning to see AI transition from a tool to a full-fledged teammate. Introducing an AI teammate brings several challenges, ranging from how human teammates perceive their new AI teammates from an \u2026"}, {"title": "Generative Models for Counterfactual Explanations", "link": "https://human-interpretable-ai.github.io/assets/pdf/5_Generative_Models_for_Counte.pdf", "details": "D Kirilenko, P Barbiero, M Gjoreski, M Lu\u0161trek\u2026 - 2024", "abstract": "Counterfactual explanations have emerged as an effective method of explaining machine learning models. These explanations elucidate how to tweak the model input in order to flip its output. Generative approaches serve as a tool for creating \u2026"}, {"title": "Knowledge Planning in Large Language Models for Domain-Aligned Counseling Summarization", "link": "https://arxiv.org/pdf/2409.14907", "details": "A Srivastava, S Joshi, T Chakraborty, MS Akhtar - arXiv preprint arXiv:2409.14907, 2024", "abstract": "In mental health counseling, condensing dialogues into concise and relevant summaries (aka counseling notes) holds pivotal significance. Large Language Models (LLMs) exhibit remarkable capabilities in various generative tasks; however \u2026"}, {"title": "Investigating Layer Importance in Large Language Models", "link": "https://arxiv.org/pdf/2409.14381", "details": "Y Zhang, Y Dong, K Kawaguchi - arXiv preprint arXiv:2409.14381, 2024", "abstract": "Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in \u2026"}, {"title": "CITI: Enhancing Tool Utilizing Ability in Large Language Models without Sacrificing General Performance", "link": "https://arxiv.org/pdf/2409.13202", "details": "Y Hao, P Cao, Z Jin, H Liao, K Liu, J Zhao - arXiv preprint arXiv:2409.13202, 2024", "abstract": "Tool learning enables the Large Language Models (LLMs) to interact with the external environment by invoking tools, enriching the accuracy and capability scope of LLMs. However, previous works predominantly focus on improving model's tool \u2026"}, {"title": "Towards Building a Robust Knowledge Intensive Question Answering Model with Large Language Models", "link": "https://arxiv.org/pdf/2409.05385", "details": "HX Hong, SY Shao, WZ Wang, DM Duan, J Xiongnan - arXiv preprint arXiv \u2026, 2024", "abstract": "The development of LLMs has greatly enhanced the intelligence and fluency of question answering, while the emergence of retrieval enhancement has enabled models to better utilize external information. However, the presence of noise and \u2026"}]
