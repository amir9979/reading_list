[{"title": "Fewer Tokens and Fewer Videos: Extending Video Understanding Abilities in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2406.08024", "details": "S Chen, Y Yuan, S Chen, Z Jie, L Ma - arXiv preprint arXiv:2406.08024, 2024", "abstract": "Amidst the advancements in image-based Large Vision-Language Models (image- LVLM), the transition to video-based models (video-LVLM) is hindered by the limited availability of quality video data. This paper addresses the challenge by leveraging \u2026"}, {"title": "WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences", "link": "https://arxiv.org/pdf/2406.11069", "details": "Y Lu, D Jiang, W Chen, WY Wang, Y Choi, BY Lin - arXiv preprint arXiv:2406.11069, 2024", "abstract": "Recent breakthroughs in vision-language models (VLMs) emphasize the necessity of benchmarking human preferences in real-world multimodal interactions. To address this gap, we launched WildVision-Arena (WV-Arena), an online platform that collects \u2026"}, {"title": "Bridging Operator Learning and Conditioned Neural Fields: A Unifying Perspective", "link": "https://arxiv.org/pdf/2405.13998", "details": "S Wang, JH Seidman, S Sankaran, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Operator learning is an emerging area of machine learning which aims to learn mappings between infinite dimensional function spaces. Here we uncover a connection between operator learning architectures and conditioned neural fields \u2026"}, {"title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models", "link": "https://aclanthology.org/2024.findings-naacl.18.pdf", "details": "Z Su, Z Lin, B Baixue, H Chen, S Hu, W Zhou, G Ding\u2026 - Findings of the Association \u2026, 2024", "abstract": "Generative language models are usually pre-trained on large text corpus via predicting the next token (ie, sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language \u2026"}, {"title": "GECKO: Generative Language Model for English, Code and Korean", "link": "https://arxiv.org/pdf/2405.15640", "details": "S Oh, D Kim - arXiv preprint arXiv:2405.15640, 2024", "abstract": "We introduce GECKO, a bilingual large language model (LLM) optimized for Korean and English, along with programming languages. GECKO is pretrained on the balanced, high-quality corpus of Korean and English employing LLaMA architecture \u2026"}, {"title": "Textual Inversion and Self-supervised Refinement for Radiology Report Generation", "link": "https://arxiv.org/pdf/2405.20607", "details": "Y Luo, H Li, X Wu, M Cao, X Huang, Z Zhu, P Liao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing mainstream approaches follow the encoder-decoder paradigm for generating radiology reports. They focus on improving the network structure of encoders and decoders, which leads to two shortcomings: overlooking the modality \u2026"}, {"title": "P3Sum: Preserving Author's Perspective in News Summarization with Diffusion Language Models", "link": "https://aclanthology.org/2024.naacl-long.119.pdf", "details": "Y Liu, S Feng, X Han, V Balachandran, CY Park\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "In this work, we take a first step towards designing summarization systems that are faithful to the author's intent, not only the semantic content of the article. Focusing on a case study of preserving political perspectives in news summarization, we find that \u2026"}, {"title": "ViGLUE: A Vietnamese General Language Understanding Benchmark and Analysis of Vietnamese Language Models", "link": "https://aclanthology.org/2024.findings-naacl.261.pdf", "details": "MN Tran, PV Nguyen, L Nguyen, D Dien - Findings of the Association for \u2026, 2024", "abstract": "As the number of language models has increased, various benchmarks have been suggested to assess the proficiency of the models in natural language understanding. However, there is a lack of such a benchmark in Vietnamese due to \u2026"}, {"title": "CarLLaVA: Vision language models for camera-only closed-loop driving", "link": "https://arxiv.org/abs/2406.10165", "details": "K Renz, L Chen, AM Marcu, J H\u00fcnermann, B Hanotte\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this technical report, we present CarLLaVA, a Vision Language Model (VLM) for autonomous driving, developed for the CARLA Autonomous Driving Challenge 2.0. CarLLaVA uses the vision encoder of the LLaVA VLM and the LLaMA architecture as \u2026"}]
