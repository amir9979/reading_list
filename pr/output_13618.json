[{"title": "Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual Attention for Multimodal LLMs", "link": "https://arxiv.org/pdf/2503.02597", "details": "WY Wang, Z Wang, H Suzuki, Y Kobayashi - arXiv preprint arXiv:2503.02597, 2025", "abstract": "Recent Multimodal Large Language Models (MLLMs) have demonstrated significant progress in perceiving and reasoning over multimodal inquiries, ushering in a new research era for foundation models. However, vision-language misalignment in \u2026"}, {"title": "MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data with Vision Generation Task using Discrete Visual Representations", "link": "https://arxiv.org/pdf/2503.01019", "details": "Z Zhang, Y Yu, Y Chen, X Yang, SY Yeo - arXiv preprint arXiv:2503.01019, 2025", "abstract": "Despite significant progress in Vision-Language Pre-training (VLP), current approaches predominantly emphasize feature extraction and cross-modal comprehension, with limited attention to generating or transforming visual content \u2026"}, {"title": "A dual adversarial structure of generative adversarial network for nature language generation", "link": "https://www.emerald.com/insight/content/doi/10.1108/IMDS-05-2024-0435/full/html", "details": "KL Sue, YC Chen - Industrial Management & Data Systems, 2025", "abstract": "Purpose Recently, due to the practicability in several domains, generative adversarial network (GAN) has successfully been adopted in the field of natural language generation (NLG). The purpose of this paper focuses on improving the \u2026"}, {"title": "Medical foundation large language models for comprehensive text analysis and beyond", "link": "https://www.nature.com/articles/s41746-025-01533-1", "details": "Q Xie, Q Chen, A Chen, C Peng, Y Hu, F Lin, X Peng\u2026 - npj Digital Medicine, 2025", "abstract": "Recent advancements in large language models (LLMs) show significant potential in medical applications but are hindered by limited specialized medical knowledge. We present Me-LLaMA, a family of open-source medical LLMs integrating extensive \u2026"}]
