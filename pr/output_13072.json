[{"title": "Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding", "link": "https://arxiv.org/pdf/2502.09906", "details": "TD Truong, HQ Nguyen, XB Nguyen, A Dowling, X Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual \u2026"}, {"title": "Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More", "link": "https://arxiv.org/pdf/2502.11494", "details": "Z Wen, Y Gao, S Wang, J Zhang, Q Zhang, W Li, C He\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision tokens in multimodal large language models often dominate huge computational overhead due to their excessive length compared to linguistic modality. Abundant recent methods aim to solve this problem with token pruning \u2026"}, {"title": "Auditing Prompt Caching in Language Model APIs", "link": "https://arxiv.org/pdf/2502.07776", "details": "C Gu, XL Li, R Kuditipudi, P Liang, T Hashimoto - arXiv preprint arXiv:2502.07776, 2025", "abstract": "Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if \u2026"}, {"title": "KGGen: Extracting Knowledge Graphs from Plain Text with Language Models", "link": "https://arxiv.org/pdf/2502.09956", "details": "B Mo, K Yu, J Kazdan, P Mpala, L Yu, C Cundy\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent interest in building foundation models for KGs has highlighted a fundamental challenge: knowledge-graph data is relatively scarce. The best-known KGs are primarily human-labeled, created by pattern-matching, or extracted using early NLP \u2026"}, {"title": "Unsupervised extractive opinion summarization based on text simplification and sentiment guidance", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425003823", "details": "R Wang, T Lan, Z Wu, L Liu - Expert Systems with Applications, 2025", "abstract": "Unsupervised opinion summarization aims to extract representative content from a set of reviews without relying on golden references. Traditional unsupervised methods often struggle with non-consensus opinions and lack conciseness in the \u2026"}, {"title": "Information Extraction from Clinical Texts with Generative Pre-trained Transformer Models", "link": "https://www.medsci.org/v22p1015.htm", "details": "MS Kim, P Chung, N Aghaeepour, N Kim - International Journal of Medical Sciences, 2025", "abstract": "Purpose: Processing and analyzing clinical texts are challenging due to its unstructured nature. This study compared the performance of GPT (Generative Pre- trained Transformer)-3.5 and GPT-4 for extracting information from clinical text \u2026"}, {"title": "Refine Knowledge of Large Language Models via Adaptive Contrastive Learning", "link": "https://arxiv.org/pdf/2502.07184", "details": "Y Li, H Huang, J Kuang, Y Li, SY Guo, C Qu, X Tan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "How to alleviate the hallucinations of Large Language Models (LLMs) has always been the fundamental goal pursued by the LLMs research community. Looking through numerous hallucination-related studies, a mainstream category of methods \u2026"}, {"title": "Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2502.07601", "details": "J Xu, SY Lo, B Safaei, VM Patel, I Dwivedi - arXiv preprint arXiv:2502.07601, 2025", "abstract": "Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the traditional unsupervised AD setting that requires a large number of normal samples to train a model, ZSAD is more practical for handling data-restricted real-world \u2026"}, {"title": "Bridging Contrastive Learning and Domain Adaptation: Theoretical Perspective and Practical Application", "link": "https://arxiv.org/pdf/2502.00052", "details": "GI Quintana, L Vancamberg, V Jugnon, A Desolneux\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This work studies the relationship between Contrastive Learning and Domain Adaptation from a theoretical perspective. The two standard contrastive losses, NT- Xent loss (Self-supervised) and Supervised Contrastive loss, are related to the Class \u2026"}]
