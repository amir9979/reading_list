[{"title": "Low-Rank Few-Shot Adaptation of Vision-Language Models", "link": "https://arxiv.org/pdf/2405.18541", "details": "M Zanella, IB Ayed - arXiv preprint arXiv:2405.18541, 2024", "abstract": "Recent progress in the few-shot adaptation of Vision-Language Models (VLMs) has further pushed their generalization capabilities, at the expense of just a few labeled samples within the target downstream task. However, this promising, already quite \u2026"}, {"title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective", "link": "https://arxiv.org/pdf/2405.16747", "details": "A Tomihari, I Sato - arXiv preprint arXiv:2405.16747, 2024", "abstract": "The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data. This success is largely \u2026"}, {"title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models", "link": "https://arxiv.org/pdf/2406.02224", "details": "T Fan, G Ma, Y Kang, H Gu, L Fan, Q Yang - arXiv preprint arXiv:2406.02224, 2024", "abstract": "Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small \u2026"}, {"title": "Few-Shot Learning With Enhancements to Data Augmentation and Feature Extraction", "link": "https://ieeexplore.ieee.org/abstract/document/10547346/", "details": "Y Zhang, M Gong, J Li, K Feng, M Zhang - IEEE Transactions on Neural Networks and \u2026, 2024", "abstract": "The few-shot image classification task is to enable a model to identify novel classes by using only a few labeled samples as references. In general, the more knowledge a model has, the more robust it is when facing novel situations. Although directly \u2026"}, {"title": "EVA-X: A Foundation Model for General Chest X-ray Analysis with Self-supervised Learning", "link": "https://arxiv.org/pdf/2405.05237", "details": "J Yao, X Wang, Y Song, H Zhao, J Ma, Y Chen, W Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The diagnosis and treatment of chest diseases play a crucial role in maintaining human health. X-ray examination has become the most common clinical examination means due to its efficiency and cost-effectiveness. Artificial intelligence analysis \u2026"}, {"title": "Non-Expert Programmers in the Generative AI Future", "link": "https://www.feldmanmolly.com/chiwork2024-author-version.pdf", "details": "MQ Feldman, CJ Anderson - 2024", "abstract": "Generative AI is rapidly transforming the practice of programming. At the same time, our understanding of who writes programs, for what purposes, and how they program, has been evolving. By facilitating natural-language-to-code interactions \u2026"}, {"title": "Exploring and Mitigating Shortcut Learning for Generative Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.602.pdf", "details": "Z Sun, Y Xiao, J Li, Y Ji, W Chen, M Zhang - Proceedings of the 2024 Joint \u2026, 2024", "abstract": "Recent generative large language models (LLMs) have exhibited incredible instruction-following capabilities while keeping strong task completion ability, even without task-specific fine-tuning. Some works attribute this to the bonus of the new \u2026"}, {"title": "A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks", "link": "https://arxiv.org/pdf/2405.10251", "details": "X Ni, P Li - arXiv preprint arXiv:2405.10251, 2024", "abstract": "Recent efforts have evaluated large language models (LLMs) in areas such as commonsense reasoning, mathematical reasoning, and code generation. However, to the best of our knowledge, no work has specifically investigated the performance \u2026"}, {"title": "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "link": "https://arxiv.org/pdf/2405.16908", "details": "G Yona, R Aharoni, M Geva - arXiv preprint arXiv:2405.16908, 2024", "abstract": "We posit that large language models (LLMs) should be capable of expressing their intrinsic uncertainty in natural language. For example, if the LLM is equally likely to output two contradicting answers to the same question, then its generated response \u2026"}]
