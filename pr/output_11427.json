[{"title": "PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.09613%3F", "details": "C Yang, X Dong, X Zhu, W Su, J Wang, H Tian, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (VLMs) have been extended to understand both images and videos. Visual token compression is leveraged to reduce the considerable token length of visual inputs. To meet the needs of different tasks \u2026"}, {"title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding", "link": "https://arxiv.org/pdf/2412.16158%3F", "details": "C Tao, S Su, X Zhu, C Zhang, Z Chen, J Liu, W Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advance of Large Language Models (LLMs) has catalyzed the development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid modality-specific encoders, offer a promising alternative to the compositional ones \u2026"}, {"title": "DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision Language Models in Real-World Scenarios with Driving Theory Tests", "link": "https://arxiv.org/pdf/2501.04671", "details": "C Corbi\u00e8re, S Roburin, S Montariol, A Bosselut, A Alahi - arXiv preprint arXiv \u2026, 2025", "abstract": "Large vision-language models (LVLMs) augment language models with visual understanding, enabling multimodal reasoning. However, due to the modality gap between textual and visual data, they often face significant challenges, such as over \u2026"}, {"title": "A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges", "link": "https://arxiv.org/pdf/2412.11936", "details": "Y Yan, J Su, J He, F Fu, X Zheng, Y Lyu, K Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mathematical reasoning, a core aspect of human cognition, is vital across many domains, from educational problem-solving to scientific advancements. As artificial general intelligence (AGI) progresses, integrating large language models (LLMs) \u2026"}, {"title": "Vision Language Models as Values Detectors", "link": "https://arxiv.org/pdf/2501.03957", "details": "GA Abbo, T Belpaeme - arXiv preprint arXiv:2501.03957, 2025", "abstract": "Large Language Models integrating textual and visual inputs have introduced new possibilities for interpreting complex data. Despite their remarkable ability to generate coherent and contextually relevant text based on visual stimuli, the \u2026"}, {"title": "HandsOnVLM: Vision-Language Models for Hand-Object Interaction Prediction", "link": "https://arxiv.org/pdf/2412.13187", "details": "C Bao, J Xu, X Wang, A Gupta, H Bharadhwaj - arXiv preprint arXiv:2412.13187, 2024", "abstract": "How can we predict future interaction trajectories of human hands in a scene given high-level colloquial task specifications in the form of natural language? In this paper, we extend the classic hand trajectory prediction task to two tasks involving \u2026"}, {"title": "Foundation model of ECG diagnosis: Diagnostics and explanations of any form and rhythm on ECG", "link": "https://www.cell.com/cell-reports-medicine/fulltext/S2666-3791\\(24\\)00646-3", "details": "Y Tian, Z Li, Y Jin, M Wang, X Wei, L Zhao, Y Liu, J Liu\u2026 - Cell Reports Medicine, 2024", "abstract": "We propose a knowledge-enhanced electrocardiogram (ECG) diagnosis foundation model (KED) that utilizes large language models to incorporate domain-specific knowledge of ECG signals. This model is trained on 800,000 ECGs from nearly \u2026"}, {"title": "Do language models understand time?", "link": "https://arxiv.org/pdf/2412.13845", "details": "X Ding, L Wang - arXiv preprint arXiv:2412.13845, 2024", "abstract": "Large language models (LLMs) have revolutionized video-based computer vision applications, including action recognition, anomaly detection, and video summarization. Videos inherently pose unique challenges, combining spatial \u2026"}, {"title": "Language Models as Continuous Self-Evolving Data Engineers", "link": "https://arxiv.org/pdf/2412.15151%3F", "details": "P Wang, M Wang, Z Ma, X Yang, S Feng, D Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert \u2026"}]
