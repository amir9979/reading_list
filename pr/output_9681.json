[{"title": "Mixed Distillation Helps Smaller Language Models Reason Better", "link": "https://aclanthology.org/2024.findings-emnlp.91.pdf", "details": "L Chenglin, Q Chen, L Li, C Wang, F Tao, Y Li, Z Chen\u2026 - Findings of the Association \u2026, 2024", "abstract": "As large language models (LLMs) have demonstrated impressive multiple step-by- step reasoning capabilities in recent natural language processing (NLP) reasoning tasks, many studies are interested in distilling reasoning abilities into smaller \u2026"}, {"title": "Metaaligner: Towards generalizable multi-objective alignment of language models", "link": "https://openreview.net/pdf%3Fid%3DdIVb5C0QFf", "details": "K Yang, Z Liu, Q Xie, J Huang, T Zhang, S Ananiadou - The Thirty-eighth Annual \u2026, 2024", "abstract": "Recent advancements in large language models (LLMs) focus on aligning to heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are dependent on the policy model \u2026"}, {"title": "Reducing Distraction in Long-Context Language Models by Focused Learning", "link": "https://arxiv.org/pdf/2411.05928", "details": "Z Wu, B Liu, R Yan, L Chen, T Delteil - arXiv preprint arXiv:2411.05928, 2024", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced their capacity to process long contexts. However, effectively utilizing this long context remains a challenge due to the issue of distraction, where irrelevant \u2026"}, {"title": "S $^{2} $ FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity", "link": "https://openreview.net/pdf%3Fid%3DlEUle8S4xQ", "details": "X Yang, J Leng, G Guo, J Zhao, R Nakada, L Zhang\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in \u2026"}, {"title": "Counterfactual Explanations via Riemannian Latent Space Traversal", "link": "https://arxiv.org/pdf/2411.02259%3F", "details": "P Pegios, A Feragen, AA Hansen, G Arvanitidis - arXiv preprint arXiv:2411.02259, 2024", "abstract": "The adoption of increasingly complex deep models has fueled an urgent need for insight into how these models make predictions. Counterfactual explanations form a powerful tool for providing actionable explanations to practitioners. Previously \u2026"}, {"title": "CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models", "link": "https://arxiv.org/pdf/2411.04329", "details": "J Li, H Le, Y Zhou, C Xiong, S Savarese, D Sahoo - arXiv preprint arXiv:2411.04329, 2024", "abstract": "Pre-trained on massive amounts of code and text data, large language models (LLMs) have demonstrated remarkable achievements in performing code generation tasks. With additional execution-based feedback, these models can act as agents \u2026"}, {"title": "BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment", "link": "https://aclanthology.org/2024.emnlp-main.623.pdf", "details": "W Xu, J Li, WY Wang, L Li - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Direct alignment from preferences (DAP) has emerged as a promising paradigm for aligning large language models (LLMs) to human desiderata from pre-collected, offline preference datasets. While recent studies indicate that existing offline DAP \u2026"}, {"title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "link": "https://aclanthology.org/2024.emnlp-main.128.pdf", "details": "Y Wan, W Wang, Y Yang, Y Yuan, J Huang, P He\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT- 4\\. Despite LLMs' prowess in tasks like writing assistance, code generation, and \u2026"}, {"title": "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems", "link": "https://aclanthology.org/2024.emnlp-main.915.pdf", "details": "J Zhao, J Tong, Y Mou, M Zhang, Q Zhang, XJ Huang - Proceedings of the 2024 \u2026, 2024", "abstract": "Human cognition exhibits systematic compositionality, the algebraic ability to generate infinite novel combinations from finite learned components, which is the key to understanding and reasoning about complex logic. In this work, we investigate the \u2026"}]
