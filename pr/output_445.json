'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [HTML] [Adapting transformer-based language models for heart '
[{"title": "HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild", "link": "https://arxiv.org/pdf/2403.04307", "details": "Z Zhu, Z Sun, Y Yang - arXiv preprint arXiv:2403.04307, 2024", "abstract": "Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains. Recent benchmarks designed to assess LLM hallucinations within conventional NLP tasks, such as knowledge-intensive question \u2026"}, {"title": "Emergent Abilities in Reduced-Scale Generative Language Models", "link": "https://arxiv.org/html/2404.02204v1", "details": "S Muckatira, V Deshpande, V Lialin, A Rumshisky - arXiv preprint arXiv:2404.02204, 2024", "abstract": "Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study \u2026"}, {"title": "Identifying data-driven subtypes of major depressive disorder with electronic health records", "link": "https://www.sciencedirect.com/science/article/pii/S0165032724005858", "details": "A Sharma, PF Verhaak, TH McCoy, RH Perlis\u2026 - Journal of Affective \u2026, 2024", "abstract": "Background Efforts to reduce the heterogeneity of major depressive disorder (MDD) by identifying subtypes have not yet facilitated treatment personalization or investigation of biology, so novel approaches merit consideration. Methods We \u2026"}, {"title": "Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records", "link": "https://arxiv.org/pdf/2403.09226", "details": "A Ziletti, L D'Ambrosi - arXiv preprint arXiv:2403.09226, 2024", "abstract": "Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. Querying these databases to answer epidemiological questions is challenging due to the intricacy of medical \u2026"}, {"title": "Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models", "link": "https://arxiv.org/html/2404.02936v1", "details": "J Zhang, J Sun, E Yeats, Y Ouyang, M Kuo, J Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. The current state-of-the-art approach, Min-K \u2026"}, {"title": "Algorithmic progress in language models", "link": "https://arxiv.org/html/2403.05812v1", "details": "A Ho, T Besiroglu, E Erdil, D Owen, R Rahman, ZC Guo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We investigate the rate at which algorithms for pre-training language models have improved since the advent of deep learning. Using a dataset of over 200 language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we find that \u2026"}, {"title": "Understanding emergent abilities of language models from the loss perspective", "link": "https://arxiv.org/pdf/2403.15796", "details": "Z Du, A Zeng, Y Dong, J Tang - arXiv preprint arXiv:2403.15796, 2024", "abstract": "Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) \u2026"}, {"title": "Concept-aware Data Construction Improves In-context Learning of Language Models", "link": "https://arxiv.org/pdf/2403.09703", "details": "M \u0160tef\u00e1nik, M Kadl\u010d\u00edk, P Sojka - arXiv preprint arXiv:2403.09703, 2024", "abstract": "Many recent language models (LMs) are capable of in-context learning (ICL), manifested in the LMs' ability to perform a new task solely from natural-language instruction. Previous work curating in-context learners assumes that ICL emerges \u2026"}, {"title": "Sailor: Open Language Models for South-East Asia", "link": "https://arxiv.org/pdf/2404.03608", "details": "L Dou, Q Liu, G Zeng, J Guo, J Zhou, W Lu, M Lin - arXiv preprint arXiv:2404.03608, 2024", "abstract": "We present Sailor, a family of open language models ranging from 0.5 B to 7B parameters, tailored for South-East Asian (SEA) languages. These models are continually pre-trained from Qwen1. 5, a great language model for multilingual use \u2026"}]
