'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Autonomous Data Selection with Language Models for Mat'
[{"title": "Impact of Preference Noise on the Alignment Performance of Generative Language Models", "link": "https://arxiv.org/pdf/2404.09824", "details": "Y Gao, D Alon, D Metzler - arXiv preprint arXiv:2404.09824, 2024", "abstract": "A key requirement in developing Generative Language Models (GLMs) is to have their values aligned with human values. Preference-based alignment is a widely used paradigm for this purpose, in which preferences over generation pairs are first \u2026"}, {"title": "Context versus Prior Knowledge in Language Models", "link": "https://arxiv.org/pdf/2404.04633", "details": "K Du, V Sn\u00e6bjarnarson, N Stoehr, JC White, A Schein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "To answer a question, language models often need to integrate prior knowledge learned during pretraining and new information presented in context. We hypothesize that models perform this integration in a predictable way across different \u2026"}, {"title": "Can Language Models Solve Olympiad Programming?", "link": "https://arxiv.org/pdf/2404.10952", "details": "Q Shi, M Tang, K Narasimhan, S Yao - arXiv preprint arXiv:2404.10952, 2024", "abstract": "Computing olympiads contain some of the most challenging problems for humans, requiring complex algorithmic reasoning, puzzle solving, in addition to generating efficient code. However, it has been understudied as a domain to evaluate language \u2026"}, {"title": "IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe Biomedical Natural Language Inference for Clinical Trials", "link": "https://arxiv.org/pdf/2404.04510", "details": "S Mandal, A Modi - arXiv preprint arXiv:2404.04510, 2024", "abstract": "Large Language models (LLMs) have demonstrated state-of-the-art performance in various natural language processing (NLP) tasks across multiple domains, yet they are prone to shortcut learning and factual inconsistencies. This research investigates \u2026"}, {"title": "An Improved Machine Learning Model for Pulmonary Embolism Detection and Segmentation", "link": "https://www.preprints.org/manuscript/202404.1810/download/final_file", "details": "K Do\u011fan, T SEL\u00c7UK, A ALKAN - 2024", "abstract": "Pulmonary Embolism (PE) is the obstruction of blood arteries in the lungs by a blood clot. The mortality risk for PE is approximately 30%. Detecting pulmonary embolism in the segmental arteries of the lung is more challenging than in the main arteries \u2026"}, {"title": "Dual Modalities of Text: Visual and Textual Generative Pre-training", "link": "https://arxiv.org/pdf/2404.10710", "details": "Y Chai, Q Liu, J Xiao, S Wang, Y Sun, H Wu - arXiv preprint arXiv:2404.10710, 2024", "abstract": "Harnessing visual texts represents a burgeoning frontier in the evolution of language modeling. In this paper, we introduce a novel pre-training framework for a suite of pixel-based autoregressive language models, pre-training on a corpus of over 400 \u2026"}, {"title": "Quality of Answers of Generative Large Language Models Versus Peer Users for Interpreting Laboratory Test Results for Lay Patients: Evaluation Study", "link": "https://www.jmir.org/2024/1/e56655/", "details": "Z He, B Bhasuran, Q Jin, S Tian, K Hanna, C Shavor\u2026 - Journal of Medical Internet \u2026, 2024", "abstract": "Background Although patients have easy access to their electronic health records and laboratory test result data through patient portals, laboratory test results are often confusing and hard to understand. Many patients turn to web-based forums or \u2026"}, {"title": "More Room for Language: Investigating the Effect of Retrieval on Language Models", "link": "https://arxiv.org/pdf/2404.10939", "details": "D Samuel, LGG Charpentier, S Wold - arXiv preprint arXiv:2404.10939, 2024", "abstract": "Retrieval-augmented language models pose a promising alternative to standard language modeling. During pretraining, these models search in a corpus of documents for contextually relevant information that could aid the language \u2026"}, {"title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI", "link": "https://arxiv.org/pdf/2404.16006", "details": "K Ying, F Meng, J Wang, Z Li, H Lin, Y Yang, H Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal \u2026"}]
