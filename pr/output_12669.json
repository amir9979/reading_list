[{"title": "Comparing Few-Shot Prompting of GPT-4 LLMs with BERT Classifiers for Open-Response Assessment in Tutor Equity Training", "link": "https://arxiv.org/pdf/2501.06658", "details": "S Kakarla, C Borchers, D Thomas, S Bhushan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Assessing learners in ill-defined domains, such as scenario-based human tutoring training, is an area of limited research. Equity training requires a nuanced understanding of context, but do contemporary large language models (LLMs) have \u2026"}, {"title": "EHealth: A Chinese Biomedical Language Model Built via Multi-Level Text Discrimination", "link": "https://ieeexplore.ieee.org/abstract/document/10857372/", "details": "Q Wang, S Dai, B Xu, Y Lyu, H Wu, H Wang - IEEE Transactions on Audio, Speech \u2026, 2025", "abstract": "Pre-trained language models (PLMs) have recently revolutionized the field of natural language processing, impacting not only the general domain but also the biomedical domain. Most previous studies on constructing biomedical PLMs relied simply on \u2026"}, {"title": "DORA: Dynamic Optimization Prompt for Continuous Reflection of LLM-based Agent", "link": "https://aclanthology.org/2025.coling-main.504.pdf", "details": "K Li, T Zhao, W Zhou, S Hu - Proceedings of the 31st International Conference on \u2026, 2025", "abstract": "Autonomous agents powered by large language models (LLMs) hold significant potential across various domains. The Reflection framework is designed to help agents learn from past mistakes in complex tasks. While previous research has \u2026"}]
