[{"title": "Reasoning with Latent Thoughts: On the Power of Looped Transformers", "link": "https://arxiv.org/pdf/2502.17416", "details": "N Saunshi, N Dikkala, Z Li, S Kumar, SJ Reddi - arXiv preprint arXiv:2502.17416, 2025", "abstract": "Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim--many reasoning problems \u2026"}, {"title": "DeepThink: Aligning Language Models with Domain-Specific User Intents", "link": "https://arxiv.org/pdf/2502.05497", "details": "Y Li, M Luo, Y Gong, C Lin, J Jiao, Y Liu, K Huang - arXiv preprint arXiv:2502.05497, 2025", "abstract": "Supervised fine-tuning with synthesized instructions has been a common practice for adapting LLMs to domain-specific QA tasks. However, the synthesized instructions deviate from real user questions and expected answers. This study proposes a novel \u2026"}, {"title": "Hallucination Detection in Large Language Models with Metamorphic Relations", "link": "https://arxiv.org/pdf/2502.15844", "details": "B Yang, MAA Mamun, JM Zhang, G Uddin - arXiv preprint arXiv:2502.15844, 2025", "abstract": "Large Language Models (LLMs) are prone to hallucinations, eg, factually incorrect information, in their responses. These hallucinations present challenges for LLM- based applications that demand high factual accuracy. Existing hallucination \u2026"}, {"title": "The Law of Knowledge Overshadowing: Towards Understanding, Predicting, and Preventing LLM Hallucination", "link": "https://arxiv.org/pdf/2502.16143", "details": "Y Zhang, S Li, C Qian, J Liu, P Yu, C Han, YR Fung\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Hallucination is a persistent challenge in large language models (LLMs), where even with rigorous quality control, models often generate distorted facts. This paradox, in which error generation continues despite high-quality training data, calls for a deeper \u2026"}, {"title": "Verify when Uncertain: Beyond Self-Consistency in Black Box Hallucination Detection", "link": "https://arxiv.org/pdf/2502.15845", "details": "Y Xue, K Greenewald, Y Mroueh, B Mirzasoleiman - arXiv preprint arXiv:2502.15845, 2025", "abstract": "Large Language Models (LLMs) suffer from hallucination problems, which hinder their reliability in sensitive applications. In the black-box setting, several self- consistency-based techniques have been proposed for hallucination detection. We \u2026"}, {"title": "Self-Taught Agentic Long Context Understanding", "link": "https://arxiv.org/pdf/2502.15920", "details": "Y Zhuang, X Yu, J Wu, X Sun, Z Wang, J Liu, Y Su\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a \u2026"}, {"title": "CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought", "link": "https://arxiv.org/pdf/2502.17214", "details": "B Zhang, R Zhang - arXiv preprint arXiv:2502.17214, 2025", "abstract": "Large language models (LLMs) excel in many tasks but struggle to accurately quantify uncertainty in their generated responses. This limitation makes it challenging to detect misinformation and ensure reliable decision-making. Existing \u2026"}, {"title": "\" See the World, Discover Knowledge\": A Chinese Factuality Evaluation for Large Vision Language Models", "link": "https://arxiv.org/pdf/2502.11718", "details": "J Gu, Y Wang, P Bu, C Wang, Z Wang, T Song, D Wei\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The evaluation of factual accuracy in large vision language models (LVLMs) has lagged behind their rapid development, making it challenging to fully reflect these models' knowledge capacity and reliability. In this paper, we introduce the first \u2026"}, {"title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models", "link": "https://arxiv.org/pdf/2502.10250%3F", "details": "GK Kumar, I Chaabane, K Wu - arXiv preprint arXiv:2502.10250, 2025", "abstract": "Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved \u2026"}]
