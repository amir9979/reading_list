[{"title": "Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training", "link": "https://arxiv.org/pdf/2504.13123", "details": "X Zhang, Y Zeng, X Huang, H Hu, R Xie, H Hu, Z Kang - arXiv preprint arXiv \u2026, 2025", "abstract": "In recent years, the field of vision-language model pre-training has experienced rapid advancements, driven primarily by the continuous enhancement of textual capabilities in large language models. However, existing training paradigms for \u2026"}, {"title": "MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers", "link": "https://arxiv.org/pdf/2504.10551", "details": "L Zhao, Q Liu, W Chen, L Chen, R Sun, M Hou, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Empirical Risk Minimization (ERM) models often rely on spurious correlations between features and labels during the learning process, leading to shortcut learning behavior that undermines robustness generalization performance. Current research \u2026"}, {"title": "ChatEXAONEPath: An Expert-level Multimodal Large Language Model for Histopathology Using Whole Slide Images", "link": "https://arxiv.org/pdf/2504.13023", "details": "S Kim, S Lee, J Jang - arXiv preprint arXiv:2504.13023, 2025", "abstract": "Recent studies have made significant progress in developing large language models (LLMs) in the medical domain, which can answer expert-level questions and demonstrate the potential to assist clinicians in real-world clinical scenarios. Studies \u2026"}, {"title": "Counterfactual Fairness Evaluation of Machine Learning Models on Educational Datasets", "link": "https://arxiv.org/pdf/2504.11504", "details": "W Kim, H Kim - arXiv preprint arXiv:2504.11504, 2025", "abstract": "As machine learning models are increasingly used in educational settings, from detecting at-risk students to predicting student performance, algorithmic bias and its potential impacts on students raise critical concerns about algorithmic fairness \u2026"}, {"title": "Boundary-guided Contrastive Learning for Semi-supervised Medical Image Segmentation", "link": "https://ieeexplore.ieee.org/abstract/document/10946212/", "details": "Y Yang, J Zhuang, G Sun, R Wang, J Su - IEEE Transactions on Medical Imaging, 2025", "abstract": "Semi-supervised learning methods, compared to fully supervised learning, offer significant potential to alleviate the burden of manual annotations on clinicians. By leveraging unlabeled data, these methods can aid in the development of medical \u2026"}, {"title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training", "link": "https://arxiv.org/pdf/2504.13161", "details": "S Diao, Y Yang, Y Fu, X Dong, D Su, M Kliegl, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Pre-training datasets are typically collected from web content and lack inherent domain divisions. For instance, widely used datasets like Common Crawl do not include explicit domain labels, while manually curating labeled datasets such as The \u2026"}, {"title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models", "link": "https://arxiv.org/pdf/2504.00869%3F", "details": "X Huang, J Wu, H Liu, X Tang, Y Zhou - arXiv preprint arXiv:2504.00869, 2025", "abstract": "Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from \u2026"}, {"title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning", "link": "https://arxiv.org/pdf/2504.00406", "details": "J Han, W Buntine, E Shareghi - arXiv preprint arXiv:2504.00406, 2025", "abstract": "Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources \u2026"}, {"title": "Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation", "link": "https://arxiv.org/pdf/2504.12108", "details": "S Cai, L Ding, D Tao - arXiv preprint arXiv:2504.12108, 2025", "abstract": "The rapid development of Large Language Models (LLMs) has intensified concerns about content traceability and potential misuse. Existing watermarking schemes for sampled text often face trade-offs between maintaining text quality and ensuring \u2026"}]
