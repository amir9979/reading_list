[{"title": "Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning", "link": "https://arxiv.org/pdf/2502.19634", "details": "J Pan, C Liu, J Wu, F Liu, J Zhu, HB Li, C Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show \u2026"}, {"title": "Towards Statistical Factuality Guarantee for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2502.20560%3F", "details": "Z Li, C Yan, NJ Jackson, W Cui, B Li, J Zhang, BA Malin - arXiv preprint arXiv \u2026, 2025", "abstract": "Advancements in Large Vision-Language Models (LVLMs) have demonstrated promising performance in a variety of vision-language tasks involving image- conditioned free-form text generation. However, growing concerns about \u2026"}, {"title": "O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models", "link": "https://arxiv.org/pdf/2503.12096", "details": "A Sharifdeen, MA Munir, S Baliah, S Khan, MH Khan - arXiv preprint arXiv \u2026, 2025", "abstract": "Test-time prompt tuning for vision-language models (VLMs) is getting attention because of their ability to learn with unlabeled data without fine-tuning. Although test- time prompt tuning methods for VLMs can boost accuracy, the resulting models tend \u2026"}, {"title": "Enhancing Multi-hop Reasoning in Vision-Language Models via Self-Distillation with Multi-Prompt Ensembling", "link": "https://arxiv.org/pdf/2503.01754%3F", "details": "G Wu, H Song, Y Wang, Q Yan, Y Tian, LL Cheong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multi-modal large language models have seen rapid advancement alongside large language models. However, while language models can effectively leverage chain- of-thought prompting for zero or few-shot learning, similar prompting strategies are \u2026"}, {"title": "Abn-BLIP: Abnormality-aligned Bootstrapping Language-Image Pre-training for Pulmonary Embolism Diagnosis and Report Generation from CTPA", "link": "https://arxiv.org/pdf/2503.02034", "details": "Z Zhong, Y Wang, L Bi, Z Ma, SH Ahn, CJ Mullin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical imaging plays a pivotal role in modern healthcare, with computed tomography pulmonary angiography (CTPA) being a critical tool for diagnosing pulmonary embolism and other thoracic conditions. However, the complexity of \u2026"}, {"title": "MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data with Vision Generation Task using Discrete Visual Representations", "link": "https://arxiv.org/pdf/2503.01019", "details": "Z Zhang, Y Yu, Y Chen, X Yang, SY Yeo - arXiv preprint arXiv:2503.01019, 2025", "abstract": "Despite significant progress in Vision-Language Pre-training (VLP), current approaches predominantly emphasize feature extraction and cross-modal comprehension, with limited attention to generating or transforming visual content \u2026"}, {"title": "Paving the way for scientific foundation models: enhancing generalization and robustness in PDEs with constraint-aware pre-training", "link": "https://arxiv.org/pdf/2503.19081", "details": "A Totounferoush, S Kotchourko, MW Mahoney, S Staab - arXiv preprint arXiv \u2026, 2025", "abstract": "Partial differential equations (PDEs) govern a wide range of physical systems, but solving them efficiently remains a major challenge. The idea of a scientific foundation model (SciFM) is emerging as a promising tool for learning transferable \u2026"}, {"title": "FullDiT: Multi-Task Video Generative Foundation Model with Full Attention", "link": "https://arxiv.org/pdf/2503.19907", "details": "X Ju, W Ye, Q Liu, Q Wang, X Wang, P Wan, D Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Current video generative foundation models primarily focus on text-to-video tasks, providing limited control for fine-grained video content creation. Although adapter- based approaches (eg, ControlNet) enable additional controls with minimal fine \u2026"}, {"title": "CoCa-CXR: Contrastive Captioners Learn Strong Temporal Structures for Chest X-Ray Vision-Language Understanding", "link": "https://arxiv.org/pdf/2502.20509", "details": "Y Chen, S Xu, A Sellergren, Y Matias, A Hassidim\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models have proven to be of great benefit for medical image analysis since they learn rich semantics from both images and reports. Prior efforts have focused on better alignment of image and text representations to enhance \u2026"}]
