[{"title": "DiffuseVAE++: Mitigating training-sampling mismatch based on additional noise for higher fidelity image generation", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225004862", "details": "X Yang, W Luo, H Ning, G Zhang, W Sun, S Ma - Neurocomputing, 2025", "abstract": "Abstract Denoising Diffusion Probabilistic Models (DDPMs) have demonstrated remarkable results in image generation. However, there exist a mismatch between the training and sampling process in current diffusion models, in addition, the U-Net \u2026"}, {"title": "Controlled Model Debiasing through Minimal and Interpretable Updates", "link": "https://arxiv.org/pdf/2502.21284", "details": "F Di Gennaro, T Laugel, V Grari, M Detyniecki - arXiv preprint arXiv:2502.21284, 2025", "abstract": "Traditional approaches to learning fair machine learning models often require rebuilding models from scratch, generally without accounting for potentially existing previous models. In a context where models need to be retrained frequently, this can \u2026"}, {"title": "How far can we go with ImageNet for Text-to-Image generation?", "link": "https://arxiv.org/pdf/2502.21318", "details": "L Degeorge, A Ghosh, N Dufour, D Picard\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent text-to-image (T2I) generation models have achieved remarkable results by training on billion-scale datasets, following abigger is better'paradigm that prioritizes data quantity over quality. We challenge this established paradigm by demonstrating \u2026"}, {"title": "A Synergy Scoring Filter for Unsupervised Anomaly Detection with Noisy Data", "link": "https://arxiv.org/abs/2502.13992", "details": "F Wang, C Liu, P Haibo, L Shi - arXiv preprint arXiv:2502.13992, 2025", "abstract": "Noise-inclusive fully unsupervised anomaly detection (FUAD) holds significant practical relevance. Although various methods exist to address this problem, they are limited in both performance and scalability. Our work seeks to overcome these \u2026"}]
