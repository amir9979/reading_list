[{"title": "H2R Bridge: Transferring vision-language models to few-shot intention meta-perception in human robot collaboration", "link": "https://www.sciencedirect.com/science/article/abs/pii/S0278612525000779", "details": "D Wu, Q Zhao, J Fan, J Qi, P Zheng, J Hu - Journal of Manufacturing Systems, 2025", "abstract": "Human\u2013robot collaboration enhances efficiency by enabling robots to work alongside human operators in shared tasks. Accurately understanding human intentions is critical for achieving a high level of collaboration. Existing methods \u2026"}, {"title": "Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance", "link": "https://arxiv.org/pdf/2503.13445", "details": "NY Siegel, N Heess, M Perez-Ortiz, OM Camburu - arXiv preprint arXiv:2503.13445, 2025", "abstract": "As large language models (LLMs) become increasingly capable, ensuring that their self-generated explanations are faithful to their internal decision-making process is critical for safety and oversight. In this work, we conduct a comprehensive \u2026"}, {"title": "Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist with Interpretable Reasoning", "link": "https://arxiv.org/pdf/2504.01911", "details": "Y Xu, H Kimlee, Y Xiao, D Luo - arXiv preprint arXiv:2504.01911, 2025", "abstract": "Large Language Models (LLMs) are playing an expanding role in physics research by enhancing reasoning, symbolic manipulation, and numerical computation. However, ensuring the reliability and interpretability of their outputs remains a \u2026"}, {"title": "2-D Transformer: Extending Large Language Models to Long-Context With Few Memory", "link": "https://ieeexplore.ieee.org/abstract/document/10937248/", "details": "X He, J Liu, Y Duan - IEEE Transactions on Neural Networks and Learning \u2026, 2025", "abstract": "The ability of processing long contexts is crucial for large language models (LLMs), but training LLMs with a long-context window requires substantial computational resources. Many sought to mitigate this through the sparse attention mechanism \u2026"}, {"title": "Step-by-Step Correction of LLM-based Math Word Problems Solutions", "link": "https://ieeexplore.ieee.org/abstract/document/10889273/", "details": "Y Li, DM Ubaidali, L Wang, W Zhang - \u2026 2025-2025 IEEE International Conference on \u2026, 2025", "abstract": "Following the success of Large Language Models (LLMs) in language tasks, LLMs have been adapted for reasoning in math word problems (MWPs). MWP is a complex task that requires both semantic understanding of text and mathematical reasoning \u2026"}, {"title": "TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining", "link": "https://arxiv.org/pdf/2504.02107", "details": "J Li, M Armandpour, I Mirzadeh, S Mehta, V Shankar\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) trained on historical web data inevitably become outdated. We investigate evaluation strategies and update methods for LLMs as new data becomes available. We introduce a web-scale dataset for time-continual \u2026"}]
