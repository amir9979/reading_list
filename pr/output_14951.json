[{"title": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning", "link": "https://arxiv.org/pdf/2504.03635", "details": "X Wang, S Tan, M Jin, WY Wang, R Panda, Y Shen - arXiv preprint arXiv:2504.03635, 2025", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks requiring complex reasoning. However, the effects of scaling on their reasoning abilities remain insufficiently understood. In this paper, we \u2026"}, {"title": "PARIC: Probabilistic Attention Regularization for Language Guided Image Classification from Pre-trained Vison Language Models", "link": "https://arxiv.org/pdf/2503.11360%3F", "details": "M Nautiyal, SA Gheorghe, K Stefa, L Ju, IM Sintorn\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Language-guided attention frameworks have significantly enhanced both interpretability and performance in image classification; however, the reliance on deterministic embeddings from pre-trained vision-language foundation models to \u2026"}, {"title": "Pre-trained Language Models and Few-shot Learning for Medical Entity Extraction", "link": "https://arxiv.org/pdf/2504.04385", "details": "X Wang, G Liu, B Zhu, J He, H Zheng, H Zhang - arXiv preprint arXiv:2504.04385, 2025", "abstract": "This study proposes a medical entity extraction method based on Transformer to enhance the information extraction capability of medical literature. Considering the professionalism and complexity of medical texts, we compare the performance of \u2026"}, {"title": "Clinical ModernBERT: An efficient and long context encoder for biomedical text", "link": "https://arxiv.org/pdf/2504.03964", "details": "SA Lee, A Wu, JN Chiang - arXiv preprint arXiv:2504.03964, 2025", "abstract": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on large scale biomedical literature, clinical notes, and medical ontologies, incorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with their \u2026"}, {"title": "Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages", "link": "https://arxiv.org/pdf/2503.11609", "details": "M Farina, M Mancini, G Iacca, E Ricci - arXiv preprint arXiv:2503.11609, 2025", "abstract": "An old-school recipe for training a classifier is to (i) learn a good feature extractor and (ii) optimize a linear layer atop. When only a handful of samples are available per category, as in Few-Shot Adaptation (FSA), data are insufficient to fit a large number \u2026"}, {"title": "CLEAR: Addressing Representation Contamination in Multimodal Healthcare Analytics", "link": "https://dl.acm.org/doi/abs/10.1145/3690624.3709164", "details": "G Su, K Zheng, T Zhao, J Yin - Proceedings of the 31st ACM SIGKDD Conference on \u2026, 2025", "abstract": "Electronic health records (EHRs) are the de facto standard for analyzing comprehensive patient conditions. Existing methods mainly employ specialized neural networks to extract modality-specific information, followed by modality \u2026"}, {"title": "Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregressive Representation Alignment", "link": "https://arxiv.org/pdf/2503.07334%3F", "details": "X Xie, J Liu, Z Lin, H Fan, Z Han, Y Tang, L Qu - arXiv preprint arXiv:2503.07334, 2025", "abstract": "We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural changes. Unlike prior work that requires complex \u2026"}, {"title": "LVMed-R2: Perception and Reflection-driven Complex Reasoning for Medical Report Generation", "link": "https://arxiv.org/pdf/2504.02885", "details": "H Wang, S Ye, J Lin, U Naseem, J Kim - arXiv preprint arXiv:2504.02885, 2025", "abstract": "Large vision-language models (LVMs) hold a great promise for automating medical report generation, potentially reducing the burden of manual reporting. State-of-the- art (SOTA) research fine-tunes general LVMs with medical data to align radiology \u2026"}]
