[{"title": "Bootstrapping Vision-language Models for Self-supervised Remote Physiological Measurement", "link": "https://arxiv.org/pdf/2407.08507", "details": "Z Yue, M Shi, H Wang, S Ding, Q Chen, S Yang - arXiv preprint arXiv:2407.08507, 2024", "abstract": "Facial video-based remote physiological measurement is a promising research area for detecting human vital signs (eg, heart rate, respiration frequency) in a non-contact way. Conventional approaches are mostly supervised learning, requiring extensive \u2026"}, {"title": "Enhancing Robustness of Vision-Language Models through Orthogonality Learning and Cross-Regularization", "link": "https://arxiv.org/pdf/2407.08374", "details": "J Li, Z Jie, E Ricci, L Ma, N Sebe - arXiv preprint arXiv:2407.08374, 2024", "abstract": "Efficient finetuning of vision-language models (VLMs) like CLIP for specific downstream tasks is gaining significant attention. Previous works primarily focus on prompt learning to adapt the CLIP into a variety of downstream tasks, however \u2026"}, {"title": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation", "link": "https://arxiv.org/pdf/2407.08441", "details": "R Cantini, G Cosenza, A Orsino, D Talia - arXiv preprint arXiv:2407.08441, 2024", "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence, demonstrating remarkable computational power and linguistic capabilities. However, these models are inherently prone to various biases stemming from their training \u2026"}, {"title": "Specialist vision-language models for clinical ophthalmology", "link": "https://arxiv.org/abs/2407.08410", "details": "R Holland, TRP Taylor, C Holmes, S Riedl, J Mai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Clinicians spend a significant amount of time reviewing medical images and transcribing their findings regarding patient diagnosis, referral and treatment in text form. Vision-language models (VLMs), which automatically interpret images and \u2026"}, {"title": "SEED-Story: Multimodal Long Story Generation with Large Language Model", "link": "https://arxiv.org/pdf/2407.08683", "details": "S Yang, Y Ge, Y Li, Y Chen, Y Ge, Y Shan, Y Chen - arXiv preprint arXiv:2407.08683, 2024", "abstract": "With the remarkable advancements in image generation and open-form text generation, the creation of interleaved image-text content has become an increasingly intriguing field. Multimodal story generation, characterized by producing \u2026"}, {"title": "From Distributional to Overton Pluralism: Investigating Large Language Model Alignment", "link": "https://arxiv.org/pdf/2406.17692", "details": "T Lake, E Choi, G Durrett - arXiv preprint arXiv:2406.17692, 2024", "abstract": "The alignment process changes several properties of a large language model's (LLM's) output distribution. We analyze two aspects of post-alignment distributional shift of LLM responses. First, we re-examine previously reported reductions in \u2026"}, {"title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors", "link": "https://arxiv.org/pdf/2406.14598", "details": "T Xie, X Qi, Y Zeng, Y Huang, UM Sehwag, K Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Evaluating aligned large language models'(LLMs) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing evaluation efforts, however, face three limitations that we address with SORRY \u2026"}, {"title": "MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis", "link": "https://arxiv.org/pdf/2407.04106", "details": "A Alkhaldi, R Alnajim, L Alabdullatef, R Alyahya\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recent advancements in artificial intelligence (AI) have precipitated significant breakthroughs in healthcare, particularly in refining diagnostic procedures. However, previous studies have often been constrained to limited functionalities. This study \u2026"}, {"title": "Towards a Holistic Framework for Multimodal Large Language Models in Three-dimensional Brain CT Report Generation", "link": "https://arxiv.org/pdf/2407.02235", "details": "CY Li, KJ Chang, CF Yang, HY Wu, W Chen, H Bansal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Multi-modal large language models (MLLMs) have been given free rein to explore exciting medical applications with a primary focus on radiology report generation. Nevertheless, the preliminary success in 2D radiology captioning is incompetent to \u2026"}]
