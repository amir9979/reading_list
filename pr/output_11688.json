[{"title": "Fake News Detection and Fact Checking in X posts from Ecuador Chequea and Ecuador Verifica using Spanish Language Models", "link": "https://rte.espol.edu.ec/index.php/tecnologica/article/download/1219/817", "details": "MT Bernabe, MA Garcia-Cumbreras, LA Urena-Lopez - Revista Tecnol\u00f3gica-ESPOL, 2024", "abstract": "Currently, verifying news content before its dissemination poses a significant challenge due to the rapidity with which it spreads and the ease of replication. These factors contribute to the proliferation of fake news. Collaborative initiatives like Duke \u2026"}, {"title": "Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models", "link": "https://yikunhan.me/pdfs/AAAI_2025_UQ_LLM.pdf", "details": "K Cox, J Xu, Y Han, R Xu, T Li, CY Hsu, T Chen\u2026 - 2025", "abstract": "An interesting behavior in large language models (LLMs) is prompt sensitivity. When provided with different but semantically equivalent versions of the same prompt, models may produce very different distributions of answers. This suggests that the \u2026"}, {"title": "Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models", "link": "https://arxiv.org/pdf/2501.08248", "details": "Y Qiu, V Embar, Y Zhang, N Jaitly, SB Cohen, B Han - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in long-context language models (LCLMs) promise to transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With their expanded context windows, LCLMs can process entire knowledge bases and \u2026"}, {"title": "LLMic: Romanian Foundation Language Model", "link": "https://arxiv.org/pdf/2501.07721", "details": "VA B\u0103doiu, MV Dumitru, AM Gherghescu, A Agache\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks with commercial models leading the way. While open models usually operate at a smaller scale, they maintain competitiveness \u2026"}, {"title": "MiniMax-01: Scaling Foundation Models with Lightning Attention", "link": "https://arxiv.org/pdf/2501.08313", "details": "A Li, B Gong, B Yang, B Shan, C Liu, C Zhu, C Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient \u2026"}]
