[{"title": "Evaluation and mitigation of cognitive biases in medical language models", "link": "https://www.nature.com/articles/s41746-024-01283-6", "details": "S Schmidgall, C Harris, I Essien, D Olshvang\u2026 - npj Digital Medicine, 2024", "abstract": "Increasing interest in applying large language models (LLMs) to medicine is due in part to their impressive performance on medical exam questions. However, these exams do not capture the complexity of real patient\u2013doctor interactions because of \u2026"}, {"title": "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging", "link": "https://arxiv.org/pdf/2410.15661", "details": "C Na, I Magnusson, AH Jha, T Sherborne, E Strubell\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Training data compositions for Large Language Models (LLMs) can significantly affect their downstream performance. However, a thorough data ablation study exploring large sets of candidate data mixtures is typically prohibitively expensive \u2026"}, {"title": "Metaaligner: Towards generalizable multi-objective alignment of language models", "link": "https://openreview.net/pdf%3Fid%3DdIVb5C0QFf", "details": "K Yang, Z Liu, Q Xie, J Huang, T Zhang, S Ananiadou - The Thirty-eighth Annual \u2026, 2024", "abstract": "Recent advancements in large language models (LLMs) focus on aligning to heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are dependent on the policy model \u2026"}, {"title": "Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting", "link": "https://aclanthology.org/2024.findings-emnlp.400.pdf", "details": "M Naguib, X Tannier, A Neveol - Findings of the Association for Computational \u2026, 2024", "abstract": "Large language models (LLMs) have become the preferred solution for many natural language processing tasks. In low-resource environments such as specialized domains, their few-shot capabilities are expected to deliver high performance \u2026"}, {"title": "Foundation models in healthcare require rethinking reliability", "link": "https://www.nature.com/articles/s42256-024-00924-5", "details": "T Grote, T Freiesleben, P Berens - Nature Machine Intelligence, 2024", "abstract": "A new class of AI models, called foundation models, has entered healthcare. Foundation models violate several basic principles of the standard machine learning paradigm for assessing reliability, making it necessary to rethink what guarantees \u2026"}, {"title": "Large language models enabled multiagent ensemble method for efficient EHR data labeling", "link": "https://arxiv.org/pdf/2410.16543", "details": "J Huang, K Nezafati, I Villanueva-Miranda, Z Gu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study introduces a novel multiagent ensemble method powered by LLMs to address a key challenge in ML-data labeling, particularly in large-scale EHR datasets. Manual labeling of such datasets requires domain expertise and is labor \u2026"}, {"title": "Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language Models", "link": "https://openreview.net/pdf%3Fid%3Dnvn80cscVm", "details": "L Wei, Z Tan, C Li, J Wang, W Huang - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Large Language Models (LLMs) have transformed natural language processing and extended their powerful capabilities to multi-modal domains. As LLMs continue to advance, it is crucial to develop diverse and appropriate metrics for their evaluation \u2026"}, {"title": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning", "link": "https://arxiv.org/pdf/2411.05193", "details": "J Hong, A Dragan, S Levine - arXiv preprint arXiv:2411.05193, 2024", "abstract": "Value-based reinforcement learning (RL) can in principle learn effective policies for a wide range of multi-turn problems, from games to dialogue to robotic control, including via offline RL from static previously collected datasets. However, despite \u2026"}, {"title": "Mmie: Massive multimodal interleaved comprehension benchmark for large vision-language models", "link": "https://arxiv.org/pdf/2410.10139", "details": "P Xia, S Han, S Qiu, Y Zhou, Z Wang, W Zheng, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this \u2026"}]
