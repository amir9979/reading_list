[{"title": "On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning", "link": "https://arxiv.org/pdf/2406.14197", "details": "F Nowak, A Svete, A Butoi, R Cotterell - arXiv preprint arXiv:2406.14197, 2024", "abstract": "The performance of modern language models (LMs) has been improved by chain-of- thought (CoT) reasoning, ie, the process of generating intermediate results that guide the model towards a final answer. A possible explanation for this improvement is that \u2026"}, {"title": "Word Embeddings Are Steers for Language Models", "link": "https://blender.cs.illinois.edu/paper/lmsteer2024.pdf", "details": "C Han, J Xu, M Li, Y Fung, C Sun, N Jiang\u2026", "abstract": "Abstract Language models (LMs) automatically learn word embeddings during pre- training on language corpora. Although word embeddings are usually interpreted as feature vectors for individual words, their roles in language model generation remain \u2026"}, {"title": "$\\texttt {MoE-RBench} $: Towards Building Reliable Language Models with Sparse Mixture-of-Experts", "link": "https://arxiv.org/pdf/2406.11353", "details": "G Chen, X Zhao, T Chen, Y Cheng - arXiv preprint arXiv:2406.11353, 2024", "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new \u2026"}, {"title": "ViGLUE: A Vietnamese General Language Understanding Benchmark and Analysis of Vietnamese Language Models", "link": "https://aclanthology.org/2024.findings-naacl.261.pdf", "details": "MN Tran, PV Nguyen, L Nguyen, D Dien - Findings of the Association for \u2026, 2024", "abstract": "As the number of language models has increased, various benchmarks have been suggested to assess the proficiency of the models in natural language understanding. However, there is a lack of such a benchmark in Vietnamese due to \u2026"}, {"title": "Language Models can be Deductive Solvers", "link": "https://aclanthology.org/2024.findings-naacl.254.pdf", "details": "J Feng, R Xu, J Hao, H Sharma, Y Shen, D Zhao\u2026 - Findings of the Association \u2026, 2024", "abstract": "Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit \u2026"}, {"title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space", "link": "https://arxiv.org/pdf/2406.09325", "details": "T Ashuach, M Tutek, Y Belinkov - arXiv preprint arXiv:2406.09325, 2024", "abstract": "Large language models (LLMs) risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in training data, causing privacy concerns. Current approaches to address this issue involve costly dataset \u2026"}, {"title": "Monitoring Latent World States in Language Models with Propositional Probes", "link": "https://arxiv.org/pdf/2406.19501", "details": "J Feng, S Russell, J Steinhardt - arXiv preprint arXiv:2406.19501, 2024", "abstract": "Language models are susceptible to bias, sycophancy, backdoors, and other tendencies that lead to unfaithful responses to the input context. Interpreting internal states of language models could help monitor and correct unfaithful behavior. We \u2026"}, {"title": "Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain", "link": "https://arxiv.org/pdf/2406.06435", "details": "B Hu, B Ray, A Leung, A Summerville, D Joy, C Funk\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In difficult decision-making scenarios, it is common to have conflicting opinions among expert human decision-makers as there may not be a single right answer. Such decisions may be guided by different attributes that can be used to characterize \u2026"}, {"title": "Advancing High Resolution Vision-Language Models in Biomedicine", "link": "https://arxiv.org/pdf/2406.09454", "details": "Z Chen, A Pekis, K Brown - arXiv preprint arXiv:2406.09454, 2024", "abstract": "Multi-modal learning has significantly advanced generative AI, especially in vision- language modeling. Innovations like GPT-4V and open-source projects such as LLaVA have enabled robust conversational agents capable of zero-shot task \u2026"}]
