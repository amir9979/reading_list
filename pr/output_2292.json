[{"title": "WISER: Weak supervISion and supErvised Representation learning to improve drug response prediction in cancer", "link": "https://arxiv.org/pdf/2405.04078", "details": "K Shubham, A Jayagopal, SM Danish, P AP, V Rajan - arXiv preprint arXiv \u2026, 2024", "abstract": "Cancer, a leading cause of death globally, occurs due to genomic changes and manifests heterogeneously across patients. To advance research on personalized treatment strategies, the effectiveness of various drugs on cells derived from cancers \u2026"}, {"title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning", "link": "https://arxiv.org/pdf/2405.10292", "details": "Y Zhai, H Bai, Z Lin, J Pan, S Tong, Y Zhou, A Suhr\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision-language models (VLMs) fine-tuned on specialized visual instruction- following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently \u2026"}, {"title": "ECR-Chain: Advancing Generative Language Models to Better Emotion-Cause Reasoners through Reasoning Chains", "link": "https://arxiv.org/pdf/2405.10860", "details": "Z Huang, J Zhao, Q Jin - arXiv preprint arXiv:2405.10860, 2024", "abstract": "Understanding the process of emotion generation is crucial for analyzing the causes behind emotions. Causal Emotion Entailment (CEE), an emotion-understanding task, aims to identify the causal utterances in a conversation that stimulate the emotions \u2026"}, {"title": "LG AI Research & KAIST at EHRSQL 2024: Self-Training Large Language Models with Pseudo-Labeled Unanswerable Questions for a Reliable Text-to-SQL System \u2026", "link": "https://arxiv.org/pdf/2405.11162", "details": "Y Jo, S Lee, M Seo, SJ Hwang, M Lee - arXiv preprint arXiv:2405.11162, 2024", "abstract": "Text-to-SQL models are pivotal for making Electronic Health Records (EHRs) accessible to healthcare professionals without SQL knowledge. With the advancements in large language models, these systems have become more adept at \u2026"}, {"title": "Annot-Mix: Learning with Noisy Class Labels from Multiple Annotators via a Mixup Extension", "link": "https://arxiv.org/pdf/2405.03386", "details": "M Herde, L L\u00fchrs, D Huseljic, B Sick - arXiv preprint arXiv:2405.03386, 2024", "abstract": "Training with noisy class labels impairs neural networks' generalization performance. In this context, mixup is a popular regularization technique to improve training robustness by making memorizing false class labels more difficult. However, mixup \u2026"}, {"title": "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models", "link": "https://arxiv.org/pdf/2405.06134", "details": "V Raina, R Ma, C McGhee, K Knill, M Gales - arXiv preprint arXiv:2405.06134, 2024", "abstract": "Recent developments in large speech foundation models like Whisper have led to their widespread use in many automatic speech recognition (ASR) applications. These systems incorporatespecial tokens' in their vocabulary, such as $\\texttt {< \u2026"}, {"title": "Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks", "link": "https://arxiv.org/pdf/2405.10548", "details": "A Chatterjee, E Tanwar, S Dutta, T Chakraborty - arXiv preprint arXiv:2405.10548, 2024", "abstract": "Large Language Models (LLMs) have transformed NLP with their remarkable In- context Learning (ICL) capabilities. Automated assistants based on LLMs are gaining popularity; however, adapting them to novel tasks is still challenging. While colossal \u2026"}, {"title": "Addax: Memory-Efficient Fine-Tuning of Language Models with a Combination of Forward-Backward and Forward-Only Passes", "link": "https://openreview.net/pdf%3Fid%3DYtZv36CY5p", "details": "Z Li, X Zhang, M Razaviyayn - 5th Workshop on practical ML for limited/low resource \u2026", "abstract": "Fine-tuning language models (LMs) with first-order optimizers often demands excessive memory, limiting accessibility, while zeroth-order optimizers use less memory, but suffer from slow convergence depending on model size. We introduce a \u2026"}, {"title": "Filling the gaps: leveraging large language models for temporal harmonization of clinical text across multiple medical visits for clinical prediction", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/05/07/2024.05.06.24306959.full.pdf", "details": "I Choi, Q Long, E Getzen - medRxiv, 2024", "abstract": "Electronic health records offer great promise for early disease detection, treatment evaluation, information discovery, and other important facets of precision health. Clinical notes, in particular, may contain nuanced information about a patient's \u2026"}]
