[{"title": "Robot Navigation Using Physically Grounded Vision-Language Models in Outdoor Environments", "link": "https://arxiv.org/pdf/2409.20445", "details": "M Elnoor, K Weerakoon, G Seneviratne, R Xian\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present a novel autonomous robot navigation algorithm for outdoor environments that is capable of handling diverse terrain traversability conditions. Our approach, VLM-GroNav, uses vision-language models (VLMs) and integrates them with \u2026"}, {"title": "Expert-level vision-language foundation model for real-world radiology and comprehensive evaluation", "link": "https://arxiv.org/pdf/2409.16183", "details": "X Liu, G Yang, Y Luo, J Mao, X Zhang, M Gao, S Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Radiology is a vital and complex component of modern clinical workflow and covers many tasks. Recently, vision-language (VL) foundation models in medicine have shown potential in processing multimodal information, offering a unified solution for \u2026"}, {"title": "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness", "link": "https://arxiv.org/pdf/2409.17791", "details": "J Li, H Huang, Y Zhang, P Xu, X Chen, R Song, L Shi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, there has been significant interest in replacing the reward model in Reinforcement Learning with Human Feedback (RLHF) methods for Large Language Models (LLMs), such as Direct Preference Optimization (DPO) and its \u2026"}, {"title": "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?", "link": "https://arxiv.org/pdf/2410.10476", "details": "G Roccabruna, M Rizzoli, G Riccardi - arXiv preprint arXiv:2410.10476, 2024", "abstract": "The automatic detection of temporal relations among events has been mainly investigated with encoder-only models such as RoBERTa. Large Language Models (LLM) have recently shown promising performance in temporal reasoning tasks such \u2026"}, {"title": "Simulating Dynamic Tumor Contrast Enhancement in Breast MRI using Conditional Generative Adversarial Networks", "link": "https://arxiv.org/pdf/2409.18872", "details": "R Osuala, S Joshi, A Tsirikoglou, L Garrucho\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper presents a method for virtual contrast enhancement in breast MRI, offering a promising non-invasive alternative to traditional contrast agent-based DCE- MRI acquisition. Using a conditional generative adversarial network, we predict DCE \u2026"}, {"title": "SciDFM: A Large Language Model with Mixture-of-Experts for Science", "link": "https://arxiv.org/pdf/2409.18412", "details": "L Sun, D Luo, D Ma, Z Zhao, B Chen, Z Shen, S Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Recently, there has been a significant upsurge of interest in leveraging large language models (LLMs) to assist scientific discovery. However, most LLMs only focus on general science, while they lack domain-specific knowledge, such as \u2026"}, {"title": "TEOChat: A Large Vision-Language Assistant for Temporal Earth Observation Data", "link": "https://arxiv.org/pdf/2410.06234", "details": "JA Irvin, ER Liu, JC Chen, I Dormoy, J Kim, S Khanna\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision and language assistants have enabled new capabilities for interpreting natural images. These approaches have recently been adapted to earth observation data, but they are only able to handle single image inputs, limiting their use for many \u2026"}, {"title": "RING-NeRF: Rethinking Inductive Biases for Versatile and Efficient Neural Fields", "link": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05141.pdf", "details": "D Petit, S Bourgeois, D Pavel, V Gay-Bellile, F Chabot\u2026 - European Conference on \u2026, 2025", "abstract": "Abstract Recent advances in Neural Fields mostly rely on developing task-specific supervision which often complicates the models. Rather than developing hard-to- combine and specific modules, another approach generally overlooked is to directly \u2026"}, {"title": "TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans", "link": "https://arxiv.org/pdf/2409.16666", "details": "A Chatziagapi, B Chaudhuri, A Kumar, R Ranjan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce a novel framework that learns a dynamic neural radiance field (NeRF) for full-body talking humans from monocular videos. Prior work represents only the body pose or the face. However, humans communicate with their full body \u2026"}]
