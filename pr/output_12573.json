[{"title": "Minerva: A Programmable Memory Test Benchmark for Language Models", "link": "https://arxiv.org/pdf/2502.03358", "details": "M Xia, V Ruehle, S Rajmohan, R Shokri - arXiv preprint arXiv:2502.03358, 2025", "abstract": "How effectively can LLM-based AI assistants utilize their memory (context) to perform various tasks? Traditional data benchmarks, which are often manually crafted, suffer from several limitations: they are static, susceptible to overfitting, difficult to interpret \u2026"}, {"title": "Vulnerability Mitigation for Safety-Aligned Language Models via Debiasing", "link": "https://arxiv.org/pdf/2502.02153", "details": "TQ Tran, A Wachi, R Sato, T Tanabe, Y Akimoto - arXiv preprint arXiv:2502.02153, 2025", "abstract": "Safety alignment is an essential research topic for real-world AI applications. Despite the multifaceted nature of safety and trustworthiness in AI, current safety alignment methods often focus on a comprehensive notion of safety. By carefully assessing \u2026"}, {"title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning", "link": "https://arxiv.org/pdf/2502.03304", "details": "Q Tan, J Liu, Z Zhan, C Ding, Y Wang, J Lu, G Yuan - arXiv preprint arXiv:2502.03304, 2025", "abstract": "Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising \u2026"}, {"title": "Personalization Toolkit: Training Free Personalization of Large Vision Language Models", "link": "https://arxiv.org/pdf/2502.02452", "details": "S Seifi, V Dorovatas, DO Reino, R Aljundi - arXiv preprint arXiv:2502.02452, 2025", "abstract": "Large Vision Language Models (LVLMs) have significant potential to deliver personalized assistance by adapting to individual users' unique needs and preferences. Personalization of LVLMs is an emerging area that involves \u2026"}, {"title": "Leveraging Online Olympiad-Level Math Problems for LLMs Training and Contamination-Resistant Evaluation", "link": "https://arxiv.org/pdf/2501.14275", "details": "S Mahdavi, M Li, K Liu, C Thrampoulidis, L Sigal\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Advances in Large Language Models (LLMs) have sparked interest in their ability to solve Olympiad-level math problems. However, the training and evaluation of these models are constrained by the limited size and quality of available datasets, as \u2026"}, {"title": "TAPO: Task-Referenced Adaptation for Prompt Optimization", "link": "https://arxiv.org/pdf/2501.06689", "details": "W Luo, W Wang, X Li, W Zhou, P Jia, X Zhao - arXiv preprint arXiv:2501.06689, 2025", "abstract": "Prompt engineering can significantly improve the performance of large language models (LLMs), with automated prompt optimization (APO) gaining significant attention due to the time-consuming and laborious nature of manual prompt design \u2026"}, {"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models", "link": "https://arxiv.org/pdf/2502.03199", "details": "J Wu, Y Shen, S Liu, Y Tang, S Song, X Wang, L Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the \u2026"}, {"title": "Low-Rank Adapters Meet Neural Architecture Search for LLM Compression", "link": "https://arxiv.org/pdf/2501.16372", "details": "JP Mu\u00f1oz, J Yuan, N Jain - arXiv preprint arXiv:2501.16372, 2025", "abstract": "The rapid expansion of Large Language Models (LLMs) has posed significant challenges regarding the computational resources required for fine-tuning and deployment. Recent advancements in low-rank adapters have demonstrated their \u2026"}, {"title": "Dynamic link prediction: Using language models and graph structures for temporal knowledge graph completion with emerging entities and relations", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425002702", "details": "R Ong, J Sun, YK Guo, O Serban - Expert Systems with Applications, 2025", "abstract": "Abstract Knowledge graphs (KGs) represent real-world facts through entities and relations. However, static KGs fail to capture continuously emerging entities and relations over time. Temporal knowledge graphs address this by incorporating time \u2026"}]
