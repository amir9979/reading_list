[{"title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models", "link": "https://arxiv.org/pdf/2503.24235%3F", "details": "Q Zhang, F Lyu, Z Sun, L Wang, W Zhang, Z Guo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as``test-time computing''has emerged as a prominent research focus. Recent studies demonstrate \u2026"}, {"title": "Agentic Large Language Models, a survey", "link": "https://arxiv.org/pdf/2503.23037", "details": "A Plaat, M van Duijn, N van Stein, M Preuss\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "There is great interest in agentic LLMs, large language models that act as agents. We review the growing body of work in this area and provide a research agenda. Agentic LLMs are LLMs that (1) reason,(2) act, and (3) interact. We organize the \u2026"}, {"title": "Modality Plug-and-Play: Runtime Modality Adaptation in LLM-Driven Autonomous Mobile Systems", "link": "https://sites.pitt.edu/~weigao/publications/mobicom25_mpnp.pdf", "details": "K Huang, X Yin, H Huang, W Gao - ACM MobiCom, 2025", "abstract": "Multimodal reasoning by LLMs is critical to autonomous mobile systems, but the growing diversity of input data modalities prevents incorporating all modalities into LLMs. Instead, only the useful modalities should be adaptively involved at runtime \u2026"}, {"title": "Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for Large Language Models", "link": "https://arxiv.org/pdf/2504.02273", "details": "H Le, D Do, D Nguyen, S Venkatesh - arXiv preprint arXiv:2504.02273, 2025", "abstract": "Recent advances in fine-tuning large language models (LLMs) with reinforcement learning (RL) have shown promising improvements in complex reasoning tasks, particularly when paired with chain-of-thought (CoT) prompting. However, these \u2026"}, {"title": "$\\textit {Agents Under Siege} $: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks", "link": "https://arxiv.org/pdf/2504.00218", "details": "RMS Khan, Z Tan, S Yun, C Flemming, T Chen - arXiv preprint arXiv:2504.00218, 2025", "abstract": "Most discussions about Large Language Model (LLM) safety have focused on single- agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and \u2026"}, {"title": "Efficient and explainable sequential recommendation with language model", "link": "https://drive.google.com/file/d/11rlrXoCrYwH9mIJCDfw2gHwD8PCImjoK/view", "details": "Z Li, L Zou, C Ma, C Li - Information Processing & Management, 2025", "abstract": "Motivated by the outstanding success of large language models (LLMs) in a broad spectrum of NLP tasks, applying them for explainable recommendation become a cutting-edge recently. However, due to the inherent inconsistency in the information \u2026"}, {"title": "Boosting Large Language Models with Mask Fine-Tuning", "link": "https://arxiv.org/pdf/2503.22764", "details": "M Zhang, Y Bai, H Wang, Y Wang, Q Dong, Y Fu - arXiv preprint arXiv:2503.22764, 2025", "abstract": "The model is usually kept integral in the mainstream large language model (LLM) fine-tuning protocols. No works have questioned whether maintaining the integrity of the model is indispensable for performance. In this work, we introduce Mask Fine \u2026"}, {"title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models", "link": "https://arxiv.org/pdf/2503.19540", "details": "D Jung, S Lee, H Moon, C Park, H Lim - arXiv preprint arXiv:2503.19540, 2025", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced interactions between users and models. These advancements concurrently underscore the need for rigorous safety evaluations due to the \u2026"}, {"title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback", "link": "https://arxiv.org/pdf/2503.22230%3F", "details": "W Shen, G Liu, Z Wu, R Zhu, Q Yang, C Xin, Y Yue\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been \u2026"}]
