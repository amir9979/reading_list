[{"title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition", "link": "https://arxiv.org/pdf/2410.13259", "details": "Q Yang, P Wang, LD Plonsky, FL Oswald, H Chen - arXiv preprint arXiv:2410.13259, 2024", "abstract": "We examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of \u2026"}, {"title": "Reducing the Scope of Language Models with Circuit Breakers", "link": "https://arxiv.org/pdf/2410.21597", "details": "D Yunis, S Huo, C Gunasekara, D Contractor - arXiv preprint arXiv:2410.21597, 2024", "abstract": "Language models are now deployed in a wide variety of user-facing applications, often for specific purposes like answering questions about documentation or acting as coding assistants. As these models are intended for particular purposes, they \u2026"}, {"title": "From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency", "link": "https://arxiv.org/pdf/2410.05459", "details": "K Wen, H Zhang, H Lin, J Zhang - arXiv preprint arXiv:2410.05459, 2024", "abstract": "Chain-of-thought (CoT) significantly enhances the reasoning performance of large language models (LLM). While current theoretical studies often attribute this improvement to increased expressiveness and computational capacity, we argue \u2026"}, {"title": "Probing-RAG: Self-Probing to Guide Language Models in Selective Document Retrieval", "link": "https://arxiv.org/pdf/2410.13339", "details": "I Baek, H Chang, B Kim, J Lee, H Lee - arXiv preprint arXiv:2410.13339, 2024", "abstract": "Retrieval-Augmented Generation (RAG) enhances language models by retrieving and incorporating relevant external knowledge. However, traditional retrieve-and- generate processes may not be optimized for real-world scenarios, where queries \u2026"}, {"title": "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes", "link": "https://arxiv.org/pdf/2410.05052", "details": "K Nishida, K Nishida, K Saito - arXiv preprint arXiv:2410.05052, 2024", "abstract": "Loss spikes, a phenomenon in which the loss value diverges suddenly, is a fundamental issue in the pre-training of large language models. This paper supposes that the non-uniformity of the norm of the parameters is one of the causes \u2026"}, {"title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2410.10139", "details": "P Xia, S Han, S Qiu, Y Zhou, Z Wang, W Zheng, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this \u2026"}, {"title": "When Attention Sink Emerges in Language Models: An Empirical View", "link": "https://arxiv.org/pdf/2410.10781", "details": "X Gu, T Pang, C Du, Q Liu, F Zhang, C Du, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as attention sink. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV \u2026"}, {"title": "Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning", "link": "https://arxiv.org/pdf/2410.10360", "details": "Y Xu, R Zhang, X Jiang, Y Feng, Y Xiao, X Ma, R Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Retrieval-Augmented Generation (RAG) offers an effective solution to the issues faced by Large Language Models (LLMs) in hallucination generation and knowledge obsolescence by incorporating externally retrieved knowledge. However, due to \u2026"}, {"title": "Unlocking the Boundaries of Thought: A Reasoning Granularity Framework to Quantify and Optimize Chain-of-Thought", "link": "https://arxiv.org/pdf/2410.05695", "details": "Q Chen, L Qin, J Wang, J Zhou, W Che - arXiv preprint arXiv:2410.05695, 2024", "abstract": "Chain-of-Thought (CoT) reasoning has emerged as a promising approach for enhancing the performance of large language models (LLMs) on complex reasoning tasks. Recently, a series of studies attempt to explain the mechanisms underlying \u2026"}]
