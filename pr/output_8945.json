[{"title": "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models", "link": "https://arxiv.org/pdf/2410.17389", "details": "M Lin, S Shi, Y Guo, B Chalaki, V Tadiparthi, EM Pari\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The correct specification of reward models is a well-known challenge in reinforcement learning. Hand-crafted reward functions often lead to inefficient or suboptimal policies and may not be aligned with user values. Reinforcement \u2026"}, {"title": "Automatic dataset shift identification to support root cause analysis of AI performance drift", "link": "https://arxiv.org/pdf/2411.07940%3F", "details": "M Roschewitz, R Mehta, C Jones, B Glocker - arXiv preprint arXiv:2411.07940, 2024", "abstract": "Shifts in data distribution can substantially harm the performance of clinical AI models. Hence, various methods have been developed to detect the presence of such shifts at deployment time. However, root causes of dataset shifts are varied, and \u2026"}, {"title": "Fast unsupervised ground metric learning with tree-Wasserstein distance", "link": "https://arxiv.org/pdf/2411.07432", "details": "KM D\u00fcsterwald, S Hromadka, M Yamada - arXiv preprint arXiv:2411.07432, 2024", "abstract": "The performance of unsupervised methods such as clustering depends on the choice of distance metric between features, or ground metric. Commonly, ground metrics are decided with heuristics or learned via supervised algorithms. However \u2026"}, {"title": "KALM: Knowledgeable Agent by Offline Reinforcement Learning from Large Language Model Rollouts", "link": "https://openreview.net/pdf%3Fid%3Dtb1MlJCY5g", "details": "JC Pang, SH Yang, K Li, XH Chen, N Tang, Y Yu", "abstract": "Reinforcement learning (RL) traditionally trains agents using interaction data, which limits their capabilities to the scope of the training data. To create more knowledgeable agents, leveraging knowledge from large language models (LLMs) \u2026"}, {"title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models", "link": "https://arxiv.org/pdf/2411.02382%3F", "details": "G Xiong, E Xie, AH Shariatmadari, S Guo, S Bekiranov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in various scientific domains, from natural language processing to complex problem- solving tasks. Their ability to understand and generate human-like text has opened \u2026"}, {"title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas", "link": "https://arxiv.org/pdf/2410.14255", "details": "X Hu, H Fu, J Wang, Y Wang, Z Li, R Xu, Y Lu, Y Jin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Scientific innovation is pivotal for humanity, and harnessing large language models (LLMs) to generate research ideas could transform discovery. However, existing LLMs often produce simplistic and repetitive suggestions due to their limited ability in \u2026"}, {"title": "Concept Bottleneck Language Models For protein design", "link": "https://arxiv.org/pdf/2411.06090", "details": "AA Ismail, T Oikarinen, A Wang, J Adebayo, S Stanton\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce Concept Bottleneck Protein Language Models (CB-pLM), a generative masked language model with a layer where each neuron corresponds to an interpretable concept. Our architecture offers three key benefits: i) Control: We can \u2026"}, {"title": "Reducing the Scope of Language Models with Circuit Breakers", "link": "https://arxiv.org/pdf/2410.21597", "details": "D Yunis, S Huo, C Gunasekara, D Contractor - arXiv preprint arXiv:2410.21597, 2024", "abstract": "Language models are now deployed in a wide variety of user-facing applications, often for specific purposes like answering questions about documentation or acting as coding assistants. As these models are intended for particular purposes, they \u2026"}, {"title": "Trojan Activation Attack: Red-Teaming Large Language Models using Steering Vectors for Safety-Alignment", "link": "https://dl.acm.org/doi/pdf/10.1145/3627673.3679821", "details": "H Wang, K Shu - Proceedings of the 33rd ACM International Conference \u2026, 2024", "abstract": "To ensure AI safety, instruction-tuned Large Language Models (LLMs) are specifically trained to ensure alignment, which refers to making models behave in accordance with human intentions. While these models have demonstrated \u2026"}]
