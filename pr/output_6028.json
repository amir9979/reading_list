[{"title": "On Robustness-Accuracy Characterization of Language Models using Synthetic Datasets", "link": "https://openreview.net/pdf%3Fid%3DC0j44uRPcl", "details": "CY Ko, PY Chen, P Das, YS Chuang, L Daniel - First Conference on Language Modeling", "abstract": "In recent years, language models (LMs) that were pretrained at scale on diverse data have proven to be a successful approach for solving different downstream tasks. However, new concerns about proper performance evaluation have been raised \u2026"}, {"title": "A Gradient Analysis Framework for Rewarding Good and Penalizing Bad Examples in Language Models", "link": "https://arxiv.org/pdf/2408.16751", "details": "YL Tuan, WY Wang - arXiv preprint arXiv:2408.16751, 2024", "abstract": "Beyond maximum likelihood estimation (MLE), the standard objective of a language model (LM) that optimizes good examples probabilities, many studies have explored ways that also penalize bad examples for enhancing the quality of output distribution \u2026"}, {"title": "BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems", "link": "https://arxiv.org/pdf/2408.15971", "details": "W Wang, D Zhang, T Feng, B Wang, J Tang - arXiv preprint arXiv:2408.15971, 2024", "abstract": "Large Language Models (LLMs) are becoming increasingly powerful and capable of handling complex tasks, eg, building single agents and multi-agent systems. Compared to single agents, multi-agent systems have higher requirements for the \u2026"}, {"title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models", "link": "https://arxiv.org/pdf/2408.12798", "details": "Y Li, H Huang, Y Zhao, X Ma, J Sun - arXiv preprint arXiv:2408.12798, 2024", "abstract": "Generative Large Language Models (LLMs) have made significant strides across various tasks, but they remain vulnerable to backdoor attacks, where specific triggers in the prompt cause the LLM to generate adversary-desired responses. While most \u2026"}, {"title": "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models", "link": "https://arxiv.org/pdf/2408.14866", "details": "H Liu, Y Xie, Y Wang, M Shieh - arXiv preprint arXiv:2408.14866, 2024", "abstract": "Language Language Models (LLMs) face safety concerns due to potential misuse by malicious users. Recent red-teaming efforts have identified adversarial suffixes capable of jailbreaking LLMs using the gradient-based search algorithm Greedy \u2026"}, {"title": "A Law of Next-Token Prediction in Large Language Models", "link": "https://arxiv.org/pdf/2408.13442", "details": "H He, WJ Su - arXiv preprint arXiv:2408.13442, 2024", "abstract": "Large language models (LLMs) have been widely employed across various application domains, yet their black-box nature poses significant challenges to understanding how these models process input data internally to make predictions \u2026"}, {"title": "Measuring and Controlling Instruction (In) Stability in Language Model Dialogs", "link": "https://openreview.net/pdf%3Fid%3D60a1SAtH4e", "details": "K Li, T Liu, N Bashkansky, D Bau, F Vi\u00e9gas, H Pfister\u2026 - First Conference on \u2026, 2024", "abstract": "System-prompting is a standard tool for customizing language-model chatbots, enabling them to follow a specific instruction. An implicit assumption in the use of system prompts is that they will be _stable_, so the chatbot will continue to generate \u2026"}, {"title": "Focused Large Language Models are Stable Many-Shot Learners", "link": "https://arxiv.org/pdf/2408.13987", "details": "P Yuan, S Feng, Y Li, X Wang, Y Zhang, C Tan, B Pan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL \u2026"}, {"title": "Pairwise Proximal Policy Optimization: Language Model Alignment with Comparative RL", "link": "https://openreview.net/pdf%3Fid%3D7iaAlIlV2H", "details": "T Wu, B Zhu, R Zhang, Z Wen, K Ramchandran, J Jiao - First Conference on Language \u2026", "abstract": "LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior is Reinforcement Learning with Human Feedback (RLHF). This involves training a reward model with \u2026"}]
