[{"title": "Metric-DST: Mitigating Selection Bias Through Diversity-Guided Semi-Supervised Metric Learning", "link": "https://arxiv.org/pdf/2411.18442", "details": "YI Tepeli, M de Wolf, JP Goncalves - arXiv preprint arXiv:2411.18442, 2024", "abstract": "Selection bias poses a critical challenge for fairness in machine learning, as models trained on data that is less representative of the population might exhibit undesirable behavior for underrepresented profiles. Semi-supervised learning strategies like self \u2026"}, {"title": "PPLqa: An Unsupervised Information-Theoretic Quality Metric for Comparing Generative Large Language Models", "link": "https://arxiv.org/pdf/2411.15320", "details": "G Friedland, X Huang, Y Cui, V Kapoor, A Khetan\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We propose PPLqa, an easy to compute, language independent, information- theoretic metric to measure the quality of responses of generative Large Language Models (LLMs) in an unsupervised way, without requiring ground truth annotations or \u2026"}, {"title": "Exploring Visual Vulnerabilities via Multi-Loss Adversarial Search for Jailbreaking Vision-Language Models", "link": "https://arxiv.org/pdf/2411.18000", "details": "S Hao, B Hooi, J Liu, KW Chang, Z Huang, Y Cai - arXiv preprint arXiv:2411.18000, 2024", "abstract": "Despite inheriting security measures from underlying language models, Vision- Language Models (VLMs) may still be vulnerable to safety alignment issues. Through empirical analysis, we uncover two critical findings: scenario-matched \u2026"}, {"title": "Uncovering Safety Risks of Large Language Models through Concept Activation Vector", "link": "https://openreview.net/pdf%3Fid%3DUymv9ThB50", "details": "Z Xu, R Huang, C Chen, X Wang - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Despite careful safety alignment, current large language models (LLMs) remain vulnerable to various attacks. To further unveil the safety risks of LLMs, we introduce a Safety Concept Activation Vector (SCAV) framework, which effectively guides the \u2026"}, {"title": "Where does In-context Learning\\\\\\Happen in Large Language Models?", "link": "https://openreview.net/pdf%3Fid%3DLLuSjg59an", "details": "S Sia, D Mueller, K Duh - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Self-supervised large language models have demonstrated the ability to perform various tasks via in-context learning, but little is known about where the model locates the task with respect to prompt instructions and demonstration examples. In \u2026"}, {"title": "Membership Inference Attacks against Large Language Models via Self-prompt Calibration", "link": "https://fi.ee.tsinghua.edu.cn/~gaochen/papers/NeurIPS2024-SPV-MIA.pdf", "details": "W Fu, H Wang, G Liu, Y Li, T Jiang", "abstract": "Abstract Membership Inference Attacks (MIA) aim to infer whether a target data record has been utilized for model training or not. Existing MIAs designed for large language models (LLMs) can be bifurcated into two types: reference-free and \u2026"}, {"title": "Probing the Robustness of Theory of Mind in Large Language Models", "link": "https://openreview.net/pdf%3Fid%3DX8Mdv9qLOS", "details": "L Schrewe, C Nickel, L Flek - Eighth Widening NLP Workshop (WiNLP 2024) Phase \u2026", "abstract": "Theory of Mind (ToM) is considered essential in understanding the intentions and beliefs of others. Recent advancements in large language models (LLMs) like ChatGPT have sparked claims that these models exhibit ToM capabilities. However \u2026"}]
