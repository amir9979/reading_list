[{"title": "Efficient Federated Unlearning with Adaptive Differential Privacy Preservation", "link": "https://arxiv.org/pdf/2411.11044", "details": "Y Jiang, X Tong, Z Liu, H Ye, CW Tan, KY Lam - arXiv preprint arXiv:2411.11044, 2024", "abstract": "Federated unlearning (FU) offers a promising solution to effectively address the need to erase the impact of specific clients' data on the global model in federated learning (FL), thereby granting individuals the``Right to be Forgotten\". The most \u2026"}, {"title": "ConceptDrift: Uncovering Biases through the Lens of Foundation Models", "link": "https://openreview.net/pdf%3Fid%3Da78CZ8g85n", "details": "CD Paduraru, A Barbalau, R Filipescu, AL Nicolicioiu\u2026 - Interpretable AI: Past, Present and \u2026", "abstract": "Datasets and pre-trained models come with intrinsic biases. Most methods rely on spotting them by analyzing misclassified samples, in a semi-automated human- computer validation. In contrast, we propose ConceptDrift, a method that analyzes \u2026"}, {"title": "EFTViT: Efficient Federated Training of Vision Transformers with Masked Images on Resource-Constrained Edge Devices", "link": "https://arxiv.org/pdf/2412.00334", "details": "M Wu, T Chang, C Miao, J Zhou, C Li, X Xu, M Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Federated learning research has recently shifted from Convolutional Neural Networks (CNNs) to Vision Transformers (ViTs) due to their superior capacity. ViTs training demands higher computational resources due to the lack of 2D inductive \u2026"}, {"title": "Medsafetybench: Evaluating and improving the medical safety of large language models", "link": "https://openreview.net/pdf%3Fid%3DcFyagd2Yh4", "details": "T Han, A Kumar, C Agarwal, H Lakkaraju - The Thirty-eight Conference on Neural \u2026, 2024", "abstract": "As large language models (LLMs) develop increasingly sophisticated capabilities and find applications in medical settings, it becomes important to assess their medical safety due to their far-reaching implications for personal and public health \u2026"}, {"title": "Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models", "link": "https://arxiv.org/pdf/2412.02674", "details": "Y Song, H Zhang, C Eisenach, S Kakade, D Foster\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-improvement is a mechanism in Large Language Model (LLM) pre-training, post- training and test-time inference. We explore a framework where the model verifies its own outputs, filters or reweights data based on this verification, and distills the filtered \u2026"}, {"title": "Harmful Prompt Classification for Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3701268.3701271", "details": "O Gupta, M de la Cuadra Lozano, A Busalim\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Over the last few years, using LLM chatbots like ClaudeAI, Co-pilot and ChatGPT for text generation has become a regular habit for many, with over 100 million weekly users flocking to ChatGPT alone. One side effect of such vast usage of these models \u2026"}]
