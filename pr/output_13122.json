[{"title": "Enhancing Chest X-ray Classification through Knowledge Injection in Cross-Modality Learning", "link": "https://arxiv.org/pdf/2502.13447", "details": "Y Yan, B Yue, Q Li, M Huang, J Chen, Z Lan - arXiv preprint arXiv:2502.13447, 2025", "abstract": "The integration of artificial intelligence in medical imaging has shown tremendous potential, yet the relationship between pre-trained knowledge and performance in cross-modality learning remains unclear. This study investigates how explicitly \u2026"}, {"title": "MM-Retinal V2: Transfer an Elite Knowledge Spark into Fundus Vision-Language Pretraining", "link": "https://arxiv.org/pdf/2501.15798", "details": "R Wu, N Su, C Zhang, T Ma, T Zhou, Z Cui, N Tang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language pretraining (VLP) has been investigated to generalize across diverse downstream tasks for fundus image analysis. Although recent methods showcase promising achievements, they significantly rely on large-scale private \u2026"}, {"title": "EfficientLLM: Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models", "link": "https://arxiv.org/pdf/2502.06663", "details": "X Xing, Z Liu, S Xiao, B Gao, Y Liang, W Zhang, H Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Modern large language models (LLMs) driven by scaling laws, achieve intelligence emergency in large model sizes. Recently, the increasing concerns about cloud costs, latency, and privacy make it an urgent requirement to develop compact edge \u2026"}, {"title": "ACF-R+: An asymmetry-sensitive method for image-text retrieval enhanced by cross-modal fusion and re-ranking based on contrastive learning", "link": "https://www.sciencedirect.com/science/article/pii/S0925231225003145", "details": "Z Gong, Y Huang, C Yu, P Dai, X Ge, Y Shen, Y Liu - Neurocomputing, 2025", "abstract": "The task of multi-modal retrieval between the image and text modality is to find pertinent information from a designated image or textual corpus. The principal challenge lies in the integration of multi-modal representations and the discernment \u2026"}, {"title": "Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More", "link": "https://arxiv.org/pdf/2502.11494", "details": "Z Wen, Y Gao, S Wang, J Zhang, Q Zhang, W Li, C He\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision tokens in multimodal large language models often dominate huge computational overhead due to their excessive length compared to linguistic modality. Abundant recent methods aim to solve this problem with token pruning \u2026"}, {"title": "A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models", "link": "https://arxiv.org/pdf/2502.13942", "details": "H Huang, S Yuan, Y Hao, C Wen, Y Fang - arXiv preprint arXiv:2502.13942, 2025", "abstract": "A large-scale vision and language model that has been pretrained on massive data encodes visual and linguistic prior, which makes it easier to generate images and language that are more natural and realistic. Despite this, there is still a significant \u2026"}, {"title": "SKI Models: Skeleton Induced Vision-Language Embeddings for Understanding Activities of Daily Living", "link": "https://arxiv.org/pdf/2502.03459", "details": "A Sinha, D Reilly, F Bremond, P Wang, S Das - arXiv preprint arXiv:2502.03459, 2025", "abstract": "The introduction of vision-language models like CLIP has enabled the development of foundational video models capable of generalizing to unseen videos and human actions. However, these models are typically trained on web videos, which often fail \u2026"}, {"title": "Str-GCL: Structural Commonsense Driven Graph Contrastive Learning", "link": "https://openreview.net/pdf%3Fid%3DzefCoSncYR", "details": "D He, Y Huang, J Zhao, X Wang, Z Wang - THE WEB CONFERENCE 2025", "abstract": "Graph Contrastive Learning (GCL) is a widely adopted approach in unsupervised representation learning, utilizing representational constraints to derive effective embeddings. However, current GCL methods primarily focus on capturing implicit \u2026"}, {"title": "JAMC: A jigsaw-based autoencoder with masked contrastive learning for cardiovascular disease diagnosis", "link": "https://www.sciencedirect.com/science/article/pii/S0950705125001376", "details": "Y Ge, H Zhang, J Shi, D Luo, S Chang, J He, Q Huang\u2026 - Knowledge-Based Systems, 2025", "abstract": "Self-supervised learning (SSL) is a prevalent approach in the diagnosis of cardiovascular diseases. It leverages unlabeled ECG data for pre-training and thus alleviates the reliance on manual annotations. However, existing self-supervised \u2026"}]
