[{"title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models", "link": "https://arxiv.org/pdf/2505.03075", "details": "Z Shi, L Yan, W Sun, Y Feng, P Ren, X Ma, S Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Retrieval-augmented generation (RAG) integrates large language models (LLM s) with retrievers to access external knowledge, improving the factuality of LLM generation in knowledge-grounded tasks. To optimize the RAG performance, most \u2026"}, {"title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation", "link": "https://arxiv.org/pdf/2504.20859", "details": "G Hadad, H Roitman, Y Eshel, B Shapira, L Rokach - arXiv preprint arXiv:2504.20859, 2025", "abstract": "As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents``X-Cross''--a novel cross-domain sequential-recommendation model \u2026"}, {"title": "DeepCritic: Deliberate Critique with Large Language Models", "link": "https://arxiv.org/pdf/2505.00662%3F", "details": "W Yang, J Chen, Y Lin, JR Wen - arXiv preprint arXiv:2505.00662, 2025", "abstract": "As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a \u2026"}, {"title": "Task Reconstruction and Extrapolation for $\\pi_0 $ using Text Latent", "link": "https://arxiv.org/pdf/2505.03500", "details": "Q Li - arXiv preprint arXiv:2505.03500, 2025", "abstract": "Vision-language-action models (VLAs) often achieve high performance on demonstrated tasks but struggle significantly when required to extrapolate, combining skills learned from different tasks in novel ways. For instance, VLAs might \u2026"}, {"title": "UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models", "link": "https://arxiv.org/pdf/2504.21027", "details": "Y Zheng, L Liu, Y Lin, J Feng, G Zhang, D Jin, Y Li - arXiv preprint arXiv:2504.21027, 2025", "abstract": "The advent of Large Language Models (LLMs) holds promise for revolutionizing various fields traditionally dominated by human expertise. Urban planning, a professional discipline that fundamentally shapes our daily surroundings, is one \u2026"}, {"title": "Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study", "link": "https://arxiv.org/pdf/2504.16414", "details": "M Khodadad, AS Kasmaee, M Astaraki, N Sherck\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a \u2026"}, {"title": "ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data", "link": "https://arxiv.org/pdf/2504.16628", "details": "H Gu, H Wang, Y Mei, M Zhang, Y Jin - arXiv preprint arXiv:2504.16628, 2025", "abstract": "Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm \u2026"}, {"title": "A Temporal Knowledge Graph Generation Dataset Supervised Distantly by Large Language Models", "link": "https://www.nature.com/articles/s41597-025-05062-0", "details": "J Zhu, Y Fu, J Zhou, D Chen - Scientific Data, 2025", "abstract": "Abstract Knowledge graphs can be constructed by extracting triples from documents, which denotes document-level relation extraction. Each triple illustrates a fact composed of two entities and a relation. However, temporal information \u2026"}, {"title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models", "link": "https://arxiv.org/pdf/2505.02686", "details": "X Wu - arXiv preprint arXiv:2505.02686, 2025", "abstract": "Recent developments in Large Language Models (LLMs) have shifted from pre- training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act \u2026"}]
