[{"title": "**Medical** Students' Perceptions of **Large Language Models** in **Healthcare** : A Multinational Cross-Sectional Study", "link": "https://journals.sagepub.com/doi/full/10.1177/23821205251331124", "details": "F Ejas, SA Khan, A Mujahid, F AlJoker, H Mautong\u2026 - Journal of **Medical** \u2026, 2025", "abstract": "\u2026 Artificial intelligence (AI) and **large** **language** **models** (LLMs), are potential tools for enhancing **healthcare** delivery and clinical research. Presently, there is a scarcity of research regarding the viewpoints of **medical** students toward LLMs. This study \u2026"}, {"title": "KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance", "link": "https://arxiv.org/pdf/2505.15480", "details": "Q Zhong, L Ding, X Cai, J Liu, B Du, D Tao - arXiv preprint arXiv:2505.15480, 2025", "abstract": "\u2026 common approach to improve the domain-specific **question** - **answering** (QA) performance of **large** **language** **models** (LLMs). However, recent \u2026 Biomistral: A collection of opensource pretrained **large** **language** **models** for **medical** domains. In \u2026", "entry_id": "http://arxiv.org/abs/2505.15480v1", "updated": "2025-05-21 12:55:28", "published": "2025-05-21 12:55:28", "authors": "Qihuang Zhong;Liang Ding;Xiantao Cai;Juhua Liu;Bo Du;Dacheng Tao", "summary": "Supervised fine-tuning (SFT) is a common approach to improve the\ndomain-specific question-answering (QA) performance of large language models\n(LLMs). However, recent literature reveals that due to the conflicts between\nLLMs' internal knowledge and the context knowledge of training data, vanilla\nSFT using the full QA training set is usually suboptimal. In this paper, we\nfirst design a query diversification strategy for robust conflict detection and\nthen conduct a series of experiments to analyze the impact of knowledge\nconflict. We find that 1) training samples with varied conflicts contribute\ndifferently, where SFT on the data with large conflicts leads to catastrophic\nperformance drops; 2) compared to directly filtering out the conflict data,\nappropriately applying the conflict data would be more beneficial. Motivated by\nthis, we propose a simple-yet-effective Knowledge-aware Fine-tuning (namely\nKaFT) approach to effectively boost LLMs' performance. The core of KaFT is to\nadapt the training weight by assigning different rewards for different training\nsamples according to conflict level. Extensive experiments show that KaFT\nbrings consistent and significant improvements across four LLMs. More analyses\nprove that KaFT effectively improves the model generalization and alleviates\nthe hallucination.", "comment": "Accepted to ACL2025 Findings", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.15480v1;http://arxiv.org/pdf/2505.15480v1", "pdf_url": "http://arxiv.org/pdf/2505.15480v1"}, {"title": "Large language model evaluation in autoimmune disease clinical **questions** comparing ChatGPT 4o, Claude 3.5 Sonnet and Gemini 1.5 pro", "link": "https://www.nature.com/articles/s41598-025-02601-y", "details": "J Ma, J Yu, A Xie, T Huang, W Liu, M Ma, Y Tao, F Zang\u2026 - Scientific Reports, 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs) have established a presence in providing **medical** services to patients and supporting clinical practice for \u2026 To explore the ability of LLMs in **answering** clinical **questions** related to autoimmune diseases, this \u2026"}, {"title": "MedBrowseComp: Benchmarking Medical Deep Research and Computer Use", "link": "https://arxiv.org/pdf/2505.14963", "details": "S Chen, P Moreira, Y Xiao, S Schmidgall, J Warner\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 We hope it will accelerate progress toward efficient, high-accuracy **medical** **question** **answering** across modalities. \u2026 Benchmarking **large** **language** **models** on CMExam - a comprehensive chinese **medical** exam dataset. In Thirty-seventh \u2026", "entry_id": "http://arxiv.org/abs/2505.14963v1", "updated": "2025-05-20 22:42:33", "published": "2025-05-20 22:42:33", "authors": "Shan Chen;Pedro Moreira;Yuxin Xiao;Sam Schmidgall;Jeremy Warner;Hugo Aerts;Thomas Hartvigsen;Jack Gallifant;Danielle S. Bitterman", "summary": "Large language models (LLMs) are increasingly envisioned as decision-support\ntools in clinical practice, yet safe clinical reasoning demands integrating\nheterogeneous knowledge bases -- trials, primary studies, regulatory documents,\nand cost data -- under strict accuracy constraints. Existing evaluations often\nrely on synthetic prompts, reduce the task to single-hop factoid queries, or\nconflate reasoning with open-ended generation, leaving their real-world utility\nunclear. To close this gap, we present MedBrowseComp, the first benchmark that\nsystematically tests an agent's ability to reliably retrieve and synthesize\nmulti-hop medical facts from live, domain-specific knowledge bases.\nMedBrowseComp contains more than 1,000 human-curated questions that mirror\nclinical scenarios where practitioners must reconcile fragmented or conflicting\ninformation to reach an up-to-date conclusion. Applying MedBrowseComp to\nfrontier agentic systems reveals performance shortfalls as low as ten percent,\nexposing a critical gap between current LLM capabilities and the rigor demanded\nin clinical settings. MedBrowseComp therefore offers a clear testbed for\nreliable medical information seeking and sets concrete goals for future model\nand toolchain upgrades. You can visit our project page at:\nhttps://moreirap12.github.io/mbc-browse-app/", "comment": "You can visit our project page at:\n  https://moreirap12.github.io/mbc-browse-app/", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.14963v1;http://arxiv.org/pdf/2505.14963v1", "pdf_url": "http://arxiv.org/pdf/2505.14963v1"}, {"title": "Comparative Analysis Study on Automated Dataset Generation Frameworks for RAG System Performance Evaluation", "link": "https://koreascience.kr/article/JAKO202514154005683.pdf", "details": "B Kim, J Yang - The Journal of Korea Institute of Information \u2026, 2025", "abstract": "\u2026 It explains the necessity and importance of RAG technology in overcoming the limitations of **large** **language** **models** (LLMs) and \u2026 were selected, and for each, 100 **question** \u2013 **answer** pairs were generated using documents from the **medical** , financial \u2026"}, {"title": "Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries", "link": "https://arxiv.org/pdf/2505.15420", "details": "Y Wang, W Qu, Y Jiang, Z Liu, Y Liu, S Zhai, Y Dong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Retrieval-Augmented Generation (RAG) systems enhance **large** **language** **models** (LLMs) by \u2026 based on various RAG applications, including **healthcare** and story book scenarios, both on \u2026 on multi-choice **questions** (MCQ) and open-ended \u2026", "entry_id": "http://arxiv.org/abs/2505.15420v1", "updated": "2025-05-21 12:04:42", "published": "2025-05-21 12:04:42", "authors": "Yuhao Wang;Wenjie Qu;Yanze Jiang;Zichen Liu;Yue Liu;Shengfang Zhai;Yinpeng Dong;Jiaheng Zhang", "summary": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by incorporating external knowledge bases, but they are vulnerable to\nprivacy risks from data extraction attacks. Existing extraction methods\ntypically rely on malicious inputs such as prompt injection or jailbreaking,\nmaking them easily detectable via input- or output-level detection. In this\npaper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts\nknowledge extraction on RAG systems through benign queries. IKEA first\nleverages anchor concepts to generate queries with the natural appearance, and\nthen designs two mechanisms to lead to anchor concept thoroughly 'explore' the\nRAG's privacy knowledge: (1) Experience Reflection Sampling, which samples\nanchor concepts based on past query-response patterns to ensure the queries'\nrelevance to RAG documents; (2) Trust Region Directed Mutation, which\niteratively mutates anchor concepts under similarity constraints to further\nexploit the embedding space. Extensive experiments demonstrate IKEA's\neffectiveness under various defenses, surpassing baselines by over 80% in\nextraction efficiency and 90% in attack success rate. Moreover, the substitute\nRAG system built from IKEA's extractions consistently outperforms those based\non baseline methods across multiple evaluation tasks, underscoring the\nsignificant privacy risk in RAG systems.", "comment": null, "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR;cs.AI", "links": "http://arxiv.org/abs/2505.15420v1;http://arxiv.org/pdf/2505.15420v1", "pdf_url": "http://arxiv.org/pdf/2505.15420v1"}, {"title": "Nested Named Entity Recognition: A Survey of Latest Research", "link": "https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.70052", "details": "L Ji, Y Dang, Y Du, W Gao, H Zhang - Expert Systems, 2025", "abstract": "\u2026 the foundation for information extraction, knowledge **question** **answering** system, text summary, \u2026 The dataset includes nine major categories of **medical** entities, including 504 prevalent \u2026 However, a common observation is that many **large** \u2026"}, {"title": "Exploring the Innovation Opportunities for Pre-trained Models", "link": "https://arxiv.org/pdf/2505.15790", "details": "M Park, J Forlizzi, J Zimmerman - arXiv preprint arXiv:2505.15790, 2025", "abstract": "\u2026 In this paper, we use the term pre-trained model to collectively mean **Large** **Language** **Models** , \u2026 We looked at the domain (eg, **healthcare** , hospitality). We extracted pre-trained model \u2026 For example, a system made for doctors would get \u2026", "entry_id": "http://arxiv.org/abs/2505.15790v1", "updated": "2025-05-21 17:43:46", "published": "2025-05-21 17:43:46", "authors": "Minjung Park;Jodi Forlizzi;John Zimmerman", "summary": "Innovators transform the world by understanding where services are\nsuccessfully meeting customers' needs and then using this knowledge to identify\nfailsafe opportunities for innovation. Pre-trained models have changed the AI\ninnovation landscape, making it faster and easier to create new AI products and\nservices. Understanding where pre-trained models are successful is critical for\nsupporting AI innovation. Unfortunately, the hype cycle surrounding pre-trained\nmodels makes it hard to know where AI can really be successful. To address\nthis, we investigated pre-trained model applications developed by HCI\nresearchers as a proxy for commercially successful applications. The research\napplications demonstrate technical capabilities, address real user needs, and\navoid ethical challenges. Using an artifact analysis approach, we categorized\ncapabilities, opportunity domains, data types, and emerging interaction design\npatterns, uncovering some of the opportunity space for innovation with\npre-trained models.", "comment": "33 pages, 20 figures, 4 tables, DIS", "journal_ref": null, "primary_category": "cs.HC", "categories": "cs.HC;cs.AI", "links": "http://dx.doi.org/10.1145/3715336.3735753;http://arxiv.org/abs/2505.15790v1;http://arxiv.org/pdf/2505.15790v1", "pdf_url": "http://arxiv.org/pdf/2505.15790v1"}, {"title": "Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs", "link": "https://arxiv.org/pdf/2505.15524", "details": "L Gao, K Wan, W Liu, C Wang, Z Song, Z Xu, Y Wang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 Bias in **Large** **Language** **Models** (LLMs) significantly undermines their reliability and fairness. We focus on a common form of bias: when \u2026 Building on prior studies [73, 74], we focus on the **medical** and educational domains\u2014two areas where model \u2026", "entry_id": "http://arxiv.org/abs/2505.15524v1", "updated": "2025-05-21 13:50:23", "published": "2025-05-21 13:50:23", "authors": "Lang Gao;Kaiyang Wan;Wei Liu;Chenxi Wang;Zirui Song;Zixiang Xu;Yanbo Wang;Veselin Stoyanov;Xiuying Chen", "summary": "Bias in Large Language Models (LLMs) significantly undermines their\nreliability and fairness. We focus on a common form of bias: when two reference\nconcepts in the model's concept space, such as sentiment polarities (e.g.,\n\"positive\" and \"negative\"), are asymmetrically correlated with a third, target\nconcept, such as a reviewing aspect, the model exhibits unintended bias. For\ninstance, the understanding of \"food\" should not skew toward any particular\nsentiment. Existing bias evaluation methods assess behavioral differences of\nLLMs by constructing labeled data for different social groups and measuring\nmodel responses across them, a process that requires substantial human effort\nand captures only a limited set of social concepts. To overcome these\nlimitations, we propose BiasLens, a test-set-free bias analysis framework based\non the structure of the model's vector space. BiasLens combines Concept\nActivation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract\ninterpretable concept representations, and quantifies bias by measuring the\nvariation in representational similarity between the target concept and each of\nthe reference concepts. Even without labeled data, BiasLens shows strong\nagreement with traditional bias evaluation metrics (Spearman correlation r >\n0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect\nusing existing methods. For example, in simulated clinical scenarios, a\npatient's insurance status can cause the LLM to produce biased diagnostic\nassessments. Overall, BiasLens offers a scalable, interpretable, and efficient\nparadigm for bias discovery, paving the way for improving fairness and\ntransparency in LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.15524v1;http://arxiv.org/pdf/2505.15524v1", "pdf_url": "http://arxiv.org/pdf/2505.15524v1"}]
