[{"title": "Bypassing the Exponential Dependency: Looped Transformers Efficiently Learn In-context by Multi-step Gradient Descent", "link": "https://arxiv.org/pdf/2410.11268", "details": "B Chen, X Li, Y Liang, Z Shi, Z Song - arXiv preprint arXiv:2410.11268, 2024", "abstract": "In-context learning has been recognized as a key factor in the success of Large Language Models (LLMs). It refers to the model's ability to learn patterns on the fly from provided in-context examples in the prompt during inference. Previous studies \u2026"}, {"title": "Beyond Graphs: Can Large Language Models Comprehend Hypergraphs?", "link": "https://arxiv.org/pdf/2410.10083", "details": "Y Feng, C Yang, X Hou, S Du, S Ying, Z Wu, Y Gao - arXiv preprint arXiv:2410.10083, 2024", "abstract": "Existing benchmarks like NLGraph and GraphQA evaluate LLMs on graphs by focusing mainly on pairwise relationships, overlooking the high-order correlations found in real-world data. Hypergraphs, which can model complex beyond-pairwise \u2026"}, {"title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models", "link": "https://arxiv.org/pdf/2411.00836", "details": "C Zou, X Guo, R Yang, J Zhang, B Hu, H Zhang - arXiv preprint arXiv:2411.00836, 2024", "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor \u2026"}, {"title": "Vision-Language Models Can Self-Improve Reasoning via Reflection", "link": "https://arxiv.org/pdf/2411.00855", "details": "K Cheng, Y Li, F Xu, J Zhang, H Zhou, Y Liu - arXiv preprint arXiv:2411.00855, 2024", "abstract": "Chain-of-thought (CoT) has proven to improve the reasoning capability of large language models (LLMs). However, due to the complexity of multimodal scenarios and the difficulty in collecting high-quality CoT data, CoT reasoning in multimodal \u2026"}, {"title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs", "link": "https://arxiv.org/pdf/2410.12881%3F", "details": "SN Akter, S Prabhumoye, J Kamalu, S Satheesh\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models (LLMs). Yet, these approaches fall inadequate in complex, multi-hop and \u2026"}, {"title": "Pandora's Box: Towards Building Universal Attackers against Real-World Large Vision-Language Models", "link": "https://openreview.net/pdf%3Fid%3DgDpWYpocE1", "details": "D Liu, M Yang, X Qu, P Zhou, X Fang, K Tang, Y Wan\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across a wide range of multimodal understanding tasks. Nevertheless, these models are susceptible to adversarial examples. In real-world applications, existing LVLM \u2026"}, {"title": "Ada-K Routing: Boosting the Efficiency of MoE-based LLMs", "link": "https://arxiv.org/pdf/2410.10456", "details": "T Yue, L Guo, J Cheng, X Gao, J Liu - arXiv preprint arXiv:2410.10456, 2024", "abstract": "In the era of Large Language Models (LLMs), Mixture-of-Experts (MoE) architectures offer a promising approach to managing computational costs while scaling up model parameters. Conventional MoE-based LLMs typically employ static Top-K routing \u2026"}, {"title": "Continual LLaVA: Continual Instruction Tuning in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.02564", "details": "M Cao, Y Liu, Y Liu, T Wang, J Dong, H Ding, X Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Instruction tuning constitutes a prevalent technique for tailoring Large Vision Language Models (LVLMs) to meet individual task requirements. To date, most of the existing approaches are confined to single-task adaptation, whereas the \u2026"}, {"title": "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "link": "https://arxiv.org/pdf/2410.10054", "details": "P Qing, C Gao, Y Zhou, X Diao, Y Yang, S Vosoughi - arXiv preprint arXiv:2410.10054, 2024", "abstract": "Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), are known to enhance training efficiency in Large Language Models (LLMs). Due to the limited parameters of LoRA, recent studies seek to combine LoRA with Mixture-of \u2026"}]
