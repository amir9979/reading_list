[{"title": "CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models", "link": "https://arxiv.org/pdf/2406.06007", "details": "P Xia, Z Chen, J Tian, Y Gong, R Hou, Y Xu, Z Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the \u2026"}, {"title": "Residual-based Language Models are Free Boosters for Biomedical Imaging Tasks", "link": "https://openaccess.thecvf.com/content/CVPR2024W/DEF-AI-MIA/papers/Lai_Residual-based_Language_Models_are_Free_Boosters_for_Biomedical_Imaging_Tasks_CVPRW_2024_paper.pdf", "details": "Z Lai, J Wu, S Chen, Y Zhou, N Hovakimyan - \u2026 of the IEEE/CVF Conference on \u2026, 2024", "abstract": "In this study we uncover the unexpected efficacy of residual-based large language models (LLMs) as part of encoders for biomedical imaging tasks a domain traditionally devoid of language or textual data. The approach diverges from \u2026"}, {"title": "Molecular Data Programming: Towards Molecule Pseudo-labeling with Systematic Weak Supervision", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Juan_Molecular_Data_Programming_Towards_Molecule_Pseudo-labeling_with_Systematic_Weak_Supervision_CVPR_2024_paper.pdf", "details": "X Juan, K Zhou, N Liu, T Chen, X Wang - Proceedings of the IEEE/CVF Conference \u2026, 2024", "abstract": "The premise for the great advancement of molecular machine learning is dependent on a considerable amount of labeled data. In many real-world scenarios the labeled molecules are limited in quantity or laborious to derive. Recent pseudo-labeling \u2026"}, {"title": "A Deep Multimodal Representation Learning Framework for Accurate Molecular Properties Prediction", "link": "https://dl.acm.org/doi/abs/10.1145/3649476.3660377", "details": "Y Yang, Z Wang, P Ahadian, A Jerger, J Zucker, S Feng\u2026 - Proceedings of the Great \u2026, 2024", "abstract": "Drug discovery is a challenging process, requiring the optimization of compounds to become safe and effective. Predicting molecular properties is an indispensable step in the drug discovery pipeline. Traditionally, this process is costly, involving multiple \u2026"}, {"title": "Word2HyperVec: From Word Embeddings to Hypervectors for Hyperdimensional Computing", "link": "https://dl.acm.org/doi/abs/10.1145/3649476.3658795", "details": "AG Ayar, S Aygun, MH Najafi, M Margala - \u2026 of the Great Lakes Symposium on VLSI \u2026, 2024", "abstract": "Word-aware sentiment analysis has posed a significant challenge over the past decade. Despite the considerable efforts of recent language models, achieving a lightweight representation suitable for deployment on resource-constrained edge \u2026"}, {"title": "Training Compute-Optimal Protein Language Models", "link": "https://www.biorxiv.org/content/10.1101/2024.06.06.597716.full.pdf", "details": "X Cheng, B Chen, P Li, J Gong, J Tang, L Song - bioRxiv, 2024", "abstract": "We explore optimally training protein language models, an area of significant interest in biological research where guidance on best practices is limited. Most models are trained with extensive compute resources until performance gains plateau, focusing \u2026"}, {"title": "Supplementary Material VILA: On Pre-training for Visual Language Models", "link": "https://openaccess.thecvf.com/content/CVPR2024/supplemental/Lin_VILA_On_Pre-training_CVPR_2024_supplemental.pdf", "details": "J Lin, H Yin, W Ping, P Molchanov, M Shoeybi, S Han", "abstract": "We used an in-house data blend for supervised finetuning/instruction tuning during the ablation study. We followed [5] to build the FLAN-style instructions from the training set of 18 visual language datasets, as shown in Table 1. We may see that \u2026"}]
