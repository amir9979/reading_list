[{"title": "Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding", "link": "https://arxiv.org/pdf/2501.14548", "details": "Z Shui, J Zhang, W Cao, S Wang, R Guo, L Lu, L Yang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Artificial intelligence (AI) shows great potential in assisting radiologists to improve the efficiency and accuracy of medical image interpretation and diagnosis. However, a versatile AI model requires large-scale data and comprehensive annotations, which \u2026"}, {"title": "ConceptCLIP: Towards Trustworthy Medical AI via Concept-Enhanced Contrastive Langauge-Image Pre-training", "link": "https://arxiv.org/pdf/2501.15579", "details": "Y Nie, S He, Y Bie, Y Wang, Z Chen, S Yang, H Chen - arXiv preprint arXiv \u2026, 2025", "abstract": "Trustworthiness is essential for the precise and interpretable application of artificial intelligence (AI) in medical imaging. Traditionally, precision and interpretability have been addressed as separate tasks, namely medical image analysis and explainable \u2026"}, {"title": "Dynamic graph based weakly supervised deep hashing for whole slide image classification and retrieval", "link": "https://www.sciencedirect.com/science/article/pii/S1361841525000167", "details": "H Jin, J Shen, L Cui, X Shi, K Li, X Zhu - Medical Image Analysis, 2025", "abstract": "Recently, a multi-scale representation attention based deep multiple instance learning method has proposed to directly extract patch-level image features from gigapixel whole slide images (WSIs), and achieved promising performance on \u2026"}, {"title": "Bridged Semantic Alignment for Zero-shot 3D Medical Image Diagnosis", "link": "https://arxiv.org/pdf/2501.03565", "details": "H Lai, Z Jiang, Q Yao, R Wang, Z He, X Tao, W Wei\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "3D medical images such as Computed tomography (CT) are widely used in clinical practice, offering a great potential for automatic diagnosis. Supervised learning- based approaches have achieved significant progress but rely heavily on extensive \u2026"}, {"title": "SeLa-MIL: Developing an instance-level classifier via weakly-supervised self-training for whole slide image classification", "link": "https://www.sciencedirect.com/science/article/pii/S0169260725000318", "details": "Y Ma, M Yuan, A Shen, X Luo, B An, X Chen, M Wang - Computer Methods and \u2026, 2025", "abstract": "Abstract Background and Objective Pathology image classification is crucial in clinical cancer diagnosis and computer-aided diagnosis. Whole Slide Image (WSI) classification is often framed as a multiple instance learning (MIL) problem due to the \u2026"}, {"title": "Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink", "link": "https://arxiv.org/pdf/2501.15269", "details": "Y Wang, M Zhang, J Sun, C Wang, M Yang, H Xue\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Fusing visual understanding into language generation, Multi-modal Large Language Models (MLLMs) are revolutionizing visual-language applications. Yet, these models are often plagued by the hallucination problem, which involves generating \u2026"}, {"title": "A Novel Framework for Whole-Slide Pathological Image Classification Based on the Cascaded Attention Mechanism", "link": "https://www.mdpi.com/1424-8220/25/3/726", "details": "D Liu, B Hu - Sensors, 2025", "abstract": "This study introduces an innovative deep learning framework to address the limitations of traditional pathological image analysis and the pressing demand for medical resources in tumor diagnosis. With the global rise in cancer cases, manual \u2026"}, {"title": "OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for Small-Large Language Models", "link": "https://arxiv.org/pdf/2501.12975", "details": "C Sun, Y Li, D Wu, B Boulet - arXiv preprint arXiv:2501.12975, 2025", "abstract": "Large Language Models (LLMs) are highly capable but require significant computational resources for both training and inference. Within the LLM family, smaller models (those with fewer than 10 billion parameters) also perform well \u2026"}]
