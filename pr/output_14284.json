[{"title": "From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration", "link": "https://arxiv.org/pdf/2503.12821", "details": "M Song, X Qu, J Zhou, Y Cheng - arXiv preprint arXiv:2503.12821, 2025", "abstract": "Large Vision-Language Models (LVLMs) have achieved significant progress in combining visual comprehension with language generation. Despite this success, the training data of LVLMs still suffers from Long-Tail (LT) problems, where the data \u2026"}, {"title": "Boosting the Generalization and Reasoning of Vision Language Models with Curriculum Reinforcement Learning", "link": "https://arxiv.org/pdf/2503.07065", "details": "H Deng, D Zou, R Ma, H Luo, Y Cao, Y Kang - arXiv preprint arXiv:2503.07065, 2025", "abstract": "While state-of-the-art vision-language models (VLMs) have demonstrated remarkable capabilities in complex visual-text tasks, their success heavily relies on massive model scaling, limiting their practical deployment. Small-scale VLMs offer a \u2026"}, {"title": "Evaluation of Safety Cognition Capability in Vision-Language Models for Autonomous Driving", "link": "https://arxiv.org/pdf/2503.06497", "details": "E Zhang, P Gong, X Dai, Y Lv, Q Miao - arXiv preprint arXiv:2503.06497, 2025", "abstract": "Assessing the safety of vision-language models (VLMs) in autonomous driving is particularly important; however, existing work mainly focuses on traditional benchmark evaluations. As interactive components within autonomous driving \u2026"}, {"title": "UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces", "link": "https://arxiv.org/pdf/2503.06157", "details": "B Zhao, J Fang, Z Dai, Z Wang, J Zha, W Zhang, C Gao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large multimodal models exhibit remarkable intelligence, yet their embodied cognitive abilities during motion in open-ended urban 3D space remain to be explored. We introduce a benchmark to evaluate whether video-large language \u2026"}, {"title": "EfficientLLaVA: Generalizable Auto-Pruning for Large Vision-language Models", "link": "https://arxiv.org/pdf/2503.15369", "details": "Y Liang, Z Wang, X Xu, J Zhou, J Lu - arXiv preprint arXiv:2503.15369, 2025", "abstract": "While multimodal large language models demonstrate strong performance in complex reasoning tasks, they pose significant challenges related to model complexity during deployment, especially for resource-limited devices. In this paper \u2026"}, {"title": "O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models", "link": "https://arxiv.org/pdf/2503.12096", "details": "A Sharifdeen, MA Munir, S Baliah, S Khan, MH Khan - arXiv preprint arXiv \u2026, 2025", "abstract": "Test-time prompt tuning for vision-language models (VLMs) is getting attention because of their ability to learn with unlabeled data without fine-tuning. Although test- time prompt tuning methods for VLMs can boost accuracy, the resulting models tend \u2026"}, {"title": "From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2503.06260", "details": "M Dai, J Sun, Z Zhao, S Liu, R Li, J Gao, X Li - arXiv preprint arXiv:2503.06260, 2025", "abstract": "Aligning large vision-language models (LVLMs) with human preferences is challenging due to the scarcity of fine-grained, high-quality, and multimodal preference data without human annotations. Existing methods relying on direct \u2026"}, {"title": "Fine-tuning Vision Language Models with Graph-based Knowledge for Explainable Medical Image Analysis", "link": "https://arxiv.org/pdf/2503.09808", "details": "C Li, L Lux, AH Berger, MJ Menten, MR Sabuncu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Accurate staging of Diabetic Retinopathy (DR) is essential for guiding timely interventions and preventing vision loss. However, current staging models are hardly interpretable, and most public datasets contain no clinical reasoning or interpretation \u2026"}, {"title": "Question Answering Method Leveraging Large Language Models", "link": "https://books.google.co.kr/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DG1tNEQAAQBAJ%26oi%3Dfnd%26pg%3DPA409%26ots%3DMxzqA0Hz55%26sig%3Dc1_1xGvApJgAqLoBMszEFe06dp0", "details": "J Chen, S He, J Zhao, K Liu - China Conference on Knowledge Graph and Semantic \u2026", "abstract": "Recent advancements in large language models (LLMs) have been significantly propelled by pre-trained language models, enabling them to excel in various natural language processing tasks, includ-ing complex reasoning and mathematical \u2026"}]
