[{"title": "Optimizing Language Model's Reasoning Abilities with Weak Supervision", "link": "https://arxiv.org/pdf/2405.04086", "details": "Y Tong, S Wang, D Li, Y Wang, S Han, Z Lin, C Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations \u2026"}, {"title": "Enhancing Large Vision Language Models with Self-Training on Image Comprehension", "link": "https://arxiv.org/pdf/2405.19716", "details": "Y Deng, P Lu, F Yin, Z Hu, S Shen, J Zou, KW Chang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large vision language models (LVLMs) integrate large language models (LLMs) with pre-trained vision encoders, thereby activating the perception capability of the model to understand image inputs for different queries and conduct subsequent reasoning \u2026"}, {"title": "Argumentative Large Language Models for Explainable and Contestable Decision-Making", "link": "https://arxiv.org/pdf/2405.02079", "details": "G Freedman, A Dejl, D Gorur, X Yin, A Rago, F Toni - arXiv preprint arXiv:2405.02079, 2024", "abstract": "The diversity of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them a promising candidate for use in decision-making. However, they are currently limited \u2026"}, {"title": "Increasing the LLM Accuracy for Question Answering: Ontologies to the Rescue!", "link": "https://arxiv.org/pdf/2405.11706", "details": "D Allemang, J Sequeda - arXiv preprint arXiv:2405.11706, 2024", "abstract": "There is increasing evidence that question-answering (QA) systems with Large Language Models (LLMs), which employ a knowledge graph/semantic representation of an enterprise SQL database (ie Text-to-SPARQL), achieve higher \u2026"}, {"title": "Using Contrastive Learning with Generative Similarity to Learn Spaces that Capture Human Inductive Biases", "link": "https://arxiv.org/pdf/2405.19420", "details": "R Marjieh, S Kumar, D Campbell, L Zhang, G Bencomo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Humans rely on strong inductive biases to learn from few examples and abstract useful information from sensory data. Instilling such biases in machine learning models has been shown to improve their performance on various benchmarks \u2026"}, {"title": "Unlocking the Power of Spatial and Temporal Information in Medical Multimodal Pre-training", "link": "https://arxiv.org/pdf/2405.19654", "details": "J Yang, B Su, WX Zhao, JR Wen - arXiv preprint arXiv:2405.19654, 2024", "abstract": "Medical vision-language pre-training methods mainly leverage the correspondence between paired medical images and radiological reports. Although multi-view spatial images and temporal sequences of image-report pairs are available in off-the-shelf \u2026"}, {"title": "TAeKD: Teacher Assistant Enhanced Knowledge Distillation for Closed-Source Multilingual Neural Machine Translation", "link": "https://aclanthology.org/2024.lrec-main.1350.pdf", "details": "B Lv, X Liu, K Wei, P Luo, Y Yu - Proceedings of the 2024 Joint International \u2026, 2024", "abstract": "Abstract Knowledge Distillation (KD) serves as an efficient method for transferring language knowledge from open-source large language models (LLMs) to more computationally efficient models. However, challenges arise when attempting to \u2026"}, {"title": "Interpretable Multi-task Learning with Shared Variable Embeddings", "link": "https://arxiv.org/pdf/2405.06330", "details": "M \u017belaszczyk, J Ma\u0144dziuk - arXiv preprint arXiv:2405.06330, 2024", "abstract": "This paper proposes a general interpretable predictive system with shared information. The system is able to perform predictions in a multi-task setting where distinct tasks are not bound to have the same input/output structure. Embeddings of \u2026"}, {"title": "Exploring and Mitigating Shortcut Learning for Generative Large Language Models", "link": "https://aclanthology.org/2024.lrec-main.602.pdf", "details": "Z Sun, Y Xiao, J Li, Y Ji, W Chen, M Zhang - Proceedings of the 2024 Joint \u2026, 2024", "abstract": "Recent generative large language models (LLMs) have exhibited incredible instruction-following capabilities while keeping strong task completion ability, even without task-specific fine-tuning. Some works attribute this to the bonus of the new \u2026"}]
