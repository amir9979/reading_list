"*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Optimizing Language Model's Reasoning Abilities with W"
[{"title": "LMD3: Language Model Data Density Dependence", "link": "https://arxiv.org/pdf/2405.06331", "details": "J Kirchenbauer, G Honke, G Somepalli, J Geiping\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We develop a methodology for analyzing language model task performance at the individual example level based on training data density estimation. Experiments with paraphrasing as a controlled intervention on finetuning data demonstrate that \u2026"}, {"title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2405.02659", "details": "T Zhang, D Li, Q Chen, C Wang, L Huang, H Xue, X He\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Retrieval-augmented large language models (LLMs) leverage relevant content retrieved by information retrieval systems to generate correct responses, aiming to alleviate the hallucination problem. However, existing retriever-responder methods \u2026"}, {"title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation", "link": "https://arxiv.org/pdf/2405.06424", "details": "JH Lee, JO Woo, J Seok, P Hassanzadeh, W Jang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Assessing response quality to instructions in language models is vital but challenging due to the complexity of human language across different contexts. This complexity often results in ambiguous or inconsistent interpretations, making \u2026"}, {"title": "SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2405.08317", "details": "R Peri, SM Jayanthi, S Ronanki, A Bhatia, K Mundnich\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Integrated Speech and Large Language Models (SLMs) that can follow speech instructions and generate relevant text responses have gained popularity lately. However, the safety and robustness of these models remains largely unclear. In this \u2026"}, {"title": "NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli", "link": "https://arxiv.org/pdf/2405.02814", "details": "X Wang, C Li, Y Chang, J Wang, Y Wu - arXiv preprint arXiv:2405.02814, 2024", "abstract": "Large Language Models (LLMs) have become integral to a wide spectrum of applications, ranging from traditional computing tasks to advanced artificial intelligence (AI) applications. This widespread adoption has spurred extensive \u2026"}, {"title": "Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense", "link": "https://arxiv.org/pdf/2405.04655", "details": "S Shen, L Logeswaran, M Lee, H Lee, S Poria\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated substantial commonsense understanding through numerous benchmark evaluations. However, their understanding of cultural commonsense remains largely unexamined. In this paper \u2026"}, {"title": "Adversarial Robustness for Visual Grounding of Multimodal Large Language Models", "link": "https://openreview.net/pdf%3Fid%3D2r8n6kNEXN", "details": "K Gao, Y Bai, J Bai, Y Yang, ST Xia - ICLR 2024 Workshop on Reliable and \u2026, 2024", "abstract": "Multi-modal Large Language Models (MLLMs) have recently achieved enhanced performance across various vision-language tasks and unlocked visual grounding capabilities. However, the adversarial threat of visual grounding remains unexplored \u2026"}, {"title": "A Generalize Hardware Debugging Approach for Large Language Models Semi-Syntectic Datasets", "link": "https://www.techrxiv.org/doi/pdf/10.36227/techrxiv.171527592.25632661", "details": "W Fu, S Li, Y Zhao, K Yang, X Zhang, Y Jin, X Guo", "abstract": "Abstract Large Language Models (LLMs) have precipitated emerging trends towards intelligent automation. However, integrating LLMs into the hardware debug domain encounters challenges: the datasets for LLMs for hardware are often plagued by a \u2026"}, {"title": "Can large language models understand uncommon meanings of common words?", "link": "https://arxiv.org/pdf/2405.05741", "details": "J Wu, F Che, X Zheng, S Zhang, R Jin, S Nie, P Shao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) like ChatGPT have shown significant advancements across diverse natural language understanding (NLU) tasks, including intelligent dialogue and autonomous agents. Yet, lacking widely acknowledged testing \u2026"}]
