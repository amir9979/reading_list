[{"title": "Factual Knowledge Assessment of Language Models Using Distractors", "link": "https://aclanthology.org/2025.coling-main.537.pdf", "details": "HA Khodja, F Bechet, Q Brabant, A Nasr, G Lecorv\u00e9 - Proceedings of the 31st \u2026, 2025", "abstract": "Abstract Language models encode extensive factual knowledge within their parameters. The accurate assessment of this knowledge is crucial for understanding and improving these models. In the literature, factual knowledge assessment often \u2026"}, {"title": "When Evolution Strategy Meets Language Models Tuning", "link": "https://aclanthology.org/2025.coling-main.357.pdf", "details": "B Huang, Y Jiang, M Chen, Y Wang, H Chen, W Wang - Proceedings of the 31st \u2026, 2025", "abstract": "Supervised Fine-tuning has been pivotal in training autoregressive language models, yet it introduces exposure bias. To mitigate this, Post Fine-tuning, including on-policy and off-policy methods, has emerged as a solution to enhance models \u2026"}, {"title": "Representation Learning to Advance Multi-institutional Studies with Electronic Health Record Data", "link": "https://arxiv.org/pdf/2502.08547", "details": "D Zhou, H Tong, L Wang, S Liu, X Xiong, Z Gan\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The adoption of EHRs has expanded opportunities to leverage data-driven algorithms in clinical care and research. A major bottleneck in effectively conducting multi-institutional EHR studies is the data heterogeneity across systems with \u2026"}, {"title": "Minerva: A Programmable Memory Test Benchmark for Language Models", "link": "https://arxiv.org/pdf/2502.03358%3F", "details": "M Xia, V Ruehle, S Rajmohan, R Shokri - arXiv preprint arXiv:2502.03358, 2025", "abstract": "How effectively can LLM-based AI assistants utilize their memory (context) to perform various tasks? Traditional data benchmarks, which are often manually crafted, suffer from several limitations: they are static, susceptible to overfitting, difficult to interpret \u2026"}, {"title": "Vision-Language Models for Automated Chest X-ray Interpretation: Leveraging ViT and GPT-2", "link": "https://arxiv.org/pdf/2501.12356%3F", "details": "MR Islam, MZ Hossain, M Ahmed, M Samu, S Sultana - arXiv preprint arXiv \u2026, 2025", "abstract": "Radiology plays a pivotal role in modern medicine due to its non-invasive diagnostic capabilities. However, the manual generation of unstructured medical reports is time consuming and prone to errors. It creates a significant bottleneck in clinical \u2026"}, {"title": "Efficient Few-Shot Continual Learning in Vision-Language Models", "link": "https://arxiv.org/pdf/2502.04098", "details": "A Panos, R Aljundi, DO Reino, RE Turner - arXiv preprint arXiv:2502.04098, 2025", "abstract": "Vision-language models (VLMs) excel in tasks such as visual question answering and image captioning. However, VLMs are often limited by their use of pretrained image encoders, like CLIP, leading to image understanding errors that hinder overall \u2026"}, {"title": "Large Language Model Approach for Zero-Shot Information Extraction and Clustering of Japanese Radiology Reports: Algorithm Development and Validation", "link": "https://cancer.jmir.org/2025/1/e57275/", "details": "Y Yamagishi, Y Nakamura, S Hanaoka, O Abe - JMIR cancer, 2025", "abstract": "Background: The application of natural language processing in medicine has increased significantly, including tasks such as information extraction and classification. Natural language processing plays a crucial role in structuring free \u2026"}, {"title": "Large Language Models Meet Graph Neural Networks for Text-Numeric Graph Reasoning", "link": "https://arxiv.org/pdf/2501.16361", "details": "H Song, J Feng, G Li, M Province, P Payne, Y Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In real-world scientific discovery, human beings always make use of the accumulated prior knowledge with imagination pick select one or a few most promising hypotheses from large and noisy data analysis results. In this study, we introduce a \u2026"}, {"title": "Pre-Training a Graph Recurrent Network for Text Understanding", "link": "https://ieeexplore.ieee.org/abstract/document/10870153/", "details": "Y Wang, L Yang, Z Teng, M Zhou, Y Zhang - IEEE Transactions on Pattern Analysis \u2026, 2025", "abstract": "Transformer-based pre-trained models have gained much advance in recent years, Transformer architecture also becomes one of the most important backbones in natural language processing. Recent works show that the attention mechanism \u2026"}]
