[{"title": "Cost-of-Pass: An Economic Framework for Evaluating Language Models", "link": "https://arxiv.org/pdf/2504.13359", "details": "MH Erol, B El, M Suzgun, M Yuksekgonul, J Zou - arXiv preprint arXiv:2504.13359, 2025", "abstract": "The widespread adoption of AI systems in the economy hinges on their ability to generate economic value that outweighs their inference costs. Evaluating this tradeoff requires metrics that account for both performance and costs. We propose a \u2026"}, {"title": "Learning Adaptive Parallel Reasoning with Language Models", "link": "https://arxiv.org/pdf/2504.15466", "details": "J Pan, X Li, L Lian, C Snell, Y Zhou, A Yala, T Darrell\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Scaling inference-time computation has substantially improved the reasoning capabilities of language models. However, existing methods have significant limitations: serialized chain-of-thought approaches generate overly long outputs \u2026"}, {"title": "$\\pi_ {0.5} $: a Vision-Language-Action Model with Open-World Generalization", "link": "https://arxiv.org/pdf/2504.16054", "details": "P Intelligence, K Black, N Brown, J Darpinian\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In order for robots to be useful, they must perform practically relevant tasks in the real world, outside of the lab. While vision-language-action (VLA) models have demonstrated impressive results for end-to-end robot control, it remains an open \u2026"}, {"title": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning", "link": "https://arxiv.org/pdf/2504.13500", "details": "J Wang, J Jiang, Y Liu, M Zhang, X Cai - arXiv preprint arXiv:2504.13500, 2025", "abstract": "In this paper, we introduce a new\\emph {process prejudge} strategy in LLM reasoning to demonstrate that bootstrapping with process prejudge allows the LLM to adaptively anticipate the errors encountered when advancing the subsequent \u2026"}, {"title": "A Dual-Space Framework for General Knowledge Distillation of Large Language Models", "link": "https://arxiv.org/pdf/2504.11426", "details": "X Zhang, S Zhang, Y Liang, F Meng, Y Chen, J Xu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Knowledge distillation (KD) is a promising solution to compress large language models (LLMs) by transferring their knowledge to smaller models. During this process, white-box KD methods usually minimize the distance between the output \u2026"}, {"title": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning", "link": "https://arxiv.org/pdf/2504.13032", "details": "Z Wang, SX Teo, JJ Chew, W Shi - arXiv preprint arXiv:2504.13032, 2025", "abstract": "Recent advancements in large language models (LLMs) have enabled their use as agents for planning complex tasks. Existing methods typically rely on a thought- action-observation (TAO) process to enhance LLM performance, but these \u2026"}, {"title": "Teaching Large Language Models to Reason through Learning and Forgetting", "link": "https://arxiv.org/pdf/2504.11364", "details": "T Ni, A Nie, S Chaudhary, Y Liu, H Rangwala, R Fakoor - arXiv preprint arXiv \u2026, 2025", "abstract": "Leveraging inference-time search in large language models has proven effective in further enhancing a trained model's capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational \u2026"}, {"title": "Explore What LLM Does Not Know in Complex Question Answering", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/34638/36793", "details": "X Lin, Z Huang, Z Zhang, J Zhou, E Chen - Proceedings of the AAAI Conference on \u2026, 2025", "abstract": "Complex question answering (QA) is a challenging task in artificial intelligence research which requires reasoning based on related knowledge. The retrieval- augmented generation (RAG) based on large language models (LLMs) have \u2026"}, {"title": "Exploring Multimodal Prompt for Visualization Authoring with Large Language Models", "link": "https://arxiv.org/pdf/2504.13700", "details": "Z Wen, L Weng, Y Tang, R Zhang, Y Liu, B Pan, M Zhu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advances in large language models (LLMs) have shown great potential in automating the process of visualization authoring through simple natural language utterances. However, instructing LLMs using natural language is limited in precision \u2026"}]
