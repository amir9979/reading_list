[{"title": "Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information", "link": "https://arxiv.org/pdf/2505.23558", "details": "X Chu, X Chen, G Wang, Z Tan, K Huang, W Lv, T Mo\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Inference time scaling drives extended reasoning to enhance the performance of Vision-Language Models (VLMs), thus forming powerful Vision-Language Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens, causing \u2026", "entry_id": "http://arxiv.org/abs/2505.23558v2", "updated": "2025-05-30 07:37:34", "published": "2025-05-29 15:34:15", "authors": "Xu Chu;Xinrong Chen;Guanyu Wang;Zhijie Tan;Kui Huang;Wenyu Lv;Tong Mo;Weiping Li", "summary": "Inference time scaling drives extended reasoning to enhance the performance\nof Vision-Language Models (VLMs), thus forming powerful Vision-Language\nReasoning Models (VLRMs). However, long reasoning dilutes visual tokens,\ncausing visual information to receive less attention and may trigger\nhallucinations. Although introducing text-only reflection processes shows\npromise in language models, we demonstrate that it is insufficient to suppress\nhallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain\n(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a\nvision-text reflection process that guides the model to re-attention visual\ninformation during reasoning. We first propose a reinforcement learning method\nBalanced Reflective Policy Optimization (BRPO), which guides the model to\ndecide when to generate vision-text reflection on its own and balance the\nnumber and length of reflections. Then, we formally prove that VLRMs lose\nattention to visual tokens as reasoning progresses, and demonstrate that\nsupplementing visual information during reflection enhances visual attention.\nTherefore, during training and inference, Visual Token COPY and Visual Token\nROUTE are introduced to force the model to re-attention visual information at\nthe visual level, addressing the limitations of text-only reflection.\nExperiments on multiple visual QA datasets and hallucination metrics indicate\nthat Qwen-LA achieves leading accuracy performance while reducing\nhallucinations. Our code is available at: https://github.com/Liar406/Look_Again", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.LG", "links": "http://arxiv.org/abs/2505.23558v2;http://arxiv.org/pdf/2505.23558v2", "pdf_url": "http://arxiv.org/pdf/2505.23558v2"}, {"title": "Rethinking Causal Mask Attention for Vision-Language Inference", "link": "https://arxiv.org/pdf/2505.18605", "details": "X Pei, T Huang, YX Ma, C Xu - arXiv preprint arXiv:2505.18605, 2025", "abstract": "Causal attention has become a foundational mechanism in autoregressive vision- language models (VLMs), unifying textual and visual inputs under a single generative framework. However, existing causal mask-based strategies are inherited \u2026", "entry_id": "http://arxiv.org/abs/2505.18605v1", "updated": "2025-05-24 08:59:28", "published": "2025-05-24 08:59:28", "authors": "Xiaohuan Pei;Tao Huang;YanXiang Ma;Chang Xu", "summary": "Causal attention has become a foundational mechanism in autoregressive\nvision-language models (VLMs), unifying textual and visual inputs under a\nsingle generative framework. However, existing causal mask-based strategies are\ninherited from large language models (LLMs) where they are tailored for\ntext-only decoding, and their adaptation to vision tokens is insufficiently\naddressed in the prefill stage. Strictly masking future positions for vision\nqueries introduces overly rigid constraints, which hinder the model's ability\nto leverage future context that often contains essential semantic cues for\naccurate inference. In this work, we empirically investigate how different\ncausal masking strategies affect vision-language inference and then propose a\nfamily of future-aware attentions tailored for this setting. We first\nempirically analyze the effect of previewing future tokens for vision queries\nand demonstrate that rigid masking undermines the model's capacity to capture\nuseful contextual semantic representations. Based on these findings, we propose\na lightweight attention family that aggregates future visual context into past\nrepresentations via pooling, effectively preserving the autoregressive\nstructure while enhancing cross-token dependencies. We evaluate a range of\ncausal masks across diverse vision-language inference settings and show that\nselectively compressing future semantic context into past representations\nbenefits the inference.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.AI", "links": "http://arxiv.org/abs/2505.18605v1;http://arxiv.org/pdf/2505.18605v1", "pdf_url": "http://arxiv.org/pdf/2505.18605v1"}, {"title": "Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models", "link": "https://arxiv.org/pdf/2505.21200", "details": "X Tan, Y Yang, P Ye, J Zheng, B Bai, X Wang, J Hao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm for general-purpose robot control through natural language instructions. However, their high inference cost-stemming from large-scale token computation and \u2026", "entry_id": "http://arxiv.org/abs/2505.21200v1", "updated": "2025-05-27 13:47:18", "published": "2025-05-27 13:47:18", "authors": "Xudong Tan;Yaoxin Yang;Peng Ye;Jialin Zheng;Bizhe Bai;Xinyi Wang;Jia Hao;Tao Chen", "summary": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm for\ngeneral-purpose robot control through natural language instructions. However,\ntheir high inference cost-stemming from large-scale token computation and\nautoregressive decoding-poses significant challenges for real-time deployment\nand edge applications. While prior work has primarily focused on architectural\noptimization, we take a different perspective by identifying a dual form of\nredundancy in VLA models: (i) high similarity across consecutive action steps,\nand (ii) substantial redundancy in visual tokens. Motivated by these\nobservations, we propose FlashVLA, the first training-free and plug-and-play\nacceleration framework that enables action reuse in VLA models. FlashVLA\nimproves inference efficiency through a token-aware action reuse mechanism that\navoids redundant decoding across stable action steps, and an information-guided\nvisual token selection strategy that prunes low-contribution tokens. Extensive\nexperiments on the LIBERO benchmark show that FlashVLA reduces FLOPs by 55.7%\nand latency by 36.0%, with only a 0.7% drop in task success rate. These results\ndemonstrate the effectiveness of FlashVLA in enabling lightweight, low-latency\nVLA inference without retraining.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.21200v1;http://arxiv.org/pdf/2505.21200v1", "pdf_url": "http://arxiv.org/pdf/2505.21200v1"}, {"title": "Self-Training Large Language Models with Confident Reasoning", "link": "https://arxiv.org/pdf/2505.17454", "details": "H Jang, Y Jang, S Lee, J Ok, S Ahn - arXiv preprint arXiv:2505.17454, 2025", "abstract": "Large language models (LLMs) have shown impressive performance by generating reasoning paths before final answers, but learning such a reasoning path requires costly human supervision. To address this issue, recent studies have explored self \u2026", "entry_id": "http://arxiv.org/abs/2505.17454v1", "updated": "2025-05-23 04:25:10", "published": "2025-05-23 04:25:10", "authors": "Hyosoon Jang;Yunhui Jang;Sungjae Lee;Jungseul Ok;Sungsoo Ahn", "summary": "Large language models (LLMs) have shown impressive performance by generating\nreasoning paths before final answers, but learning such a reasoning path\nrequires costly human supervision. To address this issue, recent studies have\nexplored self-training methods that improve reasoning capabilities using\npseudo-labels generated by the LLMs themselves. Among these, confidence-based\nself-training fine-tunes LLMs to prefer reasoning paths with high-confidence\nanswers, where confidence is estimated via majority voting. However, such\nmethods exclusively focus on the quality of the final answer and may ignore the\nquality of the reasoning paths, as even an incorrect reasoning path leads to a\ncorrect answer by chance. Instead, we advocate the use of reasoning-level\nconfidence to identify high-quality reasoning paths for self-training,\nsupported by our empirical observations. We then propose a new self-training\nmethod, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths\nthrough Policy Optimization. Our experiments show that CORE-PO improves the\naccuracy of outputs on four in-distribution and two out-of-distribution\nbenchmarks, compared to existing self-training methods.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.CL", "links": "http://arxiv.org/abs/2505.17454v1;http://arxiv.org/pdf/2505.17454v1", "pdf_url": "http://arxiv.org/pdf/2505.17454v1"}, {"title": "Online Knowledge Distillation with Reward Guidance", "link": "https://arxiv.org/pdf/2505.18952", "details": "C Jia - arXiv preprint arXiv:2505.18952, 2025", "abstract": "This work studies knowledge distillation (KD) for large language models (LLMs) through preference optimization. We propose a reward-guided imitation learning framework for sequential KD, formulating a min-max optimization problem between \u2026", "entry_id": "http://arxiv.org/abs/2505.18952v1", "updated": "2025-05-25 02:56:18", "published": "2025-05-25 02:56:18", "authors": "Chen Jia", "summary": "This work studies knowledge distillation (KD) for large language models\n(LLMs) through preference optimization. We propose a reward-guided imitation\nlearning framework for sequential KD, formulating a min-max optimization\nproblem between the policy and reward model (RM) to minimize the performance\ngap between the student and teacher policies. Specifically, the reward\noptimization is constrained to achieve near-optimality within a confidence set\nfor preference alignment. For preference data construction, we explore both\noffline and online preference-based KD. Additionally, we reformulate the RM\nusing the $Q$-value function and extend the framework to white-box KD, where\nthe teacher policy's predicted probabilities are accessible. Theoretical\nanalysis and empirical results demonstrate the effectiveness of the proposed\nframework.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG", "links": "http://arxiv.org/abs/2505.18952v1;http://arxiv.org/pdf/2505.18952v1", "pdf_url": "http://arxiv.org/pdf/2505.18952v1"}, {"title": "AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding", "link": "https://arxiv.org/pdf/2505.20862", "details": "C Jung, Y Jang, JS Chung - arXiv preprint arXiv:2505.20862, 2025", "abstract": "Hallucination remains a major challenge in multimodal large language models (MLLMs). To address this, various contrastive decoding (CD) methods have been proposed that contrasts original logits with hallucinated logits generated from \u2026", "entry_id": "http://arxiv.org/abs/2505.20862v1", "updated": "2025-05-27 08:13:57", "published": "2025-05-27 08:13:57", "authors": "Chaeyoung Jung;Youngjoon Jang;Joon Son Chung", "summary": "Hallucination remains a major challenge in multimodal large language models\n(MLLMs). To address this, various contrastive decoding (CD) methods have been\nproposed that contrasts original logits with hallucinated logits generated from\nperturbed inputs. While CD has shown promise in vision-language models (VLMs),\nit is not well-suited for AV-LLMs, where hallucinations often emerge from both\nunimodal and cross-modal combinations involving audio, video, and language.\nThese intricate interactions call for a more adaptive and modality-aware\ndecoding strategy. In this paper, we propose Audio-Visual Contrastive Decoding\n(AVCD)-a novel, training-free decoding framework designed to model trimodal\ninteractions and suppress modality-induced hallucinations in AV-LLMs. Unlike\nprevious CD methods in VLMs that corrupt a fixed modality, AVCD leverages\nattention distributions to dynamically identify less dominant modalities and\napplies attentive masking to generate perturbed output logits. To support CD in\na trimodal setting, we also reformulate the original CD framework to jointly\nhandle audio, visual, and textual inputs. Finally, to improve efficiency, we\nintroduce entropy-guided adaptive decoding, which selectively skips unnecessary\ndecoding steps based on the model's confidence in its predictions. Extensive\nexperiments demonstrate that AVCD consistently outperforms existing decoding\nmethods. Especially, on the AVHBench dataset, it improves accuracy by 6% for\nVideoLLaMA2 and 11% for video-SALMONN, demonstrating strong robustness and\ngeneralizability.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.20862v1;http://arxiv.org/pdf/2505.20862v1", "pdf_url": "http://arxiv.org/pdf/2505.20862v1"}, {"title": "Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models", "link": "https://arxiv.org/pdf/2505.17015", "details": "R Xu, W Wang, H Tang, X Chen, X Wang, FJ Chu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Multi-modal large language models (MLLMs) have rapidly advanced in visual tasks, yet their spatial understanding remains limited to single images, leaving them ill- suited for robotics and other real-world applications that require multi-frame \u2026", "entry_id": "http://arxiv.org/abs/2505.17015v1", "updated": "2025-05-22 17:59:39", "published": "2025-05-22 17:59:39", "authors": "Runsen Xu;Weiyao Wang;Hao Tang;Xingyu Chen;Xiaodong Wang;Fu-Jen Chu;Dahua Lin;Matt Feiszli;Kevin J. Liang", "summary": "Multi-modal large language models (MLLMs) have rapidly advanced in visual\ntasks, yet their spatial understanding remains limited to single images,\nleaving them ill-suited for robotics and other real-world applications that\nrequire multi-frame reasoning. In this paper, we propose a framework to equip\nMLLMs with robust multi-frame spatial understanding by integrating depth\nperception, visual correspondence, and dynamic perception. Central to our\napproach is the MultiSPA dataset, a novel, large-scale collection of more than\n27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we\nintroduce a comprehensive benchmark that tests a wide spectrum of spatial tasks\nunder uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves\nsignificant gains over baselines and proprietary systems, demonstrating\nscalable, generalizable multi-frame reasoning. We further observe multi-task\nbenefits and early indications of emergent capabilities in challenging\nscenarios, and showcase how our model can serve as a multi-frame reward\nannotator for robotics.", "comment": "24 pages. An MLLM, dataset, and benchmark for multi-frame spatial\n  understanding. Project page: https://runsenxu.com/projects/Multi-SpatialMLLM", "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.CL", "links": "http://arxiv.org/abs/2505.17015v1;http://arxiv.org/pdf/2505.17015v1", "pdf_url": "http://arxiv.org/pdf/2505.17015v1"}, {"title": "Knowledge Grafting of Large Language Models", "link": "https://arxiv.org/pdf/2505.18502", "details": "G Du, X Zhou, J Li, Z Li, Z Shi, W Lin, HK Tang, X Li\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Cross-capability transfer is a key challenge in large language model (LLM) research, with applications in multi-task integration, model compression, and continual learning. Recent works like FuseLLM and FuseChat have demonstrated the potential \u2026", "entry_id": "http://arxiv.org/abs/2505.18502v1", "updated": "2025-05-24 04:43:24", "published": "2025-05-24 04:43:24", "authors": "Guodong Du;Xuanning Zhou;Junlin Li;Zhuo Li;Zesheng Shi;Wanyu Lin;Ho-Kin Tang;Xiucheng Li;Fangming Liu;Wenya Wang;Min Zhang;Jing Li", "summary": "Cross-capability transfer is a key challenge in large language model (LLM)\nresearch, with applications in multi-task integration, model compression, and\ncontinual learning. Recent works like FuseLLM and FuseChat have demonstrated\nthe potential of transferring multiple model capabilities to lightweight\nmodels, enhancing adaptability and efficiency, which motivates our\ninvestigation into more efficient cross-capability transfer methods. However,\nexisting approaches primarily focus on small, homogeneous models, limiting\ntheir applicability. For large, heterogeneous models, knowledge distillation\nwith full-parameter fine-tuning often overlooks the student model's intrinsic\ncapacity and risks catastrophic forgetting, while PEFT methods struggle to\neffectively absorb knowledge from source LLMs. To address these issues, we\nintroduce GraftLLM, a novel method that stores source model capabilities in a\ntarget model with SkillPack format. This approach preserves general\ncapabilities, reduces parameter conflicts, and supports forget-free continual\nlearning and model fusion. We employ a module-aware adaptive compression\nstrategy to compress parameter updates, ensuring efficient storage while\nmaintaining task-specific knowledge. The resulting SkillPack serves as a\ncompact and transferable knowledge carrier, ideal for heterogeneous model\nfusion and continual learning. Experiments across various scenarios demonstrate\nthat GraftLLM outperforms existing techniques in knowledge transfer, knowledge\nfusion, and forget-free learning, providing a scalable and efficient solution\nfor cross-capability transfer. The code is publicly available at:\nhttps://github.com/duguodong7/GraftLLM.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL;cs.LG", "links": "http://arxiv.org/abs/2505.18502v1;http://arxiv.org/pdf/2505.18502v1", "pdf_url": "http://arxiv.org/pdf/2505.18502v1"}, {"title": "SAMA: Towards Multi-Turn Referential Grounded Video Chat with Large Language Models", "link": "https://arxiv.org/pdf/2505.18812", "details": "Y Sun, H Zhang, H Ding, T Zhang, X Ma, YG Jiang - arXiv preprint arXiv:2505.18812, 2025", "abstract": "Achieving fine-grained spatio-temporal understanding in videos remains a major challenge for current Video Large Multimodal Models (Video LMMs). Addressing this challenge requires mastering two core capabilities: video referring understanding \u2026", "entry_id": "http://arxiv.org/abs/2505.18812v1", "updated": "2025-05-24 18:13:16", "published": "2025-05-24 18:13:16", "authors": "Ye Sun;Hao Zhang;Henghui Ding;Tiehua Zhang;Xingjun Ma;Yu-Gang Jiang", "summary": "Achieving fine-grained spatio-temporal understanding in videos remains a\nmajor challenge for current Video Large Multimodal Models (Video LMMs).\nAddressing this challenge requires mastering two core capabilities: video\nreferring understanding, which captures the semantics of video regions, and\nvideo grounding, which segments object regions based on natural language\ndescriptions. However, most existing approaches tackle these tasks in\nisolation, limiting progress toward unified, referentially grounded video\ninteraction. We identify a key bottleneck in the lack of high-quality, unified\nvideo instruction data and a comprehensive benchmark for evaluating\nreferentially grounded video chat. To address these challenges, we contribute\nin three core aspects: dataset, model, and benchmark. First, we introduce\nSAMA-239K, a large-scale dataset comprising 15K videos specifically curated to\nenable joint learning of video referring understanding, grounding, and\nmulti-turn video chat. Second, we propose the SAMA model, which incorporates a\nversatile spatio-temporal context aggregator and a Segment Anything Model to\njointly enhance fine-grained video comprehension and precise grounding\ncapabilities. Finally, we establish SAMA-Bench, a meticulously designed\nbenchmark consisting of 5,067 questions from 522 videos, to comprehensively\nevaluate the integrated capabilities of Video LMMs in multi-turn,\nspatio-temporal referring understanding and grounded dialogue. Extensive\nexperiments and benchmarking results show that SAMA not only achieves strong\nperformance on SAMA-Bench but also sets a new state-of-the-art on general\ngrounding benchmarks, while maintaining highly competitive performance on\nstandard visual understanding benchmarks.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2505.18812v1;http://arxiv.org/pdf/2505.18812v1", "pdf_url": "http://arxiv.org/pdf/2505.18812v1"}]
