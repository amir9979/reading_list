[{"title": "SuperBPE: Space Travel for Language Models", "link": "https://arxiv.org/pdf/2503.13423", "details": "A Liu, J Hayase, V Hofmann, S Oh, NA Smith, Y Choi - arXiv preprint arXiv \u2026, 2025", "abstract": "The assumption across nearly all language model (LM) tokenization schemes is that tokens should be subwords, ie, contained within word boundaries. While providing a seemingly reasonable inductive bias, is this common practice limiting the potential of \u2026"}, {"title": "Process-based self-rewarding language models", "link": "https://arxiv.org/pdf/2503.03746", "details": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' \u2026"}, {"title": "Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge", "link": "https://arxiv.org/pdf/2503.04036", "details": "X Cui, JTZ Wei, S Swayamdipta, R Jia - arXiv preprint arXiv:2503.04036, 2025", "abstract": "Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques \u2026"}, {"title": "Auditing language models for hidden objectives", "link": "https://arxiv.org/pdf/2503.10965", "details": "S Marks, J Treutlein, T Bricken, J Lindsey, J Marcus\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "We study the feasibility of conducting alignment audits: investigations into whether models have undesired objectives. As a testbed, we train a language model with a hidden objective. Our training pipeline first teaches the model about exploitable \u2026"}, {"title": "Sample-aware Adaptive Structured Pruning for Large Language Models", "link": "https://arxiv.org/pdf/2503.06184", "details": "J Kong, X Ma, J Wang, X Zhang - arXiv preprint arXiv:2503.06184, 2025", "abstract": "Large language models (LLMs) have achieved outstanding performance in natural language processing, but enormous model sizes and high computational costs limit their practical deployment. Structured pruning can effectively reduce the resource \u2026"}, {"title": "Rule-Guided Feedback: Enhancing Reasoning by Enforcing Rule Adherence in Large Language Models", "link": "https://arxiv.org/pdf/2503.11336", "details": "A Diallo, A Bikakis, L Dickens, A Hunter, R Miller - arXiv preprint arXiv:2503.11336, 2025", "abstract": "In this paper, we introduce Rule-Guided Feedback (RGF), a framework designed to enhance Large Language Model (LLM) performance through structured rule adherence and strategic information seeking. RGF implements a teacher-student \u2026"}, {"title": "Can Language Models Follow Multiple Turns of Entangled Instructions?", "link": "https://arxiv.org/pdf/2503.13222", "details": "C Han - arXiv preprint arXiv:2503.13222, 2025", "abstract": "Despite significant achievements in improving the instruction-following capabilities of large language models (LLMs), the ability to process multiple potentially entangled or conflicting instructions remains a considerable challenge. Real-world scenarios \u2026"}, {"title": "MMSciBench: Benchmarking Language Models on Multimodal Scientific Problems", "link": "https://arxiv.org/pdf/2503.01891", "details": "X Ye, C Li, S Chen, X Tang, W Wei - arXiv preprint arXiv:2503.01891, 2025", "abstract": "Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present \u2026"}, {"title": "Battling Misinformation: An Empirical Study on Adversarial Factuality in Open-Source Large Language Models", "link": "https://arxiv.org/pdf/2503.10690", "details": "SK Sakib, AB Das, S Ahmed - arXiv preprint arXiv:2503.10690, 2025", "abstract": "Adversarial factuality refers to the deliberate insertion of misinformation into input prompts by an adversary, characterized by varying levels of expressed confidence. In this study, we systematically evaluate the performance of several open-source large \u2026"}]
