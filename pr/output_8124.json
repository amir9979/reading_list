[{"title": "Differentially Private Learning Needs Better Model Initialization and Self-Distillation", "link": "https://arxiv.org/pdf/2410.17566", "details": "IC Ngong, JP Near, N Mireshghallah - arXiv preprint arXiv:2410.17566, 2024", "abstract": "Differentially private SGD (DPSGD) enables privacy-preserving training of language models, but often reduces utility, diversity, and linguistic quality. We introduce DPRefine, a three-phase method that initializes a model using data synthesis from a \u2026"}, {"title": "FairDgcl: Fairness-aware Recommendation with Dynamic Graph Contrastive Learning", "link": "https://arxiv.org/pdf/2410.17555", "details": "W Chen, M Yuan, Z Zhang, R Xie, F Zhuang, D Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As trustworthy AI continues to advance, the fairness issue in recommendations has received increasing attention. A recommender system is considered unfair when it produces unequal outcomes for different user groups based on user-sensitive \u2026"}, {"title": "TAEGAN: Generating Synthetic Tabular Data For Data Augmentation", "link": "https://arxiv.org/pdf/2410.01933%3F", "details": "J Li, Z Zhao, K Yee, U Javaid, B Sikdar - arXiv preprint arXiv:2410.01933, 2024", "abstract": "Synthetic tabular data generation has gained significant attention for its potential in data augmentation, software testing and privacy-preserving data sharing. However, most research has primarily focused on larger datasets and evaluating their quality in \u2026"}, {"title": "ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context Information in Multi-Turn Multimodal Medical Dialogue", "link": "https://arxiv.org/pdf/2409.17610", "details": "Z Li, C Zou, S Ma, Z Yang, C Du, Y Tang, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rocketing prosperity of large language models (LLMs) in recent years has boosted the prevalence of vision-language models (VLMs) in the medical sector. In our online medical consultation scenario, a doctor responds to the texts and images \u2026"}, {"title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE", "link": "https://arxiv.org/pdf/2409.17508", "details": "X Zhu, Y Hu, F Mo, M Li, J Wu - arXiv preprint arXiv:2409.17508, 2024", "abstract": "Multi-modal large language models (MLLMs) have shown impressive capabilities as a general-purpose interface for various visual and linguistic tasks. However, building a unified MLLM for multi-task learning in the medical field remains a thorny \u2026"}, {"title": "State-space models can learn in-context by gradient descent", "link": "https://arxiv.org/pdf/2410.11687%3F", "details": "NM Sushma, Y Tian, H Mestha, N Colombo, D Kappel\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Deep state-space models (Deep SSMs) have shown capabilities for in-context learning on autoregressive tasks, similar to transformers. However, the architectural requirements and mechanisms enabling this in recurrent networks remain unclear \u2026"}, {"title": "Evolutionary Contrastive Distillation for Language Model Alignment", "link": "https://arxiv.org/pdf/2410.07513", "details": "J Katz-Samuels, Z Li, H Yun, P Nigam, Y Xu, V Petricek\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The ability of large language models (LLMs) to execute complex instructions is essential for their real-world applications. However, several recent studies indicate that LLMs struggle with challenging instructions. In this paper, we propose \u2026"}, {"title": "Inference-time language model alignment via integrated value guidance", "link": "https://arxiv.org/pdf/2409.17819%3F", "details": "Z Liu, Z Zhou, Y Wang, C Yang, Y Qiao - arXiv preprint arXiv:2409.17819, 2024", "abstract": "Large language models are typically fine-tuned to align with human preferences, but tuning large models is computationally intensive and complex. In this work, we introduce $\\textit {Integrated Value Guidance} $(IVG), a method that uses implicit and \u2026"}, {"title": "FaithEval: Can Your Language Model Stay Faithful to Context, Even If\" The Moon is Made of Marshmallows\"", "link": "https://arxiv.org/pdf/2410.03727", "details": "Y Ming, S Purushwalkam, S Pandit, Z Ke, XP Nguyen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Ensuring faithfulness to context in large language models (LLMs) and retrieval- augmented generation (RAG) systems is crucial for reliable deployment in real-world applications, as incorrect or unsupported information can erode user trust. Despite \u2026"}]
