'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [How Important is Domain Specificity in Language Models'
[{"title": "Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models", "link": "https://arxiv.org/html/2402.15637v1", "details": "Y Xiang, H Yan, L Gui, Y He - arXiv preprint arXiv:2402.15637, 2024", "abstract": "In-context learning has become a popular paradigm in natural language processing. However, its performance can be significantly influenced by the order of in-context demonstration examples. In this paper, we found that causal language models \u2026"}, {"title": "RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models", "link": "https://arxiv.org/html/2403.02271v1", "details": "S Najafi, A Fyshe - arXiv preprint arXiv:2403.02271, 2024", "abstract": "Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter- efficient fine-tuning methods that optimize input prompts or adjust a small number of \u2026"}, {"title": "Grounding Language Models for Visual Entity Recognition", "link": "https://arxiv.org/pdf/2402.18695", "details": "Z Xiao, M Gong, P Cascante-Bonilla, X Zhang, J Wu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of \u2026"}, {"title": "Ehragent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records", "link": "https://openreview.net/pdf%3Fid%3DZjXEzFE0Qy", "details": "W Shi, R Xu, Y Zhuang, Y Yu, J Zhang, H Wu, Y Zhu\u2026 - ICLR 2024 Workshop on \u2026, 2024", "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent, an LLM agent empowered with \u2026"}, {"title": "\" My Answer is C\": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models", "link": "https://arxiv.org/pdf/2402.14499", "details": "X Wang, B Ma, C Hu, L Weber-Genzel, P R\u00f6ttger\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The \u2026"}, {"title": "Distribution Shifts Are Bottlenecks: Extensive Evaluation for Grounding Language Models to Knowledge Bases", "link": "https://aclanthology.org/2024.eacl-srw.7.pdf", "details": "Y Shu, Z Yu - Proceedings of the 18th Conference of the European \u2026, 2024", "abstract": "Grounding language models (LMs) to knowledge bases (KBs) helps to obtain rich and accurate facts. However, it remains challenging because of the enormous size, complex structure, and partial observability of KBs. One reason is that current \u2026"}, {"title": "Common 7B Language Models Already Possess Strong Math Capabilities", "link": "https://arxiv.org/html/2403.04706v1", "details": "C Li, W Wang, J Hu, Y Wei, N Zheng, H Hu, Z Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models", "link": "https://arxiv.org/pdf/2403.02436", "details": "X Lu, Y Zhao, B Qin - arXiv preprint arXiv:2403.02436, 2024", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "Revisiting Knowledge Distillation for Autoregressive Language Models", "link": "https://arxiv.org/pdf/2402.11890", "details": "Q Zhong, L Ding, L Shen, J Liu, B Du, D Tao - arXiv preprint arXiv:2402.11890, 2024", "abstract": "Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically \u2026"}]
