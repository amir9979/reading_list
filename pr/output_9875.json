[{"title": "DiCoDe: Diffusion-Compressed Deep Tokens for Autoregressive Video Generation with Language Models", "link": "https://arxiv.org/pdf/2412.04446", "details": "Y Li, Y Ge, Y Ge, P Luo, Y Shan - arXiv preprint arXiv:2412.04446, 2024", "abstract": "Videos are inherently temporal sequences by their very nature. In this work, we explore the potential of modeling videos in a chronological and scalable manner with autoregressive (AR) language models, inspired by their success in natural language \u2026"}, {"title": "Self-Generated Critiques Boost Reward Modeling for Language Models", "link": "https://arxiv.org/pdf/2411.16646%3F", "details": "Y Yu, Z Chen, A Zhang, L Tan, C Zhu, RY Pang, Y Qian\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to \u2026"}, {"title": "Evolutionary Pre-Prompt Optimization for Mathematical Reasoning", "link": "https://arxiv.org/pdf/2412.04291", "details": "M Videau, A Leite, M Schoenauer, O Teytaud - arXiv preprint arXiv:2412.04291, 2024", "abstract": "Recent advancements have highlighted that large language models (LLMs), when given a small set of task-specific examples, demonstrate remarkable proficiency, a capability that extends to complex reasoning tasks. In particular, the combination of \u2026"}, {"title": "RedStone: Curating General, Code, Math, and QA Data for Large Language Models", "link": "https://arxiv.org/pdf/2412.03398", "details": "Y Chang, L Cui, L Dong, S Huang, Y Huang, Y Huang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pre-training Large Language Models (LLMs) on high-quality, meticulously curated datasets is widely recognized as critical for enhancing their performance and generalization capabilities. This study explores the untapped potential of Common \u2026"}, {"title": "ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.15268", "details": "J Chen, T Zhang, S Huang, Y Niu, L Zhang, L Wen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Despite the recent breakthroughs achieved by Large Vision Language Models (LVLMs) in understanding and responding to complex visual-textual contexts, their inherent hallucination tendencies limit their practical application in real-world \u2026"}, {"title": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models", "link": "https://arxiv.org/pdf/2412.02830", "details": "H Tran, Z Yao, J Wang, Y Zhang, Z Yang, H Yu - arXiv preprint arXiv:2412.02830, 2024", "abstract": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a versatile extension to the mutual reasoning framework (rStar), aimed at enhancing reasoning accuracy and factual integrity across large language models (LLMs) for \u2026"}, {"title": "Densing Law of LLMs", "link": "https://arxiv.org/pdf/2412.04315", "details": "C Xiao, J Cai, W Zhao, G Zeng, X Han, Z Liu, M Sun - arXiv preprint arXiv:2412.04315, 2024", "abstract": "Large Language Models (LLMs) have emerged as a milestone in artificial intelligence, and their performance can improve as the model size increases. However, this scaling brings great challenges to training and inference efficiency \u2026"}, {"title": "DynRank: Improving Passage Retrieval with Dynamic Zero-Shot Prompting Based on Question Classification", "link": "https://arxiv.org/pdf/2412.00600", "details": "A Abdallah, J Mozafari, B Piryani, MM Abdelgwad\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This paper presents DynRank, a novel framework for enhancing passage retrieval in open-domain question-answering systems through dynamic zero-shot question classification. Traditional approaches rely on static prompts and pre-defined \u2026"}, {"title": "VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information", "link": "https://arxiv.org/pdf/2412.00947", "details": "R Kamoi, Y Zhang, SSS Das, RH Zhang, R Zhang - arXiv preprint arXiv:2412.00947, 2024", "abstract": "Errors in understanding visual information in images (ie, visual perception errors) remain a major source of mistakes in Large Vision Language Models (LVLMs). While further analysis is essential, there is a deficiency in datasets for evaluating the visual \u2026"}]
