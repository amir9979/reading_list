[{"title": "SmartGuard: Leveraging Large Language Models for Network Attack Detection through Audit Log Analysis and Summarization", "link": "https://arxiv.org/pdf/2506.16981", "details": "H Zhang, S Shao, S Li, Z Zhong, Y Liu, Z Qin, K Ren - arXiv preprint arXiv:2506.16981, 2025", "abstract": "End-point monitoring solutions are widely deployed in today's enterprise environments to support advanced attack detection and investigation. These monitors continuously record system-level activities as audit logs and provide deep \u2026", "entry_id": "http://arxiv.org/abs/2506.16981v1", "updated": "2025-06-20 13:19:17", "published": "2025-06-20 13:19:17", "authors": "Hao Zhang;Shuo Shao;Song Li;Zhenyu Zhong;Yan Liu;Zhan Qin;Kui Ren", "summary": "End-point monitoring solutions are widely deployed in today's enterprise\nenvironments to support advanced attack detection and investigation. These\nmonitors continuously record system-level activities as audit logs and provide\ndeep visibility into security events. Unfortunately, existing methods of\nsemantic analysis based on audit logs have low granularity, only reaching the\nsystem call level, making it difficult to effectively classify highly covert\nbehaviors. Additionally, existing works mainly match audit log streams with\nrule knowledge bases describing behaviors, which heavily rely on expertise and\nlack the ability to detect unknown attacks and provide interpretive\ndescriptions. In this paper, we propose SmartGuard, an automated method that\ncombines abstracted behaviors from audit event semantics with large language\nmodels. SmartGuard extracts specific behaviors (function level) from incoming\nsystem logs and constructs a knowledge graph, divides events by threads, and\ncombines event summaries with graph embeddings to achieve information diagnosis\nand provide explanatory narratives through large language models. Our\nevaluation shows that SmartGuard achieves an average F1 score of 96\\% in\nassessing malicious behaviors and demonstrates good scalability across multiple\nmodels and unknown attacks. It also possesses excellent fine-tuning\ncapabilities, allowing experts to assist in timely system updates.", "comment": null, "journal_ref": null, "primary_category": "cs.CR", "categories": "cs.CR", "links": "http://arxiv.org/abs/2506.16981v1;http://arxiv.org/pdf/2506.16981v1", "pdf_url": "http://arxiv.org/pdf/2506.16981v1"}, {"title": "Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models", "link": "https://openreview.net/pdf%3Fid%3D19kqoNoc2N", "details": "Y Li, Z Liu, EP Xing - Forty-second International Conference on Machine \u2026", "abstract": "Optimizing data mixtures for supervised fine-tuning (SFT) of large language models (LLMs) is critical for developing general-purpose models, yet this area remains underexplored. In this paper, we frame data mixing as an optimization problem and \u2026"}, {"title": "How Do Users Identify and Perceive Stereotypes? Understanding User Perspectives on Stereotypical Biases in Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3715275.3732207", "details": "H Lim, D Choi, H Hong - Proceedings of the 2025 ACM Conference on Fairness \u2026, 2025", "abstract": "Warning: This article contains stereotypical and offensive contents. Stereotypical biases in large language models (LLMs) have the potential to result in discriminatory responses, posing harm to users and disrupting interactions. While prior research \u2026"}, {"title": "Uncovering the Linguistic Roots of Bias: Insights and Mitigation in Large Language Models", "link": "https://dl.acm.org/doi/pdf/10.1145/3715275.3732127", "details": "L Benson, A Okutan, R Vasan - Proceedings of the 2025 ACM Conference on \u2026, 2025", "abstract": "The complexity, diversity, and opacity of large language models (LLMs) present significant challenges in ensuring fairness and trustworthiness. While existing research has made strides in creating interpretable and controllable models, there \u2026"}, {"title": "Andromeda: Debugging Database Performance Issues with Retrieval-Augmented Large Language Models", "link": "https://dl.acm.org/doi/abs/10.1145/3722212.3725080", "details": "P Wang, S Chen, J Fan, B Wu, N Tang, J Tan - Companion of the 2025 International \u2026, 2025", "abstract": "Debugging performance issues in a database management system (DBMS) is tedious and challenging, even for experienced database administrators (DBAs). Thus, with the rapid advancement of large language models (LLMs), developing an \u2026"}, {"title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models", "link": "https://arxiv.org/pdf/2506.17585", "details": "Y Huang, S Chen, J Pei, M Zaheer, B Dhingra - arXiv preprint arXiv:2506.17585, 2025", "abstract": "Trustworthy language models should provide both correct and verifiable answers. While language models can sometimes attribute their outputs to pretraining data, their citations are often unreliable due to hallucination. As a result, current systems \u2026", "entry_id": "http://arxiv.org/abs/2506.17585v1", "updated": "2025-06-21 04:48:05", "published": "2025-06-21 04:48:05", "authors": "Yukun Huang;Sanxing Chen;Jian Pei;Manzil Zaheer;Bhuwan Dhingra", "summary": "Trustworthy language models should provide both correct and verifiable\nanswers. While language models can sometimes attribute their outputs to\npretraining data, their citations are often unreliable due to hallucination. As\na result, current systems insert citations by querying an external retriever at\ninference time, introducing latency, infrastructure dependence, and\nvulnerability to retrieval noise. We explore whether LLMs can be made to\nreliably attribute to the documents seen during (continual)\npretraining--without test-time retrieval--by revising the training process. To\nevaluate this, we release CitePretrainBench, a benchmark that mixes real-world\ncorpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and\nprobes both short-form (single fact) and long-form (multi-fact) citation tasks.\nOur approach follows a two-stage process: (1) continual pretraining to bind\nfacts to persistent document identifiers, and (2) instruction tuning to elicit\ncitation behavior. We find that simple Passive Indexing, which appends an\nidentifier to each document, helps memorize verbatim text but fails on\nparaphrased or compositional facts. Instead, we propose Active Indexing, which\ncontinually pretrains on synthetic QA pairs that (1) restate each fact in\ndiverse compositional forms, and (2) require bidirectional source-to-fact and\nfact-to-source generation, jointly teaching the model to generate content from\na cited source and to attribute its own answers. Experiments with Qwen2.5-7B\nand 3B show that Active Indexing consistently outperforms Passive Indexing\nacross all tasks and models, with citation precision gains up to 30.2 percent.\nOur ablation studies reveal that performance continues to improve as we scale\nthe amount of augmented data, showing a clear upward trend even at 16 times the\noriginal token count.", "comment": null, "journal_ref": null, "primary_category": "cs.AI", "categories": "cs.AI;cs.CL;cs.LG", "links": "http://arxiv.org/abs/2506.17585v1;http://arxiv.org/pdf/2506.17585v1", "pdf_url": "http://arxiv.org/pdf/2506.17585v1"}, {"title": "Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation", "link": "https://arxiv.org/pdf/2506.17088", "details": "J Cheng, T Su, J Yuan, G He, J Liu, X Tao, J Xie, H Li - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) often exhibit\\textit {hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of- Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step \u2026", "entry_id": "http://arxiv.org/abs/2506.17088v1", "updated": "2025-06-20 15:49:37", "published": "2025-06-20 15:49:37", "authors": "Jiahao Cheng;Tiancheng Su;Jia Yuan;Guoxiu He;Jiawei Liu;Xinqi Tao;Jingwen Xie;Huaxia Li", "summary": "Large Language Models (LLMs) often exhibit \\textit{hallucinations},\ngenerating factually incorrect or semantically irrelevant content in response\nto prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by\nencouraging step-by-step reasoning, but its impact on hallucination detection\nremains underexplored. To bridge this gap, we conduct a systematic empirical\nevaluation. We begin with a pilot experiment, revealing that CoT reasoning\nsignificantly affects the LLM's internal states and token probability\ndistributions. Building on this, we evaluate the impact of various CoT\nprompting methods on mainstream hallucination detection methods across both\ninstruction-tuned and reasoning-oriented LLMs. Specifically, we examine three\nkey dimensions: changes in hallucination score distributions, variations in\ndetection accuracy, and shifts in detection confidence. Our findings show that\nwhile CoT prompting helps reduce hallucination frequency, it also tends to\nobscure critical signals used for detection, impairing the effectiveness of\nvarious detection methods. Our study highlights an overlooked trade-off in the\nuse of reasoning. Code is publicly available at:\nhttps://anonymous.4open.science/r/cot-hallu-detect.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2506.17088v1;http://arxiv.org/pdf/2506.17088v1", "pdf_url": "http://arxiv.org/pdf/2506.17088v1"}, {"title": "Boosting Retrieval-Augmented Generation with Generation-Augmented Retrieval: A Co-Training Approach", "link": "https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/tang-2025-boosting.pdf", "details": "Y Tang, R Zhang, J Guo, M de Rijke, Y Fan, X Cheng - 2025", "abstract": "Large language models (LLMs) have shown success in knowledgeintensive tasks, including closed-book question answering and entity linking. However, their susceptibility to hallucination undermines their reliability. Retrieval-augmented \u2026"}]
