[{"title": "Bootstrapping Language Models with DPO Implicit Rewards", "link": "https://arxiv.org/pdf/2406.09760", "details": "C Chen, Z Liu, C Du, T Pang, Q Liu, A Sinha\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Human alignment in large language models (LLMs) is an active area of research. A recent groundbreaking work, direct preference optimization (DPO), has greatly simplified the process from past work in reinforcement learning from human feedback \u2026"}, {"title": "Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning", "link": "https://arxiv.org/pdf/2406.12742", "details": "B Zhao, Y Zong, L Zhang, T Hospedales - arXiv preprint arXiv:2406.12742, 2024", "abstract": "The advancement of large language models (LLMs) has significantly broadened the scope of applications in natural language processing, with multi-modal LLMs extending these capabilities to integrate and interpret visual data. However, existing \u2026"}, {"title": "Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts", "link": "https://arxiv.org/pdf/2406.12034", "details": "J Kang, L Karlinsky, H Luo, Z Wang, J Hansen, J Glass\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Self-MoE, an approach that transforms a monolithic LLM into a compositional, modular system of self-specialized experts, named MiXSE (MiXture of Self-specialized Experts). Our approach leverages self-specialization, which \u2026"}, {"title": "Achieving Sparse Activation in Small Language Models", "link": "https://arxiv.org/pdf/2406.06562", "details": "J Song, K Huang, X Yin, B Yang, W Gao - arXiv e-prints, 2024", "abstract": "Sparse activation, which selectively activates only an input-dependent set of neurons in inference, is a useful technique to reduce the computing cost of Large Language Models (LLMs) without retraining or adaptation efforts. However, whether it can be \u2026"}, {"title": "CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models", "link": "https://arxiv.org/pdf/2406.12257", "details": "Y Li, Z Xu, F Jiang, L Niu, D Sahabandu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The remarkable performance of large language models (LLMs) in generation tasks has enabled practitioners to leverage publicly available models to power custom applications, such as chatbots and virtual assistants. However, the data used to train \u2026"}, {"title": "Querying as Prompt: Parameter-Efficient Learning for Multimodal Language Model", "link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Querying_as_Prompt_Parameter-Efficient_Learning_for_Multimodal_Language_Model_CVPR_2024_paper.pdf", "details": "T Liang, J Huang, M Kong, L Chen, Q Zhu - Proceedings of the IEEE/CVF Conference \u2026, 2024", "abstract": "Recent advancements in language models pre-trained on large-scale corpora have significantly propelled developments in the NLP domain and advanced progress in multimodal tasks. In this paper we propose a Parameter-Efficient multimodal \u2026"}, {"title": "Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models", "link": "https://arxiv.org/pdf/2406.12311", "details": "D Jo, T Kim, Y Kim, JJ Kim - arXiv preprint arXiv:2406.12311, 2024", "abstract": "Binarization, which converts weight parameters to binary values, has emerged as an effective strategy to reduce the size of large language models (LLMs). However, typical binarization techniques significantly diminish linguistic effectiveness of LLMs \u2026"}, {"title": "Large Scale Transfer Learning for Tabular Data via Language Modeling", "link": "https://arxiv.org/pdf/2406.12031", "details": "J Gardner, JC Perdomo, L Schmidt - arXiv preprint arXiv:2406.12031, 2024", "abstract": "Tabular data--structured, heterogeneous, spreadsheet-style data with rows and columns--is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and \u2026"}, {"title": "Leveraging Generative Large Language Models with Visual Instruction and Demonstration Retrieval for Multimodal Sarcasm Detection", "link": "https://aclanthology.org/2024.naacl-long.97.pdf", "details": "B Tang, B Lin, H Yan, S Li - Proceedings of the 2024 Conference of the North \u2026, 2024", "abstract": "Multimodal sarcasm detection aims to identify sarcasm in the given image-text pairs and has wide applications in the multimodal domains. Previous works primarily design complex network structures to fuse the image-text modality features for \u2026"}]
