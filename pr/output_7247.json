[{"title": "OLMoE: Open Mixture-of-Experts Language Models", "link": "https://arxiv.org/pdf/2409.02060", "details": "N Muennighoff, L Soldaini, D Groeneveld, K Lo\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to \u2026"}, {"title": "The impact of limited access to digital health records on doctors and their willingness to adopt electronic health record systems", "link": "https://journals.sagepub.com/doi/pdf/10.1177/20552076241281626", "details": "MM Bouh, F Hossain, P Paul, MM Rahman, R Islam\u2026 - Digital health, 2024", "abstract": "Objective Research over the past decade has extensively covered the benefits of electronic health records in developing countries. Yet, the specific impact of their limited access on doctors' workload and clinical decision-making, particularly in \u2026"}, {"title": "Extracting lung cancer staging descriptors from pathology reports: A generative language model approach", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001382", "details": "H Cho, S Yoo, B Kim, S Jang, L Sunwoo, S Kim, D Lee\u2026 - Journal of Biomedical \u2026, 2024", "abstract": "Background In oncology, electronic health records contain textual key information for the diagnosis, staging, and treatment planning of patients with cancer. However, text data processing requires a lot of time and effort, which limits the utilization of these \u2026"}, {"title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks", "link": "https://arxiv.org/pdf/2409.06173", "details": "G Chochlakis, NM Pandiyan, K Lerman, S Narayanan - arXiv preprint arXiv \u2026, 2024", "abstract": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the dominant technique for performing natural language tasks, as it does not require updating the model parameters with gradient-based methods. ICL promises to\" \u2026"}, {"title": "On the Relationship between Truth and Political Bias in Language Models", "link": "https://arxiv.org/pdf/2409.05283", "details": "S Fulay, W Brannon, S Mohanty, C Overney\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Language model alignment research often attempts to ensure that models are not only helpful and harmless, but also truthful and unbiased. However, optimizing these objectives simultaneously can obscure how improving one aspect might impact the \u2026"}, {"title": "How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?", "link": "https://arxiv.org/pdf/2409.02253", "details": "SA Taghanaki, J Lambourne, A Mongkhounsavath - arXiv preprint arXiv:2409.02253, 2024", "abstract": "Large foundation models have revolutionized the field, yet challenges remain in optimizing multi-modal models for specialized visual tasks. We propose a novel, generalizable methodology to identify preferred image distributions for black-box \u2026"}, {"title": "Investigating Layer Importance in Large Language Models", "link": "https://arxiv.org/pdf/2409.14381", "details": "Y Zhang, Y Dong, K Kawaguchi - arXiv preprint arXiv:2409.14381, 2024", "abstract": "Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in \u2026"}]
