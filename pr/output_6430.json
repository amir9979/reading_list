[{"title": "Interactive Machine Teaching by Labeling Rules and Instances", "link": "https://arxiv.org/pdf/2409.05199", "details": "G Karamanolakis, D Hsu, L Gravano - arXiv preprint arXiv:2409.05199, 2024", "abstract": "Weakly supervised learning aims to reduce the cost of labeling data by using expert- designed labeling rules. However, existing methods require experts to design effective rules in a single shot, which is difficult in the absence of proper guidance \u2026"}, {"title": "VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images", "link": "https://arxiv.org/pdf/2408.16176", "details": "M Maruf, A Daw, KS Mehrab, HB Manogaran, A Neog\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Images are increasingly becoming the currency for documenting biodiversity on the planet, providing novel opportunities for accelerating scientific discoveries in the field of organismal biology, especially with the advent of large vision-language models \u2026"}, {"title": "Untie the Knots: An Efficient Data Augmentation Strategy for Long-Context Pre-Training in Language Models", "link": "https://arxiv.org/pdf/2409.04774", "details": "J Tian, D Zheng, Y Cheng, R Wang, C Zhang, D Zhang - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLM) have prioritized expanding the context window from which models can incorporate more information. However, training models to handle long contexts presents significant challenges. These include the scarcity of high \u2026"}, {"title": "Non-instructional Fine-tuning: Enabling Instruction-Following Capabilities in Pre-trained Language Models without Instruction-Following Data", "link": "https://arxiv.org/pdf/2409.00096", "details": "J Xie, S Syu, H Lee - arXiv preprint arXiv:2409.00096, 2024", "abstract": "Instruction fine-tuning is crucial for today's large language models (LLMs) to learn to follow instructions and align with human preferences. Conventionally, supervised data, including the instruction and the correct response, is required for instruction fine \u2026"}, {"title": "Language Models as Reasoners for Out-of-Distribution Detection", "link": "https://link.springer.com/chapter/10.1007/978-3-031-68738-9_30", "details": "K Kirchheim, F Ortmeier - \u2026 Conference on Computer Safety, Reliability, and \u2026, 2024", "abstract": "Deep neural networks (DNNs) are prone to making wrong predictions with high confidence for data that does not stem from their training distribution. Consequentially, out-of-distribution (OOD) detection is important in safety-critical \u2026"}, {"title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "link": "https://arxiv.org/pdf/2408.06663", "details": "K Sun, M Dredze - arXiv preprint arXiv:2408.06663, 2024", "abstract": "The development of large language models leads to the formation of a pre-train-then- align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream \u2026"}, {"title": "SSGU-CD: A combined semantic and structural information graph U-shaped network for document-level Chemical-Disease interaction extraction", "link": "https://www.sciencedirect.com/science/article/pii/S1532046424001370", "details": "P Nie, J Ning, M Lin, Z Yang, L Wang - Journal of Biomedical Informatics, 2024", "abstract": "Document-level interaction extraction for Chemical-Disease is aimed at inferring the interaction relations between chemical entities and disease entities across multiple sentences. Compared with sentence-level relation extraction, document-level \u2026"}, {"title": "Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution", "link": "https://arxiv.org/pdf/2408.10548", "details": "Y Ruan, X Lan, J Ma, Y Dong, K He, M Feng - arXiv preprint arXiv:2408.10548, 2024", "abstract": "Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds \u2026"}, {"title": "LoRA $^ 2$: Multi-Scale Low-Rank Approximations for Fine-Tuning Large Language Models", "link": "https://arxiv.org/pdf/2408.06854", "details": "JC Zhang, YJ Xiong, HX Qiu, DH Zhu, CM Xia - arXiv preprint arXiv:2408.06854, 2024", "abstract": "Fine-tuning large language models (LLMs) with high parameter efficiency for downstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA) significantly reduces the number of trainable parameters for fine-tuning. Although it \u2026"}]
