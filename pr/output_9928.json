[{"title": "From language models over tokens to language models over characters", "link": "https://arxiv.org/pdf/2412.03719", "details": "T Vieira, B LeBrun, M Giulianelli, JL Gastaldi, B DuSell\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Modern language models are internally--and mathematically--distributions over token strings rather than\\emph {character} strings, posing numerous challenges for programmers building user applications on top of them. For example, if a prompt is \u2026"}, {"title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models", "link": "https://arxiv.org/pdf/2411.09595", "details": "Z Wang, J Lorraine, Y Wang, H Su, J Zhu, S Fidler\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This work explores expanding the capabilities of large language models (LLMs) pretrained on text to generate 3D meshes within a unified model. This offers key advantages of (1) leveraging spatial knowledge already embedded in LLMs, derived \u2026"}, {"title": "Spider 2.0: Evaluating language models on real-world enterprise text-to-sql workflows", "link": "https://arxiv.org/pdf/2411.07763", "details": "F Lei, J Chen, Y Ye, R Cao, D Shin, H Su, Z Suo, H Gao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Real-world enterprise text-to-SQL workflows often involve complex cloud or local data across various database systems, multiple SQL queries in various dialects, and diverse operations from data transformation to analytics. We introduce Spider 2.0, an \u2026"}, {"title": "Sneaking Syntax into Transformer Language Models with Tree Regularization", "link": "https://arxiv.org/pdf/2411.18885", "details": "A Nandi, CD Manning, S Murty - arXiv preprint arXiv:2411.18885, 2024", "abstract": "While compositional accounts of human language understanding are based on a hierarchical tree-like process, neural models like transformers lack a direct inductive bias for such tree structures. Introducing syntactic inductive biases could unlock more \u2026"}, {"title": "Contrastive concept-phrase pre-training for generating clinically accurate and interpretable chest X-ray reports", "link": "https://link.springer.com/article/10.1007/s00521-024-10640-1", "details": "A Tubaishat, T Zia, D Windridge, M Nawaz, S Razzaq - Neural Computing and \u2026, 2024", "abstract": "Automated radiology report generation is an emerging field for improving patient care and alleviating radiologist workload. However, existing methods face a range of challenges such as limited data availability, clinical metric performance, and \u2026"}, {"title": "Enhancing Vision-Language Model Safety through Progressive Concept-Bottleneck-Driven Alignment", "link": "https://arxiv.org/pdf/2411.11543", "details": "Z Liu, Y Nie, Y Tan, X Yue, Q Cui, C Wang, X Zhu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Benefiting from the powerful capabilities of Large Language Models (LLMs), pre- trained visual encoder models connected to LLMs form Vision Language Models (VLMs). However, recent research shows that the visual modality in VLMs is highly \u2026"}, {"title": "A Benchmark for Long-Form Medical Question Answering", "link": "https://arxiv.org/pdf/2411.09834", "details": "P Hosseini, JM Sin, B Ren, BG Thomas, E Nouri\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "There is a lack of benchmarks for evaluating large language models (LLMs) in long- form medical question answering (QA). Most existing medical QA evaluation benchmarks focus on automatic metrics and multiple-choice questions. While \u2026"}]
