[{"title": "DATETIME: A new benchmark to measure LLM translation and reasoning capabilities", "link": "https://arxiv.org/pdf/2504.16155", "details": "E Gaere, F Wangenheim - arXiv preprint arXiv:2504.16155, 2025", "abstract": "This paper introduces DATETIME, a new high-quality benchmark designed to evaluate the translation and reasoning abilities of a Large Language Model (LLM) on datetimes. A datetime is simply a date and a time, for example'11th. february. 2023 \u2026"}, {"title": "Reinforcement Learning from Human Feedback", "link": "https://arxiv.org/pdf/2504.12501", "details": "N Lambert - arXiv preprint arXiv:2504.12501, 2025", "abstract": "Reinforcement learning from human feedback (RLHF) has become an important technical and storytelling tool to deploy the latest machine learning systems. In this book, we hope to give a gentle introduction to the core methods for people with some \u2026"}, {"title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer", "link": "https://arxiv.org/pdf/2504.10462", "details": "W Lei, J Wang, H Wang, X Li, JH Liew, J Feng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained \u2026"}, {"title": "Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study", "link": "https://arxiv.org/pdf/2504.16414", "details": "M Khodadad, AS Kasmaee, M Astaraki, N Sherck\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a \u2026"}, {"title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language Models", "link": "https://arxiv.org/pdf/2504.14245", "details": "Y Ji, Y Hong, J Zhan, H Chen, H Zhu, W Wang, L Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Progress in image generation raises significant public security concerns. We argue that fake image detection should not operate as a\" black box\". Instead, an ideal approach must ensure both strong generalization and transparency. Recent \u2026"}, {"title": "Establishing Reliability Metrics for Reward Models in Large Language Models", "link": "https://arxiv.org/pdf/2504.14838", "details": "Y Chen, Y Liu, X Wang, Q Yu, G Huzhang, A Zeng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The reward model (RM) that represents human preferences plays a crucial role in optimizing the outputs of large language models (LLMs), eg, through reinforcement learning from human feedback (RLHF) or rejection sampling. However, a long \u2026"}, {"title": "Aligning Anime Video Generation with Human Feedback", "link": "https://arxiv.org/pdf/2504.10044", "details": "B Zhu, Y Jiang, B Xu, S Yang, M Yin, Y Wu, H Sun\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Anime video generation faces significant challenges due to the scarcity of anime data and unusual motion patterns, leading to issues such as motion distortion and flickering artifacts, which result in misalignment with human preferences. Existing \u2026"}, {"title": "How Large Language Models Are Changing MOOC Essay Answers: A Comparison of Pre-and Post-LLM Responses", "link": "https://arxiv.org/pdf/2504.13038", "details": "L Lepp\u00e4nen, L Aunimo, A Hellas, JK Nurminen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The release of ChatGPT in late 2022 caused a flurry of activity and concern in the academic and educational communities. Some see the tool's ability to generate human-like text that passes at least cursory inspections for factual accuracy``often \u2026"}, {"title": "ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data", "link": "https://arxiv.org/pdf/2504.16628", "details": "H Gu, H Wang, Y Mei, M Zhang, Y Jin - arXiv preprint arXiv:2504.16628, 2025", "abstract": "Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm \u2026"}]
