[{"title": "Comparing Commercial and Open-Source Large Language Models for Labeling Chest Radiograph Reports", "link": "https://pubs.rsna.org/doi/abs/10.1148/radiol.241139", "details": "FJ Dorfner, L J\u00fcrgensen, L Donle, F Al Mohamad\u2026 - Radiology, 2024", "abstract": "Background Rapid advances in large language models (LLMs) have led to the development of numerous commercial and open-source models. While recent publications have explored OpenAI's GPT-4 to extract information of interest from \u2026"}, {"title": "Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language Models", "link": "https://openreview.net/pdf%3Fid%3Dnvn80cscVm", "details": "L Wei, Z Tan, C Li, J Wang, W Huang - The Thirty-eighth Annual Conference on \u2026, 2024", "abstract": "Large Language Models (LLMs) have transformed natural language processing and extended their powerful capabilities to multi-modal domains. As LLMs continue to advance, it is crucial to develop diverse and appropriate metrics for their evaluation \u2026"}, {"title": "Towards Better Interpretability of Sepsis Prediction by Deep Neural Networks with Variable-wise Attribution Maps", "link": "https://hal.science/hal-04776927v1/file/paper_submitted_version.pdf", "details": "PE Thiboud, V Wargnier-Dauchelle, M Lefort\u2026 - 2024", "abstract": "Because of the multi-symptomatic nature of sepsis, its prediction is challenging as it requires considering subtle changes in multiple monitored variables across time. Recent works based on deep neural networks improved the prediction performance \u2026"}, {"title": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning", "link": "https://arxiv.org/pdf/2411.05193", "details": "J Hong, A Dragan, S Levine - arXiv preprint arXiv:2411.05193, 2024", "abstract": "Value-based reinforcement learning (RL) can in principle learn effective policies for a wide range of multi-turn problems, from games to dialogue to robotic control, including via offline RL from static previously collected datasets. However, despite \u2026"}, {"title": "Accelerating Blockwise Parallel Language Models with Draft Refinement", "link": "https://openreview.net/pdf%3Fid%3DKT6F5Sw0eg", "details": "T Kim, AT Suresh, KA Papineni, M Riley, S Kumar\u2026 - The Thirty-eighth Annual \u2026", "abstract": "Autoregressive language models have achieved remarkable advancements, yet their potential is often limited by the slow inference speeds associated with sequential token generation. Blockwise parallel decoding (BPD) was proposed by Stern et \u2026"}, {"title": "Investigating the Generalizability of Pretrained Language Models across Multiple Dimensions: A Case Study of NLI and MRC", "link": "https://aclanthology.org/2024.genbench-1.11.pdf", "details": "R Dutt, S Choudhury, V Rao, C Rose, VGV Vydiswaran - Proceedings of the 2nd \u2026, 2024", "abstract": "Generalization refers to the ability of machine learning models to perform well on dataset distributions different from the one it was trained on. While several pre- existing works have characterized the generalizability of NLP models across different \u2026"}, {"title": "BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays", "link": "https://arxiv.org/pdf/2410.21969", "details": "Y Zhou, TLH Faith, Y Xu, S Leng, X Xu, Y Liu, RSM Goh - arXiv preprint arXiv \u2026, 2024", "abstract": "Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream \u2026"}, {"title": "Unsupervised Foundation Model-Agnostic Slide-Level Representation Learning", "link": "https://arxiv.org/pdf/2411.13623", "details": "T Lenz, P Neidlinger, M Ligero, G W\u00f6lflein\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Representation learning of pathology whole-slide images (WSIs) has primarily relied on weak supervision with Multiple Instance Learning (MIL). This approach leads to slide representations highly tailored to a specific clinical task. Self-supervised \u2026"}, {"title": "Multimodal Large Language Models Make Text-to-Image Generative Models Align Better", "link": "https://openreview.net/pdf%3Fid%3DIRXyPm9IPW", "details": "X Wu, S Huang, G Wang, J Xiong, F Wei - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts. Despite these advances \u2026"}]
