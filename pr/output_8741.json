[{"title": "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "link": "https://aclanthology.org/2024.findings-emnlp.61.pdf", "details": "X Wei, H Chen, H Yu, H Fei, Q Liu - Findings of the Association for Computational \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) have achieved notable success in commonsense reasoning tasks, benefiting from their extensive world knowledge acquired through extensive pretraining. While approaches like Chain-of-Thought \u2026"}, {"title": "LM2: A Simple Society of Language Models Solves Complex Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.920.pdf", "details": "G Juneja, S Dutta, T Chakraborty - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Despite demonstrating emergent reasoning abilities, Large Language Models (LLMS) often lose track of complex, multi-step reasoning. Existing studies show that providing guidance via decomposing the original question into multiple subproblems \u2026"}, {"title": "How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers", "link": "https://openreview.net/pdf%3Fid%3D67tRrjgzsh", "details": "X Lu, Y Zhao, B Qin, L Huo, Q Yang, D Xu", "abstract": "Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot \u2026"}, {"title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "link": "https://aclanthology.org/2024.emnlp-main.128.pdf", "details": "Y Wan, W Wang, Y Yang, Y Yuan, J Huang, P He\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT- 4\\. Despite LLMs' prowess in tasks like writing assistance, code generation, and \u2026"}, {"title": "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties", "link": "https://aclanthology.org/2024.emnlp-main.1137.pdf", "details": "K Yu, Z Zhang, F Hu, S Storks, J Chai - Proceedings of the 2024 Conference on \u2026, 2024", "abstract": "A major reason behind the recent success of large language models (LLMs) is their incontext learning capability, which makes it possible to rapidly adapt them to downstream textbased tasks by prompting them with a small number of relevant \u2026"}, {"title": "Improving Referring Ability for Biomedical Language Models", "link": "https://aclanthology.org/2024.findings-emnlp.375.pdf", "details": "J Jiang, F Cheng, A Aizawa - Findings of the Association for Computational \u2026, 2024", "abstract": "Existing auto-regressive large language models (LLMs) are primarily trained using documents from general domains. In the biomedical domain, continual pre-training is a prevalent method for domain adaptation to inject professional knowledge into \u2026"}, {"title": "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "link": "https://aclanthology.org/2024.emnlp-main.313.pdf", "details": "M Xu, Y Li, K Sun, T Qian - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Large language models (LLMs) have shown excellent capability for solving reasoning problems. Existing approaches do not differentiate the question difficulty when designing prompting methods for them. Clearly, a simple method cannot elicit \u2026"}, {"title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models", "link": "https://aclanthology.org/2024.emnlp-main.566.pdf", "details": "XH Feng, C Chen, Y Li, Z Lin - Proceedings of the 2024 Conference on Empirical \u2026, 2024", "abstract": "Pre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for \u2026"}, {"title": "Is GPT-4V (ision) All You Need for Automating Academic Data Visualization? Exploring Vision-Language Models' Capability in Reproducing Academic Charts", "link": "https://aclanthology.org/2024.findings-emnlp.485.pdf", "details": "Z Zhang, W Ma, S Vosoughi - Findings of the Association for Computational \u2026, 2024", "abstract": "While effective data visualization is crucial to present complex information in academic research, its creation demands significant expertise in both data management and graphic design. We explore the potential of using Vision \u2026"}]
