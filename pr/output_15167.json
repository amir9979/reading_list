[{"title": "Enhancing LLMs via High-Knowledge Data Selection", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/34555/36710", "details": "F Duan, X Zhang, S Wang, H Que, Y Liu, W Rong\u2026 - Proceedings of the AAAI \u2026, 2025", "abstract": "Abstract The performance of Large Language Models (LLMs) is intrinsically linked to the quality of its training data. Although several studies have proposed methods for high-quality data selection, they do not consider the importance of knowledge \u2026"}, {"title": "Exploring Conversational Adaptability: Assessing the Proficiency of Large Language Models in Dynamic Alignment with Updated User Intent", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/34534/36689", "details": "YC Chen, HH Huang - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "This paper presents a practical problem in dialogue systems: the capability to adapt to changing user intentions and resolve inconsistencies in conversation histories. It is crucial in scenarios like train ticket booking, where travel plans often change \u2026"}, {"title": "Language Models of Code are Few-Shot Planners and Reasoners for Multi-Document Summarization with Attribution", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/34676/36831", "details": "A Nandy, S Bandyopadhyay - Proceedings of the AAAI Conference on Artificial \u2026, 2025", "abstract": "Document summarization has greatly benefited from advances in large language models (LLMs). In real-world situations, summaries often need to be generated from multiple documents with diverse sources and authors, lacking a clear information \u2026"}, {"title": "Multi-View Empowered Structural Graph Wordification for Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/34652/36807", "details": "Z Liu, L Wu, M He, Z Guan, H Zhao, N Feng - Proceedings of the AAAI Conference on \u2026, 2025", "abstract": "Significant efforts have been dedicated to integrating the powerful Large Language Models (LLMs) with diverse modalities, particularly focusing on the fusion of language, vision and audio data. However, the graph-structured data, which is \u2026"}, {"title": "HLMEA: Unsupervised Entity Alignment Based on Hybrid Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/33294/35449", "details": "X Jin, Z Wang, J Chen, L Yang, B Oh, S Hwang, J Li - Proceedings of the AAAI \u2026, 2025", "abstract": "Entity alignment (EA) is crucial for integrating knowledge graphs (KGs) constructed from diverse sources. Conventional unsupervised EA approaches attempt to eliminate human intervention but often suffer from accuracy limitations. With the rise \u2026"}, {"title": "Large language models are human-like annotators", "link": "https://link.springer.com/chapter/10.1007/978-3-031-88720-8_45", "details": "M Marreddy, SR Oota, M Gupta - European Conference on Information Retrieval, 2025", "abstract": "Large Language Models Are Human-Like Annotators | SpringerLink Skip to main content Advertisement Springer Nature Link Account Menu Find a journal Publish with us Track your research Search Cart 1.Home 2.Advances in Information \u2026"}, {"title": "Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models", "link": "https://arxiv.org/pdf/2503.12072", "details": "A Ravichander, J Fisher, T Sorensen, X Lu, Y Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-quality training data has proven crucial for developing performant large language models (LLMs). However, commercial LLM providers disclose few, if any, details about the data used for training. This lack of transparency creates multiple \u2026"}]
