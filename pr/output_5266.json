[{"title": "Enhancing Text-to-SQL Parsing through Question Rewriting and Execution-Guided Refinement", "link": "https://aclanthology.org/2024.findings-acl.120.pdf", "details": "W Mao, R Wang, J Guo, J Zeng, C Gao, P Han, C Liu - Findings of the Association for \u2026, 2024", "abstract": "Abstract Large Language Model (LLM)-based approach has become the mainstream for Text-to-SQL task and achieves remarkable performance. In this paper, we augment the existing prompt engineering methods by exploiting the database \u2026"}, {"title": "Teaching Small Language Models to Reason for Knowledge-Intensive Multi-Hop Question Answering", "link": "https://aclanthology.org/2024.findings-acl.464.pdf", "details": "X Li, S He, F Lei, JY JunYang, T Su, K Liu, J Zhao - Findings of the Association for \u2026, 2024", "abstract": "Abstract Large Language Models (LLMs) can teach small language models (SLMs) to solve complex reasoning tasks (eg, mathematical question answering) by Chain-of- thought Distillation (CoTD). Specifically, CoTD fine-tunes SLMs by utilizing rationales \u2026"}, {"title": "Language Models Don't Learn the Physical Manifestation of Language", "link": "https://aclanthology.org/2024.acl-long.195.pdf", "details": "B Lee, J Lim - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "We argue that language-only models don't learn the physical manifestation of language. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-Test. These tasks highlight a \u2026"}, {"title": "Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models", "link": "https://aclanthology.org/2024.acl-long.404.pdf", "details": "Q Ai, J Li, J Dai, J Zhou, L Liu, H Jiang, S Shi - \u2026 of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields. In this paper, we propose a new paradigm for interactive and instructional graph data \u2026"}, {"title": "Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination", "link": "https://aclanthology.org/2024.acl-long.490.pdf", "details": "N Yang, T Kang, SJ Choi, H Lee, K Jung - Proceedings of the 62nd Annual Meeting of \u2026, 2024", "abstract": "Instruction-following language models often show undesirable biases. These undesirable biases may be accelerated in the real-world usage of language models, where a wide range of instructions is used through zero-shot example prompting. To \u2026"}, {"title": "Data-Centric Explainable Debiasing for Improving Fairness in Pre-trained Language Models", "link": "https://aclanthology.org/2024.findings-acl.226.pdf", "details": "Y Li, M Du, R Song, X Wang, Y Wang - Findings of the Association for Computational \u2026, 2024", "abstract": "Human-like social bias of pre-trained language models (PLMs) on downstream tasks have attracted increasing attention. The potential flaws in the training data are the main factor that causes unfairness in PLMs. Existing data-centric debiasing \u2026"}, {"title": "Is Table Retrieval a Solved Problem? Exploring Join-Aware Multi-Table Retrieval", "link": "https://aclanthology.org/2024.acl-long.148.pdf", "details": "PB Chen, Y Zhang, D Roth - Proceedings of the 62nd Annual Meeting of the \u2026, 2024", "abstract": "Retrieving relevant tables containing the necessary information to accurately answer a given question over tables is critical to open-domain question-answering (QA) systems. Previous methods assume the answer to such a question can be found \u2026"}, {"title": "MEDFuse: Multimodal EHR Data Fusion with Masked Lab-Test Modeling and Large Language Models", "link": "https://arxiv.org/pdf/2407.12309", "details": "TMN Phan, CT Dao, C Wu, JZ Wang, S Liu, JE Ding\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Electronic health records (EHRs) are multimodal by nature, consisting of structured tabular features like lab tests and unstructured clinical notes. In real-life clinical practice, doctors use complementary multimodal EHR data sources to get a clearer \u2026"}, {"title": "How Useful is Continued Pre-Training for Generative Unsupervised Domain Adaptation?", "link": "https://aclanthology.org/2024.repl4nlp-1.9.pdf", "details": "R Uppaal, Y Li, J Hu - Proceedings of the 9th Workshop on Representation \u2026, 2024", "abstract": "Recent breakthroughs in scale have enabled the emergence of powerful generative language models, and the ability to fine-tune these models on various tasks by casting them into prompts or instructions. In this landscape, the problem of \u2026"}]
