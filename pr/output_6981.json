[{"title": "Transfer Learning with Clinical Concept Embeddings from Large Language Models", "link": "https://arxiv.org/pdf/2409.13893", "details": "Y Gao, R Bao, Y Ji, Y Sun, C Song, JP Ferraro, Y Ye - arXiv preprint arXiv:2409.13893, 2024", "abstract": "Knowledge sharing is crucial in healthcare, especially when leveraging data from multiple clinical sites to address data scarcity, reduce costs, and enable timely interventions. Transfer learning can facilitate cross-site knowledge transfer, but a \u2026"}, {"title": "Can we only use guideline instead of shot in prompt?", "link": "https://arxiv.org/pdf/2409.12979", "details": "J Chen, S Wang, Z Li, W Xiong, L Qu, Z Xu, Y Qi - arXiv preprint arXiv:2409.12979, 2024", "abstract": "Currently, prompting techniques can be mainly divided into two categories: 1) shot method implicitly inspires the model to answer the question by mimicing the steps in the given example, eg, the few-shot CoT. 2) Guideline method explicitly instructs the \u2026"}, {"title": "Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science", "link": "https://arxiv.org/pdf/2409.14673", "details": "T Wang, X Xu, Y Wang, Y Jiang - arXiv preprint arXiv:2409.14673, 2024", "abstract": "Real-world applications of large language models (LLMs) in computational social science (CSS) tasks primarily depend on the effectiveness of instruction tuning (IT) or in-context learning (ICL). While IT has shown highly effective at fine-tuning LLMs for \u2026"}, {"title": "Knowledge Planning in Large Language Models for Domain-Aligned Counseling Summarization", "link": "https://arxiv.org/pdf/2409.14907", "details": "A Srivastava, S Joshi, T Chakraborty, MS Akhtar - arXiv preprint arXiv:2409.14907, 2024", "abstract": "In mental health counseling, condensing dialogues into concise and relevant summaries (aka counseling notes) holds pivotal significance. Large Language Models (LLMs) exhibit remarkable capabilities in various generative tasks; however \u2026"}, {"title": "Atari-GPT: Investigating the Capabilities of Multimodal Large Language Models as Low-Level Policies for Atari Games", "link": "https://arxiv.org/pdf/2408.15950", "details": "NR Waytowich, D White, MD Sunbeam, VG Goecks - arXiv preprint arXiv:2408.15950, 2024", "abstract": "Recent advancements in large language models (LLMs) have expanded their capabilities beyond traditional text-based tasks to multimodal domains, integrating visual, auditory, and textual data. While multimodal LLMs have been extensively \u2026"}, {"title": "Investigating Layer Importance in Large Language Models", "link": "https://arxiv.org/pdf/2409.14381", "details": "Y Zhang, Y Dong, K Kawaguchi - arXiv preprint arXiv:2409.14381, 2024", "abstract": "Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in \u2026"}, {"title": "PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large Language Models", "link": "https://arxiv.org/pdf/2409.15188", "details": "Z Wang, F Yuan, V LeBaron, T Flickinger, LE Barnes - arXiv preprint arXiv \u2026, 2024", "abstract": "Effective patient-provider communication is crucial in clinical care, directly impacting patient outcomes and quality of life. Traditional evaluation methods, such as human ratings, patient feedback, and provider self-assessments, are often limited by high \u2026"}, {"title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "link": "https://arxiv.org/pdf/2409.13853", "details": "Z Wang, R Bao, Y Wu, J Taylor, C Xiao, F Zheng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Pretrained large language models (LLMs) have revolutionized natural language processing (NLP) tasks such as summarization, question answering, and translation. However, LLMs pose significant security risks due to their tendency to memorize \u2026"}, {"title": "Unveiling Narrative Reasoning Limits of Large Language Models with Trope in Movie Synopses", "link": "https://arxiv.org/pdf/2409.14324", "details": "HT Su, YC Hsu, X Lin, XQ Shi, Y Niu, HY Hsu, H Lee\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) equipped with chain-of-thoughts (CoT) prompting have shown significant multi-step reasoning capabilities in factual content like mathematics, commonsense, and logic. However, their performance in narrative \u2026"}]
