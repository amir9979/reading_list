[{"title": "Inference retrieval-augmented multi-modal chain-of-thoughts reasoning for language models", "link": "https://ieeexplore.ieee.org/abstract/document/10888701/", "details": "Q He, S Qian, J Zhang, C Wang - ICASSP 2025-2025 IEEE International Conference \u2026, 2025", "abstract": "Recent advancements in Large Language Models (LLMs) have catalyzed the exploration of Chain of Thought (CoT) approaches, particularly in extending their application to multimodal tasks to enhance reasoning capabilities. However, current \u2026"}, {"title": "Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models", "link": "https://arxiv.org/pdf/2503.16779", "details": "M Wu, T Zhu, H Han, X Zhang, W Shao, W Chen - arXiv preprint arXiv:2503.16779, 2025", "abstract": "Tool learning can further broaden the usage scenarios of large language models (LLMs). However most of the existing methods either need to finetune that the model can only use tools seen in the training data, or add tool demonstrations into the \u2026"}, {"title": "Promote, Suppress, Iterate: How Language Models Answer One-to-Many Factual Queries", "link": "https://arxiv.org/pdf/2502.20475", "details": "TL Yan, R Jia - arXiv preprint arXiv:2502.20475, 2025", "abstract": "To answer one-to-many factual queries (eg, listing cities of a country), a language model (LM) must simultaneously recall knowledge and avoid repeating previous answers. How are these two subtasks implemented and integrated internally? Across \u2026"}, {"title": "MMSciBench: Benchmarking Language Models on Multimodal Scientific Problems", "link": "https://arxiv.org/pdf/2503.01891", "details": "X Ye, C Li, S Chen, X Tang, W Wei - arXiv preprint arXiv:2503.01891, 2025", "abstract": "Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present \u2026"}, {"title": "FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models", "link": "https://arxiv.org/pdf/2502.17924", "details": "H Lin, Y Deng, Y Gu, W Zhang, J Ma, SK Ng, TS Chua - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have significantly advanced the fact-checking studies. However, existing automated fact-checking evaluation methods rely on static datasets and classification metrics, which fail to automatically evaluate the \u2026"}, {"title": "Probench: Benchmarking large language models in competitive programming", "link": "https://arxiv.org/pdf/2502.20868", "details": "L Yang, R Jin, L Shi, J Peng, Y Chen, D Xiong - arXiv preprint arXiv:2502.20868, 2025", "abstract": "With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging, large language models (LLMs) have entered a new phase of development. However, existing benchmarks for coding evaluation are gradually inadequate to assess the \u2026"}, {"title": "Scalable best-of-n selection for large language models via self-certainty", "link": "https://arxiv.org/pdf/2502.18581", "details": "Z Kang, X Zhao, D Song - arXiv preprint arXiv:2502.18581, 2025", "abstract": "Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for \u2026"}, {"title": "Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models", "link": "https://arxiv.org/pdf/2503.17523", "details": "L Qiu, F Sha, K Allen, Y Kim, T Linzen, S van Steenkiste - arXiv preprint arXiv \u2026, 2025", "abstract": "Artificial intelligence systems based on large language models (LLMs) are increasingly used as agents that interact with users and with the world. To do so successfully, LLMs need to construct internal representations of the world and form \u2026"}, {"title": "Assessing Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks", "link": "https://openreview.net/pdf%3Fid%3D3YyyiyV4B6", "details": "F Lin, S Mao, E La Malfa, V Hofmann, A de Wynter\u2026 - Workshop on Reasoning and \u2026", "abstract": "Language is not monolithic. While benchmarks, including those designed for multiple languages, are often used as proxies to evaluate the performance of Large Language Models (LLMs), they tend to overlook the nuances of within-language \u2026"}]
