[{"title": "The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models", "link": "https://arxiv.org/pdf/2411.08870%3F", "details": "DP Jeong, P Mani, S Garg, ZC Lipton, M Oberst - arXiv preprint arXiv:2411.08870, 2024", "abstract": "Several recent works seek to develop foundation models specifically for medical applications, adapting general-purpose large language models (LLMs) and vision- language models (VLMs) via continued pretraining on publicly available biomedical \u2026"}, {"title": "TURSpider: A Turkish Text-to-SQL Dataset and LLM-Based Study", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10753591.pdf", "details": "AB Kanburoglu, FB Tek - IEEE Access, 2024", "abstract": "This paper introduces TURSpider, a novel Turkish Text-to-SQL dataset developed through human translation of the widely used Spider dataset, aimed at addressing the current lack of complex, cross-domain SQL datasets for the Turkish language \u2026"}, {"title": "Text-to-SQL Systems in the Era of Advanced Large Language Models", "link": "https://era.library.ualberta.ca/items/3db9c207-9248-4760-8f82-0f6f308ff3ff/download/d817de66-5fe8-47fb-b065-1e5cf7644244", "details": "M Pourreza - 2024", "abstract": "Text-to-SQL conversion, the process of transforming natural language queries into executable SQL commands, stands at the forefront of bridging human linguistic capabilities with the structured logic of databases. This dissertation embarks on a \u2026"}, {"title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models", "link": "https://openreview.net/pdf%3Fid%3DLC1QAqhePv", "details": "D Zhang, Z Hu, S Zhoubian, Z Du, K Yang, Z Wang\u2026 - The Thirty-eight Conference on \u2026", "abstract": "Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving \u2026"}, {"title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models", "link": "https://arxiv.org/pdf/2410.18785%3F", "details": "Q Li, X Liu, Z Tang, P Dong, Z Li, X Pan, X Chu - arXiv preprint arXiv:2410.18785, 2024", "abstract": "Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality, with many methods excelling across these criteria. Some \u2026"}, {"title": "Continual LLaVA: Continual Instruction Tuning in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2411.02564", "details": "M Cao, Y Liu, Y Liu, T Wang, J Dong, H Ding, X Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Instruction tuning constitutes a prevalent technique for tailoring Large Vision Language Models (LVLMs) to meet individual task requirements. To date, most of the existing approaches are confined to single-task adaptation, whereas the \u2026"}, {"title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models", "link": "https://arxiv.org/pdf/2411.00836", "details": "C Zou, X Guo, R Yang, J Zhang, B Hu, H Zhang - arXiv preprint arXiv:2411.00836, 2024", "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor \u2026"}, {"title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines", "link": "https://arxiv.org/pdf/2410.21220", "details": "Z Zhang, Y Zhang, X Ding, X Yue - arXiv preprint arXiv:2410.21220, 2024", "abstract": "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This \u2026"}, {"title": "Dynamic Strategy Planning for Efficient Question Answering with Large Language Models", "link": "https://arxiv.org/pdf/2410.23511", "details": "T Parekh, P Prakash, A Radovic, A Shekher\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Research has shown the effectiveness of reasoning (eg, Chain-of-Thought), planning (eg, SelfAsk), and retrieval augmented generation strategies to improve the performance of Large Language Models (LLMs) on various tasks, such as question \u2026"}]
