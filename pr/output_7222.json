[{"title": "Mutual Prompt Leaning for Vision Language Models", "link": "https://link.springer.com/article/10.1007/s11263-024-02243-z", "details": "S Long, Z Zhao, J Yuan, Z Tan, J Liu, J Feng, S Wang\u2026 - International Journal of \u2026, 2024", "abstract": "Large pre-trained vision language models (VLMs) have demonstrated impressive representation learning capabilities, but their transferability across various downstream tasks heavily relies on prompt learning. Since VLMs consist of text and \u2026"}, {"title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks", "link": "https://arxiv.org/pdf/2409.07353", "details": "MZ Hossain, A Imteaj - arXiv preprint arXiv:2409.07353, 2024", "abstract": "Large Vision-Language Models (LVLMs), trained on multimodal big datasets, have significantly advanced AI by excelling in vision-language tasks. However, these models remain vulnerable to adversarial attacks, particularly jailbreak attacks, which \u2026"}, {"title": "Pushing the Limits of Vision-Language Models in Remote Sensing without Human Annotations", "link": "https://arxiv.org/pdf/2409.07048", "details": "K Cha, D Yu, J Seo - arXiv preprint arXiv:2409.07048, 2024", "abstract": "The prominence of generalized foundation models in vision-language integration has witnessed a surge, given their multifarious applications. Within the natural domain, the procurement of vision-language datasets to construct these foundation \u2026"}, {"title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions", "link": "https://arxiv.org/pdf/2409.18042", "details": "K Chen, Y Gou, R Huang, Z Liu, D Tan, J Xu, C Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and \u2026"}, {"title": "Inference-Time Language Model Alignment via Integrated Value Guidance", "link": "https://arxiv.org/pdf/2409.17819", "details": "Z Liu, Z Zhou, Y Wang, C Yang, Y Qiao - arXiv preprint arXiv:2409.17819, 2024", "abstract": "Large language models are typically fine-tuned to align with human preferences, but tuning large models is computationally intensive and complex. In this work, we introduce $\\textit {Integrated Value Guidance} $(IVG), a method that uses implicit and \u2026"}, {"title": "Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for Privacy-Preserving Personalization of Large Language Models", "link": "https://arxiv.org/pdf/2409.09510", "details": "A Salemi, H Zamani - arXiv preprint arXiv:2409.09510, 2024", "abstract": "Privacy-preserving methods for personalizing large language models (LLMs) are relatively under-explored. There are two schools of thought on this topic:(1) generating personalized outputs by personalizing the input prompt through retrieval \u2026"}, {"title": "ComAlign: Compositional Alignment in Vision-Language Models", "link": "https://arxiv.org/pdf/2409.08206", "details": "A Abdollah, A Izadi, A Saghafian, R Vahidimajd\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Vision-language models (VLMs) like CLIP have showcased a remarkable ability to extract transferable features for downstream tasks. Nonetheless, the training process of these models is usually based on a coarse-grained contrastive loss between the \u2026"}, {"title": "Fine-tuning Large Language Models to Improve Accuracy and Comprehensibility of Automated Code Review", "link": "https://dl.acm.org/doi/pdf/10.1145/3695993", "details": "Y Yu, G Rong, H Shen, H Zhang, D Shao, M Wang\u2026 - ACM Transactions on \u2026, 2024", "abstract": "As code review is a tedious and costly software quality practice, researchers have proposed several machine learning-based methods to automate the process. The primary focus has been on accuracy, that is, how accurately the algorithms are able \u2026"}, {"title": "Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia", "link": "https://arxiv.org/pdf/2409.17391", "details": "Z Zhou, J Wang, D Lin, K Chen - arXiv preprint arXiv:2409.17391, 2024", "abstract": "Though Large Language Models (LLMs) have shown remarkable abilities in mathematics reasoning, they are still struggling with performing numeric operations accurately, such as addition and multiplication. Numbers can be tokenized into \u2026"}]
