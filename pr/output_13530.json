[{"title": "Gradient Imbalance in Direct Preference Optimization", "link": "https://arxiv.org/pdf/2502.20847", "details": "Q Ma, J Shi, C Jin, JN Hwang, S Belongie, L Li - arXiv preprint arXiv:2502.20847, 2025", "abstract": "Direct Preference Optimization (DPO) has been proposed as a promising alternative to Proximal Policy Optimization (PPO) based Reinforcement Learning with Human Feedback (RLHF). However, empirical evaluations consistently reveal suboptimal \u2026"}, {"title": "Valuable Hallucinations: Realizable Non-realistic Propositions", "link": "https://arxiv.org/pdf/2502.11113", "details": "Q Chen, B Wang - arXiv preprint arXiv:2502.11113, 2025", "abstract": "This paper introduces the first formal definition of valuable hallucinations in large language models (LLMs), addressing a gap in the existing literature. We provide a systematic definition and analysis of hallucination value, proposing methods for \u2026"}, {"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models", "link": "https://arxiv.org/pdf/2502.03199%3F", "details": "J Wu, Y Shen, S Liu, Y Tang, S Song, X Wang, L Cai - arXiv preprint arXiv \u2026, 2025", "abstract": "Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the \u2026"}, {"title": "Multilingual Language Model Pretraining using Machine-translated Data", "link": "https://arxiv.org/pdf/2502.13252", "details": "J Wang, Y Lu, M Weber, M Ryabinin, D Adelani\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "High-resource languages such as English, enables the pretraining of high-quality large language models (LLMs). The same can not be said for most other languages as LLMs still underperform for non-English languages, likely due to a gap in the \u2026"}, {"title": "Promote, Suppress, Iterate: How Language Models Answer One-to-Many Factual Queries", "link": "https://arxiv.org/pdf/2502.20475", "details": "TL Yan, R Jia - arXiv preprint arXiv:2502.20475, 2025", "abstract": "To answer one-to-many factual queries (eg, listing cities of a country), a language model (LM) must simultaneously recall knowledge and avoid repeating previous answers. How are these two subtasks implemented and integrated internally? Across \u2026"}, {"title": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs", "link": "https://arxiv.org/pdf/2502.14645", "details": "Y Wu, L Ding, L Shen, D Tao - arXiv preprint arXiv:2502.14645, 2025", "abstract": "Knowledge editing allows for efficient adaptation of large language models (LLMs) to new information or corrections without requiring full retraining. However, prior methods typically focus on either single-language editing or basic multilingual \u2026"}, {"title": "Efficient Response Generation Method Selection for Fine-Tuning Large Language Models", "link": "https://arxiv.org/pdf/2502.11779", "details": "X Ren, Q Chen, L Liu - arXiv preprint arXiv:2502.11779, 2025", "abstract": "The training data for fine-tuning large language models (LLMs) is typically structured as input-output pairs. However, for many tasks, there can be multiple equally valid output variations for the same input. Recent studies have observed that the choice of \u2026"}, {"title": "ProBench: Benchmarking Large Language Models in Competitive Programming", "link": "https://arxiv.org/pdf/2502.20868", "details": "L Yang, R Jin, L Shi, J Peng, Y Chen, D Xiong - arXiv preprint arXiv:2502.20868, 2025", "abstract": "With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging, large language models (LLMs) have entered a new phase of development. However, existing benchmarks for coding evaluation are gradually inadequate to assess the \u2026"}, {"title": "Fact or Guesswork? Evaluating Large Language Model's Medical Knowledge with Structured One-Hop Judgment", "link": "https://arxiv.org/pdf/2502.14275", "details": "J Li, Y Wang, K Zhang, Y Cai, B Hooi, N Peng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have been widely adopted in various downstream task domains. However, their ability to directly recall and apply factual medical knowledge remains under-explored. Most existing medical QA benchmarks assess \u2026"}]
