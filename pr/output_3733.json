[{"title": "MACAROON: Training Vision-Language Models To Be Your Engaged Partners", "link": "https://arxiv.org/pdf/2406.14137", "details": "S Wu, YR Fung, S Li, Y Wan, KW Chang, H Ji - arXiv preprint arXiv:2406.14137, 2024", "abstract": "Large vision-language models (LVLMs), while proficient in following instructions and responding to diverse questions, invariably generate detailed responses even when questions are ambiguous or unanswerable, leading to hallucinations and bias \u2026"}, {"title": "LIONs: An Empirically Optimized Approach to Align Language Models", "link": "https://arxiv.org/pdf/2407.06542", "details": "X Yu, Q Wu, Y Li, Z Yu - arXiv preprint arXiv:2407.06542, 2024", "abstract": "Alignment is a crucial step to enhance the instruction-following and conversational abilities of language models. Despite many recent work proposing new algorithms, datasets, and training pipelines, there is a lack of comprehensive studies measuring \u2026"}]
