[{"title": "Language Model Uncertainty Quantification with Attention Chain", "link": "https://arxiv.org/pdf/2503.19168", "details": "Y Li, R Qiang, L Moukheiber, C Zhang - arXiv preprint arXiv:2503.19168, 2025", "abstract": "Accurately quantifying a large language model's (LLM) predictive uncertainty is crucial for judging the reliability of its answers. While most existing research focuses on short, directly answerable questions with closed-form outputs (eg, multiple \u2026"}, {"title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models", "link": "https://arxiv.org/pdf/2503.18931", "details": "Y Chen, L Meng, W Peng, Z Wu, YG Jiang - arXiv preprint arXiv:2503.18931, 2025", "abstract": "Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of \u2026"}, {"title": "Breaking the Encoder Barrier for Seamless Video-Language Understanding", "link": "https://arxiv.org/pdf/2503.18422", "details": "H Li, Y Zhang, L Guo, X Yue, J Liu - arXiv preprint arXiv:2503.18422, 2025", "abstract": "Most Video-Large Language Models (Video-LLMs) adopt an encoder-decoder framework, where a vision encoder extracts frame-wise features for processing by a language model. However, this approach incurs high computational costs \u2026"}, {"title": "Towards conversational diagnostic artificial intelligence", "link": "https://www.nature.com/articles/s41586-025-08866-7", "details": "T Tu, M Schaekermann, A Palepu, K Saab, J Freyberg\u2026 - Nature, 2025", "abstract": "At the heart of medicine lies physician\u2013patient dialogue, where skillful history-taking enables effective diagnosis, management and enduring trust,. Artificial intelligence (AI) systems capable of diagnostic dialogue could increase accessibility and quality \u2026"}, {"title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation", "link": "https://arxiv.org/pdf/2504.06861", "details": "D Jagpal, X Chen, VP Namboodiri - arXiv preprint arXiv:2504.06861, 2025", "abstract": "Zero-shot, training-free, image-based text-to-video generation is an emerging area that aims to generate videos using existing image-based diffusion models. Current methods in this space require specific architectural changes to image generation \u2026"}, {"title": "Map: Evaluation and multi-agent enhancement of large language models for inpatient pathways", "link": "https://arxiv.org/pdf/2503.13205%3F", "details": "Z Chen, Z Peng, X Liang, C Wang, P Liang, L Zeng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Inpatient pathways demand complex clinical decision-making based on comprehensive patient information, posing critical challenges for clinicians. Despite advancements in large language models (LLMs) in medical applications, limited \u2026"}, {"title": "Multimodal AI and Large Language Models for Orthopantomography Radiology Report Generation and Q&A", "link": "https://www.mdpi.com/2571-5577/8/2/39", "details": "C Dasanayaka, K Dandeniya, MB Dissanayake\u2026 - Applied System Innovation, 2025", "abstract": "Access to high-quality dental healthcare remains a challenge in many countries due to limited resources, lack of trained professionals, and time-consuming report generation tasks. An intelligent clinical decision support system (ICDSS), which can \u2026"}, {"title": "How Good is my Histopathology Vision-Language Foundation Model? A Holistic Benchmark", "link": "https://arxiv.org/pdf/2503.12990", "details": "RA Majzoub, H Malik, M Naseer, Z Zaheer, T Mahmood\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recently, histopathology vision-language foundation models (VLMs) have gained popularity due to their enhanced performance and generalizability across different downstream tasks. However, most existing histopathology benchmarks are either \u2026"}, {"title": "LLaVAction: evaluating and training multi-modal large language models for action recognition", "link": "https://arxiv.org/pdf/2503.18712", "details": "S Ye, H Qi, A Mathis, MW Mathis - arXiv preprint arXiv:2503.18712, 2025", "abstract": "Understanding human behavior requires measuring behavioral actions. Due to its complexity, behavior is best mapped onto a rich, semantic structure such as language. The recent development of multi-modal large language models (MLLMs) \u2026"}]
