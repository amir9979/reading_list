[{"title": "Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction", "link": "https://arxiv.org/pdf/2503.00902", "details": "L Liu, C Zhang, L Wu, C Zhao, Z Hu, M He, J Fan - arXiv preprint arXiv:2503.00902, 2025", "abstract": "Self-reflection for Large Language Models (LLMs) has gained significant attention. Existing approaches involve models iterating and improving their previous responses based on LLMs' internal reflection ability or external feedback. However \u2026"}, {"title": "DeepThink: Aligning Language Models with Domain-Specific User Intents", "link": "https://arxiv.org/pdf/2502.05497", "details": "Y Li, M Luo, Y Gong, C Lin, J Jiao, Y Liu, K Huang - arXiv preprint arXiv:2502.05497, 2025", "abstract": "Supervised fine-tuning with synthesized instructions has been a common practice for adapting LLMs to domain-specific QA tasks. However, the synthesized instructions deviate from real user questions and expected answers. This study proposes a novel \u2026"}, {"title": "DeLTa: A Decoding Strategy based on Logit Trajectory Prediction Improves Factuality and Reasoning Ability", "link": "https://arxiv.org/pdf/2503.02343", "details": "Y He, Y Takase, Y Ishibashi, H Shimodaira - arXiv preprint arXiv:2503.02343, 2025", "abstract": "Large Language Models (LLMs) are increasingly being used in real-world applications. However, concerns about the reliability of the content they generate persist, as it frequently deviates from factual correctness or exhibits deficiencies in \u2026"}, {"title": "HoT: Highlighted Chain of Thought for Referencing Supportive Facts from Inputs", "link": "https://arxiv.org/pdf/2503.02003", "details": "T Nguyen, L Bolton, MR Taesiri, AT Nguyen - arXiv preprint arXiv:2503.02003, 2025", "abstract": "An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To \u2026"}, {"title": "(How) Do Language Models Track State?", "link": "https://arxiv.org/pdf/2503.02854", "details": "BZ Li, ZC Guo, J Andreas - arXiv preprint arXiv:2503.02854, 2025", "abstract": "Transformer language models (LMs) exhibit behaviors--from storytelling to code generation--that appear to require tracking the unobserved state of an evolving world. How do they do so? We study state tracking in LMs trained or fine-tuned to \u2026"}, {"title": "Personalization Toolkit: Training Free Personalization of Large Vision Language Models", "link": "https://arxiv.org/pdf/2502.02452%3F", "details": "S Seifi, V Dorovatas, DO Reino, R Aljundi - arXiv preprint arXiv:2502.02452, 2025", "abstract": "Large Vision Language Models (LVLMs) have significant potential to deliver personalized assistance by adapting to individual users' unique needs and preferences. Personalization of LVLMs is an emerging area that involves \u2026"}, {"title": "MMSciBench: Benchmarking Language Models on Multimodal Scientific Problems", "link": "https://arxiv.org/pdf/2503.01891", "details": "X Ye, C Li, S Chen, X Tang, W Wei - arXiv preprint arXiv:2503.01891, 2025", "abstract": "Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present \u2026"}, {"title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models", "link": "https://arxiv.org/pdf/2502.10250%3F", "details": "GK Kumar, I Chaabane, K Wu - arXiv preprint arXiv:2502.10250, 2025", "abstract": "Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved \u2026"}, {"title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks", "link": "https://arxiv.org/pdf/2503.02656", "details": "P Suganthan, F Moiseev, L Yan, J Wu, J Ni, J Han\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Decoder-based transformers, while revolutionizing language modeling and scaling to immense sizes, have not completely overtaken encoder-heavy architectures in natural language processing. Specifically, encoder-only models remain dominant in \u2026"}]
