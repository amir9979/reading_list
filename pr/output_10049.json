[{"title": "Association Between Patient Portal Engagement and Weight Loss Outcomes in Patients After Bariatric Surgery: Longitudinal Observational Study Using Electronic \u2026", "link": "https://www.jmir.org/2024/1/e56573/", "details": "X Zhang, K Kang, C Yan, Y Feng, S Vandekar, D Yu\u2026 - Journal of Medical Internet \u2026, 2024", "abstract": "Background Bariatric surgery is an effective intervention for obesity, but comprehensive postoperative self-management is essential for optimal outcomes. While patient portals are generally seen as beneficial in engaging patients in health \u2026"}, {"title": "Entity Matching with Large Language Models as Weak and Strong Labellers", "link": "https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3D-78xEQAAQBAJ%26oi%3Dfnd%26pg%3DPA58%26ots%3DMaNslCU7yC%26sig%3DnTWN8q385JIA9ILIy__hkTsWMk0", "details": "H Du, N Hurley, A Lawlor - New Trends in Database and Information Systems \u2026", "abstract": "A number of recent studies have shown that pre-trained large language models (LLMs) display highly competitive performance on entity matching tasks while acting in a zero shot manner, thus reducing the need for labelled training data. However \u2026"}, {"title": "Prompting Large Language Models for Clinical Temporal Relation Extraction", "link": "https://arxiv.org/pdf/2412.04512", "details": "J He, L Rasmy, H Li, J Li, Z Sun, E Yu, D Zhi, C Tao - arXiv preprint arXiv:2412.04512, 2024", "abstract": "Objective: This paper aims to prompt large language models (LLMs) for clinical temporal relation extraction (CTRE) in both few-shot and fully supervised settings. Materials and Methods: This study utilizes four LLMs: Encoder-based GatorTron \u2026"}, {"title": "M $^ 3$ PC: Test-time Model Predictive Control for Pretrained Masked Trajectory Model", "link": "https://arxiv.org/pdf/2412.05675", "details": "K Wen, Y Hu, Y Mu, L Ke - arXiv preprint arXiv:2412.05675, 2024", "abstract": "Recent work in Offline Reinforcement Learning (RL) has shown that a unified Transformer trained under a masked auto-encoding objective can effectively capture the relationships between different modalities (eg, states, actions, rewards) within \u2026"}, {"title": "Enhanced Computationally Efficient Long LoRA Inspired Perceiver Architectures for Auto-Regressive Language Modeling", "link": "https://arxiv.org/pdf/2412.06106", "details": "K Mahmood, S Huang - arXiv preprint arXiv:2412.06106, 2024", "abstract": "The Transformer architecture has revolutionized the Natural Language Processing field and is the backbone of Large Language Models (LLMs). The Transformer uses the attention mechanism that computes the pair-wise similarity between its input \u2026"}, {"title": "The Vulnerability of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?", "link": "https://arxiv.org/pdf/2412.03597", "details": "S Banerjee, A Agarwal, E Singh - arXiv preprint arXiv:2412.03597, 2024", "abstract": "The pursuit of leaderboard rankings in Large Language Models (LLMs) has created a fundamental paradox: models excel at standardized tests while failing to demonstrate genuine language understanding and adaptability. Our systematic \u2026"}]
