[{"title": "Image2Struct: Benchmarking Structure Extraction for Vision-Language Models", "link": "https://arxiv.org/pdf/2410.22456", "details": "JS Roberts, T Lee, CH Wong, M Yasunaga, Y Mai\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We introduce Image2Struct, a benchmark to evaluate vision-language models (VLMs) on extracting structure from images. Our benchmark 1) captures real-world use cases, 2) is fully automatic and does not require human judgment, and 3) is \u2026"}, {"title": "Effective and Efficient Adversarial Detection for Vision-Language Models via A Single Vector", "link": "https://arxiv.org/pdf/2410.22888", "details": "Y Huang, F Zhu, J Tang, P Zhou, W Lei, J Lv, TS Chua - arXiv preprint arXiv \u2026, 2024", "abstract": "Visual Language Models (VLMs) are vulnerable to adversarial attacks, especially those from adversarial images, which is however under-explored in literature. To facilitate research on this critical safety problem, we first construct a new laRge-scale \u2026"}]
