[{"title": "Aneta Lisowska1, 2, Nehama Lewis3, Paula Gabanelli4, Itske Fraterman5, Lucia", "link": "https://drive.google.com/file/d/1PVOO59J2hsm1Poy82XmA59z3H57U2it4/view", "details": "SQ Sacchi, S Wilk, M Peleg", "abstract": "The CAPABLE project aims to enhance the well-being of home-managed cancer patients by implementing a coaching system that recommends evidence-based health behavior change interventions and supports patient compliance. This study \u2026"}, {"title": "Optimizing acoustic system simulations with Bayesian Neural Networks (BNN): Reducing computational effort through intelligent sampling", "link": "https://pub.dega-akustik.de/DAS-DAGA_2025/files/upload/paper/167.pdf", "details": "A Hildenbrand, S Marburg", "abstract": "With the increasing capabilities of modern machine learning algorithms surrogate models for numerical simulation are widely used. The reason for that is that numerical simulation can be very time consuming and computationally expensive \u2026"}, {"title": "Causal View of Time Series Imputation: Some Identification Results on Missing Mechanism", "link": "https://arxiv.org/pdf/2505.07180", "details": "R Cai, K Zheng, J Huang, Z Li, Z Chen, B Xu, Z Hao - arXiv preprint arXiv:2505.07180, 2025", "abstract": "Time series imputation is one of the most challenge problems and has broad applications in various fields like health care and the Internet of Things. Existing methods mainly aim to model the temporally latent dependencies and the generation \u2026"}, {"title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation", "link": "https://arxiv.org/pdf/2505.06027", "details": "S Vasilev, C Herold, B Liao, SH Hashemi, S Khadivi\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a \u2026"}, {"title": "Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\\mathbf {\\texttt {D}} $ ual-$\\mathbf {\\texttt {H}} $ ead $\\mathbf {\\texttt {O}} \u2026", "link": "https://arxiv.org/pdf/2505.07675", "details": "S Kang, DB Lee, H Jang, SJ Hwang - arXiv preprint arXiv:2505.07675, 2025", "abstract": "Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resource \u2026"}]
