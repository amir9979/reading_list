[{"title": "LLM Compiler: Foundation Language Models for Compiler Optimization", "link": "https://dl.acm.org/doi/pdf/10.1145/3708493.3712691", "details": "C Cummins, V Seeker, D Grubisic, B Roziere\u2026 - Proceedings of the 34th \u2026, 2025", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is \u2026"}, {"title": "Layer by Layer: Uncovering Hidden Representations in Language Models", "link": "https://arxiv.org/pdf/2502.02013", "details": "O Skean, MR Arefin, D Zhao, N Patel, J Naghiyev\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "From extracting features to generating text, the outputs of large language models (LLMs) typically rely on their final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that \u2026"}, {"title": "Length-Controlled Margin-Based Preference Optimization without Reference Model", "link": "https://arxiv.org/pdf/2502.14643", "details": "G Li, T Xia, Y Chang, Y Wu - arXiv preprint arXiv:2502.14643, 2025", "abstract": "Direct Preference Optimization (DPO) is a widely adopted offline algorithm for preference-based reinforcement learning from human feedback (RLHF), designed to improve training simplicity and stability by redefining reward functions. However \u2026"}, {"title": "Diversity-driven Data Selection for Language Model Tuning through Sparse Autoencoder", "link": "https://arxiv.org/pdf/2502.14050", "details": "X Yang, S Nie, L Liu, S Gururangan, U Karn, R Hou\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Current pre-trained large language models typically need instruction tuning to align with human preferences. However, instruction tuning data is often quantity-saturated due to the large volume of data collection and fast model iteration, leaving coreset \u2026"}, {"title": "FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models", "link": "https://arxiv.org/pdf/2502.17924", "details": "H Lin, Y Deng, Y Gu, W Zhang, J Ma, SK Ng, TS Chua - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) have significantly advanced the fact-checking studies. However, existing automated fact-checking evaluation methods rely on static datasets and classification metrics, which fail to automatically evaluate the \u2026"}, {"title": "WHODUNIT: Evaluation benchmark for culprit detection in mystery stories", "link": "https://arxiv.org/pdf/2502.07747", "details": "K Gupta - arXiv preprint arXiv:2502.07747, 2025", "abstract": "We present a novel data set, WhoDunIt, to assess the deductive reasoning capabilities of large language models (LLM) within narrative contexts. Constructed from open domain mystery novels and short stories, the dataset challenges LLMs to \u2026"}, {"title": "The Relationship Between Reasoning and Performance in Large Language Models--o3 (mini) Thinks Harder, Not Longer", "link": "https://arxiv.org/pdf/2502.15631", "details": "M Ballon, A Algaba, V Ginis - arXiv preprint arXiv:2502.15631, 2025", "abstract": "Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token \u2026"}, {"title": "Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models", "link": "https://arxiv.org/pdf/2502.15086", "details": "Y In, W Kim, K Yoon, S Kim, M Tanjim, K Kim, C Park - arXiv preprint arXiv \u2026, 2025", "abstract": "As the use of large language model (LLM) agents continues to grow, their safety vulnerabilities have become increasingly evident. Extensive benchmarks evaluate various aspects of LLM safety by defining the safety relying heavily on general \u2026"}, {"title": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning", "link": "https://arxiv.org/pdf/2502.18439", "details": "C Park, S Han, X Guo, A Ozdaglar, K Zhang, JK Kim - arXiv preprint arXiv:2502.18439, 2025", "abstract": "Leveraging multiple large language models (LLMs) to build collaborative multi- agentic workflows has demonstrated significant potential. However, most previous studies focus on prompting the out-of-the-box LLMs, relying on their innate capability \u2026"}]
