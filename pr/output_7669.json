[{"title": "Attention Prompting on Image for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.17143", "details": "R Yu, W Yu, X Wang - arXiv preprint arXiv:2409.17143, 2024", "abstract": "Compared with Large Language Models (LLMs), Large Vision-Language Models (LVLMs) can also accept images as input, thus showcasing more interesting emergent capabilities and demonstrating impressive performance on various vision \u2026"}, {"title": "A Unified Hallucination Mitigation Framework for Large Vision-Language Models", "link": "https://arxiv.org/pdf/2409.16494", "details": "Y Chang, L Jing, X Zhang, Y Zhang - arXiv preprint arXiv:2409.16494, 2024", "abstract": "Hallucination is a common problem for Large Vision-Language Models (LVLMs) with long generations which is difficult to eradicate. The generation with hallucinations is partially inconsistent with the image content. To mitigate hallucination, current \u2026"}, {"title": "Mutual Prompt Leaning for Vision Language Models", "link": "https://link.springer.com/article/10.1007/s11263-024-02243-z", "details": "S Long, Z Zhao, J Yuan, Z Tan, J Liu, J Feng, S Wang\u2026 - International Journal of \u2026, 2024", "abstract": "Large pre-trained vision language models (VLMs) have demonstrated impressive representation learning capabilities, but their transferability across various downstream tasks heavily relies on prompt learning. Since VLMs consist of text and \u2026"}, {"title": "ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context Information in Multi-Turn Multimodal Medical Dialogue", "link": "https://arxiv.org/pdf/2409.17610", "details": "Z Li, C Zou, S Ma, Z Yang, C Du, Y Tang, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rocketing prosperity of large language models (LLMs) in recent years has boosted the prevalence of vision-language models (VLMs) in the medical sector. In our online medical consultation scenario, a doctor responds to the texts and images \u2026"}, {"title": "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "link": "https://arxiv.org/pdf/2410.05077", "details": "FM Molfese, S Conia, R Orlando, R Navigli - arXiv preprint arXiv:2410.05077, 2024", "abstract": "Current Large Language Models (LLMs) have shown strong reasoning capabilities in commonsense question answering benchmarks, but the process underlying their success remains largely opaque. As a consequence, recent approaches have \u2026"}, {"title": "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server", "link": "https://arxiv.org/pdf/2410.05725", "details": "W Wang, X Liang, R Ye, J Chai, S Chen, Y Wang - arXiv preprint arXiv:2410.05725, 2024", "abstract": "The success of large language models (LLMs) facilitate many parties to fine-tune LLMs on their own private data. However, this practice raises privacy concerns due to the memorization of LLMs. Existing solutions, such as utilizing synthetic data for \u2026"}, {"title": "Empirical Insights on Fine-Tuning Large Language Models for Question-Answering", "link": "https://arxiv.org/pdf/2409.15825", "details": "J Ye, Y Yang, Q Zhang, T Gui, X Huang, P Wang, Z Shi\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) encode extensive world knowledge through pre- training on massive datasets, which can then be fine-tuned for the question- answering (QA) task. However, effective strategies for fine-tuning LLMs for the QA \u2026"}, {"title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models", "link": "https://arxiv.org/pdf/2410.06154", "details": "MJ Mirza, M Zhao, Z Mao, S Doveh, W Lin, P Gavrikov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task \u2026"}, {"title": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models", "link": "https://arxiv.org/pdf/2410.06741", "details": "Z Gong, H Yu, C Liao, B Liu, C Chen, J Li - arXiv preprint arXiv:2410.06741, 2024", "abstract": "Multi-task learning (MTL) benefits the fine-tuning of large language models (LLMs) by providing a single model with improved performance and generalization ability across tasks, presenting a resource-efficient alternative to developing separate \u2026"}]
