[{"title": "HalluLens: LLM Hallucination Benchmark", "link": "https://arxiv.org/pdf/2504.17550", "details": "Y Bang, Z Ji, A Schelten, A Hartshorn, T Fowler\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) often generate responses that deviate from user input or training data, a phenomenon known as\" hallucination.\" These hallucinations undermine user trust and hinder the adoption of generative AI systems. Addressing \u2026"}, {"title": "Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study", "link": "https://arxiv.org/pdf/2504.16414", "details": "M Khodadad, AS Kasmaee, M Astaraki, N Sherck\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a \u2026"}, {"title": "Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments", "link": "https://arxiv.org/pdf/2504.17087", "details": "Y Li, JH Mohamud, C Sun, D Wu, B Boulet - arXiv preprint arXiv:2504.17087, 2025", "abstract": "Large language models (LLMs) are being widely applied across various fields, but as tasks become more complex, evaluating their responses is increasingly challenging. Compared to human evaluators, the use of LLMs to support \u2026"}, {"title": "Pre-Trained Language Models for Mental Health: An Empirical Study on Arabic Q&A Classification", "link": "https://www.mdpi.com/2227-9032/13/9/985", "details": "H Alhuzali, A Alasmari - Healthcare, 2025", "abstract": "Background: Pre-Trained Language Models hold significant promise for revolutionizing mental health care by delivering accessible and culturally sensitive resources. Despite this potential, their efficacy in mental health applications \u2026"}, {"title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models", "link": "https://arxiv.org/pdf/2504.17449", "details": "J Zhang, J Wang, H Li, L Shou, K Chen, G Chen, Q Xie\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The significant computational demands of pretrained language models (PLMs), which often require dedicated hardware, present a substantial challenge in serving them efficiently, especially in multi-tenant environments. To address this, we \u2026"}, {"title": "ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection", "link": "https://arxiv.org/pdf/2504.00695", "details": "X Zhu, Z Gu, S Zheng, T Wang, T Li, H Feng, Y Xiao - arXiv preprint arXiv:2504.00695, 2025", "abstract": "Pre-training large language models (LLMs) necessitates enormous diverse textual corpora, making effective data selection a key challenge for balancing computational resources and model performance. Current methodologies primarily emphasize data \u2026"}, {"title": "Large language models are human-like annotators", "link": "https://link.springer.com/chapter/10.1007/978-3-031-88720-8_45", "details": "M Marreddy, SR Oota, M Gupta - European Conference on Information Retrieval, 2025", "abstract": "Large Language Models Are Human-Like Annotators | SpringerLink Skip to main content Advertisement Springer Nature Link Account Menu Find a journal Publish with us Track your research Search Cart 1.Home 2.Advances in Information \u2026"}, {"title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models", "link": "https://arxiv.org/pdf/2504.02882", "details": "S Jung, D Lee, S Lee, G Seo, D Lee, B Ko, J Cho\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in real- world applications, but face challenges in handling incomplete queries and out-of- scope requests. While existing approaches rely mainly on Supervised Fine-Tuning \u2026"}, {"title": "Multi-Prompting Scenario-based Movie Recommendation with Large Language Models: Real User Case Study", "link": "https://dl.acm.org/doi/abs/10.1145/3706599.3706682", "details": "R Sun, X Li, A Akella, JA Konstan - Proceedings of the Extended Abstracts of the CHI \u2026, 2025", "abstract": "This paper investigates the potential of using large language models (LLMs) for personalized movie recommendations in an online field experiment. We assess the performance of LLM recommenders using a combination of between-subject \u2026"}]
