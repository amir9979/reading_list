[{"title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models", "link": "https://arxiv.org/pdf/2410.18785%3F", "details": "Q Li, X Liu, Z Tang, P Dong, Z Li, X Pan, X Chu - arXiv preprint arXiv:2410.18785, 2024", "abstract": "Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality, with many methods excelling across these criteria. Some \u2026"}, {"title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style", "link": "https://arxiv.org/pdf/2410.16184%3F", "details": "Y Liu, Z Yao, R Min, Y Cao, L Hou, J Li - arXiv preprint arXiv:2410.16184, 2024", "abstract": "Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward \u2026"}, {"title": "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "link": "https://arxiv.org/pdf/2410.15096", "details": "OJ Kwon, DE Matsunaga, KE Kim - arXiv preprint arXiv:2410.15096, 2024", "abstract": "A critical component of the current generation of language models is preference alignment, which aims to precisely control the model's behavior to meet human needs and values. The most notable among such methods is Reinforcement \u2026"}, {"title": "Smoothie: Label Free Language Model Routing", "link": "https://openreview.net/pdf%3Fid%3DpPSWHsgqRp", "details": "N Guha, MF Chen, T Chow, IS Khare, C Re - The Thirty-eighth Annual Conference on Neural \u2026", "abstract": "Large language models (LLMs) are increasingly used in applications where LLM inputs may span many different tasks. Recent work has found that the choice of LLM is consequential, and different LLMs may be good for different input samples. Prior \u2026"}, {"title": "W2CL: A Multi-task Learning Approach to Improve Domain-Specific Sentence Classification Through Word Classification and Contrastive Learning", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9431-7_23", "details": "S Yan, Z Luo, S Luo, Y Qiu - CCF International Conference on Natural Language \u2026, 2024", "abstract": "Sentence classification task plays a crucial role in various NLP tasks. Recent studies have shown that contrastive learning can enhance the representational capability of Pre-trained Language Models (PLMs) and that different methods for constructing \u2026"}, {"title": "Improving Multimodal Large Language Models Using Continual Learning", "link": "https://arxiv.org/pdf/2410.19925", "details": "S Srivastava, MY Harun, R Shrestha, C Kanan - arXiv preprint arXiv:2410.19925, 2024", "abstract": "Generative large language models (LLMs) exhibit impressive capabilities, which can be further augmented by integrating a pre-trained vision model into the original LLM to create a multimodal LLM (MLLM). However, this integration often significantly \u2026"}, {"title": "Multi-granularity Semantic Guided Transformer for Radiology Report Generation", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9437-9_36", "details": "Y Song, X Hua, K Zhang, H Zan, R Li - \u2026 on Natural Language Processing and Chinese \u2026, 2024", "abstract": "Abstract Radiology Report Generation aims to generate accurate diagnostic reports based on medical images. Existing approaches based on the Transformer paradigm and grid features had achieved significant performance. However, this paradigm \u2026"}, {"title": "M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models", "link": "https://arxiv.org/pdf/2411.04075", "details": "C Li, Z Shangguan, Y Zhao, D Li, Y Liu, A Cohan - arXiv preprint arXiv:2411.04075, 2024", "abstract": "Existing benchmarks for evaluating foundation models mainly focus on single- document, text-only tasks. However, they often fail to fully capture the complexity of research workflows, which typically involve interpreting non-textual data and \u2026"}, {"title": "MMAD: The First-Ever Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection", "link": "https://arxiv.org/pdf/2410.09453", "details": "X Jiang, J Li, H Deng, Y Liu, BB Gao, Y Zhou, J Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In the field of industrial inspection, Multimodal Large Language Models (MLLMs) have a high potential to renew the paradigms in practical applications due to their robust language capabilities and generalization abilities. However, despite their \u2026"}]
