[{"title": "FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs", "link": "https://arxiv.org/pdf/2409.13612", "details": "B Yan, Z Zhang, L Jing, E Hossain, X Du - arXiv preprint arXiv:2409.13612, 2024", "abstract": "The rapid development of Large Vision-Language Models (LVLMs) often comes with widespread hallucination issues, making cost-effective and comprehensive assessments increasingly vital. Current approaches mainly rely on costly annotations \u2026"}, {"title": "Do Vision-Language Models Really Understand Visual Language?", "link": "https://arxiv.org/pdf/2410.00193", "details": "B Giledereli, Y Hou, Y Tu, M Sachan - arXiv preprint arXiv:2410.00193, 2024", "abstract": "Visual language is a system of communication that conveys information through symbols, shapes, and spatial arrangements. Diagrams are a typical example of a visual language depicting complex concepts and their relationships in the form of an \u2026"}, {"title": "Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models", "link": "https://arxiv.org/pdf/2409.12512", "details": "J Rao, X Liu, Z Lin, L Ding, J Li, D Tao - arXiv preprint arXiv:2409.12512, 2024", "abstract": "Knowledge distillation (KD) is a technique that compresses large teacher models by training smaller student models to mimic them. The success of KD in auto-regressive language models mainly relies on Reverse KL for mode-seeking and student \u2026"}, {"title": "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models", "link": "https://arxiv.org/pdf/2409.13592", "details": "A Nandy, Y Agarwal, A Patwa, MM Das, A Bansal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Understanding satire and humor is a challenging task for even current Vision- Language models. In this paper, we propose the challenging tasks of Satirical Image Detection (detecting whether an image is satirical), Understanding (generating the \u2026"}, {"title": "Training language models to self-correct via reinforcement learning", "link": "https://arxiv.org/pdf/2409.12917", "details": "A Kumar, V Zhuang, R Agarwal, Y Su, JD Co-Reyes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a \u2026"}, {"title": "Zero-Shot Multi-Hop Question Answering via Monte-Carlo Tree Search with Large Language Models", "link": "https://arxiv.org/pdf/2409.19382", "details": "S Lee, J Shin, Y Ahn, S Seo, O Kwon, KE Kim - arXiv preprint arXiv:2409.19382, 2024", "abstract": "Recent advances in large language models (LLMs) have significantly impacted the domain of multi-hop question answering (MHQA), where systems are required to aggregate information and infer answers from disparate pieces of text. However, the \u2026"}, {"title": "Top-down Activity Representation Learning for Video Question Answering", "link": "https://arxiv.org/pdf/2409.07748", "details": "Y Wang, S Haruta, D Zeng, J Vizcarra, M Kurokawa - arXiv preprint arXiv:2409.07748, 2024", "abstract": "Capturing complex hierarchical human activities, from atomic actions (eg, picking up one present, moving to the sofa, unwrapping the present) to contextual events (eg, celebrating Christmas) is crucial for achieving high-performance video question \u2026"}, {"title": "TART: An Open-Source Tool-Augmented Framework for Explainable Table-based Reasoning", "link": "https://arxiv.org/pdf/2409.11724", "details": "X Lu, L Pan, Y Ma, P Nakov, MY Kan - arXiv preprint arXiv:2409.11724, 2024", "abstract": "Current Large Language Models (LLMs) exhibit limited ability to understand table structures and to apply precise numerical reasoning, which is crucial for tasks such as table question answering (TQA) and table-based fact verification (TFV). To \u2026"}, {"title": "JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images", "link": "https://arxiv.org/pdf/2409.12953%3F", "details": "Z Wang, J Liu, CW Tang, H Alomari, A Sivakumar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Existing vision-language understanding benchmarks largely consist of images of objects in their usual contexts. As a consequence, recent multimodal large language models can perform well with only a shallow visual understanding by relying on \u2026"}]
