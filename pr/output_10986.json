[{"title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding", "link": "https://arxiv.org/pdf/2412.16158", "details": "C Tao, S Su, X Zhu, C Zhang, Z Chen, J Liu, W Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advance of Large Language Models (LLMs) has catalyzed the development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid modality-specific encoders, offer a promising alternative to the compositional ones \u2026"}, {"title": "Progressive Multi-granular Alignments for Grounded Reasoning in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.08125", "details": "QH Le, LH Dang, N Le, T Tran, TM Le - arXiv preprint arXiv:2412.08125, 2024", "abstract": "Existing Large Vision-Language Models (LVLMs) excel at matching concepts across multi-modal inputs but struggle with compositional concepts and high-level relationships between entities. This paper introduces Progressive multi-granular \u2026"}, {"title": "Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.06775", "details": "YL Lee, YH Tsai, WC Chiu - arXiv preprint arXiv:2412.06775, 2024", "abstract": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To \u2026"}, {"title": "Implementing Trust in Non-Small Cell Lung Cancer Diagnosis with a Conformalized Uncertainty-Aware AI Framework in Whole-Slide Images", "link": "https://www.medrxiv.org/content/10.1101/2024.12.27.24319715.full.pdf", "details": "X Zhang, T Wang, C Yan, F Najdawi, K Zhou, Y Ma\u2026 - medRxiv, 2024", "abstract": "Ensuring trustworthiness is fundamental to the development of artificial intelligence (AI) that is considered societally responsible, particularly in cancer diagnostics, where a misdiagnosis can have dire consequences. Current digital pathology AI \u2026"}, {"title": "SVM based detection of glaucoma in retinal color fundus images using RGB channel", "link": "https://www.taylorfrancis.com/chapters/edit/10.1201/9781032711157-35/svm-based-detection-glaucoma-retinal-color-fundus-images-using-rgb-channel-naresh-reddy-ganesan", "details": "BN Reddy, V Ganesan - Computer Science Engineering, 2024", "abstract": "Glaucoma is an eye disease that affects the optic nerve which is at the back of the human eye. Glaucoma can lead to vision loss and is solely responsible for blindness in humans. The symptoms are not drastic and develop slowly and tend to be hard to \u2026"}, {"title": "A Dataset of Blood Slide Images for AI-based Diagnosis of Malaria", "link": "https://www.sciencedirect.com/science/article/pii/S2352340924011521", "details": "R Nakasi, JN Nabende, JF Tusubira, A Bamundaga\u2026 - Data in Brief, 2024", "abstract": "Malaria is a major public health challenge in sub-Saharan Africa. Timely and accurate diagnosis of malaria is vital to reduce the caseload and mortality rates associated with malaria. The use of microscopy in malaria screening is the gold \u2026"}, {"title": "OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large Language Model and its Omni-Extensions", "link": "https://arxiv.org/pdf/2412.06693%3F", "details": "YK Zhang, XX Zhong, S Lu, QG Chen, DC Zhan, HJ Ye - arXiv preprint arXiv \u2026, 2024", "abstract": "The rapid advancements in Large Language Models (LLMs) have significantly expanded their applications, ranging from multilingual support to domain-specific tasks and multimodal integration. In this paper, we present OmniEvalKit, a novel \u2026"}, {"title": "HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for Vision-Language Models", "link": "https://arxiv.org/pdf/2412.08378", "details": "S Zhu, W Dong, J Song, Y Guo, B Zheng - arXiv preprint arXiv:2412.08378, 2024", "abstract": "Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into \u2026"}, {"title": "Retaining and Enhancing Pre-trained Knowledge in Vision-Language Models with Prompt Ensembling", "link": "https://arxiv.org/pdf/2412.07077", "details": "D Kim, Y Jo, M Lee, T Kim - arXiv preprint arXiv:2412.07077, 2024", "abstract": "The advancement of vision-language models, particularly the Contrastive Language- Image Pre-training (CLIP) model, has revolutionized the field of machine learning by enabling robust zero-shot learning capabilities. These capabilities allow models to \u2026"}]
