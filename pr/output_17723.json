[{"title": "Boosting multi-demographic federated learning for chest radiograph analysis using general-purpose self-supervised representations", "link": "https://www.sciencedirect.com/science/article/pii/S305057712500026X", "details": "M Lotfinia, A Tayebiarasteh, S Samiei, M Joodaki\u2026 - European Journal of \u2026, 2025", "abstract": "Background Reliable artificial intelligence (AI) models for medical image analysis often depend on large and diverse labeled datasets. Federated learning (FL) offers a decentralized and privacy-preserving approach to training but struggles in highly non \u2026"}, {"title": "Generative priors for MRI reconstruction trained from magnitude-only images using phase augmentation", "link": "https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2024.0323", "details": "G Luo, X Wang, M Blumenthal, M Schilling\u2026 - Philosophical Transactions \u2026, 2025", "abstract": "In this work, we present a workflow to construct generic and robust generative image priors from magnitude-only images. The priors can then be used for regularization in reconstruction to improve image quality. The workflow begins with the preparation of \u2026"}, {"title": "Hierarchical Self-Prompting SAM: A Prompt-Free Medical Image Segmentation Framework", "link": "https://arxiv.org/pdf/2506.02854", "details": "M Zhang, X Dai, Y Sun, J Wang, Y Yao, X Gong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Although the Segment Anything Model (SAM) is highly effective in natural image segmentation, it requires dependencies on prompts, which limits its applicability to medical imaging where manual prompts are often unavailable. Existing efforts to fine \u2026", "entry_id": "http://arxiv.org/abs/2506.02854v1", "updated": "2025-06-03 13:23:33", "published": "2025-06-03 13:23:33", "authors": "Mengmeng Zhang;Xingyuan Dai;Yicheng Sun;Jing Wang;Yueyang Yao;Xiaoyan Gong;Fuze Cong;Feiyue Wang;Yisheng Lv", "summary": "Although the Segment Anything Model (SAM) is highly effective in natural\nimage segmentation, it requires dependencies on prompts, which limits its\napplicability to medical imaging where manual prompts are often unavailable.\nExisting efforts to fine-tune SAM for medical segmentation typically struggle\nto remove this dependency. We propose Hierarchical Self-Prompting SAM\n(HSP-SAM), a novel self-prompting framework that enables SAM to achieve strong\nperformance in prompt-free medical image segmentation. Unlike previous\nself-prompting methods that remain limited to positional prompts similar to\nvanilla SAM, we are the first to introduce learning abstract prompts during the\nself-prompting process. This simple and intuitive self-prompting framework\nachieves superior performance on classic segmentation tasks such as polyp and\nskin lesion segmentation, while maintaining robustness across diverse medical\nimaging modalities. Furthermore, it exhibits strong generalization to unseen\ndatasets, achieving improvements of up to 14.04% over previous state-of-the-art\nmethods on some challenging benchmarks. These results suggest that abstract\nprompts encapsulate richer and higher-dimensional semantic information compared\nto positional prompts, thereby enhancing the model's robustness and\ngeneralization performance. All models and codes will be released upon\nacceptance.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV", "links": "http://arxiv.org/abs/2506.02854v1;http://arxiv.org/pdf/2506.02854v1", "pdf_url": "http://arxiv.org/pdf/2506.02854v1"}]
