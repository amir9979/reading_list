[{"title": "Revealing the Pragmatic Dilemma for Moral Reasoning Acquisition in Language Models", "link": "https://arxiv.org/pdf/2502.16600", "details": "G Liu, L Jiang, X Zhang, KM Johnson - arXiv preprint arXiv:2502.16600, 2025", "abstract": "Ensuring that Large Language Models (LLMs) return just responses which adhere to societal values is crucial for their broader application. Prior research has shown that LLMs often fail to perform satisfactorily on tasks requiring moral cognizance, such as \u2026"}, {"title": "GenTool: Enhancing Tool Generalization in Language Models through Zero-to-One and Weak-to-Strong Simulation", "link": "https://arxiv.org/pdf/2502.18990", "details": "J He, J Neville, M Wan, L Yang, H Liu, X Xu, X Song\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Models (LLMs) can enhance their capabilities as AI assistants by integrating external tools, allowing them to access a wider range of information. While recent LLMs are typically fine-tuned with tool usage examples during \u2026"}, {"title": "Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI", "link": "https://arxiv.org/pdf/2502.16691", "details": "E Noh, J Baek - arXiv preprint arXiv:2502.16691, 2025", "abstract": "Recent research has increasingly focused on training large language models (LLMs) using federated learning, known as FedLLM. However, responsible AI (RAI), which aims to ensure safe responses, remains underexplored in the context of FedLLM. In \u2026"}, {"title": "IPO: Your Language Model is Secretly a Preference Classifier", "link": "https://arxiv.org/pdf/2502.16182", "details": "S Garg, A Singh, S Singh, P Chopra - arXiv preprint arXiv:2502.16182, 2025", "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. While it enables LLMs to achieve human-level alignment, it often incurs significant \u2026"}, {"title": "Comprehensive analysis of transparency and accessibility of chatgpt, deepseek, and other sota large language models", "link": "https://arxiv.org/pdf/2502.18505", "details": "R Sapkota, S Raza, M Karkee - arXiv preprint arXiv:2502.18505, 2025", "abstract": "Despite increasing discussions on open-source Artificial Intelligence (AI), existing research lacks a discussion on the transparency and accessibility of state-of-the-art (SoTA) Large Language Models (LLMs). The Open Source Initiative (OSI) has \u2026"}, {"title": "Improving LLM General Preference Alignment via Optimistic Online Mirror Descent", "link": "https://arxiv.org/pdf/2502.16852", "details": "Y Zhang, D Yu, T Ge, L Song, Z Zeng, H Mi, N Jiang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated remarkable effectiveness in aligning large language models (LLMs) with human preferences. Many existing alignment approaches rely on the Bradley-Terry (BT) model \u2026"}, {"title": "From System 1 to System 2: A Survey of Reasoning Large Language Models", "link": "https://arxiv.org/pdf/2502.17419", "details": "ZZ Li, D Zhang, ML Zhang, J Zhang, Z Liu, Y Yao, H Xu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more \u2026"}, {"title": "Towards label-only membership inference attack against pre-trained large language models", "link": "https://arxiv.org/pdf/2502.18943", "details": "Y He, B Li, L Liu, Z Ba, W Dong, Y Li, Z Qin, K Ren\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Membership Inference Attacks (MIAs) aim to predict whether a data sample belongs to the model's training set or not. Although prior research has extensively explored MIAs in Large Language Models (LLMs), they typically require accessing to complete \u2026"}, {"title": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models", "link": "https://arxiv.org/pdf/2502.14529", "details": "Z Zhou, Z Li, J Zhang, Y Zhang, K Wang, Y Liu, Q Guo - arXiv preprint arXiv \u2026, 2025", "abstract": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated remarkable real-world capabilities, effectively collaborating to complete complex tasks. While these systems are designed with safety mechanisms, such as rejecting \u2026"}]
