[{"title": "Impact of **large language models** on trial-to-patient matching efficiency.", "link": "https://ascopubs.org/doi/abs/10.1200/JCO.2025.43.16_suppl.e13600", "details": "NT Rich, S Salzman, G Altay, K Nagpal, V Shetye\u2026 - 2025", "abstract": "\u2026 To enhance efficiency, we explored the incorporation of a retrieval augmented generation (RAG) **question** **answering** system based on **large** **language** **models** (LLMs) to evaluate additional unstructured data and prioritize patients for review. Methods \u2026"}, {"title": "How valuable are the **questions** and **answers** generated by **large language models** in oral and maxillofacial surgery?", "link": "https://journals.plos.org/plosone/article%3Fid%3D10.1371/journal.pone.0322529", "details": "K Kim, SB Mun, YJ Kim, BC Kim, KG Kim - PLoS One, 2025", "abstract": "\u2026 In this study, we aim to evaluate the ability of **large** **language** **models** (LLM) to generate **questions** and **answers** in oral and maxillofacial \u2026 A flow diagram showing the process by which **large** **language** **models** generate and **answer** \u2026"}, {"title": "\u2026 versus manual review in **clinical** data abstraction and deduction from real-world **medical** records of patients with melanoma for **clinical** trial eligibility assessment.", "link": "https://ascopubs.org/doi/abs/10.1200/JCO.2025.43.16_suppl.1571", "details": "C Vecchio, S Braley, LB Kennedy, J Isaacs, TG Truong\u2026 - 2025", "abstract": "\u2026 **Large** **language** **models** (LLMs), have shown promise in natural language understanding, and automating chart review and abstraction \u2026 **questions** **answered** by each of the research nurses as well as Synapsis AI. The **questions** addressed \u2026"}, {"title": "BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain", "link": "https://arxiv.org/pdf/2505.22240", "details": "Y Kim, Y Abdulle, H Wu - arXiv preprint arXiv:2505.22240, 2025", "abstract": "\u2026 Recent advances in **large** **language** **models** (LLMs) and **Question** **Answering** (QA) systems have shifted the focus from simple factoid retrieval \u2026 suited for evaluating **large** **language** **models** (LLMs) on tasks requiring multi-step reasoning and \u2026", "entry_id": "http://arxiv.org/abs/2505.22240v1", "updated": "2025-05-28 11:19:01", "published": "2025-05-28 11:19:01", "authors": "Yunsoo Kim;Yusuf Abdulle;Honghan Wu", "summary": "Biomedical reasoning often requires traversing interconnected relationships\nacross entities such as drugs, diseases, and proteins. Despite the increasing\nprominence of large language models (LLMs), existing benchmarks lack the\nability to evaluate multi-hop reasoning in the biomedical domain, particularly\nfor queries involving one-to-many and many-to-many relationships. This gap\nleaves the critical challenges of biomedical multi-hop reasoning underexplored.\nTo address this, we introduce BioHopR, a novel benchmark designed to evaluate\nmulti-hop, multi-answer reasoning in structured biomedical knowledge graphs.\nBuilt from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop\nreasoning tasks that reflect real-world biomedical complexities.\n  Evaluations of state-of-the-art models reveal that O3-mini, a proprietary\nreasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on\n2-hop tasks, outperforming proprietary models such as GPT4O and open-source\nbiomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all\nmodels exhibit significant declines in multi-hop performance, underscoring the\nchallenges of resolving implicit reasoning steps in the biomedical domain. By\naddressing the lack of benchmarks for multi-hop reasoning in biomedical domain,\nBioHopR sets a new standard for evaluating reasoning capabilities and\nhighlights critical gaps between proprietary and open-source models while\npaving the way for future advancements in biomedical LLMs.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.22240v1;http://arxiv.org/pdf/2505.22240v1", "pdf_url": "http://arxiv.org/pdf/2505.22240v1"}, {"title": "Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning", "link": "https://arxiv.org/pdf/2505.21958", "details": "Q Zhong, L Ding, F Liao, J Liu, B Du, D Tao - arXiv preprint arXiv:2505.21958, 2025", "abstract": "\u2026 Abstract\u2014Domain-specific instruction-tuning has become the defacto standard for improving the performance of **large** **language** **models** (LLMs) in specialized applications, eg, **medical** **question** **answering**. Since the instruction-tuning dataset \u2026", "entry_id": "http://arxiv.org/abs/2505.21958v1", "updated": "2025-05-28 04:18:24", "published": "2025-05-28 04:18:24", "authors": "Qihuang Zhong;Liang Ding;Fei Liao;Juhua Liu;Bo Du;Dacheng Tao", "summary": "Domain-specific instruction-tuning has become the defacto standard for\nimproving the performance of large language models (LLMs) in specialized\napplications, e.g., medical question answering. Since the instruction-tuning\ndataset might contain redundant or low-quality data, data selection (DS) is\nusually required to maximize the data efficiency. Despite the successes in the\ngeneral domain, current DS methods often struggle to select the desired data\nfor domain-specific instruction-tuning. One of the main reasons is that they\nneglect the impact of knowledge conflicts, i.e., the discrepancy between LLMs'\npretrained knowledge and context knowledge of instruction data, which could\ndamage LLMs' prior abilities and lead to hallucination. To this end, we propose\na simple-yet-effective Knowledge-aware Data Selection (namely KDS) framework to\nselect the domain-specific instruction-tuning data that meets LLMs' actual\nneeds. The core of KDS is to leverage two knowledge-aware metrics for\nquantitatively measuring knowledge conflicts from two aspects: context-memory\nknowledge alignment and intra-memory knowledge consistency. By filtering the\ndata with large knowledge conflicts and sampling the high-quality and diverse\ndata, KDS can effectively stimulate the LLMs' abilities and achieve better\ndomain-specific performance. Taking the medical domain as the testbed, we\nconduct extensive experiments and empirically prove that KDS surpasses the\nother baselines and brings significant and consistent performance gains among\nall LLMs. More encouragingly, KDS effectively improves the model generalization\nand alleviates the hallucination problem.", "comment": null, "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL", "links": "http://arxiv.org/abs/2505.21958v1;http://arxiv.org/pdf/2505.21958v1", "pdf_url": "http://arxiv.org/pdf/2505.21958v1"}, {"title": "Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review", "link": "https://arxiv.org/pdf/2505.22280", "details": "Z Xu, H Ma, G Zhang, Y Ding, C Weng, Y Peng - arXiv preprint arXiv:2505.22280, 2025", "abstract": "\u2026 and **large** **language** **models** (LLMs). These modern approaches employ self-supervised pretraining and instruct-tuning (Rohanian et al.\u2026 Appraisal and **Question** **Answering**. Current resources often rely on general corpora rather than those specifically \u2026", "entry_id": "http://arxiv.org/abs/2505.22280v1", "updated": "2025-05-28 12:17:01", "published": "2025-05-28 12:17:01", "authors": "Zihan Xu;Haotian Ma;Gongbo Zhang;Yihao Ding;Chunhua Weng;Yifan Peng", "summary": "Evidence-based medicine (EBM) is at the forefront of modern healthcare,\nemphasizing the use of the best available scientific evidence to guide clinical\ndecisions. Due to the sheer volume and rapid growth of medical literature and\nthe high cost of curation, there is a critical need to investigate Natural\nLanguage Processing (NLP) methods to identify, appraise, synthesize, summarize,\nand disseminate evidence in EBM. This survey presents an in-depth review of 129\nresearch studies on leveraging NLP for EBM, illustrating its pivotal role in\nenhancing clinical decision-making processes. The paper systematically explores\nhow NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,\nApply, and Assess. The review not only identifies current limitations within\nthe field but also proposes directions for future research, emphasizing the\npotential for NLP to revolutionize EBM by refining evidence extraction,\nevidence synthesis, appraisal, summarization, enhancing data comprehensibility,\nand facilitating a more efficient clinical workflow.", "comment": "Accepted by ACL 2025 Findings", "journal_ref": null, "primary_category": "cs.CL", "categories": "cs.CL;cs.AI", "links": "http://arxiv.org/abs/2505.22280v1;http://arxiv.org/pdf/2505.22280v1", "pdf_url": "http://arxiv.org/pdf/2505.22280v1"}, {"title": "Assessing Bias in AI Chatbot Responses", "link": "https://dzone.com/articles/assessing-bias-in-ai-chatbot-responses", "details": "B Madupati", "abstract": "\u2026 Thereby generalizing to various content generation, **question** **answering** , and even programming possibilities. With the introduction of GPT\u2026 The generation of text by chatbots, especially those that are backed up by **large** **language** **models** such \u2026"}, {"title": "Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding", "link": "https://arxiv.org/pdf/2505.21908", "details": "H Wang, Z Wu, G Kolar, H Korsapati, B Bartlett, B Hull\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "\u2026 **Large** **Language** **Models** (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private **clinical** \u2026 Chain-ofthought prompting elicits reasoning in **large** **language** **models**. Advances in neural \u2026", "entry_id": "http://arxiv.org/abs/2505.21908v1", "updated": "2025-05-28 02:54:07", "published": "2025-05-28 02:54:07", "authors": "Hanyin Wang;Zhenbang Wu;Gururaj Kolar;Hariprasad Korsapati;Brian Bartlett;Bryan Hull;Jimeng Sun", "summary": "Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement\nand operations but require labor-intensive assignment. Large Language Models\n(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of\nthe task: pretraining corpora rarely contain private clinical or billing data.\nWe introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)\nfor automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained\nwith Group Relative Policy Optimization (GRPO) using rule-based rewards,\nDRG-Sapphire introduces a series of RL enhancements to address domain-specific\nchallenges not seen in previous mathematical tasks. Our model achieves\nstate-of-the-art accuracy on the MIMIC-IV benchmark and generates\nphysician-validated reasoning for DRG assignments, significantly enhancing\nexplainability. Our study further sheds light on broader challenges of applying\nRL to knowledge-intensive, OOD tasks. We observe that RL performance scales\napproximately linearly with the logarithm of the number of supervised\nfine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally\nconstrained by the domain knowledge encoded in the base model. For OOD tasks\nlike DRG coding, strong RL performance requires sufficient knowledge infusion\nprior to RL. Consequently, scaling SFT may be more effective and\ncomputationally efficient than scaling RL alone for such tasks.", "comment": null, "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI", "links": "http://arxiv.org/abs/2505.21908v1;http://arxiv.org/pdf/2505.21908v1", "pdf_url": "http://arxiv.org/pdf/2505.21908v1"}, {"title": "Demystifying machine learning applications in general medicine", "link": "https://hal.science/tel-05088550v1/file/thesis.pdf", "details": "A Mannion - 2025", "abstract": "\u2026 In recent times, large-scale GPT style models known as LLMs ( **Large** **Language** **Models** ) have brought about a paradigm shift in NLP \u2026 given supervised task - usually NER, relation extraction or **question** **answering**. While the establishment of a \u2026"}]
