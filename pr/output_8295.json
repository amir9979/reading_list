[{"title": "Analysing the Residual Stream of Language Models Under Knowledge Conflicts", "link": "https://arxiv.org/pdf/2410.16090", "details": "Y Zhao, X Du, G Hong, AP Gema, A Devoto, H Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model \u2026"}, {"title": "Belief in the Machine: Investigating Epistemological Blind Spots of Language Models", "link": "https://arxiv.org/pdf/2410.21195", "details": "M Suzgun, T Gur, F Bianchi, DE Ho, T Icard, D Jurafsky\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As language models (LMs) become integral to fields like healthcare, law, and journalism, their ability to differentiate between fact, belief, and knowledge is essential for reliable decision-making. Failure to grasp these distinctions can lead to \u2026"}, {"title": "Applying sparse autoencoders to unlearn knowledge in language models", "link": "https://arxiv.org/pdf/2410.19278", "details": "E Farrell, YT Lau, A Conmy - arXiv preprint arXiv:2410.19278, 2024", "abstract": "We investigate whether sparse autoencoders (SAEs) can be used to remove knowledge from language models. We use the biology subset of the Weapons of Mass Destruction Proxy dataset and test on the gemma-2b-it and gemma-2-2b-it \u2026"}, {"title": "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics", "link": "https://arxiv.org/pdf/2410.21272", "details": "Y Nikankin, A Reusch, A Mueller, Y Belinkov - arXiv preprint arXiv:2410.21272, 2024", "abstract": "Do large language models (LLMs) solve reasoning tasks by learning robust generalizable algorithms, or do they memorize training data? To investigate this question, we use arithmetic reasoning as a representative task. Using causal \u2026"}, {"title": "Ensembling Finetuned Language Models for Text Classification", "link": "https://arxiv.org/pdf/2410.19889", "details": "SP Arango, M Janowski, L Purucker, A Zela, F Hutter\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Finetuning is a common practice widespread across different communities to adapt pretrained models to particular tasks. Text classification is one of these tasks for which many pretrained models are available. On the other hand, ensembles of \u2026"}, {"title": "Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in Language Models", "link": "https://openreview.net/pdf%3Fid%3DHOLs697aIx", "details": "A Dutta, YC Hsiao - NeurIPS 2024 Workshop on Open-World Agents", "abstract": "We propose a novel in-context learning algorithm for building autonomous decision- making language agents. The language agent continuously attempts to solve the same task by reasoning, acting, observing and then self-correcting each time the task \u2026"}, {"title": "CLR-Bench: Evaluating Large Language Models in College-level Reasoning", "link": "https://arxiv.org/pdf/2410.17558", "details": "J Dong, Z Hong, Y Bei, F Huang, X Wang, X Huang - arXiv preprint arXiv:2410.17558, 2024", "abstract": "Large language models (LLMs) have demonstrated their remarkable performance across various language understanding tasks. While emerging benchmarks have been proposed to evaluate LLMs in various domains such as mathematics and \u2026"}, {"title": "Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language Models", "link": "https://arxiv.org/pdf/2410.20745", "details": "Y Jin, Z Li, C Zhang, T Cao, Y Gao, P Jayarao, M Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Online shopping is a complex multi-task, few-shot learning problem with a wide and evolving range of entities, relations, and tasks. However, existing models and benchmarks are commonly tailored to specific tasks, falling short of capturing the full \u2026"}, {"title": "SWITCH: Studying with Teacher for Knowledge Distillation of Large Language Models", "link": "https://arxiv.org/pdf/2410.19503", "details": "J Koo, Y Hwang, Y Kim, T Kang, H Bae, K Jung - arXiv preprint arXiv:2410.19503, 2024", "abstract": "Despite the success of Large Language Models (LLMs), they still face challenges related to high inference costs and memory requirements. To address these issues, Knowledge Distillation (KD) has emerged as a popular method for model \u2026"}]
