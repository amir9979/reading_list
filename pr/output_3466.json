[{"title": "Self-supervised Pre-training via Multi-view Graph Information Bottleneck for Molecular Property Prediction", "link": "https://pubmed.ncbi.nlm.nih.gov/38959149/", "details": "X Zang, J Zhang, B Tang - IEEE journal of biomedical and health informatics", "abstract": "Molecular representation learning has remarkably accelerated the development of drug analysis and discovery. It implements machine learning methods to encode molecule embeddings for diverse downstream drug-related tasks. Due to the scarcity \u2026"}, {"title": "Hybrid Summarization of Medical Records for Predicting Length of Stay in the Intensive Care Unit", "link": "https://www.mdpi.com/2076-3417/14/13/5809", "details": "S Rhazzafe, F Caraffini, S Colreavy-Donnelly, Y Dhassi\u2026 - Applied Sciences, 2024", "abstract": "Electronic health records (EHRs) are a critical tool in healthcare and capture a wide array of patient information that can inform clinical decision-making. However, the sheer volume and complexity of EHR data present challenges for healthcare \u2026"}, {"title": "MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models", "link": "https://arxiv.org/pdf/2407.02775", "details": "Y Zhang, Z Yang, S Ji - arXiv preprint arXiv:2407.02775, 2024", "abstract": "Knowledge distillation is an effective technique for pre-trained language model compression. Although existing knowledge distillation methods perform well for the most typical model BERT, they could be further improved in two aspects: the relation \u2026"}]
