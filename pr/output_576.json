'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Best Practices and Lessons Learned on Synthetic Data f'
[{"title": "Can 3D Vision-Language Models Truly Understand Natural Language?", "link": "https://arxiv.org/pdf/2403.14760", "details": "W Deng, R Ding, J Yang, J Liu, Y Li, X Qi, E Ngai - arXiv preprint arXiv:2403.14760, 2024", "abstract": "Rapid advancements in 3D vision-language (3D-VL) tasks have opened up new avenues for human interaction with embodied agents or robots using natural language. Despite this progress, we find a notable limitation: existing 3D-VL models \u2026"}, {"title": "LLMs in Biomedicine: A study on clinical Named Entity Recognition", "link": "https://arxiv.org/pdf/2404.07376", "details": "M Monajatipoor, J Yang, J Stremmel, M Emami\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) demonstrate remarkable versatility in various NLP tasks but encounter distinct challenges in biomedicine due to medical language complexities and data scarcity. This paper investigates the application of LLMs in the \u2026"}, {"title": "Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning", "link": "https://arxiv.org/pdf/2403.16543", "details": "P Borchert, J De Weerdt, MF Moens - arXiv preprint arXiv:2403.16543, 2024", "abstract": "Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in few-shot relation classification. Representations of textual data extract rich information spanning the domain, entities, and relations. In this \u2026"}, {"title": "All Should Be Equal in the Eyes of LMs: Counterfactually Aware Fair Text Generation", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/29719/31235", "details": "P Banerjee, A Java, S Jandial, S Shahid\u2026 - Proceedings of the AAAI \u2026, 2024", "abstract": "Abstract Fairness in Language Models (LMs) remains a long-standing challenge, given the inherent biases in training data that can be perpetuated by models and affect the downstream tasks. Recent methods employ expensive retraining or attempt \u2026"}, {"title": "Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning of Large Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29879/31534", "details": "D Wang, L Dou, W Zhang, J Zeng, W Che - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Numerical reasoning is a vital capability for natural language processing models to understand and process numerical information in real-world scenarios. Most current methods first generate the Intermediate Meaning Representations (IMRs) of \u2026"}, {"title": "Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models", "link": "https://arxiv.org/pdf/2403.15268", "details": "H Liao, S He, Y Xu, Y Zhang, K Liu, S Liu, J Zhao - arXiv preprint arXiv:2403.15268, 2024", "abstract": "Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs). However, the former depends on external \u2026"}]
