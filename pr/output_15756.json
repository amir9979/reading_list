[{"title": "Atoxia: Red-teaming Large Language Models with Target Toxic Answers", "link": "https://aclanthology.org/anthology-files/pdf/findings/2025.findings-naacl.179.pdf", "details": "Y Du, Z Li, P Cheng, X Wan, A Gao", "abstract": "Despite the substantial advancements in artificial intelligence, large language models (LLMs) remain being challenged by generation safety. With adversarial jailbreaking prompts, one can effortlessly induce LLMs to output harmful content \u2026"}, {"title": "Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data", "link": "https://aclanthology.org/anthology-files/pdf/naacl/2025.naacl-long.191.pdf", "details": "JZMMT Li, B Van Durme, D Khashabi", "abstract": "To trust the fluent generations of large language models (LLMs), humans must be able to verify their correctness against trusted external sources. Recent efforts, such as providing citations via retrieved documents or post-hoc provenance, enhance \u2026"}, {"title": "Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study", "link": "https://arxiv.org/pdf/2504.16414", "details": "M Khodadad, AS Kasmaee, M Astaraki, N Sherck\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a \u2026"}, {"title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language Models", "link": "https://arxiv.org/pdf/2504.14245", "details": "Y Ji, Y Hong, J Zhan, H Chen, H Zhu, W Wang, L Zhang\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Progress in image generation raises significant public security concerns. We argue that fake image detection should not operate as a\" black box\". Instead, an ideal approach must ensure both strong generalization and transparency. Recent \u2026"}, {"title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models", "link": "https://arxiv.org/pdf/2504.02902", "details": "L Huang, D Li, H Liu, L Cheng - arXiv preprint arXiv:2504.02902, 2025", "abstract": "Large Language Models (LLMs) have demonstrated remarkable self-improvement capabilities, whereby models iteratively revise their outputs through self-generated feedback. While this reflective mechanism has shown promise in enhancing task \u2026"}, {"title": "Biases in Opinion Dynamics in Multi-Agent Systems of Large Language Models: A Case Study on Funding Allocation", "link": "https://aclanthology.org/anthology-files/pdf/naacl/2025.naacl-findings.101.pdf", "details": "P Cisneros-Velarde", "abstract": "We study the evolution of opinions inside a population of interacting large language models (LLMs). Every LLM needs to decide how much funding to allocate to an item with three initial possibilities: full, partial, or no funding. We identify biases that drive \u2026"}, {"title": "Establishing Reliability Metrics for Reward Models in Large Language Models", "link": "https://arxiv.org/pdf/2504.14838", "details": "Y Chen, Y Liu, X Wang, Q Yu, G Huzhang, A Zeng\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The reward model (RM) that represents human preferences plays a crucial role in optimizing the outputs of large language models (LLMs), eg, through reinforcement learning from human feedback (RLHF) or rejection sampling. However, a long \u2026"}, {"title": "ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data", "link": "https://arxiv.org/pdf/2504.16628", "details": "H Gu, H Wang, Y Mei, M Zhang, Y Jin - arXiv preprint arXiv:2504.16628, 2025", "abstract": "Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm \u2026"}, {"title": "Where is the answer? An empirical study of positional bias for parametric knowledge extraction in language model", "link": "https://aclanthology.org/anthology-files/pdf/naacl/2025.naacl-long.58.pdf", "details": "K Saito, CY Lee, K Sohn, Y Ushiku", "abstract": "Abstract Language model (LM) stores diverse factual knowledge in their parameters, which is learned during self-supervised training on unlabeled documents and is made extractable by instruction-tuning. For knowledge-intensive tasks, it is essential \u2026"}]
