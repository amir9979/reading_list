[{"title": "Commonsense Reasoning for Legged Robot Adaptation with Vision-Language Models", "link": "https://arxiv.org/pdf/2407.02666", "details": "AS Chen, AM Lessing, A Tang, G Chada, L Smith\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Legged robots are physically capable of navigating a diverse variety of environments and overcoming a wide range of obstructions. For example, in a search and rescue mission, a legged robot could climb over debris, crawl through gaps, and navigate \u2026"}, {"title": "An exploratory study of self-supervised pre-training on partially supervised multi-label classification on chest X-ray images", "link": "https://www.sciencedirect.com/science/article/pii/S156849462400629X", "details": "N Dong, M Kampffmeyer, H Su, E Xing - Applied Soft Computing, 2024", "abstract": "This paper serves as the first empirical study on self-supervised pre-training on partially supervised learning, an emerging yet unexplored learning paradigm with missing annotations. This is particularly important in the medical imaging domain \u2026"}, {"title": "MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context", "link": "https://arxiv.org/pdf/2407.02730", "details": "Z Gu, C Yin, F Liu, P Zhang - arXiv preprint arXiv:2407.02730, 2024", "abstract": "Large Vision Language Models (LVLMs) have recently achieved superior performance in various tasks on natural image and text data, which inspires a large amount of studies for LVLMs fine-tuning and training. Despite their advancements \u2026"}, {"title": "InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output", "link": "https://arxiv.org/pdf/2407.03320", "details": "P Zhang, X Dong, Y Zang, Y Cao, R Qian, L Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text- image comprehension and composition applications, achieving GPT-4V level \u2026"}, {"title": "Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment", "link": "https://arxiv.org/pdf/2407.03051", "details": "J Lee, S Park, S Hong, M Kim, DS Chang, J Choi - arXiv preprint arXiv:2407.03051, 2024", "abstract": "The rapid advancement of large language models (LLMs) has facilitated their transformation into conversational chatbots that can grasp contextual nuances and generate pertinent sentences, closely mirroring human values through advanced \u2026"}, {"title": "Investigating Decoder-only Large Language Models for Speech-to-text Translation", "link": "https://arxiv.org/pdf/2407.03169", "details": "CW Huang, H Lu, H Gong, H Inaguma, I Kulikov\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models (LLMs), known for their exceptional reasoning capabilities, generalizability, and fluency across diverse domains, present a promising avenue for enhancing speech-related tasks. In this paper, we focus on integrating decoder-only \u2026"}, {"title": "Unicoder: Scaling code large language model via universal code", "link": "https://arxiv.org/pdf/2406.16441", "details": "T Sun, L Chai, J Yang, Y Yin, H Guo, J Liu, B Wang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various downstream natural language processing (NLP) tasks. When applying LLMs for code generation, recent works mainly focus on \u2026"}, {"title": "Meta Large Language Model Compiler: Foundation Models of Compiler Optimization", "link": "https://arxiv.org/pdf/2407.02524", "details": "C Cummins, V Seeker, D Grubisic, B Roziere\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is \u2026"}]
