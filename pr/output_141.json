'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [Energy-Efficient Sleep Apnea Detection Using a Hyperdimensio'
[{"title": "Smaller Language Models are Better Zero-shot Machine-Generated Text Detectors", "link": "https://aclanthology.org/2024.eacl-short.25.pdf", "details": "N Mireshghallah, J Mattern, S Gao, R Shokri\u2026 - Proceedings of the 18th \u2026, 2024", "abstract": "As large language models are becoming more embedded in different user-facing services, it is important to be able to distinguish between human-written and machine- generated text to verify the authenticity of news articles, product reviews, etc. Thus, in \u2026"}, {"title": "Identifying prognostic factors for survival in intensive care unit patients with SIRS or sepsis by machine learning analysis on electronic health records", "link": "https://journals.plos.org/digitalhealth/article%3Fid%3D10.1371/journal.pdig.0000459", "details": "M Mollura, D Chicco, A Paglialonga, R Barbieri - PLOS Digital Health, 2024", "abstract": "Background Systemic inflammatory response syndrome (SIRS) and sepsis are the most common causes of in-hospital death. However, the characteristics associated with the improvement in the patient conditions during the ICU stay were not fully \u2026"}, {"title": "Aligning Large and Small Language Models via Chain-of-Thought Reasoning", "link": "https://aclanthology.org/2024.eacl-long.109.pdf", "details": "L Ranaldi, A Freitas - Proceedings of the 18th Conference of the European \u2026, 2024", "abstract": "Abstract Chain-of-Thought (CoT) prompting empowersthe reasoning abilities of Large Language Models (LLMs), eliciting them to solve complexreasoning tasks in a step-wise manner. However, these capabilities appear only in models with billions of \u2026"}, {"title": "The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models", "link": "https://arxiv.org/html/2403.03942v1", "details": "A Bhaskar, D Friedman, D Chen - arXiv preprint arXiv:2403.03942, 2024", "abstract": "Prior work has found that pretrained language models (LMs) fine-tuned with different random seeds can achieve similar in-domain performance but generalize differently on tests of syntactic generalization. In this work, we show that, even within a single \u2026"}]
