'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Investigating Regularization of Self-Play Language Mod'
[{"title": "MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering", "link": "https://arxiv.org/pdf/2403.19116", "details": "C Guan, M Huang, P Zhang - arXiv preprint arXiv:2403.19116, 2024", "abstract": "In today's fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested \u2026"}, {"title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models", "link": "https://arxiv.org/pdf/2403.18814", "details": "Y Li, Y Zhang, C Wang, Z Zhong, Y Chen, R Chu, S Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared \u2026"}, {"title": "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models", "link": "https://arxiv.org/pdf/2404.02575", "details": "H Chae, Y Kim, S Kim, KT Ong, B Kwak, M Kim, S Kim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Algorithmic reasoning refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of reasoning steps towards the solution. Such nature of algorithmic reasoning makes it a challenge for large \u2026"}, {"title": "VLRM: Vision-Language Models act as Reward Models for Image Captioning", "link": "https://arxiv.org/pdf/2404.01911", "details": "M Dzabraev, A Kunitsyn, A Ivaniuta - arXiv preprint arXiv:2404.01911, 2024", "abstract": "In this work, we present an unsupervised method for enhancing an image captioning model (in our case, BLIP2) using reinforcement learning and vision-language models like CLIP and BLIP2-ITM as reward models. The RL-tuned model is able to \u2026"}, {"title": "Conceptual and Unbiased Reasoning in Language Models", "link": "https://arxiv.org/pdf/2404.00205", "details": "B Zhou, H Zhang, S Chen, D Yu, H Wang, B Peng\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Conceptual reasoning, the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on large language models' capability to perform conceptual reasoning. In this work, we \u2026"}, {"title": "Med42--Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches", "link": "https://arxiv.org/pdf/2404.14779", "details": "C Christophe, PK Kanithi, P Munjal, T Raha, N Hayat\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "This study presents a comprehensive analysis and comparison of two predominant fine-tuning methodologies-full-parameter fine-tuning and parameter-efficient tuning- within the context of medical Large Language Models (LLMs). We developed and \u2026"}, {"title": "Graph learning with label attention and hyperbolic embedding for temporal event prediction in healthcare", "link": "https://www.sciencedirect.com/science/article/pii/S0925231224005071", "details": "U Naseem, S Thapa, Q Zhang, S Wang, J Rashid, L Hu\u2026 - Neurocomputing, 2024", "abstract": "The digitization of healthcare systems has led to the proliferation of electronic health records (EHRs), serving as comprehensive repositories of patient information. However, the vast volume and complexity of EHR data present challenges in \u2026"}, {"title": "Learning Transferable Negative Prompts for Out-of-Distribution Detection", "link": "https://arxiv.org/pdf/2404.03248", "details": "T Li, G Pang, X Bai, W Miao, J Zheng - arXiv preprint arXiv:2404.03248, 2024", "abstract": "Existing prompt learning methods have shown certain capabilities in Out-of- Distribution (OOD) detection, but the lack of OOD images in the target dataset in their training can lead to mismatches between OOD images and In-Distribution (ID) \u2026"}, {"title": "Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for Text-to-SQL", "link": "https://arxiv.org/pdf/2404.12560", "details": "DG Thorpe, AJ Duberstein, IA Kinsey - arXiv preprint arXiv:2404.12560, 2024", "abstract": "The current state-of-the-art (SOTA) for automated text-to-SQL still falls well short of expert human performance as measured by execution accuracy (EX) on the BIRD- SQL benchmark. The most accurate methods are also slow and expensive. To \u2026"}]
