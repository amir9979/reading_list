[{"title": "Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization", "link": "https://arxiv.org/pdf/2410.09302", "details": "G Liu, K Ji, R Zheng, Z Wu, C Dun, Q Gu, L Yan - arXiv preprint arXiv:2410.09302, 2024", "abstract": "Reinforcement Learning (RL) plays a crucial role in aligning large language models (LLMs) with human preferences and improving their ability to perform complex tasks. However, current approaches either require significant computational resources due \u2026"}, {"title": "Are Expert-Level Language Models Expert-Level Annotators?", "link": "https://arxiv.org/pdf/2410.03254", "details": "YM Tseng, WL Chen, CC Chen, HH Chen - arXiv preprint arXiv:2410.03254, 2024", "abstract": "Data annotation refers to the labeling or tagging of textual data with relevant information. A large body of works have reported positive results on leveraging LLMs as an alternative to human annotators. However, existing studies focus on classic \u2026"}, {"title": "Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models", "link": "https://arxiv.org/pdf/2410.02681%3F", "details": "S Wang, Y Li, H Wei - arXiv preprint arXiv:2410.02681, 2024", "abstract": "Confidence calibration is critical for the safe deployment of machine learning models in the real world. However, such issue in vision-language models like CLIP, particularly after fine-tuning, has not been fully addressed. In this work, we \u2026"}, {"title": "Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models", "link": "https://arxiv.org/pdf/2410.12011", "details": "K Tatariya, V Araujo, T Bauwens, M de Lhoneux - arXiv preprint arXiv:2410.12011, 2024", "abstract": "Pixel-based language models have emerged as a compelling alternative to subword- based language modelling, particularly because they can represent virtually any script. PIXEL, a canonical example of such a model, is a vision transformer that has \u2026"}, {"title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ", "link": "https://arxiv.org/pdf/2410.09613", "details": "A Poulis, E Tsalapati, M Koubarakis - arXiv preprint arXiv:2410.09613, 2024", "abstract": "Recent advancements in transformer-based language models have sparked research into their logical reasoning capabilities. Most of the benchmarks used to evaluate these models are simple: generated from short (fragments of) first-order \u2026"}, {"title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models", "link": "https://arxiv.org/pdf/2410.01335", "details": "L Bandarkar, B Muller, P Yuvraj, R Hou, N Singhal\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large \u2026"}, {"title": "MojoBench: Language Modeling and Benchmarks for Mojo", "link": "https://arxiv.org/pdf/2410.17736", "details": "N Raihan, J Santos, M Zampieri - arXiv preprint arXiv:2410.17736, 2024", "abstract": "The recently introduced Mojo programming language (PL) by Modular, has received significant attention in the scientific community due to its claimed significant speed boost over Python. Despite advancements in code Large Language Models (LLMs) \u2026"}, {"title": "Large Language Models for Clinical Text Cleansing Enhance Medical Concept Normalization", "link": "https://ieeexplore.ieee.org/iel8/6287639/6514899/10703053.pdf", "details": "A Abdulnazar, R Roller, S Schulz, M Kreuzthaler - IEEE Access, 2024", "abstract": "Most clinical information is only available as free text. Large language models (LLMs) are increasingly applied to clinical data to streamline communication, enhance the accuracy of clinical documentation, and ultimately improve healthcare \u2026"}, {"title": "Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback", "link": "https://arxiv.org/pdf/2410.05434", "details": "S Choudhury, P Sodhi - arXiv preprint arXiv:2410.05434, 2024", "abstract": "While large language models (LLMs) show impressive decision-making abilities, current methods lack a mechanism for automatic self-improvement from errors during task execution. We propose LEAP, an iterative fine-tuning framework that continually \u2026"}]
