[{"title": "Federated Koopman-Reservoir Learning for Large-Scale Multivariate Time-Series Anomaly Detection", "link": "https://arxiv.org/pdf/2503.11255", "details": "LT Le, TA Nguyen, H Shu, S Seneviratne, CS Hong\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The proliferation of edge devices has dramatically increased the generation of multivariate time-series (MVTS) data, essential for applications from healthcare to smart cities. Such data streams, however, are vulnerable to anomalies that signal \u2026"}, {"title": "Pre-training Enhanced Transformer for multivariate time series anomaly detection", "link": "https://www.sciencedirect.com/science/article/pii/S1566253525002441", "details": "C Wang, H Shi, J Hu, X Yang, J Zhang, S Du, T Li - Information Fusion, 2025", "abstract": "In a multitude of data-driven industries, the timely detection of anomalies and the assurance of service reliability have become of paramount importance. Recently, Transformer has emerged as a prominent approach in the field of multivariate time \u2026"}, {"title": "Series clustering and dynamic periodic patching-based transformer for multivariate time series forecasting", "link": "https://www.sciencedirect.com/science/article/pii/S1568494625002911", "details": "Y Wang, X Wu, J Zhang, W Wang, L Zheng, J Shang - Applied Soft Computing, 2025", "abstract": "Multivariate time series forecasting (MTSF) is widely employed in research-intensive domains, such as weather forecasting. Recently, Transformer-based models have outstanding ability to achieve SOTA performance, benefiting from its self-attention \u2026"}, {"title": "Understanding and mitigating hyperbolic dimensional collapse in graph contrastive learning", "link": "https://dl.acm.org/doi/abs/10.1145/3690624.3709249", "details": "Y Zhang, H Zhu, M Yang, J Liu, R Ying, I King\u2026 - Proceedings of the 31st \u2026, 2025", "abstract": "Learning generalizable self-supervised graph representations for downstream tasks is challenging. To this end, Contrastive Learning (CL) has emerged as a leading approach. The embeddings of CL are arranged on a hypersphere where similarity is \u2026"}, {"title": "TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification", "link": "https://arxiv.org/pdf/2503.12206", "details": "A Munir, FZ Qureshi, MH Khan, M Ali - arXiv preprint arXiv:2503.12206, 2025", "abstract": "Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot performance on image classification. However, state-of-the-art methods often rely on fine-tuning techniques like prompt learning and adapter-based tuning to optimize \u2026"}, {"title": "Preserving Angles Improves Feature Distillation of Foundation Models", "link": "https://www.researchgate.net/profile/Evelyn-Mannix/publication/386112516_Preserving_Angles_Improves_Feature_Distillation_of_Foundation_Models/links/67d4da32be849d39d679003d/Preserving-Angles-Improves-Feature-Distillation-of-Foundation-Models.pdf", "details": "EJ Mannix, L Hodgkinson, H Bondell", "abstract": "Abstract Knowledge distillation approaches compress models by training a student network using the classification outputs of a high quality teacher model, but can fail to effectively transfer the properties of computer vision foundation models from the \u2026"}]
