'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Self-AMPLIFY: Improving Small Language Models with Sel'
[{"title": "RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models", "link": "https://arxiv.org/html/2403.02271v1", "details": "S Najafi, A Fyshe - arXiv preprint arXiv:2403.02271, 2024", "abstract": "Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter- efficient fine-tuning methods that optimize input prompts or adjust a small number of \u2026"}, {"title": "Anatomical Structure-Guided Medical Vision-Language Pre-training", "link": "https://arxiv.org/html/2403.09294v1", "details": "Q Li, X Yan, J Xu, R Yuan, Y Zhang, R Feng, Q Shen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Learning medical visual representations through vision-language pre-training has reached remarkable progress. Despite the promising performance, it still faces challenges, ie, local alignment lacks interpretability and clinical relevance, and the \u2026"}, {"title": "Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods", "link": "https://arxiv.org/pdf/2403.00998", "details": "P Tsvilodub, H Wang, S Grosch, M Franke - arXiv preprint arXiv:2403.00998, 2024", "abstract": "This paper systematically compares different methods of deriving item-level predictions of language models for multiple-choice tasks. It compares scoring methods for answer options based on free generation of responses, various \u2026"}, {"title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models", "link": "https://arxiv.org/pdf/2403.08281", "details": "N Ding, Y Chen, G Cui, X Lv, R Xie, B Zhou, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three \u2026"}, {"title": "CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection", "link": "https://arxiv.org/pdf/2402.12927", "details": "SA Khan, DT Dang-Nguyen - arXiv preprint arXiv:2402.12927, 2024", "abstract": "The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a \u2026"}, {"title": "$\\texttt {COSMIC} $: Mutual Information for Task-Agnostic Summarization Evaluation", "link": "https://arxiv.org/pdf/2402.19457", "details": "M Darrin, P Formont, JCK Cheung, P Piantanida - arXiv preprint arXiv:2402.19457, 2024", "abstract": "Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks \u2026"}, {"title": "MagicClay: Sculpting Meshes With Generative Neural Fields", "link": "https://arxiv.org/pdf/2403.02460", "details": "A Barda, VG Kim, N Aigerman, AH Bermano, T Groueix - arXiv preprint arXiv \u2026, 2024", "abstract": "The recent developments in neural fields have brought phenomenal capabilities to the field of shape generation, but they lack crucial properties, such as incremental control-a fundamental requirement for artistic work. Triangular meshes, on the other \u2026"}, {"title": "Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging", "link": "https://arxiv.org/pdf/2402.14815", "details": "Y Yang, Y Liu, X Liu, A Gulhane, D Mastrodicasa\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit \u2026"}, {"title": "Generative Models for Complex Logical Reasoning over Knowledge Graphs", "link": "https://dl.acm.org/doi/pdf/10.1145/3616855.3635804", "details": "Y Liu, Y Cao, S Wang, Q Wang, G Bi - Proceedings of the 17th ACM International \u2026, 2024", "abstract": "Answering complex logical queries over knowledge graphs (KGs) is a fundamental yet challenging task. Recently, query representation has been a mainstream approach to complex logical reasoning, making the target answer and query closer \u2026"}]
