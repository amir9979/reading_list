[{"title": "Low-Rank Adaptation with Task-Relevant Feature Enhancement for Fine-tuning Language Models", "link": "https://arxiv.org/pdf/2412.09827", "details": "C Li, C Ding, K Luan, X Di - arXiv preprint arXiv:2412.09827, 2024", "abstract": "Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. LoRA is one of the most widely used methods, which assumes that the optimization process is essentially low \u2026"}, {"title": "Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models", "link": "https://arxiv.org/pdf/2412.11041", "details": "D Wu, X Lu, Y Zhao, B Qin - arXiv preprint arXiv:2412.11041, 2024", "abstract": "Although large language models (LLMs) achieve effective safety alignment at the time of release, they still face various safety challenges. A key issue is that fine- tuning often compromises the safety alignment of LLMs. To address this issue, we \u2026"}, {"title": "ReFF: Reinforcing Format Faithfulness in Language Models across Varied Tasks", "link": "https://arxiv.org/pdf/2412.09173%3F", "details": "J Yao, H Huang, Z Liu, H Wen, W Su, B Qian, Y Guo - arXiv preprint arXiv:2412.09173, 2024", "abstract": "Following formatting instructions to generate well-structured content is a fundamental yet often unmet capability for large language models (LLMs). To study this capability, which we refer to as format faithfulness, we present FormatBench, a comprehensive \u2026"}, {"title": "Too Big to Fool: Resisting Deception in Language Models", "link": "https://arxiv.org/pdf/2412.10558", "details": "MR Samsami, ML Richter, J Rodriguez, M Thakkar\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models must balance their weight-encoded knowledge with in- context information from prompts to generate accurate responses. This paper investigates this interplay by analyzing how models of varying capacities within the \u2026"}, {"title": "Rethinking Chain-of-Thought from the Perspective of Self-Training", "link": "https://arxiv.org/pdf/2412.10827", "details": "Z Wu, B Xu, R Cui, M Zhan, X Zhu, L Feng - arXiv preprint arXiv:2412.10827, 2024", "abstract": "Chain-of-thought (CoT) reasoning has emerged as an effective approach for activating latent capabilities in large language models (LLMs). We observe that CoT shares significant similarities with self-training in terms of their learning processes \u2026"}, {"title": "Do Large Language Models have Shared Weaknesses in Medical Question Answering?", "link": "https://openreview.net/pdf%3Fid%3DZjQ04tsRQl", "details": "AM Bean, K Korgul, F Krones, R McCraith, A Mahdi - Advancements In Medical \u2026, 2024", "abstract": "Large language models (LLMs) have made rapid improvement on medical benchmarks, but their unreliability remains a persistent challenge for safe real-world uses. To design for the use LLMs as a category, rather than for specific models \u2026"}, {"title": "ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models", "link": "https://arxiv.org/pdf/2412.07012", "details": "J Zhang, L Xue, L Song, J Wang, W Huang, M Shu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the rise of multimodal applications, instruction data has become critical for training multimodal language models capable of understanding complex image- based queries. Existing practices rely on powerful but costly large language models \u2026"}, {"title": "PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2412.09613", "details": "C Yang, X Dong, X Zhu, W Su, J Wang, H Tian, Z Chen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (VLMs) have been extended to understand both images and videos. Visual token compression is leveraged to reduce the considerable token length of visual inputs. To meet the needs of different tasks \u2026"}, {"title": "V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding", "link": "https://arxiv.org/pdf/2412.09616%3F", "details": "J Ge, Z Chen, J Lin, J Zhu, X Liu, J Dai, X Zhu - arXiv preprint arXiv:2412.09616, 2024", "abstract": "Vision-Language Models (VLMs) have shown promising capabilities in handling various multimodal tasks, yet they struggle in long-context scenarios, particularly in tasks involving videos, high-resolution images, or lengthy image-text documents. In \u2026"}]
