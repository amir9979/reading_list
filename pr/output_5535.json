[{"title": "R2GenCSR: Retrieving Context Samples for Large Language Model based X-ray Medical Report Generation", "link": "https://arxiv.org/pdf/2408.09743", "details": "X Wang, Y Li, F Wang, S Wang, C Li, B Jiang - arXiv preprint arXiv:2408.09743, 2024", "abstract": "Inspired by the tremendous success of Large Language Models (LLMs), existing X- ray medical report generation methods attempt to leverage large models to achieve better performance. They usually adopt a Transformer to extract the visual features of \u2026"}, {"title": "IgnitionInnovators at\" Discharge Me!\": Chain-of-Thought Instruction Finetuning Large Language Models for Discharge Summaries", "link": "https://arxiv.org/pdf/2407.17636", "details": "AQ Tang, X Zhang, MN Dinh - arXiv preprint arXiv:2407.17636, 2024", "abstract": "This paper presents our proposed approach to the Discharge Me! shared task, collocated with the 23th Workshop on Biomedical Natural Language Processing (BioNLP). In this work, we develop an LLM-based framework for solving the \u2026"}, {"title": "Improved transferability of self-supervised learning models through batch normalization finetuning", "link": "https://link.springer.com/article/10.1007/s10489-024-05758-7", "details": "K Sirotkin, M Escudero-Vi\u00f1olo, P Carballeira\u2026 - Applied Intelligence, 2024", "abstract": "Abundance of unlabelled data and advances in Self-Supervised Learning (SSL) have made it the preferred choice in many transfer learning scenarios. Due to the rapid and ongoing development of SSL approaches, practitioners are now faced with \u2026"}]
