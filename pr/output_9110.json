[{"title": "Time-to-Event Pretraining for 3D Medical Imaging", "link": "https://arxiv.org/pdf/2411.09361", "details": "Z Huo, JA Fries, A Lozano, JMJ Valanarasu\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "With the rise of medical foundation models and the growing availability of imaging data, scalable pretraining techniques offer a promising way to identify imaging biomarkers predictive of future disease risk. While current self-supervised methods \u2026"}, {"title": "Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning", "link": "https://arxiv.org/pdf/2411.00173", "details": "J Wu, D Wu, J Sun - arXiv preprint arXiv:2411.00173, 2024", "abstract": "Medical coding, the translation of unstructured clinical text into standardized medical codes, is a crucial but time-consuming healthcare practice. Though large language models (LLM) could automate the coding process and improve the efficiency of such \u2026"}, {"title": "Can the integration of new rules into a clinical decision support system reduce the incidence of acute kidney injury and hyperkalemia among hospitalized older adults \u2026", "link": "https://trialsjournal.biomedcentral.com/articles/10.1186/s13063-024-08569-w", "details": "A Payen, NE Tlili, E Cousein, L Ferret, A Le Bozec\u2026 - Trials, 2024", "abstract": "Clinical decision support systems (CDSSs) enable the automated, real-time detection of situations associated with a risk of adverse drug events (ADEs). However, the effectiveness of CDSS in reducing ADEs has yet to be demonstrated \u2026"}, {"title": "Information Extraction from Clinical Notes: Are We Ready to Switch to Large Language Models?", "link": "https://arxiv.org/pdf/2411.10020", "details": "Y Hu, X Zuo, Y Zhou, X Peng, J Huang, VK Keloth\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Backgrounds: Information extraction (IE) is critical in clinical natural language processing (NLP). While large language models (LLMs) excel on generative tasks, their performance on extractive tasks remains debated. Methods: We investigated \u2026"}]
