[{"title": "Scaling Pre-training to One Hundred Billion Data for Vision Language Models", "link": "https://arxiv.org/pdf/2502.07617", "details": "X Wang, I Alabdulmohsin, D Salz, Z Li, K Rong, X Zhai - arXiv preprint arXiv \u2026, 2025", "abstract": "We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric \u2026"}, {"title": "HiMix: Reducing Computational Complexity in Large Vision-Language Models", "link": "https://arxiv.org/pdf/2501.10318%3F", "details": "X Zhang, D Li, B Liu, Z Bao, Y Zhou, B Yang, Z Liu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Benefiting from recent advancements in large language models and modality alignment techniques, existing Large Vision-Language Models (LVLMs) have achieved prominent performance across a wide range of scenarios. However, the \u2026"}, {"title": "Scaling Large Vision-Language Models for Enhanced Multimodal Comprehension In Biomedical Image Analysis", "link": "https://arxiv.org/pdf/2501.15370", "details": "R Umeike, N Getty, F Xia, R Stevens - arXiv preprint arXiv:2501.15370, 2025", "abstract": "Large language models (LLMs) have demonstrated immense capabilities in understanding textual data and are increasingly being adopted to help researchers accelerate scientific discovery through knowledge extraction (information retrieval) \u2026"}, {"title": "MammoVLM: A generative large vision-language model for mammography-related diagnostic assistance", "link": "https://www.sciencedirect.com/science/article/pii/S1566253525000715", "details": "Z Cao, Z Deng, J Ma, J Hu, L Ma - Information Fusion, 2025", "abstract": "Inspired by the recent success of large language models (LLMs) in the general domain, many large multimodal models, such as vision-language models, have been developed to tackle problems across modalities. In the realm of breast cancer, which \u2026"}, {"title": "KIA: Knowledge-Guided Implicit Vision-Language Alignment for Chest X-Ray Report Generation", "link": "https://aclanthology.org/2025.coling-main.276.pdf", "details": "H Yin, S Zhou, P Wang, Z Wu, Y Hao - \u2026 of the 31st International Conference on \u2026, 2025", "abstract": "Report generation (RG) faces challenges in understanding complex medical images and establishing cross-modal semantic alignment in radiology image-report pairs. Previous methods often overlook fine-grained cross-modal interaction, leading to \u2026"}, {"title": "Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval", "link": "https://arxiv.org/pdf/2501.09134", "details": "D Deanda, YP Masupalli, J Yang, Y Lee, Z Cao\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Medical images and reports offer invaluable insights into patient health. The heterogeneity and complexity of these data hinder effective analysis. To bridge this gap, we investigate contrastive learning models for cross-domain retrieval, which \u2026"}, {"title": "FreqSpace-NeRF: A fourier-enhanced Neural Radiance Fields method via dual-domain contrastive learning for novel view synthesis", "link": "https://www.sciencedirect.com/science/article/pii/S009784932500010X", "details": "X Yu, X Tian, J Chen, Y Wang - Computers & Graphics, 2025", "abstract": "Abstract Inspired by Neural Radiance Field's (NeRF) groundbreaking success in novel view synthesis, current methods mostly employ variants of various deep neural network architectures, and use the combination of multi-scale feature maps with the \u2026"}, {"title": "Leveraging Language Models for Summarizing Mental State Examinations: A Comprehensive Evaluation and Dataset Release", "link": "https://aclanthology.org/2025.coling-main.182.pdf", "details": "NK Sahu, M Yadav, M Chaturvedi, S Gupta, HR Lone - Proceedings of the 31st \u2026, 2025", "abstract": "Mental health disorders affect a significant portion of the global population, with diagnoses primarily conducted through Mental State Examinations (MSEs). MSEs serve as structured assessments to evaluate behavioral and cognitive functioning \u2026"}, {"title": "Advancing General Multimodal Capability of Vision-language Models with Pyramid-descent Visual Position Encoding", "link": "https://arxiv.org/pdf/2501.10967", "details": "Z Chen, M Li, Z Chen, N Du, X Li, Y Zou - arXiv preprint arXiv:2501.10967, 2025", "abstract": "Vision-language Models (VLMs) have shown remarkable capabilities in advancing general artificial intelligence, yet the irrational encoding of visual positions persists in inhibiting the models' comprehensive perception performance across different levels \u2026"}]
