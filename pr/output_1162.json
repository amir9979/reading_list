'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Structural Pruning of Pre-trained Language Models via '
[{"title": "On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?", "link": "https://arxiv.org/pdf/2405.02266", "details": "M Zanella, IB Ayed - arXiv preprint arXiv:2405.02266, 2024", "abstract": "The development of large vision-language models, notably CLIP, has catalyzed research into effective adaptation techniques, with a particular focus on soft prompt tuning. Conjointly, test-time augmentation, which utilizes multiple augmented views \u2026"}, {"title": "Text Quality-Based Pruning for Efficient Training of Language Models", "link": "https://arxiv.org/pdf/2405.01582", "details": "V Sharma, K Padthe, N Ardalani, K Tirumala, R Howes\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In recent times training Language Models (LMs) have relied on computationally heavy training over massive datasets which makes this training process extremely laborious. In this paper we propose a novel method for numerically evaluating text \u2026"}, {"title": "Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph", "link": "https://arxiv.org/pdf/2405.02105", "details": "V Nechakhin, J D'Souza, S Eger - arXiv preprint arXiv:2405.02105, 2024", "abstract": "Structured science summaries or research contributions using properties or dimensions beyond traditional keywords enhances science findability. Current methods, such as those used by the Open Research Knowledge Graph (ORKG) \u2026"}, {"title": "Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge", "link": "https://arxiv.org/pdf/2404.05880", "details": "W Lu, Z Zeng, J Wang, Z Lu, Z Chen, H Zhuang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Jailbreaking attacks can enable Large Language Models (LLMs) to bypass the safeguard and generate harmful content. Existing jailbreaking defense methods have failed to address the fundamental issue that harmful knowledge resides within \u2026"}]
