[{"title": "Do language models understand time?", "link": "https://arxiv.org/pdf/2412.13845", "details": "X Ding, L Wang - arXiv preprint arXiv:2412.13845, 2024", "abstract": "Large language models (LLMs) have revolutionized video-based computer vision applications, including action recognition, anomaly detection, and video summarization. Videos inherently pose unique challenges, combining spatial \u2026"}, {"title": "LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation", "link": "https://arxiv.org/pdf/2412.15188", "details": "W Shi, X Han, C Zhou, W Liang, XV Lin, L Zettlemoyer\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present LlamaFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LlamaFusion \u2026"}, {"title": "GIRAFFE: Design Choices for Extending the Context Length of Visual Language Models", "link": "https://arxiv.org/pdf/2412.12735", "details": "M Li, L Li, S Gong, Q Liu - arXiv preprint arXiv:2412.12735, 2024", "abstract": "Visual Language Models (VLMs) demonstrate impressive capabilities in processing multimodal inputs, yet applications such as visual agents, which require handling multiple images and high-resolution videos, demand enhanced long-range \u2026"}, {"title": "CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology", "link": "https://arxiv.org/pdf/2412.12077", "details": "Y Sun, Y Si, C Zhu, X Gong, K Zhang, P Chen, Y Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The emergence of large multimodal models (LMMs) has brought significant advancements to pathology. Previous research has primarily focused on separately training patch-level and whole-slide image (WSI)-level models, limiting the \u2026"}, {"title": "APPLICATION OF ARTIFICIAL INTELLIGENCE IN MEDICAL IMAGE RECOGNITION", "link": "http://journals.maup.com.ua/index.php/it/issue/download/408/430%23page%3D23", "details": "S KOLOMOIETS - \u041d\u0410\u0423\u041a\u041e\u0412\u0406 \u041f\u0420\u0410\u0426\u0406 \u041c\u0406\u0416\u0420\u0415\u0413\u0406\u041e\u041d\u0410\u041b\u042c\u041d\u041e\u0407 \u0410\u041a\u0410\u0414\u0415\u041c\u0406\u0407 \u2026, 2024", "abstract": "The article is devoted to the study of image recognition in medicine using deep learning. The use of deep learning allows automating image processing and analysis, which significantly reduces the human factor and increases the accuracy of \u2026"}, {"title": "Supervision-free Vision-Language Alignment", "link": "https://arxiv.org/pdf/2501.04568", "details": "G Giannone, R Li, Q Feng, E Perevodchikov, R Chen\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Vision-language models (VLMs) have demonstrated remarkable potential in integrating visual and linguistic information, but their performance is often constrained by the need for extensive, high-quality image-text training data. Curation \u2026"}, {"title": "Token Preference Optimization with Self-Calibrated Visual-Anchored Rewards for Hallucination Mitigation", "link": "https://arxiv.org/pdf/2412.14487", "details": "J Gu, Y Wang, M Cao, P Bu, J Song, Y He, S Li\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Direct Preference Optimization (DPO) has been demonstrated to be highly effective in mitigating hallucinations in Large Vision Language Models (LVLMs) by aligning their outputs more closely with human preferences. Despite the recent progress \u2026"}, {"title": "Activating Distributed Visual Region within LLMs for Efficient and Effective Vision-Language Training and Inference", "link": "https://arxiv.org/pdf/2412.12785", "details": "S Wang, D Wang, C Zhou, Z Li, Z Fan, X Huang, Z Wei - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision-Language Models (LVLMs) typically learn visual capacity through visual instruction tuning, involving updates to both a projector and their LLM backbones. Drawing inspiration from the concept of visual region in the human brain \u2026"}, {"title": "FiVL: A Framework for Improved Vision-Language Alignment", "link": "https://arxiv.org/pdf/2412.14672", "details": "E Aflalo, GBM Stan, T Le, M Luo, S Rosenman, S Paul\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large Vision Language Models (LVLMs) have achieved significant progress in integrating visual and textual inputs for multimodal reasoning. However, a recurring challenge is ensuring these models utilize visual information as effectively as \u2026"}]
