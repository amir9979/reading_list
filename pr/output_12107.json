[{"title": "RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment", "link": "https://arxiv.org/pdf/2501.07525", "details": "D Gu, Y Gao, Y Zhou, M Zhou, D Metaxas - arXiv preprint arXiv:2501.07525, 2025", "abstract": "Automated chest radiographs interpretation requires both accurate disease classification and detailed radiology report generation, presenting a significant challenge in the clinical workflow. Current approaches either focus on classification \u2026"}, {"title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models", "link": "https://arxiv.org/pdf/2501.02955", "details": "W Hong, Y Cheng, Z Yang, W Wang, L Wang, X Gu\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "In recent years, vision language models (VLMs) have made significant advancements in video understanding. However, a crucial capability-fine-grained motion comprehension-remains under-explored in current benchmarks. To address \u2026"}, {"title": "DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision Language Models in Real-World Scenarios with Driving Theory Tests", "link": "https://arxiv.org/pdf/2501.04671", "details": "C Corbi\u00e8re, S Roburin, S Montariol, A Bosselut, A Alahi - arXiv preprint arXiv \u2026, 2025", "abstract": "Large vision-language models (LVLMs) augment language models with visual understanding, enabling multimodal reasoning. However, due to the modality gap between textual and visual data, they often face significant challenges, such as over \u2026"}, {"title": "Open-source small language models for personal medical assistant chatbots", "link": "https://www.sciencedirect.com/science/article/pii/S2666521224000644", "details": "M Magnini, G Aguzzi, S Montagna - Intelligence-Based Medicine, 2025", "abstract": "Medical chatbots are becoming essential components of telemedicine applications as tools to assist patients in the self-management of their conditions. This trend is particularly driven by advancements in natural language processing techniques with \u2026"}, {"title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models", "link": "https://arxiv.org/pdf/2501.05752", "details": "S Lee, H Park, J Kim, J Ok - arXiv preprint arXiv:2501.05752, 2025", "abstract": "Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer \u2026"}, {"title": "Mee-SLAM: Memory efficient endoscopic RGB SLAM with implicit scene representation", "link": "https://www.sciencedirect.com/science/article/pii/S0957417424031026", "details": "Y Zhou, T Li, Y Dai, J Zhang - Expert Systems with Applications, 2025", "abstract": "Endoscopic dense simultaneous localization and mapping (SLAM) plays a critical role in robot assisted surgery. Recently, SLAM systems based on neural implicit representation have demonstrated superior localization and real-time mapping \u2026"}, {"title": "Contrastive Learning in Multimodal Generative Models: Towards Interpretable and Robust 3D Image Synthesis", "link": "https://www.researchgate.net/profile/Andrew-Johnson-159/publication/387511076_Contrastive_Learning_in_Multimodal_Generative_Models_Towards_Interpretable_and_Robust_3D_Image_Synthesis/links/67720b6d117f340ec3e516d7/Contrastive-Learning-in-Multimodal-Generative-Models-Towards-Interpretable-and-Robust-3D-Image-Synthesis.pdf", "details": "A Johnson, R Lewis, D Jackson", "abstract": "Generative models have become a transformative technology in artificial intelligence, enabling the synthesis of high-quality data in domains such as computer vision, natural language processing, and multimodal understanding. Among these \u2026"}, {"title": "LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding", "link": "https://arxiv.org/pdf/2501.08282", "details": "H Li, J Chen, Z Wei, S Huang, T Hui, J Gao, X Wei\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Recent advancements in multimodal large language models (MLLMs) have shown promising results, yet existing approaches struggle to effectively handle both temporal and spatial localization simultaneously. This challenge stems from two key \u2026"}, {"title": "MedFILIP: Medical Fine-Grained Language-Image Pre-Training", "link": "https://arxiv.org/pdf/2501.10775", "details": "X Liang, X Li, F Li, J Jiang, Q Dong, W Wang, K Wang\u2026 - IEEE Journal of Biomedical \u2026, 2025", "abstract": "Medical vision-language pretraining (VLP) that leverages naturally-paired medical image-report data is crucial for medical image analysis. However, existing methods struggle to accurately characterize associations between images and diseases \u2026"}]
