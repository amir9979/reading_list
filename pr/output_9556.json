[{"title": "Multi-Hop Interpretable Meta Learning for Few-Shot Temporal Knowledge Graph Completion", "link": "https://www.sciencedirect.com/science/article/pii/S0893608024009109", "details": "L Bai, S Han, L Zhu - Neural Networks, 2024", "abstract": "Multi-hop path completion is a key part of temporal knowledge graph completion, which aims to infer complex relationships and obtain interpretable completion results. However, the traditional multi-hop path completion models mainly focus on \u2026"}, {"title": "Training and Evaluating Language Models with Template-based Data Generation", "link": "https://arxiv.org/pdf/2411.18104", "details": "Y Zhang - arXiv preprint arXiv:2411.18104, 2024", "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, these \u2026"}, {"title": "Mathematical Reasoning via Multi-step Self Questioning and Answering for Small Language Models", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9440-9_7", "details": "K Chen, J Wang, X Zhang - CCF International Conference on Natural Language \u2026, 2024", "abstract": "Mathematical reasoning is challenging for large language models (LLMs), while the scaling relationship concerning LLM capacity is under-explored. Existing works have tried to leverage the rationales of LLMs to train small language models (SLMs) for \u2026"}, {"title": "Language-Emphasized Cross-Lingual In-Context Learning for Multilingual LLM", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9437-9_26", "details": "J Li, X Wei, X Wang, N Zhuang, L Wang, J Dang - CCF International Conference on \u2026, 2024", "abstract": "With the recent rise of large language models (LLMs), in-context learning (ICL) has shown remarkable performance, eliminating the need for fine-tuning parameters and reducing the reliance on extensive labeled data. However, the intricacies of cross \u2026"}, {"title": "PqE: Zero-Shot Document Expansion for Dense Retrieval with Large Language Models", "link": "https://link.springer.com/chapter/10.1007/978-981-97-9431-7_8", "details": "J Liu, D Zou, N Chai, Y Yang, H Wang, X Song - CCF International Conference on \u2026, 2024", "abstract": "The dense retrieval model offers remarkable capabilities, yet it exhibits inconsistencies in the embedding space of queries and documents due to its dual- encoder structure. Addressing this limitation, we introduce Pseudo-query Embedding \u2026"}, {"title": "RedPajama: an Open Dataset for Training Large Language Models", "link": "https://arxiv.org/pdf/2411.12372%3F", "details": "M Weber, D Fu, Q Anthony, Y Oren, S Adams\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top \u2026"}, {"title": "Towards Tool Use Alignment of Large Language Models", "link": "https://aclanthology.org/2024.emnlp-main.82.pdf", "details": "ZY Chen, S Shen, G Shen, G Zhi, X Chen, Y Lin - Proceedings of the 2024 \u2026, 2024", "abstract": "Recently, tool use with LLMs has become one of the primary research topics as it can help LLM generate truthful and helpful responses. Existing studies on tool use with LLMs primarily focus on enhancing the tool-calling ability of LLMs. In practice, like \u2026"}, {"title": "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "link": "https://aclanthology.org/2024.emnlp-main.128.pdf", "details": "Y Wan, W Wang, Y Yang, Y Yuan, J Huang, P He\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT- 4\\. Despite LLMs' prowess in tasks like writing assistance, code generation, and \u2026"}, {"title": "Large Language Models Can Be Contextual Privacy Protection Learners", "link": "https://aclanthology.org/2024.emnlp-main.785.pdf", "details": "Y Xiao, Y Jin, Y Bai, Y Wu, X Yang, X Luo, W Yu\u2026 - Proceedings of the 2024 \u2026, 2024", "abstract": "Abstract The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data \u2026"}]
