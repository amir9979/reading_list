[{"title": "Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors", "link": "https://arxiv.org/pdf/2405.10529", "details": "J Sun, C Wang, J Wang, Y Zhang, C Xiao - arXiv preprint arXiv:2405.10529, 2024", "abstract": "Large language models have become increasingly prominent, also signaling a shift towards multimodality as the next frontier in artificial intelligence, where their embeddings are harnessed as prompts to generate textual content. Vision-language \u2026"}, {"title": "Privacy-Aware Visual Language Models", "link": "https://arxiv.org/pdf/2405.17423", "details": "L Samson, N Barazani, S Ghebreab, YM Asano - arXiv preprint arXiv:2405.17423, 2024", "abstract": "This paper aims to advance our understanding of how Visual Language Models (VLMs) handle privacy-sensitive information, a crucial concern as these technologies become integral to everyday life. To this end, we introduce a new benchmark \u2026"}, {"title": "Contrasting Multiple Representations with the Multi-Marginal Matching Gap", "link": "https://arxiv.org/pdf/2405.19532", "details": "Z Piran, M Klein, J Thornton, M Cuturi - arXiv preprint arXiv:2405.19532, 2024", "abstract": "Learning meaningful representations of complex objects that can be seen through multiple ($ k\\geq 3$) views or modalities is a core task in machine learning. Existing methods use losses originally intended for paired views, and extend them to $ k \u2026"}, {"title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions", "link": "https://arxiv.org/pdf/2405.19744", "details": "C Li, W Yang, J Zhang, J Lu, S Wang, C Zong - arXiv preprint arXiv:2405.19744, 2024", "abstract": "Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples \u2026"}, {"title": "Towards Comprehensive and Efficient Post Safety Alignment of Large Language Models via Safety Patching", "link": "https://arxiv.org/pdf/2405.13820", "details": "W Zhao, Y Hu, Z Li, Y Deng, Y Zhao, B Qin, TS Chua - arXiv preprint arXiv \u2026, 2024", "abstract": "Safety alignment of large language models (LLMs) has been gaining increasing attention. However, current safety-aligned LLMs suffer from the fragile and imbalanced safety mechanisms, which can still be induced to generate unsafe \u2026"}]
