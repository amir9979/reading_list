[{"title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions", "link": "https://arxiv.org/pdf/2405.19744", "details": "C Li, W Yang, J Zhang, J Lu, S Wang, C Zong - arXiv preprint arXiv:2405.19744, 2024", "abstract": "Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples \u2026"}, {"title": "Verbalized Machine Learning: Revisiting Machine Learning with Language Models", "link": "https://arxiv.org/pdf/2406.04344", "details": "TZ Xiao, R Bamler, B Sch\u00f6lkopf, W Liu - arXiv preprint arXiv:2406.04344, 2024", "abstract": "Motivated by the large progress made by large language models (LLMs), we introduce the framework of verbalized machine learning (VML). In contrast to conventional machine learning models that are typically optimized over a continuous \u2026"}, {"title": "Exploring Clean Label Backdoor Attacks and Defense in Language Models", "link": "https://ieeexplore.ieee.org/abstract/document/10549768/", "details": "S Zhao, LA Tuan, J Fu, J Wen, W Luo - IEEE/ACM Transactions on Audio, Speech \u2026, 2024", "abstract": "Despite being widely applied, pre-trained language models have been proven vulnerable to backdoor attacks. Backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger \u2026"}, {"title": "Specialising and Analysing Instruction-Tuned and Byte-Level Language Models for Organic Reaction Prediction", "link": "https://arxiv.org/pdf/2405.10625", "details": "J Pang, I Vuli\u0107 - arXiv preprint arXiv:2405.10625, 2024", "abstract": "Transformer-based encoder-decoder models have demonstrated impressive results in chemical reaction prediction tasks. However, these models typically rely on pretraining using tens of millions of unlabelled molecules, which can be time \u2026"}, {"title": "TAIA: Large Language Models are Out-of-Distribution Data Learners", "link": "https://arxiv.org/pdf/2405.20192", "details": "S Jiang, Y Liao, Y Zhang, Y Wang, Y Wang - arXiv preprint arXiv:2405.20192, 2024", "abstract": "Fine-tuning on task-specific question-answer pairs is a predominant method for enhancing the performance of instruction-tuned large language models (LLMs) on downstream tasks. However, in certain specialized domains, such as healthcare or \u2026"}, {"title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans", "link": "https://arxiv.org/pdf/2406.04215", "details": "Y Sakai, H Kamigaito, T Watanabe - arXiv preprint arXiv:2406.04215, 2024", "abstract": "It is very challenging to curate a dataset for language-specific knowledge and common sense in order to evaluate natural language understanding capabilities of language models. Due to the limitation in the availability of annotators, most current \u2026"}, {"title": "Zyda: A 1.3 T Dataset for Open Language Modeling", "link": "https://arxiv.org/pdf/2406.01981", "details": "Y Tokpanov, B Millidge, P Glorioso, J Pilault, A Ibrahim\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State- of-the-art language models, even at relatively smaller sizes, typically require training \u2026"}, {"title": "LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback", "link": "https://arxiv.org/pdf/2406.01771", "details": "W Lai, M Mesgar, A Fraser - arXiv preprint arXiv:2406.01771, 2024", "abstract": "To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones. While recent multilingual LLMs \u2026"}, {"title": "Typography Leads Semantic Diversifying: Amplifying Adversarial Transferability across Multimodal Large Language Models", "link": "https://arxiv.org/pdf/2405.20090", "details": "H Cheng, E Xiao, J Cao, L Yang, K Xu, J Gu, R Xu - arXiv preprint arXiv:2405.20090, 2024", "abstract": "Following the advent of the Artificial Intelligence (AI) era of large models, Multimodal Large Language Models (MLLMs) with the ability to understand cross-modal interactions between vision and text have attracted wide attention. Adversarial \u2026"}]
