[{"title": "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "link": "https://arxiv.org/pdf/2405.16908", "details": "G Yona, R Aharoni, M Geva - arXiv preprint arXiv:2405.16908, 2024", "abstract": "We posit that large language models (LLMs) should be capable of expressing their intrinsic uncertainty in natural language. For example, if the LLM is equally likely to output two contradicting answers to the same question, then its generated response \u2026"}, {"title": "Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models", "link": "https://arxiv.org/pdf/2405.16282", "details": "A Kumar, R Morabito, S Umbet, J Kabbara, A Emami - arXiv preprint arXiv \u2026, 2024", "abstract": "As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models \u2026"}, {"title": "Sparse Spectral Training and Inference on Euclidean and Hyperbolic Neural Networks", "link": "https://arxiv.org/pdf/2405.15481", "details": "J Zhao, Y Zhang, X Li, H Liu, CV Cannistraci - arXiv preprint arXiv:2405.15481, 2024", "abstract": "The growing computational demands posed by increasingly number of neural network's parameters necessitate low-memory-consumption training approaches. Previous memory reduction techniques, such as Low-Rank Adaptation (LoRA) and \u2026"}, {"title": "Mosaic Memory: Fuzzy Duplication in Copyright Traps for Large Language Models", "link": "https://arxiv.org/pdf/2405.15523", "details": "I Shilov, M Meeus, YA de Montjoye - arXiv preprint arXiv:2405.15523, 2024", "abstract": "The immense datasets used to develop Large Language Models (LLMs) often include copyright-protected content, typically without the content creator's consent. Copyright traps have been proposed to be injected into the original content \u2026"}, {"title": "Large Language Models Can Self-Correct with Minimal Effort", "link": "https://arxiv.org/pdf/2405.14092", "details": "Z Wu, Q Zeng, Z Zhang, Z Tan, C Shen, M Jiang - arXiv preprint arXiv:2405.14092, 2024", "abstract": "Intrinsic self-correct was a method that instructed large language models (LLMs) to verify and correct their responses without external feedback. Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet. We find that a simple \u2026"}, {"title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models", "link": "https://arxiv.org/pdf/2405.16057", "details": "X Lu, A Zhou, Y Xu, R Zhang, P Gao, H Li - arXiv preprint arXiv:2405.16057, 2024", "abstract": "Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine- tuning and deployment. Current post-training pruning methods, while reducing the \u2026"}, {"title": "Federated Domain-Specific Knowledge Transfer on Large Language Models Using Synthetic Data", "link": "https://arxiv.org/pdf/2405.14212", "details": "H Li, X Zhao, D Guo, H Gu, Z Zeng, Y Han, Y Song\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "As large language models (LLMs) demonstrate unparalleled performance and generalization ability, LLMs are widely used and integrated into various applications. When it comes to sensitive domains, as commonly described in federated learning \u2026"}, {"title": "DEEM: Diffusion Models Serve as the Eyes of Large Language Models for Image Perception", "link": "https://arxiv.org/pdf/2405.15232", "details": "R Luo, Y Li, L Chen, W He, TE Lin, Z Liu, L Zhang\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension \u2026"}, {"title": "Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications", "link": "https://arxiv.org/pdf/2405.15877", "details": "Y Li, C Zhao, H Lee, E Chang, Y Shi, V Chandra - arXiv preprint arXiv:2405.15877, 2024", "abstract": "Large language models (LLMs) significantly enhance the performance of various applications, but they are computationally intensive and energy-demanding. This makes it challenging to deploy them on devices with limited resources, such as \u2026"}]
