[{"title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding", "link": "https://arxiv.org/pdf/2409.03363", "details": "C Wang, Y Wang, B Hooi, Y Cai, N Peng, KW Chang - arXiv preprint arXiv \u2026, 2024", "abstract": "The training data in large language models is key to their success, but it also presents privacy and security risks, as it may contain sensitive information. Detecting pre-training data is crucial for mitigating these concerns. Existing methods typically \u2026"}, {"title": "ReXamine-Global: A Framework for Uncovering Inconsistencies in Radiology Report Generation Metrics", "link": "https://arxiv.org/pdf/2408.16208", "details": "O Banerjee, A Saenz, K Wu, W Clements, A Zia\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "Given the rapidly expanding capabilities of generative AI models for radiology, there is a need for robust metrics that can accurately measure the quality of AI-generated radiology reports across diverse hospitals. We develop ReXamine-Global, a LLM \u2026"}, {"title": "Assessing GPT and DeepL for Terminology Translation in the Medical Domain: A Comparative Study on the Human Phenotype Ontology", "link": "https://www.researchsquare.com/article/rs-4836251/latest.pdf", "details": "N Richard, A BERGER, K Dominik, T MUELLER\u2026 - 2024", "abstract": "Background This paper presents a comparative study of two state-of-the-art language models, OpenAI's GPT and DeepL, in the context of terminology translation within the medical domain. Methods This study was conducted on the Human \u2026"}, {"title": "LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin", "link": "https://aclanthology.org/2024.acl-long.106.pdf", "details": "S Dou, E Zhou, Y Liu, S Gao, W Shen, L Xiong, Y Zhou\u2026 - Proceedings of the 62nd \u2026, 2024", "abstract": "Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Substantially increasing instruction data is a direct solution to \u2026"}, {"title": "Large Language Models Can Learn Representation in Natural Language", "link": "https://aclanthology.org/2024.findings-acl.542.pdf", "details": "Y Guo, Y Liang, D Zhao, N Duan - Findings of the Association for Computational \u2026, 2024", "abstract": "One major challenge for Large Language Models (LLMs) is completing complex tasks involving multiple entities, such as tool APIs. To tackle this, one approach is to retrieve relevant entities to enhance LLMs in task completion. A crucial issue here is \u2026"}, {"title": "LLM with Relation Classifier for Document-Level Relation Extraction", "link": "https://arxiv.org/pdf/2408.13889", "details": "X Li, K Chen, Y Long, M Zhang - arXiv preprint arXiv:2408.13889, 2024", "abstract": "Large language models (LLMs) create a new paradigm for natural language processing. Despite their advancement, LLM-based methods still lag behind traditional approaches in document-level relation extraction (DocRE), a critical task \u2026"}, {"title": "Analysis of Plan-based Retrieval for Grounded Text Generation", "link": "https://arxiv.org/pdf/2408.10490", "details": "A Godbole, N Monath, S Kim, AS Rawat, A McCallum\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In text generation, hallucinations refer to the generation of seemingly coherent text that contradicts established knowledge. One compelling hypothesis is that hallucinations occur when a language model is given a generation task outside its \u2026"}, {"title": "Automatic Medical Report Generation: Methods and Applications", "link": "https://arxiv.org/pdf/2408.13988", "details": "L Guo, AM Tahir, D Zhang, ZJ Wang, RK Ward - arXiv preprint arXiv:2408.13988, 2024", "abstract": "The increasing demand for medical imaging has surpassed the capacity of available radiologists, leading to diagnostic delays and potential misdiagnoses. Artificial intelligence (AI) techniques, particularly in automatic medical report generation \u2026"}, {"title": "Similarity Retrieval and Medical Cross-Modal Attention Based Medical Report Generation", "link": "https://link.springer.com/chapter/10.1007/978-981-97-7232-2_12", "details": "X Dong, H Pan, H Lan, K Zhang, C Chen - Asia-Pacific Web (APWeb) and Web-Age \u2026, 2024", "abstract": "Medical report generation is a time-consuming and knowledge-intensive task performed by radiologists to describe various regions within medical images. Writing report manually is prone to subjective bias and errors. Consequently, medical report \u2026"}]
