[{"title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning", "link": "https://arxiv.org/pdf/2505.12477", "details": "H Van Assel, M Ibrahim, T Biancalani, A Regev\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Reconstruction and joint embedding have emerged as two leading paradigms in Self Supervised Learning (SSL). Reconstruction methods focus on recovering the original sample from a different view in input space. On the other hand, joint embedding \u2026", "entry_id": "http://arxiv.org/abs/2505.12477v1", "updated": "2025-05-18 15:54:55", "published": "2025-05-18 15:54:55", "authors": "Hugues Van Assel;Mark Ibrahim;Tommaso Biancalani;Aviv Regev;Randall Balestriero", "summary": "Reconstruction and joint embedding have emerged as two leading paradigms in\nSelf Supervised Learning (SSL). Reconstruction methods focus on recovering the\noriginal sample from a different view in input space. On the other hand, joint\nembedding methods align the representations of different views in latent space.\nBoth approaches offer compelling advantages, yet practitioners lack clear\nguidelines for choosing between them. In this work, we unveil the core\nmechanisms that distinguish each paradigm. By leveraging closed form solutions\nfor both approaches, we precisely characterize how the view generation process,\ne.g. data augmentation, impacts the learned representations. We then\ndemonstrate that, unlike supervised learning, both SSL paradigms require a\nminimal alignment between augmentations and irrelevant features to achieve\nasymptotic optimality with increasing sample size. Our findings indicate that\nin scenarios where these irrelevant features have a large magnitude, joint\nembedding methods are preferable because they impose a strictly weaker\nalignment condition compared to reconstruction based methods. These results not\nonly clarify the trade offs between the two paradigms but also substantiate the\nempirical success of joint embedding approaches on real world challenging\ndatasets.", "comment": "33 pages, 9 figures", "journal_ref": null, "primary_category": "cs.LG", "categories": "cs.LG;cs.AI;cs.CV", "links": "http://arxiv.org/abs/2505.12477v1;http://arxiv.org/pdf/2505.12477v1", "pdf_url": "http://arxiv.org/pdf/2505.12477v1"}, {"title": "Explainable depression detection using handwriting features", "link": "https://link.springer.com/chapter/10.1007/978-981-96-0994-9_34", "details": "F Prinzi, G Raimo, P Barbiero, G Cordasco, P Li\u00f2\u2026 - Advanced Neural Artificial \u2026, 2025", "abstract": "Depression is considered one of the most prevalent diseases worldwide, with a rapid increase in recent years. An interesting area of research for depression detection is the analysis of handwriting and drawing. Although machine learning models have \u2026"}, {"title": "Compatible Unsupervised Anomaly Detection with Multi-Perspective Spatio-Temporal Learning", "link": "https://www.computer.org/csdl/proceedings-article/icde/2025/360300e066/26FZChTX8L6", "details": "T Chen, B Zheng, S Liu, Z Fan, Z Xu, L Yan, K Zeng\u2026 - 2025 IEEE 41st International \u2026, 2025", "abstract": "Anomaly detection is one of the most significant tasks in industrial automatic maintenance, such as in distributed cloud systems. However, the implementation of existing anomaly detection methods is still challenging in (i) capturing the complex \u2026"}, {"title": "AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer", "link": "https://arxiv.org/pdf/2505.16463", "details": "J Shan, J Wang, L Zhao, L Cai, H Zhang, I Liritzis - arXiv preprint arXiv:2505.16463, 2025", "abstract": "Recently, vision transformers (ViTs) have achieved excellent performance on vision tasks by measuring the global self-attention among the image patches. Given $ n $ patches, they will have quadratic complexity such as $\\mathcal {O}(n^ 2) $ and the \u2026", "entry_id": "http://arxiv.org/abs/2505.16463v2", "updated": "2025-05-25 08:10:35", "published": "2025-05-22 09:44:44", "authors": "Jiquan Shan;Junxiao Wang;Lifeng Zhao;Liang Cai;Hongyuan Zhang;Ioannis Liritzis", "summary": "Recently, vision transformers (ViTs) have achieved excellent performance on\nvision tasks by measuring the global self-attention among the image patches.\nGiven $n$ patches, they will have quadratic complexity such as\n$\\mathcal{O}(n^2)$ and the time cost is high when splitting the input image\nwith a small granularity. Meanwhile, the pivotal information is often randomly\ngathered in a few regions of an input image, some tokens may not be helpful for\nthe downstream tasks. To handle this problem, we introduce an anchor-based\nefficient vision transformer (AnchorFormer), which employs the anchor tokens to\nlearn the pivotal information and accelerate the inference. Firstly, by\nestimating the bipartite attention between the anchors and tokens, the\ncomplexity will be reduced from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(mn)$, where\n$m$ is an anchor number and $m < n$. Notably, by representing the anchors with\nthe neurons in a neural layer, we can differentiable learn these distributions\nand approximate global self-attention through the Markov process. Moreover, we\nextend the proposed model to three downstream tasks including classification,\ndetection, and segmentation. Extensive experiments show the effectiveness of\nour AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs\nreduction on ImageNet classification, 81.3% higher mAP on COCO detection under\ncomparable FLOPs, as compared to the current baselines.", "comment": null, "journal_ref": null, "primary_category": "cs.CV", "categories": "cs.CV;cs.LG", "links": "http://arxiv.org/abs/2505.16463v2;http://arxiv.org/pdf/2505.16463v2", "pdf_url": "http://arxiv.org/pdf/2505.16463v2"}, {"title": "FFP: Robust, Interpretable, and Lightweight Framework for Medical Image Diagnosis", "link": "https://www.taylorfrancis.com/chapters/edit/10.1201/9781032632483-5/ffp-robust-interpretable-lightweight-framework-medical-image-diagnosis-shancheng-jiang-xing-zhang-kun-xiang-jiawen-pan-wenxiao-zheng-jiahao-xu", "details": "S Jiang, X Zhang, K Xiang, J Pan, W Zheng, J Xu - Cutting-Edge Artificial Intelligence \u2026, 2025", "abstract": "Deep learning models have been widely used in challenging tasks such as computer- aided disease diagnosis based on medical images. However, the model-wise vulnerability under adversarial perturbation hinders its deployment in practical \u2026"}, {"title": "Shall We Stop Using End-to-End Learning? A Short-Survey of Recent Concept-Based Approaches", "link": "https://link.springer.com/chapter/10.1007/978-981-96-0994-9_23", "details": "G Ciravegna - Advanced Neural Artificial Intelligence: Theories and \u2026, 2025", "abstract": "Deep Learning (DL) is driving AI-based technologies to unpredictable achievements by automatically extracting rich data representations through an End-to-End (E2E) learning process. However, E2E models are intrinsically black boxes, hindering the \u2026"}, {"title": "SIMSE: A Contrastive Learning Method Combining Sample Importance Metric and Semantic Enhancement", "link": "https://www.sciencedirect.com/science/article/pii/S0957417425016665", "details": "Y Gao, Z Zheng, W Huang, X Lin - Expert Systems with Applications, 2025", "abstract": "In complex application scenarios, the lack of accurate labels for training samples has positioned contrastive learning a key focus in self-supervised learning. This approach effectively extracts meaningful representations from unlabeled data. A \u2026"}, {"title": "Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization", "link": "https://arxiv.org/pdf/2505.15660", "details": "J Zhou, K Ye, J Liu, T Ma, Z Wang, R Qiu, KY Lin\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "The generalization capabilities of vision-language-action (VLA) models to unseen tasks are crucial to achieving general-purpose robotic manipulation in open-world settings. However, the cross-task generalization capabilities of existing VLA models \u2026", "entry_id": "http://arxiv.org/abs/2505.15660v2", "updated": "2025-05-24 15:33:43", "published": "2025-05-21 15:35:57", "authors": "Jiaming Zhou;Ke Ye;Jiayi Liu;Teli Ma;Zifan Wang;Ronghe Qiu;Kun-Yu Lin;Zhilin Zhao;Junwei Liang", "summary": "The generalization capabilities of vision-language-action (VLA) models to\nunseen tasks are crucial to achieving general-purpose robotic manipulation in\nopen-world settings. However, the cross-task generalization capabilities of\nexisting VLA models remain significantly underexplored. To address this gap, we\nintroduce AGNOSTOS, a novel simulation benchmark designed to rigorously\nevaluate cross-task zero-shot generalization in manipulation. AGNOSTOS\ncomprises 23 unseen manipulation tasks for testing, distinct from common\ntraining task distributions, and incorporates two levels of generalization\ndifficulty to assess robustness. Our systematic evaluation reveals that current\nVLA models, despite being trained on diverse datasets, struggle to generalize\neffectively to these unseen tasks. To overcome this limitation, we propose\nCross-Task In-Context Manipulation (X-ICM), a method that conditions large\nlanguage models (LLMs) on in-context demonstrations from seen tasks to predict\naction sequences for unseen tasks. Additionally, we introduce a dynamics-guided\nsample selection strategy that identifies relevant demonstrations by capturing\ncross-task dynamics. On AGNOSTOS, X-ICM significantly improves cross-task\nzero-shot generalization performance over leading VLAs. We believe AGNOSTOS and\nX-ICM will serve as valuable tools for advancing general-purpose robotic\nmanipulation.", "comment": "Project Page: https://jiaming-zhou.github.io/AGNOSTOS", "journal_ref": null, "primary_category": "cs.RO", "categories": "cs.RO;cs.CV", "links": "http://arxiv.org/abs/2505.15660v2;http://arxiv.org/pdf/2505.15660v2", "pdf_url": "http://arxiv.org/pdf/2505.15660v2"}]
