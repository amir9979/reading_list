'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [A deep transfer learning approach for sleep stage classifica'
[{"title": "Jamba: A Hybrid Transformer-Mamba Language Model", "link": "https://arxiv.org/pdf/2403.19887", "details": "O Lieber, B Lenz, H Bata, G Cohen, J Osin\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both \u2026"}, {"title": "Self-Training Language Models in Arithmetic Reasoning", "link": "https://openreview.net/pdf%3Fid%3DzBh79GuLNO", "details": "M Kadl\u010d\u00edk, M \u0160tef\u00e1nik, O Sotolar, V Martinek - ICLR 2024 Workshop on Large Language \u2026", "abstract": "Recent work shows impressive efficiency of methods for modeling human preferences but achieving further improvements with these methods requires costly human annotations of the quality of model outputs. In this work, we study the \u2026"}]
