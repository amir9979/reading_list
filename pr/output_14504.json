[{"title": "LANGALIGN: Enhancing Non-English Language Models via Cross-Lingual Embedding Alignment", "link": "https://arxiv.org/pdf/2503.18603%3F", "details": "JM Kim, YJ Lee, HJ Choi, S Jung - arXiv preprint arXiv:2503.18603, 2025", "abstract": "While Large Language Models have gained attention, many service developers still rely on embedding-based models due to practical constraints. In such cases, the quality of fine-tuning data directly impacts performance, and English datasets are \u2026"}, {"title": "When Debate Fails: Bias Reinforcement in Large Language Models", "link": "https://arxiv.org/pdf/2503.16814", "details": "J Oh, M Jeong, J Ko, SY Yun - arXiv preprint arXiv:2503.16814, 2025", "abstract": "Large Language Models $($ LLMs $) $ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self \u2026"}, {"title": "SWI: Speaking with Intent in Large Language Models", "link": "https://arxiv.org/pdf/2503.21544", "details": "Y Yin, EJ Hwang, G Carenini - arXiv preprint arXiv:2503.21544, 2025", "abstract": "Intent, typically clearly formulated and planned, functions as a cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated \u2026"}, {"title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers", "link": "https://arxiv.org/pdf/2503.00865", "details": "Y Zhao, C Liu, Y Deng, J Ying, M Aljunied, Z Li, L Bing\u2026 - arXiv preprint arXiv \u2026, 2025", "abstract": "Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced \u2026"}, {"title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models", "link": "https://arxiv.org/pdf/2503.01763%3F", "details": "Z Shi, Y Wang, L Yan, P Ren, S Wang, D Yin, Z Ren - arXiv preprint arXiv:2503.01763, 2025", "abstract": "Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful \u2026"}, {"title": "\" Whose Side Are You On?\" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection", "link": "https://arxiv.org/pdf/2503.20797", "details": "M Haroon, M Wojcieszak, A Chhabra - arXiv preprint arXiv:2503.20797, 2025", "abstract": "The rapid growth of social media platforms has led to concerns about radicalization, filter bubbles, and content bias. Existing approaches to classifying ideology are limited in that they require extensive human effort, the labeling of large datasets, and \u2026"}, {"title": "Privacy Auditing for Large Language Models with Natural Identifiers", "link": "https://openreview.net/pdf%3Fid%3Djp4XlcpRIW", "details": "L Rossi, B Marek, F Boenisch, A Dziedzic - ICLR 2025 Workshop on Foundation Models in \u2026", "abstract": "The privacy auditing for large language models (LLMs) faces significant challenges. Membership inference attacks, once considered a practical privacy auditing tool, are unreliable for pretrained LLMs due to the lack of non-member data from the same \u2026"}, {"title": "OUTLIER-AWARE PREFERENCE OPTIMIZATION FOR LARGE LANGUAGE MODELS", "link": "https://openreview.net/pdf%3Fid%3DYevRFGa9I7", "details": "P Srivastava, SS Nalli, A Deshpande, A Sharma - \u2026 in Foundation Models: The Next Frontier in \u2026", "abstract": "Aligning large language models (LLMs) to user preferences often relies on learning a reward model as a proxy from feedback. However, such reward models can fail on out-of-distribution examples and, if kept static, may reinforce incorrect preferences \u2026"}]
