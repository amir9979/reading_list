'*Sent by Google Scholar Alerts (scholaralerts-noreply@google.com). Created by [fire](https://fire.fundersclub.com/).*\n\n---\n### \n\n### \n\n### [PDF] [Can 3D Vision-Language Models Truly Understand Natural'
[{"title": "Adaptive Prompt Routing for Arbitrary Text Style Transfer with Pre-trained Language Models", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29832/31446", "details": "Q Liu, J Qin, W Ye, H Mou, Y He, K Wang - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "Recently, arbitrary text style transfer (TST) has made significant progress with the paradigm of prompt learning. In this paradigm, researchers often design or search for a fixed prompt for any input. However, existing evidence shows that large language \u2026"}, {"title": "W2P: Switching from Weak Supervision to Partial Supervision for Semantic Segmentation", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/28531/29035", "details": "F Zhang, T Pan, JH Yong, B Wang - Proceedings of the AAAI Conference on Artificial \u2026, 2024", "abstract": "Current weakly-supervised semantic segmentation (WSSS) techniques concentrate on enhancing class activation maps (CAMs) with image-level annotations. Yet, the emphasis on producing these pseudo-labels often overshadows the pivotal role of \u2026"}, {"title": "LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction", "link": "https://arxiv.org/pdf/2403.15464", "details": "H Cui, Z Shen, J Zhang, H Shao, L Qin, JC Ho, C Yang - arXiv preprint arXiv \u2026, 2024", "abstract": "Electronic health records (EHRs) contain valuable patient data for health-related prediction tasks, such as disease prediction. Traditional approaches rely on supervised learning methods that require large labeled datasets, which can be \u2026"}, {"title": "A Novel Sentence Transformer-based Natural Language Processing Approach for Schema Mapping of Electronic Health Records to the OMOP Common Data Model", "link": "https://www.medrxiv.org/content/medrxiv/early/2024/03/24/2024.03.21.24304616.full.pdf", "details": "X Zhou, LS Dhingra, A Aminorroaya, P Adejumo\u2026 - medRxiv, 2024", "abstract": "Mapping electronic health records (EHR) data to common data models (CDMs) enables the standardization of clinical records, enhancing interoperability and enabling large-scale, multi-centered clinical investigations. Using 2 large publicly \u2026"}, {"title": "Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity", "link": "https://arxiv.org/pdf/2403.05257", "details": "S Hoshino, A Kato, S Murakami, P Zhang - arXiv preprint arXiv:2403.05257, 2024", "abstract": "Learning better sentence embeddings leads to improved performance for natural language understanding tasks including semantic textual similarity (STS) and natural language inference (NLI). As prior studies leverage large-scale labeled NLI datasets \u2026"}, {"title": "RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models", "link": "https://arxiv.org/html/2403.02271v1", "details": "S Najafi, A Fyshe - arXiv preprint arXiv:2403.02271, 2024", "abstract": "Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter- efficient fine-tuning methods that optimize input prompts or adjust a small number of \u2026"}, {"title": "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models", "link": "https://arxiv.org/html/2403.02178v1", "details": "C Chen, X Wang, TE Lin, A Lv, Y Wu, X Gao, JR Wen\u2026 - arXiv preprint arXiv \u2026, 2024", "abstract": "In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains. Earlier fine- tuning approaches sought to mitigate this by leveraging more precise supervisory \u2026"}, {"title": "Concept-aware Data Construction Improves In-context Learning of Language Models", "link": "https://arxiv.org/pdf/2403.09703", "details": "M \u0160tef\u00e1nik, M Kadl\u010d\u00edk, P Sojka - arXiv preprint arXiv:2403.09703, 2024", "abstract": "Many recent language models (LMs) are capable of in-context learning (ICL), manifested in the LMs' ability to perform a new task solely from natural-language instruction. Previous work curating in-context learners assumes that ICL emerges \u2026"}, {"title": "DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency", "link": "https://ojs.aaai.org/index.php/AAAI/article/download/29578/30971", "details": "W Yao, K Yin, WK Cheung, J Liu, J Qin - Proceedings of the AAAI Conference on \u2026, 2024", "abstract": "The combination of electronic health records (EHR) and medical images is crucial for clinicians in making diagnoses and forecasting prognoses. Strategically fusing these two data modalities has great potential to improve the accuracy of machine learning \u2026"}]
